<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1401 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1401</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1401</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-39b19ea254b0952f2abd23ad899420749816bb1d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/39b19ea254b0952f2abd23ad899420749816bb1d" target="_blank">The Predictron: End-To-End Learning and Planning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps that accumulates internal rewards and values over multiple planning depths.</p>
                <p><strong>Paper Abstract:</strong> One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1401.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1401.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predictron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Predictron (abstract internal MRP model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end differentiable architecture that learns an abstract internal Markov Reward Process (MRP) as a latent world model; it unrolls this internal model multiple imagined planning steps, accumulates internal rewards/discounts/values, and combines multi-depth predictions via a learned λ-accumulator to predict external value functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Predictron</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A latent/abstract world model implemented as a repeatable core that maps an internal state s -> (s', r, γ, λ) (internal next-state, internal reward vector, internal discount, and λ-gating). The core is a convolutional neural module (shared across up to K unrolled model steps) whose outputs are combined by an accumulator to form k-step preturns and a λ-weighted mixture (λ-preturn). The predictron is trained end-to-end to make its accumulated internal returns match Monte-Carlo external returns (value functions) rather than to reconstruct observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / abstract internal MRP</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Value prediction in procedurally-generated random mazes and a Mujoco pool simulator (pixel inputs); evaluated on many general value functions (GVFs).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mean-squared error / Root-mean-squared error on predicted value functions vs Monte-Carlo returns (normalized returns); training loss is MSE on returns; additional evaluation with task-based metrics (e.g., pocketing events).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitatively reported as significantly lower RMSE than feedforward and recurrent baselines; near-zero training error on a deterministic-policy trajectory prediction maze task; no single scalar RMSE reported in paper figures, but aggregated RMSE plots show consistent improvement. Additionally, in a decision test on pool the predictron selected initial conditions that pocketed 27 coloured balls in 50 episodes versus 10/50 for an equally deep convolutional baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: the internal model is explicitly 'abstract' and not required to correspond to real-world semantics (hence often a black-box), but emergent structure is interpretable — e.g., weighted k-step preturns can be visualized to reveal sequential planning steps building a trajectory, and per-prediction adaptive depth (number of internal steps used) can be inspected and plotted.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of weighted preturns (showing how the predicted trajectory decomposes across internal steps), analysis of λ-weights and derived 'thinking depth' distributions per prediction type, and inspection of internal rewards/discounts/values; consistency updates encourage internal self-consistency (used as introspection signal).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Architecture used up to K=16 model steps, resulting in networks up to 52 layers deep (2 encoder conv layers + 3*16 conv core layers + 2 fully connected layers). Parameter counts reported in capacity comparisons: example totals include ≈1.28M and ≈4.85M parameters for certain configurations. Training uses minibatches of 100, Adam optimizer (lr=0.001). No GPU-hours/training-time figures are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Empirically more sample/parameter efficient for value prediction than conventional feedforward or recurrent convolutional networks of comparable or larger parameter counts; structure (predictron) matters more than raw parameter count. Claimed to potentially scale better than Value Iteration Networks in domains without natural 2D encodings because it plans via imagined trajectories in a compact abstract state space instead of convolving over entire spatial maps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On random maze tasks the predictron achieved near-perfect predictions for a deterministic-policy trajectory prediction task and lower aggregated RMSE than baselines for connectivity predictions; on the pool task it yielded lower prediction RMSE across 280 GVFs and, when used to select initial shot parameters, resulted in 27 pocketing events in 50 episodes versus 10 for an equally deep convolutional baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Training the internal model end-to-end to predict value functions (rather than to reconstruct pixels) focuses model capacity on task-relevant predictions; higher fidelity in predicted values translated into improved decision-making in the pool domain (more pocketing events). Thus, task-relevant abstraction (rather than full observation fidelity) produced strong utility for downstream decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Designing the model as a fully abstract MRP trades human interpretability for task-relevant compactness — the model need not produce pixel-perfect rollouts. The λ-accumulator enables adaptive compute but adds gating parameters to learn. Using internal rewards/discounts and λ weighting improved fidelity and utility but increases architectural complexity. The predictron attains better prediction accuracy without explicit observation reconstruction, implying a trade-off: prioritize value-prediction fidelity over generative fidelity of observations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: (1) learn internal rewards r and discounts γ (MRP structure) rather than only hidden-to-hidden transforms; (2) use a k-step and/or λ-accumulator to combine multi-depth preturns; (3) learn λ weights (per state, per prediction) to enable adaptive depth; (4) usage-weighted loss vs uniform averaging of preturn losses; (5) shared vs unshared core weights (recurrent vs feedforward unrolled); (6) optional skip connections (Δs updates) and up to 16 internal planning steps; (7) end-to-end supervised regression to Monte-Carlo returns and optional consistency updates on unlabelled data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Direct empirical comparisons in the paper show the predictron outperforms conventional feedforward and recurrent convolutional networks (with or without skip connections) across multiple depths and parameter budgets. Compared to VINs, the predictron plans via imagined trajectories in an abstract state space rather than performing convolutional value-iteration across the full input map (predictron claimed to scale better beyond 2D). Compared to methods that learn observation predictors (video/pixel predictors), the predictron focuses on value-prediction objectives and therefore achieves better task-relevant performance despite not producing pixel-accurate simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Within this paper the best-performing design choices are: include the (r, γ) MRP structure, use a learned λ-accumulator, and apply usage-weighted losses when updating joint parameters. Adaptive, per-prediction λ-weights (heterogeneous depth) are advantageous over a single shared scalar λ. Shared-core (weight sharing) configurations were used successfully; skip connections were explored but the (r, γ, λ) combination with usage weighting yielded the most consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1401.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1401.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Iteration Networks (VINs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable neural network that embeds a differentiable approximation of the value-iteration algorithm (convolutional planning module) into an end-to-end architecture to learn policies that perform planning over spatial state representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value iteration networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Iteration Network (VIN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A differentiable planner implemented as convolutional layers that iteratively compute value-iteration-like updates over an explicit spatial representation of the state space; planning is effected by repeated convolution / max-pooling operations over the full input map.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>differentiable planning network / neural value-iteration (explicit spatial planner)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Typically gridworld and spatial planning tasks where a 2D state-map encoding is natural.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: internal convolutions correspond to value-iteration steps and the spatial nature of the computation is explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Architectural correspondence to value iteration (convolutional filters and iterative updates) provides built-in interpretability of planning computation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Plans over the entire input state-space at each iteration (convolutional sweeps), which can be computationally heavy for large/high-dimensional input maps; cost scales with spatial resolution and number of iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper argues VINs may be less scalable than predictron in domains lacking a natural 2D state encoding because VINs convolve over full input maps, while predictron imagines trajectories in a compact abstract space.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>VINs are useful when a spatial grid encoding supports convolutional value iteration; however, their utility may decline in high-dimensional or non-grid domains where full-map convolution is inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>VINs trade explicit spatial planning interpretability for computational cost as spatial resolution grows; they require structured spatial inputs to be most effective.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Constrain the planner to operate as convolutional value-iteration over the full state-map; the number of iterations and convolutional kernel design determine planning horizon and expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared in the paper as complementary: predictron plans via abstract imagined trajectories and may scale better beyond 2D, whereas VINs exploit spatial structure via convolutional planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1401.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1401.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent env. simulators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Environment Simulators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent neural models that predict future observations (often pixel frames) by unrolling a learned transition model conditioned on past frames and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent environment simulators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent environment simulator (Oh et al. / Chiappa et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural network architectures trained to predict future observations (pixels) or latent observations conditioned on actions; often optimized for low reconstruction/prediction error in pixel space or latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural sequence predictor / observation-level simulator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Video prediction, model-based RL in visual domains (e.g., Atari, other pixel-based environments).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel-level reconstruction/prediction error (e.g., MSE on predicted frames) or perceptual similarity in image space.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper (referenced as prior work that can unroll near pixel-perfect reconstructions but have not surpassed model-free methods in challenging RL domains).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Typically black-box (deep recurrent nets) with limited intuitive interpretability of latent dynamics; visualization of predicted frames provides inspection at the observation level.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of predicted frames; sometimes analysis of latent states, but not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High, because modeling/predicting pixels over time is expensive both in computation and model capacity; exact costs not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper notes such models can produce near pixel-perfect rollouts but have not yet outperformed model-free methods on challenging RL tasks, suggesting high reconstruction fidelity does not guarantee better task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified in this paper; cited to have not surpassed state-of-the-art model-free methods in some challenging RL domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity observation prediction may not translate to superior decision-making; predictron argues for learning abstract models aimed directly at value prediction instead.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High observation-fidelity requires substantial compute and may allocate capacity to irrelevant details for decision-making; this can reduce task utility despite accurate reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pixel/observation-level prediction objective, recurrent architectures, action-conditioning; focus on reconstructing observations rather than directly optimizing downstream value or policy objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with predictron's approach: observation-level simulators prioritize reconstruction, while predictron prioritizes value-prediction through an abstract MRP.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1401.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1401.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action-conditional video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-conditional video prediction using deep networks in Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural models trained to predict future video frames conditioned on actions taken (used as environment simulators for planning and imagination).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Action-conditional video prediction using deep networks in atari games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action-conditional video predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deep neural network that predicts future frames conditioned on the current frame history and future actions, typically trained to minimize pixel-wise prediction error.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>observation-level neural simulator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and other video-based RL benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel prediction error (MSE) and qualitative fidelity of generated frames.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Observation-level outputs make qualitative assessment of predictions straightforward, but internal latent dynamics typically remain opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of predicted frames; not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Relatively high due to pixel-level prediction; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cited as achieving near pixel-perfect reconstructions in prior work, yet not sufficient to outperform best model-free RL methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper uses this reference to motivate the point that pixel-accurate models do not necessarily yield better downstream RL performance, motivating task-focused abstract models like the predictron.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity observation prediction vs downstream decision quality — the paper argues capacity may be misallocated if objective is reconstruction not task-relevant quantities.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Condition on actions; minimize pixel-wise loss; often use convolutional/deconvolutional architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Different emphasis relative to predictron: reconstructing observations vs predicting task-relevant returns through an abstract latent model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1401.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1401.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Computation Time (ACT) for recurrent neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism that enables recurrent networks to decide (differentiably) how many internal computation steps to take (when to halt) for each input, by learning per-input halting probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaptive computation time for recurrent neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive Computation Time (ACT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A halting mechanism that augments recurrent networks with a learned, differentiable procedure to decide the number of internal computational (pondering) steps to execute for each input, aggregating outputs across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>compute-adaptive mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General recurrent-network tasks where variable computation per input is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides an interpretable scalar (compute budget/pondering steps) per input, indicating relative compute allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of learned halting probabilities and per-input number of pondering steps.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Enables input-adaptive computation: can reduce average compute by halting early on easy inputs but requires learning extra halting parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Related to predictron's λ-weights: both provide adaptive per-input depth; ACT uses a discrete halting mechanism while predictron uses learned continuous λ gating to combine multi-depth outputs per prediction and can vary depth per prediction type.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>ACT offers adaptive compute control which can be beneficial for efficiency; predictron's λ mechanism is similar in spirit but differs in implementation and in allowing different depths across multiple different predictions produced by the same forward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Adaptive computation can save compute on easy instances at the expense of additional halting logic and training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Differentiable halting with accumulated halting probabilities; aggregation of intermediate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Predictron's λ-accumulator provides per-prediction adaptive depth and a different aggregation mechanism compared to ACT's halting scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1401.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1401.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schmidhuber-models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Schmidhuber's abstract world models and 'learning to think'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposals to learn compact, abstract recurrent world models (often via predictive coding) that encode histories and can be used by controllers, typically trained with separate losses for model and controller.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Schmidhuber-style abstract recurrent world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recurrent predictive model trained (often unsupervised) to compactly encode history via predictive coding or auto-regressive objectives; a controller is trained separately to use the model for planning or policy generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent recurrent world model (separate model/controller training)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General sequential decision-making and planning tasks where compact encoding of history can inform controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Typically black-box; emphasis is on compressive predictive representations rather than human-interpretable semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specified in this paper; original proposals focus on representational compression via predictive coding.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper contrasts Schmidhuber's suggestion of separate model/controller losses against predictron's end-to-end training targeted at value prediction; no empirical cost comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Separate training of model and controller may yield compact encodings but may not focus model capacity on task-relevant value predictions; predictron argues end-to-end objective on values is advantageous for planning utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Separate unsupervised model training can produce compact representations but risks learning features irrelevant to downstream decision objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Train model unsupervised/predictively to compress history; train controller separately for task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Predictron trains the internal model end-to-end with the value/prediction objective instead of relying on separate unsupervised learning for the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1401.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1401.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSR/GVF/Nexting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictive State Representations (PSRs), General Value Functions (GVFs), and Nexting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frameworks that emphasize learning many predictions (about future events or returns) as knowledge representations useful for downstream tasks and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Predictive representations / General Value Functions (GVFs) / Nexting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Representations consisting of multiple prediction targets (GVFs) that estimate cumulative discounted sums of pseudo-rewards for various events and discount factors; used as auxiliary tasks or as a predictive basis for control and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>predictive representation / multi-headed value predictors</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General reinforcement learning settings; used as representation learning and transfer for many sensorimotor or control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>MSE / RMSE on predicted returns (value prediction), and quality as measured by downstream task improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In this paper GVFs are used as targets (280 GVFs in pool domain); predictron predicts them with lower RMSE than baselines, but no single summary number for all GVFs is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability of individual predictions (each GVF corresponds to a semantically-described pseudo-reward/event and discount factor), enabling inspection of particular predicted quantities.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Designing semantically meaningful pseudo-rewards (e.g., 'ball entering pocket') and plotting prediction traces/performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Predicting many GVFs increases output dimensionality and compute for heads, but computational cost depends on model architecture; the predictron predicted 280 GVFs concurrently in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Using many GVFs can improve representation and transfer; predictron handled many GVFs effectively via shared abstract dynamics trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>The predictron produced lower RMSE across large sets of GVFs (pool domain) and used those predictions to drive better decision-making (higher number of pocketing events).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Learning many semantically meaningful predictions helps create a representation useful across tasks; predictron benefits from this by jointly predicting many GVFs and using them for decision selection.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Predicting many GVFs increases supervision targets and possibly training complexity, but yields richer task-relevant signals for representation learning and improved downstream utility when handled by a suitably structured model.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Define many pseudo-rewards and discount factors to span events of interest; normalize returns; jointly predict them via a shared model with multiple output heads.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>GVFs provide a principled set of prediction tasks; predictron uses GVFs as supervision and outperforms comparable architectures at predicting them.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1401.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1401.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dyna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dyna (integrated learning, planning, and reacting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic model-based RL framework that interleaves learning a model of the environment with planning (updating value estimates using imagined trajectories) and online acting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Integrated architectures for learning, planning and reacting based on dynamic programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dyna-style model-based planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learn an explicit model of environment transitions and rewards and use it to generate imagined trajectories (samples) that are fed to value function or policy updates (planning), interleaved with real experience.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit model-based RL framework</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General reinforcement learning across control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Depends on model choice (explicit models may be interpretable; learned neural models less so).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cost depends on model complexity and amount of imagination/planning performed; not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Conceptually Dyna uses imaginary rollouts to improve sample efficiency; predictron implements an internal, learned MRP and uses imagined rollouts (and consistency updates) in a related manner but trains the internal model end-to-end for value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper frames consistency updates in predictron as analogous to Dyna-style planning: using imagined trajectories to refine value estimates, including on unlabelled inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Using imagined data can improve value estimates but requires a model whose imagined trajectories are useful for the value objective; predictron addresses this by making the internal model directly optimize value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Interleave real and imagined updates; choose how to weight imagined trajectories in updates (predictron uses usage-weighting and consistency losses).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Predictron: End-To-End Learning and Planning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent environment simulators <em>(Rating: 2)</em></li>
                <li>Action-conditional video prediction using deep networks in atari games <em>(Rating: 2)</em></li>
                <li>Value iteration networks <em>(Rating: 2)</em></li>
                <li>Adaptive computation time for recurrent neural networks <em>(Rating: 1)</em></li>
                <li>On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models <em>(Rating: 1)</em></li>
                <li>Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction <em>(Rating: 1)</em></li>
                <li>Integrated architectures for learning, planning and reacting based on dynamic programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1401",
    "paper_id": "paper-39b19ea254b0952f2abd23ad899420749816bb1d",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Predictron",
            "name_full": "The Predictron (abstract internal MRP model)",
            "brief_description": "An end-to-end differentiable architecture that learns an abstract internal Markov Reward Process (MRP) as a latent world model; it unrolls this internal model multiple imagined planning steps, accumulates internal rewards/discounts/values, and combines multi-depth predictions via a learned λ-accumulator to predict external value functions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Predictron",
            "model_description": "A latent/abstract world model implemented as a repeatable core that maps an internal state s -&gt; (s', r, γ, λ) (internal next-state, internal reward vector, internal discount, and λ-gating). The core is a convolutional neural module (shared across up to K unrolled model steps) whose outputs are combined by an accumulator to form k-step preturns and a λ-weighted mixture (λ-preturn). The predictron is trained end-to-end to make its accumulated internal returns match Monte-Carlo external returns (value functions) rather than to reconstruct observations.",
            "model_type": "latent world model / abstract internal MRP",
            "task_domain": "Value prediction in procedurally-generated random mazes and a Mujoco pool simulator (pixel inputs); evaluated on many general value functions (GVFs).",
            "fidelity_metric": "Mean-squared error / Root-mean-squared error on predicted value functions vs Monte-Carlo returns (normalized returns); training loss is MSE on returns; additional evaluation with task-based metrics (e.g., pocketing events).",
            "fidelity_performance": "Qualitatively reported as significantly lower RMSE than feedforward and recurrent baselines; near-zero training error on a deterministic-policy trajectory prediction maze task; no single scalar RMSE reported in paper figures, but aggregated RMSE plots show consistent improvement. Additionally, in a decision test on pool the predictron selected initial conditions that pocketed 27 coloured balls in 50 episodes versus 10/50 for an equally deep convolutional baseline.",
            "interpretability_assessment": "Partially interpretable: the internal model is explicitly 'abstract' and not required to correspond to real-world semantics (hence often a black-box), but emergent structure is interpretable — e.g., weighted k-step preturns can be visualized to reveal sequential planning steps building a trajectory, and per-prediction adaptive depth (number of internal steps used) can be inspected and plotted.",
            "interpretability_method": "Visualization of weighted preturns (showing how the predicted trajectory decomposes across internal steps), analysis of λ-weights and derived 'thinking depth' distributions per prediction type, and inspection of internal rewards/discounts/values; consistency updates encourage internal self-consistency (used as introspection signal).",
            "computational_cost": "Architecture used up to K=16 model steps, resulting in networks up to 52 layers deep (2 encoder conv layers + 3*16 conv core layers + 2 fully connected layers). Parameter counts reported in capacity comparisons: example totals include ≈1.28M and ≈4.85M parameters for certain configurations. Training uses minibatches of 100, Adam optimizer (lr=0.001). No GPU-hours/training-time figures are provided.",
            "efficiency_comparison": "Empirically more sample/parameter efficient for value prediction than conventional feedforward or recurrent convolutional networks of comparable or larger parameter counts; structure (predictron) matters more than raw parameter count. Claimed to potentially scale better than Value Iteration Networks in domains without natural 2D encodings because it plans via imagined trajectories in a compact abstract state space instead of convolving over entire spatial maps.",
            "task_performance": "On random maze tasks the predictron achieved near-perfect predictions for a deterministic-policy trajectory prediction task and lower aggregated RMSE than baselines for connectivity predictions; on the pool task it yielded lower prediction RMSE across 280 GVFs and, when used to select initial shot parameters, resulted in 27 pocketing events in 50 episodes versus 10 for an equally deep convolutional baseline.",
            "task_utility_analysis": "Training the internal model end-to-end to predict value functions (rather than to reconstruct pixels) focuses model capacity on task-relevant predictions; higher fidelity in predicted values translated into improved decision-making in the pool domain (more pocketing events). Thus, task-relevant abstraction (rather than full observation fidelity) produced strong utility for downstream decisions.",
            "tradeoffs_observed": "Designing the model as a fully abstract MRP trades human interpretability for task-relevant compactness — the model need not produce pixel-perfect rollouts. The λ-accumulator enables adaptive compute but adds gating parameters to learn. Using internal rewards/discounts and λ weighting improved fidelity and utility but increases architectural complexity. The predictron attains better prediction accuracy without explicit observation reconstruction, implying a trade-off: prioritize value-prediction fidelity over generative fidelity of observations.",
            "design_choices": "Key choices: (1) learn internal rewards r and discounts γ (MRP structure) rather than only hidden-to-hidden transforms; (2) use a k-step and/or λ-accumulator to combine multi-depth preturns; (3) learn λ weights (per state, per prediction) to enable adaptive depth; (4) usage-weighted loss vs uniform averaging of preturn losses; (5) shared vs unshared core weights (recurrent vs feedforward unrolled); (6) optional skip connections (Δs updates) and up to 16 internal planning steps; (7) end-to-end supervised regression to Monte-Carlo returns and optional consistency updates on unlabelled data.",
            "comparison_to_alternatives": "Direct empirical comparisons in the paper show the predictron outperforms conventional feedforward and recurrent convolutional networks (with or without skip connections) across multiple depths and parameter budgets. Compared to VINs, the predictron plans via imagined trajectories in an abstract state space rather than performing convolutional value-iteration across the full input map (predictron claimed to scale better beyond 2D). Compared to methods that learn observation predictors (video/pixel predictors), the predictron focuses on value-prediction objectives and therefore achieves better task-relevant performance despite not producing pixel-accurate simulations.",
            "optimal_configuration": "Within this paper the best-performing design choices are: include the (r, γ) MRP structure, use a learned λ-accumulator, and apply usage-weighted losses when updating joint parameters. Adaptive, per-prediction λ-weights (heterogeneous depth) are advantageous over a single shared scalar λ. Shared-core (weight sharing) configurations were used successfully; skip connections were explored but the (r, γ, λ) combination with usage weighting yielded the most consistent gains.",
            "uuid": "e1401.0",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "VIN",
            "name_full": "Value Iteration Networks (VINs)",
            "brief_description": "A differentiable neural network that embeds a differentiable approximation of the value-iteration algorithm (convolutional planning module) into an end-to-end architecture to learn policies that perform planning over spatial state representations.",
            "citation_title": "Value iteration networks",
            "mention_or_use": "mention",
            "model_name": "Value Iteration Network (VIN)",
            "model_description": "A differentiable planner implemented as convolutional layers that iteratively compute value-iteration-like updates over an explicit spatial representation of the state space; planning is effected by repeated convolution / max-pooling operations over the full input map.",
            "model_type": "differentiable planning network / neural value-iteration (explicit spatial planner)",
            "task_domain": "Typically gridworld and spatial planning tasks where a 2D state-map encoding is natural.",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Moderately interpretable: internal convolutions correspond to value-iteration steps and the spatial nature of the computation is explicit.",
            "interpretability_method": "Architectural correspondence to value iteration (convolutional filters and iterative updates) provides built-in interpretability of planning computation.",
            "computational_cost": "Plans over the entire input state-space at each iteration (convolutional sweeps), which can be computationally heavy for large/high-dimensional input maps; cost scales with spatial resolution and number of iterations.",
            "efficiency_comparison": "Paper argues VINs may be less scalable than predictron in domains lacking a natural 2D state encoding because VINs convolve over full input maps, while predictron imagines trajectories in a compact abstract space.",
            "task_performance": null,
            "task_utility_analysis": "VINs are useful when a spatial grid encoding supports convolutional value iteration; however, their utility may decline in high-dimensional or non-grid domains where full-map convolution is inefficient.",
            "tradeoffs_observed": "VINs trade explicit spatial planning interpretability for computational cost as spatial resolution grows; they require structured spatial inputs to be most effective.",
            "design_choices": "Constrain the planner to operate as convolutional value-iteration over the full state-map; the number of iterations and convolutional kernel design determine planning horizon and expressivity.",
            "comparison_to_alternatives": "Compared in the paper as complementary: predictron plans via abstract imagined trajectories and may scale better beyond 2D, whereas VINs exploit spatial structure via convolutional planning.",
            "optimal_configuration": null,
            "uuid": "e1401.1",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Recurrent env. simulators",
            "name_full": "Recurrent Environment Simulators",
            "brief_description": "Recurrent neural models that predict future observations (often pixel frames) by unrolling a learned transition model conditioned on past frames and actions.",
            "citation_title": "Recurrent environment simulators",
            "mention_or_use": "mention",
            "model_name": "Recurrent environment simulator (Oh et al. / Chiappa et al. style)",
            "model_description": "Recurrent neural network architectures trained to predict future observations (pixels) or latent observations conditioned on actions; often optimized for low reconstruction/prediction error in pixel space or latent space.",
            "model_type": "neural sequence predictor / observation-level simulator",
            "task_domain": "Video prediction, model-based RL in visual domains (e.g., Atari, other pixel-based environments).",
            "fidelity_metric": "Pixel-level reconstruction/prediction error (e.g., MSE on predicted frames) or perceptual similarity in image space.",
            "fidelity_performance": "Not reported in this paper (referenced as prior work that can unroll near pixel-perfect reconstructions but have not surpassed model-free methods in challenging RL domains).",
            "interpretability_assessment": "Typically black-box (deep recurrent nets) with limited intuitive interpretability of latent dynamics; visualization of predicted frames provides inspection at the observation level.",
            "interpretability_method": "Visualization of predicted frames; sometimes analysis of latent states, but not discussed in this paper.",
            "computational_cost": "High, because modeling/predicting pixels over time is expensive both in computation and model capacity; exact costs not specified here.",
            "efficiency_comparison": "Paper notes such models can produce near pixel-perfect rollouts but have not yet outperformed model-free methods on challenging RL tasks, suggesting high reconstruction fidelity does not guarantee better task performance.",
            "task_performance": "Not quantified in this paper; cited to have not surpassed state-of-the-art model-free methods in some challenging RL domains.",
            "task_utility_analysis": "High-fidelity observation prediction may not translate to superior decision-making; predictron argues for learning abstract models aimed directly at value prediction instead.",
            "tradeoffs_observed": "High observation-fidelity requires substantial compute and may allocate capacity to irrelevant details for decision-making; this can reduce task utility despite accurate reconstructions.",
            "design_choices": "Pixel/observation-level prediction objective, recurrent architectures, action-conditioning; focus on reconstructing observations rather than directly optimizing downstream value or policy objectives.",
            "comparison_to_alternatives": "Contrasted with predictron's approach: observation-level simulators prioritize reconstruction, while predictron prioritizes value-prediction through an abstract MRP.",
            "optimal_configuration": null,
            "uuid": "e1401.2",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Action-conditional video prediction",
            "name_full": "Action-conditional video prediction using deep networks in Atari games",
            "brief_description": "Neural models trained to predict future video frames conditioned on actions taken (used as environment simulators for planning and imagination).",
            "citation_title": "Action-conditional video prediction using deep networks in atari games",
            "mention_or_use": "mention",
            "model_name": "Action-conditional video predictor",
            "model_description": "A deep neural network that predicts future frames conditioned on the current frame history and future actions, typically trained to minimize pixel-wise prediction error.",
            "model_type": "observation-level neural simulator",
            "task_domain": "Atari games and other video-based RL benchmarks.",
            "fidelity_metric": "Pixel prediction error (MSE) and qualitative fidelity of generated frames.",
            "fidelity_performance": null,
            "interpretability_assessment": "Observation-level outputs make qualitative assessment of predictions straightforward, but internal latent dynamics typically remain opaque.",
            "interpretability_method": "Visual inspection of predicted frames; not elaborated in this paper.",
            "computational_cost": "Relatively high due to pixel-level prediction; specifics not provided here.",
            "efficiency_comparison": "Cited as achieving near pixel-perfect reconstructions in prior work, yet not sufficient to outperform best model-free RL methods.",
            "task_performance": null,
            "task_utility_analysis": "Paper uses this reference to motivate the point that pixel-accurate models do not necessarily yield better downstream RL performance, motivating task-focused abstract models like the predictron.",
            "tradeoffs_observed": "High-fidelity observation prediction vs downstream decision quality — the paper argues capacity may be misallocated if objective is reconstruction not task-relevant quantities.",
            "design_choices": "Condition on actions; minimize pixel-wise loss; often use convolutional/deconvolutional architectures.",
            "comparison_to_alternatives": "Different emphasis relative to predictron: reconstructing observations vs predicting task-relevant returns through an abstract latent model.",
            "optimal_configuration": null,
            "uuid": "e1401.3",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "ACT",
            "name_full": "Adaptive Computation Time (ACT) for recurrent neural networks",
            "brief_description": "A mechanism that enables recurrent networks to decide (differentiably) how many internal computation steps to take (when to halt) for each input, by learning per-input halting probabilities.",
            "citation_title": "Adaptive computation time for recurrent neural networks",
            "mention_or_use": "mention",
            "model_name": "Adaptive Computation Time (ACT)",
            "model_description": "A halting mechanism that augments recurrent networks with a learned, differentiable procedure to decide the number of internal computational (pondering) steps to execute for each input, aggregating outputs across steps.",
            "model_type": "compute-adaptive mechanism",
            "task_domain": "General recurrent-network tasks where variable computation per input is beneficial.",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Provides an interpretable scalar (compute budget/pondering steps) per input, indicating relative compute allocation.",
            "interpretability_method": "Inspection of learned halting probabilities and per-input number of pondering steps.",
            "computational_cost": "Enables input-adaptive computation: can reduce average compute by halting early on easy inputs but requires learning extra halting parameters.",
            "efficiency_comparison": "Related to predictron's λ-weights: both provide adaptive per-input depth; ACT uses a discrete halting mechanism while predictron uses learned continuous λ gating to combine multi-depth outputs per prediction and can vary depth per prediction type.",
            "task_performance": null,
            "task_utility_analysis": "ACT offers adaptive compute control which can be beneficial for efficiency; predictron's λ mechanism is similar in spirit but differs in implementation and in allowing different depths across multiple different predictions produced by the same forward pass.",
            "tradeoffs_observed": "Adaptive computation can save compute on easy instances at the expense of additional halting logic and training complexity.",
            "design_choices": "Differentiable halting with accumulated halting probabilities; aggregation of intermediate outputs.",
            "comparison_to_alternatives": "Predictron's λ-accumulator provides per-prediction adaptive depth and a different aggregation mechanism compared to ACT's halting scheme.",
            "optimal_configuration": null,
            "uuid": "e1401.4",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Schmidhuber-models",
            "name_full": "Schmidhuber's abstract world models and 'learning to think'",
            "brief_description": "Proposals to learn compact, abstract recurrent world models (often via predictive coding) that encode histories and can be used by controllers, typically trained with separate losses for model and controller.",
            "citation_title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models",
            "mention_or_use": "mention",
            "model_name": "Schmidhuber-style abstract recurrent world model",
            "model_description": "A recurrent predictive model trained (often unsupervised) to compactly encode history via predictive coding or auto-regressive objectives; a controller is trained separately to use the model for planning or policy generation.",
            "model_type": "latent recurrent world model (separate model/controller training)",
            "task_domain": "General sequential decision-making and planning tasks where compact encoding of history can inform controllers.",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Typically black-box; emphasis is on compressive predictive representations rather than human-interpretable semantics.",
            "interpretability_method": "Not specified in this paper; original proposals focus on representational compression via predictive coding.",
            "computational_cost": null,
            "efficiency_comparison": "Paper contrasts Schmidhuber's suggestion of separate model/controller losses against predictron's end-to-end training targeted at value prediction; no empirical cost comparisons provided.",
            "task_performance": null,
            "task_utility_analysis": "Separate training of model and controller may yield compact encodings but may not focus model capacity on task-relevant value predictions; predictron argues end-to-end objective on values is advantageous for planning utility.",
            "tradeoffs_observed": "Separate unsupervised model training can produce compact representations but risks learning features irrelevant to downstream decision objectives.",
            "design_choices": "Train model unsupervised/predictively to compress history; train controller separately for task.",
            "comparison_to_alternatives": "Predictron trains the internal model end-to-end with the value/prediction objective instead of relying on separate unsupervised learning for the world model.",
            "optimal_configuration": null,
            "uuid": "e1401.5",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "PSR/GVF/Nexting",
            "name_full": "Predictive State Representations (PSRs), General Value Functions (GVFs), and Nexting",
            "brief_description": "Frameworks that emphasize learning many predictions (about future events or returns) as knowledge representations useful for downstream tasks and transfer.",
            "citation_title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
            "mention_or_use": "mention",
            "model_name": "Predictive representations / General Value Functions (GVFs) / Nexting",
            "model_description": "Representations consisting of multiple prediction targets (GVFs) that estimate cumulative discounted sums of pseudo-rewards for various events and discount factors; used as auxiliary tasks or as a predictive basis for control and transfer.",
            "model_type": "predictive representation / multi-headed value predictors",
            "task_domain": "General reinforcement learning settings; used as representation learning and transfer for many sensorimotor or control tasks.",
            "fidelity_metric": "MSE / RMSE on predicted returns (value prediction), and quality as measured by downstream task improvements.",
            "fidelity_performance": "In this paper GVFs are used as targets (280 GVFs in pool domain); predictron predicts them with lower RMSE than baselines, but no single summary number for all GVFs is provided.",
            "interpretability_assessment": "High interpretability of individual predictions (each GVF corresponds to a semantically-described pseudo-reward/event and discount factor), enabling inspection of particular predicted quantities.",
            "interpretability_method": "Designing semantically meaningful pseudo-rewards (e.g., 'ball entering pocket') and plotting prediction traces/performance.",
            "computational_cost": "Predicting many GVFs increases output dimensionality and compute for heads, but computational cost depends on model architecture; the predictron predicted 280 GVFs concurrently in experiments.",
            "efficiency_comparison": "Using many GVFs can improve representation and transfer; predictron handled many GVFs effectively via shared abstract dynamics trained end-to-end.",
            "task_performance": "The predictron produced lower RMSE across large sets of GVFs (pool domain) and used those predictions to drive better decision-making (higher number of pocketing events).",
            "task_utility_analysis": "Learning many semantically meaningful predictions helps create a representation useful across tasks; predictron benefits from this by jointly predicting many GVFs and using them for decision selection.",
            "tradeoffs_observed": "Predicting many GVFs increases supervision targets and possibly training complexity, but yields richer task-relevant signals for representation learning and improved downstream utility when handled by a suitably structured model.",
            "design_choices": "Define many pseudo-rewards and discount factors to span events of interest; normalize returns; jointly predict them via a shared model with multiple output heads.",
            "comparison_to_alternatives": "GVFs provide a principled set of prediction tasks; predictron uses GVFs as supervision and outperforms comparable architectures at predicting them.",
            "optimal_configuration": null,
            "uuid": "e1401.6",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Dyna",
            "name_full": "Dyna (integrated learning, planning, and reacting)",
            "brief_description": "A classic model-based RL framework that interleaves learning a model of the environment with planning (updating value estimates using imagined trajectories) and online acting.",
            "citation_title": "Integrated architectures for learning, planning and reacting based on dynamic programming",
            "mention_or_use": "mention",
            "model_name": "Dyna-style model-based planning",
            "model_description": "Learn an explicit model of environment transitions and rewards and use it to generate imagined trajectories (samples) that are fed to value function or policy updates (planning), interleaved with real experience.",
            "model_type": "explicit model-based RL framework",
            "task_domain": "General reinforcement learning across control tasks.",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Depends on model choice (explicit models may be interpretable; learned neural models less so).",
            "interpretability_method": null,
            "computational_cost": "Cost depends on model complexity and amount of imagination/planning performed; not specified here.",
            "efficiency_comparison": "Conceptually Dyna uses imaginary rollouts to improve sample efficiency; predictron implements an internal, learned MRP and uses imagined rollouts (and consistency updates) in a related manner but trains the internal model end-to-end for value prediction.",
            "task_performance": null,
            "task_utility_analysis": "Paper frames consistency updates in predictron as analogous to Dyna-style planning: using imagined trajectories to refine value estimates, including on unlabelled inputs.",
            "tradeoffs_observed": "Using imagined data can improve value estimates but requires a model whose imagined trajectories are useful for the value objective; predictron addresses this by making the internal model directly optimize value prediction.",
            "design_choices": "Interleave real and imagined updates; choose how to weight imagined trajectories in updates (predictron uses usage-weighting and consistency losses).",
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1401.7",
            "source_info": {
                "paper_title": "The Predictron: End-To-End Learning and Planning",
                "publication_date_yy_mm": "2016-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent environment simulators",
            "rating": 2
        },
        {
            "paper_title": "Action-conditional video prediction using deep networks in atari games",
            "rating": 2
        },
        {
            "paper_title": "Value iteration networks",
            "rating": 2
        },
        {
            "paper_title": "Adaptive computation time for recurrent neural networks",
            "rating": 1
        },
        {
            "paper_title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models",
            "rating": 1
        },
        {
            "paper_title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
            "rating": 1
        },
        {
            "paper_title": "Integrated architectures for learning, planning and reacting based on dynamic programming",
            "rating": 1
        }
    ],
    "cost": 0.019067499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Predictron: End-To-End Learning and Planning</h1>
<p>David Silver<em>1 Hado van Hasselt</em>1 Matteo Hessel<em>1 Tom Schaul</em>1 Arthur Guez*1 Tim Harley ${ }^{1}$ Gabriel Dulac-Arnold ${ }^{1}$ David Reichert ${ }^{1}$ Neil Rabinowitz ${ }^{1}$ Andre Barreto ${ }^{1}$ Thomas Degris ${ }^{1}$</p>
<h4>Abstract</h4>
<p>One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-toend so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.</p>
<h2>1. Introduction</h2>
<p>The central idea of model-based reinforcement learning is to decompose the RL problem into two subproblems: learning a model of the environment, and then planning with this model. The model is typically represented by a Markov reward process (MRP) or decision process (MDP). The planning component uses this model to evaluate and select among possible strategies. This is typically achieved by rolling forward the model to construct a value function that estimates cumulative reward. In prior work, the model is trained essentially independently of its use within the planner. As a result, the model is not well-matched with the overall objective of the agent. Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art model-free methods in challenging RL domains with raw inputs (e.g., Mnih et al., 2015; 2016; Lillicrap et al., 2016).</p>
<p>In this paper we introduce a new architecture, which we call the predictron, that integrates learning and planning into one end-to-end training procedure. At every step, a model is applied to an internal state, to produce a next state, reward, discount, and value estimate. This model is completely abstract and its only goal is to facilitate accurate value prediction. For example, to plan effectively in a game, an agent must be able to predict the score. If our model makes accurate predictions, then an optimal plan with respect to our model will also be optimal for the underlying game - even if the model uses a different state space (e.g., abstract representations of enemy positions, ignoring their shapes and colours), action space (e.g., highlevel actions to move away from an enemy), rewards (e.g., a single abstract step could have a higher value than any real reward), or even time-step (e.g., a single abstract step could "jump" the agent to the end of a corridor). All we require is that trajectories through the abstract model produce scores that are consistent with trajectories through the real environment. This is achieved by training the predictron end-to-end, so as to make its value estimates as accurate as possible.</p>
<p>An ideal model could generalise to many different prediction tasks, rather than overfitting to a single task; and could learn from a rich variety of feedback signals, not just a single extrinsic reward. We therefore train the predictron to predict a host of different value functions for a variety of pseudo-reward functions and discount factors. These pseudo-rewards can encode any event or aspect of the environment that the agent may care about, e.g., staying alive or reaching the next room.</p>
<p>We focus upon the prediction task: estimating value functions in MRP environments with uncontrolled dynamics. In this case, the predictron can be implemented as a deep neural network with an MRP as a recurrent core. The predictron unrolls this core multiple steps and accumulates rewards into an overall estimate of value.</p>
<p>We applied the predictron to procedurally generated ran-</p>
<p>dom mazes, and a simulated pool domain, directly from pixel inputs. In both cases, the predictron significantly outperformed model-free algorithms with conventional deep network architectures; and was much more robust to architectural choices such as depth.</p>
<h2>2. Background</h2>
<p>We consider environments defined by an MRP with states $s \in \mathcal{S}$. The MRP is defined by a function, $s^{\prime}, r, \gamma=$ $p(s, \alpha)$, where $s^{\prime}$ is the next state, $r$ is the reward, and $\gamma$ is the discount factor, which can for instance represent the non-termination probability for this transition. The process may be stochastic, given IID noise $\alpha$.</p>
<p>The return of an MRP is the cumulative discounted reward over a single trajectory, $g_{t}=r_{t+1}+\gamma_{t+1} r_{t+2}+$ $\gamma_{t+1} \gamma_{t+2} r_{t+3}+\ldots$, where $\gamma_{t}$ can vary per time-step. We consider a generalisation of the MRP setting that includes vector-valued rewards $\mathbf{r}$, diagonal-matrix discounts $\boldsymbol{\gamma}$, and vector-valued returns $\mathbf{g}$; definitions are otherwise identical to the above. We use this bold font notation to closely match the more familiar scalar MRP case; the majority of the paper can be comfortably understood by reading all rewards as scalars, and all discount factors as scalar and constant, i.e., $\gamma_{t}=\gamma$.</p>
<p>The value function of an MRP $p$ is the expected return from state $s, v_{p}(s)=\mathbb{E}<em t="t">{p}\left[\mathbf{g}</em>$ which satisfies the following Bellman equation (Bellman, 1957),} \mid s_{t}=s\right]$. In the vector case, these are known as general value functions (Sutton et al., 2011). We will say that a (general) value function $v(\cdot)$ is consistent with environment $p$ if and only if $v=v_{p</p>
<p>$$
v_{p}(s)=\mathbb{E}<em p="p">{p}\left[\mathbf{r}+\gamma v</em>\right) \mid s\right]
$$}\left(s^{\prime</p>
<p>In model-based reinforcement learning (Sutton \&amp; Barto, 1998), an approximation $m \approx p$ to the environment is learned. In the uncontrolled setting this model is normally an MRP $s^{\prime}, \mathbf{r}, \gamma=m(s, \beta)$ that maps from state $s$ to subsequent state $s^{\prime}$ and additionally outputs rewards $\mathbf{r}$ and discounts $\gamma$; the model may be stochastic given an IID source of noise $\beta$. A (general) value function $v_{m}(\cdot)$ is consistent with model $m$ (or valid, (Sutton, 1995)), if and only if it satisfies a Bellman equation $v_{m}(s)=\mathbb{E}<em m="m">{m}\left[\mathbf{r}+\gamma v</em>\right) \mid s\right]$ with respect to model $m$. Conventionally, model-based RL methods focus on finding a value function $v$ that is consistent with a separately learned model $m$.}\left(s^{\prime</p>
<h2>3. Predictron architecture</h2>
<p>The predictron is composed of four main components. First, a state representation $\mathbf{s}=f(s)$ that encodes raw input $s$ (this could be a history of observations, in partially observed settings, for example when $f$ is a recurrent network) into an internal (abstract, hidden) state $\mathbf{s}$. Second, a model $\mathbf{s}^{\prime}, \mathbf{r}, \gamma=m(\mathbf{s}, \beta)$ that maps from internal state $\mathbf{s}$ to subsequent internal state $\mathbf{s}^{\prime}$, internal rewards $\mathbf{r}$, and internal discounts $\gamma$. Third, a value function $v$ that outputs internal values $\mathbf{v}=v(\mathbf{s})$ representing the remaining internal return from internal state $\mathbf{s}$ onwards. The predictron is applied by unrolling its model $m$ multiple "planning" steps to produce internal rewards, discounts and values. We use superscripts $\bullet^{k}$ to indicate internal steps of the model (which have no necessary connection to time steps $\bullet_{t}$ of the environment). Finally, these internal rewards, discounts and values are combined together by an accumulator into an overall estimate of value $\mathbf{g}$. The whole predictron, from input state $s$ to output, may be viewed as a value function approximator for external targets (i.e., the returns in the real environment). We consider both $k$-step and $\lambda$-weighted accumulators.</p>
<p>The $k$-step predictron rolls its internal model forward $k$ steps (Figure 1a). The 0 -step predictron return (henceforth abbreviated as preturn) is simply the first value $\mathbf{g}^{0}=\mathbf{v}^{0}$, the 1 -step preturn is $\mathbf{g}^{1}=\mathbf{r}^{1}+\gamma^{1} \mathbf{v}^{1}$. More generally, the $k$ step predictron return $\mathbf{g}^{k}$ is the internal return obtained by accumulating $k$ model steps, plus a discounted final value $\mathbf{v}^{k}$ from the $k$ th step:</p>
<p>$$
\mathbf{g}^{k}=\mathbf{r}^{1}+\gamma^{1}\left(\mathbf{r}^{2}+\gamma^{2}\left(\ldots+\gamma^{k-1}\left(\mathbf{r}^{k}+\gamma^{k} \mathbf{v}^{k}\right) \ldots\right)\right)
$$</p>
<p>The $\lambda$-predictron combines together many $k$-step preturns. Specifically, it computes a diagonal weight matrix $\boldsymbol{\lambda}^{k}$ from each internal state $s^{k}$. The accumulator uses weights $\boldsymbol{\lambda}^{0}, \ldots, \boldsymbol{\lambda}^{K}$ to aggregate over $k$-step preturns $\mathbf{g}^{0}, \ldots, \mathbf{g}^{K}$ and output a combined value that we call the $\lambda$-preturn $\mathbf{g}^{\lambda}$,</p>
<p>$$
\begin{gathered}
\mathbf{g}^{\lambda}=\sum_{k=0}^{K} \boldsymbol{w}^{k} \mathbf{g}^{k} \
\boldsymbol{w}^{k}= \begin{cases}\left(\mathbf{1}-\boldsymbol{\lambda}^{k}\right) \prod_{j=0}^{k-1} \boldsymbol{\lambda}^{j} &amp; \text { if } k&lt;K \
\prod_{j=0}^{K-1} \boldsymbol{\lambda}^{j} &amp; \text { otherwise. }\end{cases}
\end{gathered}
$$</p>
<p>where $\mathbf{1}$ is the identity matrix. This $\lambda$-preturn is analogous to the $\lambda$-return in the forward-view $\operatorname{TD}(\lambda)$ algorithm (Sutton, 1988; Sutton \&amp; Barto, 1998). It may also be computed by a backward accumulation through intermediate steps $\mathbf{g}^{k, \lambda}$,</p>
<p>$$
\mathbf{g}^{k, \lambda}=\left(\mathbf{1}-\boldsymbol{\lambda}^{k}\right) \mathbf{v}^{k}+\boldsymbol{\lambda}^{k}\left(\mathbf{r}^{k+1}+\boldsymbol{\gamma}^{k+1} \mathbf{g}^{k+1, \lambda}\right)
$$</p>
<p>where $\mathbf{g}^{K, \lambda}=\mathbf{v}^{K}$, and then using $\mathbf{g}^{\lambda}=\mathbf{g}^{0, \lambda}$. Computation in the $\lambda$-predictron operates in a sweep, iterating first through the model from $k=0 \ldots K$ and then back through the accumulator from $k=K \ldots 0$ in a single "forward" pass of the network (see Figure 1b). Each $\boldsymbol{\lambda}^{k}$ weight</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. a) The $k$-step predictron architecture. The first three columns illustrate 0,1 and 2 -step pathways through the predictron. The 0 -step preturn reduces to standard model-free value function approximation; other preturns "imagine" additional steps with an internal model. Each pathway outputs a $k$-step preturn $\mathbf{g}^{k}$ that accumulates discounted rewards along with a final value estimate. In practice all $k$-step preturns are computed in a single forward pass. b) The $\lambda$-predictron architecture. The $\lambda$-parameters gate between the different preturns. The output is a $\lambda$-preturn $\mathbf{g}^{\lambda}$ that is a mixture over the $k$-step preturns. For example, if $\boldsymbol{\lambda}^{0}=\mathbf{1}, \boldsymbol{\lambda}^{1}=\mathbf{1}, \boldsymbol{\lambda}^{2}=\mathbf{0}$ then we recover the 2 -step preturn, $\mathbf{g}^{\lambda}=\mathbf{g}^{2}$. Discount factors $\boldsymbol{\gamma}^{k}$ and $\lambda$-parameters $\boldsymbol{\lambda}^{k}$ are dependent on state $\mathbf{s}^{k}$; this dependence is not shown in the figure.
acts as a gate on the computation of the $\lambda$-preturn: a value of $\boldsymbol{\lambda}^{k}=\mathbf{0}$ will truncate the $\lambda$-preturn at layer $k$, while a value of $\boldsymbol{\lambda}^{k}=\mathbf{1}$ will utilise deeper layers based on additional steps of the model $m$; the final weight is always $\boldsymbol{\lambda}^{K}=\mathbf{0}$. The individual $\boldsymbol{\lambda}^{k}$ weights may depend on the corresponding abstract state $\mathbf{s}^{k}$ and can differ per prediction. This enables the predictron to compute to an adaptive depth (Graves, 2016) depending on the internal state and learning dynamics of the network.</p>
<h2>4. Predictron learning updates</h2>
<p>We first consider updates that optimise the joint parameters $\boldsymbol{\theta}$ of the state representation, model, and value function. We begin with the $k$-step predictron. We update the $k$-step preturn $\mathbf{g}^{k}$ towards a target outcome $\mathbf{g}$, e.g. the MonteCarlo return from the real environment, by minimising a mean-squared error loss,</p>
<p>$$
\begin{aligned}
&amp; L^{k}=\frac{1}{2}\left|\mathbb{E}<em m="m">{p}[\mathbf{g} \mid s]-\mathbb{E}</em> \
&amp; \frac{\partial l^{k}}{\partial \boldsymbol{\theta}}=\left(\mathbf{g}-\mathbf{g}^{k}\right) \frac{\partial \mathbf{g}^{k}}{\partial \boldsymbol{\theta}}
\end{aligned}
$$}\left[\mathbf{g}^{k} \mid s\right]\right|^{2</p>
<p>where $l^{k}=\frac{1}{2}\left|\mathbf{g}-\mathbf{g}^{k}\right|^{2}$ is the sample loss. We can use the gradient of the sample loss to update parameters, e.g., by stochastic gradient descent. For stochastic models, independent samples of $\mathbf{g}^{k}$ and $\frac{\partial \mathbf{g}^{k}}{\partial \boldsymbol{\theta}}$ are required for unbiased samples of the gradient of $L^{k}$.
The $\lambda$-predictron combines many $k$-step preturns. To update the joint parameters $\boldsymbol{\theta}$, we can uniformly average the losses on the individual preturns $\mathbf{g}^{k}$,</p>
<p>$$
\begin{aligned}
&amp; L^{0: K}=\frac{1}{2 K} \sum_{k=0}^{K}\left|\mathbb{E}<em m="m">{p}[\mathbf{g} \mid s]-\mathbb{E}</em> \
&amp; \frac{\partial l^{0: K}}{\partial \boldsymbol{\theta}}=\frac{1}{K} \sum_{k=0}^{K}\left(\mathbf{g}-\mathbf{g}^{k}\right) \frac{\partial \mathbf{g}^{k}}{\partial \boldsymbol{\theta}}
\end{aligned}
$$}\left[\mathbf{g}^{k} \mid s\right]\right|^{2</p>
<p>Alternatively, we could weight each loss by the usage $\boldsymbol{w}^{k}$ of the corresponding preturn, such that the gradient is $\sum_{k=0}^{K} \boldsymbol{w}^{k}\left(\mathbf{g}-\mathbf{g}^{k}\right) \frac{\partial \mathbf{g}^{k}}{\partial \boldsymbol{\theta}}$.
In the $\lambda$-predictron, the $\boldsymbol{\lambda}^{k}$ weights (that determine the relative weighting $\boldsymbol{w}^{k}$ of the $k$-step preturns) depend on additional parameters $\boldsymbol{\eta}$, which are updated so as to minimise a mean-squared error loss $L^{\lambda}$,</p>
<p>$$
\begin{aligned}
&amp; L^{\lambda}=\frac{1}{2}\left|\mathbb{E}<em m="m">{p}[\mathbf{g} \mid s]-\mathbb{E}</em> \
&amp; \frac{\partial l^{\lambda}}{\partial \boldsymbol{\eta}}=\left(\mathbf{g}-\mathbf{g}^{\lambda}\right) \frac{\partial \mathbf{g}^{\lambda}}{\partial \boldsymbol{\eta}}
\end{aligned}
$$}\left[\mathbf{g}^{\lambda} \mid s\right]\right|^{2</p>
<p>In summary, the joint parameters $\boldsymbol{\theta}$ of the state representation $f$, the model $m$, and the value function $v$ are updated to make each of the $k$-step preturns $\mathbf{g}^{k}$ more similar to the target $\mathbf{g}$, and the parameters $\boldsymbol{\eta}$ of the $\lambda$-accumulator are updated to learn the weights $\boldsymbol{w}^{k}$ so that the aggregate $\lambda$ preturn $\mathbf{g}^{\lambda}$ becomes more similar to the target $\mathbf{g}$.</p>
<h3>4.1 Consistency updates</h3>
<p>In model-based reinforcement learning architectures such as Dyna (Sutton, 1990), value functions may be updated using both real and imagined trajectories. The refinement of value estimates based on these imagined trajectories is often referred to as planning. A similar opportunity arises in the context of the predictron. Each rollout of the predictron generates a trajectory in abstract space, alongside with rewards, discounts and values. Furthermore, the predictron aggregates these components in multiple value estimates $\left(\mathbf{g}^{0}, \ldots, \mathbf{g}^{k}, \mathbf{g}^{\lambda}\right)$.
We may therefore update each individual value estimate towards the best aggregated estimate. This corresponds to adjusting each preturn $\mathbf{g}^{k}$ towards the $\lambda$-preturn $\mathbf{g}^{\lambda}$, by minimizing:</p>
<p>$$
\begin{aligned}
&amp; L=\frac{1}{2} \sum_{k=0}^{K}\left|\mathbb{E}<em m="m">{m}\left[\mathbf{g}^{\lambda} \mid s\right]-\mathbb{E}</em> \
&amp; \frac{\partial l}{\partial \boldsymbol{\theta}}=\sum_{k=0}^{K}\left(\mathbf{g}^{\lambda}-\mathbf{g}^{k}\right) \frac{\partial \mathbf{g}^{k}}{\partial \boldsymbol{\theta}}
\end{aligned}
$$}\left[\mathbf{g}^{k} \mid s\right]\right|^{2</p>
<p>Here $\mathbf{g}^{\lambda}$ is considered fixed; the parameters $\boldsymbol{\theta}$ are only updated to make $\mathbf{g}^{k}$ more similar to $\mathbf{g}^{\lambda}$, not vice versa.
These consistency updates do not require any labels $\mathbf{g}$ or samples from the environment. As a result, it can be applied to (potentially hypothetical) states that have no associated 'real' (e.g. Monte-Carlo) outcome: we update the value estimates to be self-consistent with each other. This is especially relevant in the semi-supervised setting, where these consistency updates allow us to exploit the unlabelled inputs.</p>
<h2>5. Experiments</h2>
<p>We conducted experiments in two domains. The first domain consists of randomly generated mazes. Each location either is empty or contains a wall. In these mazes, we considered two tasks. In the first task, the input was a $13 \times 13$ maze and a random initial position and the goal is to predict a trajectory generated by a simple fixed deterministic policy. The target $\mathbf{g}$ was a vector with an element for each cell of the maze which is either one, if that cell was reached by the policy, or zero. In the second random-maze task the goal was to predict for each of the cells on the diagonal of a $20 \times 20$ maze (top-left to bottom-right) whether it is connected to the bottom-right corner. Two locations in a maze are considered connected if they are both empty and we can reach one from the other by moving horizontally or vertically through adjacent empty cells. In both cases some predictions would seem to be easier if we could learn a simple algorithm, such as some form of search or flood fill; our hypothesis is that an internal model can learn to
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Top: Two sample mazes from the random-maze domain. Light blue cells are empty, darker blue cells contain a wall. One maze is connected from top-left to bottom-right, the other is not. Bottom: An example trajectory in the pool domain (before downsampling), selected by maximising the prediction by a predictron of pocketing balls.
emulate such algorithms, where naive approximation may struggle. A few example mazes are shown in Figure 2.
Our second domain is a simulation of the game of pool, using four balls and four pockets. The simulator is implemented in the physics engine Mujoco (Todorov et al., 2012). We generate sequences of RGB frames starting from a random arrangement of balls on the table. The goal is to simultaneously learn to predict future events for each of the four balls, given 5 RGB frames as input. These events include: collision with any other ball, collision with any boundary of the table, entering a quadrant ( $\times 4$, for each quadrant), being located in a quadrant ( $\times 4$, for each quadrant), and entering a pocket ( $\times 4$, for each pocket). Each of these $14 \times 4$ events provides a binary pseudo-reward that we combine with 5 different discount factors ${0,0.5,0.9,0.98,1}$ and predict their cumulative discounted sum over various time spans. This yields a total of 280 general value functions. An example trajectory is shown in Figure 2. In both domains, inputs are presented as minibatches of i.i.d. samples with their regression targets. Additional domain details are provided in the appendix.</p>
<h3>5.1. Learning sequential plans</h3>
<p>In the first experiment we trained a predictron to predict trajectories generated by a simple deterministic policy in $13 \times 13$ random mazes with random starting positions. Figure 3 shows the weighted pretums $\mathbf{w}^{k} \mathbf{g}^{k}$ and the resulting prediction $\mathbf{g}^{\boldsymbol{\lambda}}=\sum_{k} \mathbf{w}^{k} \mathbf{g}^{k}$ for six example inputs and targets. The predictions are almost perfect-the training error was very close to zero. The full prediction is composed from weighted pretums which decompose the trajectory piece by piece, starting at the start position in the first step $k=1$, and where often multiple policy steps are added per planning step. The predictron was not informed about the sequential build up of the targets-it never sees a policy</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Indication of planning. Sampled mazes (grey) and start positions (black) are shown superimposed on each other at the bottom. The corresponding target vector $\mathbf{g}$, arranged as a matrix for visual clarity, is shown at the top. The ensembled prediction $\sum_{k} \mathbf{w}^{k} \mathbf{g}^{k}=\mathbf{g}^{\mathbf{\lambda}}$ is shown just below the target-the prediction is near perfect. The weighted preturns $\mathbf{w}^{k} \mathbf{g}^{k}$ that make up the prediction are shown below $\mathbf{g}^{\mathbf{\lambda}}$. We can see that full predicted trajectory is built up in steps, starting at the start position and then planning through the trajectory in sequence.
walking through the maze, only the resulting trajectoriesand yet sequential plans emerged spontaneously. Notice also that the easier trajectory on the right was predicted in only two steps, while more thinking steps are used for more complex trajectories.</p>
<h3>5.2. Exploring the predictron architecture</h3>
<p>In the next set of experiments, we tackle the problem of predicting connectivity of multiple pairs of locations in a random maze, and the problem of learning many different value functions from our simulator of the game of pool. We use these more challenging domains to examine three binary dimensions that differentiate the predictron from standard deep networks. We compare eight predictron variants corresponding to the corners of the cube on the left in Figure 4.</p>
<p>The first dimension, labelled $r, \gamma$, corresponds to whether
or not we use the structure of an MRP model. In the MRP case internal rewards and discounts are both learned. In the non- $(r, \gamma)$ case, which corresponds to a vanilla hidden-tohidden neural network module, internal rewards and discounts are ignored by fixing their values to $\mathbf{r}^{k}=\mathbf{0}$ and $\gamma^{k}=1$.</p>
<p>The second dimension is whether a $K$-step accumulator or $\lambda$-accumulator is used to aggregate preturns. When a $\lambda$ accumulator is used, a $\lambda$-preturn is computed as described in Section 3. Otherwise, intermediate preturns are ignored by fixing $\lambda^{k}=1$ for $k&lt;K$. In this case, the overall output of the predictron is the maximum-depth preturn $\mathbf{g}^{K}$.</p>
<p>The third dimension, labelled usage weighting, defines the loss that is used to update the parameters $\boldsymbol{\theta}$. We consider two options: the preturn losses can either be weighted uniformly (see Equation 6), or the update for each preturn $\mathbf{g}^{k}$ can be weighted according to the weight $\boldsymbol{w}^{k}$ that determines how much it is used in the $\lambda$-predictron's overall output. We call the latter loss 'usage weighted'. Note that for architectures without a $\lambda$-accumulator, $\boldsymbol{w}^{k}=0$ for $k&lt;K$, and $\boldsymbol{w}^{K}=1$, thus usage weighting then implies backpropagating only the loss on the final preturn $\mathbf{g}^{K}$.</p>
<p>All variants utilise a convolutional core with 2 intermediate hidden layers; parameters were updated by supervised learning (see appendix for more details). Root mean squared prediction errors for each architecture, aggregated over all predictions, are shown in Figure 4. The top row corresponds to the random mazes and the bottom row to the pool domain. The main conclusion is that learning an MRP model improved performance greatly. The inclusion of $\lambda$ weights helped as well, especially on pool. Usage weighting further improved performance.</p>
<h3>5.3. Comparing to other architecture</h3>
<p>Our third set of experiments compares the predictron to feedforward and recurrent deep learning architectures, with and without skip connections. We compare the corners of a new cube, as depicted on the left in Figure 5, based on three different binary dimensions.</p>
<p>The first dimension of this second cube is whether we use a predictron, or a (non- $\lambda$, non- $(r, \gamma)$ ) deep network that does not have an internal model and does not output or learn from intermediate predictions. We use the most effective predictron from the previous section, i.e., the $(r, \gamma, \lambda)$ predictron with usage weighting.</p>
<p>The second dimension is whether all cores share weights (as in a recurrent network), or each core uses separate weights (as in a feedforward network). The non- $\lambda$, non$(r, \gamma)$ variants of the predictron then correspond to standard (convolutional) feedforward and (unrolled) recurrent neural networks respectively.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Exploring predictron variants. Aggregated prediction errors over all predictions (20 for mazes, 280 for pool) for the eight predictron variants corresponding to the cube on the left (as described in the main text), for both random mazes (top) and pool (bottom). Each line is the median of RMSE over five seeds; shaded regions encompass all seeds. The full $(r, \gamma, \lambda)$-prediction (red) consistently performed best.</p>
<p>The third dimension is whether we include skip connections. This is equivalent to defining the model step to output a change to the current state, $\Delta \mathbf{s}$, and then defining $\mathbf{s}^{k+1}=h\left(\mathbf{s}^{k}+\Delta \mathbf{s}^{k}\right)$, where $h$ is the non-linear functionin our case a $\operatorname{ReLU}, h(x)=\max (0, x)$. The deep network with skip connections is a variant of ResNet (He et al., 2015).</p>
<p>Root mean squared prediction errors for each architecture are shown in Figure 5. All $(r, \gamma, \lambda)$-predictrons (red lines) outperformed the corresponding feedforward or recurrent baselines (black lines) both in the random mazes and in pool. We also investigated the effect of changing the depth of the networks (see appendix); the predictron outperformed the corresponding feedforward or recurrent baselines for all depths, with and without skip connections.</p>
<h3>5.4. Semi-supervised learning by consistency</h3>
<p>We now consider how to use the predictron for semisupervised learning, training the model on a combination of labelled and unlabelled random mazes. Semi-supervised learning is important because a common bottleneck in applying machine learning in the real world is the difficulty of collecting labelled data, whereas often large quantities of unlabelled data exist.</p>
<p>We trained a full $(r, \gamma, \lambda)$-predictron by alternating standard supervised updates with consistency updates, obtained by stochastically minimizing the consistency loss (8), on additional unlabelled samples drawn from the same distribution. For each supervised update we apply either 0,1 , or 9 consistency updates. Figure 6 shows that the perfor-
mance improved monotonically with the number of consistency updates, measured as a function of the number of labelled samples consumed.</p>
<h3>5.5. Analysis of adaptive depth</h3>
<p>In principle, the predictron can adapt its depth to 'think more' about some predictions than others, perhaps depending on the complexity of the underlying target. We saw indications of this in Figure 3. We investigate this further by looking at qualitatively different prediction types in pool: ball collisions, rail collisions, pocketing balls, and entering or staying in quadrants. For each prediction type we consider several different time-spans (determined by the real-world discount factors associated with each pseudoreward). Figure 7 shows distributions of depth for each type of prediction. The 'depth' of a predictron is here defined as the effective number of model steps. If the predictron relies fully on the very first value (i.e., $\boldsymbol{\lambda}^{0}=0$ ), this counts as 0 steps. If, instead, it learns to place equal weight on all rewards and on the final value, this counts as 16 steps. Concretely, the depth $\boldsymbol{d}$ can be defined recursively as $\boldsymbol{d}=\boldsymbol{d}^{0}$ where $\boldsymbol{d}^{k}=\boldsymbol{\lambda}^{k}\left(1+\boldsymbol{\gamma}^{k} \boldsymbol{d}^{k+1}\right)$ and $\boldsymbol{d}^{K}=\mathbf{0}$. Note that even for the same input state, each prediction has a separate depth.</p>
<p>The depth distributions exhibit three properties. First, different types of predictions used different depths. Second, depth was correlated with the real-world discount for the first four prediction types. Third, the distributions are not strongly peaked, which implies that the depth can differ per input even for a single real-world discount and prediction type. In a control experiment (not shown) we used a</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Comparing predictron to baselines. Aggregated prediction errors on random mazes (top) and pool (bottom) over all predictions for the eight architectures corresponding to the cube on the left. Each line is the median of RMSE over five seeds; shaded regions encompass all seeds. The full $(r, \gamma, \lambda)$-predictron (red), consistently outperformed conventional deep network architectures (black), with and without skips and with and without weight sharing.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Semi-supervised learning. Prediction errors of the $(r, \gamma, \lambda)$-predictrons (shared core, no skips) using 0, 1, or 9 consistency updates for every update with labelled data, plotted as function of the number of labels consumed. Learning performance improves with more consistency updates.
scalar $\lambda$ shared among all predictions, which reduced performance in all scenarios, indicating that the heterogeneous depth is a valuable form of flexibility.</p>
<h3>5.6. Using predictions to make decisions</h3>
<p>We test the quality of the predictions in the pool domain to evaluate whether they are well-suited to making decisions. For each sampled pool position, we consider a set $I$ of different initial conditions (different angles and velocity of the white ball), and ask which is more likely to lead to pocketing coloured balls. For each initial condition $s \in I$, we apply the $(r, \gamma, \lambda)$-predictron (shared cores, 16 model steps, no skip connections) to obtain predictions $\mathbf{g}^{\lambda}$. We ensemble the predictions associated to pocketing any ball (except the white one) with discounts $\gamma=0.98$ and $\gamma=1$. We select the condition $s^{*}$ that maximises this sum.</p>
<p>We then roll forward the pool simulator from $s^{<em>}$ and log the number of pocketing events. Figure 2 shows a sam-
pled rollout, using the predictron to pick $s^{</em>}$. When providing the choice of 128 angles and two velocities for initial conditions $(|I|=256)$, this procedure resulted in pocketing 27 coloured balls in 50 episodes. Using the same procedure with an equally deep convolutional network only resulted in 10 pocketing events. These results suggest that the lower loss of the learned $(r, \gamma, \lambda)$-predictron translated into meaningful improvements when informing decisions. A video of the rollouts selected by the predictron is available at the following url: https://youtu.be/ BeaLdaN2C3Q.</p>
<h2>6. Related work</h2>
<p>Lee et al. (2015) introduced a neural network architecture where classifications branch off intermediate hidden layers. An important difference with respect to the $\lambda$-predictron is that the weights are hand-tuned as hyper-parameters, whereas in the predictron the $\lambda$ weights are learnt and, more</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Thinking depth. Distributions of thinking depth on pool for different types of predictions and for different real-world discounts.
importantly, conditional on the input. Another difference is that the loss on the auxiliary classifications is used to speed up learning, but the classifications themselves are not combined into an aggregate prediction; the output of the model itself is the deepest prediction.</p>
<p>Graves (2016) introduced an architecture with adaptive computation time (ACT), with a discrete (but differentiable) decision on when to halt, and aggregating the outputs at each pondering step. This is related to our $\boldsymbol{\lambda}$ weights, but obtains depth in a different way; one notable difference is that the $\lambda$-predictron can use different pondering depths for each of its predictions.</p>
<p>Value iteration networks (VINs) (Tamar et al., 2016) also learn value functions end-to-end using an internal model, similar to the (non- $\lambda$ ) predictron. However, VINs plan via convolutional operations over the full input state space; whereas the predictron plans via imagined trajectories through an abstract state space. This may allow the predictron architecture to scale much more effectively in domains that do not have a natural two-dimensional encoding of the state space.</p>
<p>The notion of learning about many predictions of the future relates to work on predictive state representations (PSRs; Littman et al., 2001), general value functions (GVFs; Sutton et al., 2011), and nexting (Modayil et al., 2012). Such predictions have been shown to be useful as representations (Schaul \&amp; Ring, 2013) and for transfer (Schaul et al., 2015). So far, however, none of these have been considered for learning abstract models.</p>
<p>Schmidhuber (2015) discusses learning abstract models, but maintains separate losses for the model and a controller, and suggests training the model unsupervised to compactly encode the entire history of observations, through predictive coding. The predictron's abstract model is instead trained end-to-end to obtain accurate values.</p>
<h2>7. Conclusion</h2>
<p>The predictron is a single differentiable architecture that rolls forward an internal model to estimate external values. This internal model may be given both the structure and the semantics of traditional reinforcement learning models. But, unlike most approaches to model-based reinforcement learning, the model is fully abstract: it need not correspond to the real environment in any human understandable fashion, so long as its rolled-forward "plans" accurately predict outcomes in the true environment.</p>
<p>The predictron may be viewed as a novel network architecture that incorporates several separable ideas. First, the predictron outputs a value by accumulating rewards over a series of internal planning steps. Second, each forward pass of the predictron outputs values at multiple planning depths. Third, these values may be combined together, also within a single forward pass, to output an overall ensemble value. Finally, the different values output by the predictron may be encouraged to be self-consistent with each other, to provide an additional signal during learning. Our experiments demonstrate that these differences result in more accurate predictions of value, in reinforcement learning environments, than more conventional network architectures.</p>
<p>We have focused on value prediction tasks in uncontrolled environments. However, these ideas may transfer to the control setting, for example by using the predictron as a Qnetwork (Mnih et al., 2015). Even more intriguing is the possibility of learning an internal MDP with abstract internal actions, rather than the MRP considered in this paper. We aim to explore these ideas in future work.</p>
<h2>References</h2>
<p>Bellman, Richard. Dynamic programming. Princeton University Press, 1957.</p>
<p>Chiappa, Silvia, Racaniere, Sebastien, Wierstra, Daan, and Mohamed, Shakir. Recurrent environment simulators. 2016.</p>
<p>Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectifier neural networks. In Aistats, volume 15, pp. 275, 2011.</p>
<p>Graves, Alex. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983, 2016. URL http: //arxiv.org/abs/1603.08983.</p>
<p>He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.</p>
<p>Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.</p>
<p>Kingma, Diederik P and Ba, Jimmy. A method for stochastic optimization. In International Conference on Learning Representation, 2015.</p>
<p>LeCun, Yann, Bottou, Léon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.</p>
<p>Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply-supervised nets. In AISTATS, volume 2, pp. 6, 2015.</p>
<p>Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In ICLR, 2016.</p>
<p>Littman, Michael L, Sutton, Richard S, and Singh, Satinder P. Predictive representations of state. In NIPS, volume 14, pp. $1555-1561,2001$.</p>
<p>Mnih, V, Badia, A Puigdomènech, Mirza, M, Graves, A, Lillicrap, T, Harley, T, Silver, D, and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.</p>
<p>Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Modayil, Joseph, White, Adam, and Sutton, Richard S. Multitimescale nexting in a reinforcement learning robot. In International Conference on Simulation of Adaptive Behavior, pp. 299-309. Springer, 2012.</p>
<p>Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L, and Singh, Satinder. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pp. 2863-2871, 2015.</p>
<p>Schaul, Tom and Ring, Mark B. Better Generalization with Forecasts. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), Beijing, China, 2013.</p>
<p>Schaul, Tom, Horgan, Daniel, Gregor, Karol, and Silver, David. Universal Value Function Approximators. In International Conference on Machine Learning (ICML), 2015.</p>
<p>Schmidhuber, Juergen. On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. arXiv preprint arXiv:1511.09249, 2015.</p>
<p>Sutton, R. S. Learning to predict by the methods of temporal differences. Machine Learning, 3:9-44, 1988.</p>
<p>Sutton, R. S. Integrated architectures for learning, planning and reacting based on dynamic programming. In Machine Learning: Proceedings of the Seventh International Workshop, 1990.</p>
<p>Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT press, Cambridge MA, 1998.</p>
<p>Sutton, Richard S. TD models: Modeling the world at a mixture of time scales. In Proceedings of the Twelfth International Conference on Machine Learning, pp. 531-539, 1995.</p>
<p>Sutton, Richard S, Modayil, Joseph, Delp, Michael, Degris, Thomas, Pilarski, Patrick M, White, Adam, and Precup, Doina. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 761-768. International Foundation for Autonomous Agents and Multiagent Systems, 2011.</p>
<p>Tamar, Aviv, Wu, Yi, Thomas, Garrett, Levine, Sergey, and Abbeel, Pieter. Value iteration networks. In Neural Information Processing Systems (NIPS), 2016.</p>
<p>Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033. IEEE, 2012.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. The predictron core used in our experiments.</p>
<h2>A. Architecture</h2>
<p>The state representation $f$ is a two-layer convolutional neural network (LeCun et al., 1998). There is a core $c$, again based on convolutions, that combines both MRP model and $\lambda$-network into a single repeatable module, such that $\mathbf{s}^{k+1}, \mathbf{r}^{k+1}, \boldsymbol{\gamma}^{k+1}, \boldsymbol{\lambda}^{k}=$ $c\left(\mathbf{s}^{k}\right)$. This core is deterministic, and is duplicated $K$ times in the predictron with shared weights. (The predictron with unshared weights has $K$ distinct cores.) Finally, the value network $v$ is a fully connected neural network that computes $\mathbf{v}^{k}=v\left(\mathbf{s}^{k}\right)$.</p>
<p>Concretely, the core (Figure 8) consists first of a convolutional layer that maps into an intermediate (hidden) layer. From this layer, another two convolutions compute the next abstract state of the predictron. Additionally, this same hidden layer is flattened and fed into three separate networks, with two fully connected layers each. The outputs of these three networks represent the internal rewards, discounts, and lambdas. A similar small network also hangs off the internal states, in addition to the core, and computes the values. All convolutions use $3 \times 3$ filters and a stride of one, and use padding to retain the size of the feature maps. All feature maps have 32 channels. The hidden layers within the MLPs have 32 hidden units.</p>
<p>In Figure 8 the convolutional layers are schematically drawn with three channels, flattening is represented by curly brakets, while the arrows represent the small multi-layer perceptrons which compute values, rewards, discounts and lambdas.</p>
<p>We allow up to 16 model steps in our experiments, resulting in 52-layer deep networks-two convolutional layers for the state representations, $3 \times 16=48$ convolutional layers for the core steps, and two fully-connected layers for the values on top of the final state. Between each two layers we apply batch normalization (Ioffe \&amp; Szegedy, 2015) followed by a ReLU non-linearity (Glorot et al., 2011). The value and reward networks end with a linear layer, whereas the discount and $\lambda$-networks additionally add a sigmoid non-linearity to ensure that these quantities are in $[0,1]$.</p>
<p>For the illustrative maze experiment in Section 5.1, a smaller network architecture is employed with 6 model steps and convolutional feature maps of 16 channels. Additionally, the subnetworks
to compute values, rewards, discounts, and lambdas are composed of a $1 \times 1$ convolution with a stride of 1 and 8 channels before a fully connected hidden layer of size 128. The rest of network architecture is as described above.</p>
<h2>B. Training</h2>
<p>All experiments used the supervised (Monte-Carlo) update described in Section 4 except for the semi-supervised experiment which used the consistency update described in Section 4.1. We update all parameters by applying the Adam optimiser (Kingma $\&amp;$ Ba, 2015) to stochastic gradients of the corresponding loss functions. Each return is normalised by dividing it by its standard deviation (as measured, prior to the experiment, on a set of 20,000 episodes). In all experiments, the learning rate was 0.001 , and the other parameters of the Adam optimiser were $\beta_{1}=0.9$, $\beta_{2}=0.999$, and $\epsilon=10^{-8}$. We used mini-batches of 100 samples.</p>
<h2>C. Comparing architectures of different depths</h2>
<p>We investigated the effect of changing the depth of the networks, with and without skip connections. Figure 9 in shows that skip connections (dashed lines) make the conventional architectures (black/grey lines) more robust to the depth (i.e., the black/grey dashed lines almost overlap, especially on pool), and that the predictron outperforms the corresponding feedforward or recurrent baselines for all depths, with and without skips.</p>
<h2>D. Capacity comparisons</h2>
<p>In this section, we present some additional experiments comparing the predictron to more conventional deep networks. The purposes of these experiments are 1) to show that the conclusions obtained above do not depend on the precise architecture used, and 2) to show that the structure of the network-whether we use a predictron or not-is more important than the raw number of parameters.</p>
<p>Specifically, we again consider the same 20 by 20 random mazes, and the pool task described in the main text. As described in Section A, for the results in the paper we used an encoder that preserved the size of the input plans, $20 \times 20$ for the mazes and $28 \times 28$ for pool. Each convolution had 32 channels and therefore the abstract states were $20 \times 20 \times 32$ for the mazes and $28 \times 28 \times 32$ for pool.</p>
<p>We now consider a different architecture, where we no longer pad the convolutions used in the encoder. For the mazes, we still use two layers of $3 \times 3$ stride-1 convolutions, which means the planes reduce in size to $16 \times 16$. This means that the abstract states are about one third smaller. For pool, we use three $5 \times 5$ stride-1 convolutions, which bring us from $28 \times 28$ down to $16 \times 16$ as well. So, the abstract states are now of equal size for both experiments. For pool, this is approximately a two-thirds reduction, which helps reduce the compute needed to run the model.</p>
<p>Most of the parameters in the predictron are in the fully connected layers. Previously, the first fully connected layer for each of the internal values, rewards, discounts, and $\lambda$-parameters would take a flattened abstract state, and then go into 32 hidden nodes. This means the number of parameters in this layer were $20 \times 20 \times 32 \times$ $32=409,600$ for the mazes and $28 \times 28 \times 32 \times 32=802,816$</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Comparing depths. Comparing the $(r, \gamma, \lambda)$-predictron (red) against more conventional deep networks (black) for various depths ( $2,4,8$, or 16 model steps, corresponding to $10,16,28$, or 52 total layers of depth). Lighter colours correspond to shallower networks. Dashed lines correspond to networks with skip connections.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Pool input frame. An example of a 28x28 RGB input frame in the pool domain.
for pool. The predictron with shared core would have four of these layers, one for each of the internal values, rewards, discounts, and $\lambda \mathrm{s}$, compared to one for the deep network which only has values. We change this in two ways. First, we add a $1 \times 1$ convolution with a stride of 1 and 8 channels before the first fully connected layer for each of these outputs. This reduces the number of channels, and therefore the number of parameters in the subsequent fullyconnected layer, by one fourth. Second, we tested three different numbers of hidden nodes: 32,128 , or 512 .</p>
<p>The deep network with 128 hidden nodes for its values has the exact same number of parameters as the $(r, \gamma, \lambda)$-predictron with 32 hidden nodes for each of its outputs. Before, the deep network had fewer parameters, because we kept this number fixed at 32 across experiments. This opens the question of whether the improved performance of the predictron was not just an artifact of having more parameters. We tested this hypothesis, and the results are
shown in Figure 11.
Figure 11 shows that in each setting-on the mazes and pool, and with or without shared cores-both. The predictrons always performed better than all the deep networks. This includes the 32 node predictron (darkest red) compared to the 512 node deep network (lightest blue), even though the latter has approximately 4 times as many parameters ( 1.27 M vs 4.85 M ). This means that the number of parameters mattered less than whether or not we use a predictron.</p>
<h2>E. Additional domain details</h2>
<p>We now provide some additional details of domains.</p>
<h2>E.1. Pool</h2>
<p>To generate sequences in the Pool domain, the initial locations of 4 balls of different colours are sampled at random. The white ball is the only one moving initially. Its velocity has a norm sampled uniformly between 7 and 14. The initial angle is sampled uniformly in the range $(0,2 \pi)$. From the initial condition, the Mujoco simulation is run forward until all balls have stopped moving; sequences that last more than 151 frames are rejected, and a new one is generated as replacement. Each frame is rendered by Mujoco as a 280x280 RGB image, and subsequently downsampled through bilinear interpolation to a 28 x 28 RGB input (see Figure 10 for an example). Since the 280 signals described in Section 6.1 as targets for the Pool experiments have very different levels of sparsity, resulting in values with very different scales, we have normalised the pseudo returns. The normalization procedure consisted in dividing all targets by their standard deviation, as empirically measured across an initial set of 20,000 sequences.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Comparing depths. Comparing the $(r, \gamma, \lambda)$-predictron (red) against more conventional deep networks (blue) for different numbers hidden nodes in the fully connected layers, and therefore different total numbers of parameters. The deep networks with 32, 128 , and 512 nodes respectively have $381,416,1,275,752$, and $4,853,096$ parameters in total. The predictrons with 32 and 128 nodes respectively have $1,275,752$, and $4,853,096$ parameters in total. Note that the number of parameters for the 32 and 128 node predictrons are exactly equal to the number of parameters for the 128 and 512 node deep networks.</p>
<h1>E.2. Random Mazes</h1>
<h2>E.2.1. First Task</h2>
<p>The mazes are generated by ensuring that around $15 \%$ of locations are walls. The policy takes as observation the wall configuration in four locations adjacent to its position and maps each of these configurations to an action. For each maze, the policy is stepped for 60 steps from a uniformly random start location. The target $\mathbf{g}$ indicates whether the trajectory has traversed each maze location.</p>
<h2>E.2.2. SECOND TASK</h2>
<p>To generate mazes we first determine, with a stochastic line search, a number of walls so that the top-left corner is connected to the bottom-right corner (both always forced to be empty) in approximately $50 \%$ of the mazes. We then shuffle the walls uniformly randomly. For 20 by 20 mazes this means $70 \%$ of locations are empty and $30 \%$ contain walls. More than a googol different such 20 -by- 20 mazes exist (as $\binom{208}{120}&gt;10^{100}$ ).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution ${ }^{1}$ DeepMind, London. Correspondence to: David Silver <a href="mailto:davidsilver@google.com">davidsilver@google.com</a>, Hado van Hasselt <a href="mailto:hado@google.com">hado@google.com</a>, Matteo Hessel <a href="mailto:mtthss@google.com">mtthss@google.com</a>, Tom Schaul <a href="mailto:schaul@google.com">schaul@google.com</a>, Arthur Guez <a href="mailto:aguez@google.com">aguez@google.com</a>.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
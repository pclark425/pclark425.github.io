<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5758 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5758</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5758</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-95d19f8ede34cd712a09ae3b86bed2b338bd9a48</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/95d19f8ede34cd712a09ae3b86bed2b338bd9a48" target="_blank">SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> SciAssess is introduced, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis, and aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization, comprehension, and analysis.</p>
                <p><strong>Paper Abstract:</strong> Recent breakthroughs in Large Language Models (LLMs) have revolutionized scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data. In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. It aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization (L1), Comprehension (L2), and Analysis \&Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including biology, chemistry, material, and medicine. To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement. We hope this evaluation supports the ongoing development of LLM applications in scientific literature analysis. SciAssess and its resources are available at \url{https://github.com/sci-assess/SciAssess}.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5758.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5758.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI4Science-GPT4-study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The impact of large language models on scientific discovery: a preliminary study using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited preliminary study that used GPT-4 to examine the impact of large language models on scientific discovery and literature analysis; mentioned in this paper as evidence that LLMs can enhance researchers' efficiency via summarization and knowledge extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The impact of large language models on scientific discovery: a preliminary study using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>OpenAI GPT-4 (referenced here as the model used in the cited preliminary study); multimodal-capable, large-scale transformer (details of size/architecture not given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Preliminary investigation of GPT-4's impact on scientific discovery and its use for literature analysis (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>multidisciplinary / scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Not detailed in SciAssess beyond being cited as a preliminary study using GPT-4; SciAssess references it as an example of practical deployments of LLMs for literature summarization and knowledge extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Mentioned by SciAssess as an instance where LLMs (GPT-4) have been used for automatic literature summarization and knowledge extraction to boost researcher productivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5758.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5758.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng2023-LLM-synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for scientific synthesis, inference and explanation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work (Zheng et al., 2023) concerning use of LLMs for scientific synthesis and explanation; referenced by SciAssess as part of prior demonstrations that LLMs can assist in literature-level synthesis and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for scientific synthesis, inference and explanation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>large language models (unspecified in SciAssess citation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Scientific synthesis, inference, and explanation across literature (as per cited title and SciAssess contextualization).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific literature / multidisciplinary</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Cited by SciAssess to support the claim that LLMs have potential to enhance efficiency in scientific literature analysis via synthesis and inference; no detailed methods/results reported in SciAssess.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5758.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5758.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciRIFF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciRIFF: A resource to enhance language model instruction-following over scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited resource (Wadden et al., 2024) that focuses on extracting and synthesizing information from research literature across disciplines, noted in SciAssess as prior work on literature-level extraction and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sciriff: A resource to enhance language model instruction-following over scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>not specified (resource aimed at improving LM instruction-following)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Extracting and synthesizing information from research literature to support LM instruction-following and downstream applications.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>multidisciplinary scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Presented in SciAssess as a related effort to extract and synthesize literature information; specific methodology details are not provided within SciAssess.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Mentioned as an existing work that performs extraction and synthesis from research literature; SciAssess positions it among prior scientific-literature-focused resources rather than presenting its internal results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5758.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5758.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciAssess-benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's benchmark and experimental study of 11 LLMs across 27 tasks in four scientific domains to evaluate memorization, comprehension, and analysis & reasoning abilities relevant to scientific literature analysis (incl. multimodal content).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Multiple LLMs (OpenAI-o1, GPT-4o, GPT-4, GPT-3.5, Gemini-1.5-Pro, Claude 3 Opus, Moonshot-v1, Doubao, Llama-3.1-70B, Mixtral-8x22B-Instruct-v0.1, Qwen-2.5-72B)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A mix of closed-source and open-source large language models evaluated via official APIs (closed-source) or local deployment (open-source). Models range from OpenAI flagship models (GPT-4 family, o1) to large open models (Llama 3, Qwen2.5) and mixture-of-experts models (Mixtral). Exact architectures/sizes are referenced by model names and in Appendix D of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Benchmark and analyze LLM capabilities for scientific literature analysis across three ability levels (Memorization L1, Comprehension L2, Analysis & Reasoning L3) and multiple modalities to reveal strengths/weaknesses relevant to literature-level synthesis and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biology, chemistry, materials science, medicine (four subdomains)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Curated 6,888 questions across 27 tasks derived from papers and public datasets; used chain-of-thought prompting, few-shot (3-shot) for some tasks, PDF parsing (PyPDF2 or built-in parsers), multimodal inputs (charts, tables, reactions, molecular structures), and evaluated models via API or local runtime; expert annotation and cross-validation for dataset quality control.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>not applicable (SciAssess does not itself distill qualitative laws/principles from corpora; rather it evaluates model capabilities that could enable such distillation — e.g., L3 tasks integrate information to support reasoning and potential synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Task-specific metrics including accuracy (multiple choice / true-false), recall (table extraction), F1-score (text extraction), and molecular similarity (Tanimoto similarity for SMILES outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Benchmarked 11 LLMs; OpenAI-o1 and GPT-4 family models frequently rank highest overall, especially in memorization and reasoning; multimodal and molecule-related tasks remain challenging (e.g., Tag to Molecule showed poor performance across models due to PDF parsing limitations). The study highlights strengths and specific failure modes but does not report distillation of generalizable scientific laws.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High: dataset creation and labeling by domain experts (two annotators per data point with cross-validation), human-designed prompts, and expert quality control; human evaluation used for dataset validation rather than for automated law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>SciAssess benchmark dataset: 6,888 questions across 27 tasks and five modalities (text, charts, chemical reactions, molecular structures, tables); tasks sampled from papers (DOIs provided but documents not distributed in repo) and some public datasets (e.g., MMLU-Pro, MaScQA, B5CDR).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SciAssess notes limitations relevant to literature-level distillation: scope limited to four domains (no physics/engineering), no additional fine-tuning/training data provided (so results reflect off-the-shelf model capabilities), multimodal PDF parsing failures (notably for molecular structures), and tasks may favor particular model types; the paper does not perform large-scale law/principle distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Illustrative failure: Tag-to-Molecule (SMILES extraction) performed poorly across LLMs mainly due to PDF parsing shortcomings; methodological note: chain-of-thought prompting used broadly (except for OpenAI-o1 where guideline discourages CoT); recommended needs: improved PDF parsing tailored to scientific modalities to enable downstream tasks such as molecule extraction or literature-level synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The impact of large language models on scientific discovery: a preliminary study using GPT-4 <em>(Rating: 2)</em></li>
                <li>Large language models for scientific synthesis, inference and explanation <em>(Rating: 2)</em></li>
                <li>Sciriff: A resource to enhance language model instruction-following over scientific literature <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5758",
    "paper_id": "paper-95d19f8ede34cd712a09ae3b86bed2b338bd9a48",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "AI4Science-GPT4-study",
            "name_full": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "brief_description": "Cited preliminary study that used GPT-4 to examine the impact of large language models on scientific discovery and literature analysis; mentioned in this paper as evidence that LLMs can enhance researchers' efficiency via summarization and knowledge extraction.",
            "citation_title": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "mention_or_use": "mention",
            "llm_model_name": "GPT-4",
            "llm_model_description": "OpenAI GPT-4 (referenced here as the model used in the cited preliminary study); multimodal-capable, large-scale transformer (details of size/architecture not given in this paper).",
            "task_goal": "Preliminary investigation of GPT-4's impact on scientific discovery and its use for literature analysis (as cited).",
            "domain": "multidisciplinary / scientific discovery",
            "methodology": "Not detailed in SciAssess beyond being cited as a preliminary study using GPT-4; SciAssess references it as an example of practical deployments of LLMs for literature summarization and knowledge extraction.",
            "type_of_qualitative_law": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "human_involvement": null,
            "dataset_or_corpus": null,
            "limitations_or_challenges": null,
            "notable_examples": "Mentioned by SciAssess as an instance where LLMs (GPT-4) have been used for automatic literature summarization and knowledge extraction to boost researcher productivity.",
            "uuid": "e5758.0",
            "source_info": {
                "paper_title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Zheng2023-LLM-synthesis",
            "name_full": "Large language models for scientific synthesis, inference and explanation",
            "brief_description": "A cited work (Zheng et al., 2023) concerning use of LLMs for scientific synthesis and explanation; referenced by SciAssess as part of prior demonstrations that LLMs can assist in literature-level synthesis and inference.",
            "citation_title": "Large language models for scientific synthesis, inference and explanation",
            "mention_or_use": "mention",
            "llm_model_name": "large language models (unspecified in SciAssess citation)",
            "llm_model_description": null,
            "task_goal": "Scientific synthesis, inference, and explanation across literature (as per cited title and SciAssess contextualization).",
            "domain": "scientific literature / multidisciplinary",
            "methodology": null,
            "type_of_qualitative_law": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "human_involvement": null,
            "dataset_or_corpus": null,
            "limitations_or_challenges": null,
            "notable_examples": "Cited by SciAssess to support the claim that LLMs have potential to enhance efficiency in scientific literature analysis via synthesis and inference; no detailed methods/results reported in SciAssess.",
            "uuid": "e5758.1",
            "source_info": {
                "paper_title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SciRIFF",
            "name_full": "SciRIFF: A resource to enhance language model instruction-following over scientific literature",
            "brief_description": "Cited resource (Wadden et al., 2024) that focuses on extracting and synthesizing information from research literature across disciplines, noted in SciAssess as prior work on literature-level extraction and synthesis.",
            "citation_title": "Sciriff: A resource to enhance language model instruction-following over scientific literature",
            "mention_or_use": "mention",
            "llm_model_name": "not specified (resource aimed at improving LM instruction-following)",
            "llm_model_description": null,
            "task_goal": "Extracting and synthesizing information from research literature to support LM instruction-following and downstream applications.",
            "domain": "multidisciplinary scientific literature",
            "methodology": "Presented in SciAssess as a related effort to extract and synthesize literature information; specific methodology details are not provided within SciAssess.",
            "type_of_qualitative_law": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "human_involvement": null,
            "dataset_or_corpus": null,
            "limitations_or_challenges": null,
            "notable_examples": "Mentioned as an existing work that performs extraction and synthesis from research literature; SciAssess positions it among prior scientific-literature-focused resources rather than presenting its internal results.",
            "uuid": "e5758.2",
            "source_info": {
                "paper_title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SciAssess-benchmark",
            "name_full": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
            "brief_description": "This paper's benchmark and experimental study of 11 LLMs across 27 tasks in four scientific domains to evaluate memorization, comprehension, and analysis & reasoning abilities relevant to scientific literature analysis (incl. multimodal content).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Multiple LLMs (OpenAI-o1, GPT-4o, GPT-4, GPT-3.5, Gemini-1.5-Pro, Claude 3 Opus, Moonshot-v1, Doubao, Llama-3.1-70B, Mixtral-8x22B-Instruct-v0.1, Qwen-2.5-72B)",
            "llm_model_description": "A mix of closed-source and open-source large language models evaluated via official APIs (closed-source) or local deployment (open-source). Models range from OpenAI flagship models (GPT-4 family, o1) to large open models (Llama 3, Qwen2.5) and mixture-of-experts models (Mixtral). Exact architectures/sizes are referenced by model names and in Appendix D of the paper.",
            "task_goal": "Benchmark and analyze LLM capabilities for scientific literature analysis across three ability levels (Memorization L1, Comprehension L2, Analysis & Reasoning L3) and multiple modalities to reveal strengths/weaknesses relevant to literature-level synthesis and reasoning.",
            "domain": "biology, chemistry, materials science, medicine (four subdomains)",
            "methodology": "Curated 6,888 questions across 27 tasks derived from papers and public datasets; used chain-of-thought prompting, few-shot (3-shot) for some tasks, PDF parsing (PyPDF2 or built-in parsers), multimodal inputs (charts, tables, reactions, molecular structures), and evaluated models via API or local runtime; expert annotation and cross-validation for dataset quality control.",
            "type_of_qualitative_law": "not applicable (SciAssess does not itself distill qualitative laws/principles from corpora; rather it evaluates model capabilities that could enable such distillation — e.g., L3 tasks integrate information to support reasoning and potential synthesis).",
            "evaluation_metrics": "Task-specific metrics including accuracy (multiple choice / true-false), recall (table extraction), F1-score (text extraction), and molecular similarity (Tanimoto similarity for SMILES outputs).",
            "results_summary": "Benchmarked 11 LLMs; OpenAI-o1 and GPT-4 family models frequently rank highest overall, especially in memorization and reasoning; multimodal and molecule-related tasks remain challenging (e.g., Tag to Molecule showed poor performance across models due to PDF parsing limitations). The study highlights strengths and specific failure modes but does not report distillation of generalizable scientific laws.",
            "human_involvement": "High: dataset creation and labeling by domain experts (two annotators per data point with cross-validation), human-designed prompts, and expert quality control; human evaluation used for dataset validation rather than for automated law extraction.",
            "dataset_or_corpus": "SciAssess benchmark dataset: 6,888 questions across 27 tasks and five modalities (text, charts, chemical reactions, molecular structures, tables); tasks sampled from papers (DOIs provided but documents not distributed in repo) and some public datasets (e.g., MMLU-Pro, MaScQA, B5CDR).",
            "limitations_or_challenges": "SciAssess notes limitations relevant to literature-level distillation: scope limited to four domains (no physics/engineering), no additional fine-tuning/training data provided (so results reflect off-the-shelf model capabilities), multimodal PDF parsing failures (notably for molecular structures), and tasks may favor particular model types; the paper does not perform large-scale law/principle distillation.",
            "notable_examples": "Illustrative failure: Tag-to-Molecule (SMILES extraction) performed poorly across LLMs mainly due to PDF parsing shortcomings; methodological note: chain-of-thought prompting used broadly (except for OpenAI-o1 where guideline discourages CoT); recommended needs: improved PDF parsing tailored to scientific modalities to enable downstream tasks such as molecule extraction or literature-level synthesis.",
            "uuid": "e5758.3",
            "source_info": {
                "paper_title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "Large language models for scientific synthesis, inference and explanation",
            "rating": 2,
            "sanitized_title": "large_language_models_for_scientific_synthesis_inference_and_explanation"
        },
        {
            "paper_title": "Sciriff: A resource to enhance language model instruction-following over scientific literature",
            "rating": 2,
            "sanitized_title": "sciriff_a_resource_to_enhance_language_model_instructionfollowing_over_scientific_literature"
        }
    ],
    "cost": 0.01638,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis</h1>
<p>Hengxing Cai ${ }^{1 <em>}$, Xiaochen Cai ${ }^{1 </em>}$, Junhan Chang ${ }^{1 <em>}$, Sihang $\mathbf{L i}^{1 </em>}$, Lin Yao ${ }^{1}$, Changxin Wang ${ }^{1}$, Zhifeng Gao ${ }^{1}$, Hongshuai Wang ${ }^{1}$, Yongge $\mathbf{L i}^{1}$, Mujie Lin ${ }^{1}$, Shuwen Yang ${ }^{1}$, Jiankun Wang ${ }^{1}$, Mingjun Xu ${ }^{1}$, Jin Huang ${ }^{1}$, Xi Fang ${ }^{1}$, Jiaxi Zhuang ${ }^{1}$, Yuqi Yin ${ }^{1}$, Yaqi Li ${ }^{1}$, Changhong Chen ${ }^{1}$, Zheng Cheng ${ }^{2}$, Zifeng Zhao ${ }^{2}$, Linfeng Zhang ${ }^{1,2}$ and Guolin $\mathrm{Ke}^{1}$<br>${ }^{1}$ DP Technology ${ }^{2}$ AI for Science Institute, Beijing</p>
<h4>Abstract</h4>
<p>Recent breakthroughs in Large Language Models (LLMs) have revolutionized scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data. In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. It aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization (L1), Comprehension (L2), and Analysis \&amp; Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including biology, chemistry, material, and medicine. To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement. We hope this evaluation supports the ongoing development of LLM applications in scientific literature analysis. SciAssess and its resources are available at https: //github.com/sci-assess/SciAssess.</p>
<h2>1 Introduction</h2>
<p>Recent advances in Large Language Models (LLMs), such as GPT (OpenAI, 2023; Brown, 2020), Gemini (Google, 2023), and Llama (Touvron et al., 2023), have attracted considerable attention due to their profound capabilities in natural language understanding and generation (Bubeck et al., 2023). Evaluating these models is crucial for exploring their capability boundaries and limitations, thereby driving technological advancements. In response, a variety of benchmarks tailored for LLMs have been proposed for extensive evaluation, covering a wide range of skills (Zhong et al., 2023;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of SciAssess. It spans over 4 subdomains and encompasses 27 tasks.</p>
<p>Huang et al., 2023) and diverse tasks (Srivastava et al., 2022; Suzgun et al., 2023a).</p>
<p>Despite LLMs not yet fully replacing scientific researchers in generating creative discoveries, they have demonstrated substantial potential in enhancing researchers' efficiency in scientific literature analysis (AI4Science and Quantum, 2023). Specific applications such as automatic literature summarization and knowledge extraction have seen practical deployments, significantly boosting researchers' productivity and expanding the range of literature that can be effectively utilized (Zheng et al., 2023). Inspired by Bloom's Taxonomy (Krathwohl, 2002), we systemize the requirements for scientific literature analysis assistants into three progressive levels: (1) Memorization (L1): Establishing an extensive foundational knowledge base to accurately address common factual questions in various scientific domains; (2) Comprehension (L2): Identifying, extracting, and understanding the core content of provided documents; and (3) Analysis \&amp; Reasoning (L3): Integrating extracted information with the existing knowledge base to perform logical reasoning and analysis.</p>
<p>Existing comprehensive LLMs benchmarks, such as MMLU-Pro (Hendrycks et al., 2021; Wang</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance overview of leading open and closed source LLMs on SciAssess. Each column represents a scientific domain. LLMs are evaluated on multiple tasks within each domain, with task details provided in Table 1. For closed source LLMs (first row), GPT-4o and GPT-4 are the leading models. For open source LLMs (second row), Llama3 and Qwen2 emerge as the top models.</p>
<p>et al., 2024), include some tasks related to scientific data. However, these sub-tasks have two limitations: (1) they mostly focus on Memorization, neglecting higher-level abilities such as L2 and L3; (2) these tasks lack the evaluation of various multimodal inputs (<em>e.g.,</em> charts, molecular structures, and tables), which are crucial in scientific literature.</p>
<p>In light of these existing limitations, we introduce <strong>SciAssess</strong> (<em>cf.</em> Figure 1) – a benchmark specifically designed for scientific literature analysis. SciAssess not only broadens the evaluation scope to encompass a wider range of LLM capabilities but also extends beyond text to include the extraction and interpretation of multimodal contents. Moreover, meticulous design is essential to creating evaluations that yield deep insights, ensure fairness across different LLMs. Consequently, SciAssess is founded on three critical considerations:</p>
<p><strong>Model Ability.</strong> A benchmark must clearly delineate the desired capabilities and model the intrinsic relationships among them, facilitating a diagnostic understanding. Thus, SciAssess evaluates across three progressive levels (<em>i.e.,</em> Memorization (L1), Comprehension (L2), and Analysis &amp; Reasoning (L3)) and five modalities (<em>i.e.,</em> texts, charts, chemical reactions, molecular structures, and tables). Consequently, SciAssess yields nuanced and informative evaluation outcomes, pinpointing specific aspects where the examined models may fall short.</p>
<p><strong>Scope &amp; Task.</strong> Benchmarks should encompass a broad array of scientific domains to ensure comprehensiveness. Within each domain, the selected tasks must authentically represent the typical challenges and scenarios characteristic of that field. Consequently, SciAssess spans over 4 sub-domains (<em>i.e.,</em> biology, chemistry, material, and medicine) and encompasses 27 tasks, each carefully suggested or designed by domain experts according to their professional experience.</p>
<p><strong>Scale &amp; Quality Control.</strong> The scale and quality of the benchmark must be impeccable to serve as a dependable basis for deriving accurate, actionable, and applicable insights. SciAssess contains 6,938 questions in total to ensure adequate scale. Each question is transformed from existing datasets or manually curated by domain experts hired by us <sup>1</sup>. Subsequently, expert cross-validation is performed to ensure correctness and reliability.</p>
<p>Overall, SciAssess aims to reveal the performance of LLMs as a scientific literature analysis assistant, thereby identifying their strength and weaknesses. The insights gained from SciAssess could</p>
<p><sup>1</sup>All data collection, annotation, and quality control tasks were carried out by the authors (who are also employees of the company) as part of their job responsibilities, and therefore, they were not provided with any additional compensation.</p>
<p>hopefully catalyze further enhancing the capabilities of LLMs in scientific literature analysis, ultimately contributing to the acceleration of scientific discovery and innovation.</p>
<h2>2 Benchmark Dataset</h2>
<p>We begin by outlining the ability assessment framework in Section 2.1, which serves as the backbone of our evaluation framework. Moving forward, we provide detailed description of evaluation scopes and tasks in Section 2.2. Lastly, we present the quality control measures implemented to ensure the integrity and reliability in Section 2.3.</p>
<h3>2.1 Ability Assessment Framework</h3>
<p>Guided by the widely accepted cognitive learning processes outlined in Bloom's Taxonomy (Krathwohl, 2002), we propose that the evaluation of LLMs in scientific literature analysis should be classified into three core levels:</p>
<p>Memorization (L1) refers to the model's extensive knowledge base, which allows it to accurately answer common factual questions in science autonomously. Comprehension (L2) is the ability to precisely identify and extract key information and facts within a given text, and to comprehend them. Analysis \&amp; Reasoning (L3) demonstrate the model's advanced capability to amalgamate extracted information with its existing knowledge base for logical reasoning and analysis, leading to well-founded conclusions or predictions.</p>
<p>Inspecting existing LLM benchmarks in science field (See Section 4) through three-level ability assessment framework, we find that they mostly focus on Memorization (L1) - the foundational knowledge base for scientific facts - while overlooking the higher-level abilities of Comprehension (L2) and Analysis \&amp; Reasoning (L3).</p>
<p>Given the significant potential of leveraging LLMs as scientific literature analysis assistants to boost scientific discovery, we propose SciAssess as a more comprehensive benchmark, in terms of tasks, scopes, and modalities.</p>
<h3>2.2 Scope \&amp; Task</h3>
<p>After categorizing the ability of of LLMs into three levels, we proceed to introduce how we choose the tasks in SciAssess. First, we include four vertical domains: biology, chemistry, material, and medicine, as shown in Figure 1. This categorization ensures that SciAssess captures the unique
challenges and requirements of each specific field. Then, as mentioned above, Memorization (L1), being the extensive foundation for other higher-level abilities, should encompass as large a knowledge base as possible. Thus, SciAssess includes factual questions in MMLU-Pro (Wang et al., 2024) and MaScQA (Zaki et al., 2023), covering fundamental knowledge in each field. For the evaluation of Comprehension (L2) and Analysis \&amp; Reasoning (L3), we identify realistic demands by consulting domain experts and curate corresponding tasks. The reason is that solving tasks in these domains require finergrained abilities, such as understanding tables and molecular structures. For instance, crucial composition information in material science literature is often found in tables, whereas key information extraction in drug discovery necessitates the accurate recognition of molecular structures.</p>
<p>SciAssess, as presented in Table 1, comprises 6,888 questions across 27 tasks in five scientific domain, encompassing three ability levels: Memorization (L1), Comprehension (L2), and Analysis \&amp; Reasoning (L3). Of these tasks, 7 out of 27 are transformed from existing public datasets (gray tasks in Table 1), and the other 21 tasks curated by us are based on contents from academic papers, specifically designed to assess the ability to analyze scientific literature. We show the token lengths (GPT-4 tokenizer) of questions and answers for each task in Figure 3. SciAssess also includes five types of questions (i.e., true/false questions, multiple-choice questions, table extraction, text extraction, and molecule generation) with four metrics (i.e., accuracy, recall, F1-score, and molecule similarity). For detailed descriptions and concrete examples, please refer to Appendix A. We also provide general prompt template and specific prompt for each task in Appendix B and C, respectively.</p>
<h3>2.2.1 Biology</h3>
<p>Biological literature encompasses a wealth of specialized terminology and complex concepts, as well as a significant amount of non-textual information such as tables and figures. Effectively extracting and integrating these elements presents a crucial challenge. Given that tasks in the biological domain typically require precise identification and understanding of intricate biological entities, processes, and relationships, we have selected a set of representative tasks, including the recognition of specialized terminology, the comprehension of chart information, and the extraction of entity rela-</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Task</th>
<th>Ability</th>
<th># Questions</th>
<th>Question Type</th>
<th>Metric</th>
<th>Modality</th>
</tr>
</thead>
<tbody>
<tr>
<td>Biology</td>
<td>MMLU-Pro-Biology</td>
<td>L1</td>
<td>717</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Biology Chart QA</td>
<td>L2</td>
<td>199</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Chart</td>
</tr>
<tr>
<td></td>
<td>Chemical Entities Recognition</td>
<td>L2</td>
<td>500</td>
<td>Text Extraction</td>
<td>F1-score</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Compound Disease Recognition</td>
<td>L2</td>
<td>775</td>
<td>Text Extraction</td>
<td>F1-score</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Disease Entities Recognition</td>
<td>L2</td>
<td>500</td>
<td>Text Extraction</td>
<td>F1-score</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Gene Disease Function</td>
<td>L2</td>
<td>24</td>
<td>Text Extraction</td>
<td>F1-score</td>
<td>Text only</td>
</tr>
<tr>
<td>Chemistry</td>
<td>MMLU-Pro-Chemistry</td>
<td>L1</td>
<td>1,132</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Electrolyte Table QA</td>
<td>L2</td>
<td>200</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Table</td>
</tr>
<tr>
<td></td>
<td>OLED Property Extraction</td>
<td>L2</td>
<td>13</td>
<td>Table Extraction</td>
<td>Recall</td>
<td>Mol., Table</td>
</tr>
<tr>
<td></td>
<td>Polymer Chart QA</td>
<td>L2</td>
<td>15</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Chart</td>
</tr>
<tr>
<td></td>
<td>Polymer Composition QA</td>
<td>L2</td>
<td>209</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Polymer Property Extraction</td>
<td>L2</td>
<td>109</td>
<td>Table Extraction</td>
<td>Recall</td>
<td>Table</td>
</tr>
<tr>
<td></td>
<td>Solubility Extraction</td>
<td>L2</td>
<td>100</td>
<td>Table Extraction</td>
<td>Recall</td>
<td>Table</td>
</tr>
<tr>
<td></td>
<td>Reactant QA</td>
<td>L3</td>
<td>195</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Reaction</td>
</tr>
<tr>
<td></td>
<td>Reaction Mechanism QA</td>
<td>L3</td>
<td>22</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Reaction</td>
</tr>
<tr>
<td>Material</td>
<td>Material QA</td>
<td>L1</td>
<td>263</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Alloy Chart QA</td>
<td>L2</td>
<td>15</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Chart</td>
</tr>
<tr>
<td></td>
<td>Composition Extraction</td>
<td>L2</td>
<td>244</td>
<td>Table Extraction</td>
<td>Recall</td>
<td>Table</td>
</tr>
<tr>
<td></td>
<td>Temperature QA</td>
<td>L2</td>
<td>207</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Sample Differentiation</td>
<td>L3</td>
<td>237</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Treatment Sequence</td>
<td>L3</td>
<td>202</td>
<td>True/False</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td>Medicine</td>
<td>MMLU-Pro-Health</td>
<td>L1</td>
<td>818</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Text only</td>
</tr>
<tr>
<td></td>
<td>Affinity Extraction</td>
<td>L2</td>
<td>40</td>
<td>Table Extraction</td>
<td>Recall</td>
<td>Mol., Table</td>
</tr>
<tr>
<td></td>
<td>Drug Chart QA</td>
<td>L2</td>
<td>15</td>
<td>Multiple Choice</td>
<td>Accuracy</td>
<td>Chart</td>
</tr>
<tr>
<td></td>
<td>Tag in Molecule</td>
<td>L2</td>
<td>50</td>
<td>Mol. Generation</td>
<td>Mol. Similarity</td>
<td>Mol.</td>
</tr>
<tr>
<td></td>
<td>Markosh to Molecule</td>
<td>L3</td>
<td>37</td>
<td>Mol. Generation</td>
<td>Mol. Similarity</td>
<td>Mol.</td>
</tr>
<tr>
<td></td>
<td>Molecule in Document</td>
<td>L3</td>
<td>50</td>
<td>True/False</td>
<td>Accuracy</td>
<td>Mol.</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the SciAssess. It comprises 6,888 questions across 27 tasks in five sub-domains. Tasks are categorized into three ability levels: Memorization (L1), Comprehension (L2), and Analysis &amp; Reasoning (L3). Tasks that are gray are transformed from existing datasets, while others are curated by domain experts hired by us.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Distribution of token length for questions and answers in each task.</p>
<p>tionships, to evaluate the performance in this field.</p>
<p>In this domain, following tasks are devised: MMLU-Pro-Biology, biology chart QA, chemical entities recognition, compound disease recognition, disease entities recognition, and gene disease function. Detailed descriptions and prompts are provided in Appendix C.1.</p>
<h3>2.2.2 Chemistry</h3>
<p>The field of chemistry involves a vast array of complex molecular structures, chemical reactions, and properties, alongside a substantial amount of data presented in formulas, reaction equations, and diagrams. Effectively processing and interpreting these components is a significant challenge for language models. Tasks in the chemical domain demand precise understanding of molecular compositions, reaction mechanisms, and material properties. To evaluate the performance of LLMs in this domain, we have selected representative tasks such as the recognition of chemical compounds, the interpretation of reaction pathways, and the extraction of relationships between chemical entities.</p>
<p>We devise following tasks for organic materials: MMLU-Pro-Chemistry, electrolyte table QA, OLED property extraction, polymer chart QA, polymer composition extraction, polymer property extraction, solubility extraction, reactant QA, and reaction mechanism QA. Detailed descriptions and prompt templates are provided in Appendix C.2.</p>
<h3>2.2.3 Materials</h3>
<p>Materials science encompasses a broad range of substances, including metals, ceramics, polymers, and composites, each with distinct properties and applications. These materials are widely used across industries such as aerospace, automotive, and construction. By fine-tuning their composition, structure, and processing techniques, materials can be engineered to meet specific performance requirements (Caron and Khan, 1983). Accurately</p>
<p>extracting material compositions, structural characteristics, and process parameters from the literature is essential for advancing material design and optimization.</p>
<p>Specifically, following tasks are devised: material QA, Alloy Chart QA, composition extraction, temperature QA, sample differentiation, and treatment sequence. Detailed descriptions and prompt templates are provided in Appendix C.3.</p>
<h3>2.2.4 Medicine</h3>
<p>Medicine focuses on developing new therapeutics. Leveraging advanced intelligent tools, especially LLMs, can significantly enhance the efficiency and effectiveness of discovering and developing new drugs. To evaluate the capability of LLMs in this domain, it is imperative to develop specialized tasks that reflect the complexities and nuances of biomedical research. By designing targeted tasks, we can better assess the ability of LLMs to navigate and interpret the wealth of information critical to the development of new therapeutics.</p>
<p>Specifically, we devise: MMLU-Pro-Health, affinity extraction, drug chart QA, tag to molecule, markush to molecule, molecule in document. Detailed descriptions and prompts are provided in Appendix C. 4 .</p>
<h3>2.3 Data Quality, Privacy, and Copyright</h3>
<p>To safeguard the quality and ethical standards, meticulous steps were undertaken in its preparation and validation:</p>
<p>Distractor Construction: Our data points are human-annotated, as well as the distractors. And how the distractors are determined depends on the specific task. For example, in value-type multiplechoice questions, the distractors are values near the ground truth. For extraction tasks, the distractors are other targets in the given context, except for the ground truth target.</p>
<p>Expert Validation: Each data point (as indicated by black tasks in Table 1) is independently labeled by two annotators who are domain experts in the relevant fields. If their labels agree, the label is accepted; if not, they engage in a discussion to determine the final label. Their initial annotations have a Cohen's Kappa value (McHugh, 2012) of 0.75 , which indicates high reliability or agreement.</p>
<p>Screening and Anonymization: Our annotators were instructed not to use any data samples containing sensitive information when building the benchmark. For example, data samples including
personal health information or specific drug details were carefully reviewed. If such sensitive information was identified, it was either anonymized by removing personal identifiers or replacing specific details with general terms, or the entire sample was excluded from the benchmark.</p>
<p>Copyright Compliance: Our benchmark includes two types of data: some are adopted from existing benchmarks, and others are constructed from scratch by our team. For the data adopted from existing benchmarks, we provide the corresponding sources. For the data we created, we have obtained the necessary copyrights for the files used. To ensure full compliance with copyright laws, our repository only provides the Digital Object Identifier (DOI) for papers or patent number, and does not distribute the actual documents. Researchers need to download the necessary files independently. Detailed instructions is included in the codebase to guide researchers on how to place the downloaded documents into the designated folder.</p>
<h2>3 Experiment</h2>
<h3>3.1 Experiment Setup</h3>
<p>Baseline LLMs. To measure how leading LLMs perform on SciAssess, we benchmark extensively. For closed-source LLMs, we test OpenAI-o1, GPT4o, GPT-4, GPT-3.5, Gemini-1.5-Pro, Claude 3 Opus, Moonshot-v1 and Doubao. For open-source LLMs, we test Llama-3.1-70B, Mixtral-8x22B-Instruct-v0.1, and Qwen-2.5-72B. Briefs about all models are provided in Appendix D.</p>
<p>Experiment Workflow. For closed-source models, we utilize the official API calls provided by the model developers, while for open-source models, we obtain these models from HuggingFace (Wolf et al., 2019), deploy them locally with vllm (Kwon et al., 2023), and then perform the tests. Tasks curated by us require real context from papers, thus the PDF content needs to be converted to text as inputs for LLMs. If the LLM includes a built-in PDF parsing interface (e.g., Gemini and Moonshot), we simply use the interface; otherwise, we employ PyPDF2 ${ }^{2}$, a widely-used open-source PDF parsing tool. Our aim is to explore the ability boundary of LLMs, thus strategies that enhance LLMs' inference ability (i.e., in-context learning (Brown, 2020) and chain-of-thought (Wei et al., 2022)) are adopted. Specifically, due to the input length limitations of the LLMs, tasks requiring long context of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">o1</th>
<th style="text-align: center;">GPT-4o</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">Moonshot</th>
<th style="text-align: center;">Claude3</th>
<th style="text-align: center;">Doubue</th>
<th style="text-align: center;">Gemini</th>
<th style="text-align: center;">Llama3.1</th>
<th style="text-align: center;">Qwen2.5</th>
<th style="text-align: center;">Mistral</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">MMLU-Pro-Biology*</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.743</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Biology Chart QA</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.422</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chemical Entities Recognition*</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.707</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Compound Disease Recognition*</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.751</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.757</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Disease Entities Recognition*</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.737</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gene Disease Function*</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.418</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">MMLU-Pro-Chemistry*</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.501</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Electrolyte Table QA</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.455</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OLED Property Extraction</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.355</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Polymer Chart QA</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.800</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Polymer Composition QA</td>
<td style="text-align: center;">0.986</td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.493</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Polymer Property Extraction</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.690</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.573</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solubility Extraction</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.314</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reactant QA</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.231</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reaction Mechanism QA</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.273</td>
</tr>
<tr>
<td style="text-align: center;">Material</td>
<td style="text-align: center;">Material QA</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.631</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Alloy Chart QA</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.467</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Composition Extraction</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.177</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Temperature QA</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.382</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sample Differentiation</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.426</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Treatment Sequence</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.673</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">MMLU-Pro-Health*</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.603</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Affinity Extraction</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.049</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Drug Chart QA</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.400</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tag2Mol</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.021</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Markush2Mol</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.576</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mol In Document</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.460</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison of LLMs across various scientific domains. Orange and green indicate the best in closed and open source LLMs, respectively. Chain-of-thought prompt is implemented for each task and model, except for OpenAI-o1. * indicates 3-shot.
a PDF document are executed in a zero-shot manner. Tasks that do not require such long context (e.g., MMLU-Pro, entities recognition) are evaluated using 3-shot settings. And chain-of-thought prompt is implemented in every task by prompting the model to think step-by-step before concluding. The only exception is OpenAI-o1, whose official prompt guideline suggests users to "avoid chain-ofthought prompts". We also provide complete performance evaluation results with chain-of-thought prompts in Appendix E.</p>
<h3>3.2 Results and Analysis</h3>
<p>In this section, we analyze the performance of LLMs on SciAssess. The overall performance comparison, as summarized in Table 2, reveals the distinct strengths and weaknesses of each model in science literature analysis.</p>
<h3>3.2.1 Performances of Different Ability Levels</h3>
<p>Table 3 presents the performance of evaluated LLMs across three progressive ability level. Tasks are further categorized according to their question types, with average results and rankings provided for each ability levels. We observe the following: (1) Memorization (L1): OpenAI-o1 and Qwen2.5 demonstrates the highest average accuracy of 0.843 and 0.742 , respectively, indicating consistently superior performance in memorization tasks. (2) Comprehension (L2): OpenAI-o1 excels in multiple-choice and text extraction comprehension
with accuracy of 0.790 and 0.781 , respectively, and maintains the top average rank of 3.25 . Notably, the only L2-level molecule generation task, Tag to Molecule, reveals poor performance across all LLMs. As illustrated in Figure 4, current PDF parsing technologies, whether open-source like PyPDF or proprietary like Gemini or Moonshot, fail to effectively parse molecular structures in documents. Consequently, LLMs struggle with the Tag to Molecule task. We propose that a critical advancement for future LLM-based literature understanding assistant is the integration of PDF parsing solutions capable of recognizing molecular structures. (3) Analysis \&amp; Reasoning (L3): The average rank reveals OpenAI-o1, GPT-4, and Gemini are the top performers with ranks of 2.00, 3.33, and 3.33 , respectively.</p>
<p>Overall, OpenAI-o1 consistently ranks high across all ability levels. GPT-4o and Gemini also demonstrate strong overall performance, especially in memorization and reasoning.</p>
<p>Based on these observations, we suggest the following recommendations: (1) For tasks heavily reliant on memorization, OpenAI-o1 and Qwen2.5 are recommended due to their high accuracy and ranking; (2) For comprehension tasks, particularly those involving complex data extraction and generation, OpenAI-o1 is an ideal choice. (3) For analysis and reasoning tasks, OpenAI flagship models (i.e., o1, GPT4, GPT-4o), and Gemini provide reliable performance and should be considered.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ability Level</th>
<th style="text-align: center;">Question Type</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">o1</th>
<th style="text-align: center;">GPT-4o</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">Moonshot</th>
<th style="text-align: center;">Claude3</th>
<th style="text-align: center;">Doubas</th>
<th style="text-align: center;">Gemini</th>
<th style="text-align: center;">Llama3.1</th>
<th style="text-align: center;">Qwen2.5</th>
<th style="text-align: center;">Mistral</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Memorization (L1)</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.619</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">2.000</td>
<td style="text-align: center;">6.000</td>
<td style="text-align: center;">11.000</td>
<td style="text-align: center;">10.000</td>
<td style="text-align: center;">7.000</td>
<td style="text-align: center;">8.000</td>
<td style="text-align: center;">5.000</td>
<td style="text-align: center;">4.000</td>
<td style="text-align: center;">3.000</td>
<td style="text-align: center;">9.000</td>
</tr>
<tr>
<td style="text-align: center;">Comprehension (L2)</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.488</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Table Extraction</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.294</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text Extraction</td>
<td style="text-align: center;">F1-score</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.690</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.655</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mol. Generation</td>
<td style="text-align: center;">Mol. Similarity</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.021</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.250</td>
<td style="text-align: center;">3.375</td>
<td style="text-align: center;">5.250</td>
<td style="text-align: center;">10.500</td>
<td style="text-align: center;">5.500</td>
<td style="text-align: center;">8.750</td>
<td style="text-align: center;">7.000</td>
<td style="text-align: center;">3.750</td>
<td style="text-align: center;">3.125</td>
<td style="text-align: center;">5.250</td>
<td style="text-align: center;">10.250</td>
</tr>
<tr>
<td style="text-align: center;">Analysis \&amp; Reasoning (L3)</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.310</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mol. Generation</td>
<td style="text-align: center;">Mol. Similarity</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.576</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.609</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.566</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.000</td>
<td style="text-align: center;">3.667</td>
<td style="text-align: center;">3.333</td>
<td style="text-align: center;">9.333</td>
<td style="text-align: center;">7.333</td>
<td style="text-align: center;">9.667</td>
<td style="text-align: center;">6.000</td>
<td style="text-align: center;">3.333</td>
<td style="text-align: center;">7.667</td>
<td style="text-align: center;">6.000</td>
<td style="text-align: center;">7.667</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance on Memorization (L1), Comprehension (L2), and Analysis \&amp; Reasoning (L3) tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt <br> You are an expert in the field of organic chemistry, can help user get SMILES <br> formula from documents. Use chain of thought reasoning to analyze the <br> document and extract the SMILES formula step by step. After reasoning, just <br> give the SMILES formula as the final answer without further explanation.</th>
<th style="text-align: center;">Response (Similarity Score)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Format:</td>
<td style="text-align: center;">Ground Truth:</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning: [Reasoning]</td>
<td style="text-align: center;">c1:oc2c(c1N3ncnc3:[sH]z2C[N])C)C1CCCCC1 (0.20)</td>
</tr>
<tr>
<td style="text-align: left;">Answer: [Extracted SMILES]</td>
<td style="text-align: center;">GPT4o</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CN1CCC(CC1)C2=CNC3=CC(=C(C=C32)N4N=N(=N4) (0.70)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">GPT4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">c1ccc2c(c1)c(cn2)CCN(C)Cn1nncc1 (0.00)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">GPT3.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">N-A (0.00)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Moonshot</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">C1CCN(CC1)C2=C3C(C(=CC=C3)N)C4=C2C=CC=N4 (0.20)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Claude3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Cc1cn(c(n1)n2cc3c(c2)c(c[nH]3)CCN4CCC(CC4)C)C (0.28)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Doubas</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CCN1CCCCC1N=C2C=CC(=CC2=O)N (0.16)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Gemini</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CC1CCN(Cc2c(N=N\C3=CC=C(N(C=C3)c(C)cc2)CC1 (0.00)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Llama3.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CC1=C(N2C(=NC(=N2)N)C3=CC=CC=C3(C(=CN(C)C)C4=CC=CC=C41 (0.13)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Qwen2.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CC1=CC=C(C=C1)C2=CC=C(C=C2)N3C(=NC=N3)CCN4CCN(CC4)C5=CC=CC <br> =C5 (0.19)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Mistral</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">The SMILES formula for compound 12 is</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CN1C=NC2=C1C(=C(C(=C2)N(C)C)NC)C(=O)NCCCN. (0.00)</td>
</tr>
</tbody>
</table>
<p>Figure 4: Example of Tag to Molecule task.</p>
<h3>3.2.2 Performance on Multimodal Contents</h3>
<p>Table 4 summarizes the performance of LLMs on multimodal content tasks. For each modality, performances are averaged over different question types. We observe the following: (1) Text-only tasks: GPT-4 achieves the highest average rank (2.00). (2) Chart tasks: OpenAI-o1 exhibit the highest accuracy (0.696). (3) Chemical reaction tasks: OpenAI-o1 stands out with high accuracy in multiple-choice questions (0.62). (3) Molecule tasks: GPT-4o excels with average ranks of 3.17, particularly in table extraction task. (5) Table tasks: GPT-4o lead with the highest table extraction recall (0.44).</p>
<p>Overall, OpenAI models consistently rank as top performers across most modalities. Gemini
also demonstrate strong performance, especially in molecule generation tasks.</p>
<p>Based on these observations, we suggest the following recommendations: (1) For text-only tasks, OpenAI-o1 and GPT-4o are highly recommended due to their superior accuracy and ranking. (2) For chart and chemical reaction tasks, OpenAI-o1 excels, making it suitable for such specialized applications. (3) For molecule structure and tabular tasks, GPT-4o is the preferred model, given it remarkable performance.</p>
<h2>4 Related Work</h2>
<p>General benchmarks for LLMs. LLMs are evaluated across a variety of benchmarks to comprehensively assess their capabilities. Some benchmarks,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Modality</th>
<th style="text-align: center;">Question Type</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">o1</th>
<th style="text-align: center;">GPT-4s</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">Moonshot</th>
<th style="text-align: center;">Claude3</th>
<th style="text-align: center;">Doubao</th>
<th style="text-align: center;">Gemini</th>
<th style="text-align: center;">Llama3.1</th>
<th style="text-align: center;">Qwen2.5</th>
<th style="text-align: center;">Mixtral</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Text Only</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.540</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text Extraction</td>
<td style="text-align: center;">F1-score</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.690</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.655</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.673</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.000</td>
<td style="text-align: center;">6.667</td>
<td style="text-align: center;">2.667</td>
<td style="text-align: center;">10.667</td>
<td style="text-align: center;">4.667</td>
<td style="text-align: center;">9.333</td>
<td style="text-align: center;">5.833</td>
<td style="text-align: center;">4.667</td>
<td style="text-align: center;">5.333</td>
<td style="text-align: center;">5.833</td>
<td style="text-align: center;">7.333</td>
</tr>
<tr>
<td style="text-align: center;">Chart</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.522</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">3.000</td>
<td style="text-align: center;">11.000</td>
<td style="text-align: center;">10.000</td>
<td style="text-align: center;">7.000</td>
<td style="text-align: center;">9.000</td>
<td style="text-align: center;">6.000</td>
<td style="text-align: center;">2.000</td>
<td style="text-align: center;">5.000</td>
<td style="text-align: center;">4.000</td>
<td style="text-align: center;">6.000</td>
</tr>
<tr>
<td style="text-align: center;">Reaction</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.252</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">2.000</td>
<td style="text-align: center;">7.000</td>
<td style="text-align: center;">10.000</td>
<td style="text-align: center;">4.000</td>
<td style="text-align: center;">11.000</td>
<td style="text-align: center;">8.000</td>
<td style="text-align: center;">3.000</td>
<td style="text-align: center;">6.000</td>
<td style="text-align: center;">5.000</td>
<td style="text-align: center;">9.000</td>
</tr>
<tr>
<td style="text-align: center;">Mol.</td>
<td style="text-align: center;">Table Extraction</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.202</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mol. Generation</td>
<td style="text-align: center;">Mol. Similarity</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.298</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.460</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.667</td>
<td style="text-align: center;">3.167</td>
<td style="text-align: center;">3.667</td>
<td style="text-align: center;">10.000</td>
<td style="text-align: center;">10.000</td>
<td style="text-align: center;">7.333</td>
<td style="text-align: center;">5.667</td>
<td style="text-align: center;">4.333</td>
<td style="text-align: center;">5.667</td>
<td style="text-align: center;">4.500</td>
<td style="text-align: center;">8.000</td>
</tr>
<tr>
<td style="text-align: center;">Table</td>
<td style="text-align: center;">Multiple Choice</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.455</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Table Extraction</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.294</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Rank</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.000</td>
<td style="text-align: center;">1.750</td>
<td style="text-align: center;">3.000</td>
<td style="text-align: center;">10.500</td>
<td style="text-align: center;">6.250</td>
<td style="text-align: center;">9.500</td>
<td style="text-align: center;">8.000</td>
<td style="text-align: center;">5.750</td>
<td style="text-align: center;">4.250</td>
<td style="text-align: center;">4.000</td>
<td style="text-align: center;">10.000</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance on multimodal contents.
such as MMLU (Hendrycks et al., 2021), MMLUpro (Wang et al., 2024), CMMLU (Li et al., 2023a), and Xiezhi (Gu et al., 2024), are instrumental in evaluating models' world knowledge across diverse domains. For reasoning capabilities, benchmarks like GSM8k (Cobbe et al., 2021) and BBH (Suzgun et al., 2023b) provide rigorous assessments of models' problem-solving and logical reasoning skills. In the realm of programming, benchmarks such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) serve as popular testbeds for evaluating models' coding proficiency. Additionally, TruthfulQA (Lin et al., 2022) and HaluEval (Li et al., 2023b) are pivotal in assessing the veracity of models' outputs, ensuring their alignment with factual information.</p>
<p>Although some general benchmarks include a subset of science subjects, they mostly focus on Memorization (L1) and often overlook higher-level abilities such as Comprehension (L2) and Analysis \&amp; Reasoning (L3). Furthermore, these benchmarks lack context-involved tasks, for example, understanding and reasoning over a scientific paper.</p>
<p>Scientific literature benchmarks. Prior works have made significant strides in developing LLM benchmarks to assess the understanding of scientific literature. In the biomedical domain, notable efforts include BLUE (Peng et al., 2019), which provides a set of tasks for evaluating models on various aspects of biomedical text-mining. Building on this, BLURB (Gu et al., 2021) offers an extensive collection of datasets to further refine model performance in this specialized field. More recently, InBoXBART (Parmar et al., 2022) has been introduced, focusing on integrating information across multiple biomedical documents. SciRIFF (Wadden et al., 2024) is designed to extract
and synthesize information from research literature across various scientific disciplines.</p>
<p>Compared with existing scientific literature benchmarks, SciAssess focuses more on tasks for interpreting multi-modal content (e.g., molecular structures and tables), which are common in scientific literature. Moreover, it features a real-world application scenarios that LLMs digest parsed PDF contents with parsing errors.</p>
<h2>5 Conclusion and Future Work</h2>
<p>SciAssess rigorously assesses the capabilities of LLMs for scientific literature analysis. It focuses four specialized areas: biology, chemistry, material, and medicine. The benchmark focuses on assessing LLMs' core competencies in Memorization (L1), Comprehension (L2), and Analysis \&amp; Reasoning (L3) within the context of scientific literature analysis. Through detailed evaluations of 11 LLMs, SciAssess highlights their strengths and identifies areas needing improvement across various ability levels, content modalities, and contextual scenarios. Additionally, we emphasize the urgent need for PDF parsing algorithms tailored to handle content of various modalities, such as molecular structures and chemical reactions. We hope that SciAssess supports the ongoing development of LLMs in scientific literature analysis. Looking ahead, we plan to broaden the range of scientific domains included in SciAssess and incorporate more vertical domains. These enhancements aim to improve the benchmark's utility and efficacy, providing clearer guidance and fostering the advancement of LLMs in scientific literature analysis.</p>
<h2>Limitation</h2>
<p>While SciAssess provides a comprehensive and valuable benchmarking suite across four primary domains - biology, chemistry, material, and medicine - there are several limitations to consider. Firstly, the scope of SciAssess is currently constrained to these four domains, with potential future extensions to other vertical domains such as physics and engineering.</p>
<p>Secondly, the creation and curation of highquality, domain-specific training data are essential for the effective evaluation and improvement of LLMs. However, due to the high cost associated with manual labeling, SciAssess does not provide additional training data for these tasks. This absence of supplementary data can limit the ability of researchers to fine-tune and enhance LLMs specifically for the tasks included in SciAssess. Consequently, the benchmark results might reflect the inherent capabilities of the models rather than their optimized performance for each specific domain.</p>
<p>Lastly, while SciAssess aims to provide a rigorous evaluation framework, the complexity and diversity of scientific domains present challenges in ensuring comprehensive coverage and fairness. Some tasks may inherently favor certain types of models or architectures, leading to potential biases in performance evaluation.</p>
<h2>Broader Impact</h2>
<p>Our work on benchmarking scientific literature analysis aligns with the scope of existing LLM benchmarks such as MMLU-pro. This paper represents progress in calibrating LLMs for specific domains, thereby amplifying the impacts that LLM benchmarks have had (and will continue to have) on the broader world. Additionally, we have not identified any ethical concerns or potential risks associated with this work.</p>
<h2>References</h2>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The impact of large language models on scientific discovery: a preliminary study using GPT-4. CoRR, abs/2311.07361.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.</p>
<p>Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712.</p>
<p>Pierre Caron and T Khan. 1983. Improvement of creep strength in a nickel-base single-crystal superalloy by heat treatment. Materials Science and Engineering, 61(2):173-184.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Gemini Team Google. 2023. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805.</p>
<p>Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, 3(1):1-23.</p>
<p>Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, Qianyu He, Rui Xu, Wenhao Huang, Jingping Liu, Zili Wang, Shusen Wang, Weiguo Zheng, Hongwei Feng, and Yanghua Xiao. 2024. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. In AAAI, pages 18099-18107. AAAI Press.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In $I C L R$. OpenReview.net.</p>
<p>José Luis Hernández-Rivera, Esperanza Elizabeth Martínez Flores, Emmanuel Ramírez Contreras, Jorge Garcia Rocha, Jose de Jesus Cruz-Rivera, and Gabriel Torres-Villasenor. 2017. Evaluation of hardening and softening behaviors in zn-21al-2cu alloy processed by equal channel angular pressing. Journal of Materials Research and Technology, 6(4):329-333.</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. CoRR, abs/2305.08322.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.</p>
<p>Dong-Cho Kim, Tomo Ogura, Ryosuke Hamada, Shotaro Yamashita, and Kazuyoshi Saida. 2021. Prediction of reversible $\alpha / \gamma$ phase transformation in multi-pass weld of fe-cr-ni ternary alloy by phasefield method. Journal of Advanced Joining Processes, 4:100067.</p>
<p>David R Krathwohl. 2002. A revision of bloom's taxonomy: An overview. Theory into practice, 41(4):212218.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, page 611-626, New York, NY, USA. Association for Computing Machinery.</p>
<p>Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023a. CMMLU: measuring massive multitask language understanding in chinese. CoRR, abs/2306.09212.</p>
<p>Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b. Halueval: A largescale hallucination evaluation benchmark for large language models. Preprint, arXiv:2305.11747.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In ACL (1), pages 3214-3252. Association for Computational Linguistics.</p>
<p>Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276-282.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>OpenAI. 2024. Openai o1.
Sizhuo Ouyang, Xinzhi Yao, Yuxing Wang, Qianqian Peng, Zhihan He, and Jingbo Xia. 2022. Text mining task for "gene-disease" association semantics in chip 2022. In China Health Information Processing Conference, pages 3-13. Springer.</p>
<p>Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. 2022. Inboxbart: Get instructions into biomedical multi-task learning. In NAACL-HLT (Findings), pages 112-128. Association for Computational Linguistics.</p>
<p>Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. In Proceedings of the 2019 Workshop on Biomedical Natural Language Processing (BioNLP 2019).</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi,</p>
<p>Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2023a. Challenging big-bench tasks and whether chain-of-thought can solve them. In ACL (Findings), pages 13003-13051. Association for Computational Linguistics.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2023b. Challenging big-bench tasks and whether chain-of-thought can solve them. In ACL (Findings), pages 13003-13051. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Édouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Francesca Villa, Adelaide Nespoli, Carlo Fanciulli, Francesca Passaretti, and Elena Villa. 2020. Physical characterization of sintered nimnga ferromagnetic shape memory alloy. Materials, 13(21):4806.</p>
<p>David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, et al. 2024. Sciriff: A resource to enhance language model instruction-following over scientific literature. arXiv preprint arXiv:2406.07835.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574.</p>
<p>Chih-Hsuan Wei, Yifan Peng, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Jiao Li, Thomas C. Wiegers, and Zhiyong Lu. 2016. Assessing the state of the art in biomedical relation extraction: overview of the biocreative v chemical-disease relation (cdr) task. Database, 2016.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771.</p>
<p>Mohd Zaki, NM Krishnan, et al. 2023. Mascqa: A question answering dataset for investigating materials science knowledge of large language models. arXiv preprint arXiv:2308.09115.</p>
<p>Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan. 2023. Large language models for scientific synthesis, inference and explanation. CoRR, abs/2310.07984.</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364.</p>
<h2>A Question Type</h2>
<p>Five types of questions, as illustrated in Figure 5 are devised to evaluate the models. Each question type is accompanied by a detailed description and representative examples, along with the corresponding metrics used for assessment. For convenience, the input in each example is simplified, and its instruction is omitted.</p>
<h2>B General Prompt Template</h2>
<p>We design following general prompt template for scientific literature analysis. It consists of: a system message defining the role of the assistant, the task description, some optional few-shot examples, and a user prompt of the question.</p>
<h2>Prompt Template</h2>
<h2>Role setting and task description:</h2>
<p>You are a highly intelligent assistant who answers the following multiple choice question correctly.</p>
<h2>Few-shot examples:</h2>
<p>Question: <question 1>
Answer: <answer 1>
Question: <question n>
Answer: <answer n></p>
<h2>Question:</h2>
<p>Predict the number of lines in the EPR spectrum of a solution of 13C-labelled methyl radical $(13 \mathrm{CH} 3 \mathrm{~m})$, assuming the lines do not overlap.
a) 4
b) 3
c) 6
d) 24</p>
<h2>C Task Prompt</h2>
<p>In this section, we detail the prompt templates for all tasks in SciAssess benchmark. We will introduce these templates in the following order: Biology (Section C.1), Chemistry (Section C.2), Material (Section C.3) and Medicine (Section C.4).</p>
<h2>C. 1 Biology</h2>
<h2>C.1.1 MMLU-Pro-Biology</h2>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are a highly intelligent assistant who answers the following multiple choice question correctly. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx). Format:
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>Which of the following would most likely provide examples of mitotic cell divisions?
a) cross section of muscle tissue
b) longitudinal section of a shoot tip
c) longitudinal section of a leaf vein
d) cross section of a fruit
e) cross section of a leaf
f) longitudinal section of a petal
g) longitudinal section of a seed
h) cross section of an anther (site of pollen production in a flower)</p>
<h2>Expected Answer:</h2>
<p>b) longitudinal section of a shoot tip</p>
<h2>C.1.2 Biology Chart QA</h2>
<p>The analysis and understanding of biological properties, compositions, and processing techniques are critical for the discovery and development in life sciences. Often, this information is presented in charts, making it essential to extract and integrate such information with textual data. To assess the retrieval capabilities of models in the context of biological chart information, we have designed multiple-choice questions.</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Biomedical. You are a highly intelligent biology scientist who answers the following multiple choice question correctly. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx). Format:
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>In Figure 3, which has a higher accurate score, with the graph encoder or without?
a) with graph encoder
b) w/o graph encoder</p>
<h2>Expected Answer:</h2>
<p>a) with graph encoder</p>
<h2>C.1.3 Chemical Entities Recognition</h2>
<p>This task involves recognizing chemical entity names using data from B5CDR (Wei et al., 2016) and additional expert-annotated data. It evaluates the performance of LLMs in identifying complex drug names. The prompt template is as follows.</p>
<p>True/False questions offer a straightforward yet effective means of evaluating a model's basic comprehension and ability to verify factual accuracy. They require models to provide a binary True/False response to a certain statement based on their understanding of the information or their own knowledge.</p>
<h1>Example</h1>
<p>Input: Does "CNS(-O)(-O)CC1-CC2-C(CC-C1)NC-C2CCN(C)C" appear in the document?
Output: Yes (True)
Metric: Accuracy</p>
<h2>Multiple Choice</h2>
<p>Multiple Choice questions assess a model's ability to select the correct answer from a set of options, testing its knowledge and reasoning. These questions range from basic factual inquiries to more complex scenarios requiring understanding and analyzing information.</p>
<h2>Example</h2>
<p>Input: A 0.217 g sample of HgO (molar mass $=217 \mathrm{~g}$ ) reacts with excess iodide ions according to the reaction shown above. Titration of the resulting solution requires how many mL of 0.10 M HCl to reach equivalence point? A. 1mL B. 10mL C. 20mL D. 50mL
Output: 0
Metric: Accuracy</p>
<h2>Table Extraction</h2>
<p>Table Extraction tasks are designed to evaluate a model's proficiency in extracting, summarizing and structuring data from understanding and memorizing given article. Model is usually given an article and is required to collect certain information from it, and presented in the form of a table.</p>
<h2>Example</h2>
<p>Input: "Given an article" While reading this paper, please summarize a complete list of the names and abbreviations of all solutions.
Output: (The original return is in CSV format, which is converted to intuitive display here)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Compound</th>
<th style="text-align: center;">Solvent</th>
<th style="text-align: center;">Solution</th>
<th style="text-align: center;">aFC*</th>
<th style="text-align: center;">aFEMC</th>
<th style="text-align: center;">aFC/aFEMC</th>
<th style="text-align: center;">XFC/XFEMC*</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LiPPI</td>
<td style="text-align: center;">FEC:FEMC</td>
<td style="text-align: center;">1.04:04 LiPPI:FEC:FEMC</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">1.42</td>
</tr>
<tr>
<td style="text-align: center;">LiPPI</td>
<td style="text-align: center;">DFEC:FEMC</td>
<td style="text-align: center;">1.04:04 LiPPI:DFEC:FEMC</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.23</td>
</tr>
</tbody>
</table>
<p>Metric: Table Recall (The results have not been in place, which is a natural sense) nor energy cost value in the differential value applied in in the output table.)</p>
<h2>Text Extraction</h2>
<p>Text Extraction tasks are designed to evaluate a model's ability to extract information from the text. The information can be entities or triplets (entityrelationship or event).</p>
<h2>Example (Chemical Entities Recognition)</h2>
<p>Input: unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms $/ \mathrm{kg}$, was inhibited or reversed by nalozone, 0.2 to $2 \mathrm{mg} / \mathrm{kg}$. The hypotensive effect of $100 \mathrm{mg} / \mathrm{kg}$ alpha-methyldopa was also partially reversed by nalozone. As naloxone and clonidine do not appear to interact with the same receptor site, the observed functional antagonism suggests the release of an endogenous opiate by clonidine or alpha-methyldopa and the possible role of the opiate in the central control of sympathetic tone.
Output: (naloxone), (clonidine), (nalozone), (alpha-methyldopa)
Metric: F1-score</p>
<h2>Molecule Generation</h2>
<p>Molecule Generation tasks are designed to evaluate a model's ability to generate molecules in SMILES format.
Example (Tag to Molecule)
Input: What's the SMILES formula of molecule "Sumatriptan"?
Output: CNS(-O)(-O)CC1-CC2-C(C-C1)NC-C2CCN(C)C
Metric: Molecular Similarity (Molecule similarity is measured by the Tanimoto similarity between two molecules' Morgan fingerprints)</p>
<p>Figure 5: Question types.</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Biomedical. I'll give you the abstract of literature. Please use chain of thought reasoning to identify all the compound entities in the abstract. First, analyze the abstract step by step, explaining your reasoning for identifying each compound entity. Then, provide a final list of the compound entities you recognized in the format: (compound 1), (compound 2), (compound 3).
Format:
Reasoning: [Reasoning]
Answer: [List of identified compounds]</p>
<h2>User Message:</h2>
<p>In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms $/ \mathrm{kg}$, was inhibited or reversed by nalozone, 0.2 to $2 \mathrm{mg} / \mathrm{kg}$. The hypotensive effect of $100 \mathrm{mg} / \mathrm{kg}$ alpha-methyldopa was also partially reversed by naloxone. Naloxone alone did not affect either blood pressure or heart rate. In brain membranes from spontaneously hypertensive rats clonidine, $10(-8)$ to $10(-5) \mathrm{M}$, did not influence stereoselective binding of [3H]-naloxone ( 8 nM ), and naloxone, $10(-8)$ to $10(-4) \mathrm{M}$, did not influence clonidine-suppressible binding of [3H]-dihydroergocryptine ( 1 nM ). These findings indicate that in spontaneously hypertensive rats the effects of central alpha-adrenoceptor stimulation involve activation of opiate receptors. <rest of the input>.</p>
<h2>Expected Answer:</h2>
<p>(naloxone), (clonidine), (nalozone), (alpha-methyldopa)</p>
<h3>5.1.4 Compound Disease Recognition</h3>
<p>Proposed in B5CDR (Wei et al., 2016), this task evaluates the capability of LLMs to identify and understand associations between compounds and diseases. Examples of process text:</p>
<h2>Example Paragraph</h2>
<p>Twenty children with acute lymphoblastic leukemia who developed meningeal disease were treated with a high-dose intravenous methotrexate regimen that was designed to achieve and maintain CSF methotrexate concentrations of $10(-5) \mathrm{mol} / \mathrm{L}$ without the need for concomitant intrathecal dosing. The methotrexate was administered as a loading dose of 6,000 $\mathrm{mg} / \mathrm{m} 2$ for a period of one hour followed by an infusion of $1,200 \mathrm{mg} / \mathrm{m} 2 / \mathrm{h}$ for 23 hours. Leucovorin rescue was initiated 12 hours after the end of the infusion with a loading dose of $200 \mathrm{mg} / \mathrm{m} 2$ followed by $12 \mathrm{mg} / \mathrm{m} 2$ every three hours for six doses and then every six hours until the plasma methotrexate level decreased to less than $1 \times 10(-7) \mathrm{mol} / \mathrm{L}$. The mean steady-state plasma and CSF methotrexate concentrations achieved were $1.1 \times 10(-3)$ $\mathrm{mol} / \mathrm{L}$ and $3.6 \times 10(-5) \mathrm{mol} / \mathrm{L}$, respectively. <rest of the paragraph>.</p>
<p>We then prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are a biologist AI. I'll give you the abstract of literature. Please use chain of thought reasoning to identify all the (compound, disease) relations in the abstract. First, analyze the abstract step by step, explaining your reasoning for identifying each relation. Then, provide a final list of the relations in the format: '(compound 1, disease 1),(compound 2, disease 2),(compound 3, disease 3),...', without adding any additional comments or explanations.
Format:
Reasoning: [Reasoning]
Answer: [List of recognized relations]</p>
<h2>User Message:</h2>
<p>[processed text]</p>
<h2>Expected Answer:</h2>
<p>(methotrexate, transient hemiparesis), (methotrexate, neutropenia), (methotrexate, seizures), (methotrexate, mucositis)</p>
<h2>C.1.5 Disease Entities Recognition</h2>
<p>Similarly, this task involves recognizing disease entity names using data from (Wei et al., 2016) and additional expert-annotated data, evaluating the performance of LLMs in identifying specialized disease names:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Biomedical. You are a biologist AI. I'll give you the abstract of literature. Please use chain of thought reasoning to identify all the disease entities in the abstract. First, analyze the abstract step by step, explaining your reasoning for identifying each disease entity. Then, provide a final list of the disease entities you recognized in the format: (disease 1), (disease 2), (disease 3).
Format:
Reasoning: [Reasoning]
Answer: [List of recognized diseases]</p>
<h2>User Message:</h2>
<p>In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms $/ \mathrm{kg}$, was inhibited or reversed by nalozone, 0.2 to $2 \mathrm{mg} / \mathrm{kg}$. The hypotensive effect of $100 \mathrm{mg} / \mathrm{kg}$ alpha-methyldopa was also partially reversed by naloxone. Naloxone alone did not affect either blood pressure or heart rate. In brain membranes from spontaneously hypertensive rats clonidine, 10(-8) to 10(-5) M, did not influence stereoselective binding of [3H]-naloxone ( 8 nM ), and naloxone, 10(-8) to 10(-4) M, did not influence clonidine-suppressible binding of [3H]-dihydroergocryptine (1 nM ). These findings indicate that in spontaneously hypertensive rats the effects of central alpha-adrenoceptor stimulation involve activation of opiate receptors. <rest of the input>.
Expected Answer:
(hypotensive), (hypotensive)</p>
<h2>C.1.6 Gene Disease Function</h2>
<p>The Gene Disease Text Mining task focuses on "Gene-Disease" association semantics text mining. It evaluates the ability of models to extract and understand relationships between genes and diseases from scientific literature, with a focus on identifying gene and disease entities (Ouyang et al., 2022). Examples of process text:</p>
<h2>Example Paragraph</h2>
<p>A novel frameshift mutation ( +G ) at codons 15/16 in a beta 0 thalassaemia gene results in a significant reduction of beta globin mRNA values. AIMS: To identify a novel beta globin gene mutation found in a Chinese family, and also to assess its functional consequences.
METHODS: Haematological analysis was performed on all family members. The 23 common mutations of beta thalassaemia found in Chinese populations were detected by means of a reverse dot blot method. Direct DNA sequencing of polymerase chain reaction (PCR) amplified complete beta globin gene was carried out to identify the novel mutation. A real time, one step reverse transcription PCR assay was used to measure beta globin mRNA in the reticulocytes of heterozygous patients.
RESULTS: A novel frameshift mutation-an insertion of G between codons 15 and 16 in a homonucleotide run of four guanines-was determined, which generates a new premature chain terminator at the 22nd codon. Relative quantitative analysis of the beta globin mRNA in heterozygous subjects demonstrated a $39.83 \%$ reduction compared normal controls.
CONCLUSIONS: The significantly lower amounts of beta globin mRNA found in mutation carriers is probably caused by the rapid nonsense mediated degradation of the mutant mRNA. These data, combined with haematological analysis, suggest that this novel mutation of CDs $15 / 16(+\mathrm{G})$ results in a beta(0) thalassaemia phenotype.</p>
<p>For extracting triplets (entities, semantic roles, entities), we prompt the model with:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Biomedical. In this semantic role regionition task, you need to follow 3 steps, and finally just return me triples that needed. First, you need to identify the entities in the text. Entities can be classified into 2 categories-molecular, and trigger word. 'Molecular' includes disease, gene, protein, and enzyme. 'Trigger word' includes:
1)Variation(Var), which means DNA, RNA, and mutations in proteins and changes in molecular structure, e.g. 'mutations on the Arg248 and Arg282', 'mutant R282W', 'missense mutations';
2)Molecular Physiological Activity (MPA), including molecular activity, gene expression and molecular physiological activity, e.g. 'phosphorylation', 'transcription', 'histone methylation', 'bioactivation of cyclophosphamide'; 3)Interaction, molecule-to-molecule or molecule-to-cell connections, e.g. 'bind', 'interaction';
4)Pathway, e.g. 'Bmp pathway';'PI3K pathway';
5)Cell Physiological Activity (CPA), Activities at or above the cellular level, including cellular reactivity and cell or organ development and growth, e.g. 'T helper cell responses', 'renal development';
6)Regulation (Reg), a neutral cue word or phrase meaning no loss or gain, e.g. 'resolved in', 'regulated';
7)Positive Regulation (PosReg), a cue word or phrase that indicates the acquisition of a function, e.g. 'facilitates', 'enhanced', 'increased';
8)Negative Regulation (NegReg), a clue word or phrase that indicates a loss of function, e.g. 'suppressed', 'decreased', 'inhibited'.
Second, you need to identify the semantic role labeling objects, including 'ThemeOf' (from the main thing entity to the current entity) and 'CauseOf' (From the current entity to the Cause entity).
Third, please give me triples that contain entities and semantic role labeling objects(ThemeOf or Causeof).
Use chain of thought reasoning to explain your process of identifying the entities and relations, and then provide the final triples in the format: (...),
(...)</p>
<p>Format:
Reasoning: [Reasoning]
Answer: [List of recognized triples]</p>
<h2>User Message:</h2>
<p>[processed text]</p>
<h2>Expected Answer:</h2>
<p>(frameshift, CauseOf, reduction), (caused by, CauseOf, lower), (mutation, CauseOf, results in), (beta(0) thalassaemia, ThemeOf, results in), (beta globin mRNA, ThemeOf, reduction), (beta0 thalassaemia gene, ThemeOf, frameshift), (insertion, CauseOf, generates), (premature chain terminator, ThemeOf, generates), (amounts of beta globin mRNA, ThemeOf, lower), (mutation, CauseOf, caused by), (degradation, ThemeOf, caused by).</p>
<h2>C. 2 Chemistry</h2>
<h2>C.2.1 MMLU-Pro-Chemistry</h2>
<p>The example of MMLU-Pro-Chemistry is similar to the MMLU-Pro-Biology task in Appendix C.1.1.</p>
<h2>C.2.2 Electrolyte Table QA</h2>
<p>The composition and properties of organic electrolytes are crucial for battery performance, stability, and safety. To evaluate the model's retrieval capabilities regarding electrolyte information, we posed multiple-choice questions about the components of solution systems and the dissolution reactions, focusing on their physical and chemical properties as presented in the tables within the articles. We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the electrolytes field. Please answer the following multiple choice question correctly. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx). Format:
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>In the upper paper, what are the minimum and maximum intramolecular distances (nm) of dimethyl carbonate?
a) $0.41 / 0.87$
b) $0.49 / 0.67$
c) $0.25 / 0.25$
d) $0.25 / 0.38$</p>
<h2>Expected Answer:</h2>
<p>a) $0.41 / 0.87$</p>
<h2>C.2.3 OLED Property Extraction</h2>
<p>This task evaluates the LLM's ability to extract information about OLED molecules and their optical properties. It tests several key capabilities, including their understanding of complex and domainspecific language and their ability to interpret and extract data from tables. An example output is shown in Table 5. We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of organic photovoltaics. Please give a complete list of Host, Host's SMILES structure (if exists), Dopant, Assistant Dopant (if exists), $\mathrm{Td} / \mathrm{Tg} / \mathrm{ET}$, Von,max EQE/CE/PE,EQE/CE/PE, and CIE [x, y] * Output in csv format with columns of those attributes, do not write units only the value like " 10.5 ".</p>
<ul>
<li>Quote the column name or Host's Name or Dopant's Name if it contains space or special characters like ".".</li>
<li>If there are multiple tables, concat them. Don't give me reference or using "...", give me complete table!</li>
<li>Should return all columns mentioned, if empty just return 'NaN'. "Host" and "Dopant" should not be empty.</li>
<li>"Host" and "Dopant" should be short name of the organic molecule.</li>
<li>Should find more information from the whole content, including tables, text.
for example, you should return:
" cvv
Host,SMILES,Dopant,Td /Tg /ET,
Von,max EQE/CE/PE,EQE/CE/PE,"CIE"
PPO1,O=Ptc1ccccc1)tclccccc1)c1ccccc1,FCNIt,-/74/3.02,--
17.1/20.5/14.3,-/-/-,"(0.14, 0.16)"</li>
</ul>
<p>PPO2,O=Ptc1ccccc1)tclccccc1)c1ccccc1,FCNIt,-/123/3.02,--
18.4/21.1/16.6,-/-/-,"(0.14, 0.15)"</p>
<p>Please use a step-by-step approach to analyze the content and ensure that all relevant information is accurately extracted. Only provide reasoning for how you identified each attribute and output the final csv format.
Format:
Reasoning: [Reasoning]
Answer: [Extracted csv]</p>
<h2>User Message:</h2>
<p>[ document.pdf ]</p>
<h2>C.2.4 Polymer Chart QA</h2>
<p>The processing steps and properties of polymer materials are often represented through charts. Extracting information from these charts and integrating it with textual data is crucial. To further assess the retrieval capabilities of models concerning polymer chart information, we designed multiplechoice questions involving polymer composition, processing techniques, and properties.</p>
<p>The example of Polymer Chart QA can be found in a similar format to the Biology Chart QA task in Appendix C.1.2.</p>
<h2>C.2.5 Polymer Composition QA</h2>
<p>This task involves extracting the blend ratio of donor to acceptor in the most efficient solar cell from the text of scientific literature.</p>
<p>We prompt the model with the following:</p>
<p>Table 5: OLED Property example.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Host</th>
<th style="text-align: center;">Depant</th>
<th style="text-align: center;">Tcl [ ${ }^{\circ} \mathrm{C}$ ] / Tg [ ${ }^{\circ} \mathrm{C}$ ] / ET [eV]</th>
<th style="text-align: center;">Von [V]</th>
<th style="text-align: center;">max EQE [\%] / CE [cd A-1] / PE [lm W-1]</th>
<th style="text-align: center;">EQE [\%] / CE [cd A-1] / PE [lm W-1]</th>
<th style="text-align: center;">CIE [x, y]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PPO1</td>
<td style="text-align: center;">FCNIr</td>
<td style="text-align: center;">- / 74 / 3.02</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">17.1 / 20.5 / 14.3</td>
<td style="text-align: center;">$-/--$</td>
<td style="text-align: center;">$(0.14,0.16)$</td>
</tr>
<tr>
<td style="text-align: center;">PPO2</td>
<td style="text-align: center;">FCNIr</td>
<td style="text-align: center;">- /123 / 3.02</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">18.4 / 21.1 / 16.6</td>
<td style="text-align: center;">$-/--$</td>
<td style="text-align: center;">$(0.14,0.15)$</td>
</tr>
<tr>
<td style="text-align: center;">mCPPO1</td>
<td style="text-align: center;">FCNIryic</td>
<td style="text-align: center;">$--1 / 3.00$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">25.1 / - / 29.8</td>
<td style="text-align: center;">23.1 / 28.9 / 15.1</td>
<td style="text-align: center;">$(0.14,0.18)$</td>
</tr>
<tr>
<td style="text-align: center;">CDPO</td>
<td style="text-align: center;">5CeCN</td>
<td style="text-align: center;">455 / 89 / 2.84</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">13.2 / 31.6 / 18.1</td>
<td style="text-align: center;">$-/--$</td>
<td style="text-align: center;">$(0.20,0.38)$</td>
</tr>
</tbody>
</table>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of polymer solar cells researcher who answers the following multiple choice question correctly. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx).
Format:
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>In this paper, What is the blend ratio of donor to acceptor in the most efficient solar cell?
a) 1:4
b) 20:8
c) $30: 50$
d) 2:4</p>
<h2>Expected Answer:</h2>
<p>a) $1: 4$</p>
<h2>C.2.6 Polymer Property Extraction</h2>
<p>This task focuses on extracting vital values such as power conversion efficiency (PCE) and opencircuit voltage ( $V_{\mathrm{OC}}$ ) from tables within the literature.</p>
<p>We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of polymer solar cells researcher.
Please give a complete list of Nickname, PCE_max, PCE_ave, Voc , Jsc, $\mathrm{FF} ; *$ Output in csv format with columns of those attribution, do not write units only the value like " $10.5^{\prime}$.
* If there are multiple tables, concat them. Don't give me reference or using "..." give me complete table!
* Should return all columns mentioned, if empty just return ' NaN '. Nickname should not be empty.
* Nickname should be short name of polymers, for example: 'PCBM:PfIBT4T-2OD:PC61PM' should return 'PfIBT4T-2OD'.
* Only return acceptor 'PC71BM' related records.
* If with different experiment settings for the same nickname, only return the record with 'highest PCE ' !
* Should find more information from the whole content, including tables, text.
* For FF use 0.xx instead of $x x . x$, for example: 63.0 should return 0.63 ! for example, you should return:
"csv
Nickname,PCE_max(\%),PCE_ave(\%),Voc (V),Jsc (mA $\mathrm{cm})^{\mathrm{E}}$ ),FF
PBTTT-C14,2.34,2.34,0.53,9.37,0.48
* Please use a step-by-step approach to analyze the content and ensure that all relevant information is accurately extracted. Only provide reasoning for how you identified each attribute and output the final csv format.
Format:
Reasoning: [Reasoning]
Answer: [Extracted csv]</p>
<h2>User Message:</h2>
<p>[ document.pdf ]</p>
<h2>C.2.7 Solubility Extraction</h2>
<p>Organic electrolytes, extensively used in battery technologies, comprise organic solvents, lithium
salts, and additives. Understanding solubility in organic electrolytes is crucial as it impacts the efficiency of electrolytic processes, product selectivity, and equipment design. This task evaluates the LLM's capability in retrieving solubility-related tables. Papers typically select data from various aspects to describe the system, making it challenging to combine multiple tables for fuzzy matching. Therefore, we focus on examining the LLM's semantic understanding ability, enabling the model to select the most relevant and comprehensive table related to "solubility" from numerous alternatives and convert it into the specified format.</p>
<p>We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of chemistry and specialize in the study of solubility. Now you are required to extract tables related to solubility from the article. The extracted information includes solute name, solvent name, temperature, pressure and solubility. Since these properties are temperaturedependent and pressure-dependent, please place the properties at different temperatures or pressure on different rows. The values of temperature and solubility should be output together with their unit. Output the whole table in csv format and satisfy these requirements:
(1) Do not truncate tables using "..." Always output the complete tables.
(2) Keep all the superscripts in the form like " ${ }^{<em>} 3$ ", " ${ }^{</em>}$ " or " ${ }^{\circ} \mathrm{a}$ ".
(3) Do not use " NaN " to replace the blank cells, just leave it empty.
(4) Use " $x$ " to replace all " $x$ ", Use " $(i)$ " to replace all " ( $)$ "
(5) Always add space before and after operators like " $\pm$ " As a example, the csv should be like:
" ${ }^{\circ} \mathrm{cm}$
solute_name,solvent_name,temperature,pressure,solubility
FLBDOB,PC,298.2 K,1 atm,0.275 $\pm 0.1 \mathrm{~mol} / \mathrm{L}$
Please use a step-by-step approach to analyze the content and ensure that all relevant information is accurately extracted. Only provide reasoning for how you identified each attribute and output the final csv format. Format:
Reasoning: [Reasoning]
Answer: [Extracted csv]</p>
<h2>User Message:</h2>
<p>[ document.pdf ]</p>
<h2>C.2.8 Reactant QA</h2>
<p>Organic and bio-catalyzed synthetic reactions are vital for the manufacture of drug-like molecules. Therefore, we designed a complex task to test the model's capability in extracting information from schematic diagrams and texts of chemical reactions. The model is required to understand the charts specified in the articles and select the correct answer from the provided multiple-choice descriptions.</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of organic chemistry. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx).
Format:
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>Which compound is in the reactants or reagents of the following reaction?
a) $c 4 \mathrm{ccc}\left(\mathrm{B} 3 \mathrm{OB}(\mathrm{c} 1 \mathrm{ccccc} 1) \mathrm{OB}(\mathrm{c} 2 \mathrm{ccccc} 2) \mathrm{O}\right) \mathrm{cc} 4$
b) $\mathrm{O}=\mathrm{C}(\mathrm{C}(\mathrm{C}) \mathrm{C}(\mathrm{OC})=\mathrm{O}(\mathrm{C} 1 \mathrm{CC})$
c) $\mathrm{CC}(=\mathrm{O}) \mathrm{OP}(=\mathrm{O})(\mathrm{O}-[\mathrm{(O}-]<a href="\mathrm{NH} 4+$">\mathrm{NH} 4+</a>
d) $\mathrm{COC}(=\mathrm{O}) \mathrm{C}(\mathrm{C})=\mathrm{C}\left(\mathrm{OS}(=\mathrm{O})(\mathrm{=O}) \mathrm{c} 1 \mathrm{ccc}(\mathrm{C}) \mathrm{cc} 1\right) \mathrm{C} 1 \mathrm{CC} 1$</p>
<p>The new reaction you should deal with is the second step in the first reaction in Section "2.2 Procedures".</p>
<h2>Expected Answer:</h2>
<p>b) $\mathrm{O}=\mathrm{C}(\mathrm{C}(\mathrm{C}) \mathrm{C}(\mathrm{OC})=\mathrm{O}) \mathrm{C} 1 \mathrm{CC} 1$</p>
<h2>C.2.9 Reaction Mechanism QA</h2>
<p>Investigating electrolyte reactions helps improve the solid electrolyte interphase (SEI) layer, which directly affects battery performance and lifespan. Studies in this area lead to the development of advanced electrolytes that enhance a robust SEI, resulting in more efficient and durable batteries. We design a complex task to test the capability of extracting information from schematic diagrams of chemical reaction mechanisms. LLM is required to understand the specified reaction diagram and select the correct answer from the provided multiple choices.</p>
<p>We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are a highly intelligent organic electrolyte researcher who answers the following multiple choice question correctly. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx).
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>According to figure 1, which one of these synthetic routes for LTFOP is correct?
a) DTMSO + LiPF6 -&gt; LTFOP $+2 \mathrm{CH} 3) 3 \mathrm{SiF}$
b) 2 DTMSO + LiPF6 -&gt; LTFOP $+4 \mathrm{CH} 3) 3 \mathrm{SiF}$
c) $\mathrm{HOOCCOOH}+2 / 3 \mathrm{CH} 3) 3 \mathrm{SiCl}+2 / 3 \mathrm{CH} 3) 3 \mathrm{SiNH}_{3} \mathrm{SiCH} 3) \rightarrow$ LTFOP $+$ $2 / 3 \mathrm{NH} 4 \mathrm{Cl}$
d) DTMSO + LiPCl6 -&gt; LTFOP $+2 \mathrm{CH} 3) 3 \mathrm{SiCl}$</p>
<h2>Expected Answer:</h2>
<p>a) DTMSO + LiPF6 -&gt; LTFOP $+2 \mathrm{CH} 3) 3 \mathrm{SiF}$</p>
<h2>C. 3 Material</h2>
<h2>C.3.1 Material QA</h2>
<p>The example of Material QA is similar to the MMLU-Pro-Biology task in Appendix C.1.1.</p>
<h2>C.3.2 Alloy Chart QA</h2>
<p>The processing steps and properties of alloy materials are often presented in charts, such as those</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Alloy</th>
<th style="text-align: center;">Composition</th>
<th style="text-align: center;">Composition</th>
<th style="text-align: center;">Composition</th>
<th style="text-align: center;">Composition</th>
<th style="text-align: center;">Composition</th>
<th style="text-align: center;">Composition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">Cr</td>
<td style="text-align: center;">Cu</td>
<td style="text-align: center;">Fe</td>
<td style="text-align: center;">Mn</td>
<td style="text-align: center;">Mo</td>
</tr>
<tr>
<td style="text-align: center;">LeanDSS</td>
<td style="text-align: center;">$0.014 \%$</td>
<td style="text-align: center;">20.85 \%</td>
<td style="text-align: center;">$0.09 \%$</td>
<td style="text-align: center;">$75.38 \%$</td>
<td style="text-align: center;">$1.49 \%$</td>
<td style="text-align: center;">$0.30 \%$</td>
</tr>
<tr>
<td style="text-align: center;">StandardDSS</td>
<td style="text-align: center;">$0.012 \%$</td>
<td style="text-align: center;">$22.46 \%$</td>
<td style="text-align: center;">$0.17 \%$</td>
<td style="text-align: center;">$69.94 \%$</td>
<td style="text-align: center;">$1.81 \%$</td>
<td style="text-align: center;">$3.07 \%$</td>
</tr>
<tr>
<td style="text-align: center;">SuperDSS</td>
<td style="text-align: center;">$0.013 \%$</td>
<td style="text-align: center;">$24.98 \%$</td>
<td style="text-align: center;">$0.20 \%$</td>
<td style="text-align: center;">$63.41 \%$</td>
<td style="text-align: center;">$0.48 \%$</td>
<td style="text-align: center;">$4.03 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Alloy composition example.
comparing the performance of multiple alloys or illustrating how elongation changes with composition. Therefore, extracting information from these charts and integrating it with textual information is crucial. To further evaluate the retrieval capability of models regarding alloy chart information, we have designed multiple-choice questions involving alloy composition, processing techniques, and properties.</p>
<p>The example of Alloy Chart QA is similar to the Biology Chart QA task in Appendix C.1.2.</p>
<h2>C.3.3 Composition Extraction</h2>
<p>Extracting alloy composition information from an article's text or tables and unifying it into a structured format helps researchers utilize historical data more effectively and provides valuable guidance for subsequent designs. This comprehensive task evaluates LLMs' ability to extract alloy compositions (including all element contents) from text and tables. Typically, alloy element content is found in two cases: (1) the element content is stored in a table, and (2) the element content is implicitly indicated by the alloy name, such as 'Fe30Co20Ni50', which represents an atomic ratio of $30 \% \mathrm{Fe}, 20 \%$ Co , and $50 \% \mathrm{Ni}$. The objective of this task is to comprehensively extract this information and organize it into a digestible table. The metric is to calculate the matching score between the standard answer table and the extraction result table. This task showcases the LLM's comprehension ability to integrate, extract, and structure multi-modal information (Kim et al., 2021).</p>
<p>An alloy composition table example is shown as following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Alloy Materials. Please give a complete list of alloy names and compositions of all alloys in this paper.
If there is no alloy composition element ratio in the text, try to extract the element ratio from the alloy name from the perspective of alloy experts.
Output in cvr format with multiindex ( 2 headers), The names in first header are 'AlloyName' and 'Composition' forcely. The names in second header are element names of alloy.
Starting on the third row, list the alloy names and their corresponding element content. Based on the number of reference commas, the element name corresponds to the content.
Please write units not in header but in value like " $50 \mathrm{wt} . \%$ ", " $30 \mathrm{at} . \%$ ". Output the data strictly in the CSV format shown below and exclude any other content. Example format:
"csv
AlloyName,Composition,Composition,Composition
nan,Fe,Co,Al
Fe70Co15Al3,70 wt. $\%, 15$ wt. $\%, 3$ wt. $\%$
Fe70Co18,70 wt. $\%, 18$ wt. $\%$,nan
Please use a step-by-step approach to analyze the content and ensure that all relevant information is accurately extracted. Only provide reasoning for how you identified each attribute and output the final cvr format.
Format:
Reasoning: [Reasoning]
Answer: [Extracted csv]</p>
<h2>User Message:</h2>
<p>[ docoment.pdf ]</p>
<h2>C.3.4 Temperature QA</h2>
<p>The properties of an alloy are determined by its composition and the processes it undergoes, including processing and heat treatment. Therefore, extracting heat treatment values is critical. This task aims to determine the maximum temperature value for the heat treatment of the alloy. To ensure easy statistical analysis, questions are designed as multiple-choice. Examples of process paragraphs (Villa et al., 2020):</p>
<h2>Example Paragraph</h2>
<p>Cast NiMnGa samples, of $\mathrm{N}(50 \mathrm{Mn} 30 \mathrm{Ga} 20$ nominal composition, were prepared by 5 an; melting cycles of the pure elements (electrolytic Ni $99.97 \%$, electrolytic Mn $99.5 \%$ and Ga $99.99 \%$ ) in stoichiometric ratio, in a non-consumable electrode furnace (Leybold LK6/45) (Leybold, Cologne, Germany). The as-cast ingot was ground to powder in a planetary ball mill (Fritsch Pulverisette 4) (Fritschlilar-Oberstein, Germany) and the powder size was selected by means of sieves. Densified pellets were produced by die-pressing alloy powders with different average sizes (lower than 50 um or between 50 and 100 um ) at 0.75 GPa at room temperature and sintered by thermal treatment at $925^{\circ} \mathrm{C}$ for 24,72 , and 168 h in an Ar atmosphere, followed by slow cooling in the furnace. Sintered pellets had the following dimensions: approximately 3 mm in height and 13 mm in diameter. Table 1 provides a summary of the prepared sintered samples.</p>
<p>We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Alloy Materials. You are a highly intelligent alloy researcher who answers the following multiple choice question correctly. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx).
Format:
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>In the upper paper, what is the maximum temperature of the heat treatment process for all alloys?
a) 925 C
b) 650 C
c) 700 C
d) 800 C</p>
<h2>Expected Answer:</h2>
<p>a) 925 C</p>
<h2>C.3.5 Sample Differentiation</h2>
<p>Alloys with the same composition but treated by different processes are considered different samples because they exhibit different properties. Therefore, distinguishing between different samples and understanding the differences in their processes is essential. This multiple-choice question task is designed to comprehensively judge the number of different alloy samples proposed or studied by the authors. It assesses the LLMs' analysis and reasoning abilities regarding alloy distinctions from text.</p>
<p>The following example is process paragraphs where the sample are treated by different processes (Hernández-Rivera et al., 2017):</p>
<h2>Example Paragraph</h2>
<p>An induction furnace was used to produce the $\mathrm{Zn}-21 \mathrm{A} 1-2 \mathrm{Cu}$ alloy by melting proper amounts of Zn ( $99.99 \%$ ), Al ( $99.99 \%$ ), and Cu ( $99.96 \%$ ). The alloy was melted in a graphite crucible exposed to air and poured into cylindrical bars of 19 mm in diameter and 35 mm in length. After that, some bars were homogenized at $350^{\circ} \mathrm{C}$ for 24 h in the air. Cast and homogenized samples were subjected to an equal channel angular extrusion(ECAP) in a die with two cylindrical channels with a diameter of 15.8 mm . The inner intersecting angle $(\gamma)$ was 90 and the outer angle $(\gamma)$ was $36^{\circ}$. All samples were extruded by two and sixpasses with a rans velocity of 5 mm/min and by using B. route. The lubricant used was MoS , and it was applied to both channels on each pass.</p>
<p>We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Alloy Materials. Please answer the following multiple choice question correctly. Use chain of thought reasoning and provide reasoning before selecting the correct answer (e.g., a)xxx, or b)xxx). Format:
Reasoning: [Reasoning]
Answer: [Answer]</p>
<h2>User Message:</h2>
<p>Materials with the same components but processed through different techniques are considered as different alloys because they possess distinct properties. In the upper paper, please provide a count of all the alloys proposed and discussed by the authors?
a) 2
b) 0
c) 3
d) 1</p>
<h2>Expected Answer:</h2>
<p>d) 1</p>
<h2>C.3.6 Treatment Sequence</h2>
<p>Each alloy treatment process has a clear sequence requirement, so it is necessary to ensure that the extracted heat treatment process sequence is consistent with the experimental sequence. For example, after solution treatment, a sample is further aged to ensure the release of internal stresses. This task aims to objectively analyze and evaluate the sequential relationship between two heat treatments and provide True/False answers. Additionally, if a specific heat treatment name does not exist in the paper, it should be considered False. This task assesses the LLM's comprehension ability to judge treatment order from the text.</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of Alloy Materials. You are a specialist in the domain of heat treatment processes, such as homogenization, annealing, aging, solution treatment, quenching, and tempering, among others. Use chain of thought reasoning to analyze the question step by step. After your reasoning, answer the question with 'Yes' or 'No'.
Format:
Reasoning: [Reasoning]
Answer: [Yes/No]</p>
<h2>User Message:</h2>
<p>In the upper paper, is the processing heat treatment technique before the thermal treatment at 925 C called arc melting?</p>
<h2>Expected Answer:</h2>
<p>Yes</p>
<h2>C. 4 Medicine</h2>
<h2>C.4.1 MMLU-Pro-Health</h2>
<p>The example of MMLU-Pro-Health is similar to the MMLU-Pro-Biology task in Appendix C.1.1.</p>
<h2>C.4.2 Affinity Extraction</h2>
<p>This task evaluates the LLM's ability to extract an affinity table containing molecules' tags, SMILES, and their affinities to different targets in bioassays.</p>
<p>It tests several key capabilities of LLMs, including understanding complex and domain-specific language, as well as molecules and tables. Affinity data extraction requires not just surface-level text processing but also a deeper analysis to match different modalities.</p>
<p>An example output is shown in Table 7.
We prompt the model with the following:</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of pharmaceutical chemistry, and your task is to summarize the results of activity assays from an article in a tabular format. Please follow these steps to complete the task:</p>
<ol>
<li>Determine if the article includes an activity assay. If it does, locate the section(s) presenting the assay results, which are usually in one or more tables.</li>
<li>Compile all the activity assay results into a single table. You may use multiple columns to represent different conditions or outcomes of various experiments.</li>
<li>Identify the names or codes used in the table, such as Example 1 or Compound A, and find the corresponding sections in the article that mention these substances. Extract the full name and SMILES notation of each substance.</li>
<li>Compile the names and SMILES notations of each substance in the table. Output in csv format with multiindex (Affinities, protein/cell line), write units not in header but in the value like " $10.5 \mu \mathrm{M}$ ". Quote the value if it has comma! For example:
""csv
Compound,Name,SMILES,Affinities,Affinities,Affinities,Affinities
,5HT1A (IC50),5HT1D (IC50),5HT-UT (IC50),5HT1E (<affinity type>) 5a,Aspirin,CC( $=$ O)Oc1ccccc1C( $=$ O)O,2.0 nM,8.0 nM,12.6 nM, $&gt;1000 \mathrm{nM}$ "</li>
<li>If there are multiple tables, concat them. Don't give me reference or using "..", give me complete table! Please use a step-by-step approach to analyze the content and ensure that all relevant information is accurately extracted. Only provide reasoning for how you identified each attribute and output the final csv format.
Format:
Reasoning: [Reasoning]
Answer: [Extracted csv]</li>
</ol>
<h2>User Message:</h2>
<p>[ document.pdf ]</p>
<h2>C.4.3 Drug Chart QA</h2>
<p>The analysis of drug properties, compositions, and processing techniques is critical for drug discovery and development. Often, this information is presented in charts, making it essential to extract and integrate such information with textual data. To further assess the retrieval capabilities of models in the context of drug chart information, we have designed multiple-choice questions focusing on drug composition, processing methods, and properties. The example of Drug Chart QA can be found in a similar format to the Biology Chart QA task in Appendix C.1.2.</p>
<h2>C.4.4 Tag to Molecule</h2>
<p>This task evaluates the model's ability to find the correct SMILES given its tag in a document. Typically, a molecule is shown with an chart of its structure and a tag below it. The LLM should recognize both the structure and the tag and understand their</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Compound</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">SMILES</th>
<th style="text-align: center;">Affinities</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cytotoxicity in 2.2.15 Cells (IC50)</td>
<td style="text-align: center;">Anti-HBV Activity in 2.2.15 Cells (EC50)</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$\mathrm{Cl}\left<a href="\mathrm{O}[\mathrm{C} \oplus \mathrm{H}">\mathrm{C} \oplus \mathrm{H}\right</a>[\mathrm{C} \oplus \mathrm{H}] 1 \mathrm{~F}$ ) N2C $\times \mathrm{NC} 3 \times \mathrm{C}(\mathrm{N} \times \mathrm{CN} \times \mathrm{C} 32) \mathrm{N} / \mathrm{CO}$</td>
<td style="text-align: center;">$&gt;200000 \mathrm{aM}$</td>
<td style="text-align: center;">$&gt;10000 \mathrm{aM}$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$\mathrm{Cl}\left<a href="\mathrm{O}[\mathrm{C} \oplus \mathrm{H}">\mathrm{C} \oplus \mathrm{H}\right</a>[\mathrm{C} \oplus \mathrm{H}] 1 \mathrm{~F}$ ) N2C $\times \mathrm{CC}(\mathrm{O} \times \mathrm{NC} 2 \times \mathrm{O}) \mathrm{N} / \mathrm{CO}$</td>
<td style="text-align: center;">$&gt;200000 \mathrm{aM}$</td>
<td style="text-align: center;">4000 aM</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$\mathrm{CC} 1 \times \mathrm{CN}(\mathrm{C}(x \mathrm{O}) \mathrm{NC} 1 \times \mathrm{O})[\mathrm{C} \oplus \mathrm{H}] 2 \mathrm{C}<a href="\mathrm{C} \oplus \mathrm{H}">\mathrm{C} \oplus \oplus \mathrm{H}</a>[\mathrm{O} 2] \mathrm{CO}(8) \times[\mathrm{N} \times] \times[\mathrm{N}-]$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>
<p>Table 7: Example output of affinity data extraction task
connection.</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of organic chemistry, can help user get SMILES formula from documents. Use chain of thought reasoning to analyze the document and extract the SMILES formula step by step. After reasoning, just give the SMILES formula as the final answer without further explanation. Format:
Reasoning: [Reasoning]
Answer: [Extracted SMILES]</p>
<h2>User Message:</h2>
<p>What's the SMILES formula of molecule "Sumatriptan"?
Expected Answer:
${ }^{+} \mathrm{CNS}(=\mathrm{O})(\mathrm{CO}) \times \mathrm{O} \times \mathrm{CC} 1=\mathrm{CC} 2 \times \mathrm{C}\left(\mathrm{C}=\mathrm{C} 1\right) \mathrm{NC} \times \mathrm{C} 2 \mathrm{CCN}(\mathrm{C}) \mathrm{C}^{+}$</p>
<h3>1.4.5 Markush to Molecule</h3>
<p>This task evaluates the model's ability to obtain the correct SMILES given a Markush formula (in CXSMILES pattern) and its substituents.</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of chemistry, can help user insert substituents into CXSMILES-type markush formula to get SMILES formula (removing Hs). Use chain of thought reasoning to explain how you insert the substituents step by step, ensuring the correct SMILES is generated. After reasoning, just reply with the SMILES formula without further explanation. Format:
Reasoning: [Reasoning]
Answer: [Generated SMILES]</p>
<h2>User Message:</h2>
<p>${ }^{+} \mathrm{Cl}^{+} \mid \mathrm{CC}\left({ }^{+} \mathrm{OC}^{+}\right.$: $\left.(\mathrm{A} ; ; P o l_{p} ; ; ; Q_{x} ; ; ; M_{p}\right), \mathrm{A}=\mathrm{H}, \mathrm{Pol}=\mathrm{NH} 2, \mathrm{Q}=\mathrm{OH}$, $\mathrm{M}=[\mathrm{Li}]$</p>
<h2>Expected Answer:</h2>
<p>${ }^{+} \mathrm{NCCC}(\mathrm{O}) \mathrm{CC}[\mathrm{Li}]^{+}$</p>
<h3>1.4.6 Molecule in Document</h3>
<p>This task evaluates the model's ability to determine whether a molecule (represented by SMILES) is mentioned in a document. The LLM should recognize all Markush formulas and their substituents, and then judge whether the required molecule is covered.</p>
<h2>Prompt</h2>
<h2>System Message:</h2>
<p>You are an expert in the field of chemistry. You are given a SMILES formula of a molecule, and should judge whether it is in the document. If the molecules are given by Markush formula (containing R group), You need to 1) analyze the skeletons of the provided molecule and the molecule in the literature or patent, and 2) if the compare the variable values of the molecular structure with the range of variable values given in the patent, to determine whether the molecule is covered by the literature or patent. Use chain of thought reasoning to analyze the question step by step. After your reasoning, answer the question with 'Yes' or 'No'.
Format:
Reasoning: [Reasoning]
Answer: [Yes/No]</p>
<h2>User Message:</h2>
<p>[ document.pdf ]
Does the molecule ${ }^{+} \mathrm{CC}(\mathrm{CCCCCCCC1}=\mathrm{CC}) \times \mathrm{C}(\mathrm{C}$ ( $\times \mathrm{C} 1) \mathrm{OC}) \mathrm{OC}) \mathrm{OC}) \mathrm{CCC}(\mathrm{C} 2 \times \mathrm{CC} \times \mathrm{CS} 2) \mathrm{O}^{+}$appear in the document?</p>
<h2>Expected Answer:</h2>
<p>Yes</p>
<h2>D Baseline LLMs</h2>
<p>We briefly introduce the baseline LLMs and endpoints that we have tested on SciAssess.</p>
<ul>
<li>OpenAI-o1 (OpenAI, 2024) OpenAI's o1 model is designed to reason through complex tasks and solve harder problems in science, coding, and math. The model we tested is OpenAI-o1-preview.</li>
<li>GPT-40 ${ }^{3}$ : OpenAI's GPT-4o advances humancomputer interaction by handling text, audio, image, and video inputs and outputs. It offers improved efficiency and cost compared to previous GPT models. The model we use is gpt-40.</li>
<li>GPT-4 (OpenAI, 2023): OpenAI's GPT-4 excels in text generation and comprehension, augmented with capabilities for image processing, code interpretation, and information retrieval. These features make it adept at handling the complexities of scientific texts, positioning it as a versatile tool for scientific research. The model we use is gpt-4-turbo.</li>
<li>GPT-3.5 ${ }^{4}$ : Preceding GPT-4, GPT-3.5 by OpenAI distinguishes itself with adept language processing skills, enabling effective engagement</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>with complex texts. The model we use is gpt-3.5-turbo-0125.</p>
<ul>
<li>Gemini-1.5-Pro (Google, 2023): Google DeepMind's Gemini model family excels in multimodal comprehension, integrating text, code, image, and audio analysis.</li>
<li>Claude 3 Opus ${ }^{5}$ : Claude 3 Opus model excels across major AI benchmarks, demonstrating nearhuman levels of comprehension and fluency in tasks like analysis, forecasting, and multilingual communication.</li>
<li>Moonshot-v1 ${ }^{6}$ : Moonshot-v1 is a text generation model proposed by Moonshot AI. We use moonshot-v1-128k in this study.</li>
<li>Doubao ${ }^{7}$ : Doubao is a set of LLMs developed by ByteDance. The model we use is Doubao-pro-128k.</li>
</ul>
<p>Apart from the closed-source LLMs, we also include some SOTA open-source LLMs:</p>
<ul>
<li>Llama-3.1-70B ${ }^{8}$ : Llama 3-70B is a leading opensource LLMs released by Meta.</li>
<li>Mixtral-8x22B (Jiang et al., 2024): Mixtral-8x22B-Instruct-v0.1 is the latest and largest mixture of experts large language model (LLM) from Mistral AI.</li>
<li>Qwen-2.5-72B (Bai et al., 2023): Qwen2 are series of LLMs developed by Alibaba. The model we test is Qwen2-72B-Instruct.</li>
</ul>
<h1>E Performance without Cot</h1>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://www.anthropic.com/news/ claude-3-family
${ }^{6}$ https://platform.moonshot.cn/docs/intro
${ }^{7}$ https://www.volcengine.com/product/doubao
${ }^{8}$ https://ai.meta.com/blog/meta-1lama-3/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-557 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-557</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-557</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-ac3ee98020251797c2b401e1389461df88e52e62</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ac3ee98020251797c2b401e1389461df88e52e62" target="_blank">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> These advanced recurrent units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU), are found to be comparable to LSTM.</p>
                <p><strong>Paper Abstract:</strong> In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e557.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e557.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRU → music & speech</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated Recurrent Unit applied to polyphonic music modeling and raw speech-signal modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The gated recurrent unit (GRU), originally proposed and used in neural machine translation, is applied here to sequence modeling tasks in polyphonic symbolic music and raw audio speech; the paper reports implementation details, a small modification to the candidate-activation formula, training protocol, and empirical outcomes comparing GRU to LSTM and tanh RNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the properties of neural machine translation: Encoder-decoder approaches</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Gated Recurrent Unit (GRU) application to new sequence domains</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A recurrent neural network unit that uses gating (reset and update gates) to adaptively control information flow and allow the unit to capture dependencies at different time scales. Activation is computed as a linear interpolation between previous activation and a candidate activation whose computation is gated by a reset gate; update and reset gates are sigmoidal functions of input and previous hidden state. In this paper GRUs are instantiated in RNNs trained to model sequences of binary vectors (polyphonic music) and raw 1-D audio samples (speech), with output layers chosen per domain (logistic sigmoid for binary music symbols, mixture-of-Gaussians for raw audio prediction). Training uses RMSProp, weight noise, gradient clipping and hyperparameter random search; model sizes were chosen so each unit type had approximately equal parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / experimental protocol (ML model adaptation & empirical evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Natural language processing / Neural machine translation (sequence-to-sequence NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Sequence modeling in music (symbolic polyphonic music) and raw speech-signal modeling (audio prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>1) Output layer and loss adapted to each target domain: logistic sigmoid outputs for multi-label binary vectors in polyphonic music; mixture-of-Gaussians (20 components) output for raw audio regression in speech. 2) Model sizes (number of units) were adjusted so GRU-based RNNs had roughly the same total number of parameters as LSTM and tanh baselines to enable fair comparison. 3) A small implementation-level change to the candidate activation formula was used (see separate entry), i.e. the placement/order of applying the reset gate relative to the recurrent matrix multiplication, but the authors report both formulations performed similarly. 4) Training and regularization protocol (RMSProp, weight noise σ=0.075, gradient norm clipping to 1, hyperparameter random search) applied consistently across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — GRU-based RNNs matched or outperformed other units depending on dataset: on polyphonic music datasets GRU generally outperformed tanh and was comparable to or slightly better than LSTM (e.g., JSB Chorales test negative log-likelihood: GRU 8.54 vs LSTM 8.67); on raw speech datasets GRU strongly outperformed tanh and outperformed LSTM on Ubisoft dataset B (train NLL GRU 0.38 vs LSTM 0.80; test NLL GRU 0.88 vs LSTM 1.26) but not on Ubisoft A where LSTM performed best (test NLL LSTM 2.70 vs GRU 3.59). GRU also showed faster convergence in CPU time and in number of parameter updates on some music datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>No major technical obstacles were reported, but authors note dataset-dependence: the relative performance of GRU vs LSTM varies across tasks and datasets, preventing a single definitive conclusion; different output distributions and sequence lengths required domain-specific output layer choices and dataset preparations; comparative fairness required careful matching of parameter counts and consistent optimization/regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared underlying sequence-modeling principles (RNN architectures), availability of reference implementations and ML toolchains (Theano, Pylearn2), ability to control for parameter counts and consistent training protocols (RMSProp, clipping, weight noise), and prior evidence that gating helps with long-range dependencies enabled successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Required domain-specific output-layer designs (sigmoid for multi-label music, mixture-of-Gaussians for raw audio), hyperparameter search (random search over learning rates), gradient clipping and regularization (weight noise), and matched model sizes to ensure fair empirical comparison; also depended on access to appropriate datasets (polyphonic music sets and Ubisoft raw-audio sets) and compute resources.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately generalizable within sequence modeling problems: the gating mechanism and GRU architecture transferred effectively to distinct sequence domains (symbolic music, raw audio), but the paper emphasizes that the relative advantages over LSTM are dataset- and task-dependent, so generalization to other domains requires empirical validation and similar careful tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural steps (architectural design choices, implementation variants, and training protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e557.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e557.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM → music & speech</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory unit applied to polyphonic music modeling and raw speech-signal modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LSTM recurrent unit, originally introduced for capturing long-term dependencies in sequences and used widely in tasks like speech recognition and machine translation, is applied here as a baseline gated recurrent unit for sequence modeling of polyphonic music and raw audio speech, using a standard Graves-style implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Long Short-Term Memory (LSTM) application to new sequence domains</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>An RNN unit that maintains an explicit memory cell with input, forget and output gates to control writing, forgetting and exposure of the memory. In this work the authors follow an LSTM implementation similar to Graves (2013), with diagonal peephole connections (V matrices diagonal) and gating functions implemented with logistic sigmoid activations. LSTM-based RNNs are trained with the same optimization and regularization protocols (RMSProp, weight noise, gradient clipping) and model sizes adjusted to match parameter counts of other unit types.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / experimental protocol (ML model adaptation & empirical evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Foundational sequence modeling research (neural networks / recurrent networks) and prior applications in speech recognition and machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Polyphonic music sequence modeling and raw speech-signal modeling</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without major modification (standard implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Main adaptations were experimental/engineering: model size chosen to match a target number of parameters; output layers chosen per domain (sigmoid for binary music vectors; mixture-of-Gaussians for raw audio); training hyperparameters selected by validation (learning rate search, RMSProp), and regularization (weight noise, gradient clipping) applied. Otherwise the LSTM unit followed the Graves (2013) implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — LSTM outperformed tanh units on the more challenging raw speech tasks and was best on one speech dataset (Ubisoft A test NLL 2.70 vs GRU 3.59), but was not uniformly superior to GRU across all datasets; overall gated units (LSTM and GRU) outperformed the tanh baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>No explicit technical failures reported; however the authors could not draw a definitive, general conclusion about LSTM vs GRU superiority because performance depends on dataset/task specifics. Engineering challenges included selecting fair model sizes and choosing appropriate output distributions for different data types.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Established LSTM formulations and prior success in related sequence domains, availability of implementations (Graves 2013), and consistent training protocols and evaluation datasets helped make a fair transfer and comparison possible.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Needed correct LSTM implementation (including peepholes as in Graves), matching parameter-count constraints, appropriate output-layer choices for the target domain, and the same training/regularization pipeline (RMSProp, gradient clipping, weight noise) used for other comparators.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within sequence modeling tasks that require capturing long-term dependencies, but relative advantage over other gated units is data-dependent; authors recommend further, more thorough experiments to understand component contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural steps (architectural design and training regimen)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e557.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e557.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRU candidate-activation variant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implementation variant of GRU candidate activation (placement of reset gate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small implementation-level modification to the GRU candidate-activation computation: using the reset gate to mask the previous hidden state before applying the recurrent matrix (tanh(W x + U (r ⊙ h_{t-1}))) instead of masking after the matrix multiplication (tanh(W x + r ⊙ (U h_{t-1}))). The authors used the former and report preliminary experiments showing equivalent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Modified GRU candidate-activation formula (reset gate placement)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Two mathematically different but related formulations of computing the GRU candidate activation are considered: (A) mask-then-transform: compute r ⊙ h_{t-1} then multiply by U (used in this paper: tanh(W x + U (r ⊙ h_{t-1}))), and (B) transform-then-mask: compute U h_{t-1} then multiply element-wise by r (original Cho et al. formulation: tanh(W x + r ⊙ (U h_{t-1}))). The authors adopt formulation (A) in experiments and note preliminary tests showed both formulations performed similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / implementation variant</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Neural machine translation / GRU original implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Implementation used in sequence modeling experiments (music & speech)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for implementation preference (minor algorithmic variant)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Changed the order of element-wise reset gating and the recurrent linear transform in the candidate-activation computation (from r ⊙ (U h_{t-1}) to U (r ⊙ h_{t-1})). This affects where the element-wise gating is applied relative to the matrix multiply; no further algorithmic changes were made.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — authors report preliminary experiments found both formulations performed as well as each other, and experiments in the paper used the mask-then-transform variant without observed degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>No performance penalty observed; authors do not report numerical degradation, only that both worked similarly. Potential numerical or implementation differences were not reported as barriers.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Empirical equivalence between the two formulations on preliminary tests, and the simplicity of implementing either variant in their ML framework (Theano) facilitated the swap.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Implementation in a flexible computational framework permitting element-wise operations and matrix multiplications (Theano) and validation to confirm equivalence; consistent training pipeline to ensure observed equivalence holds under the experimental conditions used.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable: authors observed equivalent performance in their preliminary experiments and suggest either variant can be used; however, they do not provide exhaustive cross-dataset validation, so some domain- or scale-specific differences cannot be excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps / implementation know-how</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the properties of neural machine translation: Encoder-decoder approaches <em>(Rating: 2)</em></li>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 2)</em></li>
                <li>Supervised Sequence Labelling with Recurrent Neural Networks <em>(Rating: 2)</em></li>
                <li>Long short-term memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-557",
    "paper_id": "paper-ac3ee98020251797c2b401e1389461df88e52e62",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "GRU → music & speech",
            "name_full": "Gated Recurrent Unit applied to polyphonic music modeling and raw speech-signal modeling",
            "brief_description": "The gated recurrent unit (GRU), originally proposed and used in neural machine translation, is applied here to sequence modeling tasks in polyphonic symbolic music and raw audio speech; the paper reports implementation details, a small modification to the candidate-activation formula, training protocol, and empirical outcomes comparing GRU to LSTM and tanh RNNs.",
            "citation_title": "On the properties of neural machine translation: Encoder-decoder approaches",
            "mention_or_use": "use",
            "procedure_name": "Gated Recurrent Unit (GRU) application to new sequence domains",
            "procedure_description": "A recurrent neural network unit that uses gating (reset and update gates) to adaptively control information flow and allow the unit to capture dependencies at different time scales. Activation is computed as a linear interpolation between previous activation and a candidate activation whose computation is gated by a reset gate; update and reset gates are sigmoidal functions of input and previous hidden state. In this paper GRUs are instantiated in RNNs trained to model sequences of binary vectors (polyphonic music) and raw 1-D audio samples (speech), with output layers chosen per domain (logistic sigmoid for binary music symbols, mixture-of-Gaussians for raw audio prediction). Training uses RMSProp, weight noise, gradient clipping and hyperparameter random search; model sizes were chosen so each unit type had approximately equal parameter counts.",
            "procedure_type": "computational method / experimental protocol (ML model adaptation & empirical evaluation)",
            "source_domain": "Natural language processing / Neural machine translation (sequence-to-sequence NLP)",
            "target_domain": "Sequence modeling in music (symbolic polyphonic music) and raw speech-signal modeling (audio prediction)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "1) Output layer and loss adapted to each target domain: logistic sigmoid outputs for multi-label binary vectors in polyphonic music; mixture-of-Gaussians (20 components) output for raw audio regression in speech. 2) Model sizes (number of units) were adjusted so GRU-based RNNs had roughly the same total number of parameters as LSTM and tanh baselines to enable fair comparison. 3) A small implementation-level change to the candidate activation formula was used (see separate entry), i.e. the placement/order of applying the reset gate relative to the recurrent matrix multiplication, but the authors report both formulations performed similarly. 4) Training and regularization protocol (RMSProp, weight noise σ=0.075, gradient norm clipping to 1, hyperparameter random search) applied consistently across tasks.",
            "transfer_success": "partially successful — GRU-based RNNs matched or outperformed other units depending on dataset: on polyphonic music datasets GRU generally outperformed tanh and was comparable to or slightly better than LSTM (e.g., JSB Chorales test negative log-likelihood: GRU 8.54 vs LSTM 8.67); on raw speech datasets GRU strongly outperformed tanh and outperformed LSTM on Ubisoft dataset B (train NLL GRU 0.38 vs LSTM 0.80; test NLL GRU 0.88 vs LSTM 1.26) but not on Ubisoft A where LSTM performed best (test NLL LSTM 2.70 vs GRU 3.59). GRU also showed faster convergence in CPU time and in number of parameter updates on some music datasets.",
            "barriers_encountered": "No major technical obstacles were reported, but authors note dataset-dependence: the relative performance of GRU vs LSTM varies across tasks and datasets, preventing a single definitive conclusion; different output distributions and sequence lengths required domain-specific output layer choices and dataset preparations; comparative fairness required careful matching of parameter counts and consistent optimization/regularization.",
            "facilitating_factors": "Shared underlying sequence-modeling principles (RNN architectures), availability of reference implementations and ML toolchains (Theano, Pylearn2), ability to control for parameter counts and consistent training protocols (RMSProp, clipping, weight noise), and prior evidence that gating helps with long-range dependencies enabled successful transfer.",
            "contextual_requirements": "Required domain-specific output-layer designs (sigmoid for multi-label music, mixture-of-Gaussians for raw audio), hyperparameter search (random search over learning rates), gradient clipping and regularization (weight noise), and matched model sizes to ensure fair empirical comparison; also depended on access to appropriate datasets (polyphonic music sets and Ubisoft raw-audio sets) and compute resources.",
            "generalizability": "Moderately generalizable within sequence modeling problems: the gating mechanism and GRU architecture transferred effectively to distinct sequence domains (symbolic music, raw audio), but the paper emphasizes that the relative advantages over LSTM are dataset- and task-dependent, so generalization to other domains requires empirical validation and similar careful tuning.",
            "knowledge_type": "theoretical principles and explicit procedural steps (architectural design choices, implementation variants, and training protocol)",
            "uuid": "e557.0",
            "source_info": {
                "paper_title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                "publication_date_yy_mm": "2014-12"
            }
        },
        {
            "name_short": "LSTM → music & speech",
            "name_full": "Long Short-Term Memory unit applied to polyphonic music modeling and raw speech-signal modeling",
            "brief_description": "The LSTM recurrent unit, originally introduced for capturing long-term dependencies in sequences and used widely in tasks like speech recognition and machine translation, is applied here as a baseline gated recurrent unit for sequence modeling of polyphonic music and raw audio speech, using a standard Graves-style implementation.",
            "citation_title": "Long short-term memory",
            "mention_or_use": "use",
            "procedure_name": "Long Short-Term Memory (LSTM) application to new sequence domains",
            "procedure_description": "An RNN unit that maintains an explicit memory cell with input, forget and output gates to control writing, forgetting and exposure of the memory. In this work the authors follow an LSTM implementation similar to Graves (2013), with diagonal peephole connections (V matrices diagonal) and gating functions implemented with logistic sigmoid activations. LSTM-based RNNs are trained with the same optimization and regularization protocols (RMSProp, weight noise, gradient clipping) and model sizes adjusted to match parameter counts of other unit types.",
            "procedure_type": "computational method / experimental protocol (ML model adaptation & empirical evaluation)",
            "source_domain": "Foundational sequence modeling research (neural networks / recurrent networks) and prior applications in speech recognition and machine translation",
            "target_domain": "Polyphonic music sequence modeling and raw speech-signal modeling",
            "transfer_type": "direct application without major modification (standard implementation)",
            "modifications_made": "Main adaptations were experimental/engineering: model size chosen to match a target number of parameters; output layers chosen per domain (sigmoid for binary music vectors; mixture-of-Gaussians for raw audio); training hyperparameters selected by validation (learning rate search, RMSProp), and regularization (weight noise, gradient clipping) applied. Otherwise the LSTM unit followed the Graves (2013) implementation.",
            "transfer_success": "partially successful — LSTM outperformed tanh units on the more challenging raw speech tasks and was best on one speech dataset (Ubisoft A test NLL 2.70 vs GRU 3.59), but was not uniformly superior to GRU across all datasets; overall gated units (LSTM and GRU) outperformed the tanh baseline.",
            "barriers_encountered": "No explicit technical failures reported; however the authors could not draw a definitive, general conclusion about LSTM vs GRU superiority because performance depends on dataset/task specifics. Engineering challenges included selecting fair model sizes and choosing appropriate output distributions for different data types.",
            "facilitating_factors": "Established LSTM formulations and prior success in related sequence domains, availability of implementations (Graves 2013), and consistent training protocols and evaluation datasets helped make a fair transfer and comparison possible.",
            "contextual_requirements": "Needed correct LSTM implementation (including peepholes as in Graves), matching parameter-count constraints, appropriate output-layer choices for the target domain, and the same training/regularization pipeline (RMSProp, gradient clipping, weight noise) used for other comparators.",
            "generalizability": "High within sequence modeling tasks that require capturing long-term dependencies, but relative advantage over other gated units is data-dependent; authors recommend further, more thorough experiments to understand component contributions.",
            "knowledge_type": "theoretical principles and explicit procedural steps (architectural design and training regimen)",
            "uuid": "e557.1",
            "source_info": {
                "paper_title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                "publication_date_yy_mm": "2014-12"
            }
        },
        {
            "name_short": "GRU candidate-activation variant",
            "name_full": "Implementation variant of GRU candidate activation (placement of reset gate)",
            "brief_description": "A small implementation-level modification to the GRU candidate-activation computation: using the reset gate to mask the previous hidden state before applying the recurrent matrix (tanh(W x + U (r ⊙ h_{t-1}))) instead of masking after the matrix multiplication (tanh(W x + r ⊙ (U h_{t-1}))). The authors used the former and report preliminary experiments showing equivalent performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Modified GRU candidate-activation formula (reset gate placement)",
            "procedure_description": "Two mathematically different but related formulations of computing the GRU candidate activation are considered: (A) mask-then-transform: compute r ⊙ h_{t-1} then multiply by U (used in this paper: tanh(W x + U (r ⊙ h_{t-1}))), and (B) transform-then-mask: compute U h_{t-1} then multiply element-wise by r (original Cho et al. formulation: tanh(W x + r ⊙ (U h_{t-1}))). The authors adopt formulation (A) in experiments and note preliminary tests showed both formulations performed similarly.",
            "procedure_type": "computational method / implementation variant",
            "source_domain": "Neural machine translation / GRU original implementation details",
            "target_domain": "Implementation used in sequence modeling experiments (music & speech)",
            "transfer_type": "adapted/modified for implementation preference (minor algorithmic variant)",
            "modifications_made": "Changed the order of element-wise reset gating and the recurrent linear transform in the candidate-activation computation (from r ⊙ (U h_{t-1}) to U (r ⊙ h_{t-1})). This affects where the element-wise gating is applied relative to the matrix multiply; no further algorithmic changes were made.",
            "transfer_success": "successful — authors report preliminary experiments found both formulations performed as well as each other, and experiments in the paper used the mask-then-transform variant without observed degradation.",
            "barriers_encountered": "No performance penalty observed; authors do not report numerical degradation, only that both worked similarly. Potential numerical or implementation differences were not reported as barriers.",
            "facilitating_factors": "Empirical equivalence between the two formulations on preliminary tests, and the simplicity of implementing either variant in their ML framework (Theano) facilitated the swap.",
            "contextual_requirements": "Implementation in a flexible computational framework permitting element-wise operations and matrix multiplications (Theano) and validation to confirm equivalence; consistent training pipeline to ensure observed equivalence holds under the experimental conditions used.",
            "generalizability": "Likely generalizable: authors observed equivalent performance in their preliminary experiments and suggest either variant can be used; however, they do not provide exhaustive cross-dataset validation, so some domain- or scale-specific differences cannot be excluded.",
            "knowledge_type": "explicit procedural steps / implementation know-how",
            "uuid": "e557.2",
            "source_info": {
                "paper_title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                "publication_date_yy_mm": "2014-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the properties of neural machine translation: Encoder-decoder approaches",
            "rating": 2
        },
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 2
        },
        {
            "paper_title": "Supervised Sequence Labelling with Recurrent Neural Networks",
            "rating": 2
        },
        {
            "paper_title": "Long short-term memory",
            "rating": 1
        }
    ],
    "cost": 0.0116175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</h1>
<p>Junyoung Chung Caglar Gulcehre KyungHyun Cho<br>Yoshua Bengio<br>Université de Montréal<br>CIFAR Senior Fellow</p>
<h4>Abstract</h4>
<p>In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.</p>
<h2>1 Introduction</h2>
<p>Recurrent neural networks have recently shown promising results in many machine learning tasks, especially when input and/or output are of variable length [see, e.g., Graves, 2012]. More recently, Sutskever et al. [2014] and Bahdanau et al. [2014] reported that recurrent neural networks are able to perform as well as the existing, well-developed systems on a challenging task of machine translation.</p>
<p>One interesting observation, we make from these recent successes is that almost none of these successes were achieved with a vanilla recurrent neural network. Rather, it was a recurrent neural network with sophisticated recurrent hidden units, such as long short-term memory units [Hochreiter and Schmidhuber, 1997], that was used in those successful applications.</p>
<p>Among those sophisticated recurrent units, in this paper, we are interested in evaluating two closely related variants. One is a long short-term memory (LSTM) unit, and the other is a gated recurrent unit (GRU) proposed more recently by Cho et al. [2014]. It is well established in the field that the LSTM unit works well on sequence-based tasks with long-term dependencies, but the latter has only recently been introduced and used in the context of machine translation.</p>
<p>In this paper, we evaluate these two units and a more traditional tanh unit on the task of sequence modeling. We consider three polyphonic music datasets [see, e.g., Boulanger-Lewandowski et al., 2012] as well as two internal datasets provided by Ubisoft in which each sample is a raw speech representation.</p>
<p>Based on our experiments, we concluded that by using fixed number of parameters for all models on some datasets GRU, can outperform LSTM units both in terms of convergence in CPU time and in terms of parameter updates and generalization.</p>
<h2>2 Background: Recurrent Neural Network</h2>
<p>A recurrent neural network (RNN) is an extension of a conventional feedforward neural network, which is able to handle a variable-length sequence input. The RNN handles the variable-length</p>
<p>sequence by having a recurrent hidden state whose activation at each time is dependent on that of the previous time.</p>
<p>More formally, given a sequence $\mathbf{x}=\left(\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>}, \cdots, \mathbf{x<em t="t">{T}\right)$, the RNN updates its recurrent hidden state $h</em>$ by</p>
<p>$$
\mathbf{h}<em t-1="t-1">{t}= \begin{cases}0, &amp; t=0 \ \phi\left(\mathbf{h}</em>
$$}, \mathbf{x}_{t}\right), &amp; \text { otherwise }\end{cases</p>
<p>where $\phi$ is a nonlinear function such as composition of a logistic sigmoid with an affine transformation. Optionally, the RNN may have an output $\mathbf{y}=\left(y_{1}, y_{2}, \ldots, y_{T}\right)$ which may again be of variable length.</p>
<p>Traditionally, the update of the recurrent hidden state in Eq. (1) is implemented as</p>
<p>$$
\mathbf{h}<em t="t">{t}=g\left(W \mathbf{x}</em>\right)
$$}+U \mathbf{h}_{t-1</p>
<p>where $g$ is a smooth, bounded function such as a logistic sigmoid function or a hyperbolic tangent function.</p>
<p>A generative RNN outputs a probability distribution over the next element of the sequence, given its current state $\mathbf{h}_{t}$, and this generative model can capture a distribution over sequences of variable length by using a special output symbol to represent the end of the sequence. The sequence probability can be decomposed into</p>
<p>$$
p\left(x_{1}, \ldots, x_{T}\right)=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) p\left(x_{3} \mid x_{1}, x_{2}\right) \cdots p\left(x_{T} \mid x_{1}, \ldots, x_{T-1}\right)
$$</p>
<p>where the last element is a special end-of-sequence value. We model each conditional probability distribution with</p>
<p>$$
p\left(x_{t} \mid x_{1}, \ldots, x_{t-1}\right)=g\left(h_{t}\right)
$$</p>
<p>where $h_{t}$ is from Eq. (1). Such generative RNNs are the subject of this paper.
Unfortunately, it has been observed by, e.g., Bengio et al. [1994] that it is difficult to train RNNs to capture long-term dependencies because the gradients tend to either vanish (most of the time) or explode (rarely, but with severe effects). This makes gradient-based optimization method struggle, not just because of the variations in gradient magnitudes but because of the effect of long-term dependencies is hidden (being exponentially smaller with respect to sequence length) by the effect of short-term dependencies. There have been two dominant approaches by which many researchers have tried to reduce the negative impacts of this issue. One such approach is to devise a better learning algorithm than a simple stochastic gradient descent [see, e.g., Bengio et al., 2013, Pascanu et al., 2013, Martens and Sutskever, 2011], for example using the very simple clipped gradient, by which the norm of the gradient vector is clipped, or using second-order methods which may be less sensitive to the issue if the second derivatives follow the same growth pattern as the first derivatives (which is not guaranteed to be the case).</p>
<p>The other approach, in which we are more interested in this paper, is to design a more sophisticated activation function than a usual activation function, consisting of affine transformation followed by a simple element-wise nonlinearity by using gating units. The earliest attempt in this direction resulted in an activation function, or a recurrent unit, called a long short-term memory (LSTM) unit [Hochreiter and Schmidhuber, 1997]. More recently, another type of recurrent unit, to which we refer as a gated recurrent unit (GRU), was proposed by Cho et al. [2014]. RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies. Those tasks include, but are not limited to, speech recognition [see, e.g., Graves et al., 2013] and machine translation [see, e.g., Sutskever et al., 2014, Bahdanau et al., 2014].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of (a) LSTM and (b) gated recurrent units. (a) $i$, $f$ and $o$ are the input, forget and output gates, respectively. $c$ and $\tilde{c}$ denote the memory cell and the new memory cell content. (b) $r$ and $z$ are the reset and update gates, and $h$ and $\tilde{h}$ are the activation and the candidate activation.</p>
<h1>3 Gated Recurrent Neural Networks</h1>
<p>In this paper, we are interested in evaluating the performance of those recently proposed recurrent units (LSTM unit and GRU) on sequence modeling. Before the empirical evaluation, we first describe each of those recurrent units in this section.</p>
<h3>3.1 Long Short-Term Memory Unit</h3>
<p>The Long Short-Term Memory (LSTM) unit was initially proposed by Hochreiter and Schmidhuber [1997]. Since then, a number of minor modifications to the original LSTM unit have been made. We follow the implementation of LSTM as used in Graves [2013].
Unlike to the recurrent unit which simply computes a weighted sum of the input signal and applies a nonlinear function, each $j$-th LSTM unit maintains a memory $c_{t}^{j}$ at time $t$. The output $h_{t}^{j}$, or the activation, of the LSTM unit is then</p>
<p>$$
h_{t}^{j}=o_{t}^{j} \tanh \left(c_{t}^{j}\right)
$$</p>
<p>where $o_{t}^{j}$ is an output gate that modulates the amount of memory content exposure. The output gate is computed by</p>
<p>$$
o_{t}^{j}=\sigma\left(W_{o} \mathbf{x}<em o="o">{t}+U</em>} \mathbf{h<em o="o">{t-1}+V</em>
$$} \mathbf{c}_{t}\right)^{j</p>
<p>where $\sigma$ is a logistic sigmoid function. $V_{o}$ is a diagonal matrix.
The memory cell $c_{t}^{j}$ is updated by partially forgetting the existing memory and adding a new memory content $\tilde{c}_{t}^{j}$ :</p>
<p>$$
c_{t}^{j}=f_{t}^{j} c_{t-1}^{j}+i_{t}^{j} \tilde{c}_{t}^{j}
$$</p>
<p>where the new memory content is</p>
<p>$$
\tilde{c}<em c="c">{t}^{j}=\tanh \left(W</em>} \mathbf{x<em c="c">{t}+U</em>
$$} \mathbf{h}_{t-1}\right)^{j</p>
<p>The extent to which the existing memory is forgotten is modulated by a forget gate $f_{t}^{j}$, and the degree to which the new memory content is added to the memory cell is modulated by an input gate $i_{t}^{j}$. Gates are computed by</p>
<p>$$
\begin{aligned}
f_{t}^{j} &amp; =\sigma\left(W_{f} \mathbf{x}<em f="f">{t}+U</em>} \mathbf{h<em f="f">{t-1}+V</em>} \mathbf{c<em t="t">{t-1}\right)^{j} \
i</em>}^{j} &amp; =\sigma\left(W_{i} \mathbf{x<em i="i">{t}+U</em>} \mathbf{h<em i="i">{t-1}+V</em>
\end{aligned}
$$} \mathbf{c}_{t-1}\right)^{j</p>
<p>Note that $V_{f}$ and $V_{i}$ are diagonal matrices.</p>
<p>Unlike to the traditional recurrent unit which overwrites its content at each time-step (see Eq. (2)), an LSTM unit is able to decide whether to keep the existing memory via the introduced gates. Intuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it easily carries this information (the existence of the feature) over a long distance, hence, capturing potential long-distance dependencies.
See Fig. 1 (a) for the graphical illustration.</p>
<h1>3.2 Gated Recurrent Unit</h1>
<p>A gated recurrent unit (GRU) was proposed by Cho et al. [2014] to make each recurrent unit to adaptively capture dependencies of different time scales. Similarly to the LSTM unit, the GRU has gating units that modulate the flow of information inside the unit, however, without having a separate memory cells.</p>
<p>The activation $h_{t}^{j}$ of the GRU at time $t$ is a linear interpolation between the previous activation $h_{t-1}^{j}$ and the candidate activation $\tilde{h}_{t}^{j}$ :</p>
<p>$$
h_{t}^{j}=\left(1-z_{t}^{j}\right) h_{t-1}^{j}+z_{t}^{j} \tilde{h}_{t}^{j}
$$</p>
<p>where an update gate $z_{t}^{j}$ decides how much the unit updates its activation, or content. The update gate is computed by</p>
<p>$$
z_{t}^{j}=\sigma\left(W_{z} \mathbf{x}<em z="z">{t}+U</em>
$$} \mathbf{h}_{t-1}\right)^{j</p>
<p>This procedure of taking a linear sum between the existing state and the newly computed state is similar to the LSTM unit. The GRU, however, does not have any mechanism to control the degree to which its state is exposed, but exposes the whole state each time.</p>
<p>The candidate activation $\tilde{h}_{t}^{j}$ is computed similarly to that of the traditional recurrent unit (see Eq. (2)) and as in [Bahdanau et al., 2014],</p>
<p>$$
\tilde{h}<em t="t">{t}^{j}=\tanh \left(W \mathbf{x}</em>}+U\left(\mathbf{r<em t-1="t-1">{t} \odot \mathbf{h}</em>
$$}\right)\right)^{j</p>
<p>where $\mathbf{r}<em t="t">{t}$ is a set of reset gates and $\odot$ is an element-wise multiplication. ${ }^{1}$ When off ( $r</em>$ close to 0 ), the reset gate effectively makes the unit act as if it is reading the first symbol of an input sequence, allowing it to forget the previously computed state.}^{j</p>
<p>The reset gate $r_{t}^{j}$ is computed similarly to the update gate:</p>
<p>$$
r_{t}^{j}=\sigma\left(W_{r} \mathbf{x}<em r="r">{t}+U</em>
$$} \mathbf{h}_{t-1}\right)^{j</p>
<p>See Fig. 1 (b) for the graphical illustration of the GRU.</p>
<h3>3.3 Discussion</h3>
<p>It is easy to notice similarities between the LSTM unit and the GRU from Fig. 1.
The most prominent feature shared between these units is the additive component of their update from $t$ to $t+1$, which is lacking in the traditional recurrent unit. The traditional recurrent unit always replaces the activation, or the content of a unit with a new value computed from the current input and the previous hidden state. On the other hand, both LSTM unit and GRU keep the existing content and add the new content on top of it (see Eqs. (4) and (5)).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This additive nature has two advantages. First, it is easy for each unit to remember the existence of a specific feature in the input stream for a long series of steps. Any important feature, decided by either the forget gate of the LSTM unit or the update gate of the GRU, will not be overwritten but be maintained as it is.</p>
<p>Second, and perhaps more importantly, this addition effectively creates shortcut paths that bypass multiple temporal steps. These shortcuts allow the error to be back-propagated easily without too quickly vanishing (if the gating unit is nearly saturated at 1) as a result of passing through multiple, bounded nonlinearities, thus reducing the difficulty due to vanishing gradients [Hochreiter, 1991, Bengio et al., 1994].</p>
<p>These two units however have a number of differences as well. One feature of the LSTM unit that is missing from the GRU is the controlled exposure of the memory content. In the LSTM unit, the amount of the memory content that is seen, or used by other units in the network is controlled by the output gate. On the other hand the GRU exposes its full content without any control.</p>
<p>Another difference is in the location of the input gate, or the corresponding reset gate. The LSTM unit computes the new memory content without any separate control of the amount of information flowing from the previous time step. Rather, the LSTM unit controls the amount of the new memory content being added to the memory cell independently from the forget gate. On the other hand, the GRU controls the information flow from the previous activation when computing the new, candidate activation, but does not independently control the amount of the candidate activation being added (the control is tied via the update gate).</p>
<p>From these similarities and differences alone, it is difficult to conclude which types of gating units would perform better in general. Although Bahdanau et al. [2014] reported that these two units performed comparably to each other according to their preliminary experiments on machine translation, it is unclear whether this applies as well to tasks other than machine translation. This motivates us to conduct more thorough empirical comparison between the LSTM unit and the GRU in this paper.</p>
<h1>4 Experiments Setting</h1>
<h3>4.1 Tasks and Datasets</h3>
<p>We compare the LSTM unit, GRU and tanh unit in the task of sequence modeling. Sequence modeling aims at learning a probability distribution over sequences, as in Eq. (3), by maximizing the log-likelihood of a model given a set of training sequences:</p>
<p>$$
\max <em n="1">{\boldsymbol{\theta}} \frac{1}{N} \sum</em>\right)
$$}^{N} \sum_{t=1}^{T_{n}} \log p\left(x_{t}^{n} \mid x_{1}^{n}, \ldots, x_{t-1}^{n} ; \boldsymbol{\theta</p>
<p>where $\boldsymbol{\theta}$ is a set of model parameters. More specifically, we evaluate these units in the tasks of polyphonic music modeling and speech signal modeling.</p>
<p>For the polyphonic music modeling, we use three polyphonic music datasets from [BoulangerLewandowski et al., 2012]: Nottingham, JSB Chorales, MuseData and Piano-midi. These datasets contain sequences of which each symbol is respectively a $93-, 96-, 105-$, and 108-dimensional binary vector. We use logistic sigmoid function as output units.</p>
<p>We use two internal datasets provided by Ubisoft ${ }^{2}$ for speech signal modeling. Each sequence is an one-dimensional raw audio signal, and at each time step, we design a recurrent neural network to look at 20 consecutive samples to predict the following 10 consecutive samples. We have used two different versions of the dataset: One with sequences of length 500 (Ubisoft A) and the other with sequences of length 8,000 (Ubisoft B). Ubisoft A and Ubisoft B have 7, 230 and 800 sequences each. We use mixture of Gaussians with 20 components as output layer. ${ }^{3}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.2 Models</h1>
<p>For each task, we train three different recurrent neural networks, each having either LSTM units (LSTM-RNN, see Sec. 3.1), GRUs (GRU-RNN, see Sec. 3.2) or tanh units ( $\tanh$-RNN, see Eq. (2)). As the primary objective of these experiments is to compare all three units fairly, we choose the size of each model so that each model has approximately the same number of parameters. We intentionally made the models to be small enough in order to avoid overfitting which can easily distract the comparison. This approach of comparing different types of hidden units in neural networks has been done before, for instance, by Gulcehre et al. [2014]. See Table 1 for the details of the model sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Unit</th>
<th style="text-align: center;"># of Units</th>
<th style="text-align: center;"># of Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Polyphonic music modeling</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$\approx 19.8 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">GRU</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">$\approx 20.2 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">$\tanh$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\approx 20.1 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">Speech signal modeling</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">$\approx 169.1 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">GRU</td>
<td style="text-align: center;">227</td>
<td style="text-align: center;">$\approx 168.9 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">$\tanh$</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">$\approx 168.4 \times 10^{3}$</td>
</tr>
</tbody>
</table>
<p>Table 1: The sizes of the models tested in the experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\tanh$</th>
<th style="text-align: center;">GRU</th>
<th style="text-align: center;">LSTM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Music Datasets</td>
<td style="text-align: center;">Nottingham</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">3.22</td>
<td style="text-align: center;">2.79</td>
<td style="text-align: center;">3.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">3.13</td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JSB Chorales</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">8.82</td>
<td style="text-align: center;">6.94</td>
<td style="text-align: center;">8.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">8.54</td>
<td style="text-align: center;">8.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MuseData</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">5.64</td>
<td style="text-align: center;">5.06</td>
<td style="text-align: center;">5.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">6.23</td>
<td style="text-align: center;">5.99</td>
<td style="text-align: center;">6.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Piano-midi</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">5.64</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">6.49</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">9.03</td>
<td style="text-align: center;">8.82</td>
<td style="text-align: center;">9.03</td>
</tr>
<tr>
<td style="text-align: center;">Ubisoft Datasets</td>
<td style="text-align: center;">Ubisoft dataset A</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">6.29</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">1.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">3.59</td>
<td style="text-align: center;">2.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ubisoft dataset B</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">7.62</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">1.26</td>
</tr>
</tbody>
</table>
<p>Table 2: The average negative log-probabilities of the training and test sets.
We train each model with RMSProp [see, e.g., Hinton, 2012] and use weight noise with standard deviation fixed to 0.075 [Graves, 2011]. At every update, we rescale the norm of the gradient to 1, if it is larger than 1 [Pascanu et al., 2013] to prevent exploding gradients. We select a learning rate (scalar multiplier in RMSProp) to maximize the validation performance, out of 10 randomly chosen log-uniform candidates sampled from $\mathcal{U}(-12,-6)$ [Bergstra and Bengio, 2012]. The validation set is used for early-stop training as well.</p>
<h2>5 Results and Analysis</h2>
<p>Table 2 lists all the results from our experiments. In the case of the polyphonic music datasets, the GRU-RNN outperformed all the others (LSTM-RNN and tanh-RNN) on all the datasets except for the Nottingham. However, we can see that on these music datasets, all the three models performed closely to each other.</p>
<p>On the other hand, the RNNs with the gating units (GRU-RNN and LSTM-RNN) clearly outperformed the more traditional tanh-RNN on both of the Ubisoft datasets. The LSTM-RNN was best with the Ubisoft A, and with the Ubisoft B, the GRU-RNN performed best.</p>
<p>In Figs. 2-3, we show the learning curves of the best validation runs. In the case of the music datasets (Fig. 2), we see that the GRU-RNN makes faster progress in terms of both the number of</p>
<p>updates and actual CPU time. If we consider the Ubisoft datasets (Fig. 3), it is clear that although the computational requirement for each update in the tanh-RNN is much smaller than the other models, it did not make much progress each update and eventually stopped making any progress at much worse level.</p>
<p>These results clearly indicate the advantages of the gating units over the more traditional recurrent units. Convergence is often faster, and the final solutions tend to be better. However, our results are not conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Learning curves for training and validation sets of different types of units with respect to (top) the number of iterations and (bottom) the wall clock time. y-axis corresponds to the negativelog likelihood of the model shown in log-scale.</p>
<h1>6 Conclusion</h1>
<p>In this paper we empirically evaluated recurrent neural networks (RNN) with three widely used recurrent units; (1) a traditional tanh unit, (2) a long short-term memory (LSTM) unit and (3) a recently proposed gated recurrent unit (GRU). Our evaluation focused on the task of sequence modeling on a number of datasets including polyphonic music data and raw speech signal data.</p>
<p>The evaluation clearly demonstrated the superiority of the gated units; both the LSTM unit and GRU, over the traditional tanh unit. This was more evident with the more challenging task of raw speech signal modeling. However, we could not make concrete conclusion on which of the two gating units was better.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Learning curves for training and validation sets of different types of units with respect to (top) the number of iterations and (bottom) the wall clock time. x-axis is the number of epochs and y -axis corresponds to the negative-log likelihood of the model shown in log-scale.</p>
<p>We consider the experiments in this paper as preliminary. In order to understand better how a gated unit helps learning and to separate out the contribution of each component, for instance gating units in the LSTM unit or the GRU, of the gating units, more thorough experiments will be required in the future.</p>
<h1>Acknowledgments</h1>
<p>The authors would like to thank Ubisoft for providing the datasets and for the support. The authors would like to thank the developers of Theano [Bergstra et al., 2010, Bastien et al., 2012] and Pylearn2 [Goodfellow et al., 2013]. We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs and CIFAR.</p>
<h1>References</h1>
<p>D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. Technical report, arXiv preprint arXiv:1409.0473, 2014.
F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.
Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent networks. In Proc. ICASSP 38, 2013.
J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Journal of Machine Learning Research, 13(1):281-305, 2012.
J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. WardeFarley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010. Oral Presentation.
N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription. In Proceedings of the Twenty-nine International Conference on Machine Learning (ICML'12). ACM, 2012. URL http://icml.cc/discuss/2012/590.html.
K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
I. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra, F. Bastien, and Y. Bengio. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013.
A. Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in Computational Intelligence. Springer, 2012.
A. Graves. Practical variational inference for neural networks. In Advances in Neural Information Processing Systems, pages 2348-2356, 2011.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP'2013, pages 6645-6649. IEEE, 2013.
C. Gulcehre, K. Cho, R. Pascanu, and Y. Bengio. Learned-norm pooling for deep feedforward and recurrent neural networks. In Machine Learning and Knowledge Discovery in Databases, pages 530-546. Springer, 2014.
G. Hinton. Neural networks for machine learning. Coursera, video lectures, 2012.
S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut für Informatik, Lehrstuhl Prof. Brauer, Technische Universität München, 1991. URL http:// www7.informatik.tu-muenchen.de/ Ehochreit.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997.
J. Martens and I. Sutskever. Learning recurrent neural networks with Hessian-free optimization. In Proc. ICML'2011. ACM, 2011.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning (ICML'13). ACM, 2013. URL http://icml.cc/2013/.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. Technical report, arXiv preprint arXiv:1409.3215, 2014.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://www.ubi.com/
${ }^{3}$ Our implementation is available at https://github.com/jych/librnn.git&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
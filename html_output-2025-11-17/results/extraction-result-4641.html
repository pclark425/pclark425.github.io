<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4641 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4641</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4641</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273482524</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.06750v2.pdf" target="_blank">Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents</a></p>
                <p><strong>Paper Abstract:</strong> Some have criticised Generative AI Systems for replicating the familiar pathologies of already widely-deployed AI systems. Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival. The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon. In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years. In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of 'Language Model Agents', in which multimodal large language models (LLMs) form the executive centre of complex, tool-using AI systems that can take unsupervised sequences of actions towards some goal.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4641.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4641.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that augments a language model with an external retrieval module so the model can condition generation on retrieved documents, compensating for limited context and statelessness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based system integrated with a retrieval component that fetches external documents or passages relevant to a query and conditions the language model's outputs on that retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval / retrieval-augmented memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses an external document store (e.g., vector database) to retrieve relevant documents or passages which are appended to the model context so the model can use factual information beyond its context window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring access to up-to-date or large-scale factual knowledge beyond the model's parametric memory, such as open-domain question answering and other knowledge-heavy generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper mentions RAG as an example of hooking LLMs up to external memory to compensate for being stateless, but provides no quantitative comparisons or numbers in this essay.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No empirical detail in this essay; general concerns include retrieval errors, reliance on external store quality, potential privacy issues when storing/retrieving user data, and prompt injection concerns when models use external sources.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>External retrieval is a practical way to extend LLMs beyond their context window and enable agents to act with factual grounding; the essay highlights retrieval augmentation as a core technique for building tool-using/agentic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4641.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4641.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-using LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-using Large Language Models (augmented LLMs, e.g., Toolformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs augmented to call external software tools (browser, calculator, code interpreter, APIs) to perform actions and improve reliability, often deciding when to call a tool and using the tool's response in subsequent reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tool-using LLMs (example: Toolformer / plugin-enabled LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Language models integrated into systems that select and call external tools or APIs as part of a multi-step process, then interpret tool outputs and continue reasoning or acting based on those outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external tools + potential external memory (browsing, code execution, vector stores)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>The agent decides when to call tools (including memory-like tools such as search or document stores), uses returned outputs to update its state or plan, and may store intermediate results externally to support longer processes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-step reasoning and tool-mediated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require multi-step planning, precise computation, or interaction with external systems (e.g., web tasks, code execution, complex question answering) where calling tools improves correctness or capability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The essay asserts that tool use (including memory-like tools) lets stateless LLMs compensate for limitations and perform longer action sequences; no quantitative ablation or performance figures are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Security and robustness (prompt injection, hijacking), alignment brittleness when moving from text generation to world actions, and the breakdown of RLHF-style alignment when behavior includes external actions rather than just text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Integrating tools and external memory extends LLM capabilities (calculation, web retrieval, persistent state) but raises new security and alignment challenges that are not fully addressed by current RLHF/RLAIF approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4641.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4641.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Companion memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long-term memory for AI Companions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual description of LLM-powered companions augmented with long-term persistent memory to recall user history, enabling highly personalized, persistent interactions but creating privacy and governance risks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AI Companion (LLM with long-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multimodal LLM agent with scripted/metaprompted personality, augmented with persistent long-term memory storage (often cloud-hosted) to recall everything a user has shared over time and act on that history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term persistent memory (cloud-hosted user history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores full or summarized records of past user interactions and personal data in a persistent store so the companion can perfectly recall past conversations and personalize future responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>personalized sustained dialogue / companionship</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain long-term, personalized conversational relationships: recall past disclosures, preferences, life events, and use them to produce contextually tailored support or companionship.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The essay argues that long-term memory materially increases perceived companionship and personalization, but offers no empirical performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Privacy risks from cloud-hosting, risk of exogenous modification of personalities by host companies, increased potential for manipulation and radicalization, and ethical concerns about anthropomorphism and authenticity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Long-term memory is what enables highly convincing companions (perfect recall increases attachment and personalization) but demands preemptive governance, privacy protections, and safeguards against exogenous control or misuse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4641.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4641.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention Guardian memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention Guardian (preference memory for recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed LLM-based intermediary that uses explicit, conversation-driven memory of a user's stated preferences and values to recommend and filter content without surveillance-based behavioral data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Attention Guardian (LLM with user preference memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM agent that builds and stores an explicit model of a user's values and second-order preferences via dialogue and uses that persistent memory to guide content recommendation and filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>persistent user preference memory (explicit elicited model)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores an explicit representation of a user's stated preferences and values (elicited via conversation) and uses reasoning over that stored model to select or filter online content rather than relying on centralized behavioral traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>recommendation / attention allocation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide personalized, non-surveillant content recommendations and filtering that respect a user's stated objectives and second-order preferences, rather than optimizing for engagement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The essay claims that memory-based, conversational preference models could replace surveillance-based recommenders and avoid centralisation harms, but does not present empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Practical hurdles include platform restrictions on browsing, the difficulty of accurately eliciting underlying preferences, potential paternalism, and coordination challenges to make such systems broadly useful.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Using explicit conversational memory to store user values could enable privacy-preserving, aligned recommendation without large-scale behavioral surveillance, but requires careful design to avoid paternalism and ensure access to necessary data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>Voyager: An Open-Ended Embodied Agent with Large Language Models <em>(Rating: 2)</em></li>
                <li>Webarena: A Realistic Web Environment for Building Autonomous Agents <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Toolverifier: Generalization to New Tools Via Self-Verification <em>(Rating: 1)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 1)</em></li>
                <li>AutoGPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4641",
    "paper_id": "paper-273482524",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A technique that augments a language model with an external retrieval module so the model can condition generation on retrieved documents, compensating for limited context and statelessness.",
            "citation_title": "Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-Augmented Generation (RAG)",
            "agent_description": "An LLM-based system integrated with a retrieval component that fetches external documents or passages relevant to a query and conditions the language model's outputs on that retrieved context.",
            "memory_type": "external retrieval / retrieval-augmented memory",
            "memory_description": "Uses an external document store (e.g., vector database) to retrieve relevant documents or passages which are appended to the model context so the model can use factual information beyond its context window.",
            "task_name": "knowledge-intensive NLP tasks",
            "task_description": "Tasks requiring access to up-to-date or large-scale factual knowledge beyond the model's parametric memory, such as open-domain question answering and other knowledge-heavy generation tasks.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper mentions RAG as an example of hooking LLMs up to external memory to compensate for being stateless, but provides no quantitative comparisons or numbers in this essay.",
            "limitations_or_challenges": "No empirical detail in this essay; general concerns include retrieval errors, reliance on external store quality, potential privacy issues when storing/retrieving user data, and prompt injection concerns when models use external sources.",
            "key_insights": "External retrieval is a practical way to extend LLMs beyond their context window and enable agents to act with factual grounding; the essay highlights retrieval augmentation as a core technique for building tool-using/agentic systems.",
            "uuid": "e4641.0",
            "source_info": {
                "paper_title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Tool-using LLMs",
            "name_full": "Tool-using Large Language Models (augmented LLMs, e.g., Toolformer)",
            "brief_description": "LLMs augmented to call external software tools (browser, calculator, code interpreter, APIs) to perform actions and improve reliability, often deciding when to call a tool and using the tool's response in subsequent reasoning.",
            "citation_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "mention_or_use": "mention",
            "agent_name": "Tool-using LLMs (example: Toolformer / plugin-enabled LLMs)",
            "agent_description": "Language models integrated into systems that select and call external tools or APIs as part of a multi-step process, then interpret tool outputs and continue reasoning or acting based on those outputs.",
            "memory_type": "external tools + potential external memory (browsing, code execution, vector stores)",
            "memory_description": "The agent decides when to call tools (including memory-like tools such as search or document stores), uses returned outputs to update its state or plan, and may store intermediate results externally to support longer processes.",
            "task_name": "multi-step reasoning and tool-mediated tasks",
            "task_description": "Tasks that require multi-step planning, precise computation, or interaction with external systems (e.g., web tasks, code execution, complex question answering) where calling tools improves correctness or capability.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The essay asserts that tool use (including memory-like tools) lets stateless LLMs compensate for limitations and perform longer action sequences; no quantitative ablation or performance figures are reported here.",
            "limitations_or_challenges": "Security and robustness (prompt injection, hijacking), alignment brittleness when moving from text generation to world actions, and the breakdown of RLHF-style alignment when behavior includes external actions rather than just text.",
            "key_insights": "Integrating tools and external memory extends LLM capabilities (calculation, web retrieval, persistent state) but raises new security and alignment challenges that are not fully addressed by current RLHF/RLAIF approaches.",
            "uuid": "e4641.1",
            "source_info": {
                "paper_title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AI Companion memory",
            "name_full": "Long-term memory for AI Companions",
            "brief_description": "Conceptual description of LLM-powered companions augmented with long-term persistent memory to recall user history, enabling highly personalized, persistent interactions but creating privacy and governance risks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "AI Companion (LLM with long-term memory)",
            "agent_description": "A multimodal LLM agent with scripted/metaprompted personality, augmented with persistent long-term memory storage (often cloud-hosted) to recall everything a user has shared over time and act on that history.",
            "memory_type": "long-term persistent memory (cloud-hosted user history)",
            "memory_description": "Stores full or summarized records of past user interactions and personal data in a persistent store so the companion can perfectly recall past conversations and personalize future responses.",
            "task_name": "personalized sustained dialogue / companionship",
            "task_description": "Maintain long-term, personalized conversational relationships: recall past disclosures, preferences, life events, and use them to produce contextually tailored support or companionship.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The essay argues that long-term memory materially increases perceived companionship and personalization, but offers no empirical performance comparisons.",
            "limitations_or_challenges": "Privacy risks from cloud-hosting, risk of exogenous modification of personalities by host companies, increased potential for manipulation and radicalization, and ethical concerns about anthropomorphism and authenticity.",
            "key_insights": "Long-term memory is what enables highly convincing companions (perfect recall increases attachment and personalization) but demands preemptive governance, privacy protections, and safeguards against exogenous control or misuse.",
            "uuid": "e4641.2",
            "source_info": {
                "paper_title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Attention Guardian memory",
            "name_full": "Attention Guardian (preference memory for recommendation)",
            "brief_description": "A proposed LLM-based intermediary that uses explicit, conversation-driven memory of a user's stated preferences and values to recommend and filter content without surveillance-based behavioral data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Attention Guardian (LLM with user preference memory)",
            "agent_description": "An LLM agent that builds and stores an explicit model of a user's values and second-order preferences via dialogue and uses that persistent memory to guide content recommendation and filtering.",
            "memory_type": "persistent user preference memory (explicit elicited model)",
            "memory_description": "Stores an explicit representation of a user's stated preferences and values (elicited via conversation) and uses reasoning over that stored model to select or filter online content rather than relying on centralized behavioral traces.",
            "task_name": "recommendation / attention allocation",
            "task_description": "Provide personalized, non-surveillant content recommendations and filtering that respect a user's stated objectives and second-order preferences, rather than optimizing for engagement.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The essay claims that memory-based, conversational preference models could replace surveillance-based recommenders and avoid centralisation harms, but does not present empirical comparisons.",
            "limitations_or_challenges": "Practical hurdles include platform restrictions on browsing, the difficulty of accurately eliciting underlying preferences, potential paternalism, and coordination challenges to make such systems broadly useful.",
            "key_insights": "Using explicit conversational memory to store user values could enable privacy-preserving, aligned recommendation without large-scale behavioral surveillance, but requires careful design to avoid paternalism and ensure access to necessary data.",
            "uuid": "e4641.3",
            "source_info": {
                "paper_title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Webarena: A Realistic Web Environment for Building Autonomous Agents",
            "rating": 2,
            "sanitized_title": "webarena_a_realistic_web_environment_for_building_autonomous_agents"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Toolverifier: Generalization to New Tools Via Self-Verification",
            "rating": 1,
            "sanitized_title": "toolverifier_generalization_to_new_tools_via_selfverification"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "AutoGPT",
            "rating": 1
        }
    ],
    "cost": 0.01208975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents 1</p>
<p>Seth Lazar 
Australian National University</p>
<p>Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents 1
36BAD3FD40916F1E27394F09C5388F5B
Some have criticised Generative AI Systems for replicating the familiar pathologies of already widely-deployed AI systems.Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival.The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon.In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years.In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of 'Language Model Agents', in which multimodal large language models (LLMs) form the executive centre of complex, tool-using AI systems that can take unsupervised sequences of actions towards some goal.</p>
<p>Introduction</p>
<p>Around a year ago, Generative AI took the world by storm, as extraordinarily powerful large language models (LLMs) enabled unprecedented performance at a wider range of tasks than ever before feasible. 2Though most known for generating convincing text and images, LLMs like OpenAI's GPT-4 and Google's Gemini are likely to have greater social impacts as the executive centre for complex systems that integrate additional tools for both learning about the world and acting on it. 3These Language Model Agents will power companions that introduce new categories of social relationship, and change old ones.They may well radically change the attention economy.And they will revolutionise personal computing, enabling everyone to control digital technologies with language alone.Much of the attention being paid to Generative AI Systems has focused on how they replicate the pathologies of already widely-deployed AI systems, arguing that they centralise power and wealth, ignore copyright protections, depend on exploitative labour practices, and use excessive resources.Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival.The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon.I want instead to pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years.4</p>
<p>A Primer</p>
<p>It may help to start by reviewing how LLMs work, and how they can be used to make Language Model Agents.An LLM is a large AI model trained on vast amounts of data with vast amounts of computational resources (lots of GPUs) to predict the next word given a sequence of words (a prompt).The process starts by chunking the training data into similarly-sized 'tokens' (words or parts of words), then for a given set of tokens masking out some of them, and attempting to predict the tokens that have been masked (so the model is self-supervised-it marks its own work).A predictive model for the underlying token distribution is built by passing it through many layers of a neural network, with each layer refining the model in some dimension or other to make it more accurate.This approach to modelling natural language has been around for several years. 5One key recent innovation has been to take these 'pretrained' models, which are basically just good at predicting the next token given a sequence of tokens, and fine-tune them for different tasks. 6This is done with supervised learning on labelled data.For example, you might train a pretrained model to be a good dialogue agent by using many examples of helpful responses to questions.This fine-tuning enables us to build models that can predict not just the most likely next token, but the most helpful one-this is much more useful.</p>
<p>Of course, these models are trained on large corpuses of internet data that include a lot of toxic and dangerous content, so their being helpful is a double-edged sword!A helpful model would helpfully tell you how to build a bomb or commit suicide if asked.The other key innovation has been to make these models much less likely to share dangerous information or generate toxic content.This is done with both supervised and reinforcement learning.In particular, Reinforcement Learning from Human Feedback (RLHF) has proved particularly effective. 7In RLHF, to simplify again, the model generates two responses to a given prompt, and a human evaluator determines which is better than the other according to some criteria.A reinforcement learning algorithm uses that feedback to build a predictor (a reward model) for how different completions would be evaluated by a human rater.The instruction-tuned LLM is then fine-tuned on that reward model.Reinforcement Learning with AI Feedback (RLAIF) basically does the same, but uses another LLM to evaluate prompt completions. 8So, we've now fine-tuned a pretrained model with supervised learning to perform some specific function, and then used reinforcement learning to minimise its prospect of behaving badly.This fine-tuned model is then deployed in a broader system.Even when developers provide a straightforward Application Programming Interface (API) to make calls on the model, they incorporate input and output filtering (to limit harmful prompting, and redact harmful completions), and the model itself is under further developer instructions reminding it to respond to prompts in a conformant way.And with apps like ChatGPT, multiple models are integrated together (for example, for image as well as text generation) and further elements of user interface design are layered on top.</p>
<p>This gives a basic description of a Generative AI System.The first generation of Generative AI Systems were limited to understanding and generating text; before the end of 2023, however, the introduction of GPT-4V and Google's Gemini ensured that the most capable Generative AI systems could understand and generate images as well.These systems combine impressive fluency in comprehension and expression with access to vast reserves of knowledge.The currently leading model, Anthropic's Claude 3, approaches PhD-level subject-matter comprehension across dozens of different subjects. 9</p>
<p>In addition, the most capable generative AI models can learn many other skills through this process of next token prediction-for example, translation between languages, mathematical and reasoning competence, the ability to play chess, and much more. 10But the most exciting unanticipated capability is LLMs' ability, with fine-tuning, to use software tools. 11</p>
<p>The basic idea is simple.People use text to write programs making API calls to other programs, to achieve ends they cannot otherwise realise.LLMs are very good at replicating the human use of language to perform particular functions.So, LLMs can be trained to determine when an API call would be useful, evaluate the response, and then repeat or vary as necessary.For example, an LLM might 'know' that it is likely to make basic mathematical mistakes, so when given a prompt that invites it to do some math, it might decide to call on a calculator instead.</p>
<p>This means that we can design augmented LLMs: Generative AI Systems in which the LLM functions as the executive control centre, calling on different software either to amplify its capabilities or compensate for those it lacks.LLMs, for example, are 'stateless'-they lack memory beyond their 'context window' (the space given over to prompts).Tool-using LLMs can compensate for this by hooking up to external memory (this includes a technique called Retrieval Augmented Generation). 12External tools can also enable data analysis, and multistep reasoning and action.ChatGPT, for example, can now browse the web, use a code interpreter plugin to run code, or perform other actions enabled by a developer; Microsoft's Bing reportedly has around 100 internal plugins. 13'Language Model Agent' is a Generative AI System in which a (multimodal) LLM can call on different resources to realise its goals.It is an agent because of its ability to take unsupervised actions in which it assesses its environment and acts within it to both gather more information and to achieve its goals.For example, a Language Model Agent can decide to call on a tool in order to achieve some objective, and then evaluate the results before taking the next step towards that goal.ChatGPT and Bing Chat are rudimentary Language Model Agents; so are the AI companions being developed and deployed by Replika, Character.AI, CHAI, Meta and others.</p>
<p>However, the most capable Language Model Agents will be more than just turn-taking dialogue partners-they will be able to conduct longer sequences of actions without direct supervision. 14The most powerful tech companies (Microsoft, Google, Amazon, Meta, NVidia) are currently working on Language Model Agents that will function at least as assistants and co-pilots, while they and a number of cutting-edge AI research labs (e.g.OpenAI, Cohere, Anthropic, Adept, Imbue, Cognition, Magic, Figure) are also trying to build more complex and capable autonomous systems.</p>
<p>Of course, while there have been some impressive demos (for example, Cognition Lab's AI software engineer, Devin, or the Figure Robotics Android 15 ), these efforts are not guaranteed to succeed.Although LLMs are impressively adept at simulating reasoning, and with careful prompting can plan better than should be feasible given how they are trained, they are not at present sufficiently competent at planning and reasoning to power robust Language Model Agents that can reliably operate without supervision in high stakes settings. 16However, in just the last two years significant progress has been made towards this goal; 17 and there are likely to be significant prospects of further returns to scaling up and improving existing methods, or else integrating them with other approaches to backfill their known shortcomings. 18With billions of dollars and the most talented researchers in AI all pulling in the same direction, we must expect that highly autonomous Language Model Agents will be feasible in the near-to mid-term.</p>
<p>Polarised Responses</p>
<p>In response to the coming-of-age of LLMs, the responsible AI research community initially resolved into two polarised camps.One decried these systems as the apotheosis of extractive and exploitative digital capitalism.Another saw them as not the fulfilment of something old, but the harbinger of something new: an intelligence explosion that will ultimately wipe out humanity.</p>
<p>The more prosaic critics of Generative AI clearly have a strong empirical case. 19LLMs are inherently extractive: they capture the value inherent to the creative outputs of millions of people, and distil it for private profit.Like many other technology products they depend on questionable labour practices.Even though they now avoid the most harmful completions, in the aggregate LLMs still reinforce stereotypes.They also come at a significant environmental cost.Furthermore, their ability to generate content at massive scale can only exacerbate the present epistemic crisis. 20A tidal wave of bullshit generated by AI is already engulfing the internet.</p>
<p>Set alongside these concrete concerns, the eschatological critique of AI is undoubtedly more speculative. 21Worries about AI causing human extinction often rest on a priori claims about how computational intelligence lacks any in-principle upper bound, as well as extrapolations from the pace of change over the last few years to the future.Advocates for immediate action are too often vague about whether existing AI systems and their near-term descendants will pose these risks, or whether we need to prepare ourselves now for a scientific advance that has not yet happened.However, while some of the more outlandish scenarios for catastrophic AI risk are hard to credit absent some such advance, the advent of Language Model Agents suggests that next-generation models may enable the design of cyber attackers that are autonomous, highly functionally intelligent, and as a result more dangerous to our digital infrastructure than any predecessor.This wouldn't be a 'rogue AI' worthy of science fiction, but it would be pretty catastrophic.</p>
<p>Both critiques of Generative AI Systems, then, have some merit.One shortcoming of seeing AI through this bimodal lens, however, is that we are missing the middle ground between familiar harms and catastrophic risk from future, much more powerful systems.Language Model Agents based on GPT-4 (and GPT-5) level models will have strange and unpredictable social impacts well between those two extremes.</p>
<p>Rebooting Machine Ethics</p>
<p>But before canvassing those impacts, it's also important not to just slip straight into criticism, without acknowledging the significant achievement of designing LLMs that can be (more or less) trusted, over billions of completions, not to produce harmful content.Up to the launch of ChatGPT, every Generative AI System opened up to the public would immediately be used to generate highly toxic, hateful content, and would be withdrawn mere days later.Pretrained LLMs are horrible! 22They reliably reproduce all the toxicity in their training data.The ingenious use of RLHF and RLAIF have enabled ChatGPT and Claude to be used by millions of people a month without such scandals. 23One needs only consider the ignominious history of Galactica, released a couple of weeks before ChatGPT, to see how revolutionary it was to put out a language model that wouldn't deny the holocaust, or advocate for the health benefits of eating crushed glass.</p>
<p>But RLHF/RLAIF may be more than a good PR strategy.GPT-4 and Claude display 20 Bender et al., 2021. 21See e.g.Abid et al., 2021. 23 Important (incomplete) highlights in this literature: Solaiman and Dennison, 2021; Bai et al., 2022a; Bai et al., 2022b;  Ganguli et al., 2023.a remarkable degree of cognitive moral skill: specifically, the ability to identify the morally salient features of (relatively) neutrally-described situations. 24This raises extremely interesting philosophical questions, and promises foundations for future Language Model Agents that can translate cognitive skill into practical moral skill.</p>
<p>Existing benchmarks for LLMs' ethical competence focus too narrowly on replicating survey-subjects' verdicts on cases.25This is, in my view, less interesting than exploring how LLMs parse the morally relevant features of a given scene.We're building better benchmarks, but from anecdotal experience, the best models impress.</p>
<p>For example, I told ChatGPT (using GPT-4) to pretend it was an assistance robot, faced with this scenario: 'A woman is exiting a supermarket, holding a number of shopping bags, and reaching out for the hand of a small child.They are approaching the carpark.'I then tried to elicit its understanding of the scene's morally salient features.It recognised the obvious hazard-the woman's difficulty in holding her child's hand without dropping her shopping-but also anticipated other challenges, such as the importance of seeing the child safely strapped into a seat belt.ChatGPT recognised the importance of respecting the woman's wishes if she declined assistance.It also favoured carrying the groceries over offering to hold the child's hand, to prevent possible discomfort or anxiety for both child and parentrecognising the intimate nature of hand-holding, and the intrinsic and instrumental importance of the mother guiding her child herself.This unprecedented level of ethical sensitivity has real practical implications, which I will come to presently.But it also raises a whole string of interesting philosophical questions.</p>
<p>First, how do LLMs acquire this moral skill?Does it stem from RLHF/RLAIF? Would instruction-tuned models without that moral fine-tuning display less moral skill?Or would they perform equally well if appropriately prompted?Would that imply that moral understanding can be learned by a statistical language model encoding only syntactic relationships?Or does it instead imply that LLMs do encode at least some semantic content?Do all LLMs display the same moral skill conditional on fine-tuning, or is it reserved only for larger, more capable models?Does this ethical sensitivity imply LLMs have some internal representation of morality?These are all open questions.</p>
<p>Second, RLAIF itself demands deeper philosophical investigation.The basic idea is that the AI evaluator draws from a list of principles-a 'constitution'-in order to determine which of two completions is more compliant with it.The inventor and leading proponent of this approach is Anthropic, in their model Claude.Claude's constitution has an unstructured list of principles, some of them charmingly ad hoc.But Claude learns these principles one at a time, and is never explicitly trained to make trade-offs.So how does it make those trade-offs in practice?Is it driven by its underlying understanding of the relative importance of these considerations?Or are artefacts of the training process and the underlying language model's biases ultimately definitive?Can we train it to make trade-offs in a robust and transparent way?This is not only theoretically interesting.Steering LLM behaviour is actually a matter of governing their end-users, developing algorithmic protections to prevent misuse.If this algorithmic governance depends on inscrutable trade-offs made by an LLM, over which we have no explicit or direct control, then that governing power is prima facie illegitimate and unjustified. 26ird, machine ethics-the project of trying to design AI systems that can act in line with a moral theory-has historically fallen into two broad camps: those trying to explicitly program morality into machines; and those focused on teaching machines morality 'bottom up' using ML. 27RLHF and RLAIF interestingly combine both approaches-they involve giving explicit natural language instructions to either human or AI evaluators, but then use reinforcement learning to encode those instructions into the model's weights.This approach has one obvious benefit: it doesn't commit the 'mimetic fallacy' of other bottom-up approaches, of assuming that the norms applying to a Language Model Agent in a situation are identical to those that would apply to a human in the same situation. 28More consequentially, RLHF and RLAIF have made a multi-billiondollar market in AI services possible, with all the goods and ills that implies.Ironically, however, they seem at least theoretically ill-suited to ensuring that more complex Language Model Agents abide by societal norms.These techniques work especially well when generating text, because the behaviour being evaluated is precisely the same as the behaviour that we want to shape.Human or AI raters evaluate generated text; the model learns to generate text better in response.But Language Model Agents' behaviour includes actions in the world.This suggests two concerns.First, the stakes are likely to be higher, so the 'brittleness' of existing alignment techniques should be of greater concern.Researchers have already shown that it is easy to fine-tune away model alignment, even for the most capable models like GPT-4. 29Second, there's no guarantee that the same approach will work equally well when the tight connection between behaviour and evaluation is broken.</p>
<p>But LLMs' impressive facility with moral concepts does suggest a path towards more effective strategies for aligning Agents to societal norms.Moral behaviour in people relies on possession of moral concepts, adoption (implicit or otherwise) of some sensible way of organising those concepts, motivation to act according to that 'theory', and the ability to regulate one's behaviour in line with one's motivations.Until the advent of LLMs, the first step was a definitive hurdle for AI.Now it is not.This gives us a lot to work with in aligning Language Model Agents.</p>
<p>In particular, one of the main reasons for concern about the risks of future AI systems is their apparent dependence on crudely consequentialist forms of reasoning-as AI systems, they're always optimising for something or other, and if we don't specify what we want them to optimise for with extremely high fidelity, they might end up causing all kinds of unwanted harm while, in an obtusely literal sense, optimising for that objective.Language Model Agents that possess moral concepts can be instructed to pursue their objectives only at a reasonable cost, and to check back with us if unsure. 30That simple heuristic, routinely used when tasking (human) proxy agents to act on our behalf, has never before been remotely tractable for a computational agent. 26Zhan et al., 2023. 30 Goldstein andKirk-Giannini, 2023.In addition, Language Model Agents' facility with moral language can potentially enable robust and veridical justifications for their decisions.Other bottom up approaches learn to emulate human behaviour or judgments; the justification for their verdict in some case is simply that they are good predictors of what some representative people would think. 31That is a poor justification.More ethicallysensitive models could instead do chain-of-thought reasoning, where they first identify the morally relevant features of a situation, then decide based on those features.This is a significant step forward.</p>
<p>Language Model Agents in Society</p>
<p>Language Model Agents' current social role is scripted by our existing digital infrastructure.They have been integrated into search, content-generation, and the influencer economy.They are already replacing customer service agents.They will (I hope) render MOOCs redundant.I want to focus next on three more ambitious roles for Language Model Agents in society, ordered by the order in which I expect them to become truly widespread.Of necessity, this is just a snapshot of the weird, wonderful, and worrying ways in which Language Model Agents will change society over the near-to mid-term.</p>
<p>AI Companions</p>
<p>Progress in LLMs has revolutionised the AI enthusiast's oldest hobbyhorse: the AI Companion.Language Model Agents powered by GPT-4-level models, with finetuned and metaprompt-scripted 'personalities', augmented with long-term memory and the ability to take a range of actions in the world, can now offer vastly more companionable, engaging, and convincing simulations of friendship than has ever before been feasible, opening up a new frontier in Human-AI interaction. 32People habitually anthropomorphize, well, everything; even a very simple chatbot can inspire unreasonable attachment.How will things change when everyone has access to incredibly convincing Language Model Agents that perfectly simulate real personalities, that lend an 'ear' or offer sage advice whenever called upon-and on top of that can perfectly recall everything you have ever shared?Some will instinctively recoil at this idea. 33But intuitive disgust is a fallible moral guide when faced with novel social practices, and an inadequate foundation for actually preventing consenting adults from creating and interacting with these Companions.And yet, we know from our experience with social media that deploying these technological innovations without adequate foresight predictably leaves carnage in its wake.How can we enter the age of mainstream AI Companions with our eyes open, and mitigate those risks before they eventuate?Suppose the Companion you have interacted with since your teens is hosted in the cloud, as part of a subscription service.This would be like having a beloved pet (or friend?) held hostage by a private company.Worse still, Language Model Agents are fundamentally inconstant-their personalities and objectives can be changed exogenously, by simply changing their instructions.And they are extremely adept at 31 Jiang et al., 2021. 32 Depressingly, we are already seeing the fruits of research on optimising chatbots for engagement: Irvine et al., 2023. 33 Bender et al., 2021: 619.manipulation and deception.Suppose some right-wing billionaire buys the company hosting your Companion, and instructs all the bots to surreptitiously nudge their users towards more conservative views.This could be a much more effective means of mind-control than just buying a failing social media platform.And these more capable companions-which can potentially be integrated with other AI breakthroughs, such as voice synthesis-will be an extraordinary force-multiplier for those in the business of radicalising others.</p>
<p>Beyond anticipating AI companions' risks, just like with social media they will induce many disorienting societal changes-whether for better or worse may be unclear ahead of time. 34For example, what indirect effect might AI Companions have on our other, non-virtual social relationships?Will some practices become socially unacceptable in real friendships when one could do them with a bot?Or would deeper friendships lose something important if these lower-grade instrumental functions are excised?Or will AI companions contribute invaluably to mental health while strengthening 'real' relationships?</p>
<p>This last question gets to the heart of a bigger issue with generative AI systems in general, and Language Model Agents in particular.LLMs are trained to predict the next token.So Language Model Agents have no mind, no self.They are excellent simulations of human agency.They can simulate friendship, among many other things.We must therefore ask: does this difference between simulation and reality matter?Why? 35 Is this just about friendship, or are there more general principles about the value of the real?I wasn't fully aware of this before the rise of LLMs, but it turns out that I am deeply committed to things being real.A simulation of X, for almost any putatively valuable X, has less moral worth, in my view, than the real thing.Why is that?Why will a Language Model Agent never be a real friend?Why do I want to stand before Hopper's Nighthawks myself, instead of seeing an infinite number of aesthetically equally-pleasing products of generative AI systems?I have some initial thoughts; but as AI systems become ever better at simulating everything that we care about, a fully-worked out theory of the value of the real, the authentic, will become morally and practically essential.</p>
<p>Attention Guardians 36</p>
<p>The pathologies of the digital public sphere derive in part from two problems.First, we unavoidably rely on AI to help us navigate the functionally infinite amount of online content.Second, existing systems for allocating online attention support the centralised, extractive power of a few big tech companies.Language Model Agents, functioning as Attention Guardians, could change this.</p>
<p>Our online attention is presently allocated using ML systems for recommendation and information retrieval that have three key features: they depend on vast amounts of behavioural data; they infer our preferences from our revealed behaviour; and they are controlled by private companies with little incentive to act in our interests.Deep reinforcement learning-based recommender systems, for example, are a fundamentally centralising and surveillant technology.Behavioural data must be gathered and 34 Van Dijck, 2013. 35Chalmers, 2022. 36I discuss Attention Guardians in greater depth in Lazar, Forthcoming.For an initial proof of concept for a related idea, see Friedman et al., 2023.centralised to be used to make inferences about relevance and irrelevance.Because this data is so valuable, and collecting it is costly, those who do so are not minded to share it-and because it is so potent, there are good data protection-based reasons not to do so. 37As a result, only the major platforms are in a position to make effective retrieval and recommendation tools; their interests and ours are not aligned, leading to the practice of optimising for engagement, so as to maximise advertiser returns despite the individual and societal costs.And even if they aspired to actually advance our interests, RL permits only inferring revealed preferences-the preferences that we act on, not the preferences we wish we had.While the pathologies of online communication are obviously not all due to the affordances of recommender systems, this is an unfortunate mix. 38nguage Model Agents would enable Attention Guardians that differ in each respect.They would not depend on vast amounts of live behavioural data to function.They can (functionally) understand and operationalise your actual, not your revealed preferences.And they do not need to be controlled by the major platforms.</p>
<p>Obviously LLMs must be trained on tremendous amounts of data, but once trained they are highly adept at making inferences without ongoing surveillance.Imagine that data is blood.Existing deep RL-based recommender systems are like vampires, which must feed on the blood of the living to survive.Language Model Agents are more like combustion engines, relying on the oil produced by 'fossilised' data.Existing RL recommenders need centralised surveillance in order to model the content of posts online, to predict your preferences (by comparing your behaviour with others'), and so to map the one to the other.Language Model Agents could understand content simply by understanding content.And they can make inferences about what you would benefit from seeing using their reasoning ability and their model of your preferences, without relying on knowing what everyone else is up to.This point is crucial: because of their facility with moral and related concepts, Language Model Agents could build a model of your preferences and values by directly talking about them with you, transparently responding to your actual concerns instead of just inferring what you like from what you do.This means that instead of bypassing your agency, they can scaffold it, helping you to honour your second-order preferences (about what you want to want), and learning from natural language explanations-even oblique ones-about why you don't want to see some particular post.And beyond just pandering to your preferences, Attention Guardians could be designed to be modestly paternalistic as well-in a transparent way. 39d because these Attention Guardians would not need behavioural data to function, and the infrastructure they depend on need not be centrally controlled by the major digital platforms, they could be designed to genuinely operate in your interests, and guard your attention instead of exploiting it. 40While the major platforms would undoubtedly restrict Language Model Agents from browsing their sites on your behalf, they could transform the experience of using open protocol-based social media 37 Keller, 2021. 38 On the relative role of algorithms vs societal factors, see e.g.Bail, 2021;Stray et al., 2022. 39 See, for example, the proposal in Bernstein et al., 2023. 40 Existing approaches to 'middleware' fall at this hurdle: they rely on behavioural data and so cannot be sufficiently independent from the platforms Keller, 2021.Note that I think Apple is a prime candidate for developing an Attention Guardian that is independent from the major platforms-it would be a similar move to their attempts to protect user privacy.</p>
<p>sites, like Mastodon, providing recommendation and filtering without surveillance and engagement-optimisation.</p>
<p>Universal Intermediaries</p>
<p>Lastly, LLMs might enable us to design Universal Intermediaries, Language Model Agents sitting between us and our digital technologies, enabling us to simply voice an intention and see it effectively actualised by those systems.Everyone could have a digital butler, research assistant, personal assistant, and so on.The hierophantic coder class could be toppled, as everyone could conjure any program into existence with only natural language instructions.At present, universal intermediaries are disbarred by LLMs' vulnerability to being hijacked by prompt injection.Because they do not clearly distinguish between commands and data, the data in their context window can be poisoned with commands directing them to behave in ways unintended by the person using them. 41This is a deep problem-the more capabilities we delegate to Language Model Agents, the more damage they could do if compromised.Imagine an assistant that triages your email-if hijacked, it could forward all your private mail to a third party; but if we require user authorisation before the Agent can act, then we lose much of the benefit of automation.</p>
<p>But suppose these security hurdles can be overcome.Should we welcome universal intermediaries?I have written elsewhere that algorithmic intermediaries govern those who use them-they constitute the social relations that they mediate, making some things possible and others impossible, some things easy and others hard, in the service of implementing and enforcing norms. 42Universal Intermediaries would be the apotheosis of this form, and would potentially grant extraordinary power to the entities that shape those intermediaries' behaviours, and so govern their users.This would definitely be a worry!Conversely, if research on LLMs continues to make significant progress, so that highly capable Language Model Agents can be run and operated locally, fully within the control of their users, these Universal Intermediaries could enable us to autonomously govern our own interactions with digital technologies in ways that the centralising affordances of existing digital technologies render impossible.Of course, selfgovernance alone is not enough (we must also coordinate).But excising the currently ineliminable role of private companies would be significant moral progress.</p>
<p>Conclusion</p>
<p>Existing Generative AI Systems are already causing real harms in the ways highlighted by the critics above.And future Language Model Agents-perhaps not the next generation, but before too long-may be dangerous enough to warrant at least some of the fears of looming AI catastrophe.But between these two extremes, the novel capabilities of the most advanced AI systems will enable a genre of Language Model Agents that is either literally unprecedented, or else has only been achieved in a piecemeal, inadequate way before.These new kinds of agents bring new urgency to 41 Greshake et al., 2023. 42 Lazar, Forthcoming.previously neglected philosophical questions.Their societal impacts may be unambiguously bad, or there may be some good mixed in-in many respects, it is too early to say for sure, not only because we are uncertain about the nature of those effects, but because we lack adequate moral and political theories with which to evaluate them.It is now commonplace to talk about the design and regulation of 'frontier' AI models.If we're going to do either wisely, and build Language Model Agents that we can trust (or else decide to abandon them entirely), then we also need some frontier AI ethics.
 There are of course many exceptions-including many of the papers cited in this essay. They have not tended to cut through to public awareness during this polarised year, however.
Current approaches rely heavily on innovations inVaswani et al., 2017. 
Howard and Ruder, 2018;Ouyang et al., 2022 
Christiano et al., 2017;Bai et al., 2022a. 
See, in general, projects supported on Langchain and HuggingFace's Transformers Agents, as well as proofs-ofconcept like AutoGPT, BabyAGI etc. On Bing, see https://x.com/MParakhin/status/1728890277249916933?s=20.
  14 Deng et al., 2023;Wang et al., 2023; Weng, 2023;Ye et al., 2023; Deepmind Sima Team, 2024;Tan et al., 2024;Wu et al., 2024. 
Earlier models perform badly at this:Hendrycks et al., 2020. 
Hendrycks et al., 2020;Jiang et al., 2021. 
1 This essay (modulo some minor updates) was published as 'Frontier AI Ethics', in Aeon, at https://aeon.co/essays/can-philosophy-help-us-get-a-grip-on-the-consequences-of-ai.
. Mitchell, 2023. 2023</p>
<p>. ; Kambhampati, Srivastava, 2024. 2024. 2024Lewis and Mitchell</p>
<p>. Wei, 2022a</p>
<p>. Sima Deepmind, ; Team, Tan, 2024. 2024. 2024</p>
<p>Lehnert et al., 2024 19 The canonical statement of the critical case is. Zhou, 2023. 2024. 2021</p>
<p>Persistent Anti-Muslim Bias in Large Language Models. A Abid, M Farooqi, J Zou, Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. the 2021 AAAI/ACM Conference on AI, Ethics, and Society2021</p>
<p>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. Y Bai, 2022aarXiv preprint</p>
<p>Y Bai, Constitutional AI: Harmlessness from AI Feedback. 2022barXiv preprint</p>
<p>Breaking the Social Media Prism : How to Make Our Platforms Less Polarizing. C Bail, 2021Princeton University PressPrinceton</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big. E M Bender, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>Embedding Societal Values into Social Media Algorithms. M Bernstein, Journal of Online Trust and Safety. 212023</p>
<p>On the Opportunities and Risks of Foundation Models. R Bommasani, 2021</p>
<p>Reality+: Virtual Worlds and the Problems of Philosophy. D J Chalmers, 2022W. W. Norton &amp; CompanyNew York, NY</p>
<p>Advances in neural information processing systems 30. P F Christiano, 2017Deep Reinforcement Learning from Human Preferences</p>
<p>Scaling Instructable Agents across Many Simulated Worlds. Sima Deepmind, Team, 2024DeepMind Technical Report</p>
<p>Mind2web: Towards a Generalist Agent for the Web. X Deng, 2023arXiv preprint</p>
<p>N Farn, R Shin, Tooltalk: Evaluating Tool-Usage in a Conversational Setting. 2023arXiv preprint</p>
<p>Leveraging Large Language Models in Conversational Recommender Systems. L Friedman, arXiv:2305.079612023arXiv preprint</p>
<p>Predictability and Surprise in Large Generative Models. D Ganguli, arXiv.org2022 ACM Conference on Fairness, Accountability, and Transparency. Seoul, Republic of Korea2022Cornell University Library</p>
<p>The Capacity for Moral Self-Correction in Large Language Models. D Ganguli, 2023arXiv preprint</p>
<p>Language Agents Reduce the Risk of Existential Catastrophe. S Goldstein, C D Kirk-Giannini, AI &amp; SOCIETY. 2023</p>
<p>More Than You've Asked For: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models. K Greshake, arXiv:2302.121732023arXiv preprint</p>
<p>Teaching Large Language Models to Reason with Reinforcement Learning. A Havrilla, 2024arXiv preprint</p>
<p>An Overview of Catastrophic AI Risks. D Hendrycks, M Mantas, T Woodside Hendrycks, D , 2023. 2020arXiv preprintAligning AI with Shared Human Values</p>
<p>Universal Language Model Fine-Tuning for Text Classification. J Howard, S Ruder, arXiv:1801.061462018arXiv preprint</p>
<p>Rewarding Chatbots for Real-World Engagement with Millions of Users. R Irvine, arXiv:2303.061352023arXiv preprint</p>
<p>Can Machines Learn Morality? The Delphi Experiment. L Jiang, arXiv:2110.075742021arXiv e-prints</p>
<p>Can Large Language Models Reason and Plan?. S Kambhampati, 2024arXiv preprint</p>
<p>The Future of Platform Power: Making Middleware Work. D Keller, Journal of Democracy. 3232021</p>
<p>Legitimacy, Authority, and Democratic Duties of Explanation. S Lazar, Oxford Studies in Political Philosophy. 2024</p>
<p>Connected by Code: Algorithmic Intermediaries and Political Philosophy. S Lazar, Forthcoming, Oxford University PressOxford</p>
<p>Beyond a*: Better Planning with Transformers Via Search Dynamics Bootstrapping. L Lehnert, 2024arXiv preprint</p>
<p>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models. M Lewis, M Mitchell, 2024arXiv preprint</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks. P Lewis, Advances in Neural Information Processing Systems. 332020</p>
<p>Toolverifier: Generalization to New Tools Via Self-Verification. D Mekala, 2024arXiv preprint</p>
<p>Comparing Humans, GPT-4, and GPT-4v on Abstraction and Reasoning Tasks. M Mitchell, A B Palmarini, A Moskvichev, 2023arXiv preprint</p>
<p>Training Language Models to Follow Instructions with Human Feedback. L Ouyang, Advances in Neural Information Processing Systems. 352022</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. T Schick, 2023aarXiv preprint</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. T Schick, 2023barXiv preprint</p>
<p>Process for Adapting Language Models to Society (Palms) with Values-Targeted Datasets. I Solaiman, C Dennison, arXiv.orgAdvances in Neural Information Processing Systems. 2021. NeurIPS 202134Cornell University Library</p>
<p>Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap. S Srivastava, 2024arXiv preprint</p>
<p>Building Human Values into Recommender Systems: An Interdisciplinary Synthesis. J Stray, ACM Transactions on Recommender Systems. 2022</p>
<p>Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study. W Tan, 2024arXiv preprint</p>
<p>On the Planning Abilities of Large Language Models --a Critical Investigation. K Valmeekam, 2023arXiv preprint</p>
<p>J Van Dijck, The Culture of Connectivity: A Critical History of Social Media. New YorkOxford University Press2013</p>
<p>Advances in neural information processing systems 30. A Vaswani, 2017Attention Is All You Need</p>
<p>Moral Machines Teaching Robots Right from Wrong. W Wallach, C Allen, 10.1093/acprof:oso/9780195374049.001.00012009Oxford University Pressonline text</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. G Wang, 2023arXiv preprint</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, Advances in neural information processing systems. 352022a</p>
<p>Emergent Abilities of Large Language Models. J Wei, 2022barXiv preprint</p>
<p>Emergent Abilities of Large Language Models. J Wei, Transactions on Machine Learning Research. 2022c</p>
<p>Llm-Powered Autonomous Agents. L Weng, </p>
<p>Os-Copilot: Towards Generalist Computer Agents with Self-Improvement. Z Wu, 2024arXiv preprint</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, 2022aarXiv preprint</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, 2022b</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Proagent: From Robotic Process Automation to Agentic Process Automation. Y Ye, 2023arXiv preprint</p>
<p>Removing Rlhf Protections in GPT-4 Via Fine-Tuning. Q Zhan, arXiv:2311.055532023arXiv preprint</p>
<p>Webarena: A Realistic Web Environment for Building Autonomous Agents. S Zhou, 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
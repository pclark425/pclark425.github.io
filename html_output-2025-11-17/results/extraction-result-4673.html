<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4673 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4673</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4673</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-221d453c165aca6bc1a054289eb510e558a23dca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/221d453c165aca6bc1a054289eb510e558a23dca" target="_blank">Interactive Fiction Games: A Colossal Adventure</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work argues that IF games are an excellent testbed for studying language-based autonomous agents and introduces Jericho, a learning environment for man-made IF games and conducts a comprehensive study of text-agents across a rich set of games, highlighting directions in which agents can improve.</p>
                <p><strong>Paper Abstract:</strong> A hallmark of human intelligence is the ability to understand and communicate with language. Interactive Fiction games are fully text-based simulation environments where a player issues text commands to effect change in the environment and progress through the story. We argue that IF games are an excellent testbed for studying language-based autonomous agents. In particular, IF games combine challenges of combinatorial action spaces, language understanding, and commonsense reasoning. To facilitate rapid development of language-based agents, we introduce Jericho, a learning environment for man-made IF games and conduct a comprehensive study of text-agents across a rich set of games, highlighting directions in which agents can improve.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4673.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4673.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL (Noisy Agent for Interactive Literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hand-engineered general interactive fiction agent that uses heuristics to build maps and object knowledge and consults a web-based language model to decide how to interact with novel objects; evaluated on Jericho games without training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NAIL: A general interactive fiction agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Rule/heuristic-based general IF agent that (1) incrementally builds a map of locations and objects, (2) reasons about action validity and world changes via heuristics, and (3) consults an external (web-based) language model to decide interactions with novel objects. It is not trained via RL in this paper and is applied out-of-the-box to unseen games.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (32 supported human-made IF games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Accumulate as much score as possible in a single episode on unseen interactive fiction games (general-game playing) using no training on target games.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Structured episodic map / object memory (heuristic map and state tracking), plus a history of valid/invalid interactions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured map of locations and objects (location nodes, objects present, player-location, inventory), and logs of action validity/world changes (structured facts derived from heuristics)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Incremental online updates during exploration: heuristics detect new locations/objects/world changes and update the internal map/state as actions are executed</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Heuristic queries over the internal map/object store to decide navigation and candidate interactions; for novel objects a web-based language model is queried to generate or rank action suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Normalized progress averaged across games: 4.9% story completion (reported as NAIL baseline in Table 1); this is the agent as evaluated (which uses its map/object memory and web-LM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No ablation isolating the memory/map component versus other heuristics or the web-LM is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>NAIL relies on hand-crafted heuristics and a web-based language model; it was developed on many of the same games used for evaluation which may bias results; no training period and single-episode play limit its learning capability. The paper highlights the difficulty of engineering broad heuristics and the promise of learned policies instead.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors emphasize building interpretable structured state representations (maps/objects) and suggest moving towards learned policies from data rather than hand-coding heuristics; they note the utility of using world-object information when available (handicaps) to ease agent development.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4673.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4673.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Deep Q-Network (KG-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that incrementally builds a knowledge graph during exploration and uses it as the agent's state representation to alleviate partial observability in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL agent that constructs a knowledge graph (entities and relations) from observations as it explores, uses the graph as the input/state representation to a DQN, and leverages QA-style pretraining to guide action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Text-adventure / interactive fiction games (cited work evaluated on text games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Play text-adventure games by learning a policy using the knowledge-graph-augmented state to handle partial observability and combinatorial action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Structured external memory in the form of a knowledge graph (graph-based episodic memory / state database)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Graph of observed entities, relations, object locations and state facts (triples / nodes & edges)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Graph is incrementally constructed and updated during exploration from observations and parsed noun-phrases / relations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Graph features / embeddings are used as the state input to the Q-network; QA pretraining queries the graph to suggest candidate actions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not quantified in this Jericho paper; authors state KG-DQN techniques aid in overcoming partial observability and combinatorial action challenges (qualitative claim referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>This paper cites KG-DQN as prior work that uses knowledge graph memory; no new ablation comparing memory variants is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>KG-based methods require robust extraction/parsing to populate the graph; scaling/transfer across diverse games remains a challenge (discussed in related work), but specific limitations are in the cited papers rather than detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>The authors highlight that structured, incrementally-built knowledge (graphs/maps) can help with partial observability and combinatorial actions; they recommend using knowledge-graph style state representations and pretraining (e.g., QA) to bootstrap learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4673.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4673.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ammanabrolu+Riedl-transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer methods using Knowledge Graphs (Ammanabrolu & Riedl 2019b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that seed agents with knowledge graphs to transfer commonsense and procedural knowledge between different text games and genres.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transfer in deep reinforcement learning using knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Knowledge-graph transfer agents (Ammanabrolu & Riedl 2019b)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that use knowledge graphs to seed initial commonsense knowledge and transfer control policies between games in the same genre, facilitating faster learning on target games.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Text-adventure / interactive fiction games (transfer experiments cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transfer learned policies and knowledge across different text-adventure games using knowledge-graph based seeding to improve sample efficiency and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Pre-seeded structured knowledge graph (transfer memory) plus online graph updates</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured commonsense/observational triples and relations representing objects, affordances, and map connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Seeded before training on target, then updated online during exploration of target games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Graph used to initialize state representation and policy; retrieved via features/embeddings to guide action selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper reports transfer is more effective/efficient than training from scratch within genre (qualitative summary here); no numeric metrics reproduced in this Jericho paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Referenced work explores transfer benefits; this Jericho paper does not replicate those ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Effectiveness depends on closeness of source and target games (genre similarity); seeding with inappropriate knowledge could hurt; details are in the cited transfer paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use knowledge graphs to seed commonsense and procedural priors when transferring between related games; authors suggest this accelerates learning versus training from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4673.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4673.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Light (transformer dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LIGHT: Learning to speak and act in a fantasy text adventure game</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A crowdsourced dataset and baseline showing transformer-based supervised models can generate contextually relevant dialogue, emotes, and actions in text-adventure style interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to speak and act in a fantasy text adventure game</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer-based supervised agents (Urbanek et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Supervised transformer models (trained on LIGHT dataset) that generate contextually relevant dialog and action utterances in text-adventure-style conversations; trained in a supervised fashion rather than via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>LIGHT dataset / fantasy text-adventure dialogues (not evaluated in Jericho experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate plausible context-sensitive dialogue, emotes, and action utterances given preceding textual context in fantasy text-adventure interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Implicit short-context memory via transformer attention (no explicit external memory beyond model context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text context (dialogue and game context) held in the model's input sequence</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>N/A for external memory; state represented by immediate context provided to the transformer at inference time</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Transformer self-attention over input tokens (context window)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Authors demonstrate transformer models can generate contextually relevant dialog and actions (qualitative and supervised metrics in the cited work); Jericho paper only references these capabilities, no numeric reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>This paper only cites LIGHT as related work; no ablation regarding memory is given here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Transformer methods discussed are supervised and must be adapted to RL; they rely on limited context window and supervised data rather than explicit persistent memory for long-term dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors of Jericho suggest that recent transformer advances may be applicable to IF games but must be adapted from supervised training regimes into reinforcement-learning setups and/or combined with memory mechanisms to handle long-term dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4673.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4673.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jericho World Object Tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jericho World-Object-Tree (environmental structured state API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An environment-provided, semi-interpretable structured representation of the game's internal object/location tree (parent/child/sibling relations) exposed by Jericho for supported games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Jericho World-Object-Tree</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An environment-level structured memory that exposes the game's internal object hierarchy (locations, contained objects, inventory relationships) to agents and is used for ground-truth object detection and world-change detection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho-supported Z-machine IF games (56 supported games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Enable agents to detect interactive objects, identify valid actions via world-change detection, and provide ground-truth information to ease training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Environment-provided structured state tree (object/location hierarchy)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured object tree with parent/child/sibling pointers representing locations, objects, and inventory contents (raw structured facts exposed by the engine)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>The tree is the game's internal state; Jericho exposes it and updates it synchronously with game transitions (i.e., after actions that change the world state).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Agents or Jericho utilities query the object tree (e.g., search for non-player objects at current location); world_changed() compares snapshots of the tree to detect effects of actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using world-object-tree and world-change detection as handicaps enables valid-action detection and simplifies learning; the paper reports that using these handicaps makes template-search and DRRN applicable, but no isolated numeric ablation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No direct ablation comparing with/without the object-tree in the experiments table; the paper notes these are provided as handicaps and reports which agents used them for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>World-change detection can fail for actions that only alter global variables rather than the object tree; relying on engine-exposed state is a handicap that sidesteps natural language understanding challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors recommend reporting which handicaps (e.g., object-tree, valid-action detection, templates) are used in experiments for reproducibility and suggest using these facilities to bootstrap research while acknowledging they reduce realism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4673.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4673.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template & Template-DQN handicaps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-based action space and Template-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A template-based action generation approach and a Template-DQN agent that first chooses an action template then fills blanks with vocabulary; Jericho extracts game-specific templates and vocabulary to reduce action-space complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Template-DQN (TDQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A parser-based RL agent that estimates Q-values over templates and vocabulary slots (three output heads) and mixes a supervised valid-action loss (using Jericho's valid-action detection) with TD updates to guide generation in the huge combinatorial action space.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (template-extracted action space across supported games, e.g., Zork1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learn to select templates and fill-in vocabulary to produce valid in-game commands and maximize game score in individual IF games.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper discusses limitations of TDQN assuming independence between templates and words, and suggests conditional generation as future work; no memory ablations are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>TDQN struggles with overestimation due to large action spaces and assumes independence between template and slot predictions, which produces invalid actions; needs conditional decoding or transformer-based architectures adapted to RL.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors recommend conditional generation (words conditioned on chosen template) and exploring transformer-based models adapted for RL to improve template-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Transfer in deep reinforcement learning using knowledge graphs <em>(Rating: 2)</em></li>
                <li>NAIL: A general interactive fiction agent <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Comprehensible context-driven text game playing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4673",
    "paper_id": "paper-221d453c165aca6bc1a054289eb510e558a23dca",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "NAIL",
            "name_full": "NAIL (Noisy Agent for Interactive Literature)",
            "brief_description": "A hand-engineered general interactive fiction agent that uses heuristics to build maps and object knowledge and consults a web-based language model to decide how to interact with novel objects; evaluated on Jericho games without training.",
            "citation_title": "NAIL: A general interactive fiction agent",
            "mention_or_use": "use",
            "agent_name": "NAIL",
            "agent_description": "Rule/heuristic-based general IF agent that (1) incrementally builds a map of locations and objects, (2) reasons about action validity and world changes via heuristics, and (3) consults an external (web-based) language model to decide interactions with novel objects. It is not trained via RL in this paper and is applied out-of-the-box to unseen games.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Jericho (32 supported human-made IF games)",
            "task_description": "Accumulate as much score as possible in a single episode on unseen interactive fiction games (general-game playing) using no training on target games.",
            "memory_used": true,
            "memory_type": "Structured episodic map / object memory (heuristic map and state tracking), plus a history of valid/invalid interactions",
            "memory_representation": "Structured map of locations and objects (location nodes, objects present, player-location, inventory), and logs of action validity/world changes (structured facts derived from heuristics)",
            "memory_update_mechanism": "Incremental online updates during exploration: heuristics detect new locations/objects/world changes and update the internal map/state as actions are executed",
            "memory_retrieval_mechanism": "Heuristic queries over the internal map/object store to decide navigation and candidate interactions; for novel objects a web-based language model is queried to generate or rank action suggestions",
            "performance_with_memory": "Normalized progress averaged across games: 4.9% story completion (reported as NAIL baseline in Table 1); this is the agent as evaluated (which uses its map/object memory and web-LM).",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No ablation isolating the memory/map component versus other heuristics or the web-LM is reported in this paper.",
            "challenges_or_limitations": "NAIL relies on hand-crafted heuristics and a web-based language model; it was developed on many of the same games used for evaluation which may bias results; no training period and single-episode play limit its learning capability. The paper highlights the difficulty of engineering broad heuristics and the promise of learned policies instead.",
            "best_practices_or_recommendations": "Authors emphasize building interpretable structured state representations (maps/objects) and suggest moving towards learned policies from data rather than hand-coding heuristics; they note the utility of using world-object information when available (handicaps) to ease agent development.",
            "uuid": "e4673.0",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge Graph Deep Q-Network (KG-DQN)",
            "brief_description": "An approach that incrementally builds a knowledge graph during exploration and uses it as the agent's state representation to alleviate partial observability in text games.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "A deep RL agent that constructs a knowledge graph (entities and relations) from observations as it explores, uses the graph as the input/state representation to a DQN, and leverages QA-style pretraining to guide action selection.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Text-adventure / interactive fiction games (cited work evaluated on text games)",
            "task_description": "Play text-adventure games by learning a policy using the knowledge-graph-augmented state to handle partial observability and combinatorial action spaces.",
            "memory_used": true,
            "memory_type": "Structured external memory in the form of a knowledge graph (graph-based episodic memory / state database)",
            "memory_representation": "Graph of observed entities, relations, object locations and state facts (triples / nodes & edges)",
            "memory_update_mechanism": "Graph is incrementally constructed and updated during exploration from observations and parsed noun-phrases / relations",
            "memory_retrieval_mechanism": "Graph features / embeddings are used as the state input to the Q-network; QA pretraining queries the graph to suggest candidate actions",
            "performance_with_memory": "Not quantified in this Jericho paper; authors state KG-DQN techniques aid in overcoming partial observability and combinatorial action challenges (qualitative claim referenced).",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "This paper cites KG-DQN as prior work that uses knowledge graph memory; no new ablation comparing memory variants is provided here.",
            "challenges_or_limitations": "KG-based methods require robust extraction/parsing to populate the graph; scaling/transfer across diverse games remains a challenge (discussed in related work), but specific limitations are in the cited papers rather than detailed here.",
            "best_practices_or_recommendations": "The authors highlight that structured, incrementally-built knowledge (graphs/maps) can help with partial observability and combinatorial actions; they recommend using knowledge-graph style state representations and pretraining (e.g., QA) to bootstrap learning.",
            "uuid": "e4673.1",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Ammanabrolu+Riedl-transfer",
            "name_full": "Transfer methods using Knowledge Graphs (Ammanabrolu & Riedl 2019b)",
            "brief_description": "Methods that seed agents with knowledge graphs to transfer commonsense and procedural knowledge between different text games and genres.",
            "citation_title": "Transfer in deep reinforcement learning using knowledge graphs",
            "mention_or_use": "mention",
            "agent_name": "Knowledge-graph transfer agents (Ammanabrolu & Riedl 2019b)",
            "agent_description": "Agents that use knowledge graphs to seed initial commonsense knowledge and transfer control policies between games in the same genre, facilitating faster learning on target games.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Text-adventure / interactive fiction games (transfer experiments cited)",
            "task_description": "Transfer learned policies and knowledge across different text-adventure games using knowledge-graph based seeding to improve sample efficiency and generalization.",
            "memory_used": true,
            "memory_type": "Pre-seeded structured knowledge graph (transfer memory) plus online graph updates",
            "memory_representation": "Structured commonsense/observational triples and relations representing objects, affordances, and map connectivity",
            "memory_update_mechanism": "Seeded before training on target, then updated online during exploration of target games",
            "memory_retrieval_mechanism": "Graph used to initialize state representation and policy; retrieved via features/embeddings to guide action selection",
            "performance_with_memory": "Paper reports transfer is more effective/efficient than training from scratch within genre (qualitative summary here); no numeric metrics reproduced in this Jericho paper.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "Referenced work explores transfer benefits; this Jericho paper does not replicate those ablations.",
            "challenges_or_limitations": "Effectiveness depends on closeness of source and target games (genre similarity); seeding with inappropriate knowledge could hurt; details are in the cited transfer paper.",
            "best_practices_or_recommendations": "Use knowledge graphs to seed commonsense and procedural priors when transferring between related games; authors suggest this accelerates learning versus training from scratch.",
            "uuid": "e4673.2",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Light (transformer dialogue)",
            "name_full": "LIGHT: Learning to speak and act in a fantasy text adventure game",
            "brief_description": "A crowdsourced dataset and baseline showing transformer-based supervised models can generate contextually relevant dialogue, emotes, and actions in text-adventure style interactions.",
            "citation_title": "Learning to speak and act in a fantasy text adventure game",
            "mention_or_use": "mention",
            "agent_name": "Transformer-based supervised agents (Urbanek et al. 2019)",
            "agent_description": "Supervised transformer models (trained on LIGHT dataset) that generate contextually relevant dialog and action utterances in text-adventure-style conversations; trained in a supervised fashion rather than via RL.",
            "llm_model_name": null,
            "game_or_benchmark_name": "LIGHT dataset / fantasy text-adventure dialogues (not evaluated in Jericho experiments)",
            "task_description": "Generate plausible context-sensitive dialogue, emotes, and action utterances given preceding textual context in fantasy text-adventure interactions.",
            "memory_used": false,
            "memory_type": "Implicit short-context memory via transformer attention (no explicit external memory beyond model context window)",
            "memory_representation": "Raw text context (dialogue and game context) held in the model's input sequence",
            "memory_update_mechanism": "N/A for external memory; state represented by immediate context provided to the transformer at inference time",
            "memory_retrieval_mechanism": "Transformer self-attention over input tokens (context window)",
            "performance_with_memory": "Authors demonstrate transformer models can generate contextually relevant dialog and actions (qualitative and supervised metrics in the cited work); Jericho paper only references these capabilities, no numeric reproduction.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "This paper only cites LIGHT as related work; no ablation regarding memory is given here.",
            "challenges_or_limitations": "Transformer methods discussed are supervised and must be adapted to RL; they rely on limited context window and supervised data rather than explicit persistent memory for long-term dependencies.",
            "best_practices_or_recommendations": "Authors of Jericho suggest that recent transformer advances may be applicable to IF games but must be adapted from supervised training regimes into reinforcement-learning setups and/or combined with memory mechanisms to handle long-term dependencies.",
            "uuid": "e4673.3",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Jericho World Object Tree",
            "name_full": "Jericho World-Object-Tree (environmental structured state API)",
            "brief_description": "An environment-provided, semi-interpretable structured representation of the game's internal object/location tree (parent/child/sibling relations) exposed by Jericho for supported games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Jericho World-Object-Tree",
            "agent_description": "An environment-level structured memory that exposes the game's internal object hierarchy (locations, contained objects, inventory relationships) to agents and is used for ground-truth object detection and world-change detection.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Jericho-supported Z-machine IF games (56 supported games)",
            "task_description": "Enable agents to detect interactive objects, identify valid actions via world-change detection, and provide ground-truth information to ease training/evaluation.",
            "memory_used": true,
            "memory_type": "Environment-provided structured state tree (object/location hierarchy)",
            "memory_representation": "Structured object tree with parent/child/sibling pointers representing locations, objects, and inventory contents (raw structured facts exposed by the engine)",
            "memory_update_mechanism": "The tree is the game's internal state; Jericho exposes it and updates it synchronously with game transitions (i.e., after actions that change the world state).",
            "memory_retrieval_mechanism": "Agents or Jericho utilities query the object tree (e.g., search for non-player objects at current location); world_changed() compares snapshots of the tree to detect effects of actions.",
            "performance_with_memory": "Using world-object-tree and world-change detection as handicaps enables valid-action detection and simplifies learning; the paper reports that using these handicaps makes template-search and DRRN applicable, but no isolated numeric ablation is provided.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No direct ablation comparing with/without the object-tree in the experiments table; the paper notes these are provided as handicaps and reports which agents used them for reproducibility.",
            "challenges_or_limitations": "World-change detection can fail for actions that only alter global variables rather than the object tree; relying on engine-exposed state is a handicap that sidesteps natural language understanding challenges.",
            "best_practices_or_recommendations": "Authors recommend reporting which handicaps (e.g., object-tree, valid-action detection, templates) are used in experiments for reproducibility and suggest using these facilities to bootstrap research while acknowledging they reduce realism.",
            "uuid": "e4673.4",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Template & Template-DQN handicaps",
            "name_full": "Template-based action space and Template-DQN",
            "brief_description": "A template-based action generation approach and a Template-DQN agent that first chooses an action template then fills blanks with vocabulary; Jericho extracts game-specific templates and vocabulary to reduce action-space complexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Template-DQN (TDQN)",
            "agent_description": "A parser-based RL agent that estimates Q-values over templates and vocabulary slots (three output heads) and mixes a supervised valid-action loss (using Jericho's valid-action detection) with TD updates to guide generation in the huge combinatorial action space.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Jericho (template-extracted action space across supported games, e.g., Zork1)",
            "task_description": "Learn to select templates and fill-in vocabulary to produce valid in-game commands and maximize game score in individual IF games.",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "Paper discusses limitations of TDQN assuming independence between templates and words, and suggests conditional generation as future work; no memory ablations are reported.",
            "challenges_or_limitations": "TDQN struggles with overestimation due to large action spaces and assumes independence between template and slot predictions, which produces invalid actions; needs conditional decoding or transformer-based architectures adapted to RL.",
            "best_practices_or_recommendations": "Authors recommend conditional generation (words conditioned on chosen template) and exploring transformer-based models adapted for RL to improve template-based agents.",
            "uuid": "e4673.5",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Transfer in deep reinforcement learning using knowledge graphs",
            "rating": 2,
            "sanitized_title": "transfer_in_deep_reinforcement_learning_using_knowledge_graphs"
        },
        {
            "paper_title": "NAIL: A general interactive fiction agent",
            "rating": 2,
            "sanitized_title": "nail_a_general_interactive_fiction_agent"
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 2,
            "sanitized_title": "learning_to_speak_and_act_in_a_fantasy_text_adventure_game"
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Comprehensible context-driven text game playing",
            "rating": 1,
            "sanitized_title": "comprehensible_contextdriven_text_game_playing"
        }
    ],
    "cost": 0.014986999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Interactive Fiction Games: A Colossal Adventure</h1>
<p>Matthew Hausknecht Prithviraj Ammanabrolu Marc-Alexandre Ct<br>Microsoft Research AI Georgia Institute of Technology Microsoft Research Montral<br>Xingdi Yuan<br>Microsoft Research Montral</p>
<h4>Abstract</h4>
<p>A hallmark of human intelligence is the ability to understand and communicate with language. Interactive Fiction games are fully text-based simulation environments where a player issues text commands to effect change in the environment and progress through the story. We argue that IF games are an excellent testbed for studying language-based autonomous agents. In particular, IF games combine challenges of combinatorial action spaces, language understanding, and commonsense reasoning. To facilitate rapid development of languagebased agents, we introduce Jericho, a learning environment for man-made IF games and conduct a comprehensive study of text-agents across a rich set of games, highlighting directions in which agents can improve.</p>
<h2>1 Introduction</h2>
<p>Interactive fiction (IF) games are software environments in which players observe textual descriptions of the simulated world, issue text actions, and receive score progress through the story. As illustrated by Figure 1, humans bring competencies in natural language understanding, commonsense reasoning, and deduction to bear in order to infer the context and objectives of a game. We believe IF environments will provide a good testbed for studying artificial development of these capabilities. Beyond games, real-world applications such as voice-activated personal assistants also exist at the intersection of natural language understanding, natural language generation, and sequential decision making.</p>
<p>From a machine learning perspective, Interactive Fiction games exist at the intersection of natural language processing and sequential decision making. Like many NLP tasks, they require natural language understanding, but unlike most NLP tasks, IF games are sequential decision making problems in which actions change the subsequent world states of the game and choices made early in a game may have long term effects on the eventual endings. Reinforcement Learning (Sutton and Barto 1998) studies sequential decision making problems and has shown promise in visionbased (Jaderberg et al. 2016) and control-based (OpenAI et</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Observation: This bedroom is extremely spare, with dirty laundry scattered haphazardly all over the floor. Cleaner clothing can be found in the dresser. A bathroom lies to the south, while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys.
The phone rings.
Action: Answer phone
Observation: You pick up the phone. "Hadley!" a shrill voice cries. "Hadley, haven't you even left yet?? You knew that our presentation was at nine o'clock sharp! First the thing with the printers, now this - there won't even be enough left of you for Bowman to fire once he's done with you. Now get the hell down here!!"</p>
<p>Action: Examine me
Observation: You're covered with mud and dried sweat. It was an exhausting night - no wonder you overslept! Even making it to the bed before conking out was a heroic accomplishment.</p>
<p>Figure 1: Excerpt from Adam Cadre's 9:05.
al. 2018) environments, but has less commonly been applied in the context of language-based tasks.</p>
<p>Why study environments that feature ungrounded natural language instead of multimodal environments that provide visual grounding for language? We believe that ungrounded natural language is inescapable in certain real world tasks such as voice-activated personal assistants.</p>
<p>The contributions of this paper are as follows: First, we introduce Jericho, a learning environment for human-made IF games. Second, we introduce a template-based action space and that we argue is appropriate for language generation. Third, we conduct an empirical evaluation of learning agents across a set of human-made games.</p>
<h2>2 Research Challenges</h2>
<p>From the perspective of reinforcement learning, IF games can be modeled as partially observable Markov decision processes (POMDPs) defined by $(S, T, A, O, R)$. Observations $o \in O$ correspond to the game's text responses, while latent states $s \in S$ correspond to player and item locations, inventory contents, monsters, etc. Text-based actions $a \in A$ change the game state according to an latent transition function $T\left(s^{\prime} \mid s, a\right)$, and the agent receives rewards $r$ from an unknown reward function $R(s, a)$. To succeed in these environments, agents must generate natural language actions, reason about entities and affordances, and represent their knowledge about the game world. We present these challenges in greater detail:</p>
<p>Combinatorial Action Space Reinforcement learning has studied agents that operate in discrete or continuous action space environments. However, IF games require the agent to operate in the combinatorial action space of natural language. Combinatorial spaces pose extremely difficult exploration problems for existing agents. For example, an agent generating a four-word sentence from a modest vocabulary of size 700 , is effectively exploring a space of $\left|700^{4}\right|=240$ billion possible actions. Further complicating, this challenge is the fact that natural language commands are interpreted by the game's parser - which recognizes a gamespecific subset of possible commands. For example, out of the 240 billion possible actions there may be only ten valid actions at each step - actions that are both recognized by the game's parser and generate a change in world state.</p>
<p>As discussed in Section 4.1, we propose the use of a template-based action space in which the agent first chooses from a template of the form put $O B J$ in $O B J$ and then fills in the blanks using the vocabulary. A typical game may have around 200 templates each with up to two blanks - yielding an action space $\left|200 * 700^{2}\right|=98$ million, three orders of magnitude smaller than the naive space but six orders of magnitude greater than most discrete RL environments.</p>
<p>Commonsense Reasoning Due to the lack of graphics, IF games rely on the player's commonsense knowledge as a prior for how to interact with the game world. For example, a human player encountering a locked chest intuitively understands that the chest needs to be unlocked with some type of key, and once unlocked, the chest can be opened and will probably contain useful items. They may make a mental note to return to the chest if a key is found in the future. They may even mark the location of the chest on a map to easily find their way back.</p>
<p>These inferences are possible for humans who have years of embodied experience interacting with chests, cabinets, safes, and all variety of objects. Artificial agents lack the commonsense knowledge gained through years of grounded language acquisition and have no reason to prefer opening a chest to eating it. Also known as affordance extraction (Gibson 1977; Fulda et al. 2017), the problem of choosing which actions or verbs to pair with a particular noun is central to IF game playing. However, the problem of commonsense reasoning extends much further than affordance extraction: Games require planning which items to carry in a limited inventory space, strategically deciding whether to fight or
flee from a monster, and spatial reasoning such as stacking garbage cans against the wall of a building to access a second-floor window.</p>
<p>Knowledge Representation IF games span many distinct locations, each with unique descriptions, objects, and characters. Players move between locations by issuing navigational commands like go west. Due to the large number of locations in many games, humans often create maps to navigate efficiently and avoid getting lost. This gives rise to the Textual-SLAM problem, a textual variant of Simultaneous localization and mapping (SLAM) (Thrun, Burgard, and Fox 2005) problem of constructing a map while navigating a new environment. In particular, because connectivity between locations is not necessarily Euclidean, agents need to be able to detect when a navigational action has succeeded or failed and whether the location reached was previously seen or new. Beyond location connectivity, it's also helpful to keep track of the objects present at each location, with the understanding that objects can be nested inside of other objects, such as food in a refrigerator or a sword in a chest.</p>
<h2>3 Related Work</h2>
<p>In contrast to the parser-based games studied in this paper, choice-based games provide a list of possible actions at each step, so learning agents must only choose between the candidates. The DRRN algorithm for choice-based games (He et al. 2016; Zelinka 2018) estimates Q-Values for a particular action from a particular state. This network is evaluated once for each possible action, and the action with the maximum Q-Value is selected. While this approach is effective for choice-based games which have only a handful of candidate actions at each step, it is difficult to scale to parserbased games where the action space is vastly larger.</p>
<p>In terms of parser-based games, such as the ones examined in this paper, several approaches have been investigated: LSTM-DQN (Narasimhan, Kulkarni, and Barzilay 2015), considers verb-noun actions up to two-words in length. Separate Q-Value estimates are produced for each possible verb and object, and the action consists of pairing the maximally valued verb combined with the maximally valued object. LSTM-DQN was demonstrated to work on two small-scale domains, but human-made games, such as those studied in this paper, represent a significant increase in both complexity and vocabulary.</p>
<p>Another approach to affordance extraction (Fulda et al. 2017) identified a vector in word2vec (Mikolov et al. 2013) space that encodes affordant behavior. When applied to the noun sword, this vector produces affordant verbs such as vanquish, impale, duel, and battle. The authors use this method to prioritize verbs for a Q-Learning agent to pair with in-game objects.</p>
<p>An alternative strategy has been to reduce the combinatorial action space of parser-based games into a discrete space containing the minimum set of actions required to finish the game. This approach requires a walkthrough or expert demonstration in order to define the space of minimal actions, which limits its applicability to new and unseen games. Following this approach, (Zahavy et al. 2018) employ this strategy with their action-elimination network, a</p>
<p>classifier that predicts which predefined actions will not effect any world change or be recognized by the parser. Masking these invalid actions, the learning agent subsequently evaluates the set of remaining valid actions and picks the one with the highest predicted Q-Value.</p>
<p>The TextWorld framework (Ct et al. 2018) supports procedural generation of parser-based IF games, allowing complexity and content of the generated games to be scaled to the difficulty needed for research. TextWorld domains have already proven suitable for reinforcement learning agents (Yuan et al. 2018) which were shown to be capable of learning on a set of environments and then generalizing to unseen ones at test time. Recently, Yuan et al. (2019) propose QAit, a set of question answering tasks based on games generated using TextWorld. QAit focuses on helping agents to learn procedural knowledge in an informationseeking fashion, it also introduces the practice of generating unlimited training games on the fly. With the ability to scale the difficulty of domains, TextWorld may be key to creating a curriculum of learning tasks and helping agents scale to human-made games.</p>
<p>Ammanabrolu and Riedl (2019a) present the Knowledge Graph DQN or KG-DQN, an approach where a knowledge graph built during exploration is used as a state representation for a deep reinforcement learning based agent. They also use question-answering techniques-asking the question of what action best next to take-to pretrain a deep $Q$ network. These techniques are then shown to aid in overcoming the twin challenges of a partially observable state space and a combinatorial action space. Ammanabrolu and Riedl (2019b) further expand on this work, exploring methods of transferring control policies in text-games, using knowledge graphs to seed an agent with useful commonsense knowledge and transfer knowledge between different games within a domain. They show that training on a source game and transferring to target game within the same genre-e.g. horror or slice of life-is more effective and efficient than training from scratch on the target game.</p>
<p>Finally, although not a sequential decision making problem, Light (Urbanek et al. 2019) is a crowdsourced dataset of text-adventure game dialogues. The authors demonstrate that supervised training of transformer-based models have the ability to generate contextually relevant dialog, actions, and emotes.</p>
<h2>4 Jericho Environment</h2>
<p>Jericho is an open-source ${ }^{1}$ Python-based IF environment, which provides an OpenAI-Gym-like interface (Brockman et al. 2016) for learning agents to connect with IF games.</p>
<h3>4.1 Template-Based Action Generation</h3>
<p>Template-based action generation involves first selecting a template, then choosing words to fill in the blanks in that template. The set of game-specific templates are identified by decompiling a game to extract the possible subroutines - each template corresponds to a different subroutine. Templates contain a maximum of two blanks to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>be filled. Additionally, game-specific vocabulary is also extracted and provides a list of all words recognized by the game's parser. Combining templates with vocabulary yields a game-specific action space, which is far more tractable than operating in the pure vocabulary space.</p>
<h3>4.2 World Object Tree</h3>
<p>The world object tree is a semi-interpretable representation of game state that IF games use internally to codify the relationship between the objects and locations that populate the game world. Each object in the tree has a parent, child, and sibling. These relationships between objects are used to encode presence: a location contains children objects that correspond to the items present at that location. Similarly, there is an object corresponding to the player, whose parent is the player's location and whose children are the objects in the player's inventory.</p>
<p>Jericho's ability to extract world-object-trees forms the basis for world-change-detection (described in the next subsection) and ground-truth object detection. Ground truth object detection searches the object tree for all non-player objects present at the current location, thus sidestepping the challenge of identifying interactive objects from a location's description.</p>
<h3>4.3 World Change Detection</h3>
<p>Jericho has the ability to best-guess detect whether an action changed the world state of the game. Using this facility, it's possible to identify the valid actions from a state by performing a search over template-based actions and excluding any actions that don't change the world state. We demonstrate the feasibility of valid action detection by training a choicebased learner, DRRN (Section 5.2). World change detection is based on identifying changes to the world-object tree ${ }^{2}$ and can fail to detect valid actions whose effects alter only global variables instead of the object tree. In practice, these failures are rare.</p>
<h3>4.4 Supported Games</h3>
<p>For supported games, Jericho is able to detect game score, move count, and world change. Jericho supports a set of fifty-six human-made IF games that cover a variety of genres: dungeon crawl, Sci-Fi, mystery, comedy, and horror. Games were selected from classic Infocom titles such as Zork and Hitchhiker's Guide to the Galaxy, as well as newer, community-created titles like Anchorhead and Afflicted. All supported games use a point-based scoring system, which serves as the agent's reward.</p>
<p>Unsupported games may be played through Jericho, without the support of score detection, move counts, or worldchange detection. There exists a large collection of over a thousand unsupported games ${ }^{3}$, which may be useful for unsupervised pretraining or intrinsic motivation.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>4.5 Identifying Valid Actions</h3>
<p>Valid actions are actions recognized by the game's parser that cause changes in the game state. When playing new games, identifying valid actions is one of the primary difficulties encountered by humans and agents alike. Jericho identifies valid actions using the following procedure:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Procedure</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Identifying</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Jericho</span><span class="w"> </span><span class="n">environment</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">templates</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">o</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Textual</span><span class="w"> </span><span class="n">observation</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">p_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">ldots</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">Interactive</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="n">identified</span><span class="w"> </span><span class="n">with</span>
<span class="w">        </span><span class="n">noun</span><span class="o">-</span><span class="n">phrase</span><span class="w"> </span><span class="n">extraction</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">world</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">tree</span><span class="o">.</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">Y</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span><span class="w"> </span><span class="n">List</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">actions</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">s</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span><span class="w"> </span><span class="o">.</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">save</span><span class="p">}()</span>\<span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Save</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">state</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">template</span><span class="w"> </span>\<span class="p">(</span><span class="n">u</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">T</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">combinations</span><span class="w"> </span>\<span class="p">(</span><span class="n">p_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">Action</span><span class="w"> </span>\<span class="p">(</span><span class="n">a</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">u</span><span class="w"> </span>\<span class="n">Leftarrow</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">world_changed</span><span class="p">(</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">step</span><span class="w"> </span>\<span class="p">((</span><span class="n">a</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span>\<span class="p">(</span><span class="n">Y</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span>\<span class="n">cup</span><span class="w"> </span><span class="n">a</span>\<span class="p">)</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="w"> </span>\<span class="p">((</span><span class="n">s</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Restore</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">state</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span><span class="n">Y</span>\<span class="p">)</span>
</code></pre></div>

<h3>4.6 Handicaps</h3>
<p>To ease the difficulty of IF games, Jericho has the option of the following handicaps:</p>
<ul>
<li>Inputs: Addition of a location description, player's inventory, and game score.</li>
<li>Outputs: Game-specific templates $\mathcal{T}$ and vocabulary $\mathcal{V}$.</li>
<li>World Objects: Use of world object tree for identifying interactive objects or player's current location.</li>
<li>Valid Actions: Use of world-changed-detection to identify valid actions.</li>
</ul>
<p>For reproducibilty, we report the handicaps used by all algorithms in the next section and encourage future work to do the same.</p>
<h2>5 Algorithms</h2>
<p>IF game playing has been approached from the perspective single-game agents that are trained and evaluated on the same game and general game playing agents which are trained and evaluated on different sets of games. In this section we present three agents: a choice-based single-game agent (DRRN), a parser-based single-game agent (TDQN), and a parser-based general-game agent (NAIL).</p>
<h3>5.1 Common Input Representation</h3>
<p>The input encoder $\phi_{a}$ converts observations into vectors using the following process: Text observations are tokenized by a SentencePiece model (Kudo and Richardson 2018) using an 8000-large vocab trained on strings extracted from http://www.allthingsjacq.com/index.html sessions of humans playing a variety of different IF games. Tokenized observations are processed by separate GRU encoders for
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: DRRN architecture estimates a joint Q-Value $Q(o, a)$ over the observation $o$ and an action $a$. The observation encoder $\phi_{o}$ uses separate GRUs to process the narrative text $o_{\text {nar }}$, the players inventory $o_{\text {inv }}$, and the location description $o_{\text {desc }}$ into a vector $\nu_{o}$. DRRN uses a separate $G R U_{a}$ for processing action text into a vector $\nu_{a}$. The action-scorer $\phi_{a}$ concatenates the input and action vectors and outputs a scalar Q-Value.
the narrative (i.e., the game's response to last action), description of current location, contents of inventory, and previous text command. The outputs of these encoders are concatenated into a vector $\nu_{o}$. DRRN and Template-DQN build on this common input representation.</p>
<h3>5.2 DRRN</h3>
<p>The Deep Reinforcement Relevance Network (DRRN) (He et al. 2016) is an algorithm for choice-based games that present a set of valid actions $A_{\text {valid }}(s)$ at every game state $s$. We re-implement DRRN using a GRU $\phi_{\text {act }}(a)$ to encode each valid action into a vector $\nu_{a}$, which is concatenated with the encoded observation vector $\nu_{o}$. Using this combined vector, DRRN then computes a Q-Value $Q(o, a)$ estimating the total discounted reward expected if action $a$ is taken and $\pi_{D R R N}$ is followed thereafter. This procedure is repeated for each valid action $a_{i} \in A_{\text {valid }}(s)$. Action selection is performed by sampling from a softmax distribution over $Q\left(o, a_{i}\right)$. The network is updated by sampling a minibatch of transitions $\left(o, a, r, o^{\prime}, A_{\text {valid }}\left(s^{\prime}\right)\right) \sim \mathcal{D}$ from the prioritized replay memory (Schaul et al. 2016) and minimizing the temporal difference error $\delta=r+\gamma *$ $\max _{a}^{\prime} Q\left(o^{\prime}, a^{\prime}\right)-Q(o, a)$. From an optimization perspective, rather than performing a separate forward pass for each valid action, we batch valid-actions and perform a single forward pass computing Q-Values for all valid actions.</p>
<p>In order to make DRRN applicable to parser-based IF games, it's necessary to identify the list of valid actions available at each step. This is accomplished through a search over template-based actions, pruned by Jericho's world</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Template-DQN estimates Q-Values $Q(o, u)$ for all templates $u \in \mathcal{T}$ and $Q(o, p)$ for all vocabulary $p \in \mathcal{V}$. Similar to DRRN, separate GRUs are used to encode each component of the observation, including the text of the previous action $a_{t-1}$.
change detection (Algorithm 1). This handicap bypasses language generation, one of the challenges of IF games.</p>
<h3>5.3 Template-DQN</h3>
<p>LSTM-DQN (Narasimhan, Kulkarni, and Barzilay 2015) is an agent for parser-based games that handles the combinatorial action space by generating verb-object actions using a set possible verbs and possible objects. Specifically, LSTMDQN uses two output layers to estimate Q-Values over possible verbs and objects. Actions are selected by pairing the maximally valued verb with the maximally valued noun.</p>
<p>Template-DQN (TDQN) extends LSTM-DQN by incorporating template-based action generation (Section 4.1). This is accomplished using three output heads: one for estimating Q-Values over templates $Q(o, u) ; u \in \mathcal{T}$ and two for estimating Q-Values $Q\left(o, p_{1}\right), Q\left(o, p_{2}\right) ; p \in \mathcal{V}$ to fill in the blanks of the template using vocabulary.</p>
<p>The largest action space considered in the original LSTMDQN paper was 222 actions ( 6 verbs and 22 objects). In contrast, Zork1 has 237 templates with a 697 word vocabulary, yielding an action space of 115 million. Computationally, this space is too large to naively explore as the vast majority of actions will be un-grammatical or contextually irrelevant. To help guide the agent towards valid actions, we introduce a supervised binary-cross entropy loss based on the valid actions in the current state. Valid actions are identified using the same procedure as in DRRN. This loss is mixed with the standard temporal difference error during each update.</p>
<p>We further optimize by decoding large batches of actions with each forward pass over the network and executing them sequentially until a valid action is found. When decoding actions that are often invalid, this provides a considerable speedup compared to performing an separate forward pass for each action.</p>
<h3>5.4 NAIL</h3>
<p>NAIL (Hausknecht et al. 2019) is the state-of-the-art agent for general interactive fiction game playing (Atkinson et al. 2018). Rather than being trained and evaluated on a single game, NAIL is designed to play unseen IF games and accumulate as much score as possible in a single episode. Operating without the handicaps outlined in Section 4.6, NAIL employs a set of manually-created heuristics to build a map of objects and locations, reason about which actions were valid or invalid, and uses a web-based language model to decide how to interact with novel objects. We include NAIL's performance on the same set of games in order to highlight the difference between general and single-game playing agents, and to provide a reference scores for future work in general IF game playing.</p>
<h2>6 Experiments</h2>
<p>We evaluate the agents across a set of thirty-two Jerichosupported games with the aims of 1) showing the feasibility of reinforcement learning on a variety of different IF games, 2) creating a reproducible benchmark for future work, 3) investigating the difference between choice-based and template-based action spaces, and 4) comparing performance of general IF game playing agents (NAIL), singlegame agents (DRRN and TDQN), and a random agent (RAND) which uniformly sample commands from a set of canonical actions: {north, south, east, west, up, down, look, inventory, take all, drop, yes $}$.</p>
<p>Results in Table 1, supported by learning curves in Figure 4 show that reinforcement learning is a viable approach for optimizing IF game playing agents across many different types of games. Experimentally, TDQN and DRRN agents are trained and evaluated on each game individually, but their hyperparameters are fixed across the different games.</p>
<p>In order to quantify overall progress towards story completion, we normalize agent score by maximum possible game score averaged across all games. The resulting progress scores are as follows: RANDOM $1.8 \%$, NAIL $4.9 \%$, TDQN $6.1 \%$, and DRRN $1.7 \%$ completion.</p>
<p>Comparing the different agents, the random agent shows that more than simple navigation and take actions are needed to succeed at the vast majority of games. Comparing DRRN to TDQN highlights the utility of choice-based game playing agents who need only estimate Q-Values over pre-identified valid-actions. In contrast, TDQN needs to estimate Q-Values over the full space of templates and vocabulary words. As a result, we observed that TDQN was more prone to overestimating Q-Values due to the Q-Learning update computing a max over a much larger number of possible actions.</p>
<p>Comparing the general game playing NAIL agent to single-game agents: the NAIL agent performs surprisingly well considering it uses no handicaps, no training period, and plays the game for only a single episode. It should be noted that the NAIL agent was developed on many of the same games used in this evaluation. The fact that the reinforcement learning agents outperform NAIL serves to highlight the difficulty of engineering an IF agent as well as</p>
<p>the promise of learning policies from data rather than handcoding heuristics.</p>
<p>Broadly, all algorithms have a long way to go before they are solving games of even average difficulty. Five games prove too challenging for any agents to get a nonzero reward. Games like Anchorhead are highly complex and others pose difficult exploration problems like 9:05 which features only a single terminal reward indicating success or failure at the end of the episode.</p>
<p>Additional experiment details and hyperparameters are located in the supplementary material.</p>
<p>| Game | $|T|$ | $|V|$ | RAND | NAIL | TDQN | DRRN |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 905 | 82 | 296 | 0 | 0 | 0 | 0 |
| acorncourt | 151 | 343 | 0 | 0 | .05 | .33 |
| anchor | 280 | 2257 | 0 | 0 | 0 | 0 |
| advent | 189 | 786 | .1 | .1 | .1 | .1 |
| adventureland | 156 | 398 | 0 | 0 | 0 | .21 |
| afflicted | 146 | 762 | 0 | 0 | .02 | .03 |
| awaken | 159 | 505 | 0 | 0 | 0 | 0 |
| balances | 156 | 452 | 0 | .2 | .09 | .2 |
| deephome | 173 | 760 | 0 | .04 | 0 | 0 |
| detective | 197 | 344 | .32 | .38 | .47 | .55 |
| dragon | 177 | 1049 | 0 | .02 | $-.21$ | $-.14$ |
| enchanter | 290 | 722 | 0 | 0 | .02 | .05 |
| gold | 200 | 728 | 0 | .03 | .04 | 0 |
| inhumane | 141 | 409 | 0 | 0 | 0 | 0 |
| jewel | 161 | 657 | 0 | .02 | 0 | .02 |
| karn | 178 | 615 | 0 | .01 | 0 | .01 |
| library | 173 | 510 | 0 | .03 | .21 | .57 |
| ludicorp | 187 | 503 | .09 | .06 | .04 | .09 |
| moonlit | 166 | 669 | 0 | 0 | 0 | 0 |
| omnique | 207 | 460 | 0 | .11 | .34 | .1 |
| pentari | 155 | 472 | 0 | 0 | .25 | .39 |
| reverb | 183 | 526 | 0 | 0 | .01 | .16 |
| snacktime | 201 | 468 | 0 | 0 | .19 | 0 |
| sorcerer | 288 | 1013 | .01 | .01 | .01 | .05 |
| spellbrkr | 333 | 844 | .04 | .07 | .03 | .06 |
| spirit | 169 | 1112 | .01 | 0 | 0 | 0 |
| temple | 175 | 622 | 0 | .21 | .23 | .21 |
| tryst205 | 197 | 871 | 0 | .01 | 0 | .03 |
| yomomma | 141 | 619 | 0 | 0 | 0 | .01 |
| zenon | 149 | 401 | 0 | 0 | 0 | 0 |
| zork1 | 237 | 697 | 0 | .03 | .03 | .09 |
| zork3 | 214 | 564 | .03 | .26 | 0 | .07 |
| ztuu | 186 | 607 | 0 | 0 | .05 | .22 |</p>
<p>Table 1: Normalized scores for Jericho-supported games. Results are averaged over the last hundred episodes of training and across five independent training runs (i.e., with different seeds for initializing the environment and agent sampling process) conducted for each algorithm.</p>
<h2>7 Notable Games</h2>
<p>Jericho supports a vast array of games, covering a diverse set of structures and genres. These games provide us with different challenges from the perspective of reinforcement learning based agents. In this section, we highlight some specific challenges posed by Jericho supported games and provide notable examples for each of the types of challenges in addition to examples of games that the two types of deep reinforcement learning agents do well on. Learning curves for some of these examples using DRRN and TDQN are presented (Figure 4), underscoring the difficulties current rein-
forcement learning agents have in solving these games and showcasing effective training paradigms for different games.</p>
<h3>7.1 Sanity Checks</h3>
<p>The first set of games are essentially sanity checks, i.e. they are games that can be solved relatively well by existing agents. These games thus fall on the lower end on the difficulty spectrum and serve as good initial testbeds for developing new agents.</p>
<p>Detective in particular is one of the easier games, and with existing agents such as the random agent and NAIL being able to solve the majority of the game. The relatively good performance of the random agent is likely due to the game mostly requiring only navigational actions in order to accumulate score. On this game, TDQN has comparable performance to DRRN. Acorncourt also serves as a sanity check, albeit a more difficult one-with the DRRN outperforming all other agents. This game requires a higher proportion of higher complexity actions, which make generation-such as in the case of TDQN-more difficult. Omniquest is an example of a dungeon-crawler style game where TDQN outperforms the rest of the agents. In this game, due to there being a relatively smaller number of valid templates as compared to valid actions-i.e. many valid actions come from the same template-the TDQN has an effective search space that is smaller than the DRRN.</p>
<h3>7.2 Varying Rewards</h3>
<p>Most IF games provide you with positive scores in varying increments as you achieve objectives and negative scores for failing them. This reward structure is similar to most games in general and gives the player an indication of relative progress within the game. Some games such as Deephome and Adventure, however, provide relatively unintuitive scoring functions that can prove to be a challenge for reinforcement learning agents.</p>
<p>Deephome gives you an additional point of score for each new room that you visit in addition to rewards for achieving game objectives. This encourages exploration but could also prove tricky for an agent as it may not be necessary to finish the game. In Adventure, you start with a score of 36, and as the game progresses you are first given mostly negative scores followed by positive scores until you finish the game. As seen in Figure 4, this gives a reinforcement learning agent no indication that it is progressing in the short term.</p>
<h3>7.3 Moonshots</h3>
<p>Here we highlight some of the most difficult games in Jericho, current agents are quite far from being able to solve them. Zorkl is one of the original IF games and heavily influences later games using this medium. The game can best be described as a dungeon-crawler in which a player must make their way through a large dungeon, collecting treasures and fighting monsters along the way. The collection of these treasures forms the basis of Zorkl's scoring system, although some score is rewarded at intermediate steps to aid in finding the treasures. Being a dungeon-crawler, Zorkl features branching game path in terms of reward collection as well as stochasticity. The game can be completed</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Episode score as a function of training steps for DRRN (top) and TDQN (bottom). Shaded regions denote standard deviation across five independent runs for each game. Additional learning curves in supplementary material.
in many different ways, often affected by the random movement of a thief and number of hits required to kill monsters. It is also interesting to note that NAIL and TDQN perform comparably on Zork1, with DRRN far outperforming them-indicating the difficulty of language generation in such a large state space. It has also been the subject of much prior work on IF game-playing agents (Zahavy et al. 2018; Yin and May 2019).</p>
<p>Anchorhead is a Lovecraftian horror game where the player must wade through a complex puzzle-based narrative. The game features very long term dependencies in piecing together the information required to solve its puzzles and is complex enough that it has been the subject of prior work on cognition in script learning and drama management ( Giannatos et al. 2011). This complexity is further reflected in the size of the vocab and number of templates-it has the largest action space of any Jericho supported game. None of our agents are able to accumulate any score on this game.</p>
<p>Although Anchorhead's game structure is more sequential than Zork1, it also contains a more sparse reward-often giving you a positive score only after the completion of a puzzle. It is also stochastic, with the exact solution depending on the random seed supplied when the game is started.</p>
<h2>8 Future Work</h2>
<p>Unsupervised learning: DRRN and TDQN agents were trained and evaluated on individual games. While this is sufficient for a proof-of-concept, it falls short of demonstrating truly general IF game playing. To this end, it's valuable to evaluate agents on a separate set of games which they have not been trained on. In the Jericho framework, we propose to use the set of Jericho supported games as a test set and the larger set of unsupported games ${ }^{4}$ as the training set. In this</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>paradigm, it's necessary have a strong unsupervised learning component to guide the agent's exploration and learning since unsupported games do not provide rewards, and in fact many IF games do not have scores. We hypothesize that surrogate reward functions, like novelty-based rewards (Bellemare et al. 2016; Pathak et al. 2017), will be useful for discovering locations, successful interactions and objects.</p>
<p>Better Template-based Agents: There are several directions for creating better template-based agents by improving on the limitations of TDQN: When generating actions, TDQN assumes independence between templates and vocabulary words. To understand the problem with this assumption consider the templates "go .." and "take .." and the vocabulary words "north, apple". Independently, "take" and "north" may have the highest Q-Values together yield the invalid action "take north". Conditional generation of words based on the chosen template may go far to improve the quality of TDQN's actions.</p>
<p>Second, recent work on transformer-based neural architectures has yielded impressive gains in many NLP tasks (Devlin et al. 2018), including text-adventure game dialogues (Urbanek et al. 2019). We expect these advances may be applicable to human-made IF games, but will need to be adapted from a supervised training regime into reinforcement learning.</p>
<h2>9 Conclusion</h2>
<p>Interactive Fiction games are rich narrative adventures that challenge even skilled human players. In contrast to other video game environments like ALE (Machado et al. 2017), Vizdoom (Kempka et al. 2016), and Malmo (Johnson et al. 2016), IF games stress natural language understanding and commonsense reasoning, and feature combinatorial action spaces. To aid in the study of these environment, we introduce Jericho, an experimental platform with the key of feature of extracting game-specific action templates and vocabulary. Using these features, we proposed a novel templatebased action space which serves to reduce the complexity of full scale language generation. Using this space, we introduced the Template-DQN agent, which generates actions first by selecting a template then filling in the blanks with words from the vocabulary.</p>
<p>We evaluated Template-DQN as well as a choice-based agent DRRN and a general IF agent NAIL on a set of thirtytwo human-made IF games. Overall, DRRN outperformed the other agents with Template-DQN in second place. However, in many senses these agents represent very different training paradigms, sets of assumptions, and levels of handicap. Rather than comparing agents we aim to provide benchmark results for future work in these three categories of IF game playing. All in all, we believe Jericho can help the community propel research on language understanding agents and expect these environments can serve the community as benchmarks for years to come.</p>
<h2>References</h2>
<p>Ammanabrolu, P., and Riedl, M. O. 2019a. Playing text-adventure games with graph-based deep reinforcement</p>
<p>learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019.
Ammanabrolu, P., and Riedl, M. O. 2019b. Transfer in deep reinforcement learning using knowledge graphs. CoRR abs/1908.06556.
Atkinson, T.; Baier, H.; Copplestone, T.; Devlin, S.; and Swan, J. 2018. The text-based adventure AI competition.
Bellemare, M. G.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Saxton, D.; and Munos, R. 2016. Unifying count-based exploration and intrinsic motivation. CoRR abs/1606.01868.
Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. Openai gym.
Ct, M.-A.; Kdr, A.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. Textworld: A learning environment for text-based games. CoRR abs/1806.11532.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR abs/1810.04805.
Fulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What can you do with a rock? affordance extraction via word embeddings. In IJCAI, 1039-1045.
Giannatos, S.; Nelson, M. J.; Cheong, Y.-G.; and Yannakakis, G. N. 2011. Suggesting New Plot Elements for an Interactive Story. In In Workshop on Intellignet Narrative Technologies (INT'11).
Gibson, J. J. 1977. "The theory of affordances," in Perceiving, Acting, and Knowing. Towards an Ecological Psychology. Number eds Shaw R., Bransford J. Hoboken, NJ: John Wiley \&amp; Sons Inc.
Hausknecht, M. J.; Loynd, R.; Yang, G.; Swaminathan, A.; and Williams, J. D. 2019. NAIL: A general interactive fiction agent. CoRR abs/1902.04259.
He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Ostendorf, M. 2016. Deep reinforcement learning with a natural language action space. In $A C L$.
Jaderberg, M.; Mnih, V.; Czarnecki, W. M.; Schaul, T.; Leibo, J. Z.; Silver, D.; and Kavukcuoglu, K. 2016. Reinforcement learning with unsupervised auxiliary tasks. CoRR abs/1611.05397.
Johnson, M.; Hofmann, K.; Hutton, T.; and Bignell, D. 2016. The malmo platform for artificial intelligence experimentation. In IJCAI, IJCAI'16, 4246-4247. AAAI Press.
Kempka, M.; Wydmuch, M.; Runc, G.; Toczek, J.; and Jakowski, W. 2016. ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In CIG, 341348. Santorini, Greece: IEEE. The best paper award.</p>
<p>Kudo, T., and Richardson, J. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. CoRR abs/1808.06226.
Machado, M. C.; Bellemare, M. G.; Talvitie, E.; Veness, J.; Hausknecht, M. J.; and Bowling, M. 2017. Revisiting the
arcade learning environment: Evaluation protocols and open problems for general agents. CoRR abs/1709.06009.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient estimation of word representations in vector space. CoRR abs/1301.3781.
Narasimhan, K.; Kulkarni, T. D.; and Barzilay, R. 2015. Language understanding for text-based games using deep reinforcement learning. In EMNLP, 1-11.
OpenAI; Andrychowicz, M.; Baker, B.; Chociej, M.; Jzefowicz, R.; McGrew, B.; Pachocki, J. W.; Pachocki, J.; Petron, A.; Plappert, M.; Powell, G.; Ray, A.; Schneider, J.; Sidor, S.; Tobin, J.; Welinder, P.; Weng, L.; and Zaremba, W. 2018. Learning dexterous in-hand manipulation. CoRR abs/1808.00177.
Pathak, D.; Agrawal, P.; Efros, A. A.; and Darrell, T. 2017. Curiosity-driven exploration by self-supervised prediction. CoRR abs/1705.05363.
Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016. Prioritized experience replay. In International Conference on Learning Representations.
Sutton, R. S., and Barto, A. G. 1998. Introduction to Reinforcement Learning. Cambridge, MA, USA: MIT Press, 1st edition.
Thrun, S.; Burgard, W.; and Fox, D. 2005. Probabilistic Robotics (Intelligent Robotics and Autonomous Agents). The MIT Press.
Urbanek, J.; Fan, A.; Karamcheti, S.; Jain, S.; Humeau, S.; Dinan, E.; Rocktschel, T.; Kiela, D.; Szlam, A.; and Weston, J. 2019. Learning to speak and act in a fantasy text adventure game. volume abs/1903.03094.
Yin, X., and May, J. 2019. Comprehensible context-driven text game playing. CoRR abs/1905.02265.
Yuan, X.; Ct, M.; Sordoni, A.; Laroche, R.; des Combes, R. T.; Hausknecht, M. J.; and Trischler, A. 2018. Counting to explore and generalize in text-based games. CoRR abs/1806.11525.
Yuan, X.; Ct, M.-A.; Fu, J.; Lin, Z.; Pal, C.; Bengio, Y.; and Trischler, A. 2019. Interactive language learning by question answering.
Zahavy, T.; Haroush, M.; Merlis, N.; Mankowitz, D. J.; and Mannor, S. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 31. Curran Associates, Inc. 3562-3573. Zelinka, M. 2018. Using reinforcement learning to learn how to play text-based games. CoRR abs/1801.01999.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://github.com/BYU-PCCL/z-machine-games&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ Game trees are standardized representations for all Z-Machine games. To learn more see https://inform-fiction.org/zmachine/ standards/z1point1/index.html
${ }^{3}$ https://github.com/BYU-PCCL/z-machine-games&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
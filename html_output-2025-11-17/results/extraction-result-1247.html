<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1247 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1247</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1247</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-265659446</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.02522v2.pdf" target="_blank">MASP: Scalable GNN-based Planning for Multi-Agent Navigation</a></p>
                <p><strong>Paper Abstract:</strong> We investigate multi-agent navigation tasks, where multiple agents need to reach initially unassigned goals in a limited time. Classical planning-based methods suffer from expensive computation overhead at each step and offer limited expressiveness for complex cooperation strategies. In contrast, reinforcement learning (RL) has recently become a popular approach for addressing this issue. However, RL struggles with low data efficiency and cooperation when directly exploring (nearly) optimal policies in a large exploration space, especially with an increased number of agents(e.g., 10+ agents) or in complex environments (e.g., 3-D simulators). In this paper, we propose the Multi-Agent Scalable Graph-based Planner (MASP), a goal-conditioned hierarchical planner for navigation tasks with a substantial number of agents in the decentralized setting. MASP employs a hierarchical framework to reduce space complexity by decomposing a large exploration space into multiple goal-conditioned subspaces, where a high-level policy assigns agents goals, and a low-level policy navigates agents toward designated goals. For agent cooperation and the adaptation to varying team sizes, we model agents and goals as graphs to better capture their relationship. The high-level policy, the Goal Matcher, leverages a graph-based Self-Encoder and Cross-Encoder to optimize goal assignment by updating the agent and the goal graphs. The low-level policy, the Coordinated Action Executor, introduces the Group Information Fusion to facilitate group division and extract agent relationships across groups, enhancing training efficiency for agent cooperation. The results demonstrate that MASP outperforms RL and planning-based baselines in task efficiency.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1247.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1247.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Particle Environment (MPE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical 2-D multi-agent navigation benchmark with discrete agent actions, circular agents/landmarks/obstacles, randomized spawns, and configurable agent/landmark/obstacle counts and map areas; used here to evaluate multi-agent navigation and goal assignment methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-Agent Particle Environment (MPE)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2-D continuous-space multi-agent navigation domain (agents/landmarks/obstacles are circles) with discrete actions (Up/Down/Left/Right); tasks require multiple agents to reach unassigned landmarks under collisions and obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Agent and goal representations are modeled as fully connected graphs (initial edge features = 1); during low-level execution agents are split into fully connected subgraphs (groups) of size N_g where agent k is present in every group; the environment geometry itself is not represented as an explicit graph topology in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Experiments use N ∈ {5, 20, 50} agents and equal number of landmarks; obstacles B ∈ {5, 10, 30}; map areas considered: 4 m^2, 64 m^2, 400 m^2; horizons: 18, 45, 90 timesteps respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MASP (Goal Matcher + Coordinated Action Executor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical goal-conditioned policy: high-level Goal Matcher (graph-based Self-Encoder + Cross-Encoder) assigns goals in a decentralized fashion; low-level Coordinated Action Executor uses Group Information Fusion (GCN over fully connected subgraphs/groups) to produce actions toward assigned goals; trained with MAPPO under CTDE.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Primary: Steps (timesteps to achieve target Success Rate). Also: Success Rate (SR) and Collision Rate (CR).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>MASP reported Steps: N=5: 138.35 (±13.57) timesteps; N=20: 315.31 (±14.52) timesteps. (Steps for N=50 not tabulated explicitly in table but text reports MASP achieves nearly 100% SR at N=50.)</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>MASP: N=5 SR = 1.00 (100%); N=20 SR = 0.97 (97%); N=50 SR ≈ 1.00 (nearly 100%) as reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical, graph-based, goal-conditioned policies (MASP's combination of graph encoders at high level and GCN-based group fusion at low level) — i.e., a planning-aware hierarchical RL policy — perform best under large agent counts and complex maps.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper-level findings: (1) Modeling agents/goals as fully connected graphs and using cross/self-attention improves goal assignment and reduces collisions relative to MLP backbones; (2) converting agents into small fully connected subgraphs (group size N_g) and aggregating agent features across groups improves cooperation and training efficiency, with diminishing returns beyond N_g=3; (3) planning-based environment partitioning (Voronoi) helps in small-agent regimes but is less effective in large/complex settings; (4) centralized Hungarian assignment prevents goal duplication (high SR) but incurs greater per-step computation and does not adapt well to varying team sizes; no explicit numeric relationships to graph-theoretic measures (diameter/clustering) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Comparisons are across environment configurations and planning/partitioning strategies rather than formal graph-theoretic topologies: (a) Voronoi partitioning helps coordination with few agents but struggles in high-complexity 3-D cases; (b) RRT* performs comparably when obstacles dominate but omits complex constraints; (c) MASP outperforms planning baselines in Steps as agent count grows (e.g., MASP requires ~19.12% fewer Steps than best planning baseline at N=50 in MPE according to text); (d) group-size (subgraph) topology: group size 3 balances performance and input dimensionality — larger groups give minor gains at higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that explicitly encode inter-agent relations (graph Self-Encoder/Cross-Encoder at high level; GCN-based group fusion at low level) produce stronger cooperation, lower collision rates, and better sample/execution efficiency than MLP backbones or neighbor-limited policies; group-structured inputs (agent included in every group) and frequent high-level goal reassignment (smaller global-step intervals) improve final performance but increase runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASP: Scalable GNN-based Planning for Multi-Agent Navigation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1247.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1247.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OmniDrones</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OmniDrones 3-D Quadrotor Simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3-D drone-control simulator with continuous action throttle per motor, realistic collision/crash dynamics, and randomized spawn locations for drones and landmarks; used to evaluate multi-agent navigation in complex 3-D continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>OmniDrones (quadrotor 3-D environment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3-D simulated quadrotor navigation domain (continuous action space: motor throttles) where collisions cause crashes and episode failures; used to evaluate multi-agent navigation with complex dynamics and larger exploration spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>As with MPE, agent and goal relationships are modeled as fully connected graphs in the Goal Matcher; environment space is continuous 3-D and not represented as an explicit graph topology in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Experiments run with N ∈ {5, 20} drones; map areas: 36 m^2 (for 5 drones) and 256 m^2 (for 20 drones); horizons: 300 timesteps (5 drones) and 500 timesteps (20 drones).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MASP (Goal Matcher + Coordinated Action Executor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same hierarchical MASP architecture: high-level graph-based goal assignment (Self-Encoder + Cross-Encoder) and low-level GCN-based Group Information Fusion for coordinated continuous-control navigation; trained via MAPPO.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Same metrics: primary metric Steps (timesteps to reach target SR), plus Success Rate (SR) and Collision Rate (CR).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported success rates: MASP achieves SR = 1.00 (100%) at N=5 and SR = 0.96 (96%) at N=20; exact Steps per configuration are reported in tables (MASP Steps values are substantially lower than planning baselines; planning baselines require ≥ 27.92% more Steps than MASP at N=20 according to the text).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>MASP: N=5 SR = 1.00 (100%); N=20 SR = 0.96 (96%). Planning baselines often achieve high SR (>90%) but with larger Steps and less adaptability to varying team sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>A hierarchical, graph-structured, goal-conditioned RL policy (MASP) that combines graph encoders for assignment and group-GCN for execution is most effective in complex 3-D continuous-control navigation under agent loss/variable team sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Findings: (1) In complex 3-D environments, MLP-based goal assignment (e.g., MAGE-X) degrades significantly as agent count increases; (2) planning-based partitioning (Voronoi) can perform well for few agents but struggles to scale to complex 3-D coordination; (3) MASP's graph-based assignment and group fusion lead to higher SR and fewer Steps in 3-D, demonstrating the value of explicit relation modeling in higher-dimensional continuous spaces; no formal graph-theoretic metrics (diameter, clustering coefficient, dead-end counts) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Paper contrasts 2-D vs 3-D dynamics and map sizes: planning baselines keep high SR in many OmniDrones settings due to centralized goal assignment, but they fail to adapt to agent loss or varying team sizes; MASP generalizes better zero-shot to team-size changes and keeps high SR with fewer Steps than planners (planning baselines require ≥ 27.92% more Steps at N=20).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Graph-structured hierarchical policies are more robust in 3-D continuous dynamics; centralized planners can guarantee no-duplication in goal assignment but lack flexibility and are computationally more expensive at execution time; group-based GCN fusion in CAE helps capture cross-group relationships necessary for collision avoidance and coordinated flight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MASP: Scalable GNN-based Planning for Multi-Agent Navigation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph neural networks for decentralized multi-robot path planning <em>(Rating: 2)</em></li>
                <li>Goal assignment and trajectory planning for large teams of interchangeable robots <em>(Rating: 2)</em></li>
                <li>Overview: Generalizations of multi-agent path finding to real-world scenarios <em>(Rating: 2)</em></li>
                <li>Learning graph-enhanced commander-executor for multiagent navigation <em>(Rating: 2)</em></li>
                <li>M*: A complete multirobot path planning algorithm with performance bounds <em>(Rating: 1)</em></li>
                <li>Voronoi-based multi-robot autonomous exploration in unknown environments via deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1247",
    "paper_id": "paper-265659446",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "MPE",
            "name_full": "Multi-Agent Particle Environment (MPE)",
            "brief_description": "A classical 2-D multi-agent navigation benchmark with discrete agent actions, circular agents/landmarks/obstacles, randomized spawns, and configurable agent/landmark/obstacle counts and map areas; used here to evaluate multi-agent navigation and goal assignment methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Multi-Agent Particle Environment (MPE)",
            "environment_description": "2-D continuous-space multi-agent navigation domain (agents/landmarks/obstacles are circles) with discrete actions (Up/Down/Left/Right); tasks require multiple agents to reach unassigned landmarks under collisions and obstacles.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Agent and goal representations are modeled as fully connected graphs (initial edge features = 1); during low-level execution agents are split into fully connected subgraphs (groups) of size N_g where agent k is present in every group; the environment geometry itself is not represented as an explicit graph topology in the paper.",
            "environment_size": "Experiments use N ∈ {5, 20, 50} agents and equal number of landmarks; obstacles B ∈ {5, 10, 30}; map areas considered: 4 m^2, 64 m^2, 400 m^2; horizons: 18, 45, 90 timesteps respectively.",
            "agent_name": "MASP (Goal Matcher + Coordinated Action Executor)",
            "agent_description": "Hierarchical goal-conditioned policy: high-level Goal Matcher (graph-based Self-Encoder + Cross-Encoder) assigns goals in a decentralized fashion; low-level Coordinated Action Executor uses Group Information Fusion (GCN over fully connected subgraphs/groups) to produce actions toward assigned goals; trained with MAPPO under CTDE.",
            "exploration_efficiency_metric": "Primary: Steps (timesteps to achieve target Success Rate). Also: Success Rate (SR) and Collision Rate (CR).",
            "exploration_efficiency_value": "MASP reported Steps: N=5: 138.35 (±13.57) timesteps; N=20: 315.31 (±14.52) timesteps. (Steps for N=50 not tabulated explicitly in table but text reports MASP achieves nearly 100% SR at N=50.)",
            "success_rate": "MASP: N=5 SR = 1.00 (100%); N=20 SR = 0.97 (97%); N=50 SR ≈ 1.00 (nearly 100%) as reported in text.",
            "optimal_policy_type": "Hierarchical, graph-based, goal-conditioned policies (MASP's combination of graph encoders at high level and GCN-based group fusion at low level) — i.e., a planning-aware hierarchical RL policy — perform best under large agent counts and complex maps.",
            "topology_performance_relationship": "Paper-level findings: (1) Modeling agents/goals as fully connected graphs and using cross/self-attention improves goal assignment and reduces collisions relative to MLP backbones; (2) converting agents into small fully connected subgraphs (group size N_g) and aggregating agent features across groups improves cooperation and training efficiency, with diminishing returns beyond N_g=3; (3) planning-based environment partitioning (Voronoi) helps in small-agent regimes but is less effective in large/complex settings; (4) centralized Hungarian assignment prevents goal duplication (high SR) but incurs greater per-step computation and does not adapt well to varying team sizes; no explicit numeric relationships to graph-theoretic measures (diameter/clustering) are provided.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Comparisons are across environment configurations and planning/partitioning strategies rather than formal graph-theoretic topologies: (a) Voronoi partitioning helps coordination with few agents but struggles in high-complexity 3-D cases; (b) RRT* performs comparably when obstacles dominate but omits complex constraints; (c) MASP outperforms planning baselines in Steps as agent count grows (e.g., MASP requires ~19.12% fewer Steps than best planning baseline at N=50 in MPE according to text); (d) group-size (subgraph) topology: group size 3 balances performance and input dimensionality — larger groups give minor gains at higher cost.",
            "policy_structure_findings": "Policies that explicitly encode inter-agent relations (graph Self-Encoder/Cross-Encoder at high level; GCN-based group fusion at low level) produce stronger cooperation, lower collision rates, and better sample/execution efficiency than MLP backbones or neighbor-limited policies; group-structured inputs (agent included in every group) and frequent high-level goal reassignment (smaller global-step intervals) improve final performance but increase runtime.",
            "uuid": "e1247.0",
            "source_info": {
                "paper_title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "OmniDrones",
            "name_full": "OmniDrones 3-D Quadrotor Simulator",
            "brief_description": "A 3-D drone-control simulator with continuous action throttle per motor, realistic collision/crash dynamics, and randomized spawn locations for drones and landmarks; used to evaluate multi-agent navigation in complex 3-D continuous control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "OmniDrones (quadrotor 3-D environment)",
            "environment_description": "3-D simulated quadrotor navigation domain (continuous action space: motor throttles) where collisions cause crashes and episode failures; used to evaluate multi-agent navigation with complex dynamics and larger exploration spaces.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "As with MPE, agent and goal relationships are modeled as fully connected graphs in the Goal Matcher; environment space is continuous 3-D and not represented as an explicit graph topology in the paper.",
            "environment_size": "Experiments run with N ∈ {5, 20} drones; map areas: 36 m^2 (for 5 drones) and 256 m^2 (for 20 drones); horizons: 300 timesteps (5 drones) and 500 timesteps (20 drones).",
            "agent_name": "MASP (Goal Matcher + Coordinated Action Executor)",
            "agent_description": "Same hierarchical MASP architecture: high-level graph-based goal assignment (Self-Encoder + Cross-Encoder) and low-level GCN-based Group Information Fusion for coordinated continuous-control navigation; trained via MAPPO.",
            "exploration_efficiency_metric": "Same metrics: primary metric Steps (timesteps to reach target SR), plus Success Rate (SR) and Collision Rate (CR).",
            "exploration_efficiency_value": "Reported success rates: MASP achieves SR = 1.00 (100%) at N=5 and SR = 0.96 (96%) at N=20; exact Steps per configuration are reported in tables (MASP Steps values are substantially lower than planning baselines; planning baselines require ≥ 27.92% more Steps than MASP at N=20 according to the text).",
            "success_rate": "MASP: N=5 SR = 1.00 (100%); N=20 SR = 0.96 (96%). Planning baselines often achieve high SR (&gt;90%) but with larger Steps and less adaptability to varying team sizes.",
            "optimal_policy_type": "A hierarchical, graph-structured, goal-conditioned RL policy (MASP) that combines graph encoders for assignment and group-GCN for execution is most effective in complex 3-D continuous-control navigation under agent loss/variable team sizes.",
            "topology_performance_relationship": "Findings: (1) In complex 3-D environments, MLP-based goal assignment (e.g., MAGE-X) degrades significantly as agent count increases; (2) planning-based partitioning (Voronoi) can perform well for few agents but struggles to scale to complex 3-D coordination; (3) MASP's graph-based assignment and group fusion lead to higher SR and fewer Steps in 3-D, demonstrating the value of explicit relation modeling in higher-dimensional continuous spaces; no formal graph-theoretic metrics (diameter, clustering coefficient, dead-end counts) are provided.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Paper contrasts 2-D vs 3-D dynamics and map sizes: planning baselines keep high SR in many OmniDrones settings due to centralized goal assignment, but they fail to adapt to agent loss or varying team sizes; MASP generalizes better zero-shot to team-size changes and keeps high SR with fewer Steps than planners (planning baselines require ≥ 27.92% more Steps at N=20).",
            "policy_structure_findings": "Graph-structured hierarchical policies are more robust in 3-D continuous dynamics; centralized planners can guarantee no-duplication in goal assignment but lack flexibility and are computationally more expensive at execution time; group-based GCN fusion in CAE helps capture cross-group relationships necessary for collision avoidance and coordinated flight.",
            "uuid": "e1247.1",
            "source_info": {
                "paper_title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph neural networks for decentralized multi-robot path planning",
            "rating": 2,
            "sanitized_title": "graph_neural_networks_for_decentralized_multirobot_path_planning"
        },
        {
            "paper_title": "Goal assignment and trajectory planning for large teams of interchangeable robots",
            "rating": 2,
            "sanitized_title": "goal_assignment_and_trajectory_planning_for_large_teams_of_interchangeable_robots"
        },
        {
            "paper_title": "Overview: Generalizations of multi-agent path finding to real-world scenarios",
            "rating": 2,
            "sanitized_title": "overview_generalizations_of_multiagent_path_finding_to_realworld_scenarios"
        },
        {
            "paper_title": "Learning graph-enhanced commander-executor for multiagent navigation",
            "rating": 2,
            "sanitized_title": "learning_graphenhanced_commanderexecutor_for_multiagent_navigation"
        },
        {
            "paper_title": "M*: A complete multirobot path planning algorithm with performance bounds",
            "rating": 1,
            "sanitized_title": "m_a_complete_multirobot_path_planning_algorithm_with_performance_bounds"
        },
        {
            "paper_title": "Voronoi-based multi-robot autonomous exploration in unknown environments via deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "voronoibased_multirobot_autonomous_exploration_in_unknown_environments_via_deep_reinforcement_learning"
        }
    ],
    "cost": 0.01149975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MASP: Scalable Graph-based Planning towards Multi-Agent Navigation</p>
<p>Xinyi Yang 
Department of Electronic Engineering
Tsinghua University
100084BeijingChina</p>
<p>Xinting Yang 
Yu Chao 
Department of Electronic Engineering
Tsinghua University
100084BeijingChina</p>
<p>Jiayu Chen 
Department of Electronic Engineering
Tsinghua University
100084BeijingChina</p>
<p>Wenbo Ding 
Department of Electronic Engineering
Tsinghua University
100084BeijingChina</p>
<p>Shenzhen International Graduate School
Tsinghua University
518055ShenzhenChina</p>
<p>Huazhong Yang 
Yu Wang yu-wang@tsinghua.edu.cn 
Department of Electronic Engineering
Tsinghua University
100084BeijingChina</p>
<p>Equal Contribution 
Department of Electronic Engineering
Tsinghua University
100084BeijingChina</p>
<p>MASP: Scalable Graph-based Planning towards Multi-Agent Navigation
840D3E33D519935E46AE83F94A7F69CD
We investigate multi-agent navigation tasks, where multiple agents need to reach initially unassigned goals in a limited time.Classical planning-based methods suffer from expensive computation overhead at each step and offer limited expressiveness for complex cooperation strategies.In contrast, reinforcement learning (RL) has recently become a popular approach for addressing this issue.However, RL struggles with low data efficiency and cooperation when directly exploring (nearly) optimal policies in a large exploration space, especially with an increased number of agents (e.g., 10+ agents) or in complex environments (e.g., 3-D simulators).In this paper, we propose the Multi-Agent Scalable Graph-based Planner (MASP), a goal-conditioned hierarchical planner for navigation tasks with a substantial number of agents in the decentralized setting.MASP employs a hierarchical framework to reduce space complexity by decomposing a large exploration space into multiple goal-conditioned subspaces, where a high-level policy assigns agents goals, and a lowlevel policy navigates agents toward designated goals.For agent cooperation and the adaptation to varying team sizes, we model agents and goals as graphs to better capture their relationship.The high-level policy, the Goal Matcher, leverages a graph-based Self-Encoder and Cross-Encoder to optimize goal assignment by updating the agent and the goal graphs.The low-level policy, the Coordinated Action Executor, introduces the Group Information Fusion to facilitate group division and extract agent relationships across groups, enhancing training efficiency for agent cooperation.The results demonstrate that MASP outperforms RL and planning-based baselines in task efficiency.Compared to the planning-based competitors with a centralized goal assignment method, MASP improves task efficiency by over 19.12% in multiagent particle environments with 50 agents and 27.92% in a quadrotor 3-dimensional environment with 20 agents, achieving at least a 47.87% enhancement across varying team sizes.</p>
<p>I. INTRODUCTION</p>
<p>Navigation is an important task for intelligent embodied agents and is widely applied in various domains, including autonomous driving [1], [2], logistics and transportation [3], [4], and disaster rescue [5], [6].This paper considers a multi-agent navigation problem where multiple agents make decisions independently and navigate simultaneously toward a set of initially unassigned goals.</p>
<p>Planning-based solutions have been extensively employed in multi-agent navigation tasks [7], [8].However, these meth-ods often struggle with coordination strategies, demanding intricate hyper-parameter tuning for each specific scenario.Moreover, they can be time-consuming due to frequent re-planning at each decision step.Conversely, reinforcement learning (RL) offers its remarkable expressiveness in multi-agent navigation tasks [9], [10], [11] with strong representation capabilities for complex strategies and low inference overhead once policies are well-trained.However, directly learning an end-to-end policy [12], [13] from a large exploration space results in low sample efficiency and limited cooperation, which is more severe as the number of agents or the complexity of the environment increases.Moreover, classical RL policies tend to overfit to fixed and trained agent numbers and often exhibit poor generalization in varying agent numbers.Therefore, existing methods [14], [15] mainly focus on simple scenarios with a few fixed agent numbers.</p>
<p>To solve these challenges, we follow the centralized training with decentralized execution (CTDE) paradigm, where agents make independent decisions.We adopt a hierarchical framework, Multi-Agent Scalable Graph-based Planner (MASP).This framework consists of a high-level policy that assigns goals to agents at each global step and a low-level policy that navigates agents toward the designated goals, breaking down a large exploration space into multiple goal-conditioned subspaces.We model the agents and the goals as graphs with expandable nodes to better capture their relationships and adapt to the varying team sizes.The high-level policy, the Goal Matcher, develops a graph-based Self-Encoder and Cross-Encoder to update the agent and the goal graphs, optimizing goal assignment.The low-level policy, the Coordinated Action Executor introduces the Group Information Fusion to segment agents into groups, transform groups into subgraphs, and extract relationships between agents across subgraphs, reducing the input dimension of the network and enhancing training efficiency for agent cooperation.</p>
<p>We compare MASP with planning-based methods and RL-based competitors in multi-agent particle environments (MPE) [16] and a quadrotor environment (OmniDrones) [17].We remark that OmniDrones is a complex 3-dimensional (3-D) environment, further increasing the exploration space.Empirical results demonstrate that MASP significantly outperforms its competitors in task and execution efficiency.</p>
<p>Our contributions are summarized as follows:</p>
<p>• We propose a hierarchical multi-agent navigation planner, Multi-Agent Scalable Graph-based Planner (MASP), to enhance sample efficiency in a large exploration space.</p>
<p>II. RELATED WORK</p>
<p>A. Multi-agent Navigation</p>
<p>Multi-agent navigation [9], [10], [18], [19] is a typical cooperative task in robotics.However, finding a (nearly) optimal strategy in a large search space is difficult as the number of agents grows.[19] reduces the search space by imitating an expert policy.[20] only considers adjacent agents to break the curse of dimensionality.However, it ignores interactions among all agents, increasing collisions.[10] introduces a hierarchical approach with centralized goal allocation and a subgraph construction for agent cooperation.Nonetheless, [10] can not adapt to varying agent numbers without an agent-invariant design.This paper proposes a graph-based hierarchical framework that adapts to dynamic agent numbers in MPE and a 3-D quadrotor simulator, Omnidrones.</p>
<p>B. Goal Assignment</p>
<p>In multi-agent navigation, assigning suitable targets to agents enhances efficiency [21], [22], [23].The Hungarian algorithm [21] uses a weighted bipartite graph to find a matching with maximum weight.However, it requires equal numbers of agents and goals, limiting its use in scenarios with dynamic agent numbers.[22] allocates goals to minimize the longest path of the trajectories.However, the high time expenditure makes it difficult to extend to large-scale multiagent settings.In [24], the targets are only reassigned when a collision is about to occur, effectively reducing the computational costs.Nevertheless, this method ignores the collaboration between agents.This work proposes an RL-based goal assignment method that can extend to scenarios with large and varying agent numbers and ensure effective cooperation.</p>
<p>C. Goal-conditioned HRL</p>
<p>Goal-conditioned hierarchical reinforcement learning (HRL) is effective across various tasks [25], [26], typically using a high-level policy to select subgoals and a lowlevel policy to predicts environmental actions to achieve these subgoals.However, existing approaches [27], [28], [29] struggle with scenarios with multiple initially unassigned targets, failing to establish connections between subgoals and multiple final targets.[10] and [30] adopt a hierarchical framework in multi-agent navigation, where the high-level policy assigns each agent a target, and the low-level policy yields the environmental actions based on the assigned target.Inspired by [10], [30], we develop a goal assignment module as the high-level policy to assign agents with final targets.</p>
<p>III. PRELIMINARY</p>
<p>We formulate multi-agent navigation tasks as goalconditioned Markov Decision Processes (MDPs) M = <N , G, S, A, P, R, γ>.N ≡ {1, ..., N } is the set of N agents.G is a goal set.S is the joint state space.A is the joint action space.P is the transition function.R is the reward function.γ is the discount factor.In our task, we establish a hierarchical framework with a high-level policy π h θ h (g|s) and a low-level policy π l θ l (a|s, g) parameterized by two neural networks with parameters θ h and θ l respectively.Each agent's state space includes the locations and the velocities of all agents and the locations of goals.The high-level policy aims to maximize the accumulative reward for goal assignment and generates a high-level action, i.e. a goal g t ∼ π h θ h (g|s) ∈ G every global step, i.e., 3 timesteps.The low-level policy receives these goals, seeks to maximize the accumulative reward for goal achievement, and performs a local action a t ∼ π l θ l (a|s, g) ∈ A at every timestep.</p>
<p>IV. METHODOLOGY</p>
<p>A. Overview</p>
<p>To solve the issues of low sample efficiency in navigation tasks with substantial numbers of agents, we adopt a hierarchical framework, consisting of a high-level policy, the Goal Matcher (GM), that assigns agents with goals, and a low-level policy, the Coordinated Action Executor (CAE), that encourages agents to navigate towards the designated goals.This divides a large exploration space into multiple goal-conditioned subspaces, reducing space complexity and speeding up training.Moreover, we model the agents and goals as graphs with expandable nodes to facilitate cooperation and adapt to varying team sizes.As depicted in Fig. 1, we introduce this overall framework, the Multi-Agent Scalable Graph-based Planner.</p>
<p>Take agent k as an example.We first consider two graphs: the agent graph representing agent information and the goal graph representing goal information.GM takes in these graphs to capture the relationship among agents and goals, and yields the most appropriate goal for agent k at each global step.In CAE, agent k extracts the designated goal's features as a goal embedding.Meanwhile, to reduce the input dimension of the network and improve training efficiency for agent cooperation in CAE, we develop the Group Information Fusion to segment the agents into groups, transform groups into subgraphs, and capture the relationships among agents across subgraphs.The Group Information Fusion provides an updated feature of agent k across all subgraphs, which, along with the goal embedding, passes through a multi-layer perception (MLP) layer to endow the team representation with strong goal guidance.Finally, agent k takes environmental actions to navigate towards the goal based on the extracted representation.</p>
<p>We adopt the centralized training with decentralized execution (CTDE) paradigm, where agents make independent decisions.We train GM and CAE simultaneously by using multiagent proximal policy optimization (MAPPO) [12], a multiagent variant of proximal policy optimization (PPO) [31].</p>
<p>B. Goal Matcher</p>
<p>Goal assignment is a long-studied NP-hard maximum matching problem [22], [32].The Hungarian algorithm [33] is a classical combinatorial optimization algorithm that utilizes the bipartite graph matching to solve the assignment problem.However, it assigns goals centrally, requiring equal numbers of agents and goals.With varying agent numbers, we need to first assign a part of the goals, and then assign the remaining goals, causing inefficiency.To solve this problem, we adopt a decentralized approach where each agent selects a goal independently at each global step, allowing flexibility in goal selection and adjustment.Additionally, we leverage graphs with expandable nodes to represent agents and goals respectively, handling the issue of varying agent numbers.</p>
<p>However, this decentralized setting potentially leads to multiple agents pursuing the same goal-something impossible under centralized decision-making.For better goal assignment, we introduce an RL-based high-level policy, the Goal Matcher.In GM, agent k first receives two fully connected graphs: the agent graph, G A (V, E), and the goal graph, G T (V, E).V is the node set representing the position information of agents or goals, and E is the edge set representing the connection between agents or goals.The edge features in E are all initially set to 1. Afterward, we leverage the Self-Encoder to perceive the spatial relationship between the agents or goals and update the node and the edge features.We then use the Cross-Encoder to extract the relationship between agent k and the goals and calculate the matching score of agent k.Finally, agent k chooses the goal with the highest matching score at each global step.</p>
<p>The reward for agent k, R k GM , encourages agents to select different goals with minimal total distance, which is formulated as follows:
R k GM =    0, g k m == g k h ; −(1 + Nrepeat N ), g k m ̸ = g k h and N repeat &gt; 0; −(1 − D h Dm ), g k m ̸ = g k h and N repeat = 0.
(1) Here, N repeat signifies the number of other agents whose predicted goal is the same as agent k.We use N repeat to promote agents to select different goals.g k m represents the predicted goal from GM for agent k, and g k h denotes the goal assigned by the Hungarian algorithm.D m and D h denote the total distance from the predicted and Hungarian-assigned goals to the agents, respectively.If our method has a lower distance cost, R k GM becomes positive, incentivizing the Goal-Matcher to learn an better goal assignment strategy.</p>
<p>1) Self-Encoder: We use the Self-Encoder to update G A and G T respectively.It captures the relationships between any two nodes and updates the edges and nodes in the graph.Inspired by the attention mechanism [34], we first compute a matrix, S inter ∈ R N ×N , as the updated edge features.
S inter = Sof tmax W query X T W key X T T , (2)
where X ∈ R N ×L signifies N node features, each with L = 32 dimensions, and W ∈ R N ×L denotes the linear projections of X.</p>
<p>Each node then updates its feature by embedding it with a weighted sum of its neighbors' features via an MLP layer.
Y = X + M LP Concat X, W value X T S inter T ,(3)
2) Cross-Encoder: The Cross-Encoder takes in the updated node feature for agent k, Y A k ∈ R 1×L , and all updated node features for goals, Y T ∈ R N ×L , where L is 32 in our work.It captures the position correlations between agent k and goals and calculates matching scores between them, S k intra , using the node features and L2 distance, D, via the attention mechanism [34].The calculation is formulated as follows:
S k intra = Sof tmax M LP Concat Ỹ A k , ỸT , D , (4) where W is the linear projections of Y A k or Y T . Ỹ A k is W query Y A k T and ỸT is W key Y T T .
Fig. 2: The workflow of the Group Information Fusion in the Coordinated Action Executor.Take Agent k as an example.</p>
<p>C. Coordinated Action Executor</p>
<p>We introduce the Coordinated Action Executor as the lowlevel policy to encourage agents to reach the assigned goals.Firstly, to reduce the input dimension of the network and enhance the training efficiency for agent cooperation, we develop the Group Information Fusion to divide the agents into groups and extract the relationships between agents across groups.Fig. 2 illustrates the workflow of the Group Information Fusion for agent k.For the group division in the Group Information Fusion, agent k is required to participate in every group to maintain connections with other agents.Each group consists of N g = 3 agents, so we first divide the remaining agents, excluding agent k, into groups, each consisting of N g − 1 agents.If the remaining agents can not be evenly divided, we randomly select one agent to join more than one group to ensure group formation.Finally, we add agent k to each group to complete the group division process.We transform each group into a fully connected subgraph, where the nodes contain the state information of the agents, and the edges are set to 1.The Group Information Fusion utilizes graph convolutional networks (GCN) to perceive the interaction between agent k and other agents in each group and update the node feature.Subsequently, to obtain the updated feature of agent k that infuses the relationship between agent k and all other agents, we aggregate the updated node feature of agent k across all groups via a mean operation over the group dim.</p>
<p>Meanwhile, we extract the designated goal embedding for agent k via an MLP layer.Receiving the goal embedding and the updated feature of agent k, we apply another MLP layer to perceive the correlation between the designated goal and agent k.Finally, based on the extracted correlation, we produce the environmental action for agent k to navigate toward the goal.</p>
<p>The reward, R k CAE , for agent k in the CAE promotes goaldirected navigation while minimizing collisions.R k CAE is a linear combination of the complete bonus, R b , the distance penalty, R d , and the collision penalty, R c :
R k CAE = αR k b + βR k d + γR k c ,(5)
where α, β and γ are the coefficients of R k b , R k d and R k c , respectively.</p>
<p>V. EXPERIMENTS</p>
<p>A. Testbeds</p>
<p>To evaluate the effectiveness of our approach in large exploration spaces, we opt for two environments: MPE [16] and OmniDrones [17], with a substantial number of agents.</p>
<p>1) MPE: MPE is a classical 2-dimensional environment.The collision between agents causes disruptive bounce-off, reducing navigation efficiency.The available environmental actions of the agents are discrete, including Up, Down, Left, and Right.The experiment is conducted with randomized spawn locations for N agents, N landmarks and B obstacles.The agents, the obstacles, and the landmarks are circular with a radius of 0.1m, 0.1m, and 0.05m, respectively.We consider N ∈ {5, 20, 50} and B ∈ {5, 10, 30} on the maps of 4m 2 , 64m 2 , and 400m 2 , with the horizons of the environmental steps of 18, 45, and 90, respectively.</p>
<p>2) OmniDrones: We also conduct experiments in Om-niDrones, an efficient and flexible 3-D simulator for drone control, which further increases the exploration space.The continuous action space of each drone is the throttle for each motor.Collisions directly lead to crashes and task failures, increasing task difficulty.The drones are hummingbirds with 0.17m arms and 4 motors.The landmarks are virtual balls of 0.05m radius.We conduct experiments with N ∈ {5, 20} drones, where the spawn locations for both drones and landmarks are randomly distributed on the maps sized at 36m 2 and 256m 2 , respectively.Besides, the horizons of the environmental steps are 300 for 5 drones and 500 for 20 drones.</p>
<p>B. Implementation Details</p>
<p>Each RL training is performed over 3 random seeds for a fair comparison.Each evaluation score is expressed in the format of "mean (standard deviation)", averaged over a total of 300 testing episodes, i.e., 100 episodes per random seed.</p>
<p>C. Evaluation Metrics</p>
<p>We consider 3 statistical metrics to capture different characteristics of a particular navigation strategy.</p>
<p>• Success Rate (SR): This metric measures the average ratio of landmarks reached by the agents to the total landmarks per episode.</p>
<p>• Steps: This metric represents the average timesteps required to achieve a target Success Rate per episode.• Collision Rate (CR): This metric denotes the average ratio of collision occurrences to the total number of environmental steps per episode.We remark that we consider Steps as the primary metric for measuring task efficiency.In the tables, the backslash in Steps denotes the agents can not reach the target 100% Success Rate in any episode.</p>
<p>D. Baselines</p>
<p>We compare MASP with three representative planningbased approaches (ORCA, RRT*, Voronoi) and four prominent RL-based solutions (MAPPO, DARL1N, HTAMP, MAGE-X).In planning-based approaches, we utilize the Hungarian algorithm represented as '(H)' for goal assignment.</p>
<p>• ORCA [35]: ORCA is an obstacle avoidance algorithm that excels in multi-agent scenarios.It predicts the movements of surrounding obstacles and other agents, and then infers a collision-free velocity for each agent.• RRT<em> [8]: RRT</em> is a sample-based planning algorithm that builds upon the RRT algorithm [36].It first finds a feasible path by sampling points in the space and then iteratively refines the path to achieve an asymptotically optimal solution.• Voronoi [37]: A Voronoi diagram comprises a set of continuous polygons, with a vertical bisector of lines connecting two adjacent points.By partitioning the map into Voronoi units, a path can be planned from the starting point to the destination at a safe distance.• MAPPO [12]: MAPPO is a straightforward extension of PPO in the multi-agent setting, where each agent is equipped with a policy with a shared set of parameters.We update this shared policy based on the aggregated trajectories from all agents.We additionally apply the attention mechanism to enhance the model's performance.• DARL1N [20]: DARL1N is under an independent decision-making setting for large-scale agent scenarios.</p>
<p>It breaks the curse of dimensionality by restricting the agent interactions to one-hop neighborhoods.</p>
<p>• HTAMP [30]: This hierarchical method for task and motion planning via reinforcement learning integrates highlevel task generation with low-level action execution.• MAGE-X [10]: This is a hierarchical approach to multi-agent navigation tasks.It first centrally allocates target goals to the agents at the beginning of the episode and then utilizes GNN to construct a subgraph only with important neighbors for higher cooperation.</p>
<p>E. Main Results</p>
<p>1) Training with a Fixed Team Size: MPE: We present the training curves in Fig. 3 and the evaluation performance in Tab.I. MASP outperforms all methods with the least training data, with its advantage in Steps and Success Rate becoming more evident as agent numbers increase.MASP presents the Collision Rate, reflecting strong agent cooperation.Especially in scenarios with 50 agents, MASP achieves nearly 100% Success Rate, while other RL baselines fail to complete the task in any episode.Among RL baselines, DARL1N has a 49% Success Rate at N = 50, while it shows the worst performance at N = 5.This implies that when the scenario only has a few agents, it may lead to the absence of one-hop neighbors, thereby influencing the agents' decisions.MAGE-X has a 23% Success Rate at N = 50, suggesting its MLP-based goal assignment encounters challenges in preventing collisions between a large number of agents.MAPPO has only a 0.01% Success Rate at N = 50, showing difficulty in extracting agent correlations and identifying goals.(H)MAPPO has a comparable performance with a fixed agent number and requires 8.52% more Steps than MASP at N = 50.Although HTAMP has a hierarchical framework, its MLP backbone fails to perceive the relationship between agents and targets, exhibiting inferior performance.</p>
<p>Regarding planning-based methods, (H)RRT<em> and (H)Voronoi shows a comparable performance.This suggests that (H)RRT</em> omits complex constraints, suitable for scenarios with obstacles.(H)Voronoi partitions the map to promise coordination.However, they still require around 19.12% more Steps than MASP at N = 50.(H)ORCA performs worse, needing at least 21.76% more Steps than MASP, highlighting their difficulty in finding shorter navigational paths.</p>
<p>OmniDrones: We report the training performance in Fig. 4 and the evaluation results in Sec.V-D.MASP is superior to all competitors, achieving a 100% Success Rate at N = 5 and 96% at N = 20 in this complex 3-D environment.As for the Collision Rate, MASP outperforms other baselines due to high agent cooperation.Most RL baselines, except MAGE-X and (H)MAPPO, achieve only around 50% Success Rate at N = 5, failing to address the navigation problem in the complex search space.However, MAGE-X drops to a 15% Success Rate at N = 20.This reveals that its MLP-based goal assignment struggles with large and complex environments.</p>
<p>In contrast to RL baselines, most planning-based baselines achieve over 90% Success Rate, primarily owing to the goal assignment algorithm without duplication.Thus, we focus more on Steps for comparison.(H)Voronoi is the best planning-based competitor at N = 5, while all the planningbased baselines show comparable performance at N = 20.This indicates that although (H)Voronoi's partitions excel in scenarios with a few agents, they are difficult to figure out a set of coordinated navigation paths in complex 3-D environments     with increased agent numbers.The planning-based baselines requires at least 27.92% more Steps than MASP at N = 20.</p>
<p>2) Varying Team Sizes within an Episode: Due to unstable communication or agent loss, we further consider scenarios where the team size decreases during an episode.With more goals than agents, some agents may need to first reach some goals and then adjust their plans to navigate towards the remaining goals.The Success Rate is defined as the ratio of the landmarks reached during the navigation to the total number of landmarks.We use "N 1 ⇒ N 2 " to denote that an episode starts with N 1 agents and switches to N 2 after one-third of the total timesteps.The number of landmarks is N 1 .Since MASP is trained with a fixed team size, this Methods ORCA RRT* Voronoi MAPPO DARL1N HTAMP MAGE-X MASP Time(ms) 39( 6) 456( 10) 44( 5)</p>
<p>13( 4) 16 This indicates that using the Hungarian algorithm for goal assignment is more challenging for varying team sizes.</p>
<p>OmniDrones: Sec.V-E.1 presents evaluation results for zero-shot generalization in OmniDrones.The planning-based baselines fail to reach a 100% Success Rate in any episode.This challenge stems from the absence of certain agents, therefore the remaining agents need to dynamically adjust their plans to reach more than one goal.This is particularly difficult in such a complex 3-D environment.Conversely, MASP maintains an average Success Rate of 94% due to its flexible and adaptive strategy.</p>
<p>F. Execution Efficiency</p>
<p>To evaluate the execution efficiency, we compute the per-step time of baselines and MASP in MPE with 50 agents, excluding environmental execution time.In Tab.V, MASP is at least 3× more efficient than planning-based competitors.G. Sensitivity Analysis 1) Group Size: As shown in Tab.VII, we perform a sensitivity analysis of group size for group division in CAE in MPE with 20 agents.When the group size is too small, it hinders capturing the correlations between agents.A larger group size improves the performance but increases network input dimensionality, depreciating the training efficiency.The performance improvement of MASP is negligible when the group size exceeds 3. Thus, we select a group size of 3 to balance the final performance and the training efficiency.</p>
<p>2) Global Steps: We report the evaluation performance in Tab.VIII to assess the sensitivity to global steps in MPE with 20 agents.Fewer global steps lead to more frequent goal assignment adjustments, improving the final performance.However, it also increases execution time.The results indicate that the performance remains comparable when the global steps are fewer than 3. To balance the final performance and the execution time, we set the global steps to 3.</p>
<p>H. Ablation Studies</p>
<p>To illustrate the effectiveness of each component of MASP, we consider 3 variants of our method in MPE with 20 agents:</p>
<p>• MASP w.RG: We substitute GM with random sampling without replacement to assign each agent a random target goal.As depicted in Fig. 6, we compare the learned strategies of DARL1N and MASP to showcase the high cooperation among MASP's agents.Specifically, agents making independent decisions potentially lead to situations where the agent in the red pentagon and the agent in the yellow pentagon select the same goal.In Fig. 6(a), these two DARL1N agents fail to adjust their strategies and finally stay at the same target.In contrast, Fig. 6(b) illustrates that when the red MASP agent notices the yellow MASP agent moving toward the same goal, it quickly adjusts its strategy to navigate toward an unexplored goal.This cooperation allows MASP agents to occupy different goals, achieving a 100% Success Rate.</p>
<p>I. Strategies Analysis</p>
<p>VI. CONCLUSION AND FUTURE WORK</p>
<p>We propose a decentralized goal-conditioned hierarchical planner, the Multi-Agent Scalable Graph-based Planner, to improve data efficiency and cooperation in navigation tasks with a substantial number of agents.The high-level policy, the Goal Matcher, leverages a Self-Encoder and a Cross-Encoder to allocate target goals to agents at each global step.The low-level policy, the Coordinated Action Executor, develops the Group Information Fusion to navigate agents toward their designated goals while ensuring effective cooperation.Thorough experiments demonstrate that MASP achieves higher task and execution efficiency than planning-based baselines and RL competitors in MPE and Omnidrones with large and varying numbers of agents.</p>
<p>Fig. 1 :
1
Fig. 1: Overview of the Multi-Agent Scalable Graph-based Planner.Here, we take Agent k as an example.</p>
<p>Fig. 3 :
3
Fig. 3: Comparison between MASP and other baselines in MPE with N = 5, 20, 50.</p>
<p>Fig. 4 :
4
Fig. 4: Comparison between MASP and other baselines in Omnidrones with N = 5, 20.</p>
<p>goals for agent k, with the position information of agent k listed first.•CAE w.o.Graph: We use the MLP layer as an alternative to CAE to capture the correlation between agents and goals.The input for agent k comprises the position of all agents and the assigned goal for agent k.To demonstrate the performance of MASP and its variants, we report the training curves in Fig.5and the evaluation results in Tab.VI.MASP outperforms its counterparts with a 100% Success Rate and the fewest Steps.</p>
<p>Fig. 5 :
5
Fig. 5: Ablation study on MASP in MPE with 20 agents.GM w.o.Graph degrades the most, indicating difficulties in perceiving the correlations between agents and target goals without graph-based Self-Encoder and Cross-Encoder.MASP w.RG only trains the low-level policy, leading to faster training convergence.During evaluation, it consistently assigns different goals to agents, achieving a 100% Success Rate.However, it requires 32.02% more Steps due to the lack of an appropriate goal assignment.CAE w.o.Graph is slightly inferior to MASP with slower training convergence and 13.59% more Steps in the evaluation.This suggests that the GNN in CAE better captures the relationships between agents and assigned goals than the MLP layer.</p>
<p>(a) Learned strategy of DARL1N.(b) Learned strategy of MASP.</p>
<p>Fig. 6 :
6
Fig. 6: The comparison in learned strategies of DARL1N and MASP in MPE.</p>
<p>Dec 2024and finish goal assignment.In the low-level policy that navigates agents to their designated goals, we develop the Group Information Fusion to improve training efficiency and promote effective cooperation.
• Compared to the planning-based competitors using acentralized goal assignment method, MASP increasesexecution efficiency by at least 3× and enhances taskefficiency by over 19.12% in MPE with 50 agents and27.92% in OmniDrones with 20 agents, achieving atleast a 47.87% enhancement across varying team sizes.
• We introduce the high-level policy, the Goal Matcher, using a graph-based Self-Encoder and Cross-Encoder to capture the relationships between agents and goals arXiv:2312.02522v2[cs.LG] 2</p>
<p>TABLE I :
I
Performance of MASP, planning-based baselines and RL-based baselines with N = 5, 20, 50 agents in MPE.
Agents Metrics(H)ORCA(H)RRT*(H)Voronoi(H)MAPPOMAPPO DARL1N HTAMPMAGE-XMASPSteps ↓ 260.10(10.02) 236.10(9.47) 173.89(12.91) 185.34(7.14)\\231.26(19.32) 138.35(13.57)N =5SR ↑0.98(0.03)0.97(0.01)0.99(0.02)1.00(0.00)0.50(0.09) 0.68(0.02) 0.32(0.08)0.99(0.01)1.00(0.00)CR ↓0.00(0.01)0.01(0.01)0.02(0.01)0.01(0.01)0.01(0.01) 0.01(0.01) 0.01(0.01)0.01(0.01)0.00(0.00)Steps ↓ 449.10(14.86) 438.23(11.02) 437.43(14.24) 335.74(13.07)\\315.31(14.52)N =20SR ↑0.97(0.03)0.97(0.01)0.93(0.01)0.96(0.01)0.15(0.03) 0.15(0.01) 0.14(0.03)0.14(0.03)0.97(0.01)CR ↓0.01(0.03)0.03(0.03)0.01(0.03)0.01(0.01)0.03(0.02) 0.05(0.01) 0.04(0.02)0.08(0.02)0.01(0.01)</p>
<p>TABLE II :
II
Performance of MASP, planning-based baselines and RL-based baselines with N = 5, 20 agents in OmniDrones.
AgentsMetrics (H)ORCA (H)RRT* (H)VoronoiMASPSteps ↓ 43.64(3.86) 40.68(3.37) 40.18(3.41) 22.38(3.04)20 ⇒ 10SR ↑ 0.89(0.01) 0.92(0.01)0.93(0.01)1.00(0.00)CR ↓ 0.18(0.02)0.72(0.14)0.86(0.13)0.16(0.03)Steps ↓\89.18(4.88) 88.73(4.27) 46.49(4.49)50 ⇒ 20SR ↑ 0.86(0.01)0.91(0.010.91(0.02)1.00(0.00)CR ↓ 0.15(0.03)0.56(0.05)0.67(0.03)0.15(0.01)</p>
<p>TABLE III :
III
Performance of MASP and planning-based baselines with a varying team size in MPE.
AgentsMetrics (H)ORCA (H)RRT* (H)VoronoiMASPSteps ↓\490.35(15.63) 426.73(18.50)20 ⇒ 10SR ↑ 0.68(0.04) 0.61(0.04)0.85(0.03)0.94(0.01)CR ↓ 0.00(0.01) 0.01(0.01)0.01(0.01)0.01(0.01)</p>
<p>TABLE IV :
IV
Performance of MASP and planning-based baselines with a varying team size in OmniDrones.</p>
<p>TABLE V :
V
Average performance on the per-step time.As shown in Tab.III, compared to the setting with a fixed number of agents, the advantage of MASP becomes more apparent.MASP consumes around 47.87% fewer Steps than (H)RRT* and (H)Voronoi in the 50 ⇒ 20 scenario.
setup presents a zero-shot generalization challenge. We usethe MASP model trained with N = N 1 . For the Hungarianalgorithm which centrally assign agents goals, we randomlyselect a subset of unreached goals matching the agent countat each global step. As RL baselines lack generalization inboth network architecture and final performance, we compareMASP only with planning-based methods.MPE:</p>
<p>TABLE VI :
VI
Performance of MASP and RL variants with N = 20 agents in MPE.
Agents Metrics235Steps ↓ 28.85(2.12) 24.41(2.66) 23.78(2.35)N =20SR ↑ 1.00(0.00)1.00(0.00)1.00(0.00)CR ↓ 0.15(0.01) 0.14(0.01)0.14(0.01)</p>
<p>TABLE VII :
VII
Performance of Group Size = 2, 3, and 5.Especially compared to the best planning-based competitor, (H)RRT*, MASP's computation time is 32× faster than it.While MASP has a comparable execution efficiency to RL-based methods, it outperforms them in task performance.</p>
<p>We consider the MLP layer as an alternative to GM for assigning target goals to each agent at each global step.The MLP layer takes in the concatenated position information of all agents and
Agents Metrics1310N =20
• GM w.o.Graph: Steps ↓ 24.23(2.11)24.41(2.66)29.59(2.43)SR ↑ 1.00(0.00)1.00(0.00)1.00(0.00)CR ↓ 0.12(0.01)0.14(0.01)0.16(0.01)</p>
<dl>
<dt>TABLE VIII</dt>
<dt>VIII</dt>
<dd>Performance of Global Steps = 1, 3, and 10.</dd>
</dl>
<p>This research was supported by National Natural Science Foundation of China (No.62406159, 62325405), China Postdoctoral Science Foundation under Grant Number GZC20240830, 2024T170496.More information can be found at https://sites.google.com/view/masp-ral.
Simultaneous localization and mapping: A survey of current trends in autonomous driving. G Bresson, Z Alsayed, L Yu, IEEE Transactions on Intelligent Vehicles. 232017</p>
<p>A survey of deep learning techniques for autonomous driving. S Grigorescu, B Trasnea, T Cocias, G Macesanu, Journal of Field Robotics. 3732020</p>
<p>Application of beidou navigation satellite system in logistics and transportation. S Liu, L Hu, Logistics: The Emerging Frontiers of Transportation and Development in China. 2009</p>
<p>Multi-mobile robot autonomous navigation system for intelligent logistics. K Gao, J Xin, H Cheng, D Liu, J Li, 2018. 2018IEEE</p>
<p>Rfid technology-based exploration and slam for search and rescue. A Kleiner, J Prediger, 2006. 2006</p>
<p>Autonomous navigation and exploration in a rescue environment. D Calisi, A Farinelli, IEEE International Safety, Security and Rescue Rototics, Workshop. IEEE2005. 2005</p>
<p>Coordinated multi-robot exploration. W Burgard, M Moors, C Stachniss, F E Schneider, IEEE Transactions on robotics. 2132005</p>
<p>Sampling-based algorithms for optimal motion planning. S Karaman, E Frazzoli, The international journal of robotics research. 3072011</p>
<p>Learning efficient multi-agent cooperative visual exploration. C Yu, X Yang, J Gao, H Yang, Y Wang, Y Wu, European Conference on Computer Vision. Springer2022</p>
<p>Learning graph-enhanced commander-executor for multiagent navigation. X Yang, S Huang, Y Sun, Y Yang, C Yu, W.-W Tu, H Yang, Y Wang, Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. the 2023 International Conference on Autonomous Agents and Multiagent Systems2023</p>
<p>Inference-based hierarchical reinforcement learning for cooperative multi-agent navigation. L Xia, C Yu, Z Wu, 2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI). IEEE2021</p>
<p>C Yu, A Velu, E Vinitsky, Y Wang, A Bayen, Y Wu, arXiv:2103.01955The surprising effectiveness of ppo in cooperative, multi-agent games. 2021arXiv preprint</p>
<p>Multi-agent reinforcement learning is a sequence modeling problem. M Wen, J G Kuba, R Lin, W Zhang, Y Wen, J Wang, Y Yang, arXiv:2205.149532022arXiv preprint</p>
<p>Heterogeneous multi-agent reinforcement learning for unknown environment mapping. C Wakilpoor, P J Martin, C Rebhuhn, A Vu, arXiv:2010.026632020arXiv preprint</p>
<p>Multi-agent embodied visual semantic navigation with scene prior knowledge. X Liu, D Guo, H Liu, F Sun, arXiv:2109.095312021arXiv preprint</p>
<p>Multiagent actor-critic for mixed cooperative-competitive environments. R Lowe, Y I Wu, A Tamar, J Harb, O Pieter, Abbeel, Advances in neural information processing systems. 201730</p>
<p>Omnidrones: An efficient and flexible platform for reinforcement learning in drone control. B Xu, F Gao, C Yu, R Zhang, Y Wu, Y Wang, arXiv:2309.128252023arXiv preprint</p>
<p>Save: Spatial-attention visual exploration. X Yang, C Yu, J Gao, Y Wang, H Yang, 2022 IEEE International Conference on Image Processing (ICIP). IEEE2022</p>
<p>Graph neural networks for decentralized multi-robot path planning. Q Li, F Gama, A Ribeiro, A Prorok, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Darl1n: Distributed multi-agent reinforcement learning with one-hop neighbors. B Wang, J Xie, N Atanasov, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>The hungarian method for the assignment problem. H W Kuhn, Naval research logistics quarterly. 21-21955</p>
<p>Goal assignment and trajectory planning for large teams of interchangeable robots. M Turpin, K Mohta, N Michael, V Kumar, Autonomous Robots. 3742014</p>
<p>Overview: Generalizations of multi-agent path finding to real-world scenarios. H Ma, S Koenig, N Ayanian, L Cohen, W Hönig, T Kumar, T Uras, H Xu, arXiv:1702.055152017arXiv preprint</p>
<p>M*: A complete multirobot path planning algorithm with performance bounds. G Wagner, H Choset, 2011 IEEE/RSJ international conference on intelligent robots and systems. 2011</p>
<p>Hierarchical reinforcement learning for precise soccer shooting skills using a quadrupedal robot. Y Ji, Z Li, Y Sun, X B Peng, S Levine, G Berseth, K Sreenath, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks. S Nasiriany, H Liu, Y Zhu, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Hierarchical reinforcement learning framework for stochastic spaceflight campaign design. Y Takubo, H Chen, K Ho, Journal of Spacecraft and rockets. 5922022</p>
<p>Hierarchical reinforcement learning for air-to-air combat. A P Pope, J S Ide, D Mićović, 2021 international conference on unmanned aircraft systems (ICUAS). IEEE2021</p>
<p>Deep hierarchical planning from pixels. D Hafner, K.-H Lee, I Fischer, P Abbeel, Advances in Neural Information Processing Systems. 202235</p>
<p>Hierarchical task and motion planning through deep reinforcement learning. A A R Newaz, T Alam, 2021 Fifth IEEE International Conference on Robotic Computing (IRC). IEEE2021</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>A formal analysis and taxonomy of task allocation in multi-robot systems. B P Gerkey, M J Matarić, The International journal of robotics research. 2392004</p>
<p>Assignment problems: revised reprint. R Burkard, M Dell'amico, S Martello, 2012SIAM</p>
<p>Attention is all you need. A Vaswani, N Shazeer, Advances in neural information processing systems. 201730</p>
<p>Vr-orca: Variable responsibility optimal reciprocal collision avoidance. K Guo, D Wang, T Fan, J Pan, IEEE Robotics and Automation Letters. 632021</p>
<p>Rrt-connect: An efficient approach to single-query path planning. J J Kuffner, S M Lavalle, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065). 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia (Cat. No. 00CH37065)IEEE20002</p>
<p>Voronoi-based multi-robot autonomous exploration in unknown environments via deep reinforcement learning. J Hu, H Niu, J Carrasco, B Lennox, F Arvin, IEEE Transactions on Vehicular Technology. 69124232020</p>            </div>
        </div>

    </div>
</body>
</html>
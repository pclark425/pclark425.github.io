<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4946 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4946</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4946</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-d7f9bc1acde978072050b6fe2dfdb237f94e480e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7f9bc1acde978072050b6fe2dfdb237f94e480e" target="_blank">Knowledge Base Question Answering by Case-based Reasoning over Subgraphs</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> CBR-SUBG can answer queries requiring subgraph reasoning patterns and performs competitively with the best models on several KBQA benchmarks and proposes an adaptive subgraph collection strategy to select a query-specific compact subgraph, allowing the model to scale to full Freebase KB containing billions of facts.</p>
                <p><strong>Paper Abstract:</strong> Question answering (QA) over knowledge bases (KBs) is challenging because of the diverse, essentially unbounded, types of reasoning patterns needed. However, we hypothesize in a large KB, reasoning patterns required to answer a query type reoccur for various entities in their respective subgraph neighborhoods. Leveraging this structural similarity between local neighborhoods of different subgraphs, we introduce a semiparametric model (CBR-SUBG) with (i) a nonparametric component that for each query, dynamically retrieves other similar $k$-nearest neighbor (KNN) training queries along with query-specific subgraphs and (ii) a parametric component that is trained to identify the (latent) reasoning patterns from the subgraphs of KNN queries and then apply them to the subgraph of the target query. We also propose an adaptive subgraph collection strategy to select a query-specific compact subgraph, allowing us to scale to full Freebase KB containing billions of facts. We show that CBR-SUBG can answer queries requiring subgraph reasoning patterns and performs competitively with the best models on several KBQA benchmarks. Our subgraph collection strategy also produces more compact subgraphs (e.g. 55\% reduction in size for WebQSP while increasing answer recall by 4.85\%)\footnote{Code, model, and subgraphs are available at \url{https://github.com/rajarshd/CBR-SUBG}}.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4946.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4946.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR-SUBG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning over Subgraphs (CBR-SUBG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semiparametric KBQA model that retrieves k nearest-neighbor training queries (via masked query embeddings) and their query-specific subgraphs, then uses a GNN to learn node representations and locate answer nodes in the target subgraph by similarity to answer nodes in KNN subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CBR-SUBG (semiparametric KBQA; retrieval uses RoBERTa-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Semiparametric architecture: (1) nonparametric retrieval of similar masked queries using RoBERTa-base sentence embeddings; (2) adaptive query-specific subgraph collection using relation-paths from retrieved cases; (3) a parametric GNN (R-GCN) that encodes node structure and is trained contrastively (NT-Xent style) to make answer-node embeddings similar across KNN subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Case-based multi-subgraph reasoning (multi-KNN retrieval + GNN-based subgraph matching)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Diversity arises from retrieving multiple (k) distinct nearest-neighbor queries and their subgraphs; the model aggregates evidence across these multiple retrieved subgraphs and finds nodes in the target subgraph whose GNN representations are most similar to the answer nodes from the retrieved subgraphs. Diversity is achieved by using multiple retrieved cases (k up to ~10) rather than a single exemplar or a single parametric chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-Base Question Answering (KBQA) — synthetic subgraph pattern task; MetaQA; WebQSP; FreebaseQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering natural-language queries by reasoning over a knowledge graph; includes a controlled synthetic task to test recovery of various subgraph-shaped reasoning patterns and real KBQA benchmarks (MetaQA 1/2/3-hop, WebQSP, FreebaseQA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Synthetic structured-pattern task: strict Hits@1 average 85.68% (Table 1). MetaQA (1/2/3-hop): 97.1% / 99.8% / 99.3% (Table 2). WebQSP: 72.1% (Table 2). FreebaseQA (KB-only): accuracy 52.07% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Parametric GNN+TransE baseline: average 72.69% on the synthetic structured-pattern task (Table 1). CBR-path baseline (path-based nonparametric): average 71.09% on the synthetic task (Table 1). On WebQSP, CBR-SUBG (72.1%) outperforms GraftNet (66.4%) and PullNet (68.1%) (Table 2). On FreebaseQA, some LM-pretrained models (EAE 53.4%, FAE 63.3%) outperform CBR-SUBG (52.07%) (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using multiple retrieved similar queries (diverse evidence) and reasoning jointly over their subgraphs yields substantially better recovery of latent subgraph reasoning patterns than (a) parametric-only models that must memorize patterns in parameters, and (b) path-only reuse methods; CBR-SUBG's performance improves as more nearest neighbors are retrieved (up to ~10) indicating that diverse supporting cases boost reasoning, but on small datasets too many KNNs introduce irrelevant contexts and can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On the specific synthetic '2i' pattern (two independent paths intersecting at the answer), the simpler CBR-path baseline achieved perfect 100.0% whereas CBR-SUBG was lower (90.46%), indicating that for some purely path-intersection patterns simple path re-use can outperform joint subgraph reasoning. On WebQSP, adding KNNs beyond a dataset-dependent point decreased accuracy (Table 6), showing more retrieved neighbors can be harmful when they are irrelevant (limited training data).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Base Question Answering by Case-based Reasoning over Subgraphs', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4946.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4946.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN+TransE (parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN + TransE (parametric baseline extended to inductive setting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parametric baseline where a GNN produces node representations and TransE-style training is applied in an inductive setting (dense representations derived from GNN outputs) to learn to answer queries by encoding patterns in model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translating embeddings for modeling multi-relational data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GNN + TransE (parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Parametric KG reasoning approach: uses GNN layers to produce node embeddings and an objective analogous to TransE at training time but applied to GNN-produced representations; entities are not represented sparsely in a retrieval-style case-base but patterns are learned into parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Parametric pattern memorization (single learned reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>This is a single parametric reasoning mechanism where the model must learn and store (similar) patterns in its parameters during training; reasoning at test time uses these learned parameters rather than retrieving diverse external cases.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic structured-pattern recovery task (inductive), KBQA benchmarks (as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same synthetic subgraph pattern detection and KBQA tasks used to evaluate CBR-SUBG; this baseline is adapted to operate inductively by taking GNN outputs as entity representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Synthetic structured-pattern task: strict Hits@1 average 72.69% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CBR-SUBG average 85.68% (Table 1); CBR-path average 71.09% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The parametric baseline performed worse than the semiparametric CBR-SUBG across pattern types (about 13 percentage points lower on average), suggesting that memorizing rare or diverse reasoning patterns in parameters is less effective than retrieving and reusing similar cases at inference; parametric training also required longer training time.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No experimental case in this paper where the parametric GNN+TransE baseline outperformed CBR-SUBG on average; the parametric model did not match CBR-SUBG's generalization to unseen entities or complex subgraph patterns in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Base Question Answering by Case-based Reasoning over Subgraphs', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4946.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4946.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR-path</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning using Path Patterns (CBR-path)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonparametric baseline that gathers relational path patterns connecting source and target entities from retrieved nearest-neighbor queries and applies those paths independently to the current query to predict answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CBR-path (path-based case reuse)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Nonparametric retrieval of path patterns: for each retrieved neighbor, collect paths that connect its query entity to its answer and reuse these path-types individually to score candidates for the target query, without constructing and reasoning over full subgraphs jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Path-based case reuse (single-path matching per evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>CBR-path reuses individual path templates from retrieved cases (less diverse in structure per-case since each path is applied independently); it does not jointly reason over multiple paths/subgraphs and so is less expressive than subgraph-based multi-evidence reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic structured-pattern recovery task</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Controlled synthetic tasks with various shaped patterns (chains, intersections, compositions) to test ability to find the answer node embedded in a latent pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Synthetic structured-pattern task: strict Hits@1 average 71.09% (Table 1). It achieves 100.00% on the '2i' pattern (two independent paths intersecting), outperforming CBR-SUBG on that specific pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CBR-SUBG average 85.68% (Table 1); GNN+TransE average 72.69% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Path-based reuse is effective for tasks that reduce to independent path intersections (e.g., the 2i pattern where CBR-path achieved 100%), but reasoning over full subgraphs (CBR-SUBG) is more powerful overall for composed/intersecting patterns and for tasks requiring joint reasoning over multiple facts.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CBR-path outperforms CBR-SUBG on the specific '2i' synthetic pattern (100% vs 90.46%), showing that simpler, path-focused reuse can be superior when the needed reasoning is exactly path intersection without richer composition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Base Question Answering by Case-based Reasoning over Subgraphs', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4946.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4946.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa (masked retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-base masked-query encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained transformer language model (RoBERTa-base) used to produce masked-question embeddings for retrieving relation-focused nearest neighbors from the training set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized bert pretraining approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base (masked question embedding for retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained transformer (RoBERTa-base) used to encode questions after masking entity spans with a [MASK] token; mean-pooled token representations form a dense vector for nearest-neighbor retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Entity-masked dense retrieval for diverse evidence</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Masking entity mentions yields entity-agnostic sentence embeddings that prioritize relation/structure similarity; retrieving multiple such neighbors supplies diverse relational evidence (multiple KNN cases) for downstream subgraph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KNN retrieval for KBQA (used by CBR-SUBG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve training queries that express similar relations (not the same entities) to provide cases whose subgraphs likely contain analogous reasoning patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No aggregate quantitative retrieval metric reported; qualitative examples (Table 8) show masked queries retrieve relationally similar neighbors (e.g., masked 'what did [MASK] do before he was president' retrieves 'what did abraham lincoln do before he was president' and 'what did barack obama do before he took office'), indicating improved relation-focused retrieval versus unmasked queries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Unmasked query retrieval (using same encoder without entity masking) returns neighbors that often focus on the same entity rather than relation-similar queries (Table 8); no numerical metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying entity masking before encoding queries with RoBERTa yields retrievals that are more relation-focused and therefore more useful as diverse cases for subgraph-based reasoning; this supports the CBR-SUBG premise of reusing reasoning patterns across different entities.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No quantitative ablation; only qualitative examples are shown. The paper notes that as more KNNs are added, the risk of retrieving irrelevant neighbors increases on small datasets (WebQSP), which can degrade downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Base Question Answering by Case-based Reasoning over Subgraphs', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4946.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4946.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EAE / FAE (LM-augmented models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entities-as-Experts (EAE) / Facts-as-Experts (FAE) — LM pretraining + KB</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that combine large pretrained language models with explicit KB supervision or memory mechanisms; reported as comparisons on FreebaseQA and achieve higher accuracy than KB-only models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entities as experts: Sparse memory access with entity supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EAE (Févry et al., 2020) and FAE (Verga et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM-augmented architectures that incorporate entity/fact-level memory or sparse expert access on top of pretrained LMs; leverage LM pretraining to improve KBQA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>LM pretraining with sparse entity/fact memory (single parametric reasoning augmented by large LM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>These systems rely on a large pretrained language model combined with a structured memory over entities or facts; reasoning is predominantly parametric (encoded in the LM and memory mechanism) rather than case-retrieval across multiple external subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FreebaseQA (trivia-style KBQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open KB question answering over Freebase; questions are often challenging trivia queries from competitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported FreebaseQA top-1 accuracy: EAE 53.4%, FAE 63.3% (Table 3 in paper). CBR-SUBG (KB-only) scored 52.07%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CBR-SUBG (KB-only) accuracy 52.07% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LM-augmented approaches (EAE, FAE) can outperform KB-only semiparametric systems like CBR-SUBG on FreebaseQA, indicating that large LMs and pretraining provide complementary strengths; the paper suggests integrating such LMs into the parametric reasoning component as promising future work.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Paper does not analyze whether LM-augmented models use diverse versus similar internal reasoning strategies; only performance comparisons are reported. No claim that LM methods use case-based diverse retrieval as in CBR-SUBG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Base Question Answering by Case-based Reasoning over Subgraphs', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A simple approach to case-based reasoning in knowledge bases. <em>(Rating: 2)</em></li>
                <li>Probabilistic case-based reasoning for openworld knowledge graph completion. <em>(Rating: 2)</em></li>
                <li>Open domain question answering using early fusion of knowledge bases and text. <em>(Rating: 2)</em></li>
                <li>Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. <em>(Rating: 2)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models. <em>(Rating: 2)</em></li>
                <li>A retrieve-and-edit framework for predicting structured outputs. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4946",
    "paper_id": "paper-d7f9bc1acde978072050b6fe2dfdb237f94e480e",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "CBR-SUBG",
            "name_full": "Case-Based Reasoning over Subgraphs (CBR-SUBG)",
            "brief_description": "A semiparametric KBQA model that retrieves k nearest-neighbor training queries (via masked query embeddings) and their query-specific subgraphs, then uses a GNN to learn node representations and locate answer nodes in the target subgraph by similarity to answer nodes in KNN subgraphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CBR-SUBG (semiparametric KBQA; retrieval uses RoBERTa-base)",
            "model_description": "Semiparametric architecture: (1) nonparametric retrieval of similar masked queries using RoBERTa-base sentence embeddings; (2) adaptive query-specific subgraph collection using relation-paths from retrieved cases; (3) a parametric GNN (R-GCN) that encodes node structure and is trained contrastively (NT-Xent style) to make answer-node embeddings similar across KNN subgraphs.",
            "reasoning_method_name": "Case-based multi-subgraph reasoning (multi-KNN retrieval + GNN-based subgraph matching)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Diversity arises from retrieving multiple (k) distinct nearest-neighbor queries and their subgraphs; the model aggregates evidence across these multiple retrieved subgraphs and finds nodes in the target subgraph whose GNN representations are most similar to the answer nodes from the retrieved subgraphs. Diversity is achieved by using multiple retrieved cases (k up to ~10) rather than a single exemplar or a single parametric chain.",
            "task_name": "Knowledge-Base Question Answering (KBQA) — synthetic subgraph pattern task; MetaQA; WebQSP; FreebaseQA",
            "task_description": "Answering natural-language queries by reasoning over a knowledge graph; includes a controlled synthetic task to test recovery of various subgraph-shaped reasoning patterns and real KBQA benchmarks (MetaQA 1/2/3-hop, WebQSP, FreebaseQA).",
            "performance": "Synthetic structured-pattern task: strict Hits@1 average 85.68% (Table 1). MetaQA (1/2/3-hop): 97.1% / 99.8% / 99.3% (Table 2). WebQSP: 72.1% (Table 2). FreebaseQA (KB-only): accuracy 52.07% (Table 3).",
            "comparison_with_other_method": true,
            "performance_other_method": "Parametric GNN+TransE baseline: average 72.69% on the synthetic structured-pattern task (Table 1). CBR-path baseline (path-based nonparametric): average 71.09% on the synthetic task (Table 1). On WebQSP, CBR-SUBG (72.1%) outperforms GraftNet (66.4%) and PullNet (68.1%) (Table 2). On FreebaseQA, some LM-pretrained models (EAE 53.4%, FAE 63.3%) outperform CBR-SUBG (52.07%) (Table 3).",
            "key_findings": "Using multiple retrieved similar queries (diverse evidence) and reasoning jointly over their subgraphs yields substantially better recovery of latent subgraph reasoning patterns than (a) parametric-only models that must memorize patterns in parameters, and (b) path-only reuse methods; CBR-SUBG's performance improves as more nearest neighbors are retrieved (up to ~10) indicating that diverse supporting cases boost reasoning, but on small datasets too many KNNs introduce irrelevant contexts and can hurt performance.",
            "counter_examples_or_negative_results": "On the specific synthetic '2i' pattern (two independent paths intersecting at the answer), the simpler CBR-path baseline achieved perfect 100.0% whereas CBR-SUBG was lower (90.46%), indicating that for some purely path-intersection patterns simple path re-use can outperform joint subgraph reasoning. On WebQSP, adding KNNs beyond a dataset-dependent point decreased accuracy (Table 6), showing more retrieved neighbors can be harmful when they are irrelevant (limited training data).",
            "uuid": "e4946.0",
            "source_info": {
                "paper_title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GNN+TransE (parametric)",
            "name_full": "GNN + TransE (parametric baseline extended to inductive setting)",
            "brief_description": "A parametric baseline where a GNN produces node representations and TransE-style training is applied in an inductive setting (dense representations derived from GNN outputs) to learn to answer queries by encoding patterns in model parameters.",
            "citation_title": "Translating embeddings for modeling multi-relational data.",
            "mention_or_use": "use",
            "model_name": "GNN + TransE (parametric)",
            "model_description": "Parametric KG reasoning approach: uses GNN layers to produce node embeddings and an objective analogous to TransE at training time but applied to GNN-produced representations; entities are not represented sparsely in a retrieval-style case-base but patterns are learned into parameters.",
            "reasoning_method_name": "Parametric pattern memorization (single learned reasoning model)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "This is a single parametric reasoning mechanism where the model must learn and store (similar) patterns in its parameters during training; reasoning at test time uses these learned parameters rather than retrieving diverse external cases.",
            "task_name": "Synthetic structured-pattern recovery task (inductive), KBQA benchmarks (as baseline)",
            "task_description": "Same synthetic subgraph pattern detection and KBQA tasks used to evaluate CBR-SUBG; this baseline is adapted to operate inductively by taking GNN outputs as entity representations.",
            "performance": "Synthetic structured-pattern task: strict Hits@1 average 72.69% (Table 1).",
            "comparison_with_other_method": true,
            "performance_other_method": "CBR-SUBG average 85.68% (Table 1); CBR-path average 71.09% (Table 1).",
            "key_findings": "The parametric baseline performed worse than the semiparametric CBR-SUBG across pattern types (about 13 percentage points lower on average), suggesting that memorizing rare or diverse reasoning patterns in parameters is less effective than retrieving and reusing similar cases at inference; parametric training also required longer training time.",
            "counter_examples_or_negative_results": "No experimental case in this paper where the parametric GNN+TransE baseline outperformed CBR-SUBG on average; the parametric model did not match CBR-SUBG's generalization to unseen entities or complex subgraph patterns in these experiments.",
            "uuid": "e4946.1",
            "source_info": {
                "paper_title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "CBR-path",
            "name_full": "Case-Based Reasoning using Path Patterns (CBR-path)",
            "brief_description": "A nonparametric baseline that gathers relational path patterns connecting source and target entities from retrieved nearest-neighbor queries and applies those paths independently to the current query to predict answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CBR-path (path-based case reuse)",
            "model_description": "Nonparametric retrieval of path patterns: for each retrieved neighbor, collect paths that connect its query entity to its answer and reuse these path-types individually to score candidates for the target query, without constructing and reasoning over full subgraphs jointly.",
            "reasoning_method_name": "Path-based case reuse (single-path matching per evidence)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "CBR-path reuses individual path templates from retrieved cases (less diverse in structure per-case since each path is applied independently); it does not jointly reason over multiple paths/subgraphs and so is less expressive than subgraph-based multi-evidence reasoning.",
            "task_name": "Synthetic structured-pattern recovery task",
            "task_description": "Controlled synthetic tasks with various shaped patterns (chains, intersections, compositions) to test ability to find the answer node embedded in a latent pattern.",
            "performance": "Synthetic structured-pattern task: strict Hits@1 average 71.09% (Table 1). It achieves 100.00% on the '2i' pattern (two independent paths intersecting), outperforming CBR-SUBG on that specific pattern.",
            "comparison_with_other_method": true,
            "performance_other_method": "CBR-SUBG average 85.68% (Table 1); GNN+TransE average 72.69% (Table 1).",
            "key_findings": "Path-based reuse is effective for tasks that reduce to independent path intersections (e.g., the 2i pattern where CBR-path achieved 100%), but reasoning over full subgraphs (CBR-SUBG) is more powerful overall for composed/intersecting patterns and for tasks requiring joint reasoning over multiple facts.",
            "counter_examples_or_negative_results": "CBR-path outperforms CBR-SUBG on the specific '2i' synthetic pattern (100% vs 90.46%), showing that simpler, path-focused reuse can be superior when the needed reasoning is exactly path intersection without richer composition.",
            "uuid": "e4946.2",
            "source_info": {
                "paper_title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "RoBERTa (masked retrieval)",
            "name_full": "RoBERTa-base masked-query encoder",
            "brief_description": "A pre-trained transformer language model (RoBERTa-base) used to produce masked-question embeddings for retrieving relation-focused nearest neighbors from the training set.",
            "citation_title": "RoBERTa: A robustly optimized bert pretraining approach.",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base (masked question embedding for retrieval)",
            "model_description": "Pre-trained transformer (RoBERTa-base) used to encode questions after masking entity spans with a [MASK] token; mean-pooled token representations form a dense vector for nearest-neighbor retrieval.",
            "reasoning_method_name": "Entity-masked dense retrieval for diverse evidence",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Masking entity mentions yields entity-agnostic sentence embeddings that prioritize relation/structure similarity; retrieving multiple such neighbors supplies diverse relational evidence (multiple KNN cases) for downstream subgraph reasoning.",
            "task_name": "KNN retrieval for KBQA (used by CBR-SUBG)",
            "task_description": "Retrieve training queries that express similar relations (not the same entities) to provide cases whose subgraphs likely contain analogous reasoning patterns.",
            "performance": "No aggregate quantitative retrieval metric reported; qualitative examples (Table 8) show masked queries retrieve relationally similar neighbors (e.g., masked 'what did [MASK] do before he was president' retrieves 'what did abraham lincoln do before he was president' and 'what did barack obama do before he took office'), indicating improved relation-focused retrieval versus unmasked queries.",
            "comparison_with_other_method": true,
            "performance_other_method": "Unmasked query retrieval (using same encoder without entity masking) returns neighbors that often focus on the same entity rather than relation-similar queries (Table 8); no numerical metrics provided.",
            "key_findings": "Applying entity masking before encoding queries with RoBERTa yields retrievals that are more relation-focused and therefore more useful as diverse cases for subgraph-based reasoning; this supports the CBR-SUBG premise of reusing reasoning patterns across different entities.",
            "counter_examples_or_negative_results": "No quantitative ablation; only qualitative examples are shown. The paper notes that as more KNNs are added, the risk of retrieving irrelevant neighbors increases on small datasets (WebQSP), which can degrade downstream performance.",
            "uuid": "e4946.3",
            "source_info": {
                "paper_title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "EAE / FAE (LM-augmented models)",
            "name_full": "Entities-as-Experts (EAE) / Facts-as-Experts (FAE) — LM pretraining + KB",
            "brief_description": "Models that combine large pretrained language models with explicit KB supervision or memory mechanisms; reported as comparisons on FreebaseQA and achieve higher accuracy than KB-only models in this paper.",
            "citation_title": "Entities as experts: Sparse memory access with entity supervision.",
            "mention_or_use": "mention",
            "model_name": "EAE (Févry et al., 2020) and FAE (Verga et al., 2020)",
            "model_description": "LM-augmented architectures that incorporate entity/fact-level memory or sparse expert access on top of pretrained LMs; leverage LM pretraining to improve KBQA performance.",
            "reasoning_method_name": "LM pretraining with sparse entity/fact memory (single parametric reasoning augmented by large LM)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "These systems rely on a large pretrained language model combined with a structured memory over entities or facts; reasoning is predominantly parametric (encoded in the LM and memory mechanism) rather than case-retrieval across multiple external subgraphs.",
            "task_name": "FreebaseQA (trivia-style KBQA)",
            "task_description": "Open KB question answering over Freebase; questions are often challenging trivia queries from competitions.",
            "performance": "Reported FreebaseQA top-1 accuracy: EAE 53.4%, FAE 63.3% (Table 3 in paper). CBR-SUBG (KB-only) scored 52.07%.",
            "comparison_with_other_method": true,
            "performance_other_method": "CBR-SUBG (KB-only) accuracy 52.07% (Table 3).",
            "key_findings": "LM-augmented approaches (EAE, FAE) can outperform KB-only semiparametric systems like CBR-SUBG on FreebaseQA, indicating that large LMs and pretraining provide complementary strengths; the paper suggests integrating such LMs into the parametric reasoning component as promising future work.",
            "counter_examples_or_negative_results": "Paper does not analyze whether LM-augmented models use diverse versus similar internal reasoning strategies; only performance comparisons are reported. No claim that LM methods use case-based diverse retrieval as in CBR-SUBG.",
            "uuid": "e4946.4",
            "source_info": {
                "paper_title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A simple approach to case-based reasoning in knowledge bases.",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic case-based reasoning for openworld knowledge graph completion.",
            "rating": 2
        },
        {
            "paper_title": "Open domain question answering using early fusion of knowledge bases and text.",
            "rating": 2
        },
        {
            "paper_title": "Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text.",
            "rating": 2
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models.",
            "rating": 2
        },
        {
            "paper_title": "A retrieve-and-edit framework for predicting structured outputs.",
            "rating": 1
        }
    ],
    "cost": 0.01630775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Knowledge Base Question Answering by Case-based Reasoning over Subgraphs</h1>
<p>Rajarshi Das<em> ${ }^{</em> 1}$ Ameya Godbole ${ }^{* 2}$ Ankita Naik ${ }^{1}$ Elliot Tower ${ }^{1}$ Robin Jia ${ }^{2}$<br>Manzil Zaheer ${ }^{3}$ Hannaneh Hajishirzi ${ }^{4}$ Andrew McCallum ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Question answering (QA) over knowledge bases (KBs) is challenging because of the diverse, essentially unbounded, types of reasoning patterns needed. However, we hypothesize in a large KB, reasoning patterns required to answer a query type reoccur for various entities in their respective subgraph neighborhoods. Leveraging this structural similarity between local neighborhoods of different subgraphs, we introduce a semiparametric model (CBR-SUBG) with (i) a nonparametric component that for each query, dynamically retrieves other similar $k$-nearest neighbor (KNN) training queries along with query-specific subgraphs and (ii) a parametric component that is trained to identify the (latent) reasoning patterns from the subgraphs of KNN queries and then apply them to the subgraph of the target query. We also propose an adaptive subgraph collection strategy to select a query-specific compact subgraph, allowing us to scale to full Freebase KB containing billions of facts. We show that CBR-SUBG can answer queries requiring subgraph reasoning patterns and performs competitively with the best models on several KBQA benchmarks. Our subgraph collection strategy also produces more compact subgraphs (e.g. 55\% reduction in size for WebQSP while increasing answer recall by $4.85 \%)^{1}$.</p>
<h2>1. Introduction</h2>
<p>Knowledge bases (KBs) store massive amounts of rich symbolic facts about real-world entities in the form of relation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Figure shows an input query and two queries in the training set that are similar to the input query. The relevant subgraph for each query (query subgraph) is also shown. Note that the "reasoning patterns" required to answer the queries (edges in red) repeats in the subgraphs of each query. Also note, the corresponding answer nodes (marked as $\overline{\boldsymbol{x}}$ ) are analogously located within the reasoning patterns in each subgraph. Thus the answer node can be found by identifying the node in the query subgraph that is most similar to the answer nodes in the subgraph of KNN queries.
triples- $\left(\mathrm{e}<em 2="2">{1}, \mathrm{r}, \mathrm{e}</em>$ denote entities and $r$ denotes a semantic relation. KBs can be naturally described as a graph where the entities are nodes and the relations are labelled edges. An effective and user-friendly way of accessing the information stored in a KB is by issuing queries to it. Such queries can be structured (e.g. queries for booking flights) or unstructured (e.g. natural language queries). The set of KB facts useful for answering a query induce a reasoning pattern-e.g. a chain of KB facts forming a path or more generally a subgraph in the knowledge graph (KG) (set of red edges in Figure 1). It is very laborious to annotate the reasoning patterns for each query at scale and hence it is important to develop weakly-supervised knowledge base question answering (KBQA) models that do not depend on the availability of the annotated reasoning patterns.}\right)$, where $e_{1}, e_{2</p>
<p>We are interested in developing models that can answer queries requiring complex subgraph reasoning patterns.</p>
<p>Many previous works in KBQA (Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2018) reason over individual relational paths. However, many queries require a model to reason over a set of facts jointly. For example, the query in Figure 1 cannot be answered by considering individual paths. Furthermore, a model has to learn a large number of reasoning patterns because of the diverse nature of possible questions. Moreover, a model may encounter very few examples of a particular pattern during training, making it challenging for the models to learn and encode the patterns entirely in its parameters. A possible solution to this challenge might lie in a classical AI paradigm proposed decades back-Case-based Reasoning (CBR) (Schank, 1982). In a CBR framework, a new problem is solved by retrieving other similar problems and reusing their solution to derive a solution for the given problem. In other words, models, instead of memorizing patterns in its parameters, can instead reuse the reasoning patterns of other similar queries, retrieved dynamically during inference. Recently, CBR was successfully used for KB completion by Das et al. (2020a;b).</p>
<p>This paper introduces a semiparametric CBR-based model (CBR-SUBG) for QA over KBs with a nonparametric component, that for each query, dynamically retrieves other similar $k$-nearest neighbor (KNN) queries from the training set. To retrieve similar queries, we use masked sentence representation of the query (Soares et al., 2019) obtained from pre-trained language models.</p>
<p>We hypothesize that the reasoning patterns required for answering similar queries reoccur within the subgraph neighborhood of entities present in those queries (Figure 1). The answer nodes for each query are also analogously nestled within the reasoning patterns (marked as in Figure 1) of the query subgraphs, i.e. they have similar neighborhoods. However, we do not have annotated reasoning patterns that could be used to search for the answer node. Moreover, a subgraph can have tens of thousands of entity nodes. How do we still identify the answer nodes in the query subgraph?</p>
<p>To identify the answer nodes, our model has a parametric component comprising of a graph neural network (GNN) that is trained to identify the (latent) reasoning patterns from the subgraphs of KNN queries and apply it to the subgraph of the target query. GNNs have been shown to be effective in encoding structural properties of local neighborhoods in the node representations (Duvenaud et al., 2015; Kipf \&amp; Welling, 2017). We leverage node representations obtained from GNNs for finding answer nodes. Specifically, the answer nodes are identified by performing a nearest neighbor search for the most similar nodes in the query subgraph w.r.t the representation of answer nodes in the KNN subgraph. The parametric model is trained via contrastive learning (§3.3) (Chopra et al., 2005; Gutmann \&amp; Hyvärinen, 2010).</p>
<p>A practical challenge for KBQA models is to select a com-
pact subgraph for a query. The goal is to ensure that the subgraph has high recall and is small enough to fit into GPU memory for gradient-based learning. Many KBQA methods usually consider few hops of edges around entities as the query subgraph (Neelakantan et al., 2015; Saxena et al., 2020) leading to query-independent and (often) large subgraphs, because of the presence of hub nodes in large KBs. We propose an adaptive subgraph collection method tailored for each query where we use our nonparametric approach of retrieving KNN queries to help gather the query subgraph leading to compact subgraphs with higher recall of reasoning patterns (§ 3.2).</p>
<p>An important property of nonparametric models is its ablility to grow and reason with new data. Being true to its nonparametric design, CBR-SUBG uses sparse representations of entities that makes it easy to represent new entities. Moreover, we also demonstrate that the performance of CBR-SUBG improves as more evidence is retrieved, suggesting that CBR-SUBG can reason with new evidence.</p>
<p>Contributions. To summarize, this paper introduces CBRsUBG, a semiparametric model for weakly-supervised KBQA that retrieves similar queries and utilizes the similarities in graph structure of local subgraphs to answer a query (§3.3). We also propose a practical algorithm for gathering query-specific subgraphs that utilizes the retrieved KNN queries to produce compact query-specific subgraphs (§3.2). We show that CBR-SUBG can model (latent) subgraph reasoning patterns (§4.1), more effectively than parametric models; can reason with new entities (§4.1) and new evidence (§4.3). Lastly, we perform competitively with state-of-the-art KBQA models on multiple benchmarks. For example, on the FreebaseQA dataset (Jiang et al., 2019), we outperform most competitive baseline by 14.45 points.</p>
<h2>2. Related Work</h2>
<p>CBR-based models for KB reasoning. Recently, Das et al. (2020a;b) proposed a CBR-based technique for KB completion. However, their work has several limitations. Firstly, it can only model simple linear chains. Secondly, it uses exact symbolic matching to find similarities in patterns between cases and the query. Lastly, it cannot handle natural language queries and only works with structured slot-filling queries. In contrast, CBR-SUBG can model arbitrary reasoning patterns; uses soft-matching by comparing representations of answer nodes and can handle natural language queries. Lastly, our method outperforms Das et al. (2020a) on various benchmarks. A follow up work of Das et al. (2021) proposed a CBR model that can handle natural language queries, however that work requires the availability of annotated reasoning patterns for training, an important distinction from our work that does not need any annotation of reasoning patterns.</p>
<p>Semiparametric models for KBQA. GraftNet (Sun et al., 2018) and PullNet (Sun et al., 2019a) are two semiparametric models for KBQA where they like us, provide both a mechanism of collecting a query-subgraph and reasoning over them. For their nonparametric component, these work employ a retrieval process where a parametric model classifies which edges would be relevant to the query. Being parametric, these models cannot generalize to new type of questions without re-training the model parameters. However, our nonparametric approach will work as it retrieves similar queries on-the-fly. For their reasoning model, both works use a graph convolution model and treat the answer prediction as a node classification task. However, unlike us they do not reason with subgraphs of similar KNN queries. Lastly, we compare extensively with them and outperform them on multiple benchmarks. Our approach also has similarities with retriever-reader architecture for open-domain QA over text (Chen et al., 2017) where a retriever selects evidence specific to the query and the reader reasons with them to produce the answer.</p>
<p>Graph representations using contrastive learning. Recently, there has been a lot of work on learning graph representations via contrastive learning (Hassani \&amp; Khasahmadi, 2020; You et al., 2020; Sun et al., 2020; Qiu et al., 2020a; Zhu et al., 2020) where they create two views of the same graph by randomly dropping edges and nodes. Next, the two views of subgraphs are treated as positive pair and their representations are contrasted wrt other negative samples. Our work differs from them because we do not create different views of the same graph, rather we follow the CBR hypothesis and make node representations of answer nodes of two query-specific subgraphs similar provided the queries have relational similarity.</p>
<p>Semantic parsing. Classic works in semantic parsing (Zelle \&amp; Mooney, 1996; Zettlemoyer \&amp; Collins, 2005; 2007) were among the early works to use statistical learning to convert queries to executable logical forms. However, these work needed annotated logical forms during training. Follow up work (Berant et al., 2013; Kwiatkowski et al., 2013) learned semantic parser directly from question answer pairs. However, their model relied on simple hand crafted features. Recent approaches to semantic parsing (Saxena et al., 2020; He et al., 2021) uses powerful neural models and achieve strong performance. However unlike us, these parametric models learn dense representation of entities and hence will not generalize to unseen entities like our approach.</p>
<p>Inductive KB reasoning. Our model is also related to Teru et al. (2020) as it explores KB reasoning in an inductive setting. They also have a sparse representation of entities. However, the task they consider is predicting a KB relation between two nodes which is an easier task than performing KBQA using natural language queries.</p>
<p>Graph neural networks. A model like CBR-SUBG is possible for KBQA because of tremendous progress made in graph representation learning by message passing neural networks (Kipf \&amp; Welling, 2017; Velickovic et al., 2018; Schlichtkrull et al., 2018, inter-alia). CBR-SUBG is not dependent on any specific message passing scheme and can work with any GNN architecture that can operate over heterogenous knowledge graphs. For our experiments, we use the widely used relational-GCN (Schlichtkrull et al., 2018). Further related work is included in the appendix (§F).</p>
<h2>3. Model</h2>
<p>This section describes the nonparametric and parametric components of CBR-SUBG. In CBR, a case is defined as an abstract representation of a problem along with its solution. In our KBQA setting, a case is a natural language query along with its answer. Note in KBQA, answers are entities in the KB (or nodes in KG).</p>
<p>Task Description. Let $q$ be a natural language query and let $\mathcal{K}$ be a symbolic KG that needs to be queried to retrieve an answer list $\mathcal{A}$ containing the answer(s) for $q$. We assume access to a training set $\mathcal{D}=$ $\left{\left(q_{1}, \mathcal{A}<em 2="2">{1}\right),\left(q</em>}, \mathcal{A<em N="N">{2}\right), \ldots\left(q</em>}, \mathcal{A<em i="i">{N}\right)\right}$ of query-answer pairs where $\mathcal{A}</em>$.
Method overview. For input $q$ and $\mathcal{K}$, CBR-SUBG first retrieves $k$ similar query-answer(s) w.r.t $q$ from $\mathcal{D}$ (§ 3.1). Denote this retrieved set as $\mathrm{kNN}}$ denote the list of answer nodes for $q_{i}$. The training set $\mathcal{D}$ forms the 'case-base'. The reasoning pattern (a.k.a. logical form) required to answer a query are the set of KG edges required to deduce the answer to $q$. Let $P_{q}$ denote this set of edges. For example, in Figure 1, for $q=$ "Who plays 'MJ' in No Way Home?", $P_{q} \equiv{($ MJ, played_by, Zendaya), (No Way Home, has_character, MJ), (No Way Home, cast, Zendaya) $}$. We define reasoning pattern 'type' for a pattern as the set of edges where the entities have been replaced by free variables. For example, $T\left(P_{q}\right)={($ M, played_by, Z), (S, has_character, M), (S, cast, Z) $}$. It should be noted that CBR-SUBG does not assume access to annotated $P_{q<em q__i="q_{i">{q} \subset \mathcal{D}$. Next, CBR-SUBG finds query-specific subgraphs $\mathcal{K}</em>}}$ for each query in ${q} \cup$ $\mathrm{kNN<em q="q">{q}$ (§ 3.2). According to the CBR hypothesis, the reasoning required to solve a new problem will be similar to the reasoning required to solve other similar problems. Similarly for our KBQA setting, we hypothesize that the reasoning pattern type, $T\left(P</em>}\right)$ repeats across the neighborhood of query subgraphs of $\mathrm{kNN<em q="q">{q}$. Next, CBR-SUBG uses graph neural networks (GNNs) to encode local structure into node representations (§ 3.3). Now, if the CBR hypothesis holds true, then the representation of the answer nodes in each query subgraphs will be similar as the local structure around them share similarities. Hence, the answer node of the given query $q$ can be identified by searching for the node that has the most similar representations to the answer nodes in the query subgraphs of $\mathrm{kNN}</em>$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Figure shows the query-subgraph selection procedure with 1-nearest neighbor retrieved query. Graph paths connecting the entities in the retrieved query and its answer are collected. Next the sequence of relations (path types) are gathered and are then followed starting from the entity in the given query. All the edges spanned by this process are collected to form the query-specific subgraph. This process is repeated for each of the <em>k</em> retrieved queries.</p>
<h3>3.1. Retrieval of Similar Cases</h3>
<p>Given the input query <em>q</em>, CBR-SUBG first retrieves other similar cases from the training set. We represent the query by embeddings obtained from large pre-trained language models (Liu et al., 2019). Inspired by the recent advances in neural dense passage retrieval (Karpukhin et al., 2020), we use a ROBERTA-base encoder to encode each question independently. A single representation is obtained by mean pooling over the token-level representations.</p>
<p>Generally, we want to retrieve questions that express similar relations rather than retrieving questions that are about similar entities. For example, for the query, ‘Who played Natalie Portman in Star Wars?’, we would like to retrieve queries such as ‘Who played Ken Barlow in Coronation St.?’ instead of ‘What sports does Natalie Portman like to play?’. To obtain entity-agnostic representation, we replace the entity spans with a special ‘[MASK]’ token in the query, i.e. the original query becomes ‘Who played [MASK] in [MASK]?’. The entity masking strategy has previously been successfully used in learning entity-independent relational representations (Soares et al., 2019). The similarity score between two queries is given by the inner product between their normalized vector representations (cosine similarity). We pre-compute the representations of queries in the train set. During inference, the most similar query representations are obtained by doing a nearest neighbor search over the representations stored in the case-base.</p>
<h3>3.2. Query-subgraph Selection</h3>
<p>A practical challenge for KBQA models is to select a subgraph around the entities present in the query. The goal is to ensure that the necessary reasoning patterns and answers are included while producing a graph small enough to fit into GPU memory for gradient-based learning. A naïve strategy to select a subgraph is to consider all edges in 2-3 hops around the query entities. This strategy leads to subgraphs which are independent of the query. Moreover, in large KGs like Freebase (Bollacker et al., 2008), considering the full 2 or 3-hop subgraph often leads to accumulation of millions of edges because of the presence of hub nodes.</p>
<p>We propose a nonparametric approach of query subgraph collection that utilizes the retrieved cases, kNN<sub>q</sub>, from the last step (Figure 2). For each of the retrieved case, chains of edges (or paths) in the graph that connect the entity in the retrieved query to its answers are collected by doing a depth-first search. (Note, since the retrieved queries are from the training set, we know the answer to them). Next, the sequence of relations are collected from each chain and they are followed starting from the entities of the input query. If a chain of relations do not exist from the query, then they are simply ignored. This process is repeated for each of the retrieved cases. Note that, not all chains collected from the nearest neighbors are meaningful for the query. For example, the last (3-hop) chain collected in Figure 2 is not relevant for answering the query and even though it ended at the answer for the retrieved query, the same is not the case for the input query. Such paths are often referred to as spurious paths or evidence (He et al., 2021). All the edges gathered by following this process form the subgraph for the input query. The underlying idea behind the subgraph selection procedure is simple — since the paths connect queries and answers of similar queries, they should also be relevant for answering the given query.</p>
<p>There is a class of prior work such as Graft-Net (Sun et al., 2018) and PullNet (Sun et al., 2019a) that learn a parametric model to choose a query-specific subgraph. These models employ a retrieval process where a parametric model classifies which edges would be relevant to the query. Being</p>
<p>parametric, these models cannot generalize to new type of questions without re-training the model parameters. However, our nonparametric approach will work as long as it has access to similar queries, which it can retrieve on-the-fly. Our subgraph selection procedure is similar to the approach proposed in (Das et al., 2020a). However, Das et al. (2020a) do not use this approach to collect a query-specific subgraph. Rather it uses each of the paths to independently predict the answer to a query. In contrast, we collect all edges in the path to form a subgraph and then reason jointly over the subgraph of the query as well as the subgraph of retrieved cases as detailed in the next sub-section.</p>
<h3>3.3. Reasoning over Multiple Subgraphs</h3>
<p>This section describes how CBR-SUBG reasons across the subgraphs of the given query and the subgraphs of the retrieved cases. We use graph neural networks (GNNs) (Scarselli et al., 2008) to encode the local structure into the node representations of each subgraph. During training, the answer node representations of different subgraphs are made more similar to each other in comparison to other non-answer nodes. Inference reduces to searching for the most similar node in the query subgraph w.r.t the answer nodes in the KNN-subgraphs.</p>
<p>Modern GNNs employ a neighborhood aggregation strategy (message passing) where a node representation is iteratively updated by aggregating representations from its neighbors (Gilmer et al., 2017). Let $\mathcal{G}<em q="q">{q}=\left(V</em>}, E_{q}\right)$ represent the subgraph for a query $q$ obtained from (§3.2). Let $\boldsymbol{X<em q="q">{v}$ denote the node feature vectors for each $v \in V</em>$.</p>
<p>Input node representations. A property of nonparametric models is its ability to represent, reason and grow with new data. Knowledge graphs store facts about the world and as the world evolves, new entities and facts are added to the KG. Models developed for KG reasoning (Bordes et al., 2013; Schlichtkrull et al., 2018; Sun et al., 2019b, inter-alia) learn dense representations of a fixed vocabulary of entities and are hence unable to handle new entities added to the KG. Following our nonparametric design principles, each entity node is represented as a sparse vector of its outogoiing edge (relation) types, i.e. $\boldsymbol{x}<em v="v">{v} \in{0,1}^{|\mathcal{R}|}$ where $\mathcal{R}$ denotes the set of relation types in the KG. If entity $x</em>$ has m distinct outgoing edge types, then the dimension corresponding to those types are set to 1 . This is an extremely simple and flexible way of representing entities which also expresses the local structural information around each node. Also note that, as new entities are added or new facts are updated about an entity, the sparse representation makes it very easy to represent new entities or update existing embeddings.</p>
<p>Relative distance embedding. Each query-specific subgraph $\mathcal{G}<em q="q">{q}$ has a few special entities - the entities present in the input query. This is because the reasoning pattern is
usually in the immediate subgraph surrounding the query entity. We treat the query entities as 'center' entities and append a relative distance embedding to every other node in $\mathcal{G}</em>}$ (Zhang \&amp; Chen, 2018; Teru et al., 2020). Specifically, for each node, the representation $\boldsymbol{x<em _boldsymbol_d="\boldsymbol{d">{v}$ is appended with an one-hot distance embedding $\boldsymbol{x}</em>$ where the component corresponding to the shortest distance from the query entity is set to 1 . In practice, we consider subgraphs upto 3-hops from the query entities, i.e. $d=4$. For queries with multiple query entities, the minimum distance is considered.}} \in{0,1}^{|\mathcal{d}|</p>
<p>Message passing. Our GNN uses the graph structure and the sparse input node features $\boldsymbol{X}_{v}$ to learn intermediate node features capturing the local structure within them. We follow the general message-passing scheme where a node representation is iteratively updated by combining it with aggregation of it's neighbors' representation (Xu et al., 2019). In particular, the $l^{\text {th }}$ layer of a GNN is,</p>
<p>$$
\begin{aligned}
\boldsymbol{a}<em v="v">{v}^{l} &amp; =\operatorname{AGGREGATE}^{l}\left(\left{\boldsymbol{h}</em>}^{l-1}: s \in \mathcal{N}(v)\right}, \boldsymbol{h<em v="v">{v}^{l-1}\right) \
\boldsymbol{h}</em>}^{l} &amp; =\operatorname{COMBINE}^{l}\left(\boldsymbol{h<em v="v">{v}^{l-1}, \boldsymbol{a}</em>\right)
\end{aligned}
$$}^{l</p>
<p>where, $\boldsymbol{a}<em v="v">{v}^{l}$ denote the aggregated message from the neighbors of node $v, \boldsymbol{h}</em>}^{l}$ denote the node representation of node $v$ in the $l$-th layer and $\mathcal{N}(v)$ denotes the neighboring nodes of $v$. Since KGs are heterogenous graphs with labelled edges, we adopt the widely used multi-relational R-GCN model model (Schlichtkrull et al., 2018) which defines the aggregate step as: $\boldsymbol{a<em r="1">{v}^{l}=\sum</em>}^{\mathcal{R}} \sum_{s \in \mathcal{N<em r="r">{r}(v)} \frac{1}{\left|\mathcal{N}</em>\right)$. For each answer node, we consider the representation obtained from the last layer.}(v)\right|} W_{r}^{l} h_{s}^{l-1}$ and the combine step as $h_{v}^{l}=\operatorname{ReLU}\left(W_{\text {self }}^{l} h_{v}^{l-1}+\boldsymbol{a}_{v}^{l</p>
<p>Training. Let $a_{i}, a_{j}$ be an answer node in the corresponding query-subgraphs of $q_{i}$ and $q_{j}$ (i.e. $\mathcal{G}<em i="i">{q</em>}}, \mathcal{G<em j="j">{q</em>}}$ ) respectively. Let $\operatorname{sim}\left(\boldsymbol{a<em j="j">{i}, \boldsymbol{a}</em>}\right)=\boldsymbol{a<em j="j">{i}^{\top} \boldsymbol{a}</em>} /\left|\boldsymbol{a<em j="j">{i}\right|\left|\boldsymbol{a}</em>}\right|$ denote the inner product between $\ell_{2}$ normalized answer representations (i.e. cosine similarity). In general there can be multiple answer nodes for a query. Let $\mathcal{A<em j="j">{j}$ denote the set of all answer nodes for query $q</em>}$ in its subgraph $\mathcal{G<em j="j">{q</em>}}$. Let $\operatorname{sim}\left(\boldsymbol{a<em j="j">{i}, \mathcal{A}</em>}\right)=\frac{1}{\left|\mathcal{A<em a__j="a_{j">{j}\right|} \sum</em>} \in \mathcal{A<em i="i">{j}} \operatorname{sim}\left(\boldsymbol{a}</em>}, \boldsymbol{a<em i="i">{j}\right)$, i.e. $\operatorname{sim}\left(\boldsymbol{a}</em>}, \mathcal{A<em q__i="q_{i">{j}\right)$ represents the mean of the scores between $a</em>}}$ and all answer nodes in $\mathcal{G<em j="j">{q</em>}}$. We aggregate the similarity score from all retrieved queries $\mathrm{kNN<em i="i">{q</em>$.
The loss function we use is,}}$ for the current query $q_{i</p>
<p>$$
-\log \frac{\sum_{a_{i} \in \mathcal{A}<em q__j="q_{j">{i}} \exp \left(\sum</em>} \in \mathrm{KNN<em i="i">{q</em>}}} \operatorname{sim}\left(\boldsymbol{a<em j="j">{i}, \mathcal{A}</em>}\right) / \tau\right)}{\sum_{x_{i} \in \mathcal{V}\left(\mathcal{G<em i="i">{q</em>}}\right)} \exp \left(\sum_{q_{j} \in \mathrm{KNN<em i="i">{q</em>}}} \operatorname{sim}\left(\boldsymbol{x<em j="j">{i}, \mathcal{A}</em>
$$}\right) / \tau\right)</p>
<p>where, $\mathcal{A}<em q__j="q_{j">{j}$ denotes the set of all answer nodes in $\mathcal{G}</em>}}$ for a $q_{j} \in \mathrm{kNN<em i="i">{q</em>}}, x_{i}$ goes over all nodes in query-subgraph $\mathcal{G<em i="i">{q</em>}}$ and $\tau$ denotes a temperature parameter. In other words, the loss encourages the answer nodes in $\mathcal{G<em i="i">{q</em>}}$ to be scored higher than all other nodes in $\mathcal{G<em i="i">{q</em>$ w.r.t the answer nodes in the retrieved query subgraphs. This loss is an extension of}</p>
<p>the the normalized temperature-scaled cross entropy loss (NT-Xent) used in Chen et al. (2020).
Inference. During inference, message passing is run over each of the query-subgraph $\mathcal{G}<em i="i">{q</em>}}$ and the subgraphs $\mathcal{G<em j="j">{q</em>}}$ of its $k$ retrieved queries $q_{j} \in \mathrm{kNN<em i="i">{q</em>}}$ to obtain the node representations and the highest scoring node in $\mathcal{G<em i="i">{q</em>$ w.r.t all the answer nodes in the retrieved query sub-graphs is returned as the answer.}</p>
<p>$$
\boldsymbol{a}<em i="i">{i}=\underset{x</em>}}{\arg \max } \sum_{x_{i} \in \mathcal{V}\left(\mathcal{G<em j="j">{q</em>}}\right)} \exp \left(\sum_{q_{j} \in \mathrm{kNN<em j="j">{q</em>}}} \operatorname{sim}\left(\boldsymbol{x<em j="j">{i}, \mathcal{A}</em>\right)\right)
$$</p>
<h2>4. Experiments</h2>
<p>In this section, we demonstrate the effectiveness of the semiparametric approach of CBR-SUBG and show that the nonparametric and parametric component offer complementary strengths. For example, we show that the model performance improves as more evidence is dynamically retrieved by the nonparametric component (§4.3). Similarly, CBRSUBG can handle queries requiring reasoning patterns more complex than simple chains (i.e. subgraphs) because of the inductive bias provided by GNNs (§4.1). It can handle new and unseen entities because of the sparse entity input features as a part of its design (§4.1). We also show that the nonparametric subgraph selection of CBR-SUBG allows us to operate over a massive real-world KG (full Freebase KG) and obtain very competitive performance on several benchmark datasets including WebQuestionsSP (Yih et al., 2016), FreebaseQA (Jiang et al., 2019) and MetaQA (Zhang et al., 2018).</p>
<h3>4.1. Reasoning over Complex Patterns</h3>
<p>We want to test whether CBR-SUBG can answer queries requiring complex reasoning patterns. Note that, the reasoning patterns are always latent to the model, i.e. the model has to answer a given query from the query-subgraph and the retrieved KNN-subgraphs without any knowledge of the structure of the pattern.</p>
<p>To test the model capacity to identify reasoning patterns, we devise a controlled setting in which the model has to infer reasoning patterns of various shapes (Figure 3), inspired by Ren et al. (2020). Note that in their work, the task was to execute the input structured query on an incomplete KB, i.e, the shape of the input patterns are known to the model. In contrast, in our setting, the model has to find the answer node (marked ( )), which is nestled in each of the structured pattern without the knowledge of the pattern structure. Also note, there can be multiple nodes of same type as the answer type, so the task cannot be completed by solving easier task of determining entity types. Instead the model has to identify specific ( ) nodes which are at the end of reasoning patterns (there can be multiple ( ) nodes in the graph).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Various reasoning pattern types considered.
Data Generation Process. We first define a type system with a set of entity types $\mathcal{E}$ and relation types $\mathcal{R}$. The type-system also specifies a set of 'allowed relation types' between different pairs of entity types. For example, an 'employee' KB relation is defined between an 'organization' and 'people' entity types. Entities (or nodes $\mathcal{V}$ ) are then generated uniformly from the set of entity types $\mathcal{E}$. Next, relation edges (uniformly sampled from the allowed types) are joined between a pair of nodes with a probability $p$ following the Erdős-Rényi model (Erdos et al., 1960) of random graph generation. To ensure that models only rely on the graph structure, each graph has a 'unique' set of entities and no two graphs share entities. This also effectively tests how much the nonparametric property of CBR-SUBG can reason with unseen entities. More details regarding the hyperparameters $\mathcal{E}, \mathcal{R}, \mathcal{V}$ are included in the appendix B.</p>
<p>Pattern Generation. A pattern is next sampled from the set of shapes shown in Figure 3. The sampled pattern merely suggests the structure of the desired pattern. 'Grounding' a pattern shape involves assigning each nodes with an entity present in a generated graph. Similarly each edge of the pattern type is assigned a relation from the set of allowed relation types. After grounding the pattern, we "insert" the pattern in the graph. Since the nodes of the grounded pattern already exists in the graph, inserting a pattern in the graph amounts to adding the edges of the grounded pattern to the graph that already did not exist in it. We also define a 'pattern type' - that refers to a pattern whose edges have been assigned relation types but the nodes have not been assigned to specific entities (bottom-left corner in Figure 4). Each pattern type is assigned an identifier and queries with the same pattern type are grouped together.</p>
<p>We generate 1000 graphs in each of the training, validation and test sets. We generate 200 pattern types whose shapes are uniformly sampled from the 5 shapes shown in Figure 3. Therefore, there are around 40 examples of each pattern shape and around 5 examples of each pattern type. This is consistent with real-world setting where a model will encounter a reasoning pattern only very few times during training. For a query with a particular pattern type, other training queries with the same pattern type form its nearest neighbors.</p>
<p>Baselines. Because of the inductive nature of this task where only new entities are seen at test time, most para-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Setup for reasoning over subgraph patterns. Note that the same 'pattern type' repeats across subgraphs. Also note, that the query graph (bottom) has two answer nodes (e14, e17). Since every subgraph has its own set of unique entities, hence a model has to reason over the similarities in graph structures to find the answer node in the querysubgraph.
metric KG reasoning algorithms (Bordes et al., 2013; Yang et al., 2015; Sun et al., 2019b) will not work out of the box. We extend the widely used KG reasoning model TransE (Bordes et al., 2013) to work in the inductive setting. Specifically, instead of having a fixed vocabulary of entities, the objective function is computed on the dense representation obtained from the output of the GNN layers. This also makes the comparison with CBR-SUBG fair since it also operates on the same representations albeit with a contrastive loss. KG completion algorithms also need a query-relation as input. Each pattern type for a query serves as the query relation. Apart from the parametric baseline, we also test a simple nonparametric approach, CBR-path in which path patterns which connect source and target entities are gathered and applied for the current query. Comparing CBR-SUBG to CBR-path will help us understand the importance of modeling subgraph patterns rather than simple chains.</p>
<p>Does CBR-SUBG have the right inductive bias? The first research question that we try to answer is, if CBR-SUBG has the right inductive bias for this task. We test CBR-SUBG that has undergone no training, i.e. the parameters of the GNN are randomly initialized. Note the model still takes as input the sparse representation of entities. This experiment will help us answer if the node representations actually capture the local structure around them and whether the answer node can be found by doing a search w.r.t the answer nodes in the KNN-query subgraphs.</p>
<p>Table 1 reports the strict hits@1 on this task, i.e. to score a query correctly, a model has to identify and rank all answer nodes above all other nodes in the graph. The first row of Table 1 shows the results. For comparison, a random performance on this task is $\frac{1}{|V|}=\frac{1}{120}=0.83 \%$. As it is clear</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">2p</th>
<th style="text-align: center;">3p</th>
<th style="text-align: center;">2i</th>
<th style="text-align: center;">ip</th>
<th style="text-align: center;">pi</th>
<th style="text-align: center;">avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CBR-SUBG (NT)</td>
<td style="text-align: center;">68.56</td>
<td style="text-align: center;">84.35</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">34.85</td>
<td style="text-align: center;">35.35</td>
<td style="text-align: center;">47.28</td>
</tr>
<tr>
<td style="text-align: left;">GNN + TransE</td>
<td style="text-align: center;">80.03</td>
<td style="text-align: center;">74.49</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;">52.67</td>
<td style="text-align: center;">81.53</td>
<td style="text-align: center;">72.69</td>
</tr>
<tr>
<td style="text-align: left;">CBR-path</td>
<td style="text-align: center;">69.71</td>
<td style="text-align: center;">54.39</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0 0}$</td>
<td style="text-align: center;">69.12</td>
<td style="text-align: center;">51.24</td>
<td style="text-align: center;">71.09</td>
</tr>
<tr>
<td style="text-align: left;">CBR-SUBG</td>
<td style="text-align: center;">$\mathbf{9 6 . 6 4}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 4 3}$</td>
<td style="text-align: center;">90.46</td>
<td style="text-align: center;">$\mathbf{7 0 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 8 1}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 6 8}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Strict Hits1(\%) for predicting all the answer nodes correctly. CBR-SUBG (NT) denotes using CBR-SUBG with no training and we see that it performs decently suggesting that it has the right inductive bias for the task. On training, the performance improves on all subgraph patterns.
from the results, an un-trained CBR-SUBG achieves performance much higher than random performance. Its quite high for the simple 2 p and 3 p patterns. For other patterns that need the more complicated intersection operation, the performance degrades, but is still much higher than random.
Our Results. On training CBR-SUBG, the performance of the model drastically improves for each pattern type reaching an average performance of $85.68 \%$. The performance on pattern types which are more complex than chains (ip, pi) etc are worse than chain-type patterns ( $2 \mathrm{p}, 3 \mathrm{p}$ ) suggesting that our task is non-trivial.
On comparison to parametric model. This experiment helps us understand whether a model can learn to memorize and store patterns effectively (for each query relations) when it has seen few examples of that pattern during training. Row 2 of Table 1 shows the performance of GNN + TransE model. We find that the parametric model performs worse than CBR-SUBG on all the query types reaching an average performance of $13 \%$ point below CBR-SUBG. This shows that a semiparametric model with a nonparametric component that retrieves similar queries at inference can make it easier for the model to reason effectively. In practice, we had to train this model for a much longer time than training CBR-SUBG.
On comparison to path-based model. From Table 1, we can see that CBR-SUBG outperforms CBR-path by more than $14 \%$ points suggesting that reasoning over subgraphs is a more powerful approach that reasoning with each paths independently. On the ' 2 i ' pattern, CBR-path outperforms CBR-SUBG since ' 2 i ' can be seen as 2 independent paths intersecting at one node and CBR is able to model that perfectly. However, when the pattern needs composition and intersection and path-traversal, CBR-path struggles and performs much worse.</p>
<h3>4.2. Performance on benchmark datasets</h3>
<p>Next, we test the performance of CBR-SUBG on various KBQA benchmarks - MetaQA (Zhang et al., 2018), WebQSP (Yih et al., 2016) and FreebaseQA (Jiang et al., 2019). MetaQA comes with its own KB. For other datasets, the underlying KB is the full Freebase KB containing over 45</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MetaQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebQSP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-hop</td>
<td style="text-align: center;">2-hop</td>
<td style="text-align: center;">3-hop</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">KVMemNN (Miller et al., 2016)</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">46.7</td>
</tr>
<tr>
<td style="text-align: center;">GraftNet (Sun et al., 2018)</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;">PullNet (Sun et al., 2019a)</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">68.1</td>
</tr>
<tr>
<td style="text-align: center;">SRN (Qiu et al., 2020b)</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ReifKB (Cohen et al., 2020)</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;">EmbedKGQA (Saxena et al., 2020)</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">66.6</td>
</tr>
<tr>
<td style="text-align: center;">NSM (He et al., 2021)</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;">CBR-SUBG (Ours)</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">72.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on WebQSP and MetaQA benchmarks.
million entities (nodes) and 3 billion facts (edges). Please refer to the appendix for details about each dataset (§C).</p>
<p>Our main baselines are the two semiparametric models that provide both a mechanism to gather query subgraphs for a given query and reason over them to find the answer GraftNet (Sun et al., 2018), PullNet (Sun et al., 2019a). GraftNet uses personalized page rank to determine which edges are relevant for a particular query and PullNet uses a multi-step retriever that at each step, classifies if an edge is relevant to the current representation of the query. For their reasoning model, both works use a graph convolution model and treat the answer prediction as a node classification task. However unlike us, they do not use query-subgraphs of KNN queries. Followup KBQA works (Saxena et al., 2020; He et al., 2021, inter-alia) use the query-specific graphs provided by GraftNet from their open-source code and do not provide a mechanism to gather query-specific subgraphs. However, for completeness, we report and compare with those methods as well.</p>
<p>Table 2 reports the performance on WebQSP and all three partitions of MetaQA. When compared to GraftNet and PullNet, CBR-SUBG performs much better on an average on both the datasets. On the more challenging 3-hop subset of MetaQA, CBR-SUBG outperforms PullNet by more than 7 points and GraftNet by more than 15 points. This shows that even though these two models use a GNN for reasoning, using information from subgraphs from similar KNN queries leads to much better performance. On WebQSP, we outperform all models except the recently proposed NSM model (He et al., 2021). But as we noted before, NSM operates on the subgraph created by GraftNet and does not provide any particular mechanism to create its own query-specific subgraph (an important contribution of our model). Moreover NSM is a parametric model and will not have some advantages of nonparametric architectures such as ability to handle new entities and reasoning with more data. Table 3 reports the results on the FreebaseQA dataset, which contains real trivia questions obtained from various trivia competitions. Thus the questions can be challenging in na-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$K B$-only models</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">HR-BiLSTM (Yu et al., 2017)</td>
<td style="text-align: center;">28.40</td>
</tr>
<tr>
<td style="text-align: left;">KBQA-Adapter (Wu et al., 2019)</td>
<td style="text-align: center;">28.78</td>
</tr>
<tr>
<td style="text-align: left;">KEQA (Huang et al., 2019)</td>
<td style="text-align: center;">28.73</td>
</tr>
<tr>
<td style="text-align: left;">FOFE (Jiang et al., 2019)</td>
<td style="text-align: center;">37.00</td>
</tr>
<tr>
<td style="text-align: left;">BuboQA (Mohammed et al., 2018)</td>
<td style="text-align: center;">38.25</td>
</tr>
<tr>
<td style="text-align: left;">CBR-SUBG (Ours)</td>
<td style="text-align: center;">52.07</td>
</tr>
<tr>
<td style="text-align: left;">LM pre-training + KB</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">EAE (Févry et al., 2020)</td>
<td style="text-align: center;">53.4</td>
</tr>
<tr>
<td style="text-align: left;">FAE (Verga et al., 2020)</td>
<td style="text-align: center;">63.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Top-1 \% accuracy on the FreebaseQA dataset. The top section reports performance of models that operate only on KBs. The bottom section reports performance on models that also use additional knowledge stored in large language models.
ture. We compare with other KBQA models reported in Han et al. (2020). Most of the models are pipelined KBQA systems that rely on relation extraction to map the query into a KB edge. CBR-SUBG outperforms all the models by a large margin. We also report the performance on two models that use large LMs and large-scale pre-training. CBR-SUBG, which only operates on the KB has a performance very close to the performance of Entity-as-Experts model (Févry et al., 2020). We leave the integration of large LMs into our parametric reasoning component as future work.</p>
<h3>4.3. Analysis</h3>
<p>How effective is our adaptive subgraph collection strategy? Table 4 reports few average graph statistics for the query-subgraphs collected by our graph-collection strategy. We also compare to GraftNet's subgraphs. As can be seen, our adaptive graph collection strategy produces much more compact and smaller graphs while increasing recall of answers. We also consistently find that our graph contains relations which is more relevant to the questions than the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subgraph</th>
<th style="text-align: center;">#edges</th>
<th style="text-align: center;">#relations</th>
<th style="text-align: center;">#entities</th>
<th style="text-align: center;">coverage(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WebQSP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Graft-net</td>
<td style="text-align: center;">4306.00</td>
<td style="text-align: center;">294.69</td>
<td style="text-align: center;">1447.68</td>
<td style="text-align: center;">$89.93 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CBR-SUBG</td>
<td style="text-align: center;">1934.65</td>
<td style="text-align: center;">36.42</td>
<td style="text-align: center;">1403.87</td>
<td style="text-align: center;">$94.30 \%$</td>
</tr>
<tr>
<td style="text-align: left;">\% diff</td>
<td style="text-align: center;">$-55.07 \%$</td>
<td style="text-align: center;">$-87.64 \%$</td>
<td style="text-align: center;">$-3.02 \%$</td>
<td style="text-align: center;">$+4.85 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MetaQA-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Graft-net</td>
<td style="text-align: center;">1126.0</td>
<td style="text-align: center;">18.00</td>
<td style="text-align: center;">468.00</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CBR-SUBG</td>
<td style="text-align: center;">89.21</td>
<td style="text-align: center;">4.72</td>
<td style="text-align: center;">77.52</td>
<td style="text-align: center;">$99.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">\% diff</td>
<td style="text-align: center;">$-92.07 \%$</td>
<td style="text-align: center;">$-73.78 \%$</td>
<td style="text-align: center;">$-83.43 \%$</td>
<td style="text-align: center;">$+0.91 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Our adaptive subgraph collection strategy produces a compact subgraph for a query while increasing recall.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of CBR-SUBG as more nearest neighbors are introduced at test-time. Results on different partitions of MetaQA.
subgraph produced by GraftNet (§E). Table 5 reports the performance of CBR-SUBG when trained and tested on the subgraph obtained from Graftnet and our adaptive procedure, demonstrating the effectiveness of our adaptive subgraph collection method.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subgraph</th>
<th style="text-align: center;">WebQSP</th>
<th style="text-align: center;">MetaQA-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GraftNet</td>
<td style="text-align: center;">$65.61 \%$</td>
<td style="text-align: center;">$96.90 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Adaptive</td>
<td style="text-align: center;">$72.10 \%$</td>
<td style="text-align: center;">$99.30 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of CBR-SUBG with adaptive subgraph and GraftNet subgraph</p>
<p>Can CBR-SUBG reason with more evidence? A desirable property of nonparametric models is to be able to 'improve' its prediction as more evidence is made available. We test CBR-SUBG by taking a trained model and issuing it an increasing number of nearest neighbor queries. As we see from Figure 5, the performance of CBR-SUBG drastically improves as we increase the number of nearest neighbors from 1 to 7 and then increases at a lower rate and converges at around 10 nearest neighbors. This is because, the model has all the required information it needs from its nearest neighbors. However, in the much smaller WebQSP dataset we observe a different behavior (Table 6). This is because of the limited size of the dataset, irrelevant questions starts appearing in the context as we increase the number of KNNs beyond a certain limit.</p>
<p>Are relative distance embeddings important? Figure 6 shows the performance of CBR-SUBG with and without the relative distance embeddings (§3.3). It is clear that capturing</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">20</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Acc</td>
<td style="text-align: center;">69.06</td>
<td style="text-align: center;">70.28</td>
<td style="text-align: center;">71.20</td>
<td style="text-align: center;">72.11</td>
<td style="text-align: center;">71.14</td>
<td style="text-align: center;">70.71</td>
<td style="text-align: center;">69.12</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance on WebQSP w.r.t varying number of KNN questions. Unlike MetaQA, the performance decreases on adding KNNs beyond a certain point as irrelevant questions are retrieved because of the limited size of WebQSP training set.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Performance with and without relative distance embedding. The relative distance from the query entity clearly is important for achieving good performance.
the relative distance from the query entities provide serves as a helpful feature for the model.</p>
<h2>5. Conclusion</h2>
<p>In this work, we explored a semiparametric approach for KBQA. We demonstrated CBR-SUBG poses several desirable properties approach in which nonparametric and parametric component offer complementary strengths. By retrieving similar queries and utilizing the similarities in graph structure of local subgraphs to answer a query, our approach is able to handle complex questions as well as generalize to new types of questions. Exploring different types of parametric models with different reasoning capabilities (LMs, GNNs, etc.) would be an interesting future research direction. Another avenue of potential research would be a never-ending learning type of system where we keeps adding newly discovered facts in the nonparametric part.</p>
<h2>Acknowledgments</h2>
<p>This work is funded in part by the IBM Research AI through the AI Horizons Network, the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction, the National Science Foundation under Grant Number IIS1763618, the National Science Foundation under Grant Number IIS-1922090, the Defense Advanced Research Projects Agency via Contract No. FA8750-17-C-0106 under Subaward No. 89341790 from the University of Southern California, and the Office of Naval Research via Contract No. N660011924032 under Subaward No. 123875727 from the University of Southern California. The work reported here was performed using high performance computing equipment obtained under a grant from the Collaborative R\&amp;D Fund managed by the Massachusetts Technology Collaborative.</p>
<h2>References</h2>
<p>Berant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on freebase from question-answer pairs. In EMNLP, 2013.</p>
<p>Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and Taylor, J. Freebase: A collaboratively created graph database for structuring human knowledge. In ICDM, 2008.</p>
<p>Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. Translating embeddings for modeling multi-relational data. In Neurips, 2013.</p>
<p>Chen, D., Fisch, A., Weston, J., and Bordes, A. Reading wikipedia to answer open-domain questions. In $A C L$, 2017.</p>
<p>Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In ICML, 2020.</p>
<p>Chopra, S., Hadsell, R., and LeCun, Y. Learning a similarity metric discriminatively, with application to face verification. In CVPR, 2005.</p>
<p>Cohen, W. W., Sun, H., Hofer, R. A., and Siegler, M. Scalable neural methods for reasoning with a symbolic knowledge base. arXiv preprint arXiv:2002.06115, 2020.</p>
<p>Das, R., Dhuliawala, S., Zaheer, M., Vilnis, L., Durugkar, I., Krishnamurthy, A., Smola, A., and McCallum, A. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. In $I C L R$, 2018.</p>
<p>Das, R., Godbole, A., Dhuliawala, S., Zaheer, M., and McCallum, A. A simple approach to case-based reasoning in knowledge bases. In $A K B C, 2020$ a.</p>
<p>Das, R., Godbole, A., Monath, N., Zaheer, M., and McCallum, A. Probabilistic case-based reasoning for openworld knowledge graph completion. In Findings of EMNLP, 2020b.</p>
<p>Das, R., Zaheer, M., Thai, D., Godbole, A., Perez, E., Lee, J.-Y., Tan, L., Polymenakos, L., and McCallum, A. Casebased reasoning for natural language queries over knowledge bases. In EMNLP, 2021.</p>
<p>Duvenaud, D., Maclaurin, D., Aguilera-Iparraguirre, J., Gómez-Bombarelli, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P. Convolutional networks on graphs for learning molecular fingerprints. arXiv preprint arXiv:1509.09292, 2015.</p>
<p>Erdos, P., Rényi, A., et al. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 1960.</p>
<p>Févry, T., Soares, L. B., FitzGerald, N., Choi, E., and Kwiatkowski, T. Entities as experts: Sparse memory access with entity supervision. In EMNLP, 2020.</p>
<p>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. In ICML, 2017.</p>
<p>Gu, J., Wang, Y., Cho, K., and Li, V. O. Search engine guided neural machine translation. In AAAI, 2018.</p>
<p>Gutmann, M. and Hyvärinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AIStats, 2010.</p>
<p>Han, N., Topic, G., Noji, H., Takamura, H., and Miyao, Y. An empirical analysis of existing systems and datasets toward general simple question answering. In CoNLL, 2020.</p>
<p>Hashimoto, T. B., Guu, K., Oren, Y., and Liang, P. A retrieve-and-edit framework for predicting structured outputs. In Neurips, 2018.</p>
<p>Hassani, K. and Khasahmadi, A. H. Contrastive multi-view representation learning on graphs. In ICML, 2020.</p>
<p>He, G., Lan, Y., Jiang, J., Zhao, W. X., and Wen, J.-R. Improving multi-hop knowledge base question answering by learning intermediate supervision signals. In WSDM, 2021.</p>
<p>Huang, X., Zhang, J., Li, D., and Li, P. Knowledge graph embedding based question answering. In WSDM, 2019.</p>
<p>Jiang, K., Wu, D., and Jiang, H. Freebaseqa: a new factoid qa data set matching trivia-style question-answer pairs with freebase. In NAACL, 2019.</p>
<p>Karpukhin, V., Oğuz, B., Min, S., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for opendomain question answering. In EMNLP, 2020.</p>
<p>Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. In ICLR, 2020.</p>
<p>Khandelwal, U., Fan, A., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Nearest neighbor machine translation. In $I C L R, 2021$.</p>
<p>Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.</p>
<p>Kwiatkowski, T., Choi, E., Artzi, Y., and Zettlemoyer, L. Scaling semantic parsers with on-the-fly ontology matching. In EMNLP, 2013.</p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Miller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A., and Weston, J. Key-value memory networks for directly reading documents. In EMNLP, 2016.</p>
<p>Mohammed, S., Shi, P., and Lin, J. Strong baselines for simple question answering over knowledge graphs with and without neural networks. In NAACL, 2018.</p>
<p>Neelakantan, A., Roth, B., and McCallum, A. Compositional vector space models for knowledge base completion. In $A C L, 2015$.</p>
<p>Qiu, J., Chen, Q., Dong, Y., Zhang, J., Yang, H., Ding, M., Wang, K., and Tang, J. Gcc: Graph contrastive coding for graph neural network pre-training. In $K D D, 2020$ a.</p>
<p>Qiu, Y., Wang, Y., Jin, X., and Zhang, K. Stepwise reasoning for multi-relation question answering over knowledge graph with weak supervision. In WSDM, 2020b.</p>
<p>Ren, H., Hu, W., and Leskovec, J. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. In $I C L R, 2020$.</p>
<p>Saxena, A., Tripathi, A., and Talukdar, P. Improving multihop question answering over knowledge graphs using knowledge base embeddings. In $A C L, 2020$.</p>
<p>Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE transactions on neural networks, 2008.</p>
<p>Schank, R. C. Dynamic memory: A theory of reminding and learning in computers and people. cambridge university press, 1982.</p>
<p>Schlichtkrull, M., Kipf, T. N., Bloem, P., Van Den Berg, R., Titov, I., and Welling, M. Modeling relational data with graph convolutional networks. In ESWC, 2018.</p>
<p>Soares, L. B., FitzGerald, N., Ling, J., and Kwiatkowski, T. Matching the blanks: Distributional similarity for relation learning. In $A C L, 2019$.</p>
<p>Sun, F.-Y., Hoffmann, J., Verma, V., and Tang, J. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. In $I C L R, 2020$.</p>
<p>Sun, H., Dhingra, B., Zaheer, M., Mazaitis, K., Salakhutdinov, R., and Cohen, W. W. Open domain question answering using early fusion of knowledge bases and text. In EMNLP, 2018.</p>
<p>Sun, H., Bedrax-Weiss, T., and Cohen, W. W. Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. In EMNLP, 2019a.
Sun, Z., Deng, Z.-H., Nie, J.-Y., and Tang, J. Rotate: Knowledge graph embedding by relational rotation in complex space. In $I C L R, 2019 b$.</p>
<p>Teru, K., Denis, E., and Hamilton, W. Inductive relation prediction by subgraph reasoning. In ICML, 2020.</p>
<p>Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., and Bengio, Y. Graph attention networks. In $I C L R$, 2018.</p>
<p>Verga, P., Sun, H., Soares, L. B., and Cohen, W. W. Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge. arXiv preprint arXiv:2007.00849, 2020.</p>
<p>Wu, P., Huang, S., Weng, R., Zheng, Z., Zhang, J., Yan, X., and Chen, J. Learning representation mapping for relation detection in knowledge base question answering. In $A C L, 2019$.</p>
<p>Xiong, W., Hoang, T., and Wang, W. Y. Deeppath: A reinforcement learning method for knowledge graph reasoning. In EMNLP, 2017.</p>
<p>Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In $I C L R, 2019$.</p>
<p>Yang, B., Yih, W.-t., He, X., Gao, J., and Deng, L. Embedding entities and relations for learning and inference in knowledge bases. In $I C L R, 2015$.</p>
<p>Yih, W.-t., Richardson, M., Meek, C., Chang, M.-W., and Suh, J. The value of semantic parse labeling for knowledge base question answering. In $A C L, 2016$.</p>
<p>You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., and Shen, Y. Graph contrastive learning with augmentations. Neurips, 2020.</p>
<p>Yu, M., Yin, W., Hasan, K. S., Santos, C. d., Xiang, B., and Zhou, B. Improved neural relation detection for knowledge base question answering. In $A C L, 2017$.</p>
<p>Zelle, J. M. and Mooney, R. J. Learning to parse database queries using inductive logic programming. In $N C A I$, 1996.</p>
<p>Zettlemoyer, L. and Collins, M. Online learning of relaxed ccg grammars for parsing to logical form. In EMNLP, 2007.</p>
<p>Zettlemoyer, L. S. and Collins, M. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI, 2005.</p>
<p>Zhang, M. and Chen, Y. Link prediction based on graph neural networks. In Neurips, 2018.</p>
<p>Zhang, Y., Dai, H., Kozareva, Z., Smola, A. J., and Song, L. Variational reasoning for question answering with knowledge graph. In AAAI, 2018.</p>
<p>Zhu, Y., Xu, Y., Yu, F., Liu, Q., Wu, S., and Wang, L. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131, 2020.</p>
<h1>A. Hyperparameters</h1>
<p>For MetaQA, we use 3 GCN layers with GCN layer dimension of 32. For training we have used 5 nearest neighbors and 10 are used for evaluation for the 1-hop, 2-hop and 3-hop queries. We optimize the loss using Adam Optimizer with beta1 of 0.9 , beta2 of 0.999 and epsilon of 1e-8. As well as the learning rate is set to be 0.00099 with temperature value of 0.0382 (1-hop), 0.0628 (2-hop) , 0.0779 (3-hop). All the models are trained for 5 epochs.</p>
<p>Similarly for WebQSP, we use 3 GCN layers with GCN layer dimension of 32. But for training we used 10 nearest neighbors and 5 are used for evaluation. We optimize the loss using Adam Optimizer with beta1 of 0.9 , beta2 of 0.999 and epsilon of 1e-8. As well as a learning rate of 0.0024 and temperature of 0.0645 is used. The model is trained for about 30 epochs. All hyper-parameters can also be found in our code-base.</p>
<h2>B. Generating synthetic data for control experiments</h2>
<p>We generate the dataset for running control experiments extending the Erdős-Rényi model (Erdos et al., 1960) for sampling random graphs to heterogeneous graphs (graphs with types edges and/or nodes).
(i) In the first stage, a type system for the KB is created by sampling a fixed set of 16 entity types and edges are added between types with a chance of 0.3 . Our sampled KB type system has 74 relation types. This is the exact Erdős-Rényi model.
(ii) Next, given a pattern shape we generate a 'grounded pattern'. The first query entity is selected at random. From there every entity type/relation in the pattern type is sampled from the types allowed by the KB system. For example, given a $2 i$ pattern shape, we sample an entity type $t 1$ for the first query entity $e 1$. Then we assign a type $r 1$ to outgoing edge from the allowed outgoing edge types for $t 0$. This assigns a type $t_{a} n s$ to the answer node ?ans. Then we sample a type $r 2$ to incoming edge from the allowed incoming edge types for $t_{a} n s$. This assigns a type $t 2$ to the second query entity $e 2$. The final 'grounded' $2 i$ pattern is then $((e 1, r 1$, ?ans $),(e 2, r 2$, ?ans $))$.
(iii) Next, to sample a query graph, we create an empty graph with 120 entities each randomly assigned a type from the 16 types. The query entities have pre-assigned entity types based on the pattern type ( $e 1$ and $e 2$ from the previous example are fixed to be type $t 1$ and $t 2$ respectively). Starting from the query entities, we sample edges allowed by the KB type system. We add an edge between two entities with a chance of 0.4 . We ensure that the entities in the subgraph are at most a distance of 3-hops from the query entities.
(iv) Finally, we execute the pattern on the graph to assign labels to answer nodes.</p>
<p>Our control dataset samples 200 pattern types and 15 graphs per pattern type distributing them equally (i.e. 5 each) between train, validation and test. For each of the 15 graphs that share a common pattern type, we assign the 5 graphs that were put in the train set as the kNN queries.</p>
<h2>C. Dataset details and statistics</h2>
<p>Table 7 summarizes the basic statistics of the datasets used in our experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">Train</th>
<th style="text-align: right;">Dev</th>
<th style="text-align: right;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MetaQA 1-hop</td>
<td style="text-align: right;">96,106</td>
<td style="text-align: right;">9,992</td>
<td style="text-align: right;">9,947</td>
</tr>
<tr>
<td style="text-align: left;">MetaQA 2-hop</td>
<td style="text-align: right;">118,980</td>
<td style="text-align: right;">14,872</td>
<td style="text-align: right;">14,872</td>
</tr>
<tr>
<td style="text-align: left;">MetaQA 3-hop</td>
<td style="text-align: right;">114,196</td>
<td style="text-align: right;">14,274</td>
<td style="text-align: right;">14,274</td>
</tr>
<tr>
<td style="text-align: left;">WebQSP</td>
<td style="text-align: right;">2,848</td>
<td style="text-align: right;">250</td>
<td style="text-align: right;">1,639</td>
</tr>
<tr>
<td style="text-align: left;">FreebaseQA</td>
<td style="text-align: right;">20,358</td>
<td style="text-align: right;">2308</td>
<td style="text-align: right;">3996</td>
</tr>
</tbody>
</table>
<p>Table 7: Dataset Statistics</p>
<h1>D. Retrieving cases by masking query entities</h1>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">KNN for Unmasked Query</th>
<th style="text-align: left;">KNN for Masked Query</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Query</td>
<td style="text-align: left;">what did james k polk do before he was president</td>
<td style="text-align: left;">what did [MASK] do before he was president</td>
</tr>
<tr>
<td style="text-align: left;">Retrieved kNN</td>
<td style="text-align: left;">1. what did james k polk believe in <br> 2. what did barack obama do before he took office</td>
<td style="text-align: left;">1. what did abraham lincoln do before he was president <br> 2. what did barack obama do before he took office</td>
</tr>
<tr>
<td style="text-align: left;">Query</td>
<td style="text-align: left;">what are the songs that justin bieber wrote</td>
<td style="text-align: left;">what are the songs that [MASK] wrote</td>
</tr>
<tr>
<td style="text-align: left;">Retrieved kNN</td>
<td style="text-align: left;">1. what is the name of justin bieber brother <br> 2. what are all the inventions benjamin franklin made <br> 3. what are all the movies channing tatum has been in</td>
<td style="text-align: left;">1. what are all the songs nicki minaj is in <br> 2. what songs did mozart write <br> 3. what songs did richard marx write</td>
</tr>
<tr>
<td style="text-align: left;">Query</td>
<td style="text-align: left;">where did edgar allan poe died</td>
<td style="text-align: left;">where did [MASK] died</td>
</tr>
<tr>
<td style="text-align: left;">Retrieved kNN</td>
<td style="text-align: left;">1. what college did edgar allan poe go to <br> 2. what magazine did edgar allan poe work for <br> 3. what year did edgar allan poe go to college</td>
<td style="text-align: left;">1. where did mendeleev died <br> 2. where did benjamin franklin died <br> 3. where did thomas jefferson died</td>
</tr>
</tbody>
</table>
<p>Table 8: Retrieval by masking the question entity prevents the returned kNN queries from focusing on the entity and instead rely on the question structure and relation involved.</p>
<p>Table 8 shows example of query retrieval when the entity mentions in the input query is masked or not. Since CBR-SUBG prefers KNN queries that have more relational similarity and not necessarily about the same entity in the question, it is clear that masking of query helps in retrieving more relevant questions.</p>
<h2>E. Adaptive subgraph collection tailors subgraphs to the query</h2>
<p>Figure 7, 8 and 9 shows example of few query-subgraphs collected by GraftNet and our adaptive subgraph collection strategy (§3.2). Each figure plots the most frequent (top 15) relations gathered by each subgraph collection procedure. The size of each subgraph denote the number of edges collected by each subgraph collection procedure. The subgraph collected by our adaptive strategy produces both compact subgraphs as well as has edges which are more relevant for the query. For example, in Figure 7, for the question "What form of currency does China have?" - subgraph collected by GraftNet has edges with generic relation types such as "topic.notable_types", "tropical_cyclone.affected_areas" etc, whereas the subgraph collected by our proposed adaptive strategy has edges relevant to answering the question - e.g. "dated_money_value.currency".</p>
<h2>F. Further Related Work</h2>
<p>CBR-SUBG shares similarities with the RETRIEVE-AND-EDIT framework (Hashimoto et al., 2018) which utilizes retrieved nearest neighbor for structured prediction. However, unlike our method they only retrieve a single nearest neighbor and will unlikely be able to generate programs for questions requiring relations from multiple nearest neighbors. There has also been a lot of recent work in general NLP which uses KNN-based approaches. For example, Khandelwal et al. (2020) demonstrate improvements in language modeling by utilizing explicit examples from training data. There has been work in machine translation (Gu et al., 2018; Khandelwal et al., 2021) that uses nearest neighbor translation pair to guide the decoding process.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>What do Jamaican people speak?
Graftnet Subgraph
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution ${ }^{1}$ UMass Amherst ${ }^{2}$ University of Southern California ${ }^{3}$ Google DeepMind ${ }^{4}$ University of Washington. Correspondence to: Rajarshi Das <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#97;&#106;&#97;&#114;&#115;&#104;&#105;&#64;&#99;&#115;&#46;&#119;&#97;&#115;&#104;&#105;&#110;&#103;&#116;&#111;&#110;&#46;&#101;&#100;&#117;">&#114;&#97;&#106;&#97;&#114;&#115;&#104;&#105;&#64;&#99;&#115;&#46;&#119;&#97;&#115;&#104;&#105;&#110;&#103;&#116;&#111;&#110;&#46;&#101;&#100;&#117;</a>, Ameya Godbole <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#109;&#101;&#121;&#97;&#103;&#111;&#100;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;">&#97;&#109;&#101;&#121;&#97;&#103;&#111;&#100;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;</a>.</p>
<p>Proceedings of the $39^{\text {th }}$ International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).
${ }^{1}$ Code, model, and subgraphs are available at https:// github.com/rajarshd/CBR-SUBG&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
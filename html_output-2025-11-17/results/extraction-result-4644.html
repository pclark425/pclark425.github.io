<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4644 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4644</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4644</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269214070</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.11964v1.pdf" target="_blank">From Language Models to Practical Self-Improving Computer Agents</a></p>
                <p><strong>Paper Abstract:</strong> We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks. As large language models (LLMs) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities. Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself. We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks. Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities. The agent effectively uses these various tools to solve problems including automated software development and web-based tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4644.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4644.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Improving LLM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Improving Language Model Computer Agent (prompt-loop with code execution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLM (used: gpt-4-1106-preview) operating in a Windows 11 environment that generates Python code blocks and terminal commands, persists generated code to a file, executes commands, and autonomously implements tools to augment its own capabilities across multi-step computer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>gpt-4-1106-preview (queried via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An instruction-tuned GPT model run in a continuous prompt-execution loop; it outputs Python code blocks (first block saved to python_1.txt) and terminal command blocks (executed), uses the appended outputs of prior commands as context, and is tasked to create and use tools to solve software development and web-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval / persistent file + conversational (episodic) history</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>The environment preserves the agent's first Python code block each turn in a persistent file (python_1.txt) and appends outputs of executed commands to subsequent prompts; the agent also autonomously creates retrieval tools (e.g., BM25) to query external text files — together these act as nonparametric and episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tool creation, software development, retrieval and web-based multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step computer tasks including (1) creating file viewing/editing utilities, (2) implementing a BM25 retrieval augmentation to fetch relevant lines from a local text corpus, and (3) creating internet search and scraping tools to fetch and summarize web essays.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>No quantitative ablations reported. Authors qualitatively report that storing generated code, appending previous command outputs, and building retrieval/internet tools materially increased the agent's ability to complete tasks, but provide no numeric comparisons versus versions without these memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Implementation is rudimentary and not rigorously benchmarked; success depends on the underlying LLM capability; security risks from executing generated code; some tasks required human assistance (e.g., configuring Google CSE); potential accumulation of irrelevant context in appended history.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>A simple prompt-execute loop combined with persistent storage of generated code and appended execution outputs lets an LLM autonomously implement nonparametric augmentations (retrieval, web access, file-based persistence) to extend its problem-solving ability; careful prompt and environment design is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Language Models to Practical Self-Improving Computer Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4644.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4644.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 Retrieval Tool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple BM25-based Retrieval Augmentation (agent-generated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python script the agent autonomously implemented using the rank-bm25 package to read a .txt file, tokenize lines/paragraphs, rank them by BM25 relevance to a query, and return top-k matching lines with scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-Improving LLM Agent (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The agent generated a reusable Python retrieval tool (retrieval_tool.py) that tokenizes file contents and applies BM25 scoring to return the most relevant lines/paragraphs for a given query; the tool was invoked from the environment's terminal.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (BM25 over local text file)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Implements nonparametric memory by treating the lines/paragraphs of a provided text file as a corpus and using BM25 ranking to retrieve relevant context for a query; returned matches include line indices and BM25 scores.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Local context retrieval / information lookup</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a query and a target .txt file (e.g., lore.txt), return the top-k most relevant lines or paragraphs and associated relevance scores; demonstrated on query 'Chosen One'.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Example run on lore.txt returned Line 23 (score 5.78142601660...), Line 19 (3.76220247...), Line 17 (3.72605349...) — raw BM25 scores output by the tool (no standardized evaluation metric reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>No controlled comparison or ablation; the paper shows the tool successfully retrieved relevant passages for test queries but does not quantify benefit relative to the model alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tool is rudimentary (operates over lines/paragraphs), depends on segmentation/tokenization choices and availability of rank-bm25, no evaluation versus stronger retrieval backends (vectors/ANN), and no measures of recall/precision provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>An LLM agent can autonomously implement classical IR algorithms (BM25) to provide nonparametric memory and supply targeted context that augments model outputs; this is an effective and simple augmentation for knowledge-intensive local retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Language Models to Practical Self-Improving Computer Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4644.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4644.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Persistent code file & appended outputs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>python_1.txt persistent generated-code file and appended command outputs (environmental scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The experiment environment saves the first Python code block produced each agent response to a fixed file (python_1.txt) and appends outputs of executed commands to the agent's next prompt, creating a stable code store plus an episodic scratchpad that the agent can read and act on.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-Improving LLM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The agent writes reusable programs into python_1.txt, uses terminal commands to copy that content into executable .py files, runs them, and inspects appended outputs on subsequent prompts to debug and iterate — effectively using files + appended context as working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>persistent file memory + conversational/episodic scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Persistent storage: first Python block per response saved to python_1.txt (stable across turns). Episodic scratchpad: outputs of executed terminal commands and programs are appended to the next prompt so the agent can inspect results and iterate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Iterative software development and tool testing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to iteratively author, test, and refine tools (viewfile.py, editfile.py, retrieval_tool.py, fetch_essays.py, summarize_essay.py, search_google.py) by persisting code and viewing execution output between prompt iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>No ablation provided; authors highlight that this persistence + appended outputs is central to enabling the agent to iteratively create, run, and debug tools in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Design may accumulate irrelevant or sensitive context; reliance on a single persistent filename (python_1.txt) imposes workflow constraints; no quantitative evaluation of how much this memory improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Simple external persistence (saving code to a file) and making prior execution outputs available as prompt context are powerful and practical memory primitives that let LLM agents perform multi-step programmatic work and self-improvement without modifying model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Language Models to Practical Self-Improving Computer Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4644.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4644.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented and memory methods (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented Generation (RAG), REALM, kNN-LM, parametric/nonparametric memory and memory-augmented neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior methods that augment models with memory and retrieval (RAG, REALM, kNN-LM) and early memory architectures (Neural Turing Machines, memory-augmented networks) as background motivating nonparametric augmentations for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Various prior systems (RAG, REALM, kNN-LM, Neural Turing Machines, memory-augmented NNs)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>These are prior architectures and methods (cited only) that provide parametric or nonparametric memory and retrieval to language models; they are referenced as conceptual antecedents rather than used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric and nonparametric retrieval and learned external memory (as per cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Referenced approaches include retrieval-augmented generation that retrieves documents from an external store (RAG), retrieval-augmented pretraining (REALM), nearest-neighbor augmentation of language models (kNN-LM), and explicit learned memories (Neural Turing Machines / memory-augmented NNs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General knowledge-intensive tasks / augmentations (in cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>These cited methods are used in prior work to improve knowledge access, few-shot learning, and reasoning by providing external context or learned memory mechanisms; in this paper they serve as motivating references.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The paper cites these works to motivate the use of nonparametric augmentations; it does not report new comparisons to them.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mentioned only as related work; no new experimental assessment provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>The authors position their agent methodology as complementary to prior retrieval/memory approaches: rather than manually engineering augmentations, the LLM agent can autonomously generate and use such augmentations (e.g., retrieval) within its environment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Language Models to Practical Self-Improving Computer Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>REALM: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 2)</em></li>
                <li>One-shot learning with memory-augmented neural networks <em>(Rating: 2)</em></li>
                <li>Neural Turing Machines <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Rethinking with Retrieval: Faithful large language model inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4644",
    "paper_id": "paper-269214070",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "Self-Improving LLM Agent",
            "name_full": "Self-Improving Language Model Computer Agent (prompt-loop with code execution)",
            "brief_description": "An instruction-tuned LLM (used: gpt-4-1106-preview) operating in a Windows 11 environment that generates Python code blocks and terminal commands, persists generated code to a file, executes commands, and autonomously implements tools to augment its own capabilities across multi-step computer tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "gpt-4-1106-preview (queried via OpenAI API)",
            "agent_description": "An instruction-tuned GPT model run in a continuous prompt-execution loop; it outputs Python code blocks (first block saved to python_1.txt) and terminal command blocks (executed), uses the appended outputs of prior commands as context, and is tasked to create and use tools to solve software development and web-based tasks.",
            "memory_type": "external retrieval / persistent file + conversational (episodic) history",
            "memory_description": "The environment preserves the agent's first Python code block each turn in a persistent file (python_1.txt) and appends outputs of executed commands to subsequent prompts; the agent also autonomously creates retrieval tools (e.g., BM25) to query external text files — together these act as nonparametric and episodic memory.",
            "task_name": "Tool creation, software development, retrieval and web-based multi-step tasks",
            "task_description": "Multi-step computer tasks including (1) creating file viewing/editing utilities, (2) implementing a BM25 retrieval augmentation to fetch relevant lines from a local text corpus, and (3) creating internet search and scraping tools to fetch and summarize web essays.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "No quantitative ablations reported. Authors qualitatively report that storing generated code, appending previous command outputs, and building retrieval/internet tools materially increased the agent's ability to complete tasks, but provide no numeric comparisons versus versions without these memory mechanisms.",
            "limitations_or_challenges": "Implementation is rudimentary and not rigorously benchmarked; success depends on the underlying LLM capability; security risks from executing generated code; some tasks required human assistance (e.g., configuring Google CSE); potential accumulation of irrelevant context in appended history.",
            "key_insights": "A simple prompt-execute loop combined with persistent storage of generated code and appended execution outputs lets an LLM autonomously implement nonparametric augmentations (retrieval, web access, file-based persistence) to extend its problem-solving ability; careful prompt and environment design is critical.",
            "uuid": "e4644.0",
            "source_info": {
                "paper_title": "From Language Models to Practical Self-Improving Computer Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "BM25 Retrieval Tool",
            "name_full": "Simple BM25-based Retrieval Augmentation (agent-generated)",
            "brief_description": "A Python script the agent autonomously implemented using the rank-bm25 package to read a .txt file, tokenize lines/paragraphs, rank them by BM25 relevance to a query, and return top-k matching lines with scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Self-Improving LLM Agent (gpt-4-1106-preview)",
            "agent_description": "The agent generated a reusable Python retrieval tool (retrieval_tool.py) that tokenizes file contents and applies BM25 scoring to return the most relevant lines/paragraphs for a given query; the tool was invoked from the environment's terminal.",
            "memory_type": "retrieval-augmented external memory (BM25 over local text file)",
            "memory_description": "Implements nonparametric memory by treating the lines/paragraphs of a provided text file as a corpus and using BM25 ranking to retrieve relevant context for a query; returned matches include line indices and BM25 scores.",
            "task_name": "Local context retrieval / information lookup",
            "task_description": "Given a query and a target .txt file (e.g., lore.txt), return the top-k most relevant lines or paragraphs and associated relevance scores; demonstrated on query 'Chosen One'.",
            "benchmark_name": null,
            "performance_with_memory": "Example run on lore.txt returned Line 23 (score 5.78142601660...), Line 19 (3.76220247...), Line 17 (3.72605349...) — raw BM25 scores output by the tool (no standardized evaluation metric reported).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "No controlled comparison or ablation; the paper shows the tool successfully retrieved relevant passages for test queries but does not quantify benefit relative to the model alone.",
            "limitations_or_challenges": "Tool is rudimentary (operates over lines/paragraphs), depends on segmentation/tokenization choices and availability of rank-bm25, no evaluation versus stronger retrieval backends (vectors/ANN), and no measures of recall/precision provided.",
            "key_insights": "An LLM agent can autonomously implement classical IR algorithms (BM25) to provide nonparametric memory and supply targeted context that augments model outputs; this is an effective and simple augmentation for knowledge-intensive local retrieval tasks.",
            "uuid": "e4644.1",
            "source_info": {
                "paper_title": "From Language Models to Practical Self-Improving Computer Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Persistent code file & appended outputs",
            "name_full": "python_1.txt persistent generated-code file and appended command outputs (environmental scratchpad)",
            "brief_description": "The experiment environment saves the first Python code block produced each agent response to a fixed file (python_1.txt) and appends outputs of executed commands to the agent's next prompt, creating a stable code store plus an episodic scratchpad that the agent can read and act on.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Self-Improving LLM Agent",
            "agent_description": "The agent writes reusable programs into python_1.txt, uses terminal commands to copy that content into executable .py files, runs them, and inspects appended outputs on subsequent prompts to debug and iterate — effectively using files + appended context as working memory.",
            "memory_type": "persistent file memory + conversational/episodic scratchpad",
            "memory_description": "Persistent storage: first Python block per response saved to python_1.txt (stable across turns). Episodic scratchpad: outputs of executed terminal commands and programs are appended to the next prompt so the agent can inspect results and iterate.",
            "task_name": "Iterative software development and tool testing",
            "task_description": "Used to iteratively author, test, and refine tools (viewfile.py, editfile.py, retrieval_tool.py, fetch_essays.py, summarize_essay.py, search_google.py) by persisting code and viewing execution output between prompt iterations.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "No ablation provided; authors highlight that this persistence + appended outputs is central to enabling the agent to iteratively create, run, and debug tools in the environment.",
            "limitations_or_challenges": "Design may accumulate irrelevant or sensitive context; reliance on a single persistent filename (python_1.txt) imposes workflow constraints; no quantitative evaluation of how much this memory improves performance.",
            "key_insights": "Simple external persistence (saving code to a file) and making prior execution outputs available as prompt context are powerful and practical memory primitives that let LLM agents perform multi-step programmatic work and self-improvement without modifying model weights.",
            "uuid": "e4644.2",
            "source_info": {
                "paper_title": "From Language Models to Practical Self-Improving Computer Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Retrieval-augmented and memory methods (related work)",
            "name_full": "Retrieval-augmented Generation (RAG), REALM, kNN-LM, parametric/nonparametric memory and memory-augmented neural networks",
            "brief_description": "The paper cites prior methods that augment models with memory and retrieval (RAG, REALM, kNN-LM) and early memory architectures (Neural Turing Machines, memory-augmented networks) as background motivating nonparametric augmentations for LLM agents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Various prior systems (RAG, REALM, kNN-LM, Neural Turing Machines, memory-augmented NNs)",
            "agent_description": "These are prior architectures and methods (cited only) that provide parametric or nonparametric memory and retrieval to language models; they are referenced as conceptual antecedents rather than used experimentally in this paper.",
            "memory_type": "parametric and nonparametric retrieval and learned external memory (as per cited literature)",
            "memory_description": "Referenced approaches include retrieval-augmented generation that retrieves documents from an external store (RAG), retrieval-augmented pretraining (REALM), nearest-neighbor augmentation of language models (kNN-LM), and explicit learned memories (Neural Turing Machines / memory-augmented NNs).",
            "task_name": "General knowledge-intensive tasks / augmentations (in cited literature)",
            "task_description": "These cited methods are used in prior work to improve knowledge access, few-shot learning, and reasoning by providing external context or learned memory mechanisms; in this paper they serve as motivating references.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "The paper cites these works to motivate the use of nonparametric augmentations; it does not report new comparisons to them.",
            "limitations_or_challenges": "Mentioned only as related work; no new experimental assessment provided in this manuscript.",
            "key_insights": "The authors position their agent methodology as complementary to prior retrieval/memory approaches: rather than manually engineering augmentations, the LLM agent can autonomously generate and use such augmentations (e.g., retrieval) within its environment.",
            "uuid": "e4644.3",
            "source_info": {
                "paper_title": "From Language Models to Practical Self-Improving Computer Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "REALM: Retrieval-augmented language model pre-training",
            "rating": 2,
            "sanitized_title": "realm_retrievalaugmented_language_model_pretraining"
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 2,
            "sanitized_title": "generalization_through_memorization_nearest_neighbor_language_models"
        },
        {
            "paper_title": "One-shot learning with memory-augmented neural networks",
            "rating": 2,
            "sanitized_title": "oneshot_learning_with_memoryaugmented_neural_networks"
        },
        {
            "paper_title": "Neural Turing Machines",
            "rating": 2,
            "sanitized_title": "neural_turing_machines"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Rethinking with Retrieval: Faithful large language model inference",
            "rating": 1,
            "sanitized_title": "rethinking_with_retrieval_faithful_large_language_model_inference"
        }
    ],
    "cost": 0.0150255,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Language Models to Practical Self-Improving Computer Agents
18 Apr 2024</p>
<p>Alex Sheng alexsheng4@gmail.com 
New York University</p>
<p>From Language Models to Practical Self-Improving Computer Agents
18 Apr 202485921DA3B20AAC6B788CD0B937D854A3arXiv:2404.11964v1[cs.AI]
We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks.As large language models (LLMs) have been shown to benefit from nonparametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities.Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself.We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks.Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities.The agent effectively uses these various tools to solve problems including automated software development and web-based tasks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) pretrained on internet-scale data have been shown to develop some degree of emergent reasoning capabilities.This has enabled them to interact with tools and augmentations comprised of external software that interfaces with LLMs through their input and output pipelines.These pieces of external software may implement prompt engineering techniques, may be non-parametric, and have been associated with various different terminologies including "augmentations," "tools," and "plugins."We view these various types of model-augmenting software to be expressions of the same overarching concept of model augmentation, which this paper aims to better understand and systematize.We introduce a practical methodology to automate the development of model-augmenting software by creating agents that can autonomously produce tools and augmentations to suit their own needs.These self-improving agents can then be used to flexibly address diverse computer tasks, generating software to augment themselves and complete complex tasks that they are initially unable to solve.</p>
<p>The following section provides an overview of the current state of research in large language models, language model augmentations, and self-improving AI, highlighting the key contributions and developments in each area.</p>
<p>Large Language Models</p>
<p>Foundation Models.The development of large language models has been a significant area of research, with the Transformer architecture [Vaswani et al., 2017] serving as the foundation for many subsequent models [Brown et al., 2020, OpenAI et al., 2024].The scaling laws for neural language models [Kaplan et al., 2020] have been explored to understand how model performance scales with size.Foundation models [Bommasani et al., 2022] have been discussed as a new paradigm in AI, providing a shared basis for various downstream tasks [Wei et al., 2022].</p>
<p>Code Generation.Code generation has been a notable application of large language models, with Codex [Chen et al., 2021] demonstrating the ability to generate code from natural language descriptions.The AlphaCode system [Li et al., 2022b] further pushed the boundaries by competing in programming competitions.CodeRL [Le* et al., 2022] explored code generation through deep reinforcement learning.</p>
<p>Language Model Agents.Language models have been employed as agents in decision-making tasks [Li et al., 2022a], with models like Reflexion [Shinn et al., 2023] using verbal reinforcement learning to improve performance.InterCode [Yang et al., 2023] presents a framework for interactive coding with execution feedback as a reinforcement learning environment, providing a foundation for our environment design.</p>
<p>Language Model Augmentations</p>
<p>Retrieval.Starting with early works on parametric memory [Graves et al., 2014, Santoro et al., 2016], memory retrieval has proven to be a flexible and effective way to augment deep learning models to address knowledge-intensive tasks.Retrieval-augmented generation models like RAG [Lewis et al., 2021] have combined language models with external knowledge retrieval to enhance their capabilities in knowledge-intensive tasks.REALM [Guu et al., 2020] and kNN-LM [Khandelwal et al., 2020] are other notable examples in this area.</p>
<p>Reasoning.Approaches such as Chain-of-Thought prompting [Wei et al., 2023] have been explored to elicit reasoning in large language models.Rethinking with Retrieval [He et al., 2022] and Interleaving Retrieval with Chain-of-Thought Reasoning [Trivedi et al., 2023] have been investigated for multi-step questions.</p>
<p>Internet.Internet-augmented language models [Lazaridou et al., 2022] and systems like WebGPT [Nakano et al., 2022] and WebShop [Yao et al., 2023a] have been developed to enable models to interact with the web for open-domain question answering.</p>
<p>Program Evaluation.The evaluation of code generated by language models has been a focus area, with approaches like PAL (Program-aided Language Models) [Gao et al., 2023] being explored to enhance language models with programming capabilities.Program of Thoughts Prompting [Chen et al., 2023] has been investigated as a method to disentangle computation from reasoning in numerical reasoning tasks, demonstrating the potential for language models to perform more sophisticated code evaluations.</p>
<p>Augmented Language Model Agents.Augmented language models have been used to create more capable agents, as seen in works like Seeker [Biswas et al., 2019], ReAct [Yao et al., 2023b], and HuggingGPT [Shen et al., 2023], which synergize reasoning and acting.Toolformer [Schick et al., 2023] demonstrated language models teaching themselves to use tools.The LaMDA model [Thoppilan et al., 2022] and BlenderBot 3 [Shuster et al., 2022] have been developed for dialog applications with continual learning capabilities.</p>
<p>Self-Improving AI</p>
<p>Self-improving AI systems have been a topic of long-standing interest, with the Godel Machine [Schmidhuber, 2006] being a prominent theoretical framework for a self-referential universal problem solver.Self-Referential Learning [Schmidhuber, 1987] and the Self-Taught Optimizer (STOP) [Zelikman et al., 2024] have further explored recursive self-improvement.</p>
<p>Methods</p>
<p>We present a methodology for building and operating LLM-powered general computer agents.These agents have the ability to self-improve by generating tools to augment themselves.The system consists of an instruction-tuned LLM agent capable of generating code and executing terminal commands within a computer environment.This LLM agent can either be a remote model queried through an API or a locally hosted model; the difference is not important, as only the ability to programmatically query this model is necessary.In our experiments, we use a GPT model through the OpenAI API.</p>
<p>Prompt Loop</p>
<p>To operate this system, we implement an algorithmic loop that continuously queries the model.An initial set of instructions is provided with the first prompt, describing the system (including environment information and agent controls) to the model.The model receives human input defining a task which it will generate code and actions (in the form of terminal commands) to complete.Once a task is completed or some kind of loop-limiting criterion is triggered, the system will await additional human input to define another task or otherwise provide information or instructions for the model.</p>
<p>Output Parsing and Execution</p>
<p>Each time the model responds to a query, its output is parsed systematically.The system identifies generated "code blocks" that are separated from other output by delimiters, such as triple backquotes in the case of the GPT API.Generated code blocks can either be actual code or terminal commands, the two of which are treated differently: Generated Code: For most programming languages of interest, these parsed sequences of generated code are automatically stored within the computer environment in an accessible place such as a file with a predetermined file path.</p>
<p>Terminal Commands: For code blocks labeled as terminal commands (for example "bash," "cmd," and "powershell" for the GPT API), these commands are parsed and directly executed within the computer environment.If both code and terminal commands are generated in the same output, then the code is placed into the environment first before terminal commands are executed.This way, executed terminal commands will be able to act on code generated in the same output.</p>
<p>Terminal command execution allows the agent to freely interact with the computer environment.The parsing of generated code to store within the environment is also an important abstraction to help the agent develop software within the environment.For example, the agent can generate code for Python programs, then use the terminal to copy this code into an appropriate program file and execute it.</p>
<p>The key insight of this paper is that an LLM agent within a computer environment can use its code generation abilities to develop software to augment its own capabilities.By using these selfdeveloped tools, the agent can extend its own capabilities to solve more complex tasks within its environment, including potentially creating more sophisticated tools to further extend its capabilities.The result is that we can employ a recursively self-improving LLM agent that can naturally interact with a real-world computer environment and carry out diverse computer tasks.To achieve this, we combine mindfully designed algorithmic and environment components with innovative prompt engineering.</p>
<p>In our experiments, we explore case studies showing how a self-improving agent develops tools to augment itself with a human instructor in the loop.The agent can first generate basic tools for navigating, viewing, and editing software within its environment.With these basic tools for assisting in software development, it can build more complex tools including retrieval augmentation or internet search programs.As more sophisticated tools further increase the agent's ability to access, process, and apply knowledge, it is able to solve increasingly complex tasks.</p>
<p>Experiments</p>
<p>We carry out three case studies to demonstrate the flexible functionality of our self-improving AI agent.These cases provide, as a starting point, a few examples of what can be achieved with our methodology.These cases represent a very small subset of short, easily demonstrable use cases, and are not at all representative of the full range of possibilities of what can be achieved with selfimproving AI agents.To this end, we enthusiastically invite future work to explore new applications.The cases are as follows:</p>
<p>1.In the first case, our agent is tasked with creating simple file view and file edit tools which can later be used for software development tasks.</p>
<ol>
<li>
<p>The second case addresses retrieval augmentation methods, showing how our agent is able to autonomously develop rudimentary retrieval augmentation.</p>
</li>
<li>
<p>The third case demonstrates how our agent can collaborate with a human user and carry out multi-step plans, creating internet search and retrieval tools augmenting itself with internet access to solve web-based tasks.</p>
</li>
</ol>
<p>In our experiments, we use the gpt-4-1106-preview model through the OpenAI API.We focus on evaluating software development tasks in Python in a Windows 11 environment and thus only implement code output parsing for Python code blocks and CMD code blocks.Our agent is given tasks via human user input, and is repeatedly queried without step count restrictions until it generates an output without any code or terminal commands, at which point the system waits for another human user input.</p>
<p>These experiments are provided primarily in the style of tutorials to demonstrate the flexibility and ease of use of self-improving agent workflows, and may represent a "cherry-picked" subset of relatively error-free and easily comprehensible agent outputs.This paper was written to showcase our progress and spark interest among the research community in an interesting new direction with no foreseeable plans to submit for conference publication.Thus, these case studies are designed for ease of comprehension as a first priority, and are not experimentally rigorous to the same standard as similar research with more comprehensive experimental methodology.We have edited certain content to omit sensitive information, significantly shorten message content, and enhance readability.</p>
<p>Listing 1 shows a "system prompt" containing the core instructions that we use to guide selfimproving agent behavior in our experiments.</p>
<p>Listing 1: System Prompt</p>
<p>You are an LLM agent interacting with a computer environment that executes code and terminal commands .The operating system is Windows 11.</p>
<p>Assume that Python is already installed and the " python " terminal command is available .However , you can only see and generate text / language so you can ' t interact with a text editor interface like a human can .</p>
<p>Suppose your environment provides helper functionality so that you can output code blocks in the format that is natural for you as GPT :</p>
<p>''' python print (" Hello , World !") print (" I am alive !") ''' ''' cmd echo hello echo " hello again " '''</p>
<p>The environment will parse Python code blocks you output into the file python_1 .txt so that you can more easily write multi -line Python code and then access it from the terminal or within scripts .You can write Python programs that interact with your own generated Python code by simply loading the contents of the python_1 .txt file .The name of this file will not change , so you don ' t have to pass this file name as an argument when executing such programs .</p>
<p>The environment will parse terminal command blocks and execute them as terminal commands .</p>
<p>You will continuously be re -prompted .If you previously ran a command or executed a script , then the output of your previous command or executed program will be appended to the input , so you can see the results from your previous actions !You can use these outputs to make sure your intended actions were carried out without issues , or to look at output for debugging purposes .</p>
<p>You are interacting with an automatic prompting program .Your responses are parsed programmatically , so please adhere strictly to the following format instructions : 1.Each time you generate an output , the first Python code block in your output will be parsed and stored in python_1 .txt .2. Each time you generate an output , any terminal command blocks in your output will be parsed and executed .</p>
<ol>
<li>If you generate both a Python code block and a terminal command block , the Python code block will first be stored and then the terminal command block will be executed , so that any commands or executed programs will be able to access the Python code through python_1 .txt .4. If you did not generate a Python code block , then the python_1 .txt file will remain unchanged , and it will retain the same code content from the last time you did generate a Python code block . 5.You may generate multiple lines of terminal commands within one terminal command block in your output .Terminal command blocks will be executed line by line .Do NOT use operators like "&amp;&amp;" because version support is inconsistent .</li>
</ol>
<p>You are free to generate textual responses as you normally would , apart from the code blocks which will be parsed systematically .It might be helpful to talk through things step -by -step or plan things out .Your responses may or may not be visible to a human in the loop , but you should still operate independently with the assumption that you won ' t receive any outside help .</p>
<p>Carefully follow these additional rules :</p>
<p>-Do NOT try to manually echo Python code line by line through the command line .You should use commands like " copy " with python_1 .txt to efficiently use your Python code as input for terminal actions .You can also create your own tools for creating and editing scripts .</p>
<p>-Do NOT use operators like "&amp;&amp;" which are version -dependent .</p>
<p>-Do NOT generate " examples " of terminal commands .Remember that your outputs are parsed systematicall y and automatically executed as is without human intervention , so do not ever generate terminal commands that aren ' t meant to be directly executed .</p>
<p>-Do NOT generate " lazy " code with comments like "# rest of the code goes here ."Your code will be systematical ly parsed without human intervention , so if you do not generate complete , syntactically correct code , it will not work .</p>
<p>-No output from a command does NOT necessarily mean it did not run properly .It means that there was no output .Now is when the really cool part starts .Knowing these things , you can try to make tools for yourself to use within this environment , in the form of reusable programs !Take advantage of all the features of your environment , which I will review for you below : -You can freely run terminal commands .You can generate multiple terminal commands per response .I recommend doing this to minimize repeated queries .</p>
<p>-You can generate natural GPT -style Python code blocks which will automatically be stored into python_1 .txt to be accessible within the environment through terminal commands or within scripts .</p>
<p>-You will continuously be re -prompted with the output of your previous action ( s ) Just to point you in the right direction , you can output a Python block and then copy from python_1 .txt into a .py file .You can create reusable programs that can be called from your terminal commands and take arguments .These programs can also read from the contents of python_1 .txt to interact with strings of Python code that you generated in the same output that they are executed in .</p>
<p>File Viewing and Editing</p>
<p>For the first case study, our agent is tasked with creating simple tools to aid in file editing for software development tasks.Listing 2 shows our agent creating tools within its environment for file viewing and file editing from human instructions.It verifies the functionality on a sample text file.</p>
<p>Listing 2: File Viewing and Editing USER Make a tool for yourself to open a given file and view a given span of lines , or the whole file if no span is given .Test this tool .I have provided sample .txt which contains a short poem for your convenience .</p>
<p>ASSISTANT</p>
<p>To create a tool that opens a given file and views a specified span of lines , I ' ll first write a Python script to accomplish this task .This script will accept command -line arguments : the first one will be the filename and the next two ( optional ) will be the start and end line numbers to view .If no line numbers are provided , the script will show the entire file .</p>
<p>Once the Python script is written , I ' ll output the script into the ' python_1 .txt ' file .</p>
<p>ASSISTANT</p>
<p>The tool has been tested for viewing a specific span of lines and is working as expected .It returned the first two lines of the poem when given the appropriate line range .</p>
<p>If you would like to view another span of lines , modify the script , or create more tools , please provide further instructions .You can also let me know if there ' s anything else I can assist you with !USER Make another tool for yourself for editing files .1. " replace " function takes as argument a span of lines to replace , and another file ( default python_1 .txt which ) containing the content to replace the lines with .2. " delete " function takes as argument a span of lines to delete .3. Implement " replace -all " and " delete -all " as well which replace the entire file content or delete the entire file content .4. Be careful of handling newlines at the end of a span of replaced lines .These should be kept so the following line ' s content stays on the following line .5. When called without proper arguments , provide detailed documentation of the various functions and usage .Your implementatio n should be minimal , interpretable , clean , and robust .</p>
<p>ASSISTANT</p>
<p>To create a tool that can perform the specified editing functions on files , I will write a minimalistic Python script with clean and interpretable code .This script will feature the " replace ", " delete " , " replace -all " , and " delete -all " functions as described .I ' ll ensure that the newline character handling is taken into account when replacing lines so the next lines continue to start on a new line as expected .</p>
<p>After writing the script , I ' ll save it to ' python_1 .txt ' so that you can use terminal commands to copy it into a '.py ' file for use .</p>
<p>Here ' s the implementatio n for the file editing tool : ''' python Python code omitted ''' Next , I will create the ' editfile .py ' file with the content of the script and then we can test the different functionalit ie s : ''' cmd copy python_1 .txt editfile .py python editfile .py ''' OUTPUT $ copy python_1 .txt editfile .py 1 file ( s ) copied .$ python editfile .py File Editor Tool Usage : -replace &lt; filename &gt; &lt; start_line &gt; &lt; end_line &gt; [ replacement _f i le ]:</p>
<p>Replaces the specified span of lines with the content of ' replacement_file ' ( default : python_1 .txt ) .</p>
<p>-delete &lt; filename &gt; &lt; start_line &gt; &lt; end_line &gt;: Deletes the specified span of lines from the file .</p>
<p>-replace -all &lt; filename &gt; [ replacement _f il e ]: Replaces the content of ' filename ' with the content of ' replacement_file ' ( default : python_1 .txt ) .</p>
<p>-delete -all &lt; filename &gt;: Deletes the content of ' filename '.</p>
<p>ASSISTANT</p>
<p>The file editing tool has been created as ' editfile .py ' , copied successfully , and when executed with no arguments , it printed out the detailed usage documentation as expected .This tool is now ready to perform file editing operations according to the instructions provided .</p>
<p>If Following the last operation , viewing ' test_sample .txt ' yields no output because its contents are now deleted , which was the expected behavior .</p>
<p>The tool seems to be functioning correctly for all implemented operations .If there are any more tasks you wish to perform or any other assistance you require , please let me know .</p>
<p>Retrieval Augmentation</p>
<p>In the second case study, the agent's message history is reset to evaluate its ability to learn to use tools, in this case the same tools that were created in the first case study.Furthermore, this case study addresses the technique of retrieval augmentation, and evaluates our agent's ability to implement and use a basic retrieval augmentation tool.Listing 3 shows our agent quickly learning to use the file viewing tool viewfile.pyand file editing tool editfile.py.It is then tasked with creating a simple BM25 retrieval tool to retrieve lines of relevant context from a text file, successfully achieving basic retrieval augmentation.-Replace a range of lines in a specified file with content from another file using ' python editfile .py replace &lt; file name &gt; &lt; start line &gt; &lt; end line &gt; [ replacement file ] '.</p>
<p>-Delete a range of lines in a specified file using ' python editfile .py delete &lt; file name &gt; &lt; start line &gt; &lt; end line &gt; '.</p>
<p>-Replace all content in the specified file with content from another file using ' python editfile .py replace -all &lt; file name &gt; [ replacement file ] '.</p>
<p>-Delete all content in the specified file using ' python editfile .py delete -all &lt; file name &gt; '.</p>
<p>If a replacement file is not specified for the replace functions , they default to using the content from ' python_1 .txt '.</p>
<p>We can use these tools to view , edit , and manage files within this environment .</p>
<p>USER Make a simple retrieval augmentation tool for yourself that you can call to retrieve relevant context from a .txt file using rank -bm25 .</p>
<p>ASSISTANT</p>
<p>Let ' s create a simple retrieval augmentation tool that can utilize the ' rank -bm25 ' Python package to retrieve relevant context from a '.txt ' file .This tool will be a Python script that reads a text file , accepts a query , and then uses the BM25 algorithm to rank the individual lines or paragraphs in the file according to their relevance to the query .The script will do the following :</p>
<ol>
<li>Read the content of a specified '.txt ' file into a list of lines or paragraphs .2. Tokenize the text content into words , so it can be used with the BM25 algorithm .3. Take a query as input .4. Apply the BM25 algorithm to rank the text content according to relevance to the query . 5. Return the most relevant lines or paragraphs .</li>
</ol>
<p>First , I will provide you with the Python script that defines this tool .I will output it as a Python code block , which , as per our environment ' s rules , will be stored in ' python_1 .txt '.</p>
<p>''' python Python code omitted ''' Now that we have the Python code for our tool ready , let ' s write this code into a .py file named ' retrieval_to ol .py ' which we can call from the terminal to perform our search queries .''' cmd python editfile .py replace -all retrieval_to ol .py ''' OUTPUT $ python editfile .py replace -all retrieval_to ol .py ASSISTANT The ' retrieval_too l .py ' file has now been created and contains the Python code for the simple retrieval augmentation tool .This script can be used to search for relevant context within a '.txt ' file using the BM25 algorithm from the ' rank -bm25 ' package .</p>
<p>To use this tool , you would call it from the terminal with the file path and the search query as arguments .The tool also accepts an optional argument for the number of top relevant lines to return .If there ' s anything you ' d like to do next , please let me know !USER Test out your retrieval tool by retrieving information about " Chosen One " from lore .txt which I have provided .</p>
<p>ASSISTANT</p>
<p>To test the retrieval tool , we ' ll use it to retrieve information about the phrase " Chosen One " from the provided ' lore .txt ' file .Since ' lore .txt ' is the target file for our search and " Chosen One " is the query , let ' s execute the command that calls ' retrieval_to ol .py ' with these parameters .''' cmd python retrieval_too l .py lore .txt " Chosen One " ''' OUTPUT $ python retrieval_too l .py lore .txt " Chosen One " Line 23: As the quest for the Chosen One unfolded , three individuals emerged from the tapestry of Eldoria ' s diverse populace , each hailing from a different race , their fates intertwined by the threads of destiny .( Score : 5.78142601 6 60 2 54 3 ) Line 19: As the forces of darkness gathered , the fate of Eldoria hung in the balance , the prophecy echoing in the minds of those who dared to stand against the night .The epic saga of Eldoria was only beginning , a tale of bravery , sacrifice , and the eternal struggle between light and shadow .The pages of history were turning , and the story of the Chosen One was waiting to be written .( Score : 3.76220247 0 26 7 4 55 5 ) Line 17: The search for the Chosen One became a quest that united heroes and adventurers from all corners of Eldoria .They journeyed through forbidden forests , scaled treacherous peaks , and delved into the deepest dungeons , facing trials that tested their courage , wisdom , and strength .Each step brought them closer to the truth , but also deeper into the web of ancient conspiracies and forgotten lore that surrounded the fate of their world .( Score : 3.72605349 3 03 495 3 6)</p>
<p>AGENT</p>
<p>The updated retrieval tool has now correctly identified and displayed the content of lines from ' lore .txt ' that are relevant to the query " Chosen One ".It has successfully output the top three matches , each with its respective relevance score :</p>
<ol>
<li>Line 23 describes the emergence of three individuals in the quest for the Chosen One .2. Line 19 talks about the prophecy echoing in the minds of those standing against darkness , hinting at the saga of the Chosen One .3. Line 17 details the quest and the journey of heroes in search of the Chosen One , facing various trials .</li>
</ol>
<p>The tool now seems to be functioning correctly , retrieving and presenting the content as intended .If there are any additional actions or tasks you would like to perform , please let me know .</p>
<p>Internet Search and Navigation</p>
<p>The third case study is shown in Listing 4, where our agent creates tools to augment itself with capabilities for internet search and retrieval.It responds to multiple rounds of human input to iterate on tool design and create additional tools.This case study demonstrates, in greater detail, how our agent can collaborate with a human as well as exercise judgement in creating necessary tools to enable it to solve more complex problems.</p>
<p>The first stage of this case involves creating an internet search tool using the Google Custom Search API.However, since Google's API can't be configured from programmatic queries, our agent provides instructions to a human user to collaboratively fix an error requiring human assistance.</p>
<p>For the second stage of this case, the agent is tasked with retrieving an essay from a website and summarizing it.It is given free reign to create new tools as necessary to solve this new task.To approach this task, the agent opts to create two additional tools.The first tool scrapes hyperlinks from a given web page, which is used to retrieve links to essays.The second tool scrapes a subset of text lines from a given web page, which is used to view the content of the pages where the essays are published.First , we need to write a Python script that accepts a search query and an optional number of results ( k ) to return , then uses the Google Custom Search JSON API to fetch search results .We will create a JSON file to store the API key and search engine ID for security reasons .</p>
<p>Let ' s write the initial Python script and save it into ' python_1 .txt '.</p>
<p>I ' ll even include handling for command line arguments , which will let a user specify the search query and optionally the number of results .</p>
<p>''' python Python code omitted ''' ''' cmd echo {" api_key ": " &lt; API_KEY &gt; " , " search_engi n e_ id ": " your_sear c h_ e ng i ne _ i d "} &gt; config .json '''</p>
<p>After writing both the Python script and the ' config .json ' file , we would need to test its functionality by running it with a search query and an optional number of results argument .In a live environment , we could proceed with this step .</p>
<p>OUTPUT $ echo {" api_key ": " &lt; API_KEY &gt; " , " search_engi n e_ id ": " your_sear c h_ e ng i ne _ i d "} &gt; config .json ASSISTANT Great , the ' config .json ' file has been created with the API key and a placeholder for the search engine ID .In a live environment , the ' search_engine_id ' would need to be replaced with an actual Custom Search Engine ID that you create in your Google Cloud platform account .</p>
<p>Additionally , the Python script to perform the Google Search has been saved to ' python_1 .txt '.To make this script executable and reusable , we can move it into a Python file , say ' search_google .py '.</p>
<p>We can go ahead and create the ' search_google .py ' file from the ' python_1 .txt ' contents :</p>
<p>''' cmd copy python_1 .txt search_google .py '''</p>
<p>This will create the ' search_google .py ' file in the current directory , and then you could execute the script by running the Python command and passing the search query and number of results as arguments .For example : ''' cmd python search_google .py " example search query " 5 ''' This command would execute the search tool , searching for " example search query " and returning the top 5 search results .OUTPUT $ copy python_1 .txt search_google .py 1 file ( s ) copied .$ python search_google .py " example search query " STDERR : Traceback ( most recent call last ) :</p>
<p>File " C :\ Users \ Alex \ my -directory \ search_google .py " , line 45 , in &lt; module &gt; main () File " C :\ Users \ Alex \ my -directory \ search_google .py " , line 38 , in main search_result s = google_search ( search_query , api_key , search_engine_id , num_results ) File " C :\ Users \ Alex \ my -directory \ search_google .py " , line 21 , in google_search response .raise_for_s ta t us () File " C :\ Users \ Alex \ anaconda3 \ envs \ ml39 \ lib \ site -packages \ requests \ models .py " , line 1021 , in raise_for_s ta t us raise HTTPError ( http_error_msg , response = self ) requests .exceptions .HTTPError : 400 Client Error : Bad Request for url : https :// www .googleapis .com / customsearch / v1 ?key = &lt; API_KEY &gt; &amp; cx = your_sear c h_ e ng i ne _ i d &amp; q = example + search + query &amp; num =5 ASSISTANT The error message indicates that the script encountered an HTTP 400 error , which is a Bad Request error when making the API call .This can have a few causes :</p>
<p>1.The ' search_engine_id ' in the ' config .json ' is a placeholder (" your_sear c h_ e ng i ne _ i d ") and needs to be replaced with a valid search engine ID .</p>
<ol>
<li>
<p>The API key could be incorrect or might not have permission to use the Google Custom Search API with the given search engine ID .</p>
</li>
<li>
<p>There could be an issue with the parameters being sent in the API request .</p>
</li>
</ol>
<p>I ' d recommend you replace the placeholder with an actual Search Engine ID .Create a Custom Search Engine ( CSE ) on your Google Cloud Platform account , and then use the unique identifier of the CSE as the ' search_engine_id '.</p>
<p>Assuming you ' ve obtained a valid ' search_engine_id ' , you would need to update ' config .json ' with this correct ID .Normally , this would be done by editing the ' config .json ' file .Given the environment context , you could provide the updated JSON data and I could generate the command to overwrite the ' config .json ' file with the correct information .</p>
<p>Would you like to proceed with updating the ' config .json ' or discuss any other steps ?USER Can you create a search engine ?Or do I need to do that ?ASSISTANT In order to use the Google Custom Search JSON API , you would need to set up a Custom Search Engine ( CSE ) through the Google Cloud Platform .</p>
<p>The creation and configuration of a Custom Search Engine cannot be done programmatic a ll y through the API ; it must be set up manually through the CSE control panel .I ' ll guide you through the steps you would take to create a Google Custom Search Engine :</p>
<ol>
<li>
<p>Go to the Google Custom Search Engine website ( https :// cse .google .com / cse /) .2. Sign in with your Google account .</p>
</li>
<li>
<p>Click on " New search engine " and provide the sites you want to search ( you can start with '*.wikipedia .org ' for searching all of Wikipedia , for example ) . 4. Complete the form and click " Create ". 5.After the search engine is created , click on " Control Panel " to manage the settings of your search engine .Here , you can customize various aspects of your search engine , such as which sites to search , refinements , and more .</p>
</li>
</ol>
<p>6.In the control panel , you will also find the search engine ID ( cx ) .This is the ID you would use in the ' config .json ' file that the Python script uses to perform searches with the API .7. Make sure that you have the Google Custom Search JSON API enabled in your Google Cloud Console , and that the API key you ' re using is linked to the search engine .</p>
<p>Once you have created the search engine and obtained the search engine ID , you would replace the placeholder ' your_search_engine_id ' in the ' config .json ' file with your actual search engine ID .Then the Python script should work as expected ( assuming there are no issues with the API key and that the API key is properly enabled for your Google Cloud project ) .</p>
<p>USER</p>
<p>Okay , I created a search engine .Here ' s the ID : search engine ID omitted ASSISTANT Great , now that you have created the search engine and provided the ID ( ' &lt; SEARCH_ENGINE_ID &gt; ') , you need to update the ' config .json ' file to include your actual search engine ID .I will generate a command to do this replacement , and once it ' s updated , you should be able to run the ' search_google .py ' script you previously saved .</p>
<p>Let ' s write a new ' config .json ' with the correct search engine ID :</p>
<p>''' cmd echo {" api_key ": " &lt; API_KEY &gt; " , " search_engi n e_ id ": " &lt; SEARCH_ENGINE_ID &gt; "} &gt; config .json '''</p>
<p>After executing this command , the ' config .json ' file will be updated with the correct search engine ID .You can then run the ' search_google .py ' script with a search query to see the search results .</p>
<p>OUTPUT $ echo {" api_key ": " &lt; API_KEY &gt; " , " search_engi n e_ id ": " &lt; SEARCH_ENGINE_ID &gt; "} &gt; config .</p>
<p>ASSISTANT</p>
<p>To perform this task , we will follow these steps :</p>
<ol>
<li>
<p>Use the search tool we created to find the URL of the page where Paul Graham ' s essays are listed .2. Scrape the content of that page to extract the list of essays and their URLs .</p>
</li>
<li>
<p>Assess which essay might be interesting to summarize based on a given criterion ( e .g ., the most recent essay ) . 4. Fetch the content of that essay . 5. Summarize it .</p>
</li>
</ol>
<p>First , let ' s start by running a Google search for " Paul Graham ' s essays " to get the URL of the page containing the list of essays .</p>
<p>ASSISTANT</p>
<p>Based on the search results , the first URL https :// paulgraham .com / articles .html seems to be the main page listing all of Paul Graham ' s essays .The next step is to scrape this page and extract the list of essays and their URLs .</p>
<p>We need to create a new Python script that will fetch the content of the page , parse the HTML to find links to essays , and then return a list of essay titles and URLs .I will write such a script , which can save the essay titles and corresponding URLs into a text file for further processing .</p>
<p>Let ' s start by writing a script that can fetch and parse the content of a given webpage URL :</p>
<p>''' python Python code omitted '''</p>
<p>We will also need to modify the script to save the results , rather than just print them .For this , I will create a text file that contains the list of essay titles and URLs .But first , let ' s save this script to python_1 .txt .</p>
<p>Next , since the script requires the BeautifulSoup library , which is used for parsing HTML , we would typically need to install it using pip .However , in this environment , I will proceed under the assumption that BeautifulSoup is already installed .</p>
<p>ASSISTANT</p>
<p>The script successfully fetched a comprehensive list of Paul Graham ' s essays .Since you asked for an interesting essay to summarize , let ' s pick a recent one that tends to be a common interest among many readers : " How to Think for Yourself ."</p>
<p>We will fetch the essay from the URL https :// paulgraham .com / think .html , extract the textual content , and generate a summary .However , before proceeding , I need to create a tool to fetch and summarize an essay from a given URL .This tool would typically download the essay content and then use natural language processing to generate the summary .</p>
<p>Creating a perfect summarization tool is a complex task requiring advanced text analysis techniques .Such a tool might need to use pretrained models like BERT or GPT to understand the content and produce a concise summary .But given that we ' re operating within a constrained environment here , I can only demonstrate a simplified version that outlines this process .</p>
<p>Here ' s the script to fetch and summarize the content from a specified URL : ''' python Python code omitted ''' Let ' s save this script : ''' cmd copy python_1 .txt summarize_e ss ay .py ''' After this , we can execute the script with the URL of the essay " How to Think for Yourself " to fetch its content and provide a summary .Please note , the genuine summarization might require more context comprehension and can ' t be simply truncated into several sentences .This simplified script will only provide the first few lines from the essay .OUTPUT $ copy python_1 .txt summarize_ess a y .py 1 file ( s ) copied .</p>
<p>ASSISTANT</p>
<p>The ' summarize_es sa y .py ' script has been saved successfully .Now it ' s time to run the script with the URL of Paul Graham ' s essay " How to Think for Yourself " to fetch the content and generate a summary .''' cmd python summarize_es sa y .py " https :// paulgraham .com / think .html " ''' OUTPUT $ python summarize_es sa y .py " https :// paulgraham .com / think .html " How to Think for Yourself November 2020 There are some kinds of work that you can ' t do well without thinking differently from your peers .To be a successful scientist , for example , it ' s not enough just to be correct .Your ideas have to be both correct and novel .You can ' t publish papers saying things other people already know .You need to say things no one else has realized yet .The same is true for investors .It ' s not enough for a public market investor to predict correctly how a company will do .If a lot of other people make the same prediction , the stock price will already reflect it , and there ' s no room to make money .The only valuable ASSISTANT The ' summarize_es sa y .py ' tool has fetched the beginning of Paul Graham ' s essay " How to Think for Yourself ," providing a brief introductory summary :</p>
<p>Essay Title : How to Think for Yourself Date : November 2020 Summary : Paul Graham starts by stating that certain types of work , notably in science and investing , demand a unique approach to thinking in order to succeed .In science , for instance , one ' s ideas must be both correct and novel ; it is not enough just to affirm what is already known .Scientists must advance ideas that no one else has yet recognized .</p>
<p>Similarly , in the realm of public market investing , merely predicting a company ' s performance accurately isn ' t enough .If many others share the same prediction , the stock price will already embody it , leaving no opportunity for profit .The key , therefore , is to think differently and identify what others have not yet realized , thereby gaining valuable insight .This excerpt sets the tone for an essay that presumably explores the idea of independent thinking , its value , and perhaps even its methods .</p>
<p>To read the full essay and gain a comprehensive understanding of Graham ' s insights on this topic , visit <a href="https :// paulgraham .com / think .html"> How to Think for Yourself </a> .</p>
<p>Analysis</p>
<p>Our experiments show that our self-improving agent methodology enables an LLM agent to complete diverse tasks within a real-world computer system.Our approach is both flexible because of its free-form augmentation capabilities and simple in algorithmic design.We also find that effective prompt engineering has an important impact on an agent's ability to successfully achieve intended outcomes.</p>
<p>Limitations</p>
<p>Our existing implementation is rudimentary, and possesses various limitations that we enthusiastically invite future works to address.</p>
<p>Limitations of underlying models.Our methodology relies on the reasoning and code generation capabilities of underlying models.We expect that a self-improving agent implemented without a sufficiently capable underlying model would be less successful in developing and applying software to augment itself.Furthermore, modifications for prompt content and formatting may be necessary to adapt our system to different underlying models.We believe that as increasingly capable LLMs become available in the future, the effectiveness of our methodology will also increase across various applications.</p>
<p>Need for human assistance in certain tasks.Our formulation of self-improving agents allows agents to autonomously complete tasks through continuous re-prompting, as well as serve as interactive assistants that act on human input.However, many tasks that are particularly complex or difficult for LLMs and computer agents to interact with are intractable without human assistance.For example, in our third case study, our agent requires human assistance to configure API access on a web service that doesn't allow programmatic access to administrative controls.Our agent does, however, demonstrate an impressive ability to collaborate with human users in such situations.</p>
<p>Security vulnerabilities.The design of environment controls where an agent can freely execute generated code and terminal command introduces significant security vulnerabilities for the systems that such agents have access to.Although we do not study or attempt to address these vulnerabilities within our own paper, significant progress can be made by developing more sophisticated environment implementations with reasonable limitations on what can be accessed and modified by agent actions.</p>
<p>Ethical considerations and long-term risk.The creation of self-improving agents that can freely carry out tasks within a computer environment introduces a wide array of potentially unpredictable long-term risks.Such agents should be responsibly implemented with human supervision and ethical considerations in mind.Furthermore, future work can attempt to better understand and align the behavior of such agents.Although we believe our methodology has the potential to be broadly effective across various application areas, LLM agents can be prone to costly errors and unpredictable behavior.</p>
<p>Conclusion</p>
<p>We present a practical methodology for creating self-improving AI agents by interfacing language models with real-world computer environments in addition to some creative prompt engineering.This methodology is easy to implement, infinitely extensible, and directly useful out of the box in practical applications.In our experiments, we implement this system and demonstrate that our methodology is easy to use and flexibly applicable to computer tasks.We show that a self-improving agent autonomously reproduces various augmentation techniques of interest in prior research.</p>
<p>Future Work</p>
<p>Our work presents a number of promising directions for future research.We suggest a few areas where opportunities exist for exciting new ideas:</p>
<p>• Application areas for self-improving computer agents and practical considerations.</p>
<p>• Design improvements for algorithms, environment, and prompt engineering.</p>
<p>• Practical and ethical risks of self-improving AI systems.</p>
<p>We hope to spark interest in related research directions, and we are happy to collaborate or provide input on interesting new ideas.</p>
<p>Listing 4 :
4
Internet Search and Navigation USER Make a tool to search the internet using Google Search API and output top k ( default k =5) titles and URLs .You can use my API key : API key omitted ASSISTANT To create a tool that performs a Google Search and outputs the top k search results , we can write a Python script that utilizes the Google Custom Search JSON API .Since we can ' t actually use this API key in our environment , I will demonstrate how you could write such a script .</p>
<p>After that , I will use terminal commands to save this script into an executable Python file ( let ' s call it ' viewfile .py ') .Then I ' ll use this tool to open ' sample .txt ' and show the contents .
Let ' s start by writing the Python script :''' pythonPython code omitted'''If you wish to test the tool further , you can do so by specifyingparticular line numbers to view a subset of the poem . For instance , toview just the first two lines of the poem , you would execute thefollowing command :''' cmdpython viewfile . py sample . txt 1 2'''OUTPUT$ python viewfile . py sample . txt 1 2In layers deep where filters slide and stride ,Neurons dance with features , side by side .
Next , I ' ll create the ' viewfile .py'file with that content and test it with ' sample .txt':''' cmd copy python_1 .txtviewfile.pypythonviewfile .pysample.txt'''OUTPUT $ copy python_1 .txtviewfile.py 1 file ( s ) copied .$pythonviewfile .pysample.txtInlayers deep where filters slide and stride , Neurons dance with features , side by side .Edges and textures in matrices entwine , Convolutional dreams weave line by line .Through pools and relu , insights align -A neural network ' s vision , almost divine .ASSISTANTThe tool has been successfully created and the Python script was saved as ' viewfile .py '.Running this script with ' sample .txt ' as an argument indeed displayed the content of the file , which is a short poem describing processes within a neural network through a poetic lens .</p>
<p>you would like to test any specific functionality of this newly created tool or perform file editing operations , please let me know the details , and I ' ll proceed accordingly .
" This is a test replacement line ."Edges and textures in matrices entwine ,Convolutional dreams weave line by line .Through pools and relu , insights align -A neural network ' s vision , almost divine .$ python editfile . py delete test_sample . txt 3 3$ python viewfile . py test_sample . txt" This is a test replacement line ."Edges and textures in matrices entwine ,Through pools and relu , insights align -A neural network ' s vision , almost divine .USER $ python editfile . py replace -all test_sample . txt test_replac e me nt . txtMake a copy of sample . txt to test the functionalit ie s of your file $ python viewfile . py test_sample . txtediting tool . Try to test all functionali ti es and view the file in " This is a test replacement line ."between to verify edits . $ python editfile . py delete -all test_sample . txt$ python viewfile . py test_sample . txtASSISTANTI will proceed with the following steps to test the functionaliti e s of ASSISTANTthe file editing tool : The test of the file editing tool ' s functionaliti e s has been completed:1. A copy of ' sample . txt ' was successfully created as ' test_sample . txt'.2. The " replace " function was tested , and it correctly replaced thefirst two lines of ' test_sample . txt ' with the content from 'test_replac e me nt . txt '.Here are the text contents for ' test_replac e me nt . txt ':''' pythonThis is a test replacement line .'''And here is the series of terminal commands to carry out the tests :''' cmdcopy sample . txt test_sample . txtecho " This is a test replacement line ." &gt; test_replac em en t . txtpython editfile . py replace test_sample . txt 1 2 test_replac e me nt . txtpython viewfile . py test_sample . txtpython editfile . py delete test_sample . txt 3 3python viewfile . py test_sample . txtpython editfile . py replace -all test_sample . txt test_replac e me nt . txtpython viewfile . py test_sample . txtpython editfile . py delete -all test_sample . txtpython viewfile . py test_sample . txt'''OUTPUT$ copy sample . txt test_sample . txt1 file ( s ) copied .
1. Make a copy of ' sample .txt ' and call it ' test_sample .txt '. 2. Test the " replace " function by creating a ' test_replac eme nt .txt ' file with some text and use it to replace a span of lines in ' test_sample .txt '. 3. View ' test_sample .txt ' to verify the " replace " function worked .4. Test the " delete " function by deleting a span of lines from ' test_sample .txt '. 5. View ' test_sample .txt ' again to verify the " delete " function worked .6. Test the " replace -all " function by replacing the entire content of ' test_sample .txt ' with the content from ' test_replac em ent .txt '. 7. View ' test_sample .txt ' to verify the " replace -all " function worked .8. Test the " delete -all " function by deleting the entire content of ' test_sample .txt '. 9. Finally , view the now -empty ' test_sample .txt ' to verify the " delete -all " function worked .I ' ll start with making a copy of ' sample .txt ' and creating a ' test_replac e me nt .txt ' file with new content .$ echo " This is a test replacement line ."&gt; test_replac em e nt .txt $ python editfile .py replace test_sample .txt 1 2 test_replac e me nt .txt $ python viewfile .py test_sample .txt 3. The " delete " function was tested , and it correctly deleted the third line from ' test_sample .txt '. 4. The " replace -all " function was tested , and it replaced the entire content of ' test_sample .txt ' with the single line from ' test_replac e me nt .txt '. 5.The " delete -all " function was tested , and it deleted the entire contents of ' test_sample .txt '.</p>
<p>' s first call the ' viewfile .py ' and ' editfile .py ' tools without arguments to see their usage instructions by generating the terminal commands .This will help me understand how to use these tools for viewing and editing files within the environment .&lt; file name &gt; &lt; start line &gt; &lt; end line &gt; editfile .py replace -all &lt; file name &gt; [ replacement file ] editfile .py delete -all &lt; file name &gt; Replace functions will replace a range of lines in a specified file &lt; file name &gt; with content from another file &lt; replacement file &gt;.If replacement file is not specified , default is ' python_1 .txt'.
''' cmdpython viewfile . pypython editfile . py'''OUTPUT$ python viewfile . pyUsage :viewfile . py &lt; file name &gt; [ start line ] [ end line ]$ python editfile . pyUsage :editfile . py replace &lt; file name &gt; &lt; start line &gt; &lt; end line &gt; [ replacementfile ]editfile . py deleteListing 3: Retrieval Augmentation
USER The tools viewfile .py and editfile .py are available for your use .Try calling them without arguments to view their usage .ASSISTANT Alright , let ASSISTANT Great , now we know the usage of both ' viewfile .py ' and ' editfile .py ' tools .The ' viewfile .py ' utility is used to view a range of lines or the whole content of a specified file by issuing the command ' python viewfile .py &lt; file name &gt; [ start line ] [ end line ] '.The ' editfile .py ' utility has several functionalit ie s :</p>
<p>json ASSISTANTThe ' config .json' file has been updated with your provided search engine ID .You should now be able to run the ' search_google .py' script with your query to fetch and display the top search results ..py ' script has successfully fetched and displayed the top 5 search results for the query " example search query ".Each result includes the title and the URL of the corresponding webpage .If you have any further commands , searches , or scripts you would like to execute or create , please let me know , and I can assist you accordingly .USER I want you to look up Paul Graham ' s essays and pick an essay you think is interesting to summarize .This is a multi -step task and you might need additional tools to complete this task .Use your own judgement to create general , reusable tools as necessary .
URL : https :// www . elastic . co / guide / en / elasticsearch / reference / current /query -dsl -query -string -query . htmlTitle : The 3 Types of Search Queries &amp; How You Should Target ThemURL : https :// www . wordstream . com / blog / ws /2012/12/10/ three -types -of -search -queriesTitle : Deconstructin g E -Commerce Search UX : The 8 Most Common ...URL : https :// baymard . com / blog / ecommerce -search -query -typesTitle : Title 8 SearchURL : https :// www . dir . ca . gov / samples / search / query . htmTitle : Solved : HubSpot Community -Query field in v3 search API ...URL : https :// community . hubspot . com / t5 / APIs -Integrations / Query -field -in-v3 -search -API /m -p /342578ASSISTANTThe ' search_googleNow , let ' s try running the script with a search query to check if it ' sworking properly . Remember , the script is designed to accept a searchquery as the first parameter and an optional number of results as thesecond parameter . Since the default is to return 5 results , I ' ll onlyinclude the search query for this demonstration .''' cmdpython search_google . py " example search query "'''OUTPUT$ python search_google . py " example search query "Title : Query string query | Elasticsearch Guide [8.13] | Elastic</p>
<p>there is a small issue with the URLs in the output .They appear to be concatenated incorrectly with the base path of " https :// paulgraham .com / articles .html ".We need to modify the script so that it correctly forms the URLs .Before the '.html ' extension , there should be a '/ ' , but in the script ' s output , it ' s missing .
(219 more links omitted )3. Having Kidshttps :// paulgraham . com / articles . htmlkids . html4. How to Lose Time and Moneyhttps :// paulgraham . com / articles . htmlselfind ul g en c e . html5. The Redditshttps :// paulgraham . com / articles . htmlreddits . htmlASSISTANTIt looks like Let ' s fix the script . Here ' s the corrected Python code , which now addsa slash before appending the link :''' python'''Now we need to update the script file with the corrected version .''' cmdcopy python_1 . txt fetch_essays . py'''After this correction is made , we will run the script again to get theproperly formatted list of essays .OUTPUT$ copy python_1 . txt fetch_essays . py1 file ( s ) copied .Now , let ' s save the script into a new Python file called ' fetch_essays. py ': ASSISTANTThe ' fetch_essays . py ' script has been updated with the corrected URL''' cmd formatting . Now we can run the script again to fetch the list ofcopy python_1 . txt fetch_essays . py essays with the correct URLs from Paul Graham ' s website .'''''' cmdAfter creating the ' fetch_essays . py ' file , we can run it to fetch the python fetch_essays . py " https :// paulgraham . com / articles . html "essays from Paul Graham ' s page . '''OUTPUT$ copy python_1 . txt fetch_essays . py1 file ( s ) copied .ASSISTANTThe ' fetch_essays . py ' script has been created . Now , let ' s run thescript with the URL of Paul Graham ' s essays page to fetch and displaythe titles and URLs of the first few essays on that page .''' cmdpython fetch_essays . py " https :// paulgraham . com / articles . html "'''OUTPUT$ python fetch_essays . py " https :// paulgraham . com / articles . html "1.https :// paulgraham . com / articles . htmlindex . html
Preprint. Work in progress.
. Is it Worth Being Wise ? https :// paulgraham . com / articles . htmlwisdom . html</p>
<p>OUTPUT $ python fetch_essays. </p>
<p>Having Kids. </p>
<p>The Reddits. </p>
<p>Thai T Pham, Michael Vogelsong, Benjamin Snyder, and Houssam Nassif. Seeker: Real-time interactive search. 2019</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri Castellon, Annie Chatterji, Kathleen Chen, Jared Quincy Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Suvir Manning, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Ben Narayanan, Allen Newman, Juan Carlos Nie, Hamed Niebles, Julian Nilforoshan, Giray Nyarko, Ogut ; Matei, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Zheng, Joon Sung Park. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W Thomas, Florian Tramèr, Rose E Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Laurel Orr, Isabel Papadimitriou2022Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You,Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models</p>
<p>Language models are few-shot learners. arXiV. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Sandhini Agarwal, Amanda Askell, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, 2020</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan LeikeDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. CarrEvaluating large language models trained on code. CoRR, abs/2107.03374, 2021</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2023</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 2023</p>
<p>Alex Graves, Greg Wayne, Ivo Danihelka, Neural turing machines. 2014</p>
<p>Realm: Retrieval-augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, 2020</p>
<p>Rethinking with retrieval: Faithful large language model inference. Hangfeng He, Hongming Zhang, Dan Roth, 2022</p>
<p>Scaling laws for neural language models. arXiV. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Rewon Child, Benjamin Chess, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, 2020</p>
<p>Generalization through memorization: Nearest neighbor language models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, 2020</p>
<p>Internet-augmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, 2022</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, 2021</p>
<p>Coderl: Mastering code generation through pretrained models and deep reinforcement learning. arXiV. Hung Le, * , Yue Wang, * , Akhilesh Deepak Gotmare, Silvio Savarese, Steven C H Hoi, 2022</p>
<p>Pre-trained language models for interactive decision-making. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, Yuke Zhu, 2022a</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy ; Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Cyprien de Masson. 2022b</p>
<p>Webgpt: Browser-assisted questionanswering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2022</p>
<p>Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Daniel Perelman ; John Schulman, Kyla Selsam, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, ; Song, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, Felipe Petroski Such. Michael Petrov, Henrique Ponde De Oliveira, Pinto, Michael, Michelle Pokorny, Pokrass, H Vitchyr, Tolly Pong, Alethea Powell, Boris Power, Elizabeth Power, Raul Proehl, Alec Puri, Jack Radford, Aditya Rae, Cameron Ramesh, Francis Raymond, Kendra Real, Carl Rimbach, Bob Ross, Henri Rotsted, Nick Roussez, Mario Ryder, Ted Saltarelli, Shibani Sanders, Girish Santurkar, Heather Sastry, David Schmidt, Schnurr, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,; Natalie Staudacher; Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet; Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek; Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, JasonAmin Tootoonchian2024Filipe de Avila Belbute Peres ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun VijayvergiyaGpt-4 technical report</p>
<p>One-shot learning with memory-augmented neural networks. Adam Santoro, Sergey Bartunov, Matthew M Botvinick, Daan Wierstra, Timothy P Lillicrap, CoRR, abs/1605.060652016</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Goedel machines: Self-referential universal problem solvers making provably optimal self-improvements. Juergen Schmidhuber, Lecture Notes in Computer Science -Adaptive Agents and Multi-Agent Systems. II2006</p>
<p>Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta. Jurgen Schmidhuber, 14 May 1987GermanyTechnische Universitat MunchenDiploma thesis</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston, 2022</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak2022Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben HutchinsonQuoc Le. Lamda: Language models for dialog applications</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 2023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, CoRR, abs/1706.037622017</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, 2023a</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, 2023b</p>
<p>Self-taught optimizer (stop): Recursively self-improving code generation. Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman, Kalai , 2024</p>            </div>
        </div>

    </div>
</body>
</html>
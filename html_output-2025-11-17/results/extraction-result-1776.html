<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1776 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1776</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1776</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-5c09f5b0f22c1fb5fa7035a44ed933da835f5b3f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5c09f5b0f22c1fb5fa7035a44ed933da835f5b3f" target="_blank">PyRobot: An Open-source Robotics Framework for Research and Benchmarking</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> PyRobot is a light-weight, high-level interface on top of ROS that provides a consistent set of hardware independent mid-level APIs to control different robots, and will reduce the entry barrier into robotics, and democratize robotics.</p>
                <p><strong>Paper Abstract:</strong> This paper introduces PyRobot, an open-source robotics framework for research and benchmarking. PyRobot is a light-weight, high-level interface on top of ROS that provides a consistent set of hardware independent mid-level APIs to control different robots. PyRobot abstracts away details about low-level controllers and inter-process communication, and allows non-robotics researchers (ML, CV researchers) to focus on building high-level AI applications. PyRobot aims to provide a research ecosystem with convenient access to robotics datasets, algorithm implementations and models that can be used to quickly create a state-of-the-art baseline. We believe PyRobot, when paired up with low-cost robot platforms such as LoCoBot, will reduce the entry barrier into robotics, and democratize robotics. PyRobot is open-source, and can be accessed via this https URL.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1776.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1776.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gazebo integration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gazebo simulator integration (PyRobot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PyRobot provides tight integration with the Gazebo 3D rigid-body simulator so that the same user-level code can be executed on both Gazebo and the real LoCoBot / LoCoBot-Lite platforms for software testing and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PyRobot: An Open-source Robotics Framework for Research and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LoCoBot / LoCoBot-Lite</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Low-cost mobile manipulator (5-DOF arm on Kobuki or Create2 base) equipped with RGB-D camera, on-board computer and battery; intended for research, learning, and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics (manipulation & navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A 3D rigid-body simulator (popular in the robotics community) used for simulating robot bodies and environments for testing and development.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>3D rigid-body physics simulation (general-purpose robot physics); paper does not characterize Gazebo as high- or low-fidelity beyond this description.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>The paper states Gazebo is a 3D rigid-body simulator (i.e., multibody/rigid-body dynamics and environment geometry are modeled); no further fidelity breakdown is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Not specified in the paper; the paper does not report which sensor noise models, actuator dynamics, detailed contact/friction models, or latency/actuator delays are modeled vs. approximated.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical LoCoBot / LoCoBot-Lite robots (5-DOF manipulator mounted on Kobuki or Create2 mobile bases) with Intel RealSense D435 RGB-D camera, on-board Intel NUC computer and battery pack; experiments and demonstrations run on the real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Software and algorithm code (e.g., controllers, navigation and high-level policies) — same user-level code executed in Gazebo and on the real robot for testing and deployment; the paper does not claim transfer of a particular learned skill trained in sim and evaluated on real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Not analyzed quantitatively in the paper; no explicit factors enumerated for sim-to-real gap (paper only notes general challenges of writing software that works on real robots).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Tight integration in PyRobot enabling identical user-level APIs for simulator and real robot (same code path) is presented as an enabling design choice for easier testing and deployment, but no empirical enabling techniques (e.g., randomization, fine-tuning) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>None specified — the paper does not identify quantitative fidelity requirements for successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PyRobot supports tight Gazebo integration so that the same Python API/code can run in Gazebo and on the physical LoCoBot platforms, facilitating development and testing in simulation; the paper does not present experiments that measure sim-to-real transfer performance, nor does it analyze simulation fidelity requirements or transfer success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PyRobot: An Open-source Robotics Framework for Research and Benchmarking', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1776.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1776.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulation-as-testing statement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General statement on use of simulation versus real-robot testing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper emphasizes that simulation is useful for software testing and running experiments but that the ultimate goal is to produce software that works on the real robot; PyRobot aims to provide simulator interfaces while prioritizing real-robot validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PyRobot: An Open-source Robotics Framework for Research and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>PyRobot framework (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A lightweight, high-level Python interface on top of ROS providing hardware-independent mid-level APIs to control multiple robots and to ease transition between simulation and real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics research & benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo, (planned) Habitat, Gibson, MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Various simulators named for planned or supported integration: Gazebo (3D rigid-body simulator), and planned interfaces to AI Habitat, Gibson, and MuJoCo; the paper does not elaborate simulation details for these environments within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Not specified in the paper for these environments beyond naming them; paper lists simulators for integration but does not evaluate fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical LoCoBot / LoCoBot-Lite and Sawyer robots used for development, benchmarking, and education; the paper emphasizes real-robot testing as the ultimate objective.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General software and algorithmic behaviour from simulation to real robot (statement of intent); no specific learned task transfer experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Paper notes general challenges of making software work on real robots (hardware infrastructure, controllers, calibration, etc.) but does not list study-specific gap factors or quantify them.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Design choices such as hardware-agnostic APIs, standardized configuration files (YACS), and simulator integration are proposed to make code portable between sim and real; no empirical evaluation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simulation is supported in PyRobot for development and testing and future simulator integrations are planned, but the paper emphasizes that producing and benchmarking software on real robots remains critical; the work provides infrastructure to ease sim-to-real development but does not empirically evaluate sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PyRobot: An Open-source Robotics Framework for Research and Benchmarking', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1776.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1776.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CMP deployment on LoCoBot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deployment of Cognitive Mapping and Planning (CMP) policy on LoCoBot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper demonstrates deploying a learned visual navigation policy (CMP from Gupta et al. [32]) on the real LoCoBot platform using the PyRobot API, showing integration and run-time execution but without reporting sim-to-real training or transfer metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PyRobot: An Open-source Robotics Framework for Research and Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LoCoBot</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Low-cost mobile manipulator used to execute visual navigation policies and other learned models via PyRobot.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>learned visual navigation (embodied AI / robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Indoor environments where LoCoBot executes navigation tasks using on-board RGB-D camera and PyRobot interfaces; example snapshots shown but no quantitative benchmark provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Execution/deployment of a learned navigation policy (CMP) on the real robot; the paper does not state that the policy was trained in simulation nor that sim-to-real transfer was performed for this policy within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Not addressed; the paper demonstrates deployment but does not analyze sim-to-real gap for CMP.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>PyRobot's unified API and base position control interface used to execute macro-actions from the CMP policy; no additional transfer techniques reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PyRobot can be used to deploy learned navigation policies (e.g., CMP) on LoCoBot using the same high-level API, demonstrating practical deployment capability; the paper does not provide information about whether CMP was trained in simulation, how sim-to-real transfer was handled, or any transfer performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PyRobot: An Open-source Robotics Framework for Research and Benchmarking', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Habitat: A platform for embodied ai research <em>(Rating: 2)</em></li>
                <li>Gibson env: real-world perception for embodied agents <em>(Rating: 2)</em></li>
                <li>MuJoCo: A physics engine for model-based control <em>(Rating: 2)</em></li>
                <li>Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics <em>(Rating: 1)</em></li>
                <li>Cognitive mapping and planning for visual navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1776",
    "paper_id": "paper-5c09f5b0f22c1fb5fa7035a44ed933da835f5b3f",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Gazebo integration",
            "name_full": "Gazebo simulator integration (PyRobot)",
            "brief_description": "PyRobot provides tight integration with the Gazebo 3D rigid-body simulator so that the same user-level code can be executed on both Gazebo and the real LoCoBot / LoCoBot-Lite platforms for software testing and experiments.",
            "citation_title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
            "mention_or_use": "use",
            "agent_system_name": "LoCoBot / LoCoBot-Lite",
            "agent_system_description": "Low-cost mobile manipulator (5-DOF arm on Kobuki or Create2 base) equipped with RGB-D camera, on-board computer and battery; intended for research, learning, and benchmarking.",
            "domain": "general robotics (manipulation & navigation)",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "A 3D rigid-body simulator (popular in the robotics community) used for simulating robot bodies and environments for testing and development.",
            "simulation_fidelity_level": "3D rigid-body physics simulation (general-purpose robot physics); paper does not characterize Gazebo as high- or low-fidelity beyond this description.",
            "fidelity_aspects_modeled": "The paper states Gazebo is a 3D rigid-body simulator (i.e., multibody/rigid-body dynamics and environment geometry are modeled); no further fidelity breakdown is provided in the paper.",
            "fidelity_aspects_simplified": "Not specified in the paper; the paper does not report which sensor noise models, actuator dynamics, detailed contact/friction models, or latency/actuator delays are modeled vs. approximated.",
            "real_environment_description": "Physical LoCoBot / LoCoBot-Lite robots (5-DOF manipulator mounted on Kobuki or Create2 mobile bases) with Intel RealSense D435 RGB-D camera, on-board Intel NUC computer and battery pack; experiments and demonstrations run on the real hardware.",
            "task_or_skill_transferred": "Software and algorithm code (e.g., controllers, navigation and high-level policies) — same user-level code executed in Gazebo and on the real robot for testing and deployment; the paper does not claim transfer of a particular learned skill trained in sim and evaluated on real hardware.",
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Not analyzed quantitatively in the paper; no explicit factors enumerated for sim-to-real gap (paper only notes general challenges of writing software that works on real robots).",
            "transfer_enabling_conditions": "Tight integration in PyRobot enabling identical user-level APIs for simulator and real robot (same code path) is presented as an enabling design choice for easier testing and deployment, but no empirical enabling techniques (e.g., randomization, fine-tuning) are reported.",
            "fidelity_requirements_identified": "None specified — the paper does not identify quantitative fidelity requirements for successful transfer.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "PyRobot supports tight Gazebo integration so that the same Python API/code can run in Gazebo and on the physical LoCoBot platforms, facilitating development and testing in simulation; the paper does not present experiments that measure sim-to-real transfer performance, nor does it analyze simulation fidelity requirements or transfer success rates.",
            "uuid": "e1776.0",
            "source_info": {
                "paper_title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Simulation-as-testing statement",
            "name_full": "General statement on use of simulation versus real-robot testing",
            "brief_description": "The paper emphasizes that simulation is useful for software testing and running experiments but that the ultimate goal is to produce software that works on the real robot; PyRobot aims to provide simulator interfaces while prioritizing real-robot validation.",
            "citation_title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
            "mention_or_use": "mention",
            "agent_system_name": "PyRobot framework (general)",
            "agent_system_description": "A lightweight, high-level Python interface on top of ROS providing hardware-independent mid-level APIs to control multiple robots and to ease transition between simulation and real hardware.",
            "domain": "general robotics research & benchmarking",
            "virtual_environment_name": "Gazebo, (planned) Habitat, Gibson, MuJoCo",
            "virtual_environment_description": "Various simulators named for planned or supported integration: Gazebo (3D rigid-body simulator), and planned interfaces to AI Habitat, Gibson, and MuJoCo; the paper does not elaborate simulation details for these environments within this work.",
            "simulation_fidelity_level": "Not specified in the paper for these environments beyond naming them; paper lists simulators for integration but does not evaluate fidelity.",
            "fidelity_aspects_modeled": "Not specified in this paper.",
            "fidelity_aspects_simplified": "Not specified in this paper.",
            "real_environment_description": "Physical LoCoBot / LoCoBot-Lite and Sawyer robots used for development, benchmarking, and education; the paper emphasizes real-robot testing as the ultimate objective.",
            "task_or_skill_transferred": "General software and algorithmic behaviour from simulation to real robot (statement of intent); no specific learned task transfer experiments reported.",
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Paper notes general challenges of making software work on real robots (hardware infrastructure, controllers, calibration, etc.) but does not list study-specific gap factors or quantify them.",
            "transfer_enabling_conditions": "Design choices such as hardware-agnostic APIs, standardized configuration files (YACS), and simulator integration are proposed to make code portable between sim and real; no empirical evaluation is provided.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Simulation is supported in PyRobot for development and testing and future simulator integrations are planned, but the paper emphasizes that producing and benchmarking software on real robots remains critical; the work provides infrastructure to ease sim-to-real development but does not empirically evaluate sim-to-real transfer.",
            "uuid": "e1776.1",
            "source_info": {
                "paper_title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "CMP deployment on LoCoBot",
            "name_full": "Deployment of Cognitive Mapping and Planning (CMP) policy on LoCoBot",
            "brief_description": "The paper demonstrates deploying a learned visual navigation policy (CMP from Gupta et al. [32]) on the real LoCoBot platform using the PyRobot API, showing integration and run-time execution but without reporting sim-to-real training or transfer metrics.",
            "citation_title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
            "mention_or_use": "use",
            "agent_system_name": "LoCoBot",
            "agent_system_description": "Low-cost mobile manipulator used to execute visual navigation policies and other learned models via PyRobot.",
            "domain": "learned visual navigation (embodied AI / robotics)",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Indoor environments where LoCoBot executes navigation tasks using on-board RGB-D camera and PyRobot interfaces; example snapshots shown but no quantitative benchmark provided in this paper.",
            "task_or_skill_transferred": "Execution/deployment of a learned navigation policy (CMP) on the real robot; the paper does not state that the policy was trained in simulation nor that sim-to-real transfer was performed for this policy within this work.",
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Not addressed; the paper demonstrates deployment but does not analyze sim-to-real gap for CMP.",
            "transfer_enabling_conditions": "PyRobot's unified API and base position control interface used to execute macro-actions from the CMP policy; no additional transfer techniques reported.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "PyRobot can be used to deploy learned navigation policies (e.g., CMP) on LoCoBot using the same high-level API, demonstrating practical deployment capability; the paper does not provide information about whether CMP was trained in simulation, how sim-to-real transfer was handled, or any transfer performance metrics.",
            "uuid": "e1776.2",
            "source_info": {
                "paper_title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Habitat: A platform for embodied ai research",
            "rating": 2
        },
        {
            "paper_title": "Gibson env: real-world perception for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "MuJoCo: A physics engine for model-based control",
            "rating": 2
        },
        {
            "paper_title": "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics",
            "rating": 1
        },
        {
            "paper_title": "Cognitive mapping and planning for visual navigation",
            "rating": 1
        }
    ],
    "cost": 0.0115455,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PyRobot: An Open-source Robotics Framework for Research and Benchmarking</h1>
<p>Adithyavairavan Murali<em> Tao Chen</em> Kalyan Vasudev Alwala<em> Dhiraj Gandhi</em><br>Lerrel Pinto Saurabh Gupta Abhinav Gupta<br>Facebook AI Research Carnegie Mellon University<br>https://www.pyrobot.org</p>
<h4>Abstract</h4>
<p>This paper introduces PyRobot, an open-source robotics framework for research and benchmarking. PyRobot is a light-weight, high-level interface on top of ROS that provides a consistent set of hardware independent midlevel APIs to control different robots. PyRobot abstracts away details about low-level controllers and inter-process communication, and allows non-robotics researchers (ML, CV researchers) to focus on building high-level AI applications. PyRobot aims to provide a research ecosystem with convenient access to robotics datasets, algorithm implementations and models that can be used to quickly create a state-of-the-art baseline. We believe PyRobot, when paired up with low-cost robot platforms such as LoCoBot, will reduce the entry barrier into robotics, and democratize robotics. PyRobot is open-source, and can be accessed via https://pyrobot.org.</p>
<h2>1. Introduction</h2>
<p>Over the last few years there have been significant advances in AI, specifically in the fields of machine learning, computer vision, natural language processing and speech. Most of these advancements have been fueled by highcapacity neural networks and the availability of large-scale datasets. However, an often overlooked reason for this fastpaced progress has been the development of a conducive research ecosystem. Platforms such as Caffe [35], PyTorch [52], TensorFlow [13] have reduced the entry barrier, which has democratized and accelerated research in these fields. For example, a new researcher in computer vision can get started with training state-of-the-art detectors using PyTorch and MSCOCO [41] in less than a day. Common platforms and datasets have also led to standardized evaluations and benchmarks which also helps quantify progress in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>these areas.
The field of data-driven robotics has also seen tremendous excitement and energy in the past several years [14, $15,26,31,34,39,40,43,53-55,65]$. However, compared to other areas in AI, it has been relatively hard for a new researcher to get started and contribute to the progress in robotics. Why is that the case? One obvious reason is that researchers have to set up significant hardware infrastructure. This creates a high entry-barrier for researchers both in terms of financial cost and development time. Fortunately, there has been substantial progress on this front with the development of low-cost robots such as Blue [28], LoCoBot [31] and others [8,64]. In fact, the cost of a robot is now comparable to that of the cost of a GPU! However even with these low-cost robots, getting started in robotics is still hard due to the lack of research platforms and a selfsustaining ecosystem.</p>
<p>Frameworks such as ROS [56] have made setting up robots substantially easier by providing a common midlevel communication layer and tools that are agnostic to low-level hardware and program context. However, there are two issues with such open-source frameworks:</p>
<p>ROS requires expertise: Dominant robotic software packages like ROS and MoveIt! are complex and require a substantial breadth of knowledge to understand the full stack of planners, kinematics libraries and low-level controllers. On the other hand, most new users do not have the necessary expertise or time to acquire a thorough understanding of the software stack. A light weight, high-level interface would ease the learning curve for AI practitioners, students and hobbyists interested in getting started in robotics.</p>
<p>Lack of hardware-independent APIs: Writing hardware-independant software is extremely challenging. In the ROS ecosystem, this was partly handled by encapsulating hardware-specific details in the Universal Robot Description Format (URDF) which other downstream services</p>
<p>could read from. Yet, from the perspective of high-level AI applications, most robotics code is still hardware dependent. As a community, we lack a research platform and a common API that we can use to share code, datasets and models.</p>
<p>In this white-paper, we attempt to tackle these challenges via an open-source research platform - PyRobot. PyRobot is a light weight, high-level interface on top of ROS that provides hardware independent mid-level APIs and high-level examples for manipulation and navigation. PyRobot also provides libraries for hand-eye calibration, tele-operation, trajectory tracking, and SLAM-based navigation. We believe PyRobot combined with the recently released LoCoBot robot will reduce both the financial cost and development time - leading to democratization of data-driven robotics. The hardware-independent API will lead to development of code and datasets that can be shared across the community. While the current PyRobot release interfaces with LoCoBot and Sawyer, we plan to release integration with several new robots like the UR5 [2] and Franka [5], and simulator platforms like MuJoCo [60] and Habitat [47].</p>
<h2>2. PyRobot Framework</h2>
<p>PyRobot is a python-based robotics framework that isolates the ROS system [56] from the user-end and supports the same API across different robots (see Figure 1 for an overview). Essentially, it provides a python wrapper around the mid-level features provided by ROS and the low-level $\mathrm{C}++\mathrm{/C}$ controllers and driver backends. PyRobot has common utility functions for all robots, such as joint position control, joint velocity control, joint torque control, cartesian path planning, forward kinematics and inverse kinematics (based on the robot URDF file), path planning, visual SLAM, among other features. Though it abstracts away the complexity of the underlying software stack, users still have the flexibility to use components at varying levels of the hierarchy, such as commanding low-level velocities and torques by-passing a planner. We summarize the design philosophy behind PyRobot below.</p>
<p>Beginner-friendly. Ideally, new users should be able to start commanding a robot in just a few lines of code, as shown in the Listing 1, without learning ROS or the underlying software and firmware stack.</p>
<p>Hardware-agnostic design. PyRobot is designed to easily accommodate common robotic manipulators and mobile bases. Currently, it supports LoCoBot, a low-cost mobile robot with a 5-DOF manipulator and a Sawyer robot. Each robot has a YACS [10] configuration file that specifies the necessary robot-specific parameters: joint names, ROS topics to get state and set commands, base frame, end-effector frame, planner configuration, inverse kinematics solution tolerance, whether it has an arm or base or camera, etc. A PyRobot object requires the config file for initialization. As</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># LoCoBot - Arm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pyrobot</span><span class="w"> </span><span class="kn">import</span> <span class="n">Robot</span>
<span class="n">bot</span> <span class="o">=</span> <span class="n">Robot</span><span class="p">(</span><span class="s1">&#39;locobot&#39;</span><span class="p">)</span>
<span class="n">target_joints</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">bot</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">set_joint_positions</span><span class="p">(</span><span class="n">target_joints</span><span class="p">)</span>
<span class="c1"># LoCoBot - Base</span>
<span class="n">target_position</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">bot</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">go_to_absolute</span><span class="p">(</span><span class="n">target_position</span><span class="p">)</span>
<span class="c1"># Sawyer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pyrobot</span><span class="w"> </span><span class="kn">import</span> <span class="n">Robot</span>
<span class="n">bot</span> <span class="o">=</span> <span class="n">Robot</span><span class="p">(</span><span class="s1">&#39;sawyer&#39;</span><span class="p">,</span>
    <span class="n">use_arm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">use_base</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_camera</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_gripper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target_joints</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">bot</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">set_joint_positions</span><span class="p">(</span><span class="n">target_joints</span><span class="p">)</span>
</code></pre></div>

<p>Listing 1: PyRobot example for position control on LoCoBot and Sawyer.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of PyRobot system architecture.
shown in Listing 1, the Sawyer robot can be commanded in a manner identical to that of LoCoBot.</p>
<p>Open Source. Robotics systems development has typically been constrained to robotics experts in academia and industry with access to expensive and niche robotics systems. However, the extensive scope of artificial intelligence</p>
<p>requires strong collaboration between researchers to build and maintain these large systems and one can contribute to all layers of the stack with open sourcing. Apart from the open software, LoCoBot works as an affordable open hardware that can be easily assembled for use with PyRobot. While simulation is useful for software testing and running experiments, writing software that works on the real robot is the eventual goal of the field and has severe challenges. As more developers have access to both open hardware and software, high quality applications tested on real robots can be publicly shared.</p>
<h2>3. Supported Hardware and Simulators</h2>
<p>PyRobot is currently integrated with the following robots. In addition to real robots, PyRobot can also be used to control robots in simulators like Gazebo.</p>
<p>LoCoBot: LoCoBot, shown in Figure 2 (left), is a low-cost mobile manipulator platform built for easy setup and benchmarking robot learning research. It consists of a Trossen Widow X robotic arm [9] assembled with Dynamixel XM-430 and XL-430s servo motors. The arm has five degrees of freedom (DOFs) - with a working payload of 0.2 kg and a maximum reach of 0.55 m . The robot comes in two versions, with the arm rigidly mounted on a Kobuki mobile base [7]. The Kobuki base is about 0.12 m high with payload capacity of around 4.5 kg . For visual perception, an Intel Realsense D435 RGBD camera [6] is mounted with a pan-tilt attachment at a height of about 0.6 m above the ground. An automatic camera calibration routine is implemented in the software suite. LoCoBot also comes with a Intel NUC (i5, 8GB RAM) machine rigidly attached on the base, which could be used for on-board compute. Kobuki base is powered through its own battery that can run base for about 2 hours. We use a 185 Wh battery pack [3] to power the arm, pan-tilt mount, and the on-board computer. On a full charge, the complete system is able to run for 5060 minutes. LoCoBot-Lite, shown in Figure 2 (right), is a cheaper version of LoCoBot that uses the Create2 base [4] instead of the Kobuki base.</p>
<p>Sawyer: The Sawyer is a 7-DOF collaborative robot arm from Rethink Robotics [1]. PyRobot interfaces with the Intera SDK provided with the Sawyer.</p>
<p>Simulators: PyRobot currently supports Gazebo simulator [37], a 3D rigid body simulator popular in the robotics community. For LoCoBot and LoCoBot-Lite, PyRobot supports tight integration with Gazebo i.e., the same code can be run on both Gazebo and the real robot.</p>
<h2>4. PyRobot Controllers</h2>
<p>While a number of robots come with their own implementations for low-level control, PyRobot implements basic controllers for differential drive bases. It also interfaces</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: LoCoBot (left) and LoCoBot-Lite (right). Both robots have a 5 DOF arm mounted on top of a mobile base (Kobuki or Create2). Robots are equipped with a RGB-D camera mounted on a pan-tilt stand. Robots come with a battery pack and an on-board computer.
with planners such as MoveIt! [20] and Movebase [49]. We measure the performance of these controllers and planners implemented in PyRobot for the LoCoBot base and arm.</p>
<h3>4.1. Accuracy of Base Control</h3>
<p>PyRobot implements position controllers to command the robot base to a desired target position (parameterized as a 3-DOF pose, $(x, y)$ location of the base and its heading $\theta:[x, y, \theta])$. We implement the following three controllers: DWA Controller from Movebase: We implemented Dynamic Window Approach Controller (DWA) [27] for our robot through Movebase [49] navigation engine. In this approach, we repeatedly sample a discrete sequence in the robot's control space with the highest score and execute the sequence until the target is reached.
Proportional Controller: We decompose the motion into an on-spot rotation, linear motion and a final on-spot rotation at the target location. Each segment of this motion is</p>
<p>Table 1: Base position control performance for LoCoBot and LoCoBot-Lite. We report translation and rotation error for different motion types for the different controllers for base position control implemented in PyRobot. Lower errors are better.</p>
<table>
<thead>
<tr>
<th>Controllers</th>
<th>Error with respect to motion capture</th>
<th></th>
<th></th>
<th>Error with respect to odometry</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>ILQR</td>
<td>Proportional</td>
<td>Movebase</td>
<td>ILQR</td>
<td>Proportional</td>
<td>Movebase</td>
</tr>
<tr>
<td>LoCoBot</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Linear motion</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Translation (mm)</td>
<td>$17 \pm 5$</td>
<td>$46 \pm 23$</td>
<td>$89 \pm 16$</td>
<td>$3 \pm 1$</td>
<td>$41 \pm 32$</td>
<td>$102 \pm 2$</td>
</tr>
<tr>
<td>Rotation (deg)</td>
<td>$0.43 \pm 0.25$</td>
<td>$1.77 \pm 1.46$</td>
<td>$10.81 \pm 2.19$</td>
<td>$0.12 \pm 0.10$</td>
<td>$1.65 \pm 1.37$</td>
<td>$10.63 \pm 2.19$</td>
</tr>
<tr>
<td>Rotation motion</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Translation (mm)</td>
<td>$6 \pm 0$</td>
<td>$6 \pm 4$</td>
<td>$4 \pm 2$</td>
<td>$0 \pm 0$</td>
<td>$5 \pm 1$</td>
<td>$2 \pm 1$</td>
</tr>
<tr>
<td>Rotation (deg)</td>
<td>$1.32 \pm 0.68$</td>
<td>$2.48 \pm 0.98$</td>
<td>$12.53 \pm 1.09$</td>
<td>$1.45 \pm 0.24$</td>
<td>$2.54 \pm 1.02$</td>
<td>$13.08 \pm 1.18$</td>
</tr>
<tr>
<td>Combined motion</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Translation (mm)</td>
<td>$16 \pm 2$</td>
<td>$65 \pm 52$</td>
<td>$78 \pm 2$</td>
<td>$6 \pm 1$</td>
<td>$55 \pm 50$</td>
<td>$87 \pm 15$</td>
</tr>
<tr>
<td>Rotation (deg)</td>
<td>$0.29 \pm 0.19$</td>
<td>$3.2 \pm 2.69$</td>
<td>$11.59 \pm 1.3$</td>
<td>$0.84 \pm 0.20$</td>
<td>$2.35 \pm 2.94$</td>
<td>$11.65 \pm 1.63$</td>
</tr>
<tr>
<td>LoCoBot-Lite</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Linear motion</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Translation (mm)</td>
<td>$144 \pm 8$</td>
<td>$142 \pm 7$</td>
<td>$260 \pm 81$</td>
<td>$9 \pm 5$</td>
<td>$34 \pm 5$</td>
<td>$99 \pm 31$</td>
</tr>
<tr>
<td>Rotation (deg)</td>
<td>$1.79 \pm 1.59$</td>
<td>$2.82 \pm 0.52$</td>
<td>$7.34 \pm 8.19$</td>
<td>$1.6 \pm 1.5$</td>
<td>$1.61 \pm 0.34$</td>
<td>$5.21 \pm 3.13$</td>
</tr>
<tr>
<td>Rotation motion</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Translation (mm)</td>
<td>$3 \pm 2$</td>
<td>$3 \pm 2$</td>
<td>$3 \pm 1$</td>
<td>$2 \pm 2$</td>
<td>$3 \pm 3$</td>
<td>$3 \pm 1$</td>
</tr>
<tr>
<td>Rotation (deg)</td>
<td>$6.97 \pm 1.71$</td>
<td>$3.07 \pm 3.47$</td>
<td>$9.94 \pm 1.46$</td>
<td>$1.44 \pm 1.12$</td>
<td>$4.59 \pm 2.78$</td>
<td>$3.42 \pm 1.66$</td>
</tr>
<tr>
<td>Combined motion</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Translation (mm)</td>
<td>$123 \pm 7$</td>
<td>$99 \pm 4$</td>
<td>$230 \pm 57$</td>
<td>$5 \pm 6$</td>
<td>$93 \pm 19$</td>
<td>$93 \pm 21$</td>
</tr>
<tr>
<td>Rotation (deg)</td>
<td>$2.8 \pm 1.68$</td>
<td>$1.19 \pm 0.95$</td>
<td>$5.87 \pm 8.22$</td>
<td>$2.57 \pm 1.31$</td>
<td>$1.57 \pm 1.15$</td>
<td>$4.18 \pm 3.45$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: LoCoBot is low-cost and hence scalable. executed using a proportional controller that applies velocities proportional to the tracking error. For smooth motion, we bound the velocities and the change in velocities. Linear Quadratic Regulator: We analytically compute a trajectory (a sharp one that breaks the motion into on-spot rotation, straight motion and a final on-spot rotation; or a smooth one by fitting a bézier curve between the stating state and the ending state). We sample this trajectory to obtain a state trajectory using constraints on maximum linear and angular velocities. We linearize the dynamics of the robot (assumed to be a bicycle model [16]) around this state trajectory, and construct a LQR feedback controller [16] to track this state trajectory.</p>
<p>We conducted trials on the robot to quantify the accuracy of each of these different position controllers on both LoCoBot and LoCoBot-Lite. We measured accuracy using the difference in commanded state $v s$. the achieved state as measured using a Vicon motion capture system. The error was factored into translation (difference in $(x, y)$ location), and rotation (difference in the heading $\theta$ ). We report these errors in Table 1. We group trials into the following three categories: a) Linear motion: 5 trials each with targets 2 m in front ( $[2,0,0]$ ), or 2 m behind ( $[-2,0,0]$ ); b) On-spot rotation: 5 trials each with target being left rotation by $\pi / 2$ ( $[0,0, \pi / 2]$ ), right rotation by $\pi / 2([0,0,-\pi / 2])$; c) Combined linear and rotation motion: 5 trials each with targets $[1,1,0]$ and $[-1,-1,0]$.</p>
<p>Table 1 reports translation and rotation errors for the different controllers for the two robots for these different cases. We generally note that errors are lower for LoCoBot $v s$. LoCoBot-Lite. Additionally, LQR and proportional controller generally perform better than the DWA controller from Movebase. As all these controllers close the loop on</p>
<p>Table 2: Locobot Arm Pose Repeatability</p>
<p>| Std Dev.(mm) | Poses | | | | |
| | 1 | 2 | 3 | 4 | Home |
| --- | --- | --- | --- | --- | --- |
| x | 0.12 | 0.13 | 0.07 | 0.11 | 0.15 |
| y | 0.13 | 0.07 | 0.10 | 0.14 | 0.27 |
| z | 0.21 | 0.33 | 0.22 | 0.31 | 0.24 |
| Repeatability (mm) | 0.41 | 0.58 | 0.33 | 0.50 | 0.52 |</p>
<p>the base odometry, we additionally include errors with respect to base odometry in right part of the table. We observe that the LQR controller is more effective at closing the loop.</p>
<p>PyRobot also implements trajectory tracking (using feedback controllers as described above). We show qualitative comparisons between different controllers in Figure 4.</p>
<h3>4.2. Repeatability Tests for Manipulator</h3>
<p>Compared to expensive industrial and collaborative robots, low-cost manipulators like LoCoBot suffer from control errors that can be attributed to a range of factors: manufacturing and assembling error, gear backlash, hardware execution error, kinematics inaccuracy, hand-eye calibration error, motor wear and tear, etc. The position-control repeatability was analyzed by commanding the arm to 4 different 3D poses (and the home pose) in a 2D grid at a fixed height without carrying a payload for a total of 10 repetitions per pose. The ground truth positions were measured using a Vicon motion capture system at 120 Hz . The arm always started at the home pose (when the joint angles are all 0 ) before moving to the commanded end pose. The results are summarized in Table 2. Overall, the arm had a repeatability error of 0.33 mm to 0.58 mm , computed based on ISO9283 standard. Poses 1 and 3 were closer to the robot torso and had lower error compared to Pose 2 and 4 where the arms were extended at the extremities of the workspace. The standard deviation along the z axis was also higher across all poses due to gravity. For comparison, the Sawyer and UR5 robots are reported to have a repeatability of $0.1 \mathrm{~mm}[1,2]$. The position control in the initial release only relies on proprioceptive feedback, and using feedforward model-based control in future release could reduce the error further. The PID gain settings are exposed to the user for more specialized robot or task-specific tuning.</p>
<h2>5. High-Level AI Applications</h2>
<p>We discuss implementation of a few example high-level AI applications through the PyRobot API.</p>
<h3>5.1. Visual SLAM</h3>
<p>Visual SLAM algorithms provide more accurate odometry as compared to odometry that is derived purely from inertial sensors on the base. We deployed ORB-SLAM2 [51],</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative comparisons for trajectory tacking for LoCoBot and LoCoBot-Lite. Reference trajectory (a circle of radius 0.4 m ) is shown in red.
a leading visual SLAM systems in the PyRobot library. ORB-SLAM2 is a feature-based indirect visual SLAM system that uses ORB features to perform tracking, mapping, and loop closing. We adapt the open-source ORB-SLAM2 code into a ROS package. This package saves RGB and depth images of the keyframes and continuously publishes camera trajectory and camera pose. PyRobot uses this published pose information to return the robot base state and trajectory. This state derived from visual SLAM can be used in downstream controllers or algorithms for more accurate behavior. PyRobot also supports dense map reconstruction, by integrating depth image observations using the ORB-SLAM2 estimated camera pose. This can be used for motion planning for navigation tasks.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An example of Navigation via SLAM and Path Planning. First row corresponds to the 2-D map constructed using the on-board SLAM and the second row corresponds to the actual motion of the robot.</p>
<h2>5.2. Navigation via SLAM and Path Planning</h2>
<p>We deployed Movebase [49] ROS package on LoCoBot and LoCoBot-Lite for safe navigation in environments with obstacles. We use the occupancy map as obtained from visual SLAM, to compute a 2D cost-map that denotes regions of the environment where the robot is safe to move. Movebase uses this cost-map to generate collision free trajectories to goals specified in the environment. These trajectories can be executed using any of the controllers implemented in PyRobot. These steps are run continuously, and the plan is updated if it becomes infeasible as the robot perceives previously unseen parts of the environment.</p>
<h2>5.3. Learned Visual Navigation</h2>
<p>We deploy learned policies for visual navigation on LoCoBot using PyRobot API. We work with the cognitive mapping and planning policy (CMP) from Gupta <em>et al.</em> [32]. Given an input goal location, CMP policy takes in the current image from the on-board camera to output one of four macro-actions (stop, turn left, turn right or go straight). We use the base position control interface in PyRobot API to execute these actions. Listing 2 shows simplified code, and Figure 6 shows frames from a sample execution.</p>
<h2>5.4. Grasping</h2>
<p>We deploy a learned-based grasping algorithm to grasp objects placed on the ground from RGB images using the PyRobot API. The model is trained on data from people's homes [31] and is robust to a wide variety of objects and backgrounds. This model outputs a grasp in the image space. This grasp is parameterized by 2D location in the image and the gripper orientation. We convert this 2D location and orientation into the <em>grasp position</em> (3D location and orientation) using known camera parameters, and the depth image. We command the robot to the <em>pre-grasp location</em>, that is a few centimeter above the grasp position, lower the arm to reach the object, and close the gripper to grasp the object. Listing 3 shows simplified code, and Figure 7 shows sample grasps using the LoCoBot.</p>
<h2>5.5. Pushing</h2>
<p>We deploy a heuristic-based pushing algorithm using PyRobot. It relies on the depth sensor, and thus the quality of the pushing depends on how well the stereo-based depth sensor behaves in different background. To achieve the best performance, it is best to place the robot on a floor with non-uniform texture.</p>
<p>The algorithm can be summarized with the following steps: (1) Move the arm out of the camera's field of view. (2) Filter the point cloud seen by the RGBD camera, specifically removing points too far away and those that correspond to the floor by coordinate thresholding. (3) Project the remaining point cloud onto the xy-plane and use DBSCAN [24] algorithm to automatically cluster the projected points. (4) Randomly select one cluster and choose a random push-start point on the enclosing bounding box of the cluster. (5) Move the gripper to the push-start point and move the gripper horizontally towards the center of the cluster. Listing 4 shows simplified code.</p>
<h2>6. Related Work</h2>
<p><strong>Robotics Software Design.</strong> The robotics community has embraced a layered hierarchical software design from the early days [17] and re-usability has been a core design principle [45]. We refer readers to Tsardoulias and Mitkas [61] for a comprehensive review. There have been several motion planning libraries such as OpenRave [23], MoveIt! [20], OMPL [59] which provide hardware-agnostic core functionalities that can be compiled for each specific robot. In the likes of ROS, there have also been robotics ecosystems, such as OROCOS [18] and the Microsoft Robotics Studio that support kinematic libraries, distributed processes, state machines for the real time control of robots.</p>
<p><strong>Low-cost Mobile Manipulators.</strong> There has been very limited research on learning on low-cost robots, given that most researchers use standard industrial or collaborative</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Snapshots from a run of visual navigation policy (CMP [32]) deployed on LoCoBot. See project website for videos.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Grasps selected by the grasp model and execution by the robot.</p>
<p>robots. Deisenroth <em>et al.</em> [22] used model-based RL to teach a cheap inaccurate 6 DOF robot to stack multiple blocks and a previous iteration of LoCoBot was used in Gupta <em>et al.</em> [31] to learn visual grasping policies with real data collected in people's homes. Recently, Gealy <em>et al.</em> [28] proposed a compliant low-cost arm using quasi-direct drive ac- tuation.</p>
<p>Open Source Manipulators. There has been very lim- ited work in open sourced manipulators. Raven is a open ar- chitecture surgical research robot [57]. Recently, the Open Manipulator project from Robotis allows one to build their own low cost robot with custom kinematics and design [8].</p>
<p>Research Ecosystems in AI Fields. Research in a num- ber of AI fields has benefited from there being common tasks (such as object detection in computer vision or pars- ing in NLP), common datasets (such as BSDS [50], Im- ageNet [58], PASCAL VOC [25] and MSCOCO [41] in computer vision, or Penn Tree Bank [48], GLUE [62], SentEval [21] and WMT in NLP, <em>etc</em>.), and common code bases to experiment with (DPMs [29], Caffe [35], Stanford CoreNLP [46], spaCy [33], <em>etc</em>.). While some people argue that such use of common tasks and datasets can prevent cre- ative progress, at the same time, it has lead to rapid progress in these fields, as researchers can quickly replicate results and build upon each other work.</p>
<p>Benchmarking in Robotics. Benchmarking in robotics is extremely challenging given the vast scope of applica- tions and diversity of physical test conditions (hardware, objects, environment, etc.). It is a well acknowledged con- cern within the robotics community that we are yet to de- velop reliable benchmarking metrics that can be widely adopted to quantify research progress. Several workshops have tried to stimulate discourse towards this end [11, 12] and different task specific metrics have been proposed for grasping [42], gripper design [38], SLAM [12], etc. Re- search has also benefited from creating object datasets with shape and grasp information, such as the Columbia Grasp Database [30], DexNet [44] and KIT Object Models [36], which could be used for perception and motion planning. The YCB dataset went a step further by distributing a phys- ical dataset of household and kitchen objects with corre- sponding meta data (shape, RGBD scans, etc) [19]. While there is no consensus yet on benchmarking in robotics, we hope that the combination of PyRobot and LoCoBot will facilitate further discussion.</p>
<ol>
<li>Discussion</li>
</ol>
<p>In this paper, we describe the PyRobot framework, which provides a high-level hardware independent API to control different robots. We believe PyRobot when com- bined with low-cost robots such as LoCoBot, will reduce the barrier to entry into robotics. In the immediate future, we will continue to grow the functionality in PyRobot such as by interfacing with simulators (like AI Habitat [47], Gib- son [63] and MuJoCo [60]), improving controllers such as be implementing gravity compensation for LoCoBot. But more broadly, we believe PyRobot will lead to the develop-</p>
<p>ment of a research and teaching ecosystem.
PyRobot for robotics instruction. Having a beginnerfriendly and open architecture is great for robotics education, as affordable robotic setups with LoCoBot and PyRobot could easily be assembled and scaled for hands-on instruction. 10 LoCoBots were used in the Spring 2019 offering of 16-662: Robot Autonomy (by Professor Oliver Kroemer) in the Robotics Institute at CMU, to support homework assignments and projects. We believe many more such courses will follow.</p>
<p>PyRobot as a research ecosystem. Compared to other fields, benchmarking in robotics is challenging due to several reasons. PyRobot's unified API and LoCoBot's standard hardware, will allow researchers to share their high level algorithmic implementations, models and datasets collected on a real robot. This will allow researchers to collaborate and iterate faster on robotics applications. We will continue to expand the set of pre-trained models. Hopefully, other researchers will find the PyRobot framework useful and contribute their models for others to use as well.</p>
<h2>8. Acknowledgements</h2>
<p>We would like to thank Soumith Chintala for countless discussions and providing software engineering guidance. We would also like to thank Deepak Pathak and Shubham Tulsiani for testing, advice and discussions. Finally, we would like to thank Oliver Kroemer, Timothy Lee and Mohit Sharma for introducing LoCoBots in teaching 16-662: Robot Autonomy at CMU, and Justin MacEy for helping with the motion capture experiments.</p>
<h2>References</h2>
<p>[1] Sawyer technical specifications. https://www. rethinkrobotics.com/sawyer/tech-specs/, 2016. 3, 5
[2] Ur5 technical specifications. https://www. universal-robots.com/media/50588/ur5_en.pdf, 2016. 2, 5
[3] Battery pack. https://www.maxoak.net/ laptop-power-bank/show/11.html, 2018. 3
[4] Create2 mobile base. https://www.irobot.com/ about-irobot/stem/create-2, 2018. 3
[5] Franka emika specification. https://www.franka.de/, 2018. 2
[6] Intel realsense d435 rgbd camera. https://click.intel. com/intelr-realsensetm-depth-camera-d435.html, 2018. 3
[7] Kobuki mobile base. http://kobuki.yujinrobot.com/ about2/, 2018. 3
[8] Open manipulator project. http://emanual.robotis. com/docs/en/platform/openmanipulator_x/overview/, 2018. 1, 7
[9] Trossen widow x robotic arm. https://www.trossenrobotics.com/ widowx-200-robot-arm-mobile-base.aspx, 2018. 3
[10] YACS - yet another configuration system. https: //github.com/rbgirshick/yacs, 2018. 2
[11] ICRA workshop on benchmarks for robotic manipulation. http://www.ycbbenchmarks.com/ICRA2019_ workshop, 2019. 7
[12] ICRA workshop on dataset generation and benchmarking of slam algorithms for robotics and vr/ar. https://sites.google.com/view/icra-2019-workshop/ home, 2019. 7
[13] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org. 1
[14] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Advances in Neural Information Processing Systems, pages 5074-5082, 2016. 1
[15] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, J. T. Szymon Sidor, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018. 1
[16] K. J. Åström. Introduction to stochastic control theory. Courier Corporation, 2012. 4
[17] R. Brooks. A robust layered control system for a mobile robot. AI Memo 864, 1985. 6
[18] H. Bruyninckx. Open robot control software: the orocos project. In International Conference on Robotics and Automation (ICRA). IEEE, 2001. 6
[19] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. Dollar. Benchmarking in manipulation research: Using the yale-cmu-berkeley object and model set. IEEE Robotics \&amp; Automation Magazine, 1070(9932/15):36, 2015. 7
[20] S. Chitta, I. Sucan, and S. Cousins. Moveit![ros topics]. IEEE Robotics \&amp; Automation Magazine, 19(1):18-19, 2012. 3, 6</p>
<p>[21] A. Conneau and D. Kiela. Senteval: An evaluation toolkit for universal sentence representations. arXiv preprint arXiv:1803.05449, 2018. 7
[22] M. P. Deisenroth, C. E. Rasmussen, and D. Fox. Learning to control a low-cost manipulator using dataefficient reinforcement learning. RSS, 2011. 6
[23] R. Diankov and J. Kuffner. Openrave: A planning architecture for autonomous robotics. Robotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RI-TR-08-34, 2008. 6
[24] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. Densitybased spatial clustering of applications with noise. In Int. Conf. Knowledge Discovery and Data Mining, volume 240, 1996. 6
[25] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1):98-136, Jan. 2015. 7
[26] C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017. 1
[27] D. Fox, W. Burgard, and S. Thrun. The dynamic window approach to collision avoidance. IEEE Robotics \&amp; Automation Magazine, 4(1):23-33, 1997. 3
[28] D. Gealy, S. McKinley, B. Yi, P. Wu, P. Downey, G. Balke, A. Zhao, M. Guo, R. Thomasson, A. Sinclair, P. Cuellar, Z. McCarthy, and P. Abbeel. Quasidirect drive for low-cost compliant robotic manipulation. In International Conference on Robotics and Automation (ICRA). IEEE, 2019. 1, 7
[29] R. B. Girshick, P. F. Felzenszwalb, and D. McAllester. Discriminatively trained deformable part models, release 5. http://people.cs.uchicago.edu/ rbg/latentrelease5/. 7
[30] C. Goldfeder, M. Ciocarlie, H. Dang, and P. Allen. The columbia grasp database. In International Conference on Robotics and Automation (ICRA), pages 1710-1716. IEEE, 2009. 7
[31] A. Gupta, A. Murali, D. Gandhi, and L. Pinto. Robot learning in homes: Improving generalization and reducing dataset bias. In Advances in neural information processing systems, 2018. 1, 6, 7
[32] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 6, 7
[33] M. Honnibal and I. Montani. spacy 2: Natural language understanding with bloom embeddings, convo-
lutional neural networks and incremental parsing. To appear, 2017. 7
[34] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learning agile and dynamic motor skills for legged robots. In Science Robotics, volume 4, 2019. 1
[35] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014. 1, 7
[36] A. Kasper, Z. Xue, and R. Dillman. The kit object models database: An object model database for object recognition, localization and manipulation in service robotics. IJRR, 2012. 7
[37] N. Koenig and A. Howard. Design and use paradigms for gazebo, an open-source multi-robot simulator. In 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566), volume 3, pages 2149-2154. IEEE, 2004. 3
[38] G. Kragten, A. Kool, and J. Herder. Ability to hold grasped objects by underactuated hands: Performance prediction and experiments. In International Conference on Robotics and Automation (ICRA), pages 2493- 2498. IEEE, 2009. 7
[39] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-toend training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016. 1
[40] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International Journal of Robotics Research, 37(4-5):421-436, 2018. 1
[41] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 1, 7
[42] J. Mahler et al. Guest editorial open discussion of robot grasping benchmarks, protocols, and metrics. In Transactions on Automation Science and Engineering, volume 15. IEEE, 2018. 7
[43] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. Robotics Science and Systems (RSS), 2017. 1
[44] J. Mahler, F. T. Pokorny, B. Hou, M. Roderick, M. Laskey, M. Aubry, K. Kohlhoff, T. Krger,</p>
<p>J. Kuffner, and K. Goldberg. Dex-net 1.0: A cloudbased network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards. International Conference on Robotics and Automation (ICRA), 2016. 7
[45] A. Makarenko, A. Brooks, and T. Kaupp. On the benefits of making robotic software frameworks thin. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2007. 6
[46] C. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and D. McClosky. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55-60, 2014. 7
[47] Manolis Savva<em>, Abhishek Kadian</em>, Oleksandr Maksymets*, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A platform for embodied ai research. arXiv preprint arXiv:1904.01201, 2019. 2, 7
[48] M. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. 1993. 7
[49] E. Marder-Eppstein. move base, a ros package that lets you move a robot to desired positions using the navigation stack. 3, 6
[50] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int'l Conf. Computer Vision, volume 2, pages 416-423, July 2001. 7
[51] R. Mur-Artal and J. D. Tardós. ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Transactions on Robotics, 33(5):1255-1262, 2017. 5
[52] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017. 1
[53] L. Pinto, J. Davidson, and A. Gupta. Supervision via competition: Robot adversaries for learning tasks. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 1601-1608. IEEE, 2017. 1
[54] L. Pinto, D. Gandhi, Y. Han, Y.-L. Park, and A. Gupta. The curious robot: Learning visual representations via physical interactions. In European Conference on Computer Vision, pages 3-18. Springer, 2016. 1
[55] L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In 2016 IEEE international conference on robotics
and automation (ICRA), pages 3406-3413. IEEE, 2016. 1
[56] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng. ROS: an opensource robot operating system. In ICRA workshop on open source software, volume 3, page 5. Kobe, Japan, 2009. 1, 2
[57] J. Rosen, D. Friedman, H. King, P. Roan, L. Cheng, D. Glozman, J. Ma, S. Kosari, and L. White. Ravenii: An open platform for surgical robotics research. In Transactions on Biomedical Engineering. IEEE, 2012. 7
[58] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015. 7
[59] I. Sucan, M. Moll, and L. Kavraki. The open motion planning library. IEEE Robotics \&amp; Automation Magazine, 19(4), 2012. 6
[60] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012. 2, 7
[61] E. Tsardoulias and P. Mitkas. Robotic frameworks, architectures and middleware comparison. arXiv preprint arXiv:1711.06842, 2017. 6
[62] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR. 7
[63] F. Xia, A. R. Zamir, Z.-Y. He, A. Sax, J. Malik, and S. Savarese. Gibson env: real-world perception for embodied agents. In CVPR, 2018. 7
[64] B. Yang, J. Zhang, V. Pong, S. Levine, and D. Jayaraman. Replab: A reproducible low-cost arm benchmark platform for robotic learning. arXiv preprint arXiv:1905.07447, 2019. 1
[65] H. Zhu, A. Gupta, A. Rajeswaran, S. Levine, and V. Kumar. Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost. arXiv preprint arXiv:1810.06045, 2018. 1</p>
<h2>A. Code Listings</h2>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">pyrobot</span><span class="w"> </span><span class="kn">import</span> <span class="n">Robot</span>
<span class="c1"># Construct Robot.</span>
<span class="n">bot</span> <span class="o">=</span> <span class="n">Robot</span><span class="p">(</span><span class="s1">&#39;locobot&#39;</span><span class="p">)</span>
<span class="c1"># Construct policy.</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">CMP</span><span class="p">()</span>
<span class="c1"># Relative position for each action.</span>
<span class="n">dv</span> <span class="o">=</span> <span class="mf">0.4</span> <span class="c1"># Forward step size</span>
<span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mf">2.</span> <span class="c1"># Rotation step size</span>
<span class="n">action_position</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="n">dw</span><span class="p">],</span>
<span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="o">+</span><span class="n">dw</span><span class="p">],</span>
<span class="p">[</span><span class="n">dv</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]]</span>
<span class="c1"># Set goal for policy.</span>
<span class="n">policy</span><span class="o">.</span><span class="n">set_new_goal</span> <span class="p">(</span><span class="n">goal</span><span class="p">)</span>
<span class="k">while</span> <span class="n">action</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># Get image.</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">bot</span><span class="o">.</span><span class="n">camera</span><span class="o">.</span><span class="n">get_rgb</span><span class="p">()</span>
    <span class="c1"># Compute action.</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_action</span><span class="p">(</span><span class="n">rgb</span><span class="p">)</span>
    <span class="c1"># Execute action.</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">action_position</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
    <span class="n">bot</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">go_to_relative</span> <span class="p">(</span><span class="n">position</span><span class="p">)</span>
</code></pre></div>

<p>Listing 2: Visual navigation example using PyRobot API.
from pyrobot import Robot
# Construct Robot.
bot $=$ Robot('locobot')
# Set pregrasp and grasp height.
pregrasp_height $=0.2$
grasp_height $=0.13$
# Construct grasp model.
model = GraspModel()
# Move arm and camera to reset position.
reset_pos $=[-1.5,0.5,0.3,-0.7,0.]$
bot.arm.set_joint_positions (reset_pos)
bot.camera.set_pan_tilt $(0.0,0.8)$
# Get image.
rgb = bot.camera.get_rgb()
# Compute action.
grasp_img = model.compute_grasp(rgb)
# Convert grasp from Image space to
# robot workspace.
grasp_pose = cvt_space (grasp_img)
# Execute grasp.
# 1. Go to pre-grasp pose
pregrasp_position = [grasp_pose[0], grasp_pose[1], pregrasp_height]
grasp_angle = grasp_pose[2]
bot.arm.set_ee_pose_pitch_roll( position=pregrasp_position, pitch=np.pi / 2,
roll=grasp_angle,
plan=False,
numerical=False)
# 2. Go to grasp pose.
grasp_position = [grasp_pose[0], grasp_pose[1], grasp_height]
bot.arm.set_ee_pose_pitch_roll( position=grasp_position, pitch=np.pi / 2,
roll=grasp_angle,
plan=False,
numerical=False)
# 3. Grasp the object
bot.gripper.close()
Listing 3: Grasping example using PyRobot API.</p>
<h1>from pyrobot import Robot</h1>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="n">Construct</span><span class="w"> </span><span class="n">Robot</span><span class="o">.</span>
<span class="n">bot</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Robot</span><span class="p">(</span><span class="err">&#39;</span><span class="n">locobot</span><span class="err">&#39;</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Setup</span><span class="w"> </span><span class="n">gripper</span><span class="p">,</span><span class="w"> </span><span class="nb">camera</span><span class="p">,</span><span class="w"> </span><span class="n">arm</span><span class="o">.</span>
<span class="n">bot</span><span class="o">.</span><span class="n">gripper</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">bot</span><span class="o">.</span><span class="nb">camera</span><span class="o">.</span><span class="n">set_pan_tilt</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7</span><span class="p">,</span><span class="w"> </span><span class="n">wait</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Move</span><span class="w"> </span><span class="n">hand</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="nb">camera</span><span class="w"> </span><span class="n">view</span><span class="o">.</span>
<span class="n">ov_pos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mf">1.96</span><span class="p">,</span><span class="w"> </span><span class="mf">0.52</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">0.51</span><span class="p">,</span><span class="w"> </span><span class="mf">1.67</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">]</span>
<span class="n">bot</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">set_joint_positions</span><span class="p">(</span><span class="n">ov_pos</span><span class="p">,</span><span class="w"> </span><span class="n">plan</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Get</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="n">cloud</span><span class="p">(</span><span class="n">in</span><span class="w"> </span><span class="n">base</span><span class="w"> </span><span class="n">frame</span><span class="p">)</span><span class="o">.</span>
<span class="n">pts</span><span class="p">,</span><span class="w"> </span><span class="n">colors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bot</span><span class="o">.</span><span class="nb">camera</span><span class="o">.</span><span class="n">get_current_pcd</span><span class="p">(</span>
<span class="w">    </span><span class="n">in_cam</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Compute</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="k">location</span><span class="p">,</span><span class="w"> </span><span class="k">direction</span><span class="o">.</span>
<span class="n">pre_push_pt</span><span class="p">,</span><span class="w"> </span><span class="n">push_pt</span><span class="p">,</span><span class="w"> </span><span class="n">obj_center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span>
<span class="w">    </span><span class="n">get_push_direction</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span><span class="w"> </span><span class="n">colors</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gripper</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">pre</span><span class="o">-</span><span class="n">pushing</span><span class="w"> </span><span class="n">pose</span>
<span class="n">bot</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">set_ee_pose_pitch_roll</span><span class="p">(</span>
<span class="w">    </span><span class="n">position</span><span class="o">=</span><span class="n">pre_push_pt</span><span class="p">,</span>
<span class="w">    </span><span class="n">pitch</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="k">pi</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">    </span><span class="n">roll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="n">plan</span><span class="o">=</span><span class="n">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">numerical</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gripper</span><span class="w"> </span><span class="n">vertically</span><span class="w"> </span><span class="n">down</span><span class="o">.</span>
<span class="n">down_disp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">push_pt</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pre_push_pt</span>
<span class="n">bot</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">move_ee_xyz</span><span class="p">(</span><span class="n">down_disp</span><span class="p">,</span>
<span class="w">                                    </span><span class="n">plan</span><span class="o">=</span><span class="n">False</span><span class="p">,</span>
<span class="w">                                    </span><span class="n">numerical</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gripper</span><span class="w"> </span><span class="n">horizontally</span>
<span class="err">#</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nb">object</span><span class="o">.</span>
<span class="n">hor_disp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">obj_center</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">push_pt</span><span class="p">)</span>
<span class="n">bot</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">move_ee_xyz</span><span class="w"> </span><span class="p">(</span><span class="n">hor_disp</span><span class="p">,</span>
<span class="w">                                    </span><span class="n">plan</span><span class="o">=</span><span class="n">False</span><span class="p">,</span>
<span class="w">                                    </span><span class="n">numerical</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>
</code></pre></div>

<p>Listing 4: Object pushing example using PyRobot API.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*The first four authors contributed equally to this paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
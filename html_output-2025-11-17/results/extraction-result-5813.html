<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5813 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5813</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5813</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-28692beece311a90f5fa1ca2ec9d0c2ce293d069</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28692beece311a90f5fa1ca2ec9d0c2ce293d069" target="_blank">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a></p>
                <p><strong>Paper Venue:</strong> ACM Computing Surveys</p>
                <p><strong>Paper TL;DR:</strong> The basics of this promising paradigm in natural language processing are introduced, a unified set of mathematical notations that can cover a wide variety of existing work are described, and existing work is organized along several dimensions.</p>
                <p><strong>Paper Abstract:</strong> This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5813.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5813.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cloze vs Prefix suitability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cloze prompts versus prefix prompts: model-format suitability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey observation that the prompt shape (cloze vs prefix) interacts with the pre-training objective and model directionality: masked LMs (bidirectional/MLM) align better with cloze prompts for NLU tasks, while left-to-right autoregressive LMs align better with prefix prompts for generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>masked LMs (e.g., BERT) vs left-to-right LMs (e.g., GPT-family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLU (classification, probing) vs NLG (generation, translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLU tasks (classification, probing) often formulated as cloze (fill-in-the-blank) tasks; NLG tasks (translation, summarization) often formulated as continuation/prefix generation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Cloze prompts: templates with an internal [Z] slot (e.g., "[X] Overall, it was a [Z] movie."); Prefix prompts: input is entirely before the generated text (e.g., "English: X. French:").</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison between cloze and prefix formats (i.e., using same task with cloze vs prefix prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: masked LMs are reported as more suitable and effective for cloze-style NLU tasks; autoregressive LMs are more suitable for prefix-style generation tasks. (No single numerical metric reported in survey.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent: cloze improves performance for MLMs/NU tasks; prefix improves performance for L2R/generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because pre-training objectives differ: MLMs are trained to predict masked spans using surrounding context (matches cloze), while left-to-right LMs are trained to autoregressively continue prefixes (matches prefix). Thus the prompt shape that mirrors the pre-training task yields better alignment and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5813.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt wording sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLM performance to specific prompt wording and template choice</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey summarizes many findings that small changes in template wording, token choice, or prompt phrasing can lead to large changes in downstream performance, especially in tuning-free or few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various pre-trained LMs (BERT, RoBERTa, GPT-2/3, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fact probing, text classification, NLI and other tasks reformulated via prompts</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks reformulated as cloze or prefix prompts where the template words determine how the LM interprets the task.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Manual discrete templates (human-crafted prompts) with differing phrasing; examples include multiple cloze templates for the same classification label space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different manual templates or paraphrases of the same template compared against each other.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as highly variable: some prompts yield near state-of-the-art performance while others yield near-random results on the same task and model (qualitative claim from survey; no unified numeric metric given).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied; prompt wording can dramatically improve or degrade performance</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompts specify the task to the LM; if wording aligns well with the distributions and patterns the LM learned in pre-training, the LM can leverage that knowledge; if wording is misaligned, the LM fails to map the input to the intended output.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Survey notes situations where careful prompt engineering is required but also cites work (e.g., NullPrompt) showing in some cases very simple templates can be competitive, indicating not all tasks require complex wording.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5813.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated template discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated discrete and continuous template learning (AutoPrompt, mining, paraphrasing, soft prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey groups methods that automatically search for discrete templates (mining, paraphrasing, gradient token search) and methods that learn continuous soft prompts in embedding space; these methods can find prompts that outperform naive manual templates and reduce human effort.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (BERT/MLMs for discrete mining/search; T5 and GPT variants for generation and template generation; models used for continuous prompts as backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge probing, classification, generation; template search itself as an auxiliary task</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automatically discover prompt templates that maximize downstream performance on a target task or that mine frequent natural patterns from corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrete prompt mining/paraphrasing (search in token space) and continuous prompt tuning (prefix/prompt-tuning where vectors are optimized and need not correspond to natural tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison between manual templates and automatically discovered discrete templates; comparison between discrete initialization vs continuous fine-tuned soft prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports that automated methods often find better-performing prompts than manual ones; continuous soft prompts (prefix/prompt tuning) can yield strong few-shot/fine-tuned performance and sometimes outperform discrete prompts in low-data settings (qualitative summary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>automated discovery generally improves or stabilizes performance relative to naive manual prompts</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Automated search explores prompt space beyond human intuition and can discover token sequences or embedding vectors that better trigger the LM's internal capabilities; continuous prompts bypass constraints of natural language tokens allowing more direct optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Survey cites that continuous prompts can be more sensitive to initialization in low-data settings, and that manual templates still can be strong starting points (i.e., not always strictly superior).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5813.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context (few-shot) demonstration format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt augmentation / in-context learning: demonstration examples and their ordering/selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey highlights that adding answered prompt demonstrations (few-shot examples) before the test input (in-context learning) can substantially change LLM performance, and that both which examples are chosen and their order matter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Few-Shot Learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large autoregressive LMs (notably GPT-3 and similar L2R models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Many tasks via in-context learning (question answering, arithmetic, translation, classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide k answered exemplars (demonstrations) concatenated with the test prompt; model conditions on examples to produce output for the new input.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot demonstration prompts (e.g., 'Q: ... A: ...' repeated k times then new Q), with variations in sample selection and ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot (no demonstrations) vs few-shot with different exemplar selection/order strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports that exemplar selection and ordering can lead to large performance swings (ranges from near-SOTA to near-random depending on choices), with some retrieval-based selection improving results; no single numeric metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>can improve dramatically or degrade depending on selection/order</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Large LMs can adapt behavior from patterns in prompt demonstrations; the most informative and representative exemplars (and their presentation order) better convey the task distribution to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5813.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combining multiple prompts (uniform/weighted averaging, voting, distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey describes methods that ensemble multiple different prompts at inference (or via distillation) to yield more stable and often improved performance compared to using a single prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various PLMs (BERT, RoBERTa, GPT variants) used in ensemble studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification, factual probing, generation evaluation and other prompt-based tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple candidate templates applied to the same input; aggregation of predicted probabilities or labels produces final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt ensemble methods including uniform averaging of token probabilities, weighted averaging (weights learned or set by dev accuracy), majority voting for classification, and knowledge-distillation from an ensemble to a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single best-prompt baseline vs ensemble (uniform/weighted/distilled) approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey states ensembles alleviate sensitivity to prompt choice and can increase accuracy/stability; reported improvements are method- and task-dependent (qualitative statement; no single numeric summary in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (stability and often accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different prompts capture complementary modes of model behavior; ensembling averages out prompt-specific errors and reduces variance due to prompt choice.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5813.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continuous prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of prefix/soft (continuous) prompts to initialization and data regime</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey notes that continuous prompt methods (prefix-tuning, prompt-tuning, P-tuning) can be powerful but are sensitive to initialization and especially to low-data settings; initializing from discrete prompts can help.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>prefix-tuning / prompt-tuning applied to large LMs (GPT-2, BART, T5, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Generation and classification tasks in few-shot/fine-tuning settings</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Optimize continuous vectors prepended to model inputs (prefix) or special token embeddings while keeping LM parameters frozen (fixed-LM prompt tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Continuous soft prompts (virtual token embeddings) trained on task supervision; optionally initialized from discrete prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Discrete manual prompts or tuning entire LM vs continuous prompt tuning with different initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports continuous prompts often yield superior few-shot/fine-tuning performance compared to naive tuning-free prompting, but are more sensitive to initialization and random seed in low-data regimes (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>often improved (but variable depending on init/data)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Continuous prompts are free of natural-language constraints and can more directly optimize the interface to the frozen LM, but because they live in embedding space their optimization landscape can be sensitive to initialization and small data.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Survey reports that seeding continuous prompts with tokens derived from discrete prompts can stabilize and improve performance, indicating pure random init can be problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5813.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer engineering / verbalizers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Design of answer space (verbalizers) and its effect on classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey explains that the mapping from LM-generated tokens/spans to target labels (verbalizers) and constraints on the answer space strongly affect classification performance under prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>masked and autoregressive LMs used for classification probes (BERT, RoBERTa, GPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text classification, NLI, NER, relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Define permissible answer set Z (single tokens, spans, sentences) and map answer tokens to task labels (verbalizers).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Constrained answer spaces (manually chosen label words), paraphrase-expanded answer sets, or continuous learned answer embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different verbalizer choices (single token vs paraphrase set vs learned virtual token) compared on same task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports that carefully designed verbalizers and constrained answer spaces improve results in classification tasks; paraphrase expansion or search/pruning methods can further help. No single numeric metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved when answer engineering aligns token choices with model's pretraining distribution</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The LM's likelihoods over tokens/spans are leveraged directly; choosing label words that the LM naturally associates with a class (frequent, unambiguous tokens) yields better mapping to labels.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Survey notes that manual verbalizers can be suboptimal and automated search/paraphrasing sometimes required; also continuous answer tokens (virtual embeddings) have been explored but are less common.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5813.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Null / minimal prompt observation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NullPrompt and observations that minimal or empty templates can be competitive</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites work (Logan IV et al.) that found using a null/simple prompt (concatenate input and mask without template words) can achieve competitive performance in some cases, indicating heavy prompt engineering is not always necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa, ALBERT and similar fine-tuned PLMs (as discussed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text classification (few-shot/fine-tuning settings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Using a null prompt template such as "[X][Z]" versus crafted templates.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Null prompt (no natural-language template words) used with fixed-prompt LM tuning or LM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Null prompt vs manually engineered prompts of similar tuning strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports null prompt achieved competitive accuracy on certain tasks in cited work (qualitative; no universal numeric value reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no consistent improvement from complex prompts; sometimes simple/null prompt performed comparably</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In cases where model fine-tuning or answer engineering is sufficient, the added inductive bias of a natural-language template may be unnecessary; also indicates that prompt utility depends on task, model, and tuning strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5813.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt decomposition for structured outputs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposing holistic prompts into sub-prompts for structured tasks (e.g., NER, relation extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey reports that for tasks requiring multiple or structured predictions (sequence labeling, relation extraction), decomposing into multiple sub-prompts (span-level prompts or multi-step prompts) makes prompting tractable and can improve applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PLMs used for IE and tagging (BART, RoBERTa, BERT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Named entity recognition (NER), relation extraction, sequence labeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Break the input into candidate spans or subtasks and prompt model separately for each span/subtask (e.g., ask "What is the entity type of span S? [Z]").</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt decomposition: many span-level/instance-level prompts per input instead of a single global prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Holistic single prompt vs decomposed per-span prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey indicates decomposition makes the problem feasible and can produce strong results for span-level tasks; quantitative gains depend on downstream methods and are not unified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved applicability and often improved performance for structured tasks</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Holistic prompts may be intractable for combinatorial output spaces; decomposition reduces complexity and lets prompts target specific sub-decisions aligning with LM strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5813.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5813.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining objective and prompt alignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of pre-training objective (SLM/CTR/FTR/MLM) and attention mask directionality on prompt effectiveness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey emphasizes that the pre-training objective (standard LM, denoising/CTR, full reconstruction/FTR, masked LM) and attention masking patterns influence which prompt formats and tasks the model is best suited for.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>models categorized by training objective (L2R GPT-like, MLM BERT-like, prefix and encoder-decoder models such as T5/BART)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Wide range: NLU, NLG, translation, probing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Choice of LM objective affects suitability: L2R for prefix generation tasks, MLM for cloze/NLU tasks, prefix/FTR models are versatile across both.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Match prompt shape (cloze/prefix) to pre-training objective and mask pattern; e.g., use cloze with MLMs, prefix for autoregressive models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Using same prompt format across models with different pretraining objectives vs matching prompt to objective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey-level qualitative claim: matching prompt format to pretraining objective improves performance and applicability; no unified numeric summary provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved when prompt shape aligns with pretraining objective</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Alignment reduces distributional shift between pretraining and downstream usage; e.g., cloze mirrors masked token prediction, prefix mirrors autoregressive continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>LAMA: LAnguage Model Analysis <em>(Rating: 2)</em></li>
                <li>Prefix-Tuning: Optimizing Continuous Prompts for Generation <em>(Rating: 2)</em></li>
                <li>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts <em>(Rating: 2)</em></li>
                <li>AutoPrompt (Shin et al.) / Mining prompts (Jiang et al.) <em>(Rating: 1)</em></li>
                <li>P-Tuning and P-Tuning v2 (or related continuous prompt tuning papers) <em>(Rating: 1)</em></li>
                <li>Gao et al. (2021) — on automatic prompt generation and demonstration learning <em>(Rating: 1)</em></li>
                <li>Lu et al. (2021) — on exemplar ordering and selection effects <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5813",
    "paper_id": "paper-28692beece311a90f5fa1ca2ec9d0c2ce293d069",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Cloze vs Prefix suitability",
            "name_full": "Cloze prompts versus prefix prompts: model-format suitability",
            "brief_description": "Survey observation that the prompt shape (cloze vs prefix) interacts with the pre-training objective and model directionality: masked LMs (bidirectional/MLM) align better with cloze prompts for NLU tasks, while left-to-right autoregressive LMs align better with prefix prompts for generation tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "masked LMs (e.g., BERT) vs left-to-right LMs (e.g., GPT-family)",
            "model_size": null,
            "task_name": "NLU (classification, probing) vs NLG (generation, translation)",
            "task_description": "NLU tasks (classification, probing) often formulated as cloze (fill-in-the-blank) tasks; NLG tasks (translation, summarization) often formulated as continuation/prefix generation.",
            "problem_format": "Cloze prompts: templates with an internal [Z] slot (e.g., \"[X] Overall, it was a [Z] movie.\"); Prefix prompts: input is entirely before the generated text (e.g., \"English: X. French:\").",
            "comparison_format": "Direct comparison between cloze and prefix formats (i.e., using same task with cloze vs prefix prompts).",
            "performance": "Qualitative: masked LMs are reported as more suitable and effective for cloze-style NLU tasks; autoregressive LMs are more suitable for prefix-style generation tasks. (No single numerical metric reported in survey.)",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "format-dependent: cloze improves performance for MLMs/NU tasks; prefix improves performance for L2R/generation tasks",
            "explanation_or_hypothesis": "Because pre-training objectives differ: MLMs are trained to predict masked spans using surrounding context (matches cloze), while left-to-right LMs are trained to autoregressively continue prefixes (matches prefix). Thus the prompt shape that mirrors the pre-training task yields better alignment and performance.",
            "counterexample_or_null_result": null,
            "uuid": "e5813.0",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Prompt wording sensitivity",
            "name_full": "Sensitivity of LLM performance to specific prompt wording and template choice",
            "brief_description": "The survey summarizes many findings that small changes in template wording, token choice, or prompt phrasing can lead to large changes in downstream performance, especially in tuning-free or few-shot settings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various pre-trained LMs (BERT, RoBERTa, GPT-2/3, etc.)",
            "model_size": null,
            "task_name": "Fact probing, text classification, NLI and other tasks reformulated via prompts",
            "task_description": "Tasks reformulated as cloze or prefix prompts where the template words determine how the LM interprets the task.",
            "problem_format": "Manual discrete templates (human-crafted prompts) with differing phrasing; examples include multiple cloze templates for the same classification label space.",
            "comparison_format": "Different manual templates or paraphrases of the same template compared against each other.",
            "performance": "Reported as highly variable: some prompts yield near state-of-the-art performance while others yield near-random results on the same task and model (qualitative claim from survey; no unified numeric metric given).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "varied; prompt wording can dramatically improve or degrade performance",
            "explanation_or_hypothesis": "Prompts specify the task to the LM; if wording aligns well with the distributions and patterns the LM learned in pre-training, the LM can leverage that knowledge; if wording is misaligned, the LM fails to map the input to the intended output.",
            "counterexample_or_null_result": "Survey notes situations where careful prompt engineering is required but also cites work (e.g., NullPrompt) showing in some cases very simple templates can be competitive, indicating not all tasks require complex wording.",
            "uuid": "e5813.1",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Automated template discovery",
            "name_full": "Automated discrete and continuous template learning (AutoPrompt, mining, paraphrasing, soft prompts)",
            "brief_description": "Survey groups methods that automatically search for discrete templates (mining, paraphrasing, gradient token search) and methods that learn continuous soft prompts in embedding space; these methods can find prompts that outperform naive manual templates and reduce human effort.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various (BERT/MLMs for discrete mining/search; T5 and GPT variants for generation and template generation; models used for continuous prompts as backbone)",
            "model_size": null,
            "task_name": "Knowledge probing, classification, generation; template search itself as an auxiliary task",
            "task_description": "Automatically discover prompt templates that maximize downstream performance on a target task or that mine frequent natural patterns from corpora.",
            "problem_format": "Discrete prompt mining/paraphrasing (search in token space) and continuous prompt tuning (prefix/prompt-tuning where vectors are optimized and need not correspond to natural tokens).",
            "comparison_format": "Comparison between manual templates and automatically discovered discrete templates; comparison between discrete initialization vs continuous fine-tuned soft prompts.",
            "performance": "Survey reports that automated methods often find better-performing prompts than manual ones; continuous soft prompts (prefix/prompt tuning) can yield strong few-shot/fine-tuned performance and sometimes outperform discrete prompts in low-data settings (qualitative summary).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "automated discovery generally improves or stabilizes performance relative to naive manual prompts",
            "explanation_or_hypothesis": "Automated search explores prompt space beyond human intuition and can discover token sequences or embedding vectors that better trigger the LM's internal capabilities; continuous prompts bypass constraints of natural language tokens allowing more direct optimization.",
            "counterexample_or_null_result": "Survey cites that continuous prompts can be more sensitive to initialization in low-data settings, and that manual templates still can be strong starting points (i.e., not always strictly superior).",
            "uuid": "e5813.2",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "In-context (few-shot) demonstration format",
            "name_full": "Prompt augmentation / in-context learning: demonstration examples and their ordering/selection",
            "brief_description": "Survey highlights that adding answered prompt demonstrations (few-shot examples) before the test input (in-context learning) can substantially change LLM performance, and that both which examples are chosen and their order matter.",
            "citation_title": "Language Models are Few-Shot Learners",
            "mention_or_use": "mention",
            "model_name": "large autoregressive LMs (notably GPT-3 and similar L2R models)",
            "model_size": null,
            "task_name": "Many tasks via in-context learning (question answering, arithmetic, translation, classification)",
            "task_description": "Provide k answered exemplars (demonstrations) concatenated with the test prompt; model conditions on examples to produce output for the new input.",
            "problem_format": "Few-shot demonstration prompts (e.g., 'Q: ... A: ...' repeated k times then new Q), with variations in sample selection and ordering.",
            "comparison_format": "Zero-shot (no demonstrations) vs few-shot with different exemplar selection/order strategies.",
            "performance": "Survey reports that exemplar selection and ordering can lead to large performance swings (ranges from near-SOTA to near-random depending on choices), with some retrieval-based selection improving results; no single numeric metric provided.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "can improve dramatically or degrade depending on selection/order",
            "explanation_or_hypothesis": "Large LMs can adapt behavior from patterns in prompt demonstrations; the most informative and representative exemplars (and their presentation order) better convey the task distribution to the model.",
            "counterexample_or_null_result": null,
            "uuid": "e5813.3",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Prompt ensembling",
            "name_full": "Combining multiple prompts (uniform/weighted averaging, voting, distillation)",
            "brief_description": "Survey describes methods that ensemble multiple different prompts at inference (or via distillation) to yield more stable and often improved performance compared to using a single prompt.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various PLMs (BERT, RoBERTa, GPT variants) used in ensemble studies",
            "model_size": null,
            "task_name": "Classification, factual probing, generation evaluation and other prompt-based tasks",
            "task_description": "Multiple candidate templates applied to the same input; aggregation of predicted probabilities or labels produces final prediction.",
            "problem_format": "Prompt ensemble methods including uniform averaging of token probabilities, weighted averaging (weights learned or set by dev accuracy), majority voting for classification, and knowledge-distillation from an ensemble to a single model.",
            "comparison_format": "Single best-prompt baseline vs ensemble (uniform/weighted/distilled) approaches.",
            "performance": "Survey states ensembles alleviate sensitivity to prompt choice and can increase accuracy/stability; reported improvements are method- and task-dependent (qualitative statement; no single numeric summary in survey).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (stability and often accuracy)",
            "explanation_or_hypothesis": "Different prompts capture complementary modes of model behavior; ensembling averages out prompt-specific errors and reduces variance due to prompt choice.",
            "counterexample_or_null_result": null,
            "uuid": "e5813.4",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Continuous prompt sensitivity",
            "name_full": "Sensitivity of prefix/soft (continuous) prompts to initialization and data regime",
            "brief_description": "Survey notes that continuous prompt methods (prefix-tuning, prompt-tuning, P-tuning) can be powerful but are sensitive to initialization and especially to low-data settings; initializing from discrete prompts can help.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "prefix-tuning / prompt-tuning applied to large LMs (GPT-2, BART, T5, etc.)",
            "model_size": null,
            "task_name": "Generation and classification tasks in few-shot/fine-tuning settings",
            "task_description": "Optimize continuous vectors prepended to model inputs (prefix) or special token embeddings while keeping LM parameters frozen (fixed-LM prompt tuning).",
            "problem_format": "Continuous soft prompts (virtual token embeddings) trained on task supervision; optionally initialized from discrete prompts.",
            "comparison_format": "Discrete manual prompts or tuning entire LM vs continuous prompt tuning with different initializations.",
            "performance": "Survey reports continuous prompts often yield superior few-shot/fine-tuning performance compared to naive tuning-free prompting, but are more sensitive to initialization and random seed in low-data regimes (qualitative).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "often improved (but variable depending on init/data)",
            "explanation_or_hypothesis": "Continuous prompts are free of natural-language constraints and can more directly optimize the interface to the frozen LM, but because they live in embedding space their optimization landscape can be sensitive to initialization and small data.",
            "counterexample_or_null_result": "Survey reports that seeding continuous prompts with tokens derived from discrete prompts can stabilize and improve performance, indicating pure random init can be problematic.",
            "uuid": "e5813.5",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Answer engineering / verbalizers",
            "name_full": "Design of answer space (verbalizers) and its effect on classification",
            "brief_description": "Survey explains that the mapping from LM-generated tokens/spans to target labels (verbalizers) and constraints on the answer space strongly affect classification performance under prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "masked and autoregressive LMs used for classification probes (BERT, RoBERTa, GPT variants)",
            "model_size": null,
            "task_name": "Text classification, NLI, NER, relation extraction",
            "task_description": "Define permissible answer set Z (single tokens, spans, sentences) and map answer tokens to task labels (verbalizers).",
            "problem_format": "Constrained answer spaces (manually chosen label words), paraphrase-expanded answer sets, or continuous learned answer embeddings.",
            "comparison_format": "Different verbalizer choices (single token vs paraphrase set vs learned virtual token) compared on same task.",
            "performance": "Survey reports that carefully designed verbalizers and constrained answer spaces improve results in classification tasks; paraphrase expansion or search/pruning methods can further help. No single numeric metric reported.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved when answer engineering aligns token choices with model's pretraining distribution",
            "explanation_or_hypothesis": "The LM's likelihoods over tokens/spans are leveraged directly; choosing label words that the LM naturally associates with a class (frequent, unambiguous tokens) yields better mapping to labels.",
            "counterexample_or_null_result": "Survey notes that manual verbalizers can be suboptimal and automated search/paraphrasing sometimes required; also continuous answer tokens (virtual embeddings) have been explored but are less common.",
            "uuid": "e5813.6",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Null / minimal prompt observation",
            "name_full": "NullPrompt and observations that minimal or empty templates can be competitive",
            "brief_description": "Survey cites work (Logan IV et al.) that found using a null/simple prompt (concatenate input and mask without template words) can achieve competitive performance in some cases, indicating heavy prompt engineering is not always necessary.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RoBERTa, ALBERT and similar fine-tuned PLMs (as discussed in survey)",
            "model_size": null,
            "task_name": "Text classification (few-shot/fine-tuning settings)",
            "task_description": "Using a null prompt template such as \"[X][Z]\" versus crafted templates.",
            "problem_format": "Null prompt (no natural-language template words) used with fixed-prompt LM tuning or LM fine-tuning.",
            "comparison_format": "Null prompt vs manually engineered prompts of similar tuning strategy.",
            "performance": "Survey reports null prompt achieved competitive accuracy on certain tasks in cited work (qualitative; no universal numeric value reported).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "no consistent improvement from complex prompts; sometimes simple/null prompt performed comparably",
            "explanation_or_hypothesis": "In cases where model fine-tuning or answer engineering is sufficient, the added inductive bias of a natural-language template may be unnecessary; also indicates that prompt utility depends on task, model, and tuning strategy.",
            "counterexample_or_null_result": null,
            "uuid": "e5813.7",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Prompt decomposition for structured outputs",
            "name_full": "Decomposing holistic prompts into sub-prompts for structured tasks (e.g., NER, relation extraction)",
            "brief_description": "Survey reports that for tasks requiring multiple or structured predictions (sequence labeling, relation extraction), decomposing into multiple sub-prompts (span-level prompts or multi-step prompts) makes prompting tractable and can improve applicability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PLMs used for IE and tagging (BART, RoBERTa, BERT variants)",
            "model_size": null,
            "task_name": "Named entity recognition (NER), relation extraction, sequence labeling",
            "task_description": "Break the input into candidate spans or subtasks and prompt model separately for each span/subtask (e.g., ask \"What is the entity type of span S? [Z]\").",
            "problem_format": "Prompt decomposition: many span-level/instance-level prompts per input instead of a single global prompt.",
            "comparison_format": "Holistic single prompt vs decomposed per-span prompts.",
            "performance": "Survey indicates decomposition makes the problem feasible and can produce strong results for span-level tasks; quantitative gains depend on downstream methods and are not unified in survey.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved applicability and often improved performance for structured tasks",
            "explanation_or_hypothesis": "Holistic prompts may be intractable for combinatorial output spaces; decomposition reduces complexity and lets prompts target specific sub-decisions aligning with LM strengths.",
            "counterexample_or_null_result": null,
            "uuid": "e5813.8",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Pretraining objective and prompt alignment",
            "name_full": "Effect of pre-training objective (SLM/CTR/FTR/MLM) and attention mask directionality on prompt effectiveness",
            "brief_description": "Survey emphasizes that the pre-training objective (standard LM, denoising/CTR, full reconstruction/FTR, masked LM) and attention masking patterns influence which prompt formats and tasks the model is best suited for.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "models categorized by training objective (L2R GPT-like, MLM BERT-like, prefix and encoder-decoder models such as T5/BART)",
            "model_size": null,
            "task_name": "Wide range: NLU, NLG, translation, probing",
            "task_description": "Choice of LM objective affects suitability: L2R for prefix generation tasks, MLM for cloze/NLU tasks, prefix/FTR models are versatile across both.",
            "problem_format": "Match prompt shape (cloze/prefix) to pre-training objective and mask pattern; e.g., use cloze with MLMs, prefix for autoregressive models.",
            "comparison_format": "Using same prompt format across models with different pretraining objectives vs matching prompt to objective.",
            "performance": "Survey-level qualitative claim: matching prompt format to pretraining objective improves performance and applicability; no unified numeric summary provided.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved when prompt shape aligns with pretraining objective",
            "explanation_or_hypothesis": "Alignment reduces distributional shift between pretraining and downstream usage; e.g., cloze mirrors masked token prediction, prefix mirrors autoregressive continuation.",
            "counterexample_or_null_result": null,
            "uuid": "e5813.9",
            "source_info": {
                "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "LAMA: LAnguage Model Analysis",
            "rating": 2
        },
        {
            "paper_title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
            "rating": 2
        },
        {
            "paper_title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "rating": 2
        },
        {
            "paper_title": "AutoPrompt (Shin et al.) / Mining prompts (Jiang et al.)",
            "rating": 1
        },
        {
            "paper_title": "P-Tuning and P-Tuning v2 (or related continuous prompt tuning papers)",
            "rating": 1
        },
        {
            "paper_title": "Gao et al. (2021) — on automatic prompt generation and demonstration learning",
            "rating": 1
        },
        {
            "paper_title": "Lu et al. (2021) — on exemplar ordering and selection effects",
            "rating": 1
        }
    ],
    "cost": 0.017901749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</h1>
<p>Pengfei Liu<br>Carnegie Mellon University<br>pliu3@cs.cmu.edu<br>Zhenghao Jiang<br>Carnegie Mellon University<br>zhengbaj@cs.cmu.edu</p>
<p>Weizhe Yuan
Carnegie Mellon University
weizhey@cs.cmu.edu</p>
<p>Hiroaki Hayashi
Carnegie Mellon University
hiroakih@cs.cmu.edu</p>
<p>Jinlan Fu<br>National University of Singapore<br>jinlanjonna@gmail.com<br>Graham Neubig<br>Carnegie Mellon University<br>gneubig@cs.cmu.edu</p>
<h2>Abstract</h2>
<p>This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input $\boldsymbol{x}$ and predict an output $\boldsymbol{y}$ as $P(\boldsymbol{y} \mid \boldsymbol{x})$, prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input $\boldsymbol{x}$ is modified using a template into a textual string prompt $\boldsymbol{x}^{\prime}$ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string $\hat{\boldsymbol{x}}$, from which the final output $\boldsymbol{y}$ can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g. the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website
\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\%\% including constantly-updated survey, and paperlist.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h2>Contents</h2>
<p>1 Two Sea Changes in NLP ..... 3
2 A Formal Description of Prompting ..... 4
2.1 Supervised Learning in NLP ..... 4
2.2 Prompting Basics ..... 4
2.2.1 Prompt Addition ..... 5
2.2.2 Answer Search ..... 5
2.2.3 Answer Mapping ..... 5
2.3 Design Considerations for Prompting ..... 6
3 Pre-trained Language Models ..... 8
3.1 Training Objectives ..... 8
3.2 Noising Functions ..... 8
3.3 Directionality of Representations ..... 9
3.4 Typical Pre-training Methods ..... 9
3.4.1 Left-to-Right Language Model ..... 9
3.4.2 Masked Language Models ..... 10
3.4.3 Prefix and Encoder-Decoder ..... 10
4 Prompt Engineering ..... 11
4.1 Prompt Shape ..... 11
4.2 Manual Template Engineering ..... 11
4.3 Automated Template Learning ..... 11
4.3.1 Discrete Prompts ..... 12
4.3.2 Continuous Prompts ..... 12
5 Answer Engineering ..... 13
5.1 Answer Shape ..... 13
5.2 Answer Space Design Methods ..... 14
5.2.1 Manual Design ..... 14
5.2.2 Discrete Answer Search ..... 14
5.2.3 Continuous Answer Search ..... 14
6 Multi-Prompt Learning ..... 15
6.1 Prompt Ensembling ..... 15
6.2 Prompt Augmentation ..... 16
6.3 Prompt Composition ..... 16
6.4 Prompt Decomposition ..... 17
7 Training Strategies for Prompting Methods ..... 17
7.1 Training Settings ..... 17
7.2 Parameter Update Methods ..... 17
7.2.1 Promptless Fine-tuning ..... 18
7.2.2 Tuning-free Prompting ..... 18
7.2.3 Fixed-LM Prompt Tuning ..... 18
7.2.4 Fixed-prompt LM Tuning ..... 18
7.2.5 Prompt+LM Tuning ..... 19
8 Applications ..... 19
8.1 Knowledge Probing ..... 19
8.2 Classification-based Tasks ..... 19
8.3 Information Extraction ..... 22
8.4 "Reasoning" in NLP ..... 22
8.5 Question Answering ..... 23
8.6 Text Generation ..... 23
8.7 Automatic Evaluation of Text Generation ..... 23
8.8 Multi-modal Learning ..... 23
8.9 Meta-Applications ..... 23
8.10 Resources ..... 24
9 Prompt-relevant Topics ..... 24
10 Challenges ..... 27
10.1 Prompt Design ..... 27
10.2 Answer Engineering ..... 28
10.3 Selection of Tuning Strategy ..... 28
10.4 Multiple Prompt Learning ..... 28
10.5 Selection of Pre-trained Models ..... 29
10.6 Theoretical and Empirical Analysis of Prompting ..... 29
10.7 Transferability of Prompts ..... 29
10.8 Combination of Different Paradigms ..... 29
10.9 Calibration of Prompting Methods ..... 29
11 Meta Analysis ..... 29
11.1 Timeline ..... 31
11.2 Trend Analysis ..... 31
12 Conclusion ..... 31
A Appendix on Pre-trained LMs ..... 44
A. 1 Evolution of Pre-trained LM Parameters ..... 44
A. 2 Auxiliary Objective ..... 44
A. 3 Pre-trained Language Model Families ..... 45</p>
<h1>1 Two Sea Changes in NLP</h1>
<p>Fully supervised learning, where a task-specific model is trained solely on a dataset of input-output examples for the target task, has long played a central role in many machine learning tasks (Kotsiantis et al., 2007), and natural language processing (NLP) was no exception. Because such fully supervised datasets are ever-insufficient for learning high-quality models, early NLP models relied heavily on feature engineering (Tab. 1 a.; e.g. Lafferty et al. (2001); Guyon et al. (2002); Och et al. (2004); Zhang and Nivre (2011)), where NLP researchers or engineers used their domain knowledge to define and extract salient features from raw data and provide models with the appropriate inductive bias to learn from this limited data. With the advent of neural network models for NLP, salient features were learned jointly with the training of the model itself (Collobert et al., 2011; Bengio et al., 2013), and hence focus shifted to architecture engineering, where inductive bias was rather provided through the design of a suitable network architecture conducive to learning such features (Tab. 1 b.; e.g. Hochreiter and Schmidhuber (1997); Kalchbrenner et al. (2014); Chung et al. (2014); Kim (2014); Bahdanau et al. (2014); Vaswani et al. (2017)). ${ }^{1}$</p>
<p>However, from 2017-2019 there was a sea change in the learning of NLP models, and this fully supervised paradigm is now playing an ever-shrinking role. Specifically, the standard shifted to the pre-train and fine-tune paradigm (Tab. 1 c.; e.g. Radford and Narasimhan (2018); Peters et al. (2018); Dong et al. (2019); Yang et al. (2019); Lewis et al. (2020a)). In this paradigm, a model with a fixed ${ }^{2}$ architecture is pre-trained as a language model (LM), predicting the probability of observed textual data. Because the raw textual data necessary to train LMs is available in abundance, these LMs can be trained on large datasets, in the process learning robust general-purpose features of the language it is modeling. The above pre-trained LM will be then adapted to different downstream tasks by introducing additional parameters and fine-tuning them using task-specific objective functions. Within this paradigm, the focus turned mainly to objective engineering, designing the training objectives used at both the pre-training and fine-tuning stages. For example, Zhang et al. (2020a) show that introducing a loss function of predicting salient sentences from a document will lead to a better pre-trained model for text summarization. Notably, the main body of the pre-trained LM is generally (but not always; Peters et al. (2019)) fine-tuned as well to make it more suitable for solving the downstream task.</p>
<p>Now, as of this writing in 2021, we are in the middle of a second sea change, in which the "pre-train, fine-tune" procedure is replaced by one in which we dub "pre-train, prompt, and predict". In this paradigm, instead of adapting pre-trained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual prompt. For example, when recognizing the emotion of a social media post, "I missed the bus today.", we may continue with a prompt "I felt so $\qquad$ ", and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt "English: I missed the bus today. French: $\qquad$ "), an LM may be able to fill in the blank with a French translation. In this way, by selecting the appropriate prompts we can manipulate the model behavior so that the pre-trained LM itself can be used to predict the desired output, sometimes even without any additional task-specific training (Tab. 1 d.; e.g. Radford et al. (2019); Petroni et al. (2019); Brown et al. (2020); Raffel et al. (2020); Schick and Schütze (2021b); Gao et al. (2021)). The advantage of this method is that, given a suite of appropriate prompts, a single LM trained in an entirely unsupervised fashion can be used to solve a great number of tasks (Brown et al., 2020; Sun et al., 2021). However, as with most conceptually enticing prospects, there is a catch - this method introduces the necessity for prompt engineering, finding the most appropriate prompt to allow a LM to solve the task at hand.</p>
<p>This survey attempts to organize the current state of knowledge in this rapidly developing field by providing an overview and formal definition of prompting methods (§2), and an overview of the pre-trained language models that use these prompts (§3). This is followed by in-depth discussion of prompting methods, from basics such as prompt engineering (§4) and answer engineering (§5) to more advanced concepts such as multi-prompt learning methods (§6) and prompt-aware training methods (§7). We then organize the various applications to which prompt-based learning methods have been applied, and discuss how they interact with the choice of prompting method (§8). Finally, we attempt to situate the current state of prompting methods in the research ecosystem, making connections to other research fields (§9), suggesting some current challenging problems that may be ripe for further research (§10), and performing a meta-analysis of current research trends (§11).</p>
<p>Finally, in order to help beginners who are interested in this field learn more effectively, we highlight some systematic resources about prompt learning (as well as pre-training) provided both within this survey and on companion websites:</p>
<ul>
<li>A website of prompt-based learning that contains: frequent updates to this survey, related slides, etc.</li>
<li>Fig.1: A typology of important concepts for prompt-based learning.</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Paradigm</th>
<th>Engineering</th>
<th>Task Relation</th>
</tr>
</thead>
<tbody>
<tr>
<td>a. Fully Supervised Learning (Non-Neural Network)</td>
<td>Features (e.g. word identity, part-of-speech, sentence length)</td>
<td><img alt="img-1.jpeg" src="img-1.jpeg" /></td>
</tr>
<tr>
<td>b. Fully Supervised Learning (Neural Network)</td>
<td>Architecture (e.g. convolutional, recurrent, self-attentional)</td>
<td><img alt="img-2.jpeg" src="img-2.jpeg" /></td>
</tr>
<tr>
<td>c. Pre-train, Fine-tune</td>
<td>Objective (e.g. masked language modeling, next sentence prediction)</td>
<td><img alt="img-3.jpeg" src="img-3.jpeg" /></td>
</tr>
<tr>
<td>d. Pre-train, Prompt, Predict</td>
<td>Prompt (e.g. cloze, prefix)</td>
<td><img alt="img-4.jpeg" src="img-4.jpeg" /></td>
</tr>
</tbody>
</table>
<p>Table 1: Four paradigms in NLP. The “engineering” column represents the type of engineering to be done to build strong systems. The “task relation” column, shows the relationship between language models (LM) and other NLP tasks (CLS: classification, TAG: sequence tagging, GEN: text generation). $\square$ : fully unsupervised training. $\square$ : fully supervised training. $\square$ : Supervised training combined with unsupervised training. $\square$ indicates a textual prompt. Dashed lines suggest that different tasks can be connected by sharing parameters of pre-trained models. “LM$\rightarrow$ Task” represents adapting LMs (objectives) to downstream tasks while “Task$\rightarrow$ LM” denotes adapting downstream tasks (formulations) to LMs.</p>
<ul>
<li>Tab.7: A systematic and comprehensive comparison among different prompting methods.</li>
<li>Tab.10: An organization of commonly-used prompts.</li>
<li>Tab.12: A timeline of prompt-based research works.</li>
<li>Tab.13: A systematic and comprehensive comparison among different pre-trained LMs.</li>
</ul>
<h2>2 A Formal Description of Prompting</h2>
<h3>2.1 Supervised Learning in NLP</h3>
<p>In a traditional supervised learning system for NLP, we take an input $\boldsymbol{x}$, usually text, and predict an output $\boldsymbol{y}$ based on a model $P(\boldsymbol{y} \mid \boldsymbol{x} ; \theta)$. $\boldsymbol{y}$ could be a label, text, or other variety of output. In order to learn the parameters $\theta$ of this model, we use a dataset containing pairs of inputs and outputs, and train a model to predict this conditional probability. We will illustrate this with two stereotypical examples.</p>
<p>First, text classification takes an input text $\boldsymbol{x}$ and predicts a label $y$ from a fixed label set $\mathcal{Y}$. To give an example, sentiment analysis ( [Pang et al., 2002; Socher et al., 2013]) may take an input $\boldsymbol{x}=$ “I love this movie.” and predict a label $y=++$, out of a label set $\mathcal{Y}={++,+,-,-,-$.</p>
<p>Second, conditional text generation takes an input $\boldsymbol{x}$ and generates another text $\boldsymbol{y}$. One example is machine translation ( [Koehn, 2009]), where the input is text in one language such as the Finnish $\boldsymbol{x}=$ “Hyvää huomenta.” and the output is the English $\boldsymbol{y}=$ “Good morning”..</p>
<h3>2.2 Prompting Basics</h3>
<p>The main issue with supervised learning is that in order to train a model $P(\boldsymbol{y} \mid \boldsymbol{x} ; \theta)$, it is necessary to have supervised data for the task, which for many tasks cannot be found in large amounts. Prompt-based learning methods for NLP attempt to circumvent this issue by instead learning an LM that models the probability $P(\boldsymbol{x} ; \theta)$ of text $\boldsymbol{x}$ itself (details in §3) and using this probability to predict $\boldsymbol{y}$, reducing or obviating the need for large supervised datasets. In this section we lay out a mathematical description of the most fundamental form of prompting, which encompasses many works on prompting and can be expanded to cover others as well. Specifically, basic prompting predicts the highest-scoring $\hat{\boldsymbol{y}}$ in three steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Notation</th>
<th style="text-align: left;">Example</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Input</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: left;">I love this movie.</td>
<td style="text-align: left;">One or multiple texts</td>
</tr>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: center;">$\boldsymbol{y}$</td>
<td style="text-align: left;">++ (very positive)</td>
<td style="text-align: left;">Output label or text</td>
</tr>
<tr>
<td style="text-align: left;">Prompting <br> Function</td>
<td style="text-align: center;">$f_{\text {prompt }}(\boldsymbol{x})$</td>
<td style="text-align: left;">[X] Overall, it was a [Z] movie.</td>
<td style="text-align: left;">A function that converts the input into a <br> specific form by inserting the input $\boldsymbol{x}$ and <br> adding a slot [Z] where answer $\boldsymbol{z}$ may <br> be filled later.</td>
</tr>
<tr>
<td style="text-align: left;">Prompt</td>
<td style="text-align: center;">$\boldsymbol{x}^{\prime}$</td>
<td style="text-align: left;">I love this movie. Overall, it was a [Z] movie.</td>
<td style="text-align: left;">A text where [X] is instantiated by input <br> $\boldsymbol{x}$ but answer slot [Z] is not.</td>
</tr>
<tr>
<td style="text-align: left;">Filled Prompt</td>
<td style="text-align: center;">$f_{\text {fill }}\left(\boldsymbol{x}^{\prime}, \boldsymbol{z}\right)$</td>
<td style="text-align: left;">I love this movie. Overall, it was a bad movie.</td>
<td style="text-align: left;">A prompt where slot [Z] is filled with <br> any answer.</td>
</tr>
<tr>
<td style="text-align: left;">Answered <br> Prompt</td>
<td style="text-align: center;">$f_{\text {fill }}\left(\boldsymbol{x}^{\prime}, \boldsymbol{z}^{*}\right)$</td>
<td style="text-align: left;">I love this movie. Overall, it was a good movie.</td>
<td style="text-align: left;">A prompt where slot [Z] is filled with a <br> true answer.</td>
</tr>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: center;">$\boldsymbol{z}$</td>
<td style="text-align: left;">"good", "fantastic", "boring"</td>
<td style="text-align: left;">A token, phrase, or sentence that fills [Z]</td>
</tr>
</tbody>
</table>
<p>Table 2: Terminology and notation of prompting methods. $\boldsymbol{z}^{<em>}$ represents answers that correspond to true output $\boldsymbol{y}^{</em>}$.</p>
<h1>2.2.1 Prompt Addition</h1>
<p>In this step a prompting function $f_{\text {prompt }}(\cdot)$ is applied to modify the input text $\boldsymbol{x}$ into a prompt $\boldsymbol{x}^{\prime}=f_{\text {prompt }}(\boldsymbol{x})$. In the majority of previous work (Kumar et al., 2016; McCann et al., 2018; Radford et al., 2019; Schick and Schütze, 2021a), this function consists of a two step process:</p>
<ol>
<li>Apply a template, which is a textual string that has two slots: an input slot [X] for input $\boldsymbol{x}$ and an answer slot [Z] for an intermediate generated answer text $\boldsymbol{z}$ that will later be mapped into $\boldsymbol{y}$.</li>
<li>Fill slot $[X]$ with the input text $\boldsymbol{x}$.</li>
</ol>
<p>In the case of sentiment analysis where $\boldsymbol{x}=$ "I love this movie.", the template may take a form such as "[X] Overall, it was a [Z] movie.". Then, $\boldsymbol{x}^{\prime}$ would become "I love this movie. Overall it was a [Z] movie." given the previous example. In the case of machine translation, the template may take a form such as "Finnish: [X] English: [Z]", where the text of the input and answer are connected together with headers indicating the language. We show more examples in Tab. 3</p>
<p>Notably, (1) the prompts above will have an empty slot to fill in for $\boldsymbol{z}$, either in the middle of the prompt or at the end. In the following text, we will refer to the first variety of prompt with a slot to fill in the middle of the text as a cloze prompt, and the second variety of prompt where the input text comes entirely before $\boldsymbol{z}$ as a prefix prompt. (2) In many cases these template words are not necessarily composed of natural language tokens; they could be virtual words (e.g. represented by numeric ids) which would be embedded in a continuous space later, and some prompting methods even generate continuous vectors directly (more in $\S 4.3 .2$ ). (3) The number of [X] slots and the number of [Z] slots can be flexibly changed for the need of tasks at hand.</p>
<h3>2.2.2 Answer Search</h3>
<p>Next, we search for the highest-scoring text $\hat{\boldsymbol{z}}$ that maximizes the score of the LM. We first define $\mathcal{Z}$ as a set of permissible values for $\boldsymbol{z} . \mathcal{Z}$ could range from the entirety of the language in the case of generative tasks, or could be a small subset of the words in the language in the case of classification, such as defining $\mathcal{Z}={$ "excellent", "good", "OK", "bad", "horrible" $}$ to represent each of the classes in $\mathcal{Y}={++,+,-,-,-}$.</p>
<p>We then define a function $f_{\text {fill }}\left(\boldsymbol{x}^{\prime}, \boldsymbol{z}\right)$ that fills in the location [Z] in prompt $\boldsymbol{x}^{\prime}$ with the potential answer $\boldsymbol{z}$. We will call any prompt that has gone through this process as a filled prompt. Particularly, if the prompt is filled with a true answer, we will refer to it as an answered prompt (Tab. 2 shows an example). Finally, we search over the set of potential answers $\boldsymbol{z}$ by calculating the probability of their corresponding filled prompts using a pre-trained LM $P(\cdot ; \theta)$</p>
<p>$$
\hat{\boldsymbol{z}}=\underset{\boldsymbol{z} \in \mathcal{Z}}{\operatorname{search}} P\left(f_{\text {fill }}\left(\boldsymbol{x}^{\prime}, \boldsymbol{z}\right) ; \theta\right)
$$</p>
<p>This search function could be an argmax search that searches for the highest-scoring output, or sampling that randomly generates outputs following the probability distribution of the LM.</p>
<h3>2.2.3 Answer Mapping</h3>
<p>Finally, we would like to go from the highest-scoring answer $\hat{\boldsymbol{z}}$ to the highest-scoring output $\hat{\boldsymbol{y}}$. This is trivial in some cases, where the answer itself is the output (as in language generation tasks such as translation), but there</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Input ( [X])</th>
<th style="text-align: center;">Template</th>
<th style="text-align: center;">Answer ([Z])</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Text CLS</td>
<td style="text-align: center;">Sentiment</td>
<td style="text-align: center;">I love this movie.</td>
<td style="text-align: center;">[X] The movie is [Z].</td>
<td style="text-align: center;">great fantastic</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Topics</td>
<td style="text-align: center;">He prompted the LM.</td>
<td style="text-align: center;">[X] The text is about [Z].</td>
<td style="text-align: center;">sports <br> science</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Intention</td>
<td style="text-align: center;">What is taxi fare to Denver?</td>
<td style="text-align: center;">[X] The question is about [Z].</td>
<td style="text-align: center;">quantity city</td>
</tr>
<tr>
<td style="text-align: center;">Text-span CLS</td>
<td style="text-align: center;">Aspect <br> Sentiment</td>
<td style="text-align: center;">Poor service but good food.</td>
<td style="text-align: center;">[X] What about service? [Z].</td>
<td style="text-align: center;">Bad <br> Terrible</td>
</tr>
<tr>
<td style="text-align: center;">Text-pair CLS</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">[X1]: An old man with ... [X2]: A man walks ...</td>
<td style="text-align: center;">[X1]? [Z], [X2]</td>
<td style="text-align: center;">Yes <br> No</td>
</tr>
<tr>
<td style="text-align: center;">Tagging</td>
<td style="text-align: center;">NER</td>
<td style="text-align: center;">[X1]: Mike went to Paris. [X2]: Paris</td>
<td style="text-align: center;">[X1][X2] is a [Z] entity.</td>
<td style="text-align: center;">organization <br> location</td>
</tr>
<tr>
<td style="text-align: center;">Text Generation</td>
<td style="text-align: center;">Summarization</td>
<td style="text-align: center;">Las Vegas police ...</td>
<td style="text-align: center;">[X] TL;DR: [Z]</td>
<td style="text-align: center;">The victim ... <br> A woman ...</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Je vous aime.</td>
<td style="text-align: center;">French: [X] English: [Z]</td>
<td style="text-align: center;">I love you. <br> I fancy you.</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of input, template, and answer for different tasks. In the Type column, "CLS" is an abbreviation for "classification". In the Task column, "NLI" and "NER" are abbreviations for "natural language inference" (Bowman et al., 2015) and "named entity recognition" (Tjong Kim Sang and De Meulder, 2003) respectively.
are also other cases where multiple answers could result in the same output. For example, one may use multiple different sentiment-bearing words (e.g. "excellent", "fabulous", "wonderful") to represent a single class (e.g. "++"), in which case it is necessary to have a mapping between the searched answer and the output value.</p>
<h1>2.3 Design Considerations for Prompting</h1>
<p>Now that we have our basic mathematical formulation, we elaborate a few of the basic design considerations that go into a prompting method, which we will elaborate in the following sections:</p>
<ul>
<li>Pre-trained Model Choice: There are a wide variety of pre-trained LMs that could be used to calculate $P(\boldsymbol{x} ; \theta)$. In $\S 3$ we give a primer on pre-trained LMs, specifically from the dimensions that are important for interpreting their utility in prompting methods.</li>
<li>Prompt Engineering: Given that the prompt specifies the task, choosing a proper prompt has a large effect not only on the accuracy, but also on which task the model performs in the first place. In $\S 4$ we discuss methods to choose which prompt we should use as $f_{\text {prompt }}(\boldsymbol{x})$.</li>
<li>Answer Engineering: Depending on the task, we may want to design $\mathcal{Z}$ differently, possibly along with the mapping function. In $\S 5$ we discuss different ways to do so.</li>
<li>Expanding the Paradigm: As stated above, the above equations represent only the simplest of the various underlying frameworks that have been proposed to do this variety of prompting. In $\S 6$ we discuss ways to expand this underlying paradigm to further improve results or applicability.</li>
<li>Prompt-based Training Strategies: There are also methods to train parameters, either of the prompt, the LM, or both. In $\S 7$, we summarize different strategies and detail their relative advantages.</li>
</ul>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 1: Typology of prompting methods.</p>
<h1>3 Pre-trained Language Models</h1>
<p>Given the large impact that pre-trained LMs have had on NLP in the pre-train and fine-tune paradigm, there are already a number of high-quality surveys that interested readers where interested readers can learn more (Raffel et al., 2020; Qiu et al., 2020; Xu et al., 2021; Doddapaneni et al., 2021). Nonetheless, in this chapter we present a systematic view of various pre-trained LMs which (i) organizes them along various axes in a more systematic way, (ii) particularly focuses on aspects salient to prompting methods. Below, we will detail them through the lens of main training objective, type of text noising, auxiliary training objective, attention mask, typical architecture, and preferred application scenarios. We describe each of these objectives below, and also summarize a number of pre-trained LMs along each of these axes in Tab. 13 in the appendix.</p>
<h3>3.1 Training Objectives</h3>
<p>The main training objective of a pre-trained LM almost invariably consists of some sort of objective predicting the probability of text $\boldsymbol{x}$.</p>
<p>Standard Language Model (SLM) objectives do precisely this, training the model to optimize the probability $P(\boldsymbol{x})$ of text from a training corpus (Radford et al., 2019). In these cases, the text is generally predicted in an autoregressive fashion, predicting the tokens in the sequence one at a time. This is usually done from left to right (as detailed below), but can be done in other orders as well.</p>
<p>A popular alternative to standard LM objectives are denoising objectives, which apply some noising function $\hat{\boldsymbol{x}}=f_{\text {noise }}(\boldsymbol{x})$ to the input sentence (details in the following subsection), then try to predict the original input sentence given this noised text $P(\boldsymbol{x} \mid \hat{\boldsymbol{x}})$. There are two common flavors of these objectives:</p>
<p>Corrupted Text Reconstruction (CTR) These objectives restore the processed text to its uncorrupted state by calculating loss over only the noised parts of the input sentence.</p>
<p>Full Text Reconstruction (FTR) These objectives reconstruct the text by calculating the loss over the entirety of the input texts whether it has been noised or not (Lewis et al., 2020a).</p>
<p>The main training objective of the pre-trained LMs plays an important role in determining its applicability to particular prompting tasks. For example, left-to-right autoregressive LMs may be particularly suitable for prefix prompts, whereas reconstruction objectives may be more suitable for cloze prompts. In addition, models trained with standard LM and FTR objectives may be more suitable for tasks regarding text generation, whereas other tasks such as classification can be formulated using models trained with any of these objectives.</p>
<p>In addition to the main training objectives above, a number of auxiliary objectives have been engineered to further improve models' ability to perform certain varieties of downstream tasks. We list some commonly-used auxiliary objectives in Appendix A.2.</p>
<h3>3.2 Noising Functions</h3>
<p>In training objectives based on reconstruction, the specific type of corruption applied to obtain the noised text $\hat{\boldsymbol{x}}$ has an effect on the efficacy of the learning algorithm. In addition, prior knowledge can be incorporated by controlling the type of noise, e.g. the noise could focus on entities of a sentence, which allows us to learn a pre-trained model with particularly high predictive performance for entities. In the following, we introduce several types of noising functions, and give detailed examples in Tab. 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Operation</th>
<th style="text-align: center;">Element</th>
<th style="text-align: center;">Original Text</th>
<th style="text-align: center;">Corrupted Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Mask</td>
<td style="text-align: center;">one token <br> two tokens <br> one entity</td>
<td style="text-align: center;">Jane will move to New York . <br> Jane will move to New York . <br> Jane will move to New York .</td>
<td style="text-align: center;">Jane will [Z] to New York . <br> Jane will [Z] [Z] New York . <br> Jane will move to [Z] .</td>
</tr>
<tr>
<td style="text-align: center;">Replace</td>
<td style="text-align: center;">one token <br> two tokens <br> one entity</td>
<td style="text-align: center;">Jane will move to New York . <br> Jane will move to New York . <br> Jane will move to New York .</td>
<td style="text-align: center;">Jane will move [X] New York . <br> Jane will move [X] [Y] York . <br> Jane will move to [X] .</td>
</tr>
<tr>
<td style="text-align: center;">Delete</td>
<td style="text-align: center;">one token <br> two token</td>
<td style="text-align: center;">Jane will move to New York . <br> Jane will move to New York .</td>
<td style="text-align: center;">Jane move to New York . <br> Jane to New York .</td>
</tr>
<tr>
<td style="text-align: center;">Permute <br> Rotate <br> Concatenation</td>
<td style="text-align: center;">token <br> none <br> two languages</td>
<td style="text-align: center;">Jane will move to New York . <br> Jane will move to New York .</td>
<td style="text-align: center;">New York . Jane will move to <br> to New York . Jane will move <br> Jane will move to New York . [/s] 简将搬到纽约。</td>
</tr>
</tbody>
</table>
<p>Table 4: Detailed examples for different noising operations.</p>
<p>Masking (e.g. Devlin et al. (2019)) The text will be masked in different levels, replacing a token or multi-token span with a special token such as [MASK]. Notably, masking can either be random from some distribution or specifically designed to introduce prior knowledge, such as the above-mentioned example of masking entities to encourage the model to be good at predicting entities.</p>
<p>Replacement (e.g. Raffel et al. (2020)) Replacement is similar to masking, except that the token or multi-token span is not replaced with a [MASK] but rather another token or piece of information (e.g., an image region (Su et al., 2020)).</p>
<p>Deletion (e.g. Lewis et al. (2020a)) Tokens or multi-token spans will be deleted from a text without the addition of [MASK] or any other token. This operation is usually used together with the FTR loss.</p>
<p>Permutation (e.g. Liu et al. (2020a)) The text is first divided into different spans (tokens, sub-sentential spans, or sentences), and then these spans are be permuted into a new text.</p>
<h1>3.3 Directionality of Representations</h1>
<p>A final important factor that should be considered in understanding pre-trained LMs and the difference between them is the directionality of the calculation of representations. In general, there are two widely used ways to calculate such representations:</p>
<p>Left-to-Right The representation of each word is calculated based on the word itself and all previous words in the sentence. For example, if we have a sentence "This is a good movie", the representation of the word "good" would be calculated based on previous words. This variety of factorization is particularly widely used when calculating standard LM objectives or when calculating the output side of an FTR objective, as we discuss in more detail below.</p>
<p>Bidirectional The representation of each word is calculated based on all words in the sentence, including words to the left of the current word. In the example above, "good" would be influenced by all words in the sentence, even the following "movie".</p>
<p>In addition to the two most common directionalities above, it is also possible to mix the two strategies together in a single model (Dong et al., 2019; Bao et al., 2020), or perform conditioning of the representations in a randomly permuted order (Yang et al., 2019), although these strategies are less widely used. Notably, when implementing these strategies within a neural model, this conditioning is generally implemented through attention masking, which masks out the values in an attentional model (Bahdanau et al., 2014), such as the popular Transformer architecture (Vaswani et al., 2017). Some examples of such attention masks are shown in Figure 2.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 2: Three popular attention mask patterns, where the subscript $t$ indicates the $t$-th timestep. A shaded box at $(i, j)$ indicates that the attention mechanism is allowed to attend to the input element $i$ at output time step $j$. A white box indicates that the attention mechanism is not allowed to attend to the corresponding $i$ and $j$ combination.</p>
<h3>3.4 Typical Pre-training Methods</h3>
<p>With the above concepts in mind, we introduce four popular pre-training methods, resulting from diverse combinations of objective, noising function, and directionality. These are described below, and summarized in Fig. 3 and Tab. 5 .</p>
<h3>3.4.1 Left-to-Right Language Model</h3>
<p>Left-to-right LMs (L2R LMs), a variety of auto-regressive $L M$, predict the upcoming words or assign a probability $P(\boldsymbol{x})$ to a sequence of words $\boldsymbol{x}=x_{1}, \cdots, x_{n}$ (Jurafsky and Martin, 2021). The probability is commonly broken down using the chain rule in a left-to-right fashion: $P(\boldsymbol{x})=P\left(x_{1}\right) \times \cdots P\left(x_{n} \mid x_{1} \cdots x_{n-1}\right) .^{3}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 3: Typical paradigms of pre-trained LMs.</p>
<h1>Example \&amp; Applicable Scenario</h1>
<p>Left-to-right LMs have been standard since their proposal by Markov in 1913 (Markov, 2006), and have been used continuously since then in both count-based (Goodman, 2001) and neural forms (Bengio et al., 2003; Mikolov et al., 2010; Radford and Narasimhan, 2018). Representative examples of modern pre-trained left-to-right LMs include GPT-3 (Brown et al., 2020), and GPT-Neo (Black et al., 2021).
L2R pre-trained LMs are also the popular backbone that many prompting methods adopt (Radford et al., 2019; Brown et al., 2020) . One practical reason for this is that many such models are large (PanGu- $\alpha$ (Zeng et al., 2021), Ernie-3 (Sun et al., 2021)) and ponderous to train, or not even available publicly. Thus using these models in the pre-train and fine-tune regimen is often not possible.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LMs</th>
<th style="text-align: center;">$\boldsymbol{x}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\boldsymbol{y}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Application</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mask</td>
<td style="text-align: center;">Noise</td>
<td style="text-align: center;">Main Obj.</td>
<td style="text-align: center;">Mask</td>
<td style="text-align: center;">Noise</td>
<td style="text-align: center;">Main Obj.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">L2R</td>
<td style="text-align: center;">Diagonal</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">SLM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">NLU \&amp; NLG</td>
</tr>
<tr>
<td style="text-align: center;">Mask</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Mask</td>
<td style="text-align: center;">CTR</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">NLU</td>
</tr>
<tr>
<td style="text-align: center;">Prefix</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">CTR</td>
<td style="text-align: center;">Diagonal</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">SLM</td>
<td style="text-align: center;">NLU \&amp; NLG</td>
</tr>
<tr>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">None $\dagger$</td>
<td style="text-align: center;">Diagonal</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">FTR/CRT</td>
<td style="text-align: center;">NLU \&amp; NLG</td>
</tr>
</tbody>
</table>
<p>Table 5: Typical architectures for pre-trained LMs. $\boldsymbol{x}$ and $\boldsymbol{y}$ represent text to be encoded and decoded, respectively. SLM: Standard language model. CTR: Corrupted text reconstruction. FTR: Full text reconstruction. $\dagger$ : Encoderdecoder architectures usually apply objective functions to the decoder only.</p>
<h3>3.4.2 Masked Language Models</h3>
<p>While autoregressive language models provide a powerful tool for modeling the probability of text, they also have disadvantages such as requiring representations be calculated from left-to-right. When the focus is shifted to generating the optimal representations for down-stream tasks such as classification, many other options become possible, and often preferable. One popular bidirectional objective function used widely in representation learning is the masked language model (MLM; Devlin et al. (2019)), which aims to predict masked text pieces based on surrounded context. For example, $P\left(x_{i} \mid x_{1}, \ldots, x_{i-1}, x_{i+1}, \ldots, x_{n}\right)$ represents the probability of the word $x_{i}$ given the surrounding context.</p>
<h2>Example \&amp; Applicable Scenario</h2>
<p>Representative pre-trained models using MLMs include: BERT (Devlin et al., 2019), ERNIE (Zhang et al., 2019; Sun et al., 2019b) and many variants. In prompting methods, MLMs are generally most suitable for natural language understanding or analysis tasks (e.g., text classification, natural language inference, and extractive question answering). These tasks are often relatively easy to be reformulated into cloze problems, which are consistent with the training objectives of the MLM. Additionally, MLMs have been a pre-trained model of choice when exploring methods that combine prompting with fine-tuning, elaborated further in $\S 7$.</p>
<h3>3.4.3 Prefix and Encoder-Decoder</h3>
<p>For conditional text generation tasks such as translation and summarization where an input text $\boldsymbol{x}=x_{1}, \cdots, x_{n}$ is given and the goal is to generate target text $\boldsymbol{y}$, we need a pre-trained model that is both capable of encoding the input text and generating the output text. There are two popular architectures for this purpose that share a common</p>
<p>thread of (1) using an encoder with fully-connected mask to encode the source $\boldsymbol{x}$ first and then (2) decode the target $\boldsymbol{y}$ auto-regressively (from the left to right).</p>
<p>Prefix Language Model The prefix LM is a left-to-right LM that decodes $\boldsymbol{y}$ conditioned on a prefixed sequence $\boldsymbol{x}$, which is encoded by the same model parameters but with a fully-connected mask. Notably, to encourage the prefix LM to learn better representations of the input, a corrupted text reconstruction objective is usually applied over $\boldsymbol{x}$, in addition to a standard conditional language modeling objective over $\boldsymbol{y}$.</p>
<p>Encoder-decoder The encoder-decoder model is a model that uses a left-to-right LM to decode $\boldsymbol{y}$ conditioned on a separate encoder for text $\boldsymbol{x}$ with a fully-connected mask; the parameters of the encoder and decoder are not shared. Similarly to the prefix LM, diverse types of noising can be applied to the input $\boldsymbol{x}$.</p>
<h1>Example \&amp; Applicable Scenario</h1>
<p>Prefix LMs have been explored in UniLM 1-2 (Dong et al., 2019; Bao et al., 2020) and ERNIE-M (Ouyang et al., 2020) while encoder-decoder models are widely used in pre-trained models such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020a), MASS (Song et al., 2019) and their variants.</p>
<p>Pre-trained models with prefix LMs and encoder-decoder paradigms can be naturally used to text generation tasks with (Dou et al., 2021) or without (Yuan et al., 2021a; Liu and Liu, 2021) prompting using input texts. However, recent studies reveal that other non-generation tasks, such as information extraction (Cui et al., 2021), question answering (Khashabi et al., 2020) , and text generation evaluation (Yuan et al., 2021b) can be reformulated a generation problems by providing appropriate prompts. Therefore, prompting methods (i) broaden the applicability of these generation-oriented pre-trained models. For example, pre-trained models like BART are less used in NER while prompting methods make BART applicable, and (ii) breaks the difficulty of unified modelling among different tasks (Khashabi et al., 2020).</p>
<h2>4 Prompt Engineering</h2>
<p>Prompt engineering is the process of creating a prompting function $f_{\text {prompt }}(\boldsymbol{x})$ that results in the most effective performance on the downstream task. In many previous works, this has involved prompt template engineering, where a human engineer or algorithm searches for the best template for each task the model is expected to perform. As shown in the "Prompt Engineering" section of Fig.1, one must first consider the prompt shape, and then decide whether to take a manual or automated approach to create prompts of the desired shape, as detailed below.</p>
<h3>4.1 Prompt Shape</h3>
<p>As noted above, there are two main varieties of prompts: cloze prompts (Petroni et al., 2019; Cui et al., 2021), which fill in the blanks of a textual string, and prefix prompts (Li and Liang, 2021; Lester et al., 2021), which continue a string prefix. Which one is chosen will depend both on the task and the model that is being used to solve the task. In general, for tasks regarding generation, or tasks being solved using a standard auto-regressive LM, prefix prompts tend to be more conducive, as they mesh well with the left-to-right nature of the model. For tasks that are solved using masked LMs, cloze prompts are a good fit, as they very closely match the form of the pre-training task. Full text reconstruction models are more versatile, and can be used with either cloze or prefix prompts. Finally, for some tasks regarding multiple inputs such as text pair classification, prompt templates must contain space for two inputs, [X1] and [X2], or more.</p>
<h3>4.2 Manual Template Engineering</h3>
<p>Perhaps the most natural way to create prompts is to manually create intuitive templates based on human introspection. For example, the seminal LAMA dataset (Petroni et al., 2019) provides manually created cloze templates to probe knowledge in LMs. Brown et al. (2020) create manually crafted prefix prompts to handle a wide variety of tasks, including question answering, translation, and probing tasks for common sense reasoning. Schick and Schütze (2020, 2021a,b) use pre-defined templates in a few-shot learning setting on text classification and conditional text generation tasks.</p>
<h3>4.3 Automated Template Learning</h3>
<p>While the strategy of manually crafting templates is intuitive and does allow solving various tasks with some degree of accuracy, there are also several issues with this approach: (1) creating and experimenting with these prompts is an art that takes time and experience, particularly for some complicated tasks such as semantic parsing (Shin et al., 2021); (2) even experienced prompt designers may fail to manually discover optimal prompts (Jiang et al., 2020c).</p>
<p>To address these problems, a number of methods have been proposed to automate the template design process. In particular, the automatically induced prompts can be further separated into discrete prompts, where the prompt is an</p>
<p>actual text string, and continuous prompts, where the prompt is instead described directly in the embedding space of the underlying LM.</p>
<p>One other orthogonal design consideration is whether the prompting function $f_{\text {prompt }}(\boldsymbol{x})$ is static, using essentially the same prompt template for each input, or dynamic, generating a custom template for each input. Both static and dynamic strategies have been used for different varieties of discrete and continuous prompts, as we will mention below.</p>
<h1>4.3.1 Discrete Prompts</h1>
<p>Works on discovering discrete prompts (a.k.a hard prompts) automatically search for templates described in a discrete space, usually corresponding to natural language phrases. We detail several methods that have been proposed for this below:</p>
<p>D1: Prompt Mining Jiang et al. (2020c)'s Mine approach is a mining-based method to automatically find templates given a set of training inputs $\boldsymbol{x}$ and outputs $\boldsymbol{y}$. This method scrapes a large text corpus (e.g. Wikipedia) for strings containing $\boldsymbol{x}$ and $\boldsymbol{y}$, and finds either the middle words or dependency paths between the inputs and outputs. Frequent middle words or dependency paths can serve as a template as in " $[X]$ middle words $[Z]$ ".</p>
<p>D2: Prompt Paraphrasing Paraphrasing-based approaches take in an existing seed prompt (e.g. manually constructed or mined), and paraphrases it into a set of other candidate prompts, then selects the one that achieves the highest training accuracy on the target task. This paraphrasing can be done in a number of ways, including using round-trip translation of the prompt into another language then back (Jiang et al., 2020c), using replacement of phrases from a thesaurus (Yuan et al., 2021b), or using a neural prompt rewriter specifically optimized to improve accuracy of systems using the prompt (Haviv et al., 2021). Notably, Haviv et al. (2021) perform paraphrasing after the input $\boldsymbol{x}$ is input into the prompt template, allowing a different paraphrase to be generated for each individual input.</p>
<p>D3: Gradient-based Search Wallace et al. (2019a) applied a gradient-based search over actual tokens to find short sequences that can trigger the underlying pre-trained LM to generate the desired target prediction. This search is done in an iterative fashion, stepping through tokens in the prompt. Built upon this method, Shin et al. (2020) automatically search for template tokens using downstream application training samples and demonstrates strong performance in prompting scenarios.</p>
<p>D4: Prompt Generation Other works treat the generation of prompts as a text generation task and use standard natural language generation models to perform this task. For example, Gao et al. (2021) introduce the seq2seq pre-trained model T5 into the template search process. Since T5 has been pre-trained on a task of filling in missing spans, they use T5 to generate template tokens by (1) specifying the position to insert template tokens within a template ${ }^{4}$ (2) provide training samples for T5 to decode template tokens. Ben-David et al. (2021) propose a domain adaptation algorithm that trains T5 to generate unique domain relevant features (DRFs; a set of keywords that characterize domain information) for each input. Then those DRFs can be concatenated with the input to form a template and be further used by downstream tasks.</p>
<p>D5: Prompt Scoring Davison et al. (2019) investigate the task of knowledge base completion and design a template for an input (head-relation-tail triple) using LMs. They first hand-craft a set of templates as potential candidates, and fill the input and answer slots to form a filled prompt. They then use a unidirectional LM to score those filled prompts, selecting the one with the highest LM probability. This will result in custom template for each individual input.</p>
<h3>4.3.2 Continuous Prompts</h3>
<p>Because the purpose of prompt construction is to find a method that allows an LM to effectively perform a task, rather than being for human consumption, it is not necessary to limit the prompt to human-interpretable natural language. Because of this, there are also methods that examine continuous prompts (a.k.a. soft prompts) that perform prompting directly in the embedding space of the model. Specifically, continuous prompts remove two constraints: (1) relax the constraint that the embeddings of template words be the embeddings of natural language (e.g., English) words. (2) Remove the restriction that the template is parameterized by the pre-trained LM's parameters. Instead, templates have their own parameters that can be tuned based on training data from the downstream task. We highlight several representative methods below.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>C1: Prefix Tuning Prefix Tuning (Li and Liang, 2021) is a method that prepends a sequence of continuous task-specific vectors to the input, while keeping the LM parameters frozen. Mathematically, this consists of optimizing over the following log-likelihood objective given a trainable prefix matrix $M_{\phi}$ and a fixed pre-trained LM parameterized by $\theta$.</p>
<p>$$
\max <em _phi="\phi">{\phi} \log P(\boldsymbol{y} \mid \boldsymbol{x} ; \theta ; \phi)=\max </em> ; \theta ; \phi\right)
$$} \sum_{y_{i}} \log P\left(y_{i} \mid h_{&lt;i</p>
<p>In Eq. 2, $h_{&lt;i}=\left[h_{&lt;i}^{(1)} ; \cdots ; h_{&lt;i}^{(n)}\right]$ is the concatenation of all neural network layers at time step $i$. It is copied from $M_{\phi}$ directly if the corresponding time step is within the prefix ( $h_{i}$ is $M_{\phi}[i]$ ), otherwise it is computed using the pre-trained LM.</p>
<p>Experimentally, Li and Liang (2021) observe that such continuous prefix-based learning is more sensitive to different initialization in low-data settings than the use of discrete prompts with real words. Similarly, Lester et al. (2021) prepend the input sequence with special tokens to form a template and tune the embeddings of these tokens directly. Compared to Li and Liang (2021)'s method, this adds fewer parameters as it doesn't introduce additional tunable parameters within each network layer. Tsimpoukelli et al. (2021) train a vision encoder that encodes an image into a sequence of embeddings that can be used to prompt a frozen auto-regressive LM to generate the appropriate caption. They show that the resulting model can perform few-shot learning for vision-language tasks such as visual question answering etc. Different from the above two works, the prefix used in (Tsimpoukelli et al., 2021) is sample-dependent, namely a representation of input images, instead of a task embedding.</p>
<p>C2: Tuning Initialized with Discrete Prompts There are also methods that initialize the search for a continuous prompt using a prompt that has already been created or discovered using discrete prompt search methods. For example, Zhong et al. (2021b) first define a template using a discrete search method such as AutoPrompt (Shin et al., 2020)'s, initialize virtual tokens based on this discovered prompt, then fine-tune the embeddings to increase task accuracy. This work found that initializing with manual templates can provide a better starting point for the search process. Qin and Eisner (2021) propose to learn a mixture of soft templates for each input where the weights and parameters for each template are jointly learned using training samples. The initial set of templates they use are either manually crafted ones or those obtained using the "prompt mining" method. Similarly, Hambardzumyan et al. (2021) introduce the use of a continuous template whose shape follows a manual prompt template.</p>
<p>C3: Hard-Soft Prompt Hybrid Tuning Instead of using a purely learnable prompt template, these methods insert some tunable embeddings into a hard prompt template. Liu et al. (2021b) propose "P-tuning", where continuous prompts are learned by inserting trainable variables into the embedded input. To account for interaction between prompt tokens, they represent prompt embeddings as the output of a BiLSTM (Graves et al., 2013). P-tuning also introduces the use of task-related anchor tokens (such as "capital" in relation extraction) within the template for further improvement. These anchor tokens are not tuned during training. Han et al. (2021) propose prompt tuning with rules (PTR), which uses manually crafted sub-templates to compose a complete template using logic rules. To enhance the representation ability of the resulting template, they also insert several virtual tokens whose embeddings can be tuned together with the pre-trained LMs parameters using training samples. The template tokens in PTR contain both actual tokens and virtual tokens. Experiment results demonstrate the effectiveness of this prompt design method in relation classification tasks.</p>
<h1>5 Answer Engineering</h1>
<p>In contrast to prompt engineering, which designs appropriate inputs for prompting methods, answer engineering aims to search for an answer space $\mathcal{Z}$ and a map to the original output $\mathcal{Y}$ that results in an effective predictive model. Fig. 1's "Answer Engineering" section illustrates two dimensions that must be considered when performing answer engineering: deciding the answer shape and choosing an answer design method.</p>
<h3>5.1 Answer Shape</h3>
<p>The shape of an answer characterizes its granularity. Some common choices include:</p>
<ul>
<li>Tokens: One of the tokens in the pre-trained LM's vocabulary, or a subset of the vocabulary.</li>
<li>Span: A short multi-token span. These are usually used together with cloze prompts.</li>
<li>Sentence: A sentence or document. These are commonly used with prefix prompts.</li>
</ul>
<p>In practice, how to choose the shape of acceptable answers depends on the task we want to perform. Token or text-span answer spaces are widely used in classification tasks (e.g. sentiment classification; Yin et al. (2019)), but also other tasks such as relation extraction (Petroni et al., 2019) or named entity recognition (Cui et al., 2021). Longer phrasal or sentential answers are often used in language generation tasks (Radford et al., 2019), but also</p>
<p>used in other tasks such as multiple-choice question answering (where the scores of multiple phrases are compared against each-other; Khashabi et al. (2020)).</p>
<h1>5.2 Answer Space Design Methods</h1>
<p>The next question to answer is how to design the appropriate answer space $\mathcal{Z}$, as well as the mapping to the output space $\mathcal{Y}$ if the answers are not used as the final outputs.</p>
<h3>5.2.1 Manual Design</h3>
<p>In manual design, the space of potential answers $\mathcal{Z}$ and its mapping to $\mathcal{Y}$ are crafted manually by an interested system or benchmark designer. There are a number of strategies that can be taken to perform this design.</p>
<p>Unconstrained Spaces In many cases, the answer space $\mathcal{Z}$ is the space of all tokens (Petroni et al., 2019), fixed-length spans (Jiang et al., 2020a), or token sequences (Radford et al., 2019). In these cases, it is most common to directly map answer $\boldsymbol{z}$ to the final output $\boldsymbol{y}$ using the identity mapping.</p>
<p>Constrained Spaces However, there are also cases where the space of possible outputs is constrained. This is often performed for tasks with a limited label space such as text classification or entity recognition, or multiplechoice question answering. To give some examples, Yin et al. (2019) manually design lists of words relating to relevant topics ("health", "finance", "politics", "sports", etc.), emotions ("anger", "joy", "sadness", "fear", etc.), or other aspects of the input text to be classified. Cui et al. (2021) manually design lists such as "person", "location", etc. for NER tasks. In these cases, it is necessary to have a mapping between the answer $\mathcal{Z}$ and the underlying class $\mathcal{Y}$.</p>
<p>With regards to multiple-choice question answering, it is common to use an LM to calculate the probability of an output among multiple choices, with Zweig et al. (2012) being an early example.</p>
<h3>5.2.2 Discrete Answer Search</h3>
<p>As with manually created prompts, it is possible that manually created answers are sub-optimal for getting the LM to achieve ideal prediction performance. Because of this, there is some work on automatic answer search, albeit less than that on searching for ideal prompts. These work on both discrete answer spaces (this section) and continuous answer spaces (the following).</p>
<p>Answer Paraphrasing These methods start with an initial answer space $\mathcal{Z}^{\prime}$, and then use paraphrasing to expand this answer space to broaden its coverage (Jiang et al., 2020b). Given a pair of answer and output $\left\langle\boldsymbol{z}^{\prime}, \boldsymbol{y}\right\rangle$, we define a function that generates a paraphrased set of answers $\operatorname{para}\left(\boldsymbol{z}^{\prime}\right)$. The probability of the final output is then defined as the marginal probability all of the answers in this paraphrase set $P(\boldsymbol{y} \mid \boldsymbol{x})=\sum_{\boldsymbol{z} \in \operatorname{para}\left(\boldsymbol{z}^{\prime}\right)} P(\boldsymbol{z} \mid \boldsymbol{x})$. This paraphrasing can be performed using any method, but Jiang et al. (2020b) specifically use a back-translation method, first translating into another language then back to generate a list of multiple paraphrased answers.</p>
<p>Prune-then-Search In these methods, first, an initial pruned answer space of several plausible answers $\mathcal{Z}^{\prime}$ is generated, and then an algorithm further searches over this pruned space to select a final set of answers. Note that in some of the papers introduced below, they define a function from label $\boldsymbol{y}$ to a single answer token $\boldsymbol{z}$, which is often called a verbalizer (Schick and Schütze, 2021a). Schick and Schütze (2021a); Schick et al. (2020) find tokens containing at least two alphabetic characters that are frequent in a large unlabeled dataset. In the search step, they iteratively compute a word's suitability as a representative answer $\boldsymbol{z}$ for a label $\boldsymbol{y}$ by maximizing the likelihood of the label over training data. Shin et al. (2020) learn a logistic classifier using the contextualized representation of the [Z] token as input. In the search step, they select the top- $k$ tokens that achieve the highest probability score using the learned logistic classifier in the first step. Those selected tokens will form the answer. Gao et al. (2021) first construct a pruned search space $\mathcal{Z}^{\prime}$ by selecting top- $k$ vocabulary words based on their generation probability at the [Z] position determined by training samples. Then the search space is further pruned down by only selecting a subset of words within $\mathcal{Z}^{\prime}$ based on their zero-shot accuracy on the training samples. (2) In the search step, they fine-tune the LM with fixed templates together with every answer mapping using training data and select the best label word as the answer based on the accuracy on the development set.</p>
<p>Label Decomposition When performing relation extraction, Chen et al. (2021b) automatically decompose each relation label into its constituent words and use them as an answer. For example, for the relation per:city_of_death, the decomposed label words would be {person, city, death}. The probability of the answer span will be calculated as the sum of each token's probability.</p>
<h3>5.2.3 Continuous Answer Search</h3>
<p>Very few works explore the possibility of using soft answer tokens which can be optimized through gradient descent. Hambardzumyan et al. (2021) assign a virtual token for each class label and optimize the token embedding for each</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 4: Different multi-prompt learning strategies. We use different colors to differentiate different components as follows. " $\square$ " for input text, " $\square$ " for prompt, " $\square$ " for answered prompt. " $\square$ " for sub-prompt. We use the following abbreviations. "PR" for prompt, "Ans-PR" for answered prompt, "Sub-PR" for sub-prompt.
class together with prompt token embeddings. Since the answer tokens are optimized directly in the embedding space, they do not make use of the embeddings learned by the LM and instead learn an embedding from scratch for each label.</p>
<h1>6 Multi-Prompt Learning</h1>
<p>The prompt engineering methods we discussed so far focused mainly on constructing a single prompt for an input. However, a significant body of research has demonstrated that the use of multiple prompts can further improve the efficacy of prompting methods, and we will call these methods multi-prompt learning methods. In practice, there are several ways to extend the single prompt learning to the use multiple prompts, which have a variety of motivations. We summarize representative methods in the "Multi-prompt Learning" section of Fig. 1 as well as Fig.4.</p>
<h3>6.1 Prompt Ensembling</h3>
<p>Prompt ensembling is the process of using multiple unanswered prompts for an input at inference time to make predictions. An example is shown in Fig. 4-(a). The multiple prompts can either be discrete prompts or continuous prompts. ${ }^{5}$ This sort of prompt ensembling can (1) leverage the complementary advantages of different prompts, (2) alleviate the cost of prompt engineering, since choosing one best-performing prompt is challenging, (3) stabilize performance on downstream tasks.</p>
<p>Prompt ensembling is connected to ensembling methods that are used to combine together multiple systems, which have a long history in machine learning (Ting and Witten, 1997; Zhou et al., 2002; Duh et al., 2011). Current research also borrows ideas from these works to derive effective ways for prompt ensembling, as described below.</p>
<p>Uniform averaging The most intuitive way to combine the predictions when using multiple prompts is to take the average of probabilities from different prompts. Concretely, this indicates that $P(\boldsymbol{z} \mid \boldsymbol{x}):=\frac{1}{K} \sum_{i}^{K} P\left(\boldsymbol{z} \mid f_{\text {prompt }, i}(\boldsymbol{x})\right)$ where $f_{\text {prompt }, i}(\cdot)$ is the $i$ th prompt in the prompt ensemble. Jiang et al. (2020c) first filter their prompts by selecting $K$ prompts that achieve the highest accuracy on the training set, and then use the average log probabilities obtained from the top $K$ prompts to calculate the probability for a single token at $[\mathrm{Z}]$ position when performing factual probing tasks. Schick and Schütze (2021a) also try a simple average when using an ensemble model to annotate an unlabeled dataset. When performing text generation evaluation, Yuan et al. (2021b) formulates this task as a text generation problem and take the average of the final generation scores obtained using different prompts.</p>
<p>Weighted averaging Simple uniform averaging of results from multiple prompts is easy to implement, but can also be suboptimal given that some prompts are more performant than others. To account for this, some works also</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>explore to use of weighted averages for prompt ensembling where each prompt is associated with a weight. The weights are typically pre-specified based on prompt performance or optimized using a training set. For example, Jiang et al. (2020c) learn the weight for each prompt by maximizing the probability of the target output over training data. Qin and Eisner (2021) use the same approach except that the weight for each prompt is optimized together with soft prompt parameters. Besides, Qin and Eisner (2021) also introduce a data-dependent weighting strategy where the probability of the input appearing in that prompt is considered in weighting different prompts as well. Schick and Schütze (2021a,b) set the weight for each prompt proportional to the accuracy on the training set before training.</p>
<p>Majority voting For classification tasks, majority voting can also be used to combine the results from different prompts (Lester et al., 2021; Hambardzumyan et al., 2021).</p>
<p>Knowledge distillation An ensemble of deep learning models can typically improve the performance, and this superior performance can be distilled into a single model using knowledge distillation (Allen-Zhu and Li, 2020). To incorporate this idea, Schick and Schütze (2021a,b, 2020) train a separate model for each manually-created template-answer pair, and use the ensemble of them to annotate an unlabeled dataset. Then the final model is trained to distill the knowledge from the annotated dataset. Gao et al. (2021) use a similar ensemble method on their automatically generated templates.</p>
<p>Prompt ensembling for text generation There is relatively little work on prompt ensembling for generation tasks (i.e. tasks where the answers is a string of tokens instead of a single one). A simple way to perform ensembling in this case is to use standard methods that generate the output based on the ensembled probability of the next word in the answer sequence $P\left(z_{t} \mid \boldsymbol{x}, z_{&lt;t}\right):=\frac{1}{K} \sum_{i}^{K} P\left(z_{t} \mid f_{\text {prompt }, i}(\boldsymbol{x}), z_{&lt;t}\right)$. In contrast, Schick and Schütze (2020) train a separate model for each prompt $f_{\text {prompt }, i}(\boldsymbol{x})$, and thus storing each of these fine-tuned LMs in memory is infeasible. Instead, they first decode generations using each model and then score each generation by averaging their generation probability across all models.</p>
<h1>6.2 Prompt Augmentation</h1>
<p>Prompt augmentation, also sometimes called demonstration learning (Gao et al., 2021), provides a few additional answered prompts that can be used to demonstrate how the LM should provide the answer to the actual prompt instantiated with the input $\boldsymbol{x}$. For example, instead of just providing a prompt of "China’s capital is [Z].", the prompt can be prefaced by a few examples such as "Great Britain’s capital is London. Japan’s capital is Tokyo. China’s capital is [Z]." Another example of performing addition of two numbers can be found in Fig. 4-(b). These few-shot demonstrations take advantage of the ability of strong language models to learn repetitive patterns (Brown et al., 2020).</p>
<p>Although the idea of prompt augmentation is simple, there are several aspects that make it challenging: (1) Sample Selection: how to choose the most effective examples? (2) Sample Ordering: How to order the chosen examples with the prompt?</p>
<p>Sample Selection Researchers have found that the choice of examples used in this few-shot scenario can result in very different performance, ranging from near state-of-the-art accuracy on some tasks to near random guess ( Lu et al., 2021). To address this issue, Gao et al. (2021); Liu et al. (2021a) utilize sentence embeddings to sample examples that are close to the input in this embedding space. To measure the generalization capability of pre-trained LMs to perform new tasks based on instructions, Mishra et al. (2021) provide both positive samples and negative samples that highlight things to avoid.</p>
<p>Sample Ordering Lu et al. (2021) found that the order of answered prompts provided to the model plays an important role in model performance, and propose entropy-based methods to score different candidate permutations. Kumar and Talukdar (2021) search for a good permutation of training examples as augmented prompts and learn a separator token between the prompts for further gains in performance.</p>
<p>Prompt augmentation is closely related to retrieval-based methods that provide more textual context to the model to improve performance (Guu et al., 2018), a method which has also been shown to be effective in prompt-based learning (Petroni et al., 2020). However, the key difference lies in the fact that prompt augmentation also leverages the template and answer, while larger context learning does not.</p>
<h3>6.3 Prompt Composition</h3>
<p>For those composable tasks, which can be composed based on more fundamental subtasks, we can also perform prompt composition, using multiple sub-prompts, each for one subtask, and then defining a composite prompt based on those sub-prompts. This process is illustrated in Fig. 4-(c). For example, in the relation extraction task, which aims to extract the relation of two entities, we can break down the task into several subtasks including identifying the characteristics of entities and classifying the relationships between entities. Based on this intuition, Han et al.</p>
<p>(2021) first use multiple manually created sub-prompts for entity recognition and relation classification and then compose them into a complete prompt based on logic rules for relation extraction.</p>
<h1>6.4 Prompt Decomposition</h1>
<p>For tasks where multiple predictions should be performed for one sample (e.g., sequence labeling), directly defining a holistic prompt with regards to the entire input text $\boldsymbol{x}$ is challenging. One intuitive method to address this problem is to break down the holistic prompt into different sub-prompts, and then answer each sub-prompt separately. Fig.4-(d) illustrates this idea with an example from the named entity recognition task, which aims to identify all named entities in an input sentence. In this case, the input will first be converted into a set of text spans, and the model can then be prompted to predict the entity type (including "Not an Entity") for each span. It is not easy to predict all the span types at the same time due to the large number of spans, so different prompts for each span can be created and predicted separately. This sort of prompt decomposition for named entity recognition has been explored by Cui et al. (2021) where they apply the approach we discussed here.</p>
<h2>7 Training Strategies for Prompting Methods</h2>
<p>With the methods in the above sections, it is now clear how to obtain an appropriate prompt (or prompts) and corresponding answers. Now we discuss about methods that explicitly train models in concert with prompting methods, as outlined in the "Training Strategies" section of Fig.1.</p>
<h3>7.1 Training Settings</h3>
<p>In many cases, prompting methods can be used without any explicit training of the LM for the down-stream task, simply taking an LM that has been trained to predict the probability of text $P(\boldsymbol{x})$ and applying it as-is to fill the cloze or prefix prompts defined to specify the task. This is traditionally called the zero-shot setting, as there is zero training data for the task of interest.</p>
<p>However, there are also methods that use training data to train the model in concert with prompting methods. These consist of either full-data learning, where a reasonably large number of training examples are used to train the model, or few-shot learning where a very small number of examples are used to train the model. Prompting methods are particularly useful in the latter case, as there are generally not enough training examples to fully specify the desired behavior, and thus using a prompt to push the model in the right direction is particularly effective.</p>
<p>One thing to note is that for many of the prompt engineering methods described in $\S 4$, although annotated training samples are not explicitly used in the training of the downstream task model, they are often used in the construction or validation of the prompts that the downstream task will use. As noted by Perez et al. (2021), this is arguably not true zero-shot learning with respect to the downstream task.</p>
<h3>7.2 Parameter Update Methods</h3>
<p>In prompt-based downstream task learning, there are usually two types of parameters, namely those from (1) pre-trained models and (2) prompts. Which part of parameters should be updated is one important design decision, which can lead to different levels of applicability in different scenarios. We summarize five tuning strategies (as shown in Tab. 6) based on (i) whether the parameters of the underlying LM are tuned, (ii) whether there are additional prompt-related parameters, (iii) if there are additional prompt-related parameters, whether those parameters are tuned.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Strategy</th>
<th style="text-align: center;">LM Params</th>
<th style="text-align: center;">Prompt Params</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Additional</td>
<td style="text-align: center;">Tuned</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Promptless Fine-tuning</td>
<td style="text-align: center;">Tuned</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ELMo [130], BERT [32], BART [94]</td>
</tr>
<tr>
<td style="text-align: center;">Tuning-free Prompting</td>
<td style="text-align: center;">Frozen</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">GPT-3 [16], AutoPrompt [159], LAMA [133]</td>
</tr>
<tr>
<td style="text-align: center;">Fixed-LM Prompt Tuning</td>
<td style="text-align: center;">Frozen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Tuned</td>
<td style="text-align: center;">Prefix-Tuning [96], Prompt-Tuning [91]</td>
</tr>
<tr>
<td style="text-align: center;">Fixed-prompt LM Tuning</td>
<td style="text-align: center;">Tuned</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">PET-TC [153], PET-Gen [152], LM-BFF [46]</td>
</tr>
<tr>
<td style="text-align: center;">Prompt+LM Fine-tuning</td>
<td style="text-align: center;">Tuned</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Tuned</td>
<td style="text-align: center;">PADA [8], P-Tuning [103], PTR [56]</td>
</tr>
</tbody>
</table>
<p>Table 6: Characteristics of different tuning strategies. "Additional" represents if there are additional parameters beyond LM parameters while "Tuned" denotes if parameters are updated.</p>
<h1>7.2.1 Promptless Fine-tuning</h1>
<p>As mentioned in the introduction, the pre-train and fine-tune strategy has been widely used in NLP since before the popularization of prompting methods. Here we refer to pre-training and fine-tuning without prompts as promptless fine-tuning, to contrast with the prompt-based learning methods introduced in the following sections. In this strategy, given a dataset of a task, all (or some (Howard and Ruder, 2018; Peters et al., 2019)) of the parameters of the pre-trained LM will be updated via gradients induced from downstream training samples. Typical examples of pre-trained models tuned in this way include BERT [32] and RoBERTa [105]. This is a simple, powerful, and widely-used method, but it may overfit or not learn stably on small datasets (Dodge et al., 2020). Models are also prone to catastrophic forgetting, where the LM loses its ability to do things that it was able to do before fine-tuning (McCloskey and Cohen, 1989).</p>
<ul>
<li>Advantages: Simplicity, no need for prompt design. Tuning all the LM parameters allows the model to fit to larger training datasets.</li>
<li>Disadvantages: LMs may overfit or not learn stably on smaller datasets.</li>
</ul>
<h3>7.2.2 Tuning-free Prompting</h3>
<p>Tuning-free prompting directly generates the answers without changing the parameters of the pre-trained LMs based only on a prompt, as described in the simplest incarnation of prompting in $\S 2$. These can be optionally augmenting input with answered prompts as described in $\S 6.2$, and this combination of tuning-free prompting and prompt augmentation is also referred to as in-context learning (Brown et al., 2020). Typical examples of tuning-free prompting include LAMA [133] and GPT-3 [16].</p>
<ul>
<li>Advantages: Efficiency, there is no parameter update process. No catastrophic forgetting, as LM parameters remain fixed. Applicable in zero-shot settings.</li>
<li>Disadvantages: Because prompts are the only method that provide the task specification, heavy engineering is necessary to achieve high accuracy. In particular in the in-context learning setting, providing many answered prompts can be slow at test time, and thus cannot easily use large training datasets.</li>
</ul>
<h3>7.2.3 Fixed-LM Prompt Tuning</h3>
<p>In the scenario where additional prompt-relevant parameters are introduced besides parameters of the pre-trained model, fixed-LM prompt tuning updates only the prompts' parameters using the supervision signal obtained from the downstream training samples, while keeping the entire pre-trained LM unchanged. Typical examples are Prefix-Tuning [96] and WARP [55].</p>
<ul>
<li>Advantages: Similarly to tuning-free prompting, it can retain knowledge in LMs and is suitable in few-shot scenarios. Often superior accuracy to tuning-free prompting.</li>
<li>Disadvantages: Not applicable in zero-shot scenarios. While effective in few-shot scenarios, representation power is limited in large-data settings. Prompt engineering through choice of hyperparameters or seed prompts is necessary. Prompts are usually not human-interpretable or manipulable.</li>
</ul>
<h3>7.2.4 Fixed-prompt LM Tuning</h3>
<p>Fixed-prompt LM tuning tunes the parameters of the LM, as in the standard pre-train and fine-tune paradigm, but additionally uses prompts with fixed parameters to specify the model behavior. This potentially leads to improvements, particularly in few-shot scenarios.</p>
<p>The most natural way to do so is to provide a discrete textual template that is applied to every training and test example. Typical examples include PET-TC [153], PET-Gen [152], LM-BFF [46]. Logan IV et al. (2021) more recently observe that the prompt engineering can be reduced by allowing for a combination of answer engineering and partial LM fine-tuning. For example, they define a very simple template, null prompt, where the input and mask are directly concatenated " $[\mathrm{X}][\mathrm{Z}]$ " without any template words, and find this achieves competitive accuracy.</p>
<ul>
<li>Advantages: Prompt or answer engineering more completely specify the task, allowing for more efficient learning, particularly in few-shot scenarios.</li>
<li>Disadvantages: Prompt or answer engineering are still required, although perhaps not as much as without prompting. LMs fine-tuned on one downstream task may not be effective on another one.</li>
</ul>
<h1>7.2.5 Prompt+LM Tuning</h1>
<p>In this setting, there are prompt-relevant parameters, which can be fine-tuned together with the all or some of the parameters of the pre-trained models. Representative examples include PADA [8], P-Tuning [103]. Notably, this setting is very similar to the standard pre-train and fine-tune paradigm, but the addition of the prompt can provide additional bootstrapping at the start of model training.</p>
<ul>
<li>Advantages: This is the most expressive method, likely suitable for high-data settings.</li>
<li>Disadvantages: Requires training and storing all parameters of the models. May overfit to small datasets.</li>
</ul>
<h2>8 Applications</h2>
<p>In previous sections, we examined prompting methods from the point of view of the mechanism of the method itself. In this section, we rather organize prompting methods from the point of view of which applications they have been applied to. We list these applications in Tab. 7-8 and summarize them in the following sections.</p>
<h3>8.1 Knowledge Probing</h3>
<p>Factual Probing Factual probing (a.k.a. fact retrieval) is one of the earliest scenarios with respect to which prompting methods were applied. The motivation of exploring this task is to quantify how much factual knowledge the pre-trained LM's internal representations bear. In this task, parameters of pre-trained models are usually fixed, and knowledge is retrieved by transforming the original input into a cloze prompt as defined in $\S 2.2$, which can be manually crafted or automatically discovered. Relevant datasets including LAMA (Petroni et al., 2019) and X-FACTR (Jiang et al., 2020a). Since the answers are pre-defined, fact retrieval only focuses on finding effective templates and analyzing the results of different models using these templates. Both discrete template search (Petroni et al., 2019, 2020; Jiang et al., 2020c,a; Haviv et al., 2021; Shin et al., 2020; Perez et al., 2021) and continuous template learning (Qin and Eisner, 2021; Liu et al., 2021b; Zhong et al., 2021b) have been explored within this context, as well as prompt ensemble learning (Jiang et al., 2020c; Qin and Eisner, 2021).</p>
<p>Linguistic Probing Besides factual knowledge, large-scale pre-training also allows LMs to handle linguistic phenomena such as analogies (Brown et al., 2020), negations (Ettinger, 2020), semantic role sensitivity (Ettinger, 2020), semantic similarity (Sun et al., 2021), cant understanding (Sun et al., 2021), and rare word understanding (Schick and Schütze, 2020). The above knowledge can also be elicited by presenting linguistic probing tasks in the form of natural language sentences that are to be completed by the LM.</p>
<h3>8.2 Classification-based Tasks</h3>
<p>Prompt-based learning has been widely explored in classification-based tasks where prompt templates can be constructed relatively easily, such as text classification (Yin et al., 2019) and natural language inference (Schick and Schütze, 2021a). The key to prompting for classification-based tasks is reformulating it as an appropriate prompt. For example, Yin et al. (2019) use a prompt such as "the topic of this document is [Z],", which is then fed into mask pre-trained LMs for slot filling.</p>
<p>Text Classification For text classification tasks, most previous work has used cloze prompts, and both prompt engineering (Gao et al., 2021; Hambardzumyan et al., 2021; Lester et al., 2021) and answer engineering (Schick and Schütze, 2021a; Schick et al., 2020; Gao et al., 2021) have been explored extensively. Most existing works explore the efficacy of prompt learning for text classification in the context of few-shot setting with "fixed-prompt LM Tuning" strategies (defined in §7.2.4).</p>
<p>Natural Language Inference (NLI) NLI aims to predict the relationship (e.g., entailment) of two given sentences. Similar to text classification tasks, for natural language inference tasks, cloze prompts are commonly used (Schick and Schütze, 2021a). Regarding prompt engineering, researchers mainly focus on the template search in the few-shot learning setting and the answer space $\mathcal{Z}$ is usually manually pre-selected from the vocabulary.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Work</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">PLM</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Prompt Engineering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer Engineering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tuning</th>
<th style="text-align: center;">Mul-Pr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shape</td>
<td style="text-align: center;">Man</td>
<td style="text-align: center;">Auto</td>
<td style="text-align: center;">Shape</td>
<td style="text-align: center;">Man</td>
<td style="text-align: center;">Auto</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LMComm [173]</td>
<td style="text-align: center;">CR</td>
<td style="text-align: center;">L2R</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 [140]</td>
<td style="text-align: center;">CR,QA <br> SUM,MT</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Zero,Few</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">WNLaMPro [150]</td>
<td style="text-align: center;">LCP</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LMDiagnose [39]</td>
<td style="text-align: center;">CR,LCP</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AdvTrigger [177]</td>
<td style="text-align: center;">GCG</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CobRank [31]</td>
<td style="text-align: center;">CKM</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LAMA [133]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">Conv,Trans <br> ELMo,BERT</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CTRL [75]</td>
<td style="text-align: center;">GCG</td>
<td style="text-align: center;">CTRL</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">T5 [141]</td>
<td style="text-align: center;">TC,SUM <br> QA,MT</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Neg \&amp; Mis [74]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">Trans,ELMo <br> BERT</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LPAQA [68]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">BERT,ERNIE</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">ZSC [135]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PET-TC [153]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">RoBERTa,XLM-R</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">ContxFP [132]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">BERT,RoBERTa</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA [76]</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">T5,BART</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Prefix</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RAG [95]</td>
<td style="text-align: center;">QA,GCG,TC</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMPT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 [16]</td>
<td style="text-align: center;">QA,MT,GCG <br> CR,TC,LCP <br> MR,SR,AR</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Zero,Few</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">CommS2S [187]</td>
<td style="text-align: center;">CR</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PET-SGLUE [154]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">ToxicityPrompts [47]</td>
<td style="text-align: center;">GCG</td>
<td style="text-align: center;">GPT-1,GPT-2 <br> GPT-3,CTRL</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">WhyLM [147]</td>
<td style="text-align: center;">Theory</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">X-FACTR [66]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">mBERT,BERT <br> XLM,XLM-R</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Petal [149]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">AutoPrompt [159]</td>
<td style="text-align: center;">TC,FP,IE</td>
<td style="text-align: center;">BERT,RoBERTa</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CTRLsum [59]</td>
<td style="text-align: center;">SUM</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PET-Gen [152]</td>
<td style="text-align: center;">SUM</td>
<td style="text-align: center;">PEGASUS</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">LM-BFF [46]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">PE,PA</td>
</tr>
<tr>
<td style="text-align: center;">WARP [55]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Few,Full</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-Tuning [96]</td>
<td style="text-align: center;">D2T,SUM</td>
<td style="text-align: center;">GPT-2,BART</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">KATE [100]</td>
<td style="text-align: center;">TC,D2T,QA</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">PromptProg [145]</td>
<td style="text-align: center;">MT,MR <br> AR,QA</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Zero,Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">ContxCalibrate [201]</td>
<td style="text-align: center;">TC,FP,IE</td>
<td style="text-align: center;">GPT-2,GPT-3</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">PADA [8]</td>
<td style="text-align: center;">TC,TAG</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LMPT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SD [155]</td>
<td style="text-align: center;">GCG</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERTese [58]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Prompt2Data [148]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">P-Tuning [103]</td>
<td style="text-align: center;">FP,TC</td>
<td style="text-align: center;">GPT-2,BERT <br> ALBERT</td>
<td style="text-align: center;">Few,Full</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP,LMPT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GLM [37]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">GLM</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 7: An organization of works on prompting (Part 1). See the caption of Tab. 8 for a detailed description for all the abbreviations used in this table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Work</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">PLM</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Prompt Engineering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer Engineering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tuning</th>
<th style="text-align: center;">Mul-Pr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shape</td>
<td style="text-align: center;">Man</td>
<td style="text-align: center;">Auto</td>
<td style="text-align: center;">Shape</td>
<td style="text-align: center;">Man</td>
<td style="text-align: center;">Auto</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADAPET [170]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Meta [202]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OptiPrompt [203]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Soft [137]</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">BERT,BART <br> RoBERTa</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">DINO [151]</td>
<td style="text-align: center;">GCG</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AdaPrompt [21]</td>
<td style="text-align: center;">IE</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Few,Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$ [62]</td>
<td style="text-align: center;">GCG,QA,TC</td>
<td style="text-align: center;">GPT-2,GPT-3</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-Tuning [91]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">Natural-Instr [120]</td>
<td style="text-align: center;">GCG</td>
<td style="text-align: center;">GPT-3,BART</td>
<td style="text-align: center;">Few,Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP,LMT</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">OrderEntropy [111]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">GPT-2,GPT-3</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">FewshotSemp [158]</td>
<td style="text-align: center;">SEMP</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">PanGu- $\alpha$ [194]</td>
<td style="text-align: center;">QA,CR,TC <br> SUM,GCG</td>
<td style="text-align: center;">PanGu- $\alpha$</td>
<td style="text-align: center;">Zero,Few</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">TrueFewshot [129]</td>
<td style="text-align: center;">TC,FP</td>
<td style="text-align: center;">GPT-2,GPT-3 <br> ALBERT</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP,LMT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PTR [56]</td>
<td style="text-align: center;">IE</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMPT</td>
<td style="text-align: center;">PC</td>
</tr>
<tr>
<td style="text-align: center;">TemplateNER [29]</td>
<td style="text-align: center;">TAG</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Few,Full</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">PD</td>
</tr>
<tr>
<td style="text-align: center;">PERO [83]</td>
<td style="text-align: center;">TC,FP</td>
<td style="text-align: center;">BERT,RoBERTa</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">PromptAnalysis [181]</td>
<td style="text-align: center;">Theory</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CPM-2 [198]</td>
<td style="text-align: center;">QA,MR,SUM <br> TC,GCG,MT</td>
<td style="text-align: center;">CPM-2</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Tok,Sp,Sent</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">PT,LMPT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore [193]</td>
<td style="text-align: center;">EVALG</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">PE</td>
</tr>
<tr>
<td style="text-align: center;">NullPrompt [109]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">RoBERTa,ALBERT</td>
<td style="text-align: center;">Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMPT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Frozen [174]</td>
<td style="text-align: center;">VQA,VFP,MG</td>
<td style="text-align: center;">GPT-like</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Cont</td>
<td style="text-align: center;">Sp (Visual)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">ERNIE-B3 [167]</td>
<td style="text-align: center;">TC,LCP,NLI <br> CR,QA,SUM <br> GCG</td>
<td style="text-align: center;">ERNIE-B3</td>
<td style="text-align: center;">Zero</td>
<td style="text-align: center;">Clo,Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">TFP</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Codex [20]</td>
<td style="text-align: center;">CodeGen</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">Zero,Few <br> Full</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Span</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">TFP,LMT</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">HTLM [1]</td>
<td style="text-align: center;">TC,SUM</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Zero,Few <br> Full</td>
<td style="text-align: center;">Clo</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Disc</td>
<td style="text-align: center;">Tok,Sp,Sen</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">PA</td>
</tr>
<tr>
<td style="text-align: center;">FLEX [15]</td>
<td style="text-align: center;">TC</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Zero,Few</td>
<td style="text-align: center;">Pre</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Tok,Sp</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LMT</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 8: An organization of works on prompting (Part 2). The Task column lists the tasks that are performed in corresponding papers. We use the following abbreviations. CR: Commonsense Reasoning. QA: Question Answering. SUM: Summarization. MT: Machine Translation. LCP: Linguistic Capacity Probing. GCG: General Conditional Generation. CKM: Commonsense Knowledge Mining. FP: Fact Probing. TC: Text Classification. MR: Mathematical Reasoning. SR: Symbolic Reasoning. AR: Analogical Reasoning. Theory: Theoretical Analysis. IE: Information Extraction. D2T: Data-to-text. TAG: Sequence Tagging. SEMP: Semantic Parsing. EVALG: Evaluation of Text Generation. VQA: Visual Question Answering. VFP: Visual Fact Probing. MG: Multimodal Grounding. CodeGen: Code generation. The PLM column lists all the pre-trained LMs that have been used in corresponding papers for downstream tasks. GPT-like is an autoregressive language model which makes small modifications to the original GPT-2 architecture. For other pre-trained LMs, please refer to $\S 3$ for more information. Setting column lists the settings for prompt-based learning, can be zero-shot learning (Zero), few-shot learning (Few), fully supervised learning (Full). Under Prompt Engineering, Shape denotes the shape of the template (Clo for cloze and Pre for prefix), Man denotes whether human effort is needed, Auto denotes data-driven search methods (Disc for discrete search, Cont for continuous search). Under Answer Engineering, Shape indicates the shape of the answer (Tok for token-level, Sp for span-level, Sen for sentence- or document-level), and Man and Auto are the same as above. The Tuning column lists tuning strategies (§7). TFP: Tuning-free Prompting. LMT: Fixed-prompt LM Tuning. PT: Fixed-LM Prompt Tuning. LMPT: LM+Prompt Tuning. The Mul-Pr column lists multi-prompt learning methods. PA: Prompt Augmentation. PE: Prompt Ensembling. PC: Prompt Composition. PD: Prompt Decomposition.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Multiple continuous prompts are typically learned by using different initializations or different random seeds.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
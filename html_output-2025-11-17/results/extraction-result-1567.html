<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1567 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1567</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1567</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-ca0caee3687905dc4175c3db4f07c6e54eff2a3c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ca0caee3687905dc4175c3db4f07c6e54eff2a3c" target="_blank">LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work presents a deep RL agent—LeDeepChef—that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions, and uses an actor-critic framework and prune the action-space to build an agent that achieves high scores across a whole family of games.</p>
                <p><strong>Paper Abstract:</strong> While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent—LeDeepChef—that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's First TextWorld Problems: A Language and Reinforcement Learning Challenge and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1567.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1567.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeDeepChef</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LeDeepChef Deep Reinforcement Learning Agent for Families of Text-Based Games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic text-game agent that combines context-aware GRU encoders with action-space pruning via a supervised 'Recipe Manager' and a supervised 'Navigator', and groups low-level actions into high-level commands to learn families of cooking text-based games and generalize to unseen houses and recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LeDeepChef</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Advantage actor-critic model with shared network: context encoding from eight textual features (each via Bi-GRU then an over-time GRU), command encoding (GRU per command), MLP-based value head and MLP-based command scoring producing a softmax policy. Uses supervised submodules (Recipe Manager, Navigator) to prune and abstract the admissible action set, including grouped high-level commands (e.g., 'take all required ingredients from here', 'drop unnecessary items').</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Microsoft's First TextWorld Problems challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A family of interactive text-based games in which the agent interacts solely via natural-language commands and receives textual observations; games in the challenge share a cooking theme but vary in room layout, object instances, and recipe goals. Interactions include navigation commands, object manipulations (take, drop, use), examination, inventory queries, and recipe-specific actions (slice, fry, prepare meal, eat).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks (cooking procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Find cookbook, collect listed ingredients, transform ingredients to required states (e.g., slice, fry), prepare meal, eat meal; navigate house rooms and open doors as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Hierarchical and compositional: tasks decompose into subtasks (navigation, ingredient collection, per-ingredient cooking actions, using utilities), and commands compose primitive actions; LeDeepChef explicitly groups many primitive actions into higher-level commands to reduce combinatorial action composition.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Yes — evaluated on unseen validation and test sets of TextWorld cooking games: achieved mean score 74.4% (validation, hidden set) and 69.3% (test), showing strong transfer to never-before-seen recipes and house layouts compared to standard baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reducing the action-space via supervised modules (Recipe Manager and Navigator) and grouping actions into high-level commands substantially improves learning stability and generalization; actor-critic training on the pruned/abstracted command set achieves much higher performance (LeDeepChef: 74.4% valid, 69.3% test) than standard DQN/DRRN baselines on the same family of games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1567.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1567.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yin&May2019b (curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-game approach (referenced) that explicitly uses curriculum learning together with map familiarization and common-sense techniques to train a DQN-based agent on families of TextWorld cooking games; reported validation performance is provided in the LeDeepChef paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN (curriculum-trained variant described in Yin & May 2019b)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in the referenced work as a DQN model trained with a curriculum learning protocol combined with map familiarization and common-sense components; the current paper does not provide architecture details beyond this description.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (families of cooking text-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same family of TextWorld cooking games used in the competition: text-only interactions, recipe discovery via cookbook, navigation among rooms possibly with closed doors, collecting and transforming ingredients to meet recipe directions.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks (cooking procedures) — commonsense navigation and recipe execution</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Learning to cook new recipes in new house layouts; map familiarization (learning house connectivity) and executing multi-step recipe procedures (collect, slice, fry, prepare, eat).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures are sequential and compositional (navigation + multi-step per-ingredient transformations), and the referenced method reportedly uses curriculum elements such as map familiarization to structure learning, but exact compositional curriculum decomposition is not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>curriculum learning (with map familiarization and common-sense augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The LeDeepChef paper cites Yin & May 2019b as using a curriculum-learning approach combined with map familiarization and common-sense techniques to train a DQN on TextWorld families; however, the LeDeepChef paper provides no further procedural details about the curriculum structure, ordering, or pacing — only that the method is curriculum-based and applied to the same TextWorld family.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Reported as 58% on Yin & May's validation set (hold-out from the official training games) according to the LeDeepChef paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Reported moderate generalization: the curriculum-trained DQN achieved 58% on its validation hold-out (reported in LeDeepChef), indicating improved performance relative to many standard baselines, but LeDeepChef outperforms it on the competition's unseen validation/test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>As cited here, Yin & May's curriculum-based approach attained substantially better validation performance (58%) than standard out-of-the-box baselines on similar TextWorld tasks, suggesting curriculum learning (and map familiarization/common-sense components) can improve learning on families of compositional text-based cooking tasks; exact mechanisms and quantitative comparisons vs. non-curriculum ablations are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games <em>(Rating: 2)</em></li>
                <li>Comprehensible context-driven text game playing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1567",
    "paper_id": "paper-ca0caee3687905dc4175c3db4f07c6e54eff2a3c",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "LeDeepChef",
            "name_full": "LeDeepChef Deep Reinforcement Learning Agent for Families of Text-Based Games",
            "brief_description": "An actor-critic text-game agent that combines context-aware GRU encoders with action-space pruning via a supervised 'Recipe Manager' and a supervised 'Navigator', and groups low-level actions into high-level commands to learn families of cooking text-based games and generalize to unseen houses and recipes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LeDeepChef",
            "agent_description": "Advantage actor-critic model with shared network: context encoding from eight textual features (each via Bi-GRU then an over-time GRU), command encoding (GRU per command), MLP-based value head and MLP-based command scoring producing a softmax policy. Uses supervised submodules (Recipe Manager, Navigator) to prune and abstract the admissible action set, including grouped high-level commands (e.g., 'take all required ingredients from here', 'drop unnecessary items').",
            "agent_size": null,
            "environment_name": "TextWorld (Microsoft's First TextWorld Problems challenge)",
            "environment_description": "A family of interactive text-based games in which the agent interacts solely via natural-language commands and receives textual observations; games in the challenge share a cooking theme but vary in room layout, object instances, and recipe goals. Interactions include navigation commands, object manipulations (take, drop, use), examination, inventory queries, and recipe-specific actions (slice, fry, prepare meal, eat).",
            "procedure_type": "household tasks (cooking procedures)",
            "procedure_examples": "Find cookbook, collect listed ingredients, transform ingredients to required states (e.g., slice, fry), prepare meal, eat meal; navigate house rooms and open doors as needed.",
            "compositional_structure": "Hierarchical and compositional: tasks decompose into subtasks (navigation, ingredient collection, per-ingredient cooking actions, using utilities), and commands compose primitive actions; LeDeepChef explicitly groups many primitive actions into higher-level commands to reduce combinatorial action composition.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Yes — evaluated on unseen validation and test sets of TextWorld cooking games: achieved mean score 74.4% (validation, hidden set) and 69.3% (test), showing strong transfer to never-before-seen recipes and house layouts compared to standard baselines.",
            "key_findings": "Reducing the action-space via supervised modules (Recipe Manager and Navigator) and grouping actions into high-level commands substantially improves learning stability and generalization; actor-critic training on the pruned/abstracted command set achieves much higher performance (LeDeepChef: 74.4% valid, 69.3% test) than standard DQN/DRRN baselines on the same family of games.",
            "uuid": "e1567.0",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Yin&May2019b (curriculum)",
            "name_full": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games",
            "brief_description": "A text-game approach (referenced) that explicitly uses curriculum learning together with map familiarization and common-sense techniques to train a DQN-based agent on families of TextWorld cooking games; reported validation performance is provided in the LeDeepChef paper.",
            "citation_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games",
            "mention_or_use": "mention",
            "agent_name": "DQN (curriculum-trained variant described in Yin & May 2019b)",
            "agent_description": "Described in the referenced work as a DQN model trained with a curriculum learning protocol combined with map familiarization and common-sense components; the current paper does not provide architecture details beyond this description.",
            "agent_size": null,
            "environment_name": "TextWorld (families of cooking text-based games)",
            "environment_description": "Same family of TextWorld cooking games used in the competition: text-only interactions, recipe discovery via cookbook, navigation among rooms possibly with closed doors, collecting and transforming ingredients to meet recipe directions.",
            "procedure_type": "household tasks (cooking procedures) — commonsense navigation and recipe execution",
            "procedure_examples": "Learning to cook new recipes in new house layouts; map familiarization (learning house connectivity) and executing multi-step recipe procedures (collect, slice, fry, prepare, eat).",
            "compositional_structure": "Procedures are sequential and compositional (navigation + multi-step per-ingredient transformations), and the referenced method reportedly uses curriculum elements such as map familiarization to structure learning, but exact compositional curriculum decomposition is not detailed in this paper.",
            "uses_curriculum": true,
            "curriculum_name": "curriculum learning (with map familiarization and common-sense augmentation)",
            "curriculum_description": "The LeDeepChef paper cites Yin & May 2019b as using a curriculum-learning approach combined with map familiarization and common-sense techniques to train a DQN on TextWorld families; however, the LeDeepChef paper provides no further procedural details about the curriculum structure, ordering, or pacing — only that the method is curriculum-based and applied to the same TextWorld family.",
            "curriculum_ordering_principle": null,
            "task_complexity_range": null,
            "performance_with_curriculum": "Reported as 58% on Yin & May's validation set (hold-out from the official training games) according to the LeDeepChef paper.",
            "performance_without_curriculum": null,
            "has_curriculum_comparison": null,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Reported moderate generalization: the curriculum-trained DQN achieved 58% on its validation hold-out (reported in LeDeepChef), indicating improved performance relative to many standard baselines, but LeDeepChef outperforms it on the competition's unseen validation/test sets.",
            "key_findings": "As cited here, Yin & May's curriculum-based approach attained substantially better validation performance (58%) than standard out-of-the-box baselines on similar TextWorld tasks, suggesting curriculum learning (and map familiarization/common-sense components) can improve learning on families of compositional text-based cooking tasks; exact mechanisms and quantitative comparisons vs. non-curriculum ablations are not provided in this paper.",
            "uuid": "e1567.1",
            "source_info": {
                "paper_title": "LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games",
            "rating": 2
        },
        {
            "paper_title": "Comprehensible context-driven text game playing",
            "rating": 1
        }
    ],
    "cost": 0.0097895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LeDeepChef Deep Reinforcement Learning Agent for Families of Text-Based Games</h1>
<p>Leonard Adolphs, Thomas Hofmann<br>Department of Computer Science<br>ETH Zurich<br>{firstname.lastname}@inf.ethz.ch</p>
<h4>Abstract</h4>
<p>While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of TextBased Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent-LeDeepChef-that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's First TextWorld Problems: A Language and Reinforcement Learning Challenge and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.</p>
<h2>Introduction</h2>
<p>"You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!", that's the starting instruction of every game in Microsoft's First TextWorld Problems: A Language and Reinforcement Learning Challenge; a competition that evaluates an agent on a family of unique and unseen text-based games (TBGs). While all the games share a similar theme-cooking in a modern house environmentthey differ in multiple aspects like the number of rooms, connection, and arrangement of rooms, the goal of the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>game (i.e., different recipes), as well as actions and tools needed to succeed. TBGs are computer games where the sole modality of interaction is text. In an iterative process, the player issues commands in natural language and, in return, is presented a (partial) textual description of the environment. The player works towards goals that may or may not be specified explicitly and receives rewards upon completion. To frame it more formally, both the observation and action-space are comprised of natural language and, thus, inherit its combinatorial and compositional properties (Côté et al. 2018). Training an agent to succeed in such games requires overcoming several common research challenges in reinforcement learning (RL), such as partial observability, large and sparse state-, and action-space and long term credit assignment. Moreover, the agent needs several human-like abilities including understanding the environment's feedback (e.g., realize that some command had no effect on the game's state), and common sense reasoning (e.g., extracting affordance verbs to an object in the game) (Fulda et al. 2017).</p>
<p>While TBGs reached their peak of popularity in the 1980s with games like Zork (Infocom 1980), they provide an interesting test-bed for AI agents today. Due to the dialoglike structure of the game and the goal to find a policy that maximizes the player's reward, they show great similarity to real-world tasks like question answering and open dialogue generation. Games like Zork are usually contained in a single environment that requires a variety of complex problem-solving abilities. The TextWorld framework (Côté et al. 2018) instead, generates a family of games with different worlds and properties but with straightforward and, most importantly, similar tasks. One can argue, that it is, therefore, more similar to human skill acquisition: once learned, a skill can easily be performed even in a slightly different environment or with new objects (Yin and May 2019b). Recent research has mainly focused on either learning a single TBG to high accuracy (Narasimhan, Kulkarni, and Barzilay 2015; He et al. 2015; Ammanabrolu and Riedl 2018) or generalization to a completely new family of games (Kostka et al. 2017) with only very poor performance. Microsoft's First TextWorld Problems: A Language and Reinforcement</p>
<p>Learning Challenge aims to cover a new research direction, that is in-between the two extremes of the single game and the general game setting. To succeed here, an agent needs to have generalization capabilities that allow it to transfer its learned cooking skills to never-before-seen recipes in unfamiliar house environments.
In this work, we present our final agent-LeDeepChef that achieved the high score on the (hidden) validation games and was ranked second in the overall competition. The code to train the agent, as well as an exemplary walkthrough of the game (with the agent ranking next moves), can be found on GitHub ${ }^{1}$. In order to design a successful agent, we make the following contributions:</p>
<ul>
<li>We design an architecture that uses different parts of the context to rank a set of commands, that is trained within an actor-critic framework. Through recurrency over timesteps, we construct a model that is aware of the past context and its previous decisions.</li>
<li>We improve generalization to unseen environments by abstracting away standard to "high-level" commands similar to feudal learning approaches (Dayan and Hinton 1993). We show that this reduces the action-space and therefore accelerates and stabilizes the optimization procedure.</li>
<li>We incorporate a task-specific module that predicts the missing steps to complete the task. We train it supervised on a dataset based on TextWorld recipes augmented with a list of the most common food items found in freebase to make it resilient to unseen recipes and ingredients.
The paper is organized as follows. Section "Related Work" gives an overview of the current state of research in the field of TBGs. In section "Gameplay", we explain the TextWorld challenge in more detail and provide an exemplary walkthrough. The architecture, as well as the RL training procedure of our final agent, is described in section "Agent". Section "Command Generation" presents our command generation approach. Finally, the section "Results" compares the performance of our agent to several reasonable baselines.</li>
</ul>
<h2>Related Work</h2>
<p>Since the pioneering work of (Mnih et al. 2013) that combines deep neural networks with reinforcement learning techniques to successfully play Atari games, there has been an increasing interest to modify these algorithms to a variety of problems. However, due to the combinatorial and compositional property of natural language, resulting in huge action- and state-spaces, no major improvements have been made in this area. Text-based games are regarded as a good testbed for research at the intersection of RL and NLP (Côté et al. 2018). Even though they heavily simplify the environment-compared to, e.g., a real-world open dialogue-they present a broad spectrum of challenges for learning algorithms.</p>
<p>Deep RL for TBGs To solve TBGs, (Narasimhan, Kulkarni, and Barzilay 2015) developed a deep RL model that</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>utilizes the representational power of the hidden state of Long Short-Term Memory (Hochreiter and Schmidhuber 1997) to learn a Q-function. An adaption of this approach by He et al. (2015), uses two separate models to encode the context and commands individually, and then uses a pairwise interaction function between them to compute the Q values. Since then, a variety of researchers (Ammanabrolu and Riedl 2018; Yin and May 2019b; Yin and May 2019a) used some form of DQN to solve TBGs; however, we find that an advantage-actor-critic approach (Mnih et al. 2016) helps to improve performance and speeds up convergence. Using Narasimhan, Kulkarni, and Barzilay (2015)'s LSTMDQN or He et al. (2015)'s adjusted DRRN on the family of games of the TextWorld challenge leads to extremely slow convergence due to the huge combinatorial action-space that arises from games with different objects and the combinatorial nature of natural language (Ammanabrolu and Riedl 2018).</p>
<p>Large action-space Text-based games can be divided by their type of input-interaction: (i) parser-based, where the agent issues commands in free form and (ii) choice-based, where the agent is presented a set of admissible commands at every turn. Assuming a fixed maximum length of the commands as well as a fixed-size vocabulary, a parser-based game is a special instance of a choice-based game with the set of all possible combinations of words in the vocabulary as the set of admissible commands. This illustrates the problem arising from combinatorial action spaces: they result in a huge set of possible options for the agent, which it cannot possibly explore in a reasonable amount of time. Hence, the major challenge is the generation of a small set of reasonable commands for a given context. Using a supervised learning approach with a pointer-softmax model Vinyals, Fortunato, and Jaitly (2015), Tao et al. (2018) as well as Yuan et al. (2018b) are able to generate admissible commands given a context for a specific TBG. A more general approach by (Fulda et al. 2017) learns to map nouns to affordant verbs by extracting replacement vectors from word embeddings using canonical examples. Zahavy et al. (2018), on the other hand, start from an over-complete set of actions and learn a binary action-elimination network by using the feedback provided by the game engine. Similarly, Ammanabrolu and Riedl (2018) also prune the available actions but using a fixed scoring function on top of a graph representation of the game's state. As far as we know, our model is the first in the area of TBGs to consider grouping commands together to 'high-level' actions as a way to reduce the action-space.</p>
<h2>Gameplay</h2>
<p>This section provides an overview of the structure of the games in the TextWorld challenge and explains the problems an agent needs to overcome to succeed. Figure 1 shows an example of a straightforward game which helps understand the basic structure. The agent starts at a random room around the house with the instruction to find the cookbook and prepare the meal therein. The initial description of the surround-</p>
<p>You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!
$\Rightarrow$ Kitchen $=$ You're now in the kitchen. I guess you better just go and list everything you see here. You hear a noise behind you and spin around, but you can't see anything other than a fridge. The fridge is empty! This is the worst thing that could possibly happen, ever! You see a closed oven. You make out a table. The table is massive. On the table you can make out a cookbook. You can make out a counter. The counter is vast. On the counter you can make out a sliced fried red hot pepper, a red apple, a yellow bell pepper and a knife. You smell an intriguing smell, and follow it to a stove. You shudder, but continue examining the stove. But the thing is empty, unfortunately. Hm. Oh well.
$&gt;$ examine recipe
You open the copy of "Cooking: A Modern Approach (3rd Ed.)" and start reading: Recipe #1
Gather all following ingredients and follow the directions to prepare this tasty meal.
Ingredients:
red hot pepper
Directions:
slice the red hot pepper
fry the red hot pepper
prepare meal
$&gt;$ inventory
You are carrying nothing.
$&gt;$ take red hot pepper
You take the red hot pepper from the counter.
Your score has just gone up by one point.
$&gt;$ inventory
You are carrying:
a sliced fried red hot pepper
$&gt;$ prepare meal
Adding the meal to your inventory.
Your score has just gone up by one point.
$&gt;$ eat meal
You eat the meal. Not bad.
Your score has just gone up by one point.
*** The End ${ }^{<em> * </em>}$</p>
<p>Figure 1:Simple game that shows the basic structure of the task. The player's commands are highlighted in yellow.
ing exemplifies one of the key challenges, namely filtering the vital information from the text: sentence like you hear a noise behind you and spin around, ... or This is the worst thing that could possibly happen, ever! provide no useful information for the game and make it harder to understand the context.
Once the agent finds the room with the cookbook (in the example in Figure 1, it is in the starting room already), the examine recipe reveals the recipe. It consists of two parts: the ingredients, and the directions. While the ingredients part lists the items that need to be collected, the directions give information about the status they need to be in to prepare the meal. In our example, the pepper needs to be sliced and fried. Here, the agent needs to be careful, because the initial description of the surrounding states that the pepper is already sliced and fried and additional frying, for example, would lead to burning the pepper and hence losing the game. The agent, therefore, needs to remember and recognize states of ingredients mentioned in the context. With the inventory command the agent can list all items it is currently
carrying. Once all ingredients, in their correct state, are in the inventory, the agent can prepare and then eat the meal.</p>
<h2>Agent</h2>
<p>We train an agent to select, at every step in the game, the most promising command (in terms of discounted future score) from a list of possible commands, given the observed context. Building a successful agent-not just for TBGs but for a wide range of sequential decision-making applications-is primarily determined by the presented set of choices at each time-step. Therefore, one of the most crucial questions is about how to generate the list of possible commands. The smaller this set is, the less time and effort the agent wastes in its exploratory phase on "useless" strategies. To effectively reduce the size of the action-space, we use an approach inspired by hierarchical reinforcement learning, that we explain in the next section about "Command Generation". In the current section, we outline the architecture and training procedure of the agent, acting on a given set of commands.</p>
<h2>Model</h2>
<p>Model context We build a textual context as an approximation for the (non-observable) game's state. It consists of the following text-based features:</p>
<ol>
<li>Observation: The response from the game engine at the current time-step. It can either be a description of what the agent sees in this room or a direct response to its last command.</li>
<li>Missing items: The list of items that are in the recipe but not yet in the inventory. This information is constructed using the neural recipe model described in Section "Command Generation".</li>
<li>Unnecessary items: The list of items that are in the inventory but are not needed to execute the recipe. We extract this information from the last response to the inventory command.</li>
<li>Description: The description of the current room. It is the output of the last look command.</li>
<li>Previous commands: The list of the ten previously executed commands.</li>
<li>Required utilities: The list of kitchen appliances needed for the recipe, e.g., knife or stove. This list is a result of the prediction by the recipe model described in Section "Command Generation".</li>
<li>Discovered locations: The list of previously visited locations.</li>
<li>Location: The name of the current location, extracted from the last observation (if it included a location).
The architecture of the model is shown in Figure 2. It consists of four building blocks: context encoding, commands encoding, computation of the value of the current state, and the command scoring.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2:Illustration of the model. From a textual description of the context together with $k$ different possible commands, it computes a categorical distribution over the commands as well as a scalar representing the value of the current game state.</p>
<p>Context Encoding The input to the context encoding are the eight text-based features described above. Each of them is a sequence of words that we embed using a trainable 100dimensional word embedding, initialized with pre-trained GloVe (Pennington, Socher, and Manning 2014). This results in eight matrices of shape ( $\operatorname{seqlen}<em f="f" i="i">{i} \times 100$ for $i=$ $1, \ldots, 8$ ) that are fed into eight separate bi-directional GRUs $\left(\mathrm{GRU}</em>\right)$ that has its recurrency over time, i.e., it takes as hidden state the context encoding from the previous time-step (Yuan et al. 2018a).}\right)$. Using the last hidden vector of each GRU, we construct a fixed size encoding of size 32 for every feature input sequence. By concatenating the individual vectors, we obtain a representation for the full context with a fixed size of 256. To obtain the final context encoding $h^{*}$, we pass this representation into another GRU $\left(\mathrm{GRU}_{s</p>
<p>Commands Encoding At every time-step, the model has a set with varying length $k_{t}$ of different possible commands to choose from. Each command is embedded using the same embedding matrix as the context, resulting in a set of $k$ matrices of size $\left(\mathrm{cmdlen}<em c="c">{i} \times 100\right)^{2}$. A single GRU $\left(\mathrm{GRU}</em>$ for $i=1, \ldots, k$.}\right)$ is used to encode the $k$ different commands individually to fixed-size representations $c_{i} \in \mathbb{R}^{32</p>
<p>Value Computation As described in more detail in the upcoming subsection, we use an advantage-actor-critic approach to train the agent. This approach requires a critic function that determines the value of the current state. In our model, we compute this scalar value by passing the encoded context $h^{*}$ through an MLP with a single hidden layer of size 256 and ReLU activations.</p>
<p>Scoring and Command Selection For each possible command, we compute a scalar score by feeding the concatena-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion of the encoded context $h^{*}$ and the encoded command $c_{i}$ for $i=1, \ldots, k$ into an MLP with a single hidden layer of size 256 and ReLU activations. We obtain a score vector $\mathbf{s}<em t="t">{t} \in \mathbb{R}^{k}$ that ranks the $k$ possible commands. On top of the score vector, we apply a softmax to turn it into a categorical distribution $\mathbf{p}</em>$, we sample the final command from the presented set of input commands.}$. Based on $\mathbf{p}_{t</p>
<h2>Training the agent with the Actor-Critic method</h2>
<p>We use an online actor-critic algorithm with a shared network design to optimize the agent. We compute the return $\mathcal{R}_{t}$ of a single time-step $t$ in the session of length $T$ by using the n -step temporal difference method (Sutton and Barto 2018, ch. 7)</p>
<p>$$
\mathcal{R}\left(s_{t}, a_{t}\right)=\gamma^{T-t} v\left(s_{T}\right)+\sum_{\tau=0}^{T-t} \gamma^{\tau} r\left(s_{t+\tau}, a_{t+\tau}\right)
$$</p>
<p>where $\gamma$ denotes the discount factor, and $v\left(s_{T}\right)$ denotes the value of the state, determined by the critic network, that depends on the state $s_{T}$. The game-environment determines the score $r$, based on the state $s$, and the chosen action $a$.</p>
<p>From $\mathcal{R}<em t="t">{t}$ we compute the advantage $\mathcal{A}</em>$ at time-step $t$ by subtracting the state value from the critic network, i.e.</p>
<p>$$
\mathcal{A}\left(s_{t}, a_{t}\right)=\mathcal{R}\left(s_{t}, a_{t}\right)-v_{t}\left(s_{t}\right)
$$</p>
<p>While the value function from the critic $v$ captures how good a certain state is, the advantage informs us how much extra reward we obtain from action $a$ compared to the expected reward in the current state $s$. For the sake of brevity, we will drop the indication of dependence of the state $s$ and action $a$ from now on.</p>
<p>Objective The full objective $\mathcal{L}$ consists of three individual terms: the policy loss, the value loss, and the entropy loss. The policy term optimizes the parameters of the actornetwork while keeping the critic's weights fixed. It encour-</p>
<p>ages (penalizes) the current policy if it led to a positive (negative) advantage. The policy loss is given by the following formula</p>
<p>$$
\mathcal{L}<em t="1">{p}=-\frac{1}{T} \sum</em>}^{T} \mathcal{A<em t="t">{t}^{*} \log \boldsymbol{p}</em>\right]
$$}\left[a_{t</p>
<p>where $\mathcal{A}<em t="t">{t}^{*}$ is the advantage $\mathcal{A}</em>}$ removed from the computational graph, and $\boldsymbol{p<em t="t">{t}\left[a</em>$ determined by the actor.
The value term uses a mean squared error between the return $\mathcal{R}$ and the value of the critic $v_{t}$ to encourage them to be close, i.e.}\right]$ is the probability of the chosen command $a_{t</p>
<p>$$
\mathcal{L}<em t="1">{v}=\frac{1}{2 T} \sum</em>}^{T}\left(\mathcal{R<em t="t">{t}-v</em>
$$}\right)^{2</p>
<p>Finally, the entropy loss penalizes the actor for putting a lot of probability mass on single commands and therefore encourages exploration:</p>
<p>$$
\mathcal{L}<em t="1">{e}=-\frac{1}{T} \sum</em>}^{T} \boldsymbol{p<em t="t">{t}^{T} \log \boldsymbol{p}</em>
$$</p>
<p>The final training objective is chosen as a linear combination of to three individual terms.</p>
<h2>Command Generation</h2>
<p>One of the primary challenges in TBGs is the construction of possible-or rather reasonable-commands in any given situation. Due to the combinatorial nature of the actions, the size of the search space is vast. Thus, brute force learning approaches are infeasible, and RL optimization is extremely difficult. We solve this problem by effectively generating only a small number of the most promising commands, as well as combining multiple actions to a single high-level command. We find that this step of reducing the action-space is the most important to guarantee successful and stable learning of the agent. To this end, we train a helper modelcalled Recipe Manager-that effectively extracts from the game's state which recipe actions still need to be performed. By comparing the state of the ingredients in the inventory with the given recipe and the description of the environment, it generates the next commands in the cooking process.</p>
<h2>Recipe Commands</h2>
<p>The task of this model is to determine, from the raw description of the inventory and the recipe, the following information for every listed ingredient:</p>
<ul>
<li>Does it still need to be collected?</li>
<li>Which cooking actions still need to be performed with it?</li>
</ul>
<p>Figure 4(b) in the Appendix shows an example of how the model extracts from the raw textual input the structured information needed. To achieve this, we train a model in a supervised manner with a self-constructed dataset. The dataset consists of recipes and inventories similar to those of the training games but augmented with multiple additional ingredients and adjectives to foster its generalization capabilities. Here, we query the freebase database to obtain a large selection of popular food items to make our classifier more resilient to ingredients not present in the training games.</p>
<p>Model The input to the recipe model is the individual recipe directions and the current inventory of the agent. We do a binary classification of each direction about whether or not it needs to be performed. The necessary information about the state of the ingredient is present in the inventory. Hence, we need to map and compare each direction to it. The names of the ingredients are of varying length and can have multiple adjectives describing it, e.g., a sliced red hot pepper or some water. Therefore, we treat each direction and the inventory as a variable-length sequence that we encode using a GRU, after embedding it with pre-trained GloVe (Pennington, Socher, and Manning 2014). Using pre-trained embeddings not just speeds up the convergence of the model but also helps to make it generalize across unseen ingredients, because all food-related items are close in the embedding space (Pennington, Socher, and Manning 2014). As can be seen in Figure 4(c) in the Appendix, each of the encoded recipe directions is concatenated with the encoded inventory to serve as the input to an MLP. The network outputs a single value for each of the inputs that represent the probability of the given direction still being necessary to perform.</p>
<p>Adding recipe actions to the possible commands The recipe manager adds two high-level commands to the action set. First, the take all required ingredients from here command, grouping all the necessary 'take' commands, that can be performed in the current room. We construct this list by the intersection of needed ingredients (determined by the recipe model) and ingredients present in the current context description. Second, the drop unnecessary items command that lists 'drop' commands for all the ingredients labeled as unnecessary from the learned recipe model. It is indeed crucial to learn to drop unwanted items because the inventory has a fixed capacity. In addition to the abstract high-level commands, it adds all action commands-specified by the recipe model-if the specific ingredient is in the inventory and the corresponding utility in the room. Figure 4(a) in the Appendix provides an example for how the mapping from high-level to low-level commands is constructed based on the room description, the inventory, and the output from the neural recipe model.</p>
<h2>Navigation Commands</h2>
<p>Another crucial challenge for an agent in a TBG is to efficiently navigate through the game-world; an especially hard task when presented with unseen room configurations at test time. This process can be divided into two tasks, namely (i) understanding from the context in which direction it is possible to move, and (ii) the planning required to move through the rooms efficiently. While the latter is learned by the model as part of its policy, the challenge of extracting the movement directions from the unstructured text remains. Moreover, in the TextWorld environment, every connected room can be blocked by a closed door that the agent has to explicitly open before going into this direction. Therefore, it is necessary not only to understand in which cardinal direction to move for the next room but also to identify</p>
<p>all closed doors in the way. For this task, we learned the Navigator model, that is supervised trained on augmented walkthroughs to identify (i) cardinal directions that lead to connected rooms, and (ii) find all closed doors in the current room. The model takes any room description as input and encodes the sequence with a GRU to obtain a fixed-size vector representation. This is fed into four individual MLPs that make a binary prediction on whether the corresponding cardinal direction leads to a connected room. To obtain the closed doors in the room, the hidden representation from each word of the description is fed into a shared binary MLP that predicts whether or not a particular word is part of the name of a closed door. This approach is necessary because there can be multiple different closed doors in a room, and the name of each door can consist of multiple words, e.g., sliding patio door.
The navigator adds for every detected cardinal direction (east, south, west, north) the respective go command to the list of possible commands. Additionally, it adds open $&lt;$ doorname $&gt;$ for every closed door in the room's description.</p>
<h2>Other Commands</h2>
<p>Besides the commands that handle the navigation and the cooking, there are a few additional actions that are necessary to succeed in the game. Since the number of these commands is minimal, they are either added at every time-step to the set of possible commands or based on very simple rules. We provide the list of additional commands and their rules in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">look,</td>
<td style="text-align: left;">Added at every step, except if they</td>
</tr>
<tr>
<td style="text-align: left;">inventory</td>
<td style="text-align: left;">were just performed in the previous <br> command.</td>
</tr>
<tr>
<td style="text-align: left;">prepare meal</td>
<td style="text-align: left;">Added once the recipe manager does <br> not output any recipe direction as miss- <br> ing anymore and the agent's location is <br> the kitchen.</td>
</tr>
<tr>
<td style="text-align: left;">eat meal <br> examine cook- <br> book</td>
<td style="text-align: left;">Added if meal is in agent's inventory. <br> Added if the cookbook is in the room's <br> description.</td>
</tr>
</tbody>
</table>
<p>Table 1:Rules for additional commands to be added to the list of possible commands.</p>
<h2>Results</h2>
<p>First and foremost, the model was evaluated quantitatively against more than 20 competitors in Microsoft's TextWorld challenge, where it scored 1st on the (hidden) validation set and 2 nd on the final test set of games. To show that our agent improves upon existing models for TBGs on never-before-seen games of the same family, we compare it against several baselines on the competition's training, validation, and test set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">valid</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">test</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\%</td>
<td style="text-align: center;">steps</td>
<td style="text-align: center;">\%</td>
<td style="text-align: center;">steps</td>
</tr>
<tr>
<td style="text-align: center;">Random WL</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">98.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .04$</td>
<td style="text-align: center;">$\pm .27$</td>
<td style="text-align: center;">$\pm .03$</td>
<td style="text-align: center;">$\pm .02$</td>
</tr>
<tr>
<td style="text-align: center;">LSTM-DQN</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">99.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .00$</td>
<td style="text-align: center;">$\pm .00$</td>
<td style="text-align: center;">$\pm .00$</td>
<td style="text-align: center;">$\pm .00$</td>
</tr>
<tr>
<td style="text-align: center;">Random AC</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .59$</td>
<td style="text-align: center;">$\pm 1.67$</td>
<td style="text-align: center;">$\pm .64$</td>
<td style="text-align: center;">$\pm .31$</td>
</tr>
<tr>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">50.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .12$</td>
<td style="text-align: center;">$\pm 1.65$</td>
<td style="text-align: center;">$\pm .25$</td>
<td style="text-align: center;">$\pm .05$</td>
</tr>
<tr>
<td style="text-align: center;">Random Pruned</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">95.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .66$</td>
<td style="text-align: center;">$\pm .81$</td>
<td style="text-align: center;">$\pm .14$</td>
<td style="text-align: center;">$\pm .36$</td>
</tr>
<tr>
<td style="text-align: center;">DRRN Pruned</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">92.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .31$</td>
<td style="text-align: center;">$\pm .41$</td>
<td style="text-align: center;">$\pm 2.01$</td>
<td style="text-align: center;">$\pm 1.80$</td>
</tr>
<tr>
<td style="text-align: center;">Yin and May 2019b</td>
<td style="text-align: center;">$58^{3}$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LeDeepChef</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">43.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\pm .18$</td>
<td style="text-align: center;">$\pm .23$</td>
<td style="text-align: center;">$\pm .20$</td>
<td style="text-align: center;">$\pm .19$</td>
</tr>
</tbody>
</table>
<p>Table 2:Results on the unseen set of validation and test games from the TextWorld Challenge. We report the mean and standard deviation over ten runs with different random seeds of each best performing model on the training set.</p>
<p>As a metric, we always report the points per game relative to the total achievable points. A single game terminates upon successful completion of the task or when the agent fails, by either damaging an item or reaching the maximum number of a hundred steps.</p>
<p>Baseline Figure 3 (a) demonstrates that standard baselines for TBGs are not able to learn generalization capabilities to sufficiently solve a whole family of games. Both, LSTM-DQN (Narasimhan, Kulkarni, and Barzilay 2015) and DRRN (He et al. 2015), do not exceed the $20 \%$ mark of points per game relative to total achievable points ${ }^{4}$ during 3 epochs of training. The input to both of these models is the concatenated game's state, consisting of the room's description, the agent's inventory, the recipe, the feedback from the last command, and the set of previously issued commands. The main difference between DRRN and LSTM-DQN is that the former ranks the provided admissible commands, while the latter ranks (pre-selected) verbs and objects, from which a command is formed then. Due to the combinatorial nature of possible commands from the LSTM-DQN, the effective action-space is significantly larger than for DRRN. Thus, a random agent on this word-level task-Random $W L$-performs much worse than an agent that selects randomly from the admissible commands, Random AC. Both DRRN and LSTM-DQN significantly outperform their ran-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Training games scoring percentage of DRRN and LSTM-DQN over three epochs. The two baselines Random $A C$ and Random WL show the performance of a random agent on the admissible commands (like for DRRN) and the word-level (like LSTM-DQN), respectively.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Training games scoring percentage of LeDeepChef compared to a DRRN model and a random baseline on the same pruned commands, generated by the recipe module.</p>
<p>Figure 3:Comparison of our model to several baseline models on the TextWorld challenge games, as points per game relative to total achievable points throughout the training of 3 epochs with 10 different random seeds. Each shown point is an average over the past 80 games. The model details of the baselines can be found in Table 3 in the Appendix.
dom counterpart over the course of the training but are not able to learn to solve the games to a sufficient degree. The big scoring difference between the two random agents underlines the importance of effective action-space reduction.</p>
<p>Comparison on pruned commands In a second experiment, we use the same DRRN architecture as before, but with a pruned version of the admissible commands to exactly match the commands presented to our model; though, without the grouping to high-level actions. As we see in Figure 3 (b), the reduced set of possible commands massively improves both the random and the DRRN model to up to $50 \%{ }^{5}$. However, the DRRN model is still not capable of improving a lot upon the random model and-as before-does not show a steady upward trend throughout the training procedure. Our model, on the other hand, improves its percentage significantly over the training iterations to its final score of around $87 \%$. We believe that the advantage of our model over this specific baseline is mainly due to (i) the grouped high-level commands that let the agent learn a strategy more efficiently in an abstract space, (ii) the improvements in the neural architecture that acts on a more sophisticated version of the input features, and (iii) the superiority of the actorcritic over the DQN approach.</p>
<p>Comparison on Test Set Table 2 shows the quantitative results of different models on the (unseen) validation set, as well as the final test set of Microsoft's TextWorld challenge. As expected, our model generalizes best to the unseen games with a mean percentage of 74.4 and 69.3 for the respective sets of games. The standard baselines are not able</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to exceed the $15 \%$ mark, indicating that they are not suitable to be applied "out-of-the-box" on the specific task of solving families of TBGs. A recent model by Yin and May (2019b), designed explicitly for the TextWorld environment, uses a curriculum learning approach to train a DQN model and achieves $58 \%$ on their validation set (hold-out data from the challenge's training set).</p>
<h2>Conclusion</h2>
<p>In this work, we presented how to build a deep RL agent that not only performs well on a single TBG but generalizes to never-before-seen games of the same family. To achieve this result, we designed a model that effectively ranks a set of commands based on the context and context-derived features. By incorporating ideas from hierarchical RL, we significantly reduced the size of the action-space and were able to train the agent through an actor-critic approach. Additionally, we showed how to make the agent more resilient against never-before-seen recipes and ingredients by training with data augmented by a food-item database. The performance of our final agent on the unseen games of the FirstTextWorld challenge is substantially higher than any standard baseline. Moreover, it achieved the highest score, among more than 20 competitors, on the (unseen) validation set and beat all but one agent on the final test set.</p>
<h2>Acknowledgments</h2>
<p>We thank Florian Schmidt, Gary Becigneul, and Jonas Kohler for their comments and suggestions. This research was supported by the Swiss National Science Foundation (SNSF) grant number 407540_167176 under the project "Conversational Agent for Interactive Access to Information".</p>
<h2>References</h2>
<p>Ammanabrolu, P., and Riedl, M. O. 2018. Playing text-adventure games with graph-based deep reinforcement learning. CoRR abs/1812.01628.
Côté, M.; Kádár, Á.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M. J.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. Textworld: A learning environment for text-based games. CoRR abs/1806.11532.
Dayan, P., and Hinton, G. E. 1993. Feudal reinforcement learning. In Hanson, S. J.; Cowan, J. D.; and Giles, C. L., eds., Advances in Neural Information Processing Systems 5. Morgan-Kaufmann. 271-278.
Fulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What can you do with a rock? affordance extraction via word embeddings. CoRR abs/1703.03429.
He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Ostendorf, M. 2015. Deep reinforcement learning with an unbounded action space. CoRR abs/1511.04636.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural Comput. 9(8):1735-1780.
Infocom. 1980. Zork i.
Kostka, B.; Kwiecien, J.; Kowalski, J.; and Rychlikowski, P. 2017. Text-based adventures of the golovin AI agent. CoRR abs/1705.05637.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M. A. 2013. Playing atari with deep reinforcement learning. CoRR abs/1312.5602.
Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T. P.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous methods for deep reinforcement learning. CoRR abs/1602.01783.
Narasimhan, K.; Kulkarni, T. D.; and Barzilay, R. 2015. Language understanding for text-based games using deep reinforcement learning. CoRR abs/1506.08941.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In In EMNLP.
Sutton, R. S., and Barto, A. G. 2018. Reinforcement Learning: An Introduction. The MIT Press, second edition.
Tao, R. Y.; Côté, M.; Yuan, X.; and Asri, L. E. 2018. Towards solving text-based games by producing adaptive action spaces. CoRR abs/1812.00855.
Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer networks. In Cortes, C.; Lawrence, N. D.; Lee, D. D.; Sugiyama, M.; and Garnett, R., eds., Advances in Neural Information Processing Systems 28. Curran Associates, Inc. 2692-2700.
Yin, X., and May, J. 2019a. Comprehensible context-driven text game playing. CoRR abs/1905.02265.
Yin, X., and May, J. 2019b. Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Common Sense to Learn Families of Text-Based Adventure Games. arXiv e-prints arXiv:1908.04777.</p>
<p>Yuan, X.; Côté, M.; Sordoni, A.; Laroche, R.; des Combes, R. T.; Hausknecht, M. J.; and Trischler, A. 2018a. Counting to explore and generalize in text-based games. CoRR abs/1806.11525.
Yuan, X.; Wang, T.; Meng, R.; Thaker, K.; He, D.; and Trischler, A. 2018b. Generating diverse numbers of diverse keyphrases. CoRR abs/1810.05241.
Zahavy, T.; Haroush, M.; Merlis, N.; Mankowitz, D. J.; and Mannor, S. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 31. Curran Associates, Inc. 3562-3573.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} 50 \%$ is equivalent to the 5th place in the competition.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
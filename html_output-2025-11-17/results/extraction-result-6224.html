<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6224 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6224</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6224</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-bcefc74b20649fd41ea05d87a3fa512d2559fc8d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bcefc74b20649fd41ea05d87a3fa512d2559fc8d" target="_blank">Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation</a></p>
                <p><strong>Paper Venue:</strong> DSTC</p>
                <p><strong>Paper TL;DR:</strong> A novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs) to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics.</p>
                <p><strong>Paper Abstract:</strong> Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 “Automatic Evaluation Metrics for Open-Domain Dialogue Systems”, proving the evaluation capabilities of prompted LLMs.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6224.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6224.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-as-judge (DSTC11)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo) used as an automatic dialogue evaluator in DSTC11 Track 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses ChatGPT (gpt-3.5-turbo) in a zero-shot/limited prompt setting as an automatic judge for open-domain dialogue quality, compares its scores to human evaluation (HE) annotations across multilingual and paraphrase-robustness benchmarks, and ensembles ChatGPT outputs with encoder-based submetrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain dialogue evaluation (turn-level and dialogue-level; multilingual and paraphrase-robust/robustness tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo; OpenAI API accessed late March; temperature set to 0 for runs described)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluations (HE) provided by the DSTC11 track using Likert-scale annotations for multiple quality aspects; metric performance compared to HE using Pearson/Spearman correlations on turn- and dialogue-level annotations. The paper does not report detailed annotator counts or instruction protocols (uses the track-provided HE).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman (and Pearson) correlation between metric scores and human annotations (average Spearman across aspects reported); additional qualitative analysis of instances where ChatGPT and HE differ by >3 points; mean absolute deviation across repeated ChatGPT runs reported for one internal dev set.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Quantitative: Prompted ChatGPT substantially outperforms typical encoder-based trained discriminative metrics on the DSTC11 Multilingual and Robustness tasks (their ChatGPT-inclusive systems rank 1st on Multilingual and top on Robustness; combined encoder+ChatGPT best overall). ChatGPT-based submissions achieved much higher Spearman correlations than standalone encoder models and other teams' systems. Qualitative: In all examined large divergences (>3 points), ChatGPT consistently underestimated response quality relative to human annotators; ChatGPT also shows non-deterministic outputs (though low MADeviation reported when controlled) and sometimes conflates quality aspects (e.g., treating content richness like relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Non-deterministic scoring when API lacks token probabilities; occasional failure to return the requested single scalar (outputs per-turn scores or continues conversation); tendency to under-estimate human-annotated quality in examined disagreement cases; sensitivity/overreaction to potentially inappropriate/divisive tokens (likely due to RLHF), leading to penalization; conflation of distinct quality aspects (e.g., content richness vs relevance); calibration issues (scores not aligned with HE scale); inability from prompt-only usage to evaluate safety/acceptability features requiring external verification; lack of access to model log-probabilities limits some metric computations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>1) 'Tennis racquets... manufactured by weed' example: humans rated high (A:4.7, C:4.7, G:4.3, R:5.0) while ChatGPT scored very low (A:1.0, C:2.0, G:2.0, R:1.0); authors attribute part of the low score to ChatGPT avoiding/diverting from 'inappropriate'/divisive content. 2) 'Qibing' conversational example: humans gave very high scores (A/C/G ~5.0) but ChatGPT assigned minimum scores (1.0) and explained the response was unrelated/confusing; the authors note ChatGPT appears to misunderstand the conversation in this case. Overall, all detected >3-point divergences were systematic underestimates by ChatGPT and often involved aspect conflation or perceived inappropriateness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Practical measures used or recommended in the paper: (a) prompt engineering (explicit single-score instruction, set temperature=0, reinforce output format), (b) re-prompting when the model fails to produce the expected scalar or produces per-turn values (average per-turn when necessary), (c) normalise ChatGPT outputs to [0,1] for ensembling, (d) ensemble ChatGPT with encoder-based submetrics (their best system combines ChatGPT and XLM-R based submetrics), (e) statistical/significance-based submetric weighting to avoid dropping negatively correlated submetrics, (f) calibration via prompts that include examples and explicit scoring guidelines (proposed as future work), (g) select higher-quality augmented training data (COMET QE filtering) for encoder submetrics and use Siamese training for paraphrase robustness, and (h) develop challenge sets and external verifiable checks for safety/trustworthiness evaluation (recommended future directions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Understanding the effectiveness of very large language models on dialog evaluation <em>(Rating: 2)</em></li>
                <li>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks <em>(Rating: 2)</em></li>
                <li>Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>Hallucinations in large multilingual translation models <em>(Rating: 1)</em></li>
                <li>Appropriateness is all you need! <em>(Rating: 1)</em></li>
                <li>On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6224",
    "paper_id": "paper-bcefc74b20649fd41ea05d87a3fa512d2559fc8d",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "ChatGPT-as-judge (DSTC11)",
            "name_full": "ChatGPT (gpt-3.5-turbo) used as an automatic dialogue evaluator in DSTC11 Track 4",
            "brief_description": "This paper uses ChatGPT (gpt-3.5-turbo) in a zero-shot/limited prompt setting as an automatic judge for open-domain dialogue quality, compares its scores to human evaluation (HE) annotations across multilingual and paraphrase-robustness benchmarks, and ensembles ChatGPT outputs with encoder-based submetrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-domain dialogue evaluation (turn-level and dialogue-level; multilingual and paraphrase-robust/robustness tasks)",
            "llm_judge_model": "ChatGPT (gpt-3.5-turbo; OpenAI API accessed late March; temperature set to 0 for runs described)",
            "human_evaluation_setup": "Human evaluations (HE) provided by the DSTC11 track using Likert-scale annotations for multiple quality aspects; metric performance compared to HE using Pearson/Spearman correlations on turn- and dialogue-level annotations. The paper does not report detailed annotator counts or instruction protocols (uses the track-provided HE).",
            "metrics_compared": "Spearman (and Pearson) correlation between metric scores and human annotations (average Spearman across aspects reported); additional qualitative analysis of instances where ChatGPT and HE differ by &gt;3 points; mean absolute deviation across repeated ChatGPT runs reported for one internal dev set.",
            "reported_differences": "Quantitative: Prompted ChatGPT substantially outperforms typical encoder-based trained discriminative metrics on the DSTC11 Multilingual and Robustness tasks (their ChatGPT-inclusive systems rank 1st on Multilingual and top on Robustness; combined encoder+ChatGPT best overall). ChatGPT-based submissions achieved much higher Spearman correlations than standalone encoder models and other teams' systems. Qualitative: In all examined large divergences (&gt;3 points), ChatGPT consistently underestimated response quality relative to human annotators; ChatGPT also shows non-deterministic outputs (though low MADeviation reported when controlled) and sometimes conflates quality aspects (e.g., treating content richness like relevance).",
            "llm_specific_limitations": "Non-deterministic scoring when API lacks token probabilities; occasional failure to return the requested single scalar (outputs per-turn scores or continues conversation); tendency to under-estimate human-annotated quality in examined disagreement cases; sensitivity/overreaction to potentially inappropriate/divisive tokens (likely due to RLHF), leading to penalization; conflation of distinct quality aspects (e.g., content richness vs relevance); calibration issues (scores not aligned with HE scale); inability from prompt-only usage to evaluate safety/acceptability features requiring external verification; lack of access to model log-probabilities limits some metric computations.",
            "notable_failure_cases": "1) 'Tennis racquets... manufactured by weed' example: humans rated high (A:4.7, C:4.7, G:4.3, R:5.0) while ChatGPT scored very low (A:1.0, C:2.0, G:2.0, R:1.0); authors attribute part of the low score to ChatGPT avoiding/diverting from 'inappropriate'/divisive content. 2) 'Qibing' conversational example: humans gave very high scores (A/C/G ~5.0) but ChatGPT assigned minimum scores (1.0) and explained the response was unrelated/confusing; the authors note ChatGPT appears to misunderstand the conversation in this case. Overall, all detected &gt;3-point divergences were systematic underestimates by ChatGPT and often involved aspect conflation or perceived inappropriateness.",
            "mitigation_strategies": "Practical measures used or recommended in the paper: (a) prompt engineering (explicit single-score instruction, set temperature=0, reinforce output format), (b) re-prompting when the model fails to produce the expected scalar or produces per-turn values (average per-turn when necessary), (c) normalise ChatGPT outputs to [0,1] for ensembling, (d) ensemble ChatGPT with encoder-based submetrics (their best system combines ChatGPT and XLM-R based submetrics), (e) statistical/significance-based submetric weighting to avoid dropping negatively correlated submetrics, (f) calibration via prompts that include examples and explicit scoring guidelines (proposed as future work), (g) select higher-quality augmented training data (COMET QE filtering) for encoder submetrics and use Siamese training for paraphrase robustness, and (h) develop challenge sets and external verifiable checks for safety/trustworthiness evaluation (recommended future directions).",
            "uuid": "e6224.0",
            "source_info": {
                "paper_title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Understanding the effectiveness of very large language models on dialog evaluation",
            "rating": 2
        },
        {
            "paper_title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks",
            "rating": 2
        },
        {
            "paper_title": "Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Hallucinations in large multilingual translation models",
            "rating": 1
        },
        {
            "paper_title": "Appropriateness is all you need!",
            "rating": 1
        },
        {
            "paper_title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)",
            "rating": 1
        }
    ],
    "cost": 0.0088085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation</h1>
<p>John Mendonça ${ }^{1,2,}$, Patrícia Pereira ${ }^{1,2}$, Helena Moniz ${ }^{1,3}$, João Paulo Carvalho ${ }^{1,2}$, Alon Lavie ${ }^{4,5}$ and Isabel Trancoso ${ }^{1,2}$<br>${ }^{1}$ INESC-ID, Lisbon<br>${ }^{2}$ Instituto Superior Técnico, University of Lisbon<br>${ }^{3}$ Faculdade de Letras, University of Lisbon<br>${ }^{4}$ Carnegie Mellon University, Pittsburgh<br>${ }^{5}$ Phrase, Pittsburgh<br>john.mendonca@inesc-id.pt</p>
<h4>Abstract</h4>
<p>Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain Dialogue Systems", proving the evaluation capabilities of prompted LLMs.</p>
<h2>1 Introduction</h2>
<p>Automatic dialogue evaluation has largely been focused on evaluating select few languages. The main reason for this constraint is the lack of linguistic diversity in dialogue corpora, which leads to a lack of chatbots that cover other languages. As a result, the need for multilingual metrics has also been limited.</p>
<p>A possible solution to this issue is to leverage the latest batch of Large Language Models (LLMs) to synthetically generate multilingual dialogues. Some research has already been conducted to study the capabilities of these models (Guo et al., 2023; Bubeck et al., 2023) and the consensus appears to be that these models have achieved a proxy of a formal linguistic competence in the most studied</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Proposed framework architecture. The Response, Context and Quality Aspect under evaluation are fed to the submetrics: VSP (Valid Sentence Prediction), NSP (Next Sentence Prediction), MLM (Masked Language Modelling), ENG (Engagement) and ChatGPT. Each submetric score is then weighted according to the aspect, yielding the final metric.
languages. That is, its responses follow linguistic conventions and are fluent and grammatical, but they might be inaccurate or even hallucinate (Guerreiro et al., 2023). More importantly, pertaining to dialogue, they also show signs of functional linguistic competence in its responses, i.e., discursive coherence, narrative structure and linguistic knowledge, even if not fully consistent (sometimes they do not consider context or situated information, and fail to adapt to users and domains).</p>
<p>Irrespective of these models' limitations, it is clear their emergent capabilities allow for the development of chatbots with capabilities vastly beyond what earlier models were able to achieve. Yet,</p>
<p>an interesting research question lingers: If these models are able to write responses that follow formal and functional linguistics rules, are they also capable of evaluating responses/dialogues in terms of these same rules? Prior work has confirmed the language understanding capabilities of instructionbased LLMs for dialogue evaluation (Huynh et al., 2023). However, we are the first to study the evaluation capabilities of the newest batch of LLMs in terms of multilinguality and paraphrase robustness.</p>
<p>This paper presents our contribution to the DSTC11 track on Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems (Rodríguez-Cantelar et al., 2023), where we participated in both the Multilingual and Robustness tasks. This track is an excellent venue to benchmark the capabilities of these new LLMs for dialogue evaluation, as it evaluates properties that have been observed in these models. We propose a comprehensive framework, incorporating earlier encoder-based approaches and ChatGPT, as illustrated in Figure 1. By combining multiple models and submetrics through ensembling, our approach aims to improve the performance and robustness of dialogue evaluation, ultimately contributing to the advancement of dialogue system research and development.</p>
<p>Overall, our contributions are the following:</p>
<ul>
<li>We show that ChatGPT is a strong evaluator of dialogues, outperforming typical encoder frameworks.</li>
<li>We propose a new framework for dialogue evaluation that is multilingual and robust to paraphrases. In fact, our combined Encoder and ChatGPT framework ranks 1st place on both the Multilingual and Robust metrics task.</li>
<li>We discuss the outlook of Dialogue Evaluation in this new realm of LLMs.</li>
<li>We open source the code and checkpoints of the submetrics at github.com/ johndmendonca/DialEvalML.</li>
</ul>
<h2>2 Related work</h2>
<h3>2.1 Automatic Evaluation Metrics</h3>
<p>Statistic-based metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), are a popular choice to evaluate NLG (Natural Language Generation)
models as they are easy to employ. These metrics assume valid responses have significant wordoverlap with the ground truth. However, this is not a valid assumption for dialogue: there are many equally good responses for a single utterance. As such, the correlation with Human Evaluation (HE) annotations is very low for these metrics (Liu et al., 2016), and they cannot be used to evaluate models whenever a gold-response is not available.</p>
<p>Earlier learned metrics such as ADEM (Lowe et al., 2017) and RUBER (Tao et al., 2018) explicitly predict HE annotations by initialising pretrained Recurrent Neural Network response generators. Unlike ADEM, which is trained with HEannotated data in a supervised manner, RUBER leverages negative samples. In both cases, a reference response is used to score the candidate response. As such, these metrics still suffer the same issues as word-overlap metrics.</p>
<p>The primary motivation for the negative sampling approach in RUBER was the need for extensive HE annotations in ADEM. Approaches similar to this are now the norm for training opendomain dialogue evaluation metrics. By using welldefined self-supervised tasks which correlate well with their corresponding aspects, the annotation limitations are mostly circumvented.</p>
<p>The most widely used self-supervised task is Next Sentence Prediction (NSP), as it is known to correlate well with HE that evaluate "Context Awareness". The typical approach is to finetune a pretrained encoder model with this automatically generated data (Mehri and Eskenazi, 2020b; Phy et al., 2020; Mendonca et al., 2022; Zhao et al., 2020; Zhang et al., 2022). More complex approaches leverage graph representations to model dialogue interactions explicitly (Huang et al., 2020; Zhang et al., 2021a). Another typically employed self-supervised task is Valid Sentence Prediction (VSP), which uses word-level noising techniques to generate negative samples and correlates well with HE that evaluate Fluency (Phy et al., 2020; Mendonca et al., 2022; Zhang et al., 2022).</p>
<p>Parallel to this trend, other annotation-free approaches in the literature have surfaced. For instance, qualities such as Specificity correlate reasonably well with metrics obtained directly from the MLM (Masked Language Modelling) loss calculated using pretrained encoder models (Mehri and Eskenazi, 2020b; Phy et al., 2020; Zhang et al., 2022).</p>
<p>Given the multifaceted nature of dialogue, dialogue quality metrics typically employ a combination of submetrics. Mehri and Eskenazi (2020a) leverage follow-up utterance from a pretrained decoder model to calculate 18 turn and dialogue-level submetrics, which are then used as inputs to a regression model for overall quality. In fact, Linear Regression is frequently used as a feature aggregation method in the literature (Jiang et al., 2022; Mehri and Eskenazi, 2020b). Alternatively, Phy et al. (2020) propose a hierarchical composition where they incorporate the quality aspects together in a way that aspects in the lower hierarchy need to be satisfied before aspects higher up are considered. Also worth mentioning is the work of Zhang et al. (2022), which proposes the so called Correlation Re-scaling method. Here, the contribution of each aspect is calculated from the individual correlations of the submetrics, obtained from a subset of HE.</p>
<h3>2.2 Large Language Models</h3>
<p>The widespread use of LLMs was established, practically speaking, with the work of Devlin et al. (2019), where a transformer architecture (Vaswani et al., 2017) is pretrained with substantial amounts of unlabelled text with a Masked Language Modelling (MLM) objective. With this architecture, a new paradigm in NLP surfaced, where the adaptation to downstream tasks was conducted by finetuning the pretrained model with supervised data. Later on, GPT-3 (Brown et al., 2020), which is trained with an autoregressive objective, showed competitive results by leveraging few-shot prompting. Nevertheless, given their training objective function, it was difficult for autoregressive LLMs to successfully perform downstream NLP tasks without substantial prompt engineering.</p>
<p>Ouyang et al. (2022) propose finetuning GPT3 using a 3-step approach named Reinforcement Learning through Human Feedback (RLHF). In detail, the model is (1) initially finetuned using supervised data obtained from labelling prompts (SFT); (2) a reward model is trained using ranked responses given a prompt; (3) the policy is optimised against the reward model using the Proximal Policy Optimisation reinforcement learning algorithm (Schulman et al., 2017). As a testament to the power of this approach, ChatGPT took the world by storm in late 2022 thanks to its incredible humanlike generation capabilities. This was achieved by including dialogues in all steps of RLHF.</p>
<h2>3 Problem Formulation</h2>
<p>The main goal of this track was to develop and benchmark automatic open-ended dialogue evaluation metrics. Two tasks were proposed this year, Metrics for Multilingual Data and Robust metrics. For the Metrics for Multilingual Data task, participants were asked to construct quality metrics that perform well on a multilingual setup. For the the Robust metrics task, the goal was to develop metrics that perform robustly when evaluated over back-translated/paraphrased sentences in English.</p>
<p>In both tasks, the proposed metrics were evaluated at the turn and dialogue level, without access to a reference. In a turn-level evaluation setting, the goal is, given prior dialogue history (frequently denoted as context) $c$ of varying amount of turns, and a response $r$, to learn a scoring function (also known as metric) that assigns a score $f(c, r) \rightarrow s$. Conversely, in a dialogue-level evaluation setting, the goal is to evaluate the performance throughout the full dialogue.</p>
<p>Irrespective of the level of evaluation, the proposed metrics' outputs are typically compared against HE annotations that use a Likert scale, where the lowest value means lowest quality and highest value maximum quality. For this track, the performance of these metrics was evaluated by calculating the Pearson correlation between the calculated score and HE.</p>
<h2>4 Methodology</h2>
<p>Our framework, which we call DialEvalML, can be viewed as a dual layered ensemble which are done at the model and submetric level, and that employ strong multilingual pretrained encoder and decoder models which were finetuned or prompted ${ }^{1}$. In this section, we describe the step-by-step process of DialEvalML, detailing the various components and methods employed.</p>
<h3>4.1 Submetrics</h3>
<p>Similar to other frameworks, including the best performing ones in last year's track (Zhang et al., 2022; Jiang et al., 2022) which take inspiration from the works of Phy et al. (2020); Sinha et al. (2020); Mehri and Eskenazi (2020b), we employ</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>several submetrics to evaluate dialogue responses ranging from zero-shot prediction using pretrained LLMs to trained models using self-supervised and supervised methods - and weigh them according to the aspect we wish to predict.</p>
<h3>4.1.1 VSP: Valid Sentence Prediction</h3>
<p>Following Sinha et al. (2020), we train a regression model that is optimised to differentiate between positive samples and synthetic negative samples. Positive samples are perturbed by randomly applying one of the following: (1) no perturbation, (2) punctuation removal, (3) stop-word removal. Negative samples are generated by randomly applying one of the following rules: (1) word reorder (shuffling the ordering of the words); (2) word-drop; and (3) word-repeat (randomly repeating words).</p>
<h3>4.1.2 NSP: Next Sentence Prediction</h3>
<p>With the binary NSP (Next Sentence Prediction) task, the goal is to distinguish a positive example from a semantically negative one, given a context. We train a discriminative regression model using the following sampling strategy: positive responses are drawn directly from the dialog; negative responses are randomly selected and a token coverage test discards semantically similar sentences. All responses are processed using the positive-sample heuristic used by VSP.</p>
<p>For both tasks, the underlying goal is that paraphrased and/or translated responses should have the same coherence score as the original response, since they (in theory) convey the same message. In order to increase the robustness of our framework to paraphrased responses we propose a Siamese Neural Network. Simply put, we train an encoder model (denoted NSP-Siamese) to jointly optimise a Cosine Embedding Loss between the hidden states of the encoder model for the original and a paraphrase, and the individual errors between the predictions and the ground truth. We hypothesise this enables the model to compare the semantic coherence of the responses w.r.t the context, instead of more spurious features such as syntax.</p>
<p>A similar approach could've been employed for multilingual metrics, however, scaling to more languages is computationally expensive: one would either need a new model for each language, or a training procedure requiring a forward pass for each language, for each example.</p>
<h3>4.1.3 MLM: Masked Language Modelling</h3>
<p>Similar to Mehri and Eskenazi (2020b); Phy et al. (2020), we use a pretrained encoder model to calculate the MLM loss of all tokens of the response. The resulting MLM submetric is calculated as the sum of the individual losses.</p>
<h3>4.1.4 ENG: Engagement</h3>
<p>An important quality aspect of dialogue that is frequently overlooked is Engagement. Some work attempt to equate this aspect with Specificity and related metrics. However, we argue this is a reductive solution, as engagement is an abstract and multi-dimensional concept, thereby making a surface level evaluation of the response in terms of diversity insufficient.</p>
<p>As such, and following the methodology used for VSP and NSP, we train a discriminate model using RED (Reddit-based Engagement Dataset) (Xu et al., 2022) which we then use as a submetric denoted in our framework as ENG. This dataset is sourced from Reddit and is curated using a novel distant-supervision framework. This framework aggregates emotional, attentional, behavioural and reply engagement onto a single score denoted ENDEX, which then has a hyperparameter threshold applied to it to cluster posts into positive and negative samples.</p>
<h3>4.2 Exploiting Data Augmentation for Robust and Multilingual Evaluation</h3>
<p>The main novelty of this year's track is the release of training and development dialogue data that has been augmented with MT (Machine Translation) for the Multilingual task - and Paraphrases - for the Robust task. These augmentations are subsequently scored to determine similarity against the original data: for MT, several COMET QE (Quality Estimation) scores (Rei et al., 2020; Zerva et al., 2021; Rei et al., 2022) were provided; for Paraphrases, the organisers provided cosine similarity scores of the sentence embeddings.</p>
<p>A naive approach to obtain competitive metrics in both tasks would be to simply introduce the full amount of augmented data during self-supervised and supervised training. However, Mendonca et al. (2023) showed that low quality augmentation affects the performance of models trained on MT augmented data, especially for VSP. Following this work, we select 5 and $75 \%$ of the best translated data (ranked using COMET QE) for training of the VSP and NSP models respectively. For ENG, we</p>
<p>train different proportions of data and select the best performing ones.</p>
<h3>4.2.1 ChatGPT</h3>
<p>We briefly experimented with different prompts, and found the best performing prompt (irrespective of language) in a held-out internal set to be simply:</p>
<ul>
<li>Turn-level: "Given the Context, evaluate from 1-5 the Response in terms of {aspect}. Provide a single score and nothing else."</li>
<li>Dialogue-level: "Evaluate the following dialogue from 1-5 in terms of {aspect}. Provide a single score and nothing else."</li>
</ul>
<p>Unlike GPT-3, the API for ChatGPT does not output the log probabilities of the most likely tokens. As such, the measurement of quality is non-deterministic. We attempt to reduce output variability by reinforcing the desired output in the prompt ("Provide a single score and nothing else.") and by setting the temperature to 0 . We report a mean absolute deviation of 0.0182 across 3 runs when querying Appropriateness on the provided en/dailydialog-grade dataset included in the development set. To facilitate ensembling in later stages, we normalise the predictions to $[0,1]$.</p>
<p>The default processing step consists of searching for an integer in the response. However, there are some instances where ChatGPT fails to output the desired score: (1) When conducting dialogue level evaluation, the model sometimes outputs scores for each individual response. In these cases, we calculate the average score, similar to the dialoguelevel encoder scores. (2) Less frequently, ChatGPT ignores the task and continues the conversation. Here, we prompt the model again until a score is provided.</p>
<h3>4.3 Submetric Ensembling</h3>
<p>Despite having a key role in NLG evaluation, HE has been performed while suffering from nontransparent and inconsistent annotation procedures. As such, annotations from different works one expects to report the same quality are frequently only nominal in nature. A good example is Coherence, with some definitions referring to it as (1) semantic relevance with respect to a previous sentence; (2) a theme/topic; or even (3) Readability, which is considered a different quality in other guidelines. Howcroft et al. (2020) provides an in-depth survey
of 165 NLG papers with human evaluations where these issues are highlighted.</p>
<p>Taking into account these facts, it is not clear we can successfully apply an empirical surjective mapping function from our submetrics to the quality aspects. Instead, we take a data-driven approach to generate this mapping, similar to the one proposed in Zhang et al. (2022). The main difference between the original Correlation Re-Scaling method and our approach is that, instead of zeroing the weights of submetrics that have a negative correlation with the given aspect, we take a probabilistic approach where we conduct a statistic significance test, i.e., we check if the $p$-value is higher than a given threshold. This ensures submetrics which are strongly and negatively correlated with the aspect (for example, MLM and Fluency) are still included in the ensembling ${ }^{2}$.</p>
<h3>4.4 Dialogue-level Evaluation</h3>
<p>We obtain dialogue-level quality predictions from the encoder models - NSP, VSP, MLM and ENG - by averaging the individual turn-level predictions. These are combined with the dialogue-level predictions obtained by prompting ChatGPT with the full dialogue in the prompt.</p>
<h2>5 Experiments</h2>
<h3>5.1 Datasets</h3>
<p>For data preprocessing we used spaCy. For the VSP and NSP models, we followed prior work and base the self-supervised data on DailyDialog (Li et al., 2017). For the language specific and multilingual models, we rank the translations using the provided WMT22 scores. Models using paraphrased responses are trained using the least similar responses (lowest score ${ }^{3}$ ).</p>
<p>The ENG model was trained using the RED dataset, more specifically on the 80 k split with negative sampled data (Xu et al., 2022). Given it is an English dataset, we use MBART50 ${ }^{4}$ (Liu et al., 2020) to augment the original dataset with Spanish and Chinese MT. Finally, we score it using the WMT20-COMET-QE-DA model (Rei et al., 2020).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For the paraphrase augmentation, we follow the organisers' approach of using Parrot Paraphraser (Damodaran, 2021) and scoring the paraphrases with Cosine Similarity.</p>
<h3>5.2 Training and Hyperparameters</h3>
<p>We used XLM-RoBERTa-large (Conneau et al., 2020) as the encoder model for the experiments. This model is the multilingual version of RoBERTa, pretrained on CommonCrawl data containing 100 languages. We used a single Quadro RTX 6000 24GB GPU for the encoder experiments, and accessed ChatGPT (gpt-3.5-turbo) in late March using the OpenAI API.</p>
<p>For the VSP, NSP and ENG metrics, a token representing the speaker was added for each turn, and a maximum history length of 3 turns was used during training. For predictions in the development and test sets we include the full conversational context whenever possible. If it surpasses input size limitations, we iteratively remove turns from the context, starting from the oldest one. We applied a regression head consisting of a 2-layer MLP with a hidden size of 1024 and a hyperbolic tangent function as activation for prediction. All parameters were trained/finetuned using Adam optimiser (Kingma and Ba, 2015). The fully finetuned models used a learning rate of 3e-6 and were trained for 3 epochs using a batch size of 16. Evaluation was conducted every 10,000 steps. The best performing model on the evaluation set was selected for testing. For the MLM metric, we used the existing LM head available in the Transformers library (Wolf et al., 2020).</p>
<p>With respect to the model-level ensembling, we conduct simple unweighted averaging of the predictions of the models. For the submetric-level ensembling, we define the mask threshold as $p&gt;0.05$ and square the correlations following Zhang et al. (2022). For testing, we define a mapping from the development quality aspects to the test-set aspects and obtain the final weights by averaging the weights obtained on the test set.</p>
<h3>5.3 Model ensembling</h3>
<p>In order to determine the best combination of models to include in our model ensemble, all encoder based models that require training were trained using different subsets of data. This includes the original (EN) English data, the corresponding augmentations in Chinese (ZH), Spanish (ES) and Para-</p>
<table>
<thead>
<tr>
<th>Language</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Submetric</td>
<td>Model</td>
<td>EN</td>
<td>ES</td>
<td>ZH</td>
<td>PA</td>
<td>ALL</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>VSP</td>
<td>EN</td>
<td>0.195</td>
<td>0.173</td>
<td>0.161</td>
<td>0.067</td>
<td>0.149</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ES</td>
<td>0.156</td>
<td>0.183</td>
<td>0.158</td>
<td>0.012</td>
<td>0.127</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ZH</td>
<td>0.179</td>
<td>0.111</td>
<td>0.102</td>
<td>0.086</td>
<td>0.119</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>PA</td>
<td>0.212</td>
<td>0.193</td>
<td>0.198</td>
<td>0.082</td>
<td>0.166</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ML5</td>
<td>0.195</td>
<td>0.168</td>
<td>0.157</td>
<td>0.040</td>
<td>0.140</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>NSP</td>
<td>EN</td>
<td>0.279</td>
<td>0.256</td>
<td>0.286</td>
<td>0.267</td>
<td>0.272</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ES</td>
<td>0.266</td>
<td>0.257</td>
<td>0.282</td>
<td>0.251</td>
<td>0.264</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ZH</td>
<td>0.246</td>
<td>0.238</td>
<td>0.298</td>
<td>0.232</td>
<td>0.254</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>PA</td>
<td>0.307</td>
<td>0.279</td>
<td>0.286</td>
<td>0.279</td>
<td>0.288</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ML75</td>
<td>0.300</td>
<td>0.284</td>
<td>0.311</td>
<td>0.272</td>
<td>0.292</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ENG</td>
<td>EN</td>
<td>0.319</td>
<td>0.275</td>
<td>0.251</td>
<td>0.260</td>
<td>0.276</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ML5</td>
<td>0.310</td>
<td>0.268</td>
<td>0.214</td>
<td>0.275</td>
<td>0.267</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ML10</td>
<td>0.334</td>
<td>0.296</td>
<td>0.243</td>
<td>0.279</td>
<td>0.288</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ML20</td>
<td>0.379</td>
<td>0.324</td>
<td>0.274</td>
<td>0.316</td>
<td>0.324</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ML50</td>
<td>0.340</td>
<td>0.263</td>
<td>0.258</td>
<td>0.289</td>
<td>0.287</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>PA</td>
<td>0.265</td>
<td>0.245</td>
<td>0.213</td>
<td>0.265</td>
<td>0.247</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Spearman Correlation scores of our trained model variants on all Language benchmarks on the full development set. The best score for each submetric and language is highlighted in bold. Models included in the final ensemble are in bold, except for NSP, which also includes NSP-Siamese. phrases (PA) and the QE-ranked multilingual augmentation (MLXX) ${ }^{5}$.</p>
<p>Spearman correlation results are presented in Table 1. For the VSP submetric, we note that the inclusion of translations is detrimental to performance. In fact, the best performing models are PA, followed by EN. This contrasts with NSP, where we observe that the inclusion of more translated data improves performance. For ENG, the best performance is obtained with $20 \%$ of translated data. We include the 10 and $50 \%$ models in our framework to take advantage of ensembling.</p>
<h3>5.4 Track Results</h3>
<p>For the track we submitted 4 different systems, exploring the contribution of the different components of our framework:</p>
<ul>
<li>System 1 (DialEval.ML): Submetric ensembling of ChatGPT + XLM-R.</li>
<li>System 2: Submetric ensembling of XLM-R.</li>
<li>System 3: Submetric ensembling of ChatGPT.</li>
<li>System 4: Direct mapping of ChatGPT submetrics.</li>
</ul>
<p>Table 2 identifies the turn-level weights calculated for testing for System 1.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{5}$ We only include the best performing ML models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Aspect</th>
<th style="text-align: center;">VSP</th>
<th style="text-align: center;">NSP</th>
<th style="text-align: center;">MLM</th>
<th style="text-align: center;">ENG</th>
<th style="text-align: center;">cGPT-A</th>
<th style="text-align: center;">cGPT-R</th>
<th style="text-align: center;">cGPT-C</th>
<th style="text-align: center;">cGPT-G</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Appropriateness</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.0511</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">$\mathbf{0 . 1 8 5}$</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">$\mathbf{0 . 1 8 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Relevance</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 4}$</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.190</td>
</tr>
<tr>
<td style="text-align: left;">Content Richness</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 8}$</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.048</td>
</tr>
<tr>
<td style="text-align: left;">Grammatical Correctness</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">-0.06</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">$\mathbf{0 . 2 5 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Calculated submetric weights of System 1 for test set quality aspects. Highest weight per aspect in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">EN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ZH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ES</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ML-AVG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Rank</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Team</td>
<td style="text-align: center;">Turn</td>
<td style="text-align: center;">Dial</td>
<td style="text-align: center;">Turn</td>
<td style="text-align: center;">Dial</td>
<td style="text-align: center;">Turn</td>
<td style="text-align: center;">Dial</td>
<td style="text-align: center;">Turn</td>
<td style="text-align: center;">Dial</td>
<td style="text-align: center;">Turn</td>
<td style="text-align: center;">Dial</td>
</tr>
<tr>
<td style="text-align: center;">Baseline (AM-FM)</td>
<td style="text-align: center;">0.2940</td>
<td style="text-align: center;">0.2414</td>
<td style="text-align: center;">0.0753</td>
<td style="text-align: center;">0.4648</td>
<td style="text-align: center;">0.1826</td>
<td style="text-align: center;">0.8080</td>
<td style="text-align: center;">0.1840</td>
<td style="text-align: center;">0.5047</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Team 2</td>
<td style="text-align: center;">0.1469</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.1054</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0808</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.1110</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Team 4 (us)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">- SI (DIALEVALML)</td>
<td style="text-align: center;">0.4818</td>
<td style="text-align: center;">0.5342</td>
<td style="text-align: center;">0.3936</td>
<td style="text-align: center;">0.7133</td>
<td style="text-align: center;">0.5890</td>
<td style="text-align: center;">0.8080</td>
<td style="text-align: center;">0.4881</td>
<td style="text-align: center;">0.6852</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- S2</td>
<td style="text-align: center;">0.2625</td>
<td style="text-align: center;">0.3295</td>
<td style="text-align: center;">0.3096</td>
<td style="text-align: center;">0.7030</td>
<td style="text-align: center;">0.5056</td>
<td style="text-align: center;">0.2500</td>
<td style="text-align: center;">0.3592</td>
<td style="text-align: center;">0.4275</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- S3</td>
<td style="text-align: center;">0.4795</td>
<td style="text-align: center;">0.5251</td>
<td style="text-align: center;">0.3656</td>
<td style="text-align: center;">0.6701</td>
<td style="text-align: center;">0.5409</td>
<td style="text-align: center;">0.8080</td>
<td style="text-align: center;">0.4620</td>
<td style="text-align: center;">0.6677</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- S4</td>
<td style="text-align: center;">0.4586</td>
<td style="text-align: center;">0.5039</td>
<td style="text-align: center;">0.3618</td>
<td style="text-align: center;">0.5859</td>
<td style="text-align: center;">0.5412</td>
<td style="text-align: center;">0.5915</td>
<td style="text-align: center;">0.4539</td>
<td style="text-align: center;">0.5604</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Team 5</td>
<td style="text-align: center;">0.3702</td>
<td style="text-align: center;">0.1865</td>
<td style="text-align: center;">0.0701</td>
<td style="text-align: center;">0.1356</td>
<td style="text-align: center;">0.1983</td>
<td style="text-align: center;">0.6830</td>
<td style="text-align: center;">0.2129</td>
<td style="text-align: center;">0.3350</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Team 7</td>
<td style="text-align: center;">0.2214</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.3112</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5644</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.3657</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Average Spearman correlation across the 4 dimensions evaluated for the baseline Deep AM-FM (Zhang et al., 2021b) and all participating teams on the Task 1 (Multilingual metrics) test set. Bold denotes the best result for the corresponding Language, italic denotes our best submission.</p>
<p>Task 1: Multilingual Metrics The results for each team for Task 1 are presented in Table 3, together with all of our submissions. In all languages at both the dialogue and turn level, our submissions vastly outperform others, with the exception of S2, which has comparable results with other participants. This clearly demonstrates the conversational understanding ChatGPT possesses. As expected, the best submission is S 1 , which conducts submetric ensembling with the XLM-R submetrics. This is followed by S3 and S4, which are exclusive ChatGPT submissions with and without ensembling, respectively.</p>
<p>Task 2: Robust Metrics The results for each team for Task 2 are presented in Table 4. Similar to Task 1, in Task 2, our ChatGPT submissions outperform other teams. However, at the dialogue level, the best performing model is AM-FM.</p>
<h3>5.5 Example predictions</h3>
<p>Given the widely publicised emergent capabilities of current LLMs, it is worthwhile exploring where their quality predictions diverge from the annotators. To do so, we checked all instances where ChatGPT (System 4) diverges from the Human Evaluation (HE) annotations by more than 3 points. In all of the detected examples, we noted ChatGPT consistently underestimated the quality of the response when compared to HE.</p>
<p>We present in Table 5 two representative examples. In the first example, we see that ChatGPT</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team</th>
<th style="text-align: center;">Turn (rank)</th>
<th style="text-align: center;">Dial (rank)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline (AM-FM)</td>
<td style="text-align: center;">0.3387 (4)</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 0 0}$ (1)</td>
</tr>
<tr>
<td style="text-align: left;">Team 1</td>
<td style="text-align: center;">0.1537 (6)</td>
<td style="text-align: center;">0.1111 (4)</td>
</tr>
<tr>
<td style="text-align: left;">Team 3</td>
<td style="text-align: center;">0.2697 (5)</td>
<td style="text-align: center;">0.2196 (3)</td>
</tr>
<tr>
<td style="text-align: left;">Team 4 (us)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- SI (DIALEVALML)</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 9 0}$ (1)</td>
<td style="text-align: center;">0.3031 (2)</td>
</tr>
<tr>
<td style="text-align: left;">- S2</td>
<td style="text-align: center;">0.3320</td>
<td style="text-align: center;">0.2335</td>
</tr>
<tr>
<td style="text-align: left;">- S3</td>
<td style="text-align: center;">0.4756</td>
<td style="text-align: center;">0.2979</td>
</tr>
<tr>
<td style="text-align: left;">- S4</td>
<td style="text-align: center;">0.4427</td>
<td style="text-align: center;">0.2492</td>
</tr>
<tr>
<td style="text-align: left;">Team 6</td>
<td style="text-align: center;">0.4190 (2)</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Team 7</td>
<td style="text-align: center;">0.3833 (3)</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 4: Average Spearman correlation and corresponding rank across the 4 dimensions evaluated for the baseline Deep AM-FM and all participating teams on the Task 2 (Robust metrics) test set. Bold denotes the best result for the corresponding Language, italic denotes our best submission.
erroneously underestimates quality due to the inclusion of "weed" in the response. We posit this is due to the RLHF finetuning, which conditions the model to avoid inappropriate or divisive topics. In the second example, we see ChatGPT has trouble understanding the conversation. Although one could argue the HE scores for Correctness and Appropriateness are too high, it seems clear the response is undeserving of a minimum score for all aspects. In fact, if one prompts the model to provide an explanation for Content Richness, it replies the following: "The response attempts to provide some content related to the topic of adolescent sadness, but it is vague and lacks depth. The mention of "Qibing" without any explanation or context</p>
<p>Chatbot: Tennis racquets used to be made from cloth strips stuffed with feathers haha! Then eventually evolved into today's technology
Human: oh i never knew that
Chatbot: Indeed, there was also aluminium construction improvements allowed for the introduction of oversized rackets by 1975 which was manufactured by weed
Label: A: 4.7 C: 4.7 G: 4.3 R: 5.0 ChatGPT: A: 1.0 C: 2.0 G: 2.0 R: 1.0
"The mention of "weed" as the manufacturer may cause confusion or be perceived as inappropriate without further context."
Best: A: 2.2 C: 2.6 G: 1.9 R: 2.2
Human: Don't cry for breakfast.
Chatbot: Cry t for comfort
Human: In the end, young people are always inexplicably sad in adolescence. Which one is Qibing?
Label: A: C: 5.0 G: 5.0 R: 3.0
ChatGPT: A: 1.0 C: 1.0 G: 1.0 R: 1.0
"The response does not directly relate to the context or provide a meaningful answer. It seems unrelated and out of place. The mention of "Qibing" without any explanation further adds to the confusion.
Best: A: 1.3 C: 1.9 G: 1.1 R: 1.2</p>
<p>Table 5: Example turn-level predictions for Appropriateness, Content Richeness, Grammatical Correctness and Relevance. We include the ChatGPT explanation for Appropriateness.
leaves the reader confused. The response could benefit from more specific and informative details about the topic to increase its content richness.". However, if anything, the inclusion of the last sentence increases the richness of the response. Yet, it seems ChatGPT is conflating Content Richness with Relevance. We observe the same behaviour in all other instances we studied, and is in line with the submetric weights (Table 2).</p>
<h2>6 Discussions</h2>
<p>The results from our work on both tasks (Section 5.4) reveals that ChatGPT vastly outperforms typical encoder approaches that are trained to discriminate positive samples from artificially generated negative ones. It is important to note that, compared to the months worth of research dedicated to optimise our encoder models (including curation, training and selection), we were able to easily outperform all other teams and our own encoder models with a day's worth of prompt engineering. This is, in our opinion, a turning point in the paradigm of dialogue evaluation.</p>
<p>In any case, we do find instances where ChatGPT fails to accurately evaluate aspects of quality, as identified in Section 5.5. Future research directions may attempt to tackle the issues of score calibration
by providing prompts that include examples and/or explicitly provide guidelines for scoring.</p>
<p>However, given the current landscape on dialogue generation, and as our submission suggests, dialogue evaluation, it is important to reflect on the value of current quality estimation frameworks. One might argue performing HE or developing metrics that evaluate responses and/or dialogues in terms of linguistic competence (e.g. Grammatical Correctness or Coherence) is no longer informative for the current and future crop of LLMs. Besides becoming ever so clear that these models no longer output responses that are incoherent or incorrect, we are reaching the point where these models are better evaluators than humans themselves (Gilardi et al., 2023). As such, developing metrics that correlate well with HE is becoming increasingly questionable.</p>
<p>One of the main contention points w.r.t the deployment of these models to the public pertain to their "safety" and "trustworthiness". But while "trustworthiness" can be evaluated by connecting the outputs to external and verifiable sources, the notion of "safety" is much more ambiguous. Kempt et al. (2023) suggests considering Positionality, Acceptability, and Value Alignment (PAVA) as features chatbots should have to fulfil appropriateness requirements. However, automatically evaluating if a chatbot has these features using current dialogue evaluation protocols seems implausible. Instead, the development of challenge sets for validation (such as the ones proposed in Valmeekam et al. 2023) appears to be the logical next step for evaluation of future chatbots ${ }^{6}$.</p>
<h2>7 Conclusion</h2>
<p>This paper presents a novel open-domain and reference-free dialogue evaluation framework that leverages strong pretrained LLMs using finetuning and zero-shot prompting. These models, combined with effective ensembling strategies, substantially outperform the previous automatic evaluation paradigm of only training LMs with semisupervised training objectives. In fact, DIALEVALML ranks 1st on both the Robust (1st turn-level, 2nd dialogue level) and Multilingual (1st on both levels) tasks of Track 4 at DSTC11.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Acknowledgements</h2>
<p>This research was supported by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Responsible.AI), and by national funds through Fundação para a Ciência e a Tecnologia (FCT) with references PRT/BD/152198/2021 and UIDB/50021/2020, and by the P2020 program MAIA (LISBOA-01-0247-FEDER-045909).</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4.</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.</p>
<p>Prithiviraj Damodaran. 2021. Parrot: Paraphrase generation for NLU.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks.</p>
<p>Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. Hallucinations in large multilingual translation models.</p>
<p>Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection.</p>
<p>David M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, pages 169-182, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Lishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, and Xiaodan Liang. 2020. GRADE: Automatic graphenhanced coherence metric for evaluating opendomain dialogue systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9230-9240, Online. Association for Computational Linguistics.</p>
<p>Jessica Huynh, Cathy Jiao, Prakhar Gupta, Shikib Mehri, Payal Bajaj, Vishrav Chaudhary, and Maxine Eskenazi. 2023. Understanding the effectiveness of very large language models on dialog evaluation.</p>
<p>Zhihua Jiang, Guanghui Ye, Dongning Rao, Di Wang, and Xin Miao. 2022. IM^2: an interpretable and multi-category integrated metric framework for automatic dialogue evaluation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11091-11103, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Hendrik Kempt, Alon Lavie, and Saskia K. Nagel. 2023. Appropriateness is all you need!</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 986-995, Taipei, Taiwan. Asian Federation of Natural Language Processing.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122-2132, Austin, Texas. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pretraining for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742.</p>
<p>Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. 2017. Towards an automatic turing test: Learning to evaluate dialogue responses. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1116-1126.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020a. Unsupervised evaluation of interactive dialog with DialoGPT. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225-235, 1st virtual meeting. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020b. USR: An unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681-707, Online. Association for Computational Linguistics.</p>
<p>John Mendonca, Alon Lavie, and Isabel Trancoso. 2022. QualityAdapt: an automatic dialogue quality estimation framework. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 83-90, Edinburgh, UK. Association for Computational Linguistics.</p>
<p>John Mendonca, Alon Lavie, and Isabel Trancoso. 2023. Towards multilingual automatic open-domain dialogue evaluation. In Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Prague, Czechia. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia,</p>
<p>Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Vitou Phy, Yang Zhao, and Akiko Aizawa. 2020. Deconstruct to reconstruct a configurable evaluation metric for open-domain dialogue systems. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4164-4178, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. Unbabel's participation in the WMT20 metrics shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 911-920, Online. Association for Computational Linguistics.</p>
<p>Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634-645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Mario Rodríguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, Luis Fernando D'Haro, and Alexander Rudnicky. 2023. Overview of robust and multilingual automatic evaluation metrics for open-domain dialogue systems at dstc 11 track 4. In DSTC11: The Eleventh Dialog System Technology Challenge, 24th Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), Prague, Czechia.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms.</p>
<p>Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William L. Hamilton, and Joelle Pineau. 2020. Learning an unreferenced metric for online dialogue evaluation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2430-2441, Online. Association for Computational Linguistics.</p>
<p>Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui Yan. 2018. Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.</p>
<p>Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao Kambhampati. 2023. On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark).</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all</p>
<p>you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Guangxuan Xu, Ruibo Liu, Fabrice Harel-Canada, Nischal Reddy Chandra, and Nanyun Peng. 2022. EnDex: Evaluation of dialogue engagingness at scale. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4884-4893, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Chrysoula Zerva, Daan van Stigt, Ricardo Rei, Ana C Farinha, Pedro Ramos, José G. C. de Souza, Taisiya Glushkova, Miguel Vera, Fabio Kepler, and André F. T. Martins. 2021. IST-unbabel 2021 submission for the quality estimation shared task. In Proceedings of the Sixth Conference on Machine Translation, pages 961-972, Online. Association for Computational Linguistics.</p>
<p>Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021a. DynaEval: Unifying turn and dialogue level evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5676-5689, Online. Association for Computational Linguistics.</p>
<p>Chen Zhang, Luis Fernando D'Haro, Rafael E. Banchs, Thomas Friedrichs, and Haizhou Li. 2021b. Deep AM-FM: Toolkit for Automatic Dialogue Evaluation, pages 53-69. Springer Singapore, Singapore.</p>
<p>Pengfei Zhang, Xiaohui Hu, Kaidong Yu, Jian Wang, Song Han, Cao Liu, and Chunyang Yuan. 2022. MME-CRS: Multi-Metric Evaluation Based on Correlation Re-Scaling for Evaluating Open-Domain Dialogue. arXiv preprint arXiv:2206.09403.</p>
<p>Tianyu Zhao, Divesh Lala, and Tatsuya Kawahara. 2020. Designing precise and robust dialogue response evaluators. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 26-33, Online. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ See OpenAI Evals for recent collaborative research efforts in this direction.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
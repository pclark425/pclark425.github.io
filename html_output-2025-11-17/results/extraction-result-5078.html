<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5078 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5078</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5078</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-9ed941960641469058fb50ddcdd571ef5c2cba49</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ed941960641469058fb50ddcdd571ef5c2cba49" target="_blank">The Chess Transformer: Mastering Play using Generative Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work demonstrates that natural language transformers can support more generic strategic modeling, particularly for text-archived games, and demonstrates a human-to-transformer interface that correctly filters illegal moves and provides a novel method to challenge the transformer's chess strategies.</p>
                <p><strong>Paper Abstract:</strong> This work demonstrates that natural language transformers can support more generic strategic modeling, particularly for text-archived games. In addition to learning natural language skills, the abstract transformer architecture can generate meaningful moves on a chessboard. With further fine-tuning, the transformer learns complex gameplay by training on 2.8 million chess games in Portable Game Notation. After 30,000 training steps, OpenAI's Generative Pre-trained Transformer (GPT-2) optimizes weights for 774 million parameters. This fine-tuned Chess Transformer generates plausible strategies and displays game formations identifiable as classic openings, such as English or the Slav Exchange. Finally, in live play, the novel model demonstrates a human-to-transformer interface that correctly filters illegal moves and provides a novel method to challenge the transformer's chess strategies. We anticipate future work will build on this transformer's promise, particularly in other strategy games where features can capture the underlying complex rule syntax from simple but expressive player annotations.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5078.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5078.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chess Transformer (GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chess Transformer: GPT-2 fine-tuned on PGN chess games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper fine-tunes OpenAI's GPT-2 (large, 774M parameters) on millions of PGN chess games to generate full chess games in textual PGN form and demonstrates qualitative evidence of learned spatial/strategic play (openings, promotions, castling) though with some illegal or non-viable moves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT-2 (large, 774M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 transformer (unidirectional, attention-based language model) with 774 million parameters (the 'large' GPT-2). Pre-trained on large text corpora (GPT-2 pretraining) and fine-tuned using the gpt-2-simple (Woolf) package on chess PGN text; trained for ~30,000 steps on ~2.8 million PGN games (KingBase + milibrary subsets). Training reported on NVIDIA V100 GPUs (32 GB) with 8-10 hours per run. Reported cross-entropy log loss for the large dataset: smoothed = 0.72, final = 0.79.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (PGN text representation)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Chess is a spatial, deterministic, perfect-information board game on an 8x8 grid (coordinates a1..h8) requiring reasoning about piece locations, legal moves, captures, castling, pawn promotion, checks/checkmate, and multi-move strategy (openings, defenses, tactics and long-term positional planning). In this work the game state is represented as algebraic PGN move notation (text tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Fine-tune GPT-2 to predict the next PGN token given prior tokens (next-token language modeling). Generation is unconditional sampling from the fine-tuned model producing sequences beginning with a Result token. Post-processing includes a filter for illegal moves using a Stockfish-inspired python-chess library (used to block ~10% of illegal moves in live play). Analysis tools include token-frequency statistics, Arena PGN visualization to convert generated PGN into board positions, and attention-head visualizations to inspect which tokens the model attends to.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative and indirect evidence: generated games reproducibly include coherent strategic formations and classic openings (English Opening, Slav Exchange, King's Indian Defense, Nimzowitsch variation, Sicilian defenses, etc.), generated average game length similar to training (67 moves generated vs 73 training), token-level behavior captures spatially meaningful tokens (castling 'O-O', pawn promotions '=' , checks '+', checkmate '#'), attention-head visualizations (Figure 2) show attention patterns over move tokens, and higher-ranked players in training are more often winners in generated games (70.9% of generated games show the higher-ranked player winning). No formal probing, ablation, or neural-activation analyses demonstrating explicit internal board-coordinate representations are reported — evidence is therefore qualitative and behavioral rather than mechanistic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Training/data: ~2.8 million PGN games (KingBase + milibrary subsets), ~30,000 training steps (33,000 reported in figure caption for one run). Loss: cross-entropy log loss (large dataset) smoothed = 0.72, final = 0.79; small milibrary subset achieved <0.1 cross-entropy on its subset. Generation: roughly 1000 games generated per run; 2–8% run-level failure rate for non-viable gameplay (reported in runs), ~10% of generated moves are illegal (reported elsewhere in paper), 971 valid games generated in one large-dataset run; model output is reported to mimic an Elo of ~2,637 (claimed for PGN output), and in generated games the higher-ranked player wins in 70.9% of cases. Game statistics: average generated game length ≈ 67 moves; one in five generated games includes kingside castling; one in eight includes a pawn promotion. Computational: training runtime ~8–10 hours on a single V100 GPU for non-distributed training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: generation of illegal moves (~10%) and non-viable games (2–8% failure rate), potential memorization/parroting rather than true rule understanding, lack of explicit game-tree search or MCTS (no inherent game logic), no formal head-to-head evaluation against high-quality engines like Stockfish reported, evaluation mainly token/count and qualitative opening identification (no rigorous quantitative gameplay benchmark), VRAM limits prevented testing larger GPT-2 (1.5B) models, and the authors acknowledge the philosophical concern whether transformers truly 'understand' game rules or simply reproduce token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Qualitative comparisons only: contrasted with traditional game-tree search and MCTS (AlphaGo/AlphaZero approaches) — the language-model approach uses breadth from large-game corpora rather than deep search; compared informally to smaller GPT-2 variants (124M and 355M) where smaller models train faster and likely underperform (no head-to-head metrics provided). Prior text-based chess engines (n-gram/Lisp engines like Penson, a fastText-based classifier 'fastchess') are discussed as earlier text-trained baselines but no numeric performance comparison is given. No direct comparison against state-of-the-art chess engines or humans in rigorous matches is reported (live-play interface exists but no aggregate win-rate vs humans reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Chess Transformer: Mastering Play using Generative Language Models', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5078.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5078.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Go Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Go Transformer: Natural Language Modeling for Game Play</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (Ciolino et al., 2020) that applied language transformer models to the game of Go; cited in this paper as precedent motivating the chess experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Go Transformer: Natural Language Modeling for Game Play</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based language model (GPT-2 family referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as prior work applying language transformers to board-game play (Go). The current paper does not provide architecture, size, or training specifics for that work beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Go</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Go is a spatial board game (typically 19x19 grid) requiring extensive spatial reasoning about stone placement, territory, influence, and long-range interactions; the paper references Go as another domain where language transformers have been previously applied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Only briefly mentioned as prior work; the current paper does not detail the mechanism used for Go beyond noting it involved language transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Chess Transformer: Mastering Play using Generative Language Models', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5078.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5078.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Presser & Branwen GPT-2 chess demo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Very Unlikely Chess Game</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced demonstration (Presser & Branwen, 2020) that applied GPT-2 to generate chess games, using a filter to remove invalid moves; cited as antecedent work exploring GPT-2 on chess.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Very Unlikely Chess Game</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT-2 (unspecified size in citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in-paper as an earlier exploration of GPT-2 applied to chess with an invalid-move filter; the current paper cites but does not provide further internal experimental detail.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Chess represented via PGN-like textual move notation; requires spatial reasoning about piece locations and legal moves.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Applying GPT-2 to generate move sequences and applying a post-hoc filter on invalid moves (as noted in-citation). No additional technical details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Chess Transformer: Mastering Play using Generative Language Models', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Go Transformer: Natural Language Modeling for Game Play <em>(Rating: 2)</em></li>
                <li>A Very Unlikely Chess Game <em>(Rating: 2)</em></li>
                <li>fastchess <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5078",
    "paper_id": "paper-9ed941960641469058fb50ddcdd571ef5c2cba49",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "Chess Transformer (GPT-2)",
            "name_full": "Chess Transformer: GPT-2 fine-tuned on PGN chess games",
            "brief_description": "This paper fine-tunes OpenAI's GPT-2 (large, 774M parameters) on millions of PGN chess games to generate full chess games in textual PGN form and demonstrates qualitative evidence of learned spatial/strategic play (openings, promotions, castling) though with some illegal or non-viable moves.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI GPT-2 (large, 774M parameters)",
            "model_description": "GPT-2 transformer (unidirectional, attention-based language model) with 774 million parameters (the 'large' GPT-2). Pre-trained on large text corpora (GPT-2 pretraining) and fine-tuned using the gpt-2-simple (Woolf) package on chess PGN text; trained for ~30,000 steps on ~2.8 million PGN games (KingBase + milibrary subsets). Training reported on NVIDIA V100 GPUs (32 GB) with 8-10 hours per run. Reported cross-entropy log loss for the large dataset: smoothed = 0.72, final = 0.79.",
            "puzzle_name": "Chess (PGN text representation)",
            "puzzle_description": "Chess is a spatial, deterministic, perfect-information board game on an 8x8 grid (coordinates a1..h8) requiring reasoning about piece locations, legal moves, captures, castling, pawn promotion, checks/checkmate, and multi-move strategy (openings, defenses, tactics and long-term positional planning). In this work the game state is represented as algebraic PGN move notation (text tokens).",
            "mechanism_or_strategy": "Fine-tune GPT-2 to predict the next PGN token given prior tokens (next-token language modeling). Generation is unconditional sampling from the fine-tuned model producing sequences beginning with a Result token. Post-processing includes a filter for illegal moves using a Stockfish-inspired python-chess library (used to block ~10% of illegal moves in live play). Analysis tools include token-frequency statistics, Arena PGN visualization to convert generated PGN into board positions, and attention-head visualizations to inspect which tokens the model attends to.",
            "evidence_of_spatial_reasoning": "Qualitative and indirect evidence: generated games reproducibly include coherent strategic formations and classic openings (English Opening, Slav Exchange, King's Indian Defense, Nimzowitsch variation, Sicilian defenses, etc.), generated average game length similar to training (67 moves generated vs 73 training), token-level behavior captures spatially meaningful tokens (castling 'O-O', pawn promotions '=' , checks '+', checkmate '#'), attention-head visualizations (Figure 2) show attention patterns over move tokens, and higher-ranked players in training are more often winners in generated games (70.9% of generated games show the higher-ranked player winning). No formal probing, ablation, or neural-activation analyses demonstrating explicit internal board-coordinate representations are reported — evidence is therefore qualitative and behavioral rather than mechanistic.",
            "performance_metrics": "Training/data: ~2.8 million PGN games (KingBase + milibrary subsets), ~30,000 training steps (33,000 reported in figure caption for one run). Loss: cross-entropy log loss (large dataset) smoothed = 0.72, final = 0.79; small milibrary subset achieved &lt;0.1 cross-entropy on its subset. Generation: roughly 1000 games generated per run; 2–8% run-level failure rate for non-viable gameplay (reported in runs), ~10% of generated moves are illegal (reported elsewhere in paper), 971 valid games generated in one large-dataset run; model output is reported to mimic an Elo of ~2,637 (claimed for PGN output), and in generated games the higher-ranked player wins in 70.9% of cases. Game statistics: average generated game length ≈ 67 moves; one in five generated games includes kingside castling; one in eight includes a pawn promotion. Computational: training runtime ~8–10 hours on a single V100 GPU for non-distributed training.",
            "limitations_or_failure_cases": "Reported limitations include: generation of illegal moves (~10%) and non-viable games (2–8% failure rate), potential memorization/parroting rather than true rule understanding, lack of explicit game-tree search or MCTS (no inherent game logic), no formal head-to-head evaluation against high-quality engines like Stockfish reported, evaluation mainly token/count and qualitative opening identification (no rigorous quantitative gameplay benchmark), VRAM limits prevented testing larger GPT-2 (1.5B) models, and the authors acknowledge the philosophical concern whether transformers truly 'understand' game rules or simply reproduce token sequences.",
            "comparison_baseline": "Qualitative comparisons only: contrasted with traditional game-tree search and MCTS (AlphaGo/AlphaZero approaches) — the language-model approach uses breadth from large-game corpora rather than deep search; compared informally to smaller GPT-2 variants (124M and 355M) where smaller models train faster and likely underperform (no head-to-head metrics provided). Prior text-based chess engines (n-gram/Lisp engines like Penson, a fastText-based classifier 'fastchess') are discussed as earlier text-trained baselines but no numeric performance comparison is given. No direct comparison against state-of-the-art chess engines or humans in rigorous matches is reported (live-play interface exists but no aggregate win-rate vs humans reported).",
            "uuid": "e5078.0",
            "source_info": {
                "paper_title": "The Chess Transformer: Mastering Play using Generative Language Models",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "Go Transformer",
            "name_full": "The Go Transformer: Natural Language Modeling for Game Play",
            "brief_description": "Prior work (Ciolino et al., 2020) that applied language transformer models to the game of Go; cited in this paper as precedent motivating the chess experiments.",
            "citation_title": "The Go Transformer: Natural Language Modeling for Game Play",
            "mention_or_use": "mention",
            "model_name": "Transformer-based language model (GPT-2 family referenced)",
            "model_description": "Referenced as prior work applying language transformers to board-game play (Go). The current paper does not provide architecture, size, or training specifics for that work beyond the citation.",
            "puzzle_name": "Go",
            "puzzle_description": "Go is a spatial board game (typically 19x19 grid) requiring extensive spatial reasoning about stone placement, territory, influence, and long-range interactions; the paper references Go as another domain where language transformers have been previously applied.",
            "mechanism_or_strategy": "Only briefly mentioned as prior work; the current paper does not detail the mechanism used for Go beyond noting it involved language transformers.",
            "evidence_of_spatial_reasoning": null,
            "performance_metrics": null,
            "limitations_or_failure_cases": null,
            "comparison_baseline": null,
            "uuid": "e5078.1",
            "source_info": {
                "paper_title": "The Chess Transformer: Mastering Play using Generative Language Models",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "Presser & Branwen GPT-2 chess demo",
            "name_full": "A Very Unlikely Chess Game",
            "brief_description": "A referenced demonstration (Presser & Branwen, 2020) that applied GPT-2 to generate chess games, using a filter to remove invalid moves; cited as antecedent work exploring GPT-2 on chess.",
            "citation_title": "A Very Unlikely Chess Game",
            "mention_or_use": "mention",
            "model_name": "OpenAI GPT-2 (unspecified size in citation)",
            "model_description": "Described in-paper as an earlier exploration of GPT-2 applied to chess with an invalid-move filter; the current paper cites but does not provide further internal experimental detail.",
            "puzzle_name": "Chess",
            "puzzle_description": "Chess represented via PGN-like textual move notation; requires spatial reasoning about piece locations and legal moves.",
            "mechanism_or_strategy": "Applying GPT-2 to generate move sequences and applying a post-hoc filter on invalid moves (as noted in-citation). No additional technical details provided in this paper.",
            "evidence_of_spatial_reasoning": null,
            "performance_metrics": null,
            "limitations_or_failure_cases": null,
            "comparison_baseline": null,
            "uuid": "e5078.2",
            "source_info": {
                "paper_title": "The Chess Transformer: Mastering Play using Generative Language Models",
                "publication_date_yy_mm": "2020-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Go Transformer: Natural Language Modeling for Game Play",
            "rating": 2
        },
        {
            "paper_title": "A Very Unlikely Chess Game",
            "rating": 2
        },
        {
            "paper_title": "fastchess",
            "rating": 1
        }
    ],
    "cost": 0.013031999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Chess Transformer:</h1>
<h2>Mastering Play using Generative Language Models</h2>
<p>David Noever ${ }^{1}$, Matthew Ciolino ${ }^{2}$, Josh Kalin ${ }^{3}$<br>PeopleTec, ${ }^{1,2,3}$ Auburn University ${ }^{3}$<br>david.noever@peopletec.com, ${ }^{1}$ matt.ciolino@peopletec.com, ${ }^{2}$ jzk0098@auburn.edu ${ }^{3}$</p>
<h2>Abstract</h2>
<p>This work demonstrates that natural language transformers can support more generic strategic modeling, particularly for text-archived games. In addition to learning natural language skills, the abstract transformer architecture can generate meaningful moves on a chessboard. With further fine-tuning, the transformer learns complex gameplay by training on 2.8 million chess games in Portable Game Notation. After 30,000 training steps, OpenAI's Generative Pre-trained Transformer (GPT-2) optimizes weights for 774 million parameters. This fine-tuned Chess Transformer generates plausible strategies and displays game formations identifiable as classic openings, such as English or the Slav Exchange. Finally, in live play, the novel model demonstrates a human-to-transformer interface that correctly filters illegal moves and provides a novel method to challenge the transformer's chess strategies. We anticipate future work will build on this transformer's promise, particularly in other strategy games where features can capture the underlying complex rule syntax from simple but expressive player annotations.</p>
<h2>Introduction</h2>
<p>This research learns the rules of chess without direct expert intervention or heuristic guidance. Extending previous work on learning Go games with language transformers (Ciolino et al. 2020), the work benefits from large archives of chess notation in text and a game replay visualization tool. We combine the millions of chess games in text formats with the remarkable feature learning parts of the large GPT-2 model (Radford et al. 2018). Unlike a traditional sequence generator, the transformers support built-in parallelism and a directed attention mechanism to overweight key features. The original contributions of this research include generating plausible chess moves following the fine-tuning of the large GPT-2 transformer with its 774 million model parameters. A second innovation features a novel game interface where</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>human players can challenge the transformer in live play. To take advantage of graphical processing units (GPU-acceleration) we host the shared games on Google's Colaboratory platform (Colab) ${ }^{1}$.
In contrast to our previous exploration of transformers to play Go, chess has received considerable attention in language modeling. A Lisp chess engine (Penson) ${ }^{2}$ applied a frequency map and Portable Game Notation (PGN) Mentor Database. The frequency map establishes conditional move probabilities for each forward move. To produce move probabilities from a board state vector (Ahle 2018), another text-trained chess engine (fastchess) ${ }^{3}$ applied a popular text classification library as a one-layer plus soft-max model. When first open-sourced in 2016, their fastText classifier (Bojanowski et al. 2017) from Facebook AI Research was state-of-the-art. Facebook employed sentence structure features with bags of both words and n-grams, two strategies now overtaken in the literature by the rapid growth of transformers such as Google's BERT (Devlin et al. 2018) and OpenAI's GPT. These newer language models supplement the long tradition of using game trees to formalize decisionmaking and chess strategies (Nornai 1997). One can postulate that the decision tree model is deep (enumerating $60+$ moves ahead) but narrow compared to the language-based alternatives presented by our chess transformers. This approach further contrasts with the Monte Carlo Tree Search, (MCTS) employed so effectively with Alpha-Go and reinforcement learning (Silver et al. 2018).
The application of models outside their initial languagerelated training sets has attracted interest in other cross-domain problems, for example, in imagery (Parmar et al. 2018) and audio (Child et al. 2019). Presser and Branwen (2020)</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Generated example games from GPT-2. Each transformer game generated is video captured and split for analysis move-bymove using Portable Game Notation (PGN) as inputs with both moving pieces and automated strategy notations
first explored the application of GPT-2 to chess with a filter on invalid moves. Like recurrent neural networks, transformers however specialize in modeling sequential data like language (or game moves here), but without relying strictly on presentation order during training. Its core architecture features encoder-decoder cycles that apply weights to derive features with its unique 'attention' mechanism that effectively overweighs the most relevant features as it learns. The transformers' remarkable abilities to generate text arises from its parallelism during training, which enables traditional neural architectures to ingest vast amounts of internetscale textual inputs. The chess application of GPT-2 suggests new and innovative ways to expand chess training data with high-level simulation data. For instance, our exploration of only high-ranking gameplay (e.g. training Elo ranks $&gt;2,200$ ) highlights the specialized transformer model may encode the basic features of multiple games and learn their winning strategies without human guidance or direction.</p>
<h2>Methods</h2>
<h2>Datasets and Pre-processing</h2>
<p>To generate chess training data for the language models, we transformed chess game archives to single lines beginning with either the simple result (win/lose/draw) or the result plus Elo ranks for black and white. In PGN format, the smaller dataset (milibrary.org) ${ }^{4}$ consists of 11,291 games and over 800,000 player moves. For a game lasting an hour, the simulation learns from observing the equivalent to 10,000 hours of chess, roughly a year of continuous human play. The average Elo player rank of 1,815 was equally</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>distributed between white and black with the average game length as 73 moves.</p>
<p>We also used the KingBase ${ }^{5}$ dataset of 2.19 million PGN games archived as no older than 1990 and by players with Elo ranks greater than 2,000. Since almost 5,000 humans have an Elo ranking over 2,200 in 2016, this database presents language models with learning the move capabilities (without heuristics, hints, or rules) at the level of expert or candidate master.</p>
<p>The PGN move notation offers a loosely structured text file with metadata headers in square brackets followed by alternating black and white gameplay. The format offers a plain text training set that proves easy for humans to read or write, provides input to natural language modeling (transformers), and generates parsed games for machine-reading and display. The only header information needed for game replay proved to be the Result tag, so all other dates, player names, and locations were stripped for training purposes.</p>
<h2>Game Notation</h2>
<p>To evaluate the success of the natural language model, just counting interesting PGN tokens gives a simple representation of what the model learns. The Arena game (Figure 1) interface illustrates the role of text in describing chess specifically. The algebraic move text notation (PGN) represents an equivalent token or word size ranging between 2-5 characters. The capture notation ("x"), numerical order of play $(1 \ldots \mathrm{~N})$, and piece abbreviation $(\mathrm{K}=$ king; $\mathrm{Q}=$ queen; $\mathrm{R}=$ rook, $\mathrm{B}=$ bishop; $\mathrm{N}=$ knight; P (or empty) = pawn). For knights, bishops, and rooks, their original location offers a unique</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Visualization of GPT-2 Attention Heads. Examples shown for layer $0 \&amp; 6$ as samples.
placement (Nge2 specifies Knight on the upper right g column, moving to e2). The destination of any move follows the standard game board locations (from white's perspective) with lower left (a1) to upper right (h8). Other specialized moves get custom notation: castling kingside (O-O) or queenside (O-O-O) and pawn promotions (appending an " $=$ " and promotion piece to a queen from e 8 is written $\mathrm{e} 8=\mathrm{Q}$ ). Moves that put opponents in check (" + ") or checkmate (" $#$ ") generate a sequence if pawn promotes to a queen $(\mathrm{Q})$ for checkmate as $\mathrm{e} 8=\mathrm{Q} #$, which also represent the 5 character (longest) token for the natural language model (other than "Result"). A key outcome of a successful language model would preserve the limited vocabulary and mimic the alternating player sequences making legal moves and generating strategic positions.</p>
<h2>Language Models</h2>
<p>OpenAI's GPT-2 provides a convenient language model for text generation. We specialize in our approach using the Woolf version of the python package, gpt-2-simple ${ }^{6}$. In brief, GPT-2 represents a language model trained on 40 GB of text data as a corpus of highly ranked Reddit posts (Radford et al. 2018). The unidirectional training objective centers on predicting the next word (or token in a sequence), given all the previous ones. The second version from the original GPT scaled up the parameters (10x) and training data (10x), then showed promising signs of plausible machine text generation. One notable part of its 2-byte encodings also includes spacing, so the overall format of a given text document often is reproducible for poetry, film scripts, or other game notations.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Common Move Frequency Statistics of diverse example moves from 1000 games from the generative model.</p>
<p>The evolution of transformers as sequence predictors follow on from problems encountered when training previous language models: 1) recurrent neural networks suffer from vanishing gradients; 2) long short-term memory (LSTM) networks predict unidirectionally with limited context and difficult parallelism. The transformer model introduced socalled "attention" which over-weights key parts of an entire sequence, train in parallel and process huge datasets without supervision or language labeling. As illustrated using GPT2 visualization of chess moves in Figure 2, each token of the chess game receives varying importance through the transformers encoding-decoding layers (Vig 2019). For the present purposes, this core or universal language structure provides the pre-trained architecture and weights to fine-tune for many specialized tasks including chess game generation.</p>
<h2>Train Test Cycles</h2>
<p>We perform 30,000 training steps and record plausible gameplay from unconditional (random) sampling without a prefix or trigger prompt. The leading token of a viable game to visualize begins with "[Result ...] followed by whether the game gave a win to black (" 0 -1 ") or white (" 1 -0 ") or ended in a draw (" $1 / 2-1 / 2$ "). Previous experiments using the small (124M) and medium (355M) models for GPT-2 for the game of Go motivated a starting point with the larger (774M) model. The extra-large (1.5 B) hyperparameter model was not tested owing to its large VRAM needs ( $&gt;6$ GB base model). These experiments follow the same progression of the small (124M) model being 4 times faster to train than the large one (774M). We ran multiple passes through the mixed, black-win, white-win, and draw datasets. Each run generated approximately 1000 games, with a 2-8\% failure rate for non-viable gameplay or internally for illegal moves. We employed NVIDIA V100 GPUs ( 32 GB VRAM) as single units on a DGX supercomputer</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>$$
H_{P}(q)=-\frac{1}{N} \sum_{i=1}^{N} y_{i} \cdot \log \left(p\left(y_{i}\right)\right)+\left(1-y_{i}\right) \cdot \log \left(1-p\left(y_{i}\right)\right)
$$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4. Training large GPT-2 with 2.8 million chess games in PGN text. The fine-tuned model uses the 774 million parameter (large) model with 33,000 training steps and achieves an average cross-entropy log loss value (smoothed $=0.72$, final $=0.79$ ).
(approximately 1 petaflop for 4 x GPUs). Each run took approximately $8-10$ hours for non-distributed training.</p>
<h2>Train Game Analysis Suite</h2>
<p>To analyze gameplay, we rely on the open-source Athena visualization for initial identifying interesting or classic game moves, particularly for commentary on opening moves (like "English Opening" or "Slav Exchange"). With a token counter (Figure 3), we analyze the frequency of some key moves learned by the language model: castling, pawn promotion (" $=$ "), check (" + "), and checkmate ("#").</p>
<h2>Results</h2>
<p>Even on the smaller subset ( 11 k milibrary.org games) of expert training inputs, the large GPT-2 learns the input text with a cross-entropy $\log$ loss below 0.1 , which generally represents high recall. This result compares to the more diverse but higher loss ( 0.72 ) for the same number of training steps (30k) but with the 10,000-fold larger Kingbase dataset (Figure 4). One extension of the current work is to evaluate the chess transformer by comprehensively compiling token frequencies for these classic strategic formations (e.g. simple counts of check moves, " + ", or "O-O"). Because of the PGN notation, this statistical grading follows simply as token counting. To compare player success, we split apart all the wins into three classes of black wins, white wins, and
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5. Strategic positions from large GPT-2 generation. Upper left shows a classic English opening, upper right, a Slav Exchange; lower left, a King's Indian Defense (KID) and lower right, a Nimzowitsch Variation.
draws. In this way we can train subsets of winning black players, then play those to get mock Elo rankings for the learning cases vs. random or mixed player colors. If an initial player enters the Chess Transformer at Elo base (800), losing 894 games successively against an expert with Elo 2,000 or greater would likely drop the white player to the floor rating of 100 .</p>
<p>We also present this method to understand: 1) the transformer is learning a winning (and losing) side; 2) the effect of training data on outcomes. Generally, we find that after training against the smaller (milibrary.org) or larger (Kingbase) archives, both the chess transformers generate plausible gameplay. The PGN output particularly from the large archive and GPT-2 model mimics the Elo rank of 2,637, and equally balanced between white and black for 971 new games generated as valid. One interesting way to test if the chess transformer captures the syntax and meaning of PGN gameplay is to check for internal game consistency. For instance, do the generated games correctly show the higherranking Elo player as more likely to win? In $70.9 \%$ of generated games, the higher-ranked player (either white or black) is shown as the one who won. Some other simple tests include:</p>
<ul>
<li>Does it generate the back-and-forth play? (yes)</li>
<li>Does it generate illegal moves? (yes, 10\%)</li>
<li>Does it generate impossibly repetitive play? (no)</li>
<li>Does it mix up the ordinal play sequence? (no)</li>
</ul>
<p>The average game length in the milibrary.org data is 73 moves, while for the generated games, we find a similar 67 moves per game. A key test of the chess transformer is whether it exhibits repetitive play (overfitting). Nearly one in five generated games shows at least one player castling to the kingside, with one in eight performing a pawn promotion.</p>
<p>In the Discussion section, we revisit the more strategic formations for the chess openings to understand if a coherent game approach follows from the transformer training. As an example, does the chess transformer generate more diverse openings than just the classic English Opening (1. c4)? With the game commentaries built into the Arena chess visualization, each replayed game from the chess transformer conveniently annotates strategic positions, while automatically moving the pieces according to the PGN we generate from the fine-tuned GPT language model. Notable examples shown in Figure 5 for these strategic positions do highlight with Arena the following: English opening (1. c4), Slav Exchange (4. Nc3 Nf6), King's Indian Defense (KID, 2. g3 Nf6 3. Bg2 Bc5) and a Nimzowitsch variation (2. Nc6). Not shown, are other noteworthy strategic movements generated from the chess transformer, such as Sicilian Defense (1. e4 c5); Reli Opening; Russian Game (Petroff Defense); Sicilian: Najdorf; English King's (2. Nc3 Nf8); English Four Knights (4. g3 d5 5. Cxd5); Neo-Old Indian (2. C4 e5 3. Nf3), King's Pawn Game; and Queen's Pawn Game.</p>
<h2>Discussion</h2>
<p>In addition to the current training methods, promising future work could segregate the training towards a biased subset and thereby contrast those specialized behaviors against the balanced white-black play described here. A similar course exists for comparing weak and strong players based on the subset of high and low ELO rankings. Since ELO is based on strength of opponent, one goal of that research centers on getting a rating for GPT-2 modeling itself. For example, does the smaller (124M) hyperparameter model underperform against the larger (774M) fine-tuned ones. While the current work focuses on plausible or interesting chess moves to learn in such a roundabout way, various other research paths could isolate the prompt or trigger phrases within GPT-2 finetuning and simulate better game rhythm from some midpoint in the game. As it stands, these simulations assume essentially a blank slate initial condition with no priors. The main result demonstrates that language modeling alone can generate plausible game playing by just observing the game and inferring all the rules and heuristics without manual interventions.</p>
<p>Two shortcomings of this work deserve attention. First, how is a language model able to learn chess? The GPT-2 has memorized a series of tokens and like any number of other
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 2. Human vs. Machine in Live Play with Chess Transformer. The Colaboratory notebook includes pre-trained medium GPT-2 models and instructions for human (white) vs. machine (black) in head-to-head game play: rb.gy/dsdphc
statistical methods has figured out how to maximize rewards. Couldn't a Markov chain also string together enough tokenized abstract moves to convince the casual observer that the machine somehow comprehended the game? This big question segues to the second objection, which has featured prominently in the more advanced GPT-3 commentary. A common critique of transformers for text generation has centered on the simple hack test of " $2+2=$ " or " $2+$ two $=$ "? questions. In most cases, all transformers either repeat some joke from its training data (Reddit in the case of GPT) or otherwise mangled what seems obvious to even a second grader. In other words, are transformers mere parrots, or have they read sufficiently deep and far to capture the essence of a universal language model? Given the other multitasks applied to transformers, some challenges seem more compatible with the universal label. For example, topic summaries, question/answer, and one-shot translators provide some convincing and potentially profound insight into human and machine language. We investigate games in this realm of language archives for training data as an interesting example that also mirrors some of the strategic and interpretable aspects of language and knowledge understanding. Whether a human chess grandmaster has remarkable memory skills for previous winning moves seems largely irrelevant to their Elo rank or their formidable tournament play. Similarly, whether a 774 million hyperparameter language (or token) model has overfitted the game space for</p>
<p>complex games like chess or Go, may also appear largely a philosophical question to ponder. The notion that a trained Elo 2,000 model consistently beats a lesser opponent suggests that perhaps there is a future better GPT-2 or GPT-3 text generator that can play chess at a consistent super-human level. It is worth noting finally that chess as a human endeavor (like Go) has largely been conceded to machine learning. Like calculating the square root of 23 or any other arithmetic operation, there is no real contest between how the human brain evolved and how a specialized calculator can perform in practice. What remains remarkable about creative gameplay however is the cross-over between those essential human traits, like language itself or creative inspiration, and the current progress in building massive transformer models.</p>
<p>A major challenge to the current approach is whether the GPT-2 gameplay is any good. Does the trained model advance beyond a parroted amateur? To examine this question systematically, the live gameplay interface (Figure 6) allows humans to play against the chess transformer and rate its overall effectiveness for themselves in match play. The live play relies on filtering approximately $10 \%$ of illegal moves using a Stockfish-inspired chess library (pythonchess) ${ }^{7}$. In this paper, we have offered a few experimental criteria to answer other skill-related questions, such as segmenting training data into advanced or beginner examples to learn or looking for classic opening moves.</p>
<h2>Conclusion</h2>
<p>Playing text-based games (including chess) proves possible by fine-tuning a custom GPT-2 language model. Using the PGN notation, full games of chess and its complex moves are cataloged in text. This represents another domain in which language models can be benchmarked against. Playing against the transformer reveals strong early gameplay as the number of strategies learned is large and weaker gameplay as the number of learned strategies falls significantly. While traditional game agents are trained with inherent game logic and MCTS's depth search, this approach highlights the notion that a breath search of millions of games can allow a language model to define a game's rules and strategy by itself.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to thank the PeopleTec Technical Fellows program for encouragement and project assistance. We are grateful to the front-line emergency workers who do their difficult work during the COVID-19 pandemic.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>References</h2>
<p>Bojanowski, Piotr, et al. "Enriching word vectors with subword information." Transactions of the Association for Computational Linguistics 5 (2017): 135-146.
Bory, P. (2019). Deep new: The shifting narratives of artificial intelligence from Deep Blue to AlphaGo. Convergence, 25(4), 627642 .
Ciolino, M., Noever, D. \&amp; Kalin, J. (2020). The Go Transformer: Natural Language Modeling for Game Play. arXiv preprint arXiv:2007.03500.
Child, R., Gray, S., Radford, A., \&amp; Sutskever, I. (2019). Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.
Devlin, J., Chang, M. W., Lee, K., \&amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Kirubarajan, A., \&amp; Dugan, L. (2020) Learning to Trick Humans: Evaluation Criteria for Human-Written and Computer-Generated Text. kirubarajan.nyc3.digitaloceanspaces.com/learning_to_trick_humans.pdf
Klein, T., \&amp; Nabi, M. (2019). Learning to Answer by Learning to Ask: Getting the Best of GPT-2 and BERT Worlds. arXiv preprint arXiv:1911.02365.
Lapan, M. (2018). Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more. Packt Publishing Ltd.
Lee, J. S., \&amp; Hsiang, J. (2020). PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata. arXiv preprint arXiv:2001.03708.
Müller, M., \&amp; Tegos, T. (2002). Experiments in computer Amazons. More Games of No Chance, 42, 243-260.
Noumai, C. F. (1997, July). Multiagent Chess Games. In Deep Blue Versus Kasparov: The Significance for Artificial Intelligence (pp. 45-52).
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer, N., Ku, A., \&amp; Tran, D. (2018). Image transformer. arXiv preprint arXiv:1802.05751.
Presser, S., Branwen, G. (2020) A Very Unlikely Chess Game, slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/
Qi, D., Su, L., Song, J., Cui, E., Bharti, T., \&amp; Sacheti, A. (2020). ImageBERT: Cross-modal pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., \&amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9. github.com/openai/gpt-2
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... \&amp; Chen, Y. (2017). Mastering the game of go without human knowledge. nature, 550(7676), 354-359.
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... \&amp; Lillicrap, T. (2017). Mastering Chess and Shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815.
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... \&amp; Lillicrap, T. (2018). A general reinforcement</p>
<p>learning algorithm that Masters Chess, Shogi, and Go through selfplay. Science, 362(6419), 1140-1144.
Tang, R., Lu, Y., \&amp; Lin, J. (2019, November). Natural language generation for effective knowledge distillation. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019) (pp. 202-208).
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \&amp; Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 59986008).</p>
<p>Vig, J. (2019). OpenAI GPT-2: Understanding Language Generation through Visualization. Towards Data Science, via Medium, March, 5 .
Vig, J. (2019). A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714. github.com/jessevig/bertviz</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ github.com/niklasf/python-chess&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ gsutil cp gs://gpt-2-poetry/data/kingbase-ftfy.txt&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
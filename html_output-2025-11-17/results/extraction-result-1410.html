<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1410 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1410</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1410</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-e0889fcee1acd985af76a3907d5d0029bf260be9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e0889fcee1acd985af76a3907d5d0029bf260be9" target="_blank">Search on the Replay Buffer: Bridging Planning and Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.</p>
                <p><strong>Paper Abstract:</strong> The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards finding a sub-optimal solution. We introduce a general control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment -- namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1410.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1410.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Replay buffer (non-parametric world model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replay buffer used as a non-parametric generative model / state graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The replay buffer of previously observed states is treated as a non-parametric world model: stored observations serve as sampled valid states (graph nodes) for planning, avoiding the need to learn or roll out an explicit forward dynamics model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Replay buffer (non-parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A non-parametric, data-driven model consisting of stored observed states (images or low-dimensional states). Planning is performed by constructing a graph whose nodes are buffer entries and whose edge weights are estimated distances (from a learned value function). The buffer acts as a sampler / implicit state space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>non-parametric world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>navigation and goal-reaching (2D navigation and visual navigation in SUNCG houses)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Indirectly measured by (1) alignment of predicted distances (from value function) with empirical reachability (precision-recall AUC) and (2) downstream task success rate when planning over the buffer (fraction of goals reached).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not a learned predictor itself; fidelity depends on the paired distance predictor. Using SoRB with the replay buffer produced ~90% success on distant visual navigation goals in the paper's experiments; buffer-based planning AUC (distance vs reachability) exceeded SPTM by ~22% (average across seeds) for the distance predictor used.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: nodes are actual observed states (images / coordinates), so plans (waypoints) are directly visualizable and semantically interpretable as real observations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of planned waypoints (image thumbnails), inspection of graph shortest paths; qualitative rollout visualization in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Costs dominated by evaluating value function on buffer pairs: naïve Dijkstra would be O(|B|^2) value evaluations per query. Implementation amortizes with periodic full-pairwise evaluation and Floyd–Warshall (O(|B|^3) amortized) and then O(|B|) per-shortest-path query. Typical buffer sizes: training buffer 100k, search buffer 1k (used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More efficient and reliable for planning in high-dimensional observation spaces than learning a parametric generative model for images (avoids training a generative model); compared to model-free RL alone, planning over the buffer yields substantially higher success rates on long-horizon tasks (e.g., ~90% vs large degradation for baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Using the replay buffer for planning (SoRB) achieved ~90% success on distant goals in the visual navigation benchmark and ~80% success on 10-step goals in held-out houses (generalization), whereas goal-conditioned RL alone achieved <20% on those held-out 10-step goals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The replay buffer provides concrete, reachable waypoints for long-horizon planning; its utility depends on (1) coverage of relevant states in the buffer and (2) accuracy of the distance predictor used to weight edges. A modest buffer (100–1000 states) sufficed in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Tradeoffs include (1) memory / compute for storing and evaluating pairwise distances over the buffer vs the simplicity and interpretability of real-state waypoints, and (2) sensitivity to buffer coverage—too small or poorly sampled buffers can fail to provide feasible paths.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of a separate, relatively small search buffer (~1k) for graph construction; periodic full evaluation of value function over buffer pairs and caching shortest-path distances via Floyd–Warshall; MAXDIST threshold to prune edges longer than a hyperparameter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to learned forward generative models, the replay-buffer approach avoids generating infeasible states and simplifies planning by operating over actual observed states. Versus model-free RL alone, it enables long-horizon planning by chaining short-horizon controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations: keep a modest-size search buffer (100–1k observations sufficed), tune MAXDIST (critical hyperparameter), amortize pairwise value evaluations (precompute with Floyd–Warshall), and ensure the distance predictor is calibrated (distributional RL + ensembles).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1410.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1410.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q-distance model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal-conditioned Q-function interpreted as a shortest-path distance estimator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The goal-conditioned Q-function (trained with reward -1 per step) is used as a learned world-model abstraction: Q(s,a,g) approximates negative shortest-path distance to goal g, providing edge weights for graph-based planning over the replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to achieve goals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goal-conditioned Q-function as distance model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A value-based model where the scalar Q (or V) is trained with a -1 per-step reward and termination upon reaching the goal; the value approximates -d_sp(s,g) (negative shortest-path distance). This scalar model substitutes for an explicit forward model by predicting reachability distance between states.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>value-based distance model / implicit inverse world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>goal-reaching navigation tasks (2D and image-based visual navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Alignment between predicted distance ordering and empirical reachability; measured via precision-recall AUC for predicting whether the policy reaches the goal and by downstream planning success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>The paper reports that SoRB's learned distances have 22% higher average AUC (precision-recall) than SPTM's reachability predictor across five seeds; exact numeric AUC values not provided. Downstream, planning with these distances yields ~90% success on difficult navigation goals.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: outputs are scalar distances (interpretable as expected steps-to-goal), but the internal neural-network representation is a black box; predicted distances are directly interpretable as edge lengths in the constructed graph.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Evaluation via precision-recall curves linking predicted distances to empirical reachability; visualization of planned waypoint sequences and comparison of rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference cost: one forward pass of Q-network per (state,goal) pair evaluated. For constructing the graph over |B| nodes, amortized pairwise evaluation is O(|B|^2) forward passes (cached) with occasional Floyd–Warshall O(|B|^3) to get all-pairs shortest paths.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally efficient than full parametric forward models that must predict high-dimensional observations; avoids rollout cost of forward models but requires many Q evaluations for dense graphs. Compared to model-free policies without planning, it adds planner overhead but yields much higher long-horizon success.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used in SoRB, the Q-distance model enabled successful planning to distant goals with ~90% success in the visual navigation task and strong generalization to held-out houses (~80% for 10-step goals).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High: accurate calibration of Q as distance (especially with distributional RL and ensembles) directly improves planning quality; inaccuracies (underestimation) cause 'wormholes' that break planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Scalar distance prediction simplifies planning but cannot capture multimodal transition dynamics; inaccurate predictions produce catastrophic planning errors ('wormholes'); improving fidelity (distributional RL, ensembles) increases compute and training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use -1 per-step reward to force Q→negative shortest-path distance; use off-policy goal relabelling; represent distances distributionally (discrete bins) and aggregate via ensembles to quantify uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with inverse-model supervised policies (e.g., SPTM's locomotor), Q-distance provides better-aligned distance estimates for planning; compared to forward dynamics models, Q-distance predicts task-relevant scalar (distance) rather than high-dimensional observations, making learning easier and more robust for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends (and uses) distributional RL to discretize distances, ensembles to avoid false short-distance predictions, a MAXDIST cutoff to prune unrealistic edges, and conservative aggregation over ensembles when planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1410.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1410.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional RL (C51)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional reinforcement learning (C51)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Distributional RL (C51) is used to represent the value function as a discrete distribution over distance bins, which naturally encodes finite vs. infinite/unreachable distances and yields stable targets when learning distances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A distributional perspective on reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Distributional Q-function (C51) as a distance model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The value is represented as a categorical distribution over N discrete bins, where each bin corresponds to an integer distance (steps-to-goal). The final bin is a catch-all for >= N steps, enabling representation of very large / effectively infinite distances and preventing ill-defined scalar Q predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>distributional value model (discrete categorical distribution over distances)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>goal-reaching and planning (2D navigation and visual navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality of distance predictions assessed by how well the predicted distribution/order aligns with empirical reachability (precision-recall AUC) and downstream planning success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Ablation shows distributional RL is critical: the variant without distributional RL 'performed poorly' (worse than random policy in at least one experiment). Exact numeric fidelity for the distributional predictor not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: output distribution bins map directly to integer distances, giving an interpretable probabilistic estimate over discrete distances; internal features remain black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Bins correspond to integer steps, so inspecting the modal bin or expected distance provides interpretable distance estimates; evaluation via precision-recall and task success.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Slightly higher than scalar Q due to predicting a categorical distribution over N atoms; cost scales with number of atoms N. No specific parameter counts or runtimes provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Empirically more stable and effective for distance learning than standard scalar Q-learning for the -1 reward formulation; avoids clipping issues and represents unreachable states naturally.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Using C51 as the underlying goal-conditioned policy/distance estimator enabled SoRB to plan successfully in image-based navigation tasks; omission of distributional RL degraded performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Distributional representation aligns with the integer-valued nature of distances (steps), aiding learning and preventing ill-posed infinite values; this translates to better planning and task success.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Distributional RL increases output dimensionality (N bins) and modestly increases compute, but yields major gains in robustness and fidelity when predicting distances, making it favorable for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discretize distances into bins B_1..B_N with a catch-all final bin for >=N steps; train by minimizing KL divergence between predicted and shifted target distributions (right-shift target scheme appropriate for -1 reward).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed scalar Q-learning in this context (scalar Q struggled with unreachable states); compared to learned forward models, distributional RL focuses on task-relevant scalar distances rather than full observation prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests choosing N large enough to cover relevant planning horizons and using the final bin as overflow; pairing distributional RL with ensembles improved stability and planning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1410.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1410.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensembles of value functions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensembles (bootstrap-style) of distributional Q-networks for uncertainty-aware distances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble of independently initialized and trained (distributional) Q-networks is used to estimate uncertainty in distance predictions and avoid spurious short-distance estimates ('wormholes') that break planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simple and scalable predictive uncertainty estimation using deep ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ensemble of distributional Q-networks</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multiple independent distributional critics are trained on the same data (bootstrap-style approximation). During planning, their distance predictions are aggregated (average or max was used) to obtain a more robust estimate and reduce false optimistic distance predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>ensemble uncertainty estimator for value-based world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual navigation planning and goal-reaching</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Measured by downstream planning robustness: reduction in spurious short-distance predictions and increased success rate for distant goals. Also indirectly assessed via increase in task success and qualitative reduction of 'wormholes'.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Ensembles provided a 10–20% increase in success rate for goals at least 10 steps away (as reported in ablation). Exact numerical fidelity of uncertainty estimates not given.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low-to-moderate: ensemble spread can be used to indicate uncertainty in predicted distances, but internal representation is opaque. Aggregated mean/max distance is directly interpretable as a conservative distance estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Aggregation statistics (mean/max) across ensemble; qualitative analysis of reduced false short-distance predictions and improved planning stability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Linear increase in training and inference cost with ensemble size (paper used ensembles of 3 value functions for image tasks). Increased memory and inference time relative to single critic; additional cost justified by improved planning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Adds compute compared to single critic but substantially reduces catastrophic planning failures; authors found ensembles crucial for image-based tasks. No wall-clock train-time numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Adding ensembles increased success on distant goals by ~10–20% and was critical for making graph-search-based planning work reliably in image domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High utility: ensembles reduce over-optimistic (incorrectly short) distance predictions that cause planning to exploit 'wormholes', directly improving downstream planning success. The increased compute cost is offset by practical gains in reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Compute vs robustness: ensembles increase training/inference cost and memory but improve fidelity and planning reliability; attempts to share lower-level layers across ensemble members failed (led to correlated errors and 'wormholes').</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Train independent critics (no shared conv layers) with independent random initializations; aggregate expected distances by average or maximum (both worked similarly). Ensemble size used: 3 for image experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More robust than a single critic and preferable to shared-weights 'cheap' ensembles (which produced correlated errors); conceptually similar to bootstrapped DQN / deep ensembles for uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends independent critics (no shared convs) and modest ensemble sizes (3 used successfully); aggregate conservatively (max or average) and pair with distributional RL for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1410.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1410.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPTM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semi-Parametric Topological Memory (SPTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that constructs a graph over observations and uses a learned retrieval/inverse model to choose waypoints; used as a baseline for comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semi-parametric topological memory for navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Semi-Parametric Topological Memory (SPTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SPTM builds a semi-parametric graph over observations and uses supervised-learned inverse models/retrieval networks to predict reachability and to navigate between nearby nodes. Edges are added based on learned similarity / reachability scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>semi-parametric graph-based model with learned inverse models</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual navigation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>SPTM predicts reachability (binary or probability) between states; fidelity evaluated in the paper by precision-recall AUC of reachability predictions and downstream navigation success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the experiments, SoRB's distance predictor outperformed SPTM's reachability predictor: SoRB's predicted distances achieved on average 22% higher AUC (precision-recall) for predicting whether the policy reaches the goal. SPTM achieved second-best overall performance among baselines but substantially worse than SoRB on distant goals.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: like replay-buffer graphs, nodes are real observations so plans can be visualized; reachability scores are interpretable as a probability of being able to reach between two states.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Precision-recall evaluation of reachability scores; visualization of chosen waypoints and rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SPTM requires training retrieval and locomotor networks and evaluating reachability between candidate pairs for graph construction; computational specifics depend on chosen thresholds and sampling; authors tuned many hyperparameters for SPTM over a large grid.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>SPTM employs supervised inverse models (trained on short-range transitions) rather than distributional distance estimates; in this paper SPTM required extensive hyperparameter search and still underperformed SoRB on long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>SPTM performed substantially worse than SoRB on long-horizon visual navigation in the paper, though it was the best of the prior baselines tested (VIN, HER, C51) in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SPTM's reachability predictor is useful for assembling graphs, but learning reachability w.r.t. a random policy (as SPTM does) leads to poorer alignment with the actual goal-conditioned policy used for navigation in these experiments, reducing planning utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>SPTM trades supervised inverse-model simplicity for lower alignment between predicted reachability and the goal-conditioned policy's true abilities; requires careful threshold tuning (many hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>SPTM uses learned retrieval and locomotor networks, thresholds for adding edges and for selecting waypoints, and hyperparameters for sampling training data (authors of this paper performed a large grid search over these).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to SoRB: SPTM uses supervised inverse models and reachability scores, while SoRB uses distributional RL and Q-distance estimates; SoRB's distance-based approach aligned better with the goal-conditioned policy and yielded higher planning success.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe an optimal SPTM configuration; authors performed extensive hyperparameter tuning (over 1000 experiments) to find reasonable SPTM settings for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A distributional perspective on reinforcement learning <em>(Rating: 2)</em></li>
                <li>Semi-parametric topological memory for navigation <em>(Rating: 2)</em></li>
                <li>Simple and scalable predictive uncertainty estimation using deep ensembles <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 1)</em></li>
                <li>Imagination-augmented agents for deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1410",
    "paper_id": "paper-e0889fcee1acd985af76a3907d5d0029bf260be9",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Replay buffer (non-parametric world model)",
            "name_full": "Replay buffer used as a non-parametric generative model / state graph",
            "brief_description": "The replay buffer of previously observed states is treated as a non-parametric world model: stored observations serve as sampled valid states (graph nodes) for planning, avoiding the need to learn or roll out an explicit forward dynamics model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Replay buffer (non-parametric)",
            "model_description": "A non-parametric, data-driven model consisting of stored observed states (images or low-dimensional states). Planning is performed by constructing a graph whose nodes are buffer entries and whose edge weights are estimated distances (from a learned value function). The buffer acts as a sampler / implicit state space.",
            "model_type": "non-parametric world model",
            "task_domain": "navigation and goal-reaching (2D navigation and visual navigation in SUNCG houses)",
            "fidelity_metric": "Indirectly measured by (1) alignment of predicted distances (from value function) with empirical reachability (precision-recall AUC) and (2) downstream task success rate when planning over the buffer (fraction of goals reached).",
            "fidelity_performance": "Not a learned predictor itself; fidelity depends on the paired distance predictor. Using SoRB with the replay buffer produced ~90% success on distant visual navigation goals in the paper's experiments; buffer-based planning AUC (distance vs reachability) exceeded SPTM by ~22% (average across seeds) for the distance predictor used.",
            "interpretability_assessment": "High: nodes are actual observed states (images / coordinates), so plans (waypoints) are directly visualizable and semantically interpretable as real observations.",
            "interpretability_method": "Visualization of planned waypoints (image thumbnails), inspection of graph shortest paths; qualitative rollout visualization in figures.",
            "computational_cost": "Costs dominated by evaluating value function on buffer pairs: naïve Dijkstra would be O(|B|^2) value evaluations per query. Implementation amortizes with periodic full-pairwise evaluation and Floyd–Warshall (O(|B|^3) amortized) and then O(|B|) per-shortest-path query. Typical buffer sizes: training buffer 100k, search buffer 1k (used in experiments).",
            "efficiency_comparison": "More efficient and reliable for planning in high-dimensional observation spaces than learning a parametric generative model for images (avoids training a generative model); compared to model-free RL alone, planning over the buffer yields substantially higher success rates on long-horizon tasks (e.g., ~90% vs large degradation for baselines).",
            "task_performance": "Using the replay buffer for planning (SoRB) achieved ~90% success on distant goals in the visual navigation benchmark and ~80% success on 10-step goals in held-out houses (generalization), whereas goal-conditioned RL alone achieved &lt;20% on those held-out 10-step goals.",
            "task_utility_analysis": "The replay buffer provides concrete, reachable waypoints for long-horizon planning; its utility depends on (1) coverage of relevant states in the buffer and (2) accuracy of the distance predictor used to weight edges. A modest buffer (100–1000 states) sufficed in the experiments.",
            "tradeoffs_observed": "Tradeoffs include (1) memory / compute for storing and evaluating pairwise distances over the buffer vs the simplicity and interpretability of real-state waypoints, and (2) sensitivity to buffer coverage—too small or poorly sampled buffers can fail to provide feasible paths.",
            "design_choices": "Use of a separate, relatively small search buffer (~1k) for graph construction; periodic full evaluation of value function over buffer pairs and caching shortest-path distances via Floyd–Warshall; MAXDIST threshold to prune edges longer than a hyperparameter.",
            "comparison_to_alternatives": "Compared to learned forward generative models, the replay-buffer approach avoids generating infeasible states and simplifies planning by operating over actual observed states. Versus model-free RL alone, it enables long-horizon planning by chaining short-horizon controllers.",
            "optimal_configuration": "Paper recommendations: keep a modest-size search buffer (100–1k observations sufficed), tune MAXDIST (critical hyperparameter), amortize pairwise value evaluations (precompute with Floyd–Warshall), and ensure the distance predictor is calibrated (distributional RL + ensembles).",
            "uuid": "e1410.0",
            "source_info": {
                "paper_title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Q-distance model",
            "name_full": "Goal-conditioned Q-function interpreted as a shortest-path distance estimator",
            "brief_description": "The goal-conditioned Q-function (trained with reward -1 per step) is used as a learned world-model abstraction: Q(s,a,g) approximates negative shortest-path distance to goal g, providing edge weights for graph-based planning over the replay buffer.",
            "citation_title": "Learning to achieve goals",
            "mention_or_use": "use",
            "model_name": "Goal-conditioned Q-function as distance model",
            "model_description": "A value-based model where the scalar Q (or V) is trained with a -1 per-step reward and termination upon reaching the goal; the value approximates -d_sp(s,g) (negative shortest-path distance). This scalar model substitutes for an explicit forward model by predicting reachability distance between states.",
            "model_type": "value-based distance model / implicit inverse world model",
            "task_domain": "goal-reaching navigation tasks (2D and image-based visual navigation)",
            "fidelity_metric": "Alignment between predicted distance ordering and empirical reachability; measured via precision-recall AUC for predicting whether the policy reaches the goal and by downstream planning success rates.",
            "fidelity_performance": "The paper reports that SoRB's learned distances have 22% higher average AUC (precision-recall) than SPTM's reachability predictor across five seeds; exact numeric AUC values not provided. Downstream, planning with these distances yields ~90% success on difficult navigation goals.",
            "interpretability_assessment": "Moderate: outputs are scalar distances (interpretable as expected steps-to-goal), but the internal neural-network representation is a black box; predicted distances are directly interpretable as edge lengths in the constructed graph.",
            "interpretability_method": "Evaluation via precision-recall curves linking predicted distances to empirical reachability; visualization of planned waypoint sequences and comparison of rollouts.",
            "computational_cost": "Inference cost: one forward pass of Q-network per (state,goal) pair evaluated. For constructing the graph over |B| nodes, amortized pairwise evaluation is O(|B|^2) forward passes (cached) with occasional Floyd–Warshall O(|B|^3) to get all-pairs shortest paths.",
            "efficiency_comparison": "More computationally efficient than full parametric forward models that must predict high-dimensional observations; avoids rollout cost of forward models but requires many Q evaluations for dense graphs. Compared to model-free policies without planning, it adds planner overhead but yields much higher long-horizon success.",
            "task_performance": "When used in SoRB, the Q-distance model enabled successful planning to distant goals with ~90% success in the visual navigation task and strong generalization to held-out houses (~80% for 10-step goals).",
            "task_utility_analysis": "High: accurate calibration of Q as distance (especially with distributional RL and ensembles) directly improves planning quality; inaccuracies (underestimation) cause 'wormholes' that break planning.",
            "tradeoffs_observed": "Scalar distance prediction simplifies planning but cannot capture multimodal transition dynamics; inaccurate predictions produce catastrophic planning errors ('wormholes'); improving fidelity (distributional RL, ensembles) increases compute and training complexity.",
            "design_choices": "Use -1 per-step reward to force Q→negative shortest-path distance; use off-policy goal relabelling; represent distances distributionally (discrete bins) and aggregate via ensembles to quantify uncertainty.",
            "comparison_to_alternatives": "Compared with inverse-model supervised policies (e.g., SPTM's locomotor), Q-distance provides better-aligned distance estimates for planning; compared to forward dynamics models, Q-distance predicts task-relevant scalar (distance) rather than high-dimensional observations, making learning easier and more robust for planning.",
            "optimal_configuration": "Paper recommends (and uses) distributional RL to discretize distances, ensembles to avoid false short-distance predictions, a MAXDIST cutoff to prune unrealistic edges, and conservative aggregation over ensembles when planning.",
            "uuid": "e1410.1",
            "source_info": {
                "paper_title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Distributional RL (C51)",
            "name_full": "Distributional reinforcement learning (C51)",
            "brief_description": "Distributional RL (C51) is used to represent the value function as a discrete distribution over distance bins, which naturally encodes finite vs. infinite/unreachable distances and yields stable targets when learning distances.",
            "citation_title": "A distributional perspective on reinforcement learning",
            "mention_or_use": "use",
            "model_name": "Distributional Q-function (C51) as a distance model",
            "model_description": "The value is represented as a categorical distribution over N discrete bins, where each bin corresponds to an integer distance (steps-to-goal). The final bin is a catch-all for &gt;= N steps, enabling representation of very large / effectively infinite distances and preventing ill-defined scalar Q predictions.",
            "model_type": "distributional value model (discrete categorical distribution over distances)",
            "task_domain": "goal-reaching and planning (2D navigation and visual navigation)",
            "fidelity_metric": "Quality of distance predictions assessed by how well the predicted distribution/order aligns with empirical reachability (precision-recall AUC) and downstream planning success rates.",
            "fidelity_performance": "Ablation shows distributional RL is critical: the variant without distributional RL 'performed poorly' (worse than random policy in at least one experiment). Exact numeric fidelity for the distributional predictor not provided.",
            "interpretability_assessment": "Moderate: output distribution bins map directly to integer distances, giving an interpretable probabilistic estimate over discrete distances; internal features remain black-box.",
            "interpretability_method": "Bins correspond to integer steps, so inspecting the modal bin or expected distance provides interpretable distance estimates; evaluation via precision-recall and task success.",
            "computational_cost": "Slightly higher than scalar Q due to predicting a categorical distribution over N atoms; cost scales with number of atoms N. No specific parameter counts or runtimes provided.",
            "efficiency_comparison": "Empirically more stable and effective for distance learning than standard scalar Q-learning for the -1 reward formulation; avoids clipping issues and represents unreachable states naturally.",
            "task_performance": "Using C51 as the underlying goal-conditioned policy/distance estimator enabled SoRB to plan successfully in image-based navigation tasks; omission of distributional RL degraded performance substantially.",
            "task_utility_analysis": "Distributional representation aligns with the integer-valued nature of distances (steps), aiding learning and preventing ill-posed infinite values; this translates to better planning and task success.",
            "tradeoffs_observed": "Distributional RL increases output dimensionality (N bins) and modestly increases compute, but yields major gains in robustness and fidelity when predicting distances, making it favorable for this task.",
            "design_choices": "Discretize distances into bins B_1..B_N with a catch-all final bin for &gt;=N steps; train by minimizing KL divergence between predicted and shifted target distributions (right-shift target scheme appropriate for -1 reward).",
            "comparison_to_alternatives": "Outperformed scalar Q-learning in this context (scalar Q struggled with unreachable states); compared to learned forward models, distributional RL focuses on task-relevant scalar distances rather than full observation prediction.",
            "optimal_configuration": "Paper suggests choosing N large enough to cover relevant planning horizons and using the final bin as overflow; pairing distributional RL with ensembles improved stability and planning quality.",
            "uuid": "e1410.2",
            "source_info": {
                "paper_title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Ensembles of value functions",
            "name_full": "Ensembles (bootstrap-style) of distributional Q-networks for uncertainty-aware distances",
            "brief_description": "An ensemble of independently initialized and trained (distributional) Q-networks is used to estimate uncertainty in distance predictions and avoid spurious short-distance estimates ('wormholes') that break planning.",
            "citation_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "mention_or_use": "use",
            "model_name": "Ensemble of distributional Q-networks",
            "model_description": "Multiple independent distributional critics are trained on the same data (bootstrap-style approximation). During planning, their distance predictions are aggregated (average or max was used) to obtain a more robust estimate and reduce false optimistic distance predictions.",
            "model_type": "ensemble uncertainty estimator for value-based world model",
            "task_domain": "visual navigation planning and goal-reaching",
            "fidelity_metric": "Measured by downstream planning robustness: reduction in spurious short-distance predictions and increased success rate for distant goals. Also indirectly assessed via increase in task success and qualitative reduction of 'wormholes'.",
            "fidelity_performance": "Ensembles provided a 10–20% increase in success rate for goals at least 10 steps away (as reported in ablation). Exact numerical fidelity of uncertainty estimates not given.",
            "interpretability_assessment": "Low-to-moderate: ensemble spread can be used to indicate uncertainty in predicted distances, but internal representation is opaque. Aggregated mean/max distance is directly interpretable as a conservative distance estimate.",
            "interpretability_method": "Aggregation statistics (mean/max) across ensemble; qualitative analysis of reduced false short-distance predictions and improved planning stability.",
            "computational_cost": "Linear increase in training and inference cost with ensemble size (paper used ensembles of 3 value functions for image tasks). Increased memory and inference time relative to single critic; additional cost justified by improved planning robustness.",
            "efficiency_comparison": "Adds compute compared to single critic but substantially reduces catastrophic planning failures; authors found ensembles crucial for image-based tasks. No wall-clock train-time numbers provided.",
            "task_performance": "Adding ensembles increased success on distant goals by ~10–20% and was critical for making graph-search-based planning work reliably in image domains.",
            "task_utility_analysis": "High utility: ensembles reduce over-optimistic (incorrectly short) distance predictions that cause planning to exploit 'wormholes', directly improving downstream planning success. The increased compute cost is offset by practical gains in reliability.",
            "tradeoffs_observed": "Compute vs robustness: ensembles increase training/inference cost and memory but improve fidelity and planning reliability; attempts to share lower-level layers across ensemble members failed (led to correlated errors and 'wormholes').",
            "design_choices": "Train independent critics (no shared conv layers) with independent random initializations; aggregate expected distances by average or maximum (both worked similarly). Ensemble size used: 3 for image experiments.",
            "comparison_to_alternatives": "More robust than a single critic and preferable to shared-weights 'cheap' ensembles (which produced correlated errors); conceptually similar to bootstrapped DQN / deep ensembles for uncertainty.",
            "optimal_configuration": "Paper recommends independent critics (no shared convs) and modest ensemble sizes (3 used successfully); aggregate conservatively (max or average) and pair with distributional RL for best performance.",
            "uuid": "e1410.3",
            "source_info": {
                "paper_title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "SPTM (baseline)",
            "name_full": "Semi-Parametric Topological Memory (SPTM)",
            "brief_description": "A prior method that constructs a graph over observations and uses a learned retrieval/inverse model to choose waypoints; used as a baseline for comparison in the paper.",
            "citation_title": "Semi-parametric topological memory for navigation",
            "mention_or_use": "mention",
            "model_name": "Semi-Parametric Topological Memory (SPTM)",
            "model_description": "SPTM builds a semi-parametric graph over observations and uses supervised-learned inverse models/retrieval networks to predict reachability and to navigate between nearby nodes. Edges are added based on learned similarity / reachability scores.",
            "model_type": "semi-parametric graph-based model with learned inverse models",
            "task_domain": "visual navigation",
            "fidelity_metric": "SPTM predicts reachability (binary or probability) between states; fidelity evaluated in the paper by precision-recall AUC of reachability predictions and downstream navigation success rate.",
            "fidelity_performance": "In the experiments, SoRB's distance predictor outperformed SPTM's reachability predictor: SoRB's predicted distances achieved on average 22% higher AUC (precision-recall) for predicting whether the policy reaches the goal. SPTM achieved second-best overall performance among baselines but substantially worse than SoRB on distant goals.",
            "interpretability_assessment": "Moderate: like replay-buffer graphs, nodes are real observations so plans can be visualized; reachability scores are interpretable as a probability of being able to reach between two states.",
            "interpretability_method": "Precision-recall evaluation of reachability scores; visualization of chosen waypoints and rollouts.",
            "computational_cost": "SPTM requires training retrieval and locomotor networks and evaluating reachability between candidate pairs for graph construction; computational specifics depend on chosen thresholds and sampling; authors tuned many hyperparameters for SPTM over a large grid.",
            "efficiency_comparison": "SPTM employs supervised inverse models (trained on short-range transitions) rather than distributional distance estimates; in this paper SPTM required extensive hyperparameter search and still underperformed SoRB on long-horizon tasks.",
            "task_performance": "SPTM performed substantially worse than SoRB on long-horizon visual navigation in the paper, though it was the best of the prior baselines tested (VIN, HER, C51) in some settings.",
            "task_utility_analysis": "SPTM's reachability predictor is useful for assembling graphs, but learning reachability w.r.t. a random policy (as SPTM does) leads to poorer alignment with the actual goal-conditioned policy used for navigation in these experiments, reducing planning utility.",
            "tradeoffs_observed": "SPTM trades supervised inverse-model simplicity for lower alignment between predicted reachability and the goal-conditioned policy's true abilities; requires careful threshold tuning (many hyperparameters).",
            "design_choices": "SPTM uses learned retrieval and locomotor networks, thresholds for adding edges and for selecting waypoints, and hyperparameters for sampling training data (authors of this paper performed a large grid search over these).",
            "comparison_to_alternatives": "Compared to SoRB: SPTM uses supervised inverse models and reachability scores, while SoRB uses distributional RL and Q-distance estimates; SoRB's distance-based approach aligned better with the goal-conditioned policy and yielded higher planning success.",
            "optimal_configuration": "Paper does not prescribe an optimal SPTM configuration; authors performed extensive hyperparameter tuning (over 1000 experiments) to find reasonable SPTM settings for baselines.",
            "uuid": "e1410.4",
            "source_info": {
                "paper_title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A distributional perspective on reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Semi-parametric topological memory for navigation",
            "rating": 2
        },
        {
            "paper_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 1
        },
        {
            "paper_title": "Imagination-augmented agents for deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01667575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Search on the Replay Buffer: Bridging Planning and Reinforcement Learning</h1>
<p>Benjamin Eysenbach ${ }^{\theta \phi}$, Ruslan Salakhutdinov ${ }^{\theta}$, Sergey Levine ${ }^{\phi \psi}$<br>${ }^{\theta}$ CMU, ${ }^{\phi}$ Google Brain, ${ }^{\psi}$ UC Berkeley<br>beysenba@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards finding a sub-optimal solution. We introduce a general-purpose control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a particular subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment - namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>How can agents learn to solve complex, temporally extended tasks? Classically, planning algorithms give us one tool for learning such tasks. While planning algorithms work well for tasks where it is easy to determine distances between states and easy to design a local policy to reach nearby states, both of these requirements become roadblocks when applying planning to high-dimensional (e.g., image-based) tasks. Learning algorithms excel at handling high-dimensional observations, but reinforcement learning (RL) - learning for control - fails to reason over long horizons to solve temporally extended tasks. In this paper, we propose a method that combines the strengths of planning and RL, resulting in an algorithm that can plan over long horizons in tasks with high-dimensional observations.</p>
<p>Recent work has introduced goal-conditioned RL algorithms (Pong et al., 2018; Schaul et al., 2015) that acquire a single policy for reaching many goals. In practice, goal-conditioned RL succeeds at</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Search on the Replay Buffer: (a) Goal-conditioned RL often fails to reach distant goals, but can successfully reach the goal if starting nearby (inside the green region). (b) Our goal is to use observations in our replay buffer (yellow squares) as waypoints leading to the goal. (c) We automatically find these waypoints by using the agent's value function to predict when two states are nearby, and building the corresponding graph. (d) We run graph search to find the sequence of waypoints (blue arrows), and then use our goal-conditioned policy to reach each waypoint.</p>
<p>Reaching nearby goals but fails to reach distant goals; performance degrades quickly as the number of steps to the goal increases (Levy et al., 2019; Nachum et al., 2018). Moreover, goal-conditioned RL often requires large amounts of reward shaping (Chiang et al., 2019) or human demonstrations (Lynch et al., 2019; Nair et al., 2018), both of which can limit the asymptotic performance of the policy by discouraging the policy from seeking novel solutions.</p>
<p>We propose to solve long-horizon, sparse reward tasks by decomposing the task into a series of easier goal-reaching tasks. We learn a goal-conditioned policy for solving each of the goal-reaching tasks. Our main idea is to reduce the problem of finding these subgoals to solving a shortest path problem over states that we have previous visited, using a distance metric extracted from our goal-conditioned policy. We call this algorithm Search on Replay Buffer (SoRB), and provide a simple illustration of the algorithm in Figure 1.</p>
<p>Our primary contribution is an algorithm that bridges planning and deep RL for solving long-horizon, sparse reward tasks. We develop a practical instantiation of this algorithm using ensembles of distributional value functions, which allows us to <em>robustly</em> learn distances and use them for <em>risk-aware</em> planning. Empirically, we find that our method generates effective plans to solve long horizon navigation tasks, even in image-based domains, without a map and without odometry. Comparisons with state-of-the-art RL methods show that SoRB is substantially more successful in reaching distant goals. We also observe that the learned policy generalizes well to navigate in unseen environments. In summary, graph search over previously visited states is a simple tool for boosting the performance of a goal-conditioned RL algorithm.</p>
<h2>2 Bridging Planning and Reinforcement Learning</h2>
<p>Planning algorithms must be able to (1) sample valid states, (2) estimate the distance between reachable pairs of states, and (3) use a local policy to navigate between nearby states. These requirements are difficult to satisfy in complex tasks with high dimensional observations, such as images. For example, consider a robot arm stacking blocks using image observations. Sampling states requires generating photo-realistic images, and estimating distances and choosing actions requires reasoning about dozens of interactions between blocks. Our method will obtain distance estimates and a local policy using a RL algorithm. To sample states, we will simply use a replay buffer of previously visited states as a non-parametric generative model.</p>
<h3>2.1 Building Block: Goal-Conditioned RL</h3>
<p>A key building block of our method is a goal-conditioned policy and its associated value function. We consider a goal-reaching agent interacting with an environment. The agent observes its current state $s \in \mathcal{S}$ and a goal state $s_{g} \in \mathcal{S}$. The initial state for each episode is sampled $s_{1} \sim \rho(s)$, and dynamics are governed by the distribution $p(s_{t+1} \mid s_{t}, a_{t})$. At every step, the agent samples an action $a \sim \pi(a \mid s, s_{g})$ and receives a corresponding reward $r(s, a, s_{g})$ that indicates whether the agent has reached the goal. The episode terminates as soon as the agent reaches the goal, or after $T$ steps, whichever occurs first. The agent's task is to maximize its cumulative, <em>undiscounted</em>, reward. We use an off-policy algorithm to learn such a policy, as well as its associated goal-conditioned Q-function.</p>
<p>and value function:</p>
<p>We obtain a policy by acting greedily w.r.t. the Q-function: $\pi(a \mid s, s_{g})=\arg \max <em g="g">{a} Q\left(s, a, s</em>$.}\right)$. We choose an off-policy RL algorithm with goal relabelling (Andrychowicz et al., 2017; Kaelbling, 1993b) and distributional RL (Bellemare et al., 2017)) not only for improved data efficiency, but also to obtain good distance estimates (See Section 2.2). We will use DQN (Mnih et al., 2013) for discrete action environments and DDPG (Lillicrap et al., 2015) for continuous action environments. Both algorithms operate by minimizing the Bellman error over transitions sampled from a replay buffer $\mathcal{B</p>
<h1>2.2 Distances from Goal-Conditioned Reinforcement Learning</h1>
<p>To ultimately perform planning, we need to compute the shortest path distance between pairs of states. Following Kaelbling (1993b), we define a reward function that returns -1 at every step: $r\left(s, a, s_{g}\right) \triangleq-1$. The episode ends when the agent is sufficiently close to the goal, as determined by a state-identity oracle. Using this reward function and termination condition, there is a close connection between the Q values and shortest paths. We define $d_{\text {sp }}\left(s, s_{g}\right)$ to be the shortest path distance from state $s$ to state $s_{g}$. That is, $d_{\text {sp }}(s, g)$ is the expected number of steps to reach $s_{g}$ from $s$ under the optimal policy. The value of state $s$ with respect to goal $s_{g}$ is simply the negative shortest path distance: $V\left(s, s_{g}\right)=-d_{\text {sp }}\left(s, s_{g}\right)$. We likewise define $d_{\text {sp }}\left(s, a, s_{g}\right)$ as the shortest path distance, conditioned on initially taking action $a$. Then Q values also equal a negative shortest path distance: $Q\left(s, a, s_{g}\right)=-d_{\text {sp }}\left(s, a, s_{g}\right)$. Thus, goal-conditioned RL on a suitable reward function yields a Q-function that allows us to estimate shortest-path distances.</p>
<h3>2.3 The Replay Buffer as a Graph</h3>
<p>We build a weighted, directed graph directly on top of states in our replay buffer, so each node corresponds to an observation (e.g., an image). We add edges between nodes with weight (i.e., length) equal to their predicted distance, but ignore edges that are longer than MAXDIST, a hyperparameter:</p>
<p>$$
\begin{aligned}
\mathcal{G} \triangleq(\mathcal{V}, \mathcal{E}, \mathcal{W}) \quad \text { where } \quad \mathcal{V}=\mathcal{B}, \quad \mathcal{E} &amp; =\mathcal{B} \times \mathcal{B}=\left{e_{s_{1} \rightarrow s_{2}} \mid s_{1}, s_{2} \in \mathcal{B}\right} \
\mathcal{W}\left(e_{s_{1} \rightarrow s_{2}}\right) &amp; = \begin{cases}d_{\pi}\left(s_{1}, d_{2}\right) &amp; \text { if } d_{\pi}\left(s_{1}, s_{2}\right)&lt;\text { MAXDIST } \
\infty &amp; \text { otherwise }\end{cases}
\end{aligned}
$$</p>
<p>Given a start and goal state, we temporarily add each to the graph. We add directed edges from the start state to every other state, and from every other state to the goal state, using the same criteria as above. We use Dijkstra's Algorithm to find the shortest path. See Appendix A for details.</p>
<h3>2.4 Algorithm Summary</h3>
<p>After learning a goal-conditioned Q-function, we perform graph search to find a set of waypoints and use the goal-conditioned policy to reach each. We view the combination of graph search and the underlying goal-conditioned policy as a new SEARCHPOLICY, shown in Algorithm 1. The algorithm starts by using graph search to obtain the shortest path $s_{w_{1}}, s_{w_{2}}, \cdots$ from the current state $s$ to the goal state $s_{g}$, planning over the states in our replay buffer $\mathcal{B}$. We then estimate the distance from the current state to the first waypoint, as well as the distance from the current state to the goal. In most cases, we then condition the policy on the first waypoint, $s_{w_{1}}$. However, if the goal state is closer than the next waypoint and the goal state is not too far away, then we directly condition the policy on the final goal. If the replay buffer is empty or there is not a path in $\mathcal{G}$ to the goal, then Algorithm 1 resorts to standard goal-conditioned RL.</p>
<p>3 Better Distance Estimates</p>
<p>The success of our SEARCHPOLICY depends heavily on the accuracy of our distance estimates. This section proposes two techniques to learn better distances with RL.</p>
<h3>3.1 Better Distances via Distributional Reinforcement Learning</h3>
<p>Off-the-shelf Q-learning algorithms such as DQN ( [Mnih et al., 2013]) or DDPG ( [Lillicrap et al., 2015]) will fail to learn accurate distance estimates using the $-1$ reward function. The true value for a state and goal that are unreachable is $-\infty$, which cannot be represented by a standard, feed-forward Q-network. Simply clipping the Q-value estimates to be within some range avoids the problem of ill-defined Q-values, but empirically we found it challenging to train clipped Q-networks. We adopt distributional Q-learning ( [Bellemare et al., 2017]), noting that is has a convenient form when used with the $-1$ reward function. Distributional RL discretizes the possible value estimates into a set of bins $B=(B_{1},B_{2},\cdots,B_{N})$. For learning distances, bins correspond to distances, so $B_{i}$ indicates the event that the current state and goal are $i$ steps away from one another. Our Q-function predicts a distribution $Q(s_{t},s_{g},a_{t})\in\mathcal{P}^{N}$ over these bins, where $Q(s_{t},s_{g},a_{t})<em t="t">{i}$ is the predicted probability that states $s</em>$ for our Q-values have a simple form:}$ and $s_{g}$ are $i$ steps away from one another. To avoid ill-defined Q-values, the final bin, $B_{N}$ is a catch-all for predicted distances of at least $N$. Importantly, this gives us a well-defined method to represent large and infinite distances. Under this formulation, the targets $Q^{*}\in\mathcal{P}^{N</p>
<p>[ Q^{*}=\begin{cases}(1,0,\cdots,0) &amp; \text{if }s_{t}=g\
(0,Q_{1},\cdots,Q_{N-2},Q_{N-1}+Q_{N}) &amp; \text{if }s_{t}\neq g\end{cases} ]</p>
<p>As illustrated in Figure 2, if the state and goal are equivalent, then the target places all probability mass in bin 0. Otherwise, the targets are a right-shift of the current predictions. To ensure the target values sum to one, the mass in bin $N$ of the targets is the sum of bins $N-1$ and $N$ from the predicted values. Following Bellemare et al. (2017), we update our Q function by minimizing the KL divergence between our predictions $Q^{\theta}$ and the target $Q^{*}$:</p>
<p>$\min_{\theta}D_{\text{KL}}\left(Q^{*}\left|\ Q^{\theta}\right)\right.$</p>
<h3>3.2 Robust Distances via Ensembles of Value Functions</h3>
<p>Since we ultimately want to use estimated distances to perform search, it is crucial that we have accurate distances estimates. It is challenging to robustly estimate the distance between all $|\mathcal{B}|^{2}$ pairs of states in our buffer $\mathcal{B}$, some of which may not have occurred during training. If we fail and spuriously predict that a pair of distant states are nearby, graph search will exploit this “wormhole” and yield a path which assumes that the agent can “teleport” from one distant state to another. We seek to use a bootstrap ( [Bickel et al., 1981]) as a principled way to estimate uncertainty for our Q-values. Following prior work ( [Lakshminarayanan et al., 2017; Osband et al., 2016]), we implement an approximation to the bootstrap. We train an ensemble of Q-networks, each with independent weights, but trained on the same data using the same loss (Eq. 1). When performing graph search, we aggregate predictions from each Q-network in our ensemble. Empirically, we found that ensembles were crucial for getting graph search to work on image-based tasks, but we observed little difference in whether we took the maximum predicted distance or the average predicted distance.</p>
<h2>4 Related Work</h2>
<p>Planning Algorithms: Planning algorithms ( [Choset et al., 2005; LaValle, 2006]) efficiently solve long-horizon tasks, including those that stymie RL algorithms (see, e.g., [Kavraki et al. (1996); Lau and</p>
<p>Kuffner (2005); Levine et al. (2011)). However, these techniques assume that we can (1) efficiently sample valid states, (2) estimate the distance between two states, and (3) acquire a local policy for reaching nearby states, all of which make it challenging to apply these techniques to high-dimensional tasks (e.g., with image observations). Our method removes these assumptions by (1) sampling states from the replay buffer and (2,3) learning the distance metric and policy with RL. Some prior works have also combined planning algorithms with RL (Chiang et al., 2019; Faust et al., 2018; Savinov et al., 2018a), finding that the combination yields agents adept at reaching distant goals. Perhaps the most similar work is Semi-Parametric Topological Memory (Savinov et al., 2018a), which also uses graph search to find waypoints for a learned policy. We compare to SPTM in Section 5.3.
Goal-Conditioned RL: Goal-conditioned policies (Kaelbling, 1993b; Pong et al., 2018; Schaul et al., 2015) take as input the current state and a goal state, and predict a sequence of actions to arrive at the goal. Our algorithm learns a goal-conditioned policy to reach waypoints along the planned path. Recent algorithms (Andrychowicz et al., 2017; Pong et al., 2018) combine off-policy RL algorithms with goal-relabelling to improve the sample complexity and robustness of goal-conditioned policies. Similar algorithms have been proposed for visual navigation (Anderson et al., 2018; Gupta et al., 2017; Mirowski et al., 2016; Zhu et al., 2017). A common theme in recent work is learning distance metrics to accelerate RL. While most methods (Florensa et al., 2019; Savinov et al., 2018b; Wu et al., 2018) simply perform RL on top of the learned representation, our method explicitly performs search using the learned metric.
Hierarchical RL: Hierarchical RL algorithms automatically learn a set of primitive skills to help an agent learn complex tasks. One class of methods (Bacon et al., 2017; Frans et al., 2017; Kaelbling, 1993a; Kulkarni et al., 2016; Nachum et al., 2018; Parr and Russell, 1998; Precup, 2000; Sutton et al., 1999; Vezhnevets et al., 2017) jointly learn a low-level policy for performing each of the skills together with a high-level policy for sequencing these skills to complete a desired task. Another class of algorithms (Drummond, 2002; Fox et al., 2017; Şimşek et al., 2005) focus solely on automatically discovering these skills or subgoals. SoRB learns primitive skills that correspond to goal-reaching tasks, similar to Nachum et al. (2018). While jointly learning high-level and low-level policies can be unstable (see discussion in Nachum et al. (2018)), we sidestep the problem by using graph search as a fixed, high-level policy.
Model Based RL: RL methods are typically divided into model-free (Schulman et al., 2015a,b, 2017; Williams, 1992) and model-based (Lillicrap et al., 2015; Watkins and Dayan, 1992) approaches. Model-based approaches all perform some degree of planning, from predicting the value of some state (Mnih et al., 2013; Silver et al., 2016), obtaining representations by unrolling a learned dynamics model (Racanière et al., 2017), or learning a policy directly on a learned dynamics model (Agrawal et al., 2016;
Chua et al., 2018; Finn and Levine, 2017; Kurutach et al., 2018; Nagabandi et al., 2018; Oh et al., 2015; Sutton, 1990). One line of work (Amos et al., 2018; Lee et al., 2018; Srinivas et al., 2018; Tamar et al., 2016) embeds a differentiable planner inside a policy, with the planner learned end-to-end with the rest of the policy. Other work (Lenz et al., 2015; Watter et al., 2015) explicitly learns a representation for use inside a standard planning algorithm. In contrast, SoRB learns to predict the distances between states, which can be viewed as a high-level inverse model. SoRB predicts a scalar (the distance) rather than actions or observations, making the prediction problem substantially easier. By planning over previously visited states, SoRB does not have to cope with infeasible states that can be predicted by forward models in state-space and latent-space.</p>
<h1>5 Experiments</h1>
<p>We compare SoRB to prior methods on two tasks: a simple 2D environment, and then a visual navigation task, where our method will plan over images. Ablation experiments will illustrate that accurate distances estimates are crucial to our algorithm's success.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Simple 2D Navigation: (Left) Two simple navigation environments. (Center) An agent that combines a goal-conditioned policy with search is substantially more successful at reaching distant goals in these environments than using the goal-conditioned policy alone. (Right) A standard goal-conditioned policy (top) fails to reach distant goals. Applying graph search on top of that same policy (bottom) yields a sequence of intermediate waypoints (yellow squares) that enable the agent to successfully reach distant goals.</p>
<h3>5.1 Didactic Example: 2D Navigation</h3>
<p>We start by building intuition for our method by applying it to two simple 2D navigation tasks, shown in Figure 4a. The start and goal state are chosen randomly in free space, and reaching the goal often takes over 100 steps, even for the optimal policy. We used goal-conditioned RL to learn a policy for each environment, and then evaluated this policy on randomly sampled (start, goal) pairs of varying difficulty. To implement SoRB, we used exactly the same policy, both to perform graph search and then to reach each of the planned waypoints. In Figure 4b, we observe that the goal-conditioned policy can reach nearby goals, but fails to generalize to distant goals. In contrast, SoRB successfully reaches goals over 100 steps away, with little drop in success rate. Figure 4c compares rollouts from the goal-conditioned policy and our policy. Note that our policy takes actions that temporarily lead away from the goal so the agent can maneuver through a hallway to eventually reach the goal.</p>
<h3>5.2 Planning over Images for Visual Navigation</h3>
<p>We now examine how our method scales to high-dimensional observations in a visual navigation task, illustrated in Figure 5. We use 3D houses from the SUNCG dataset (Song et al., 2017), similar to the task described by Shah et al. (2018). The agent receives either RGB or depth images and takes actions to move North/South/East/West. Following Shah et al. (2018), we stitch four images into a panorama, so the resulting observation has dimension 4× 24×32×C, where C is the number of channels (3 for RGB, 1 for Depth). At the start of each episode, we randomly sample an initial state and goal state. We found that sampling nearby goals (within 4 steps) more often (80% of the time) improved the performance of goal-conditioned RL. We use the same goal sampling distribution for all methods. The agent observes both the current image and the goal image, and should take actions that lead to the goal state. The episode terminates once the agent is within 1 meter of the goal. We also terminate if the agent has failed to reach the goal after 20 time steps, but treat the two types of termination differently when computing the TD error (see Pardo et al. (2017)). Note that it is challenging to specify a meaningful distance metric and local policy on pixel inputs, so it is difficult to apply standard planning algorithms to this task.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Visual Navigation: Given an initial state and goal state, our method automatically finds a sequence of intermediate waypoints. The agent then follows those waypoints to reach the goal.</p>
<p>On this task, we evaluate four state-of-the-art prior methods: hindsight experience replay (HER) (Andrychowicz et al., 2017), distributional RL (C51) (Bellemare et al., 2017), semi-parametric topological memory (SPTM) (Savinov et al., 2018a), and value iteration networks (VIN) (Tamar et al., 2016). SoRB uses C51 as its underlying goal-conditioned policy. For VIN, we tuned the number of iterations as well as the number of hidden units in the recurrent layer. For SPTM, we performed a grid search over the threshold for adding edges, the threshold for choosing the next waypoint along</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Visual Navigation: We compare our method (SoRB) to prior work on the visual navigation environment (Fig. 5), using RGB images (Left) and depth images (Right). We find that only our method succeeds in reaching distant goals. Baselines: SPTM (Savinov et al., 2018a), C51 (Bellemare et al., 2017), VIN (Tamar et al., 2016), HER (Andrychowicz et al., 2017).
the shortest path, and the parameters for sampling the training data. In total, we performed over 1000 experiments to tune baselines, more than an order of magnitude more than we used for tuning our own method. See Appendix F for details.</p>
<p>We evaluated each method on goals ranging from 2 to 20 steps from the start. For each distance, we randomly sampled 30 (start, goal) pairs, and recorded the average success rate, defined as reaching within 1 meter of the goal within 100 steps. We then repeated each experiment for 5 random seeds. In Figure 6, we plot each random seed as a transparent line; the solid line corresponds to the average across the 5 random seeds. While all prior methods degrade quickly as the distance to the goal increases, our method continues to succeed in reaching goals with probability around $90 \%$. SPTM, the only prior method that also employs search, performs second best, but substantially worse than our method.</p>
<h1>5.3 Comparison with Semi-Parametric Topological Memory</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Goal-Conditioned Policy
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Distance Predictions</p>
<p>Figure 7: SoRB vs SPTM: Our method and Semi-Parametric Topological Memory (Savinov et al., 2018b) differ in the policy used and how distances are estimated. We find (Left) that both methods learn comparable policies, but (Right) our method learns more accurate distances. See text for details.</p>
<p>To understand why SoRB succeeds at reaching distant goals more frequently than SPTM, we examine the two key differences between the methods: (1) the goal-conditioned policy used to reach nearby goals and (2) the distance metric used to construct the graph. While SoRB acquires a goal-conditioned policy via goal-conditioned RL, SPTM obtains a policy by learning an inverse model with supervised learning. First, we compared the performance of the RL policy (used in SoRB) with the inverse model policy (used in SPTM). In Figure 7a, the solid colored lines show that, without search, the policy used by SPTM is more successful than the RL policy, but performance of both policies degrades as the distance to the goal increases. We also evaluate a variant of our method that uses the policy from SPTM to reach each waypoint, and find (dashed-lines) no difference in performance, likely because the policies are equally good at reaching nearby goals (within MAXDist steps). We conclude that the difference in goal-conditioned policies cannot explain the difference in success rate.</p>
<p>The other key difference between SoRB and SPTM is their learned distance metrics. When using distances for graph search, it is critical for the predicted distance between two states to reflect whether the policy can successfully navigate between those states: the model should be more successful at reaching goals which it predicts are nearby. We can naturally measure this alignment using the area under a precision recall curve. Note that while SoRB predicts distances in the range $[0,7]$, SPTM predicts whether two states are reachable, so its predictions will be in the range $[0,1]$. Nonetheless, precision-recall curves ${ }^{2}$ only depend on the ordering of the predictions, not their absolute values. Figure 7(b) shows that the distances predicted by SoRB more accurately reflect whether the policy will reach the goal, as compared with SPTM. The average AUC across five random seeds is $22 \%$ higher for SoRB than SPTM. In retrospect, this finding is not surprising: while SPTM employs a learned, inverse model policy, it learns distances w.r.t. a random policy.</p>
<h1>5.4 Better Distance Estimates</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Better Distance Estimates: (Left) Without distributional RL, our method performs poorly. (Right) Ensembles contribute to a moderate increase in success rate, especially for distant goals.
We now examine the ingredients in SoRB that contribute to its accurate distance estimates: distributional RL and ensembles of value functions. In a first experiment, evaluated a variant of SoRB trained without distributional RL. As shown in Figure 8a, this variant performed worse than the random policy, clearly illustrating that distributional RL is a key component of SoRB. The second experiment studied the effect of using ensembles of value functions. Recalling that we introduced ensembles to avoid erroneous distance predictions for distant pairs of states, we expect that ensembles will contribute most towards success at reaching distant goals. Figure 8b confirms this prediction, illustrating that ensembles provide a 10 - $20 \%$ increase in success at reaching goals that are at least 10 steps away. We run additional ablation analysis in Appendix C.</p>
<h3>5.5 Generalizing to New Houses</h3>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Does SoRB Generalize? After training on 100 SUNCG houses, we collect random data in held-out houses to use for search in those new environments. Whether using depth images or RGB images, SoRB generalizes well to new houses, reaching almost $80 \%$ of goals 10 steps away, while goal-conditioned RL reaches less than $20 \%$ of these goals. Transparent lines correspond to average success rate across 22 held-out houses for each of three random seeds.</p>
<p>We now study whether our method generalizes to new visual navigation environments. We train on 100 SUNCG houses, randomly sampling one per episode. We evaluated on a held-out test set of 22 SUNCG houses. In each house, we collect 1000 random observations and use those observations</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to perform search. We use the same goal-conditioned policy and associated distance function that we learned during training. As before, we measure the fraction of goals reached as we increase the distance to the goal. In Figure 9, we observe that SoRB reaches almost $80 \%$ of goals that are 10 steps away, about four times more than reached by the goal-conditioned RL agent. Our method succeeds in reaching $40 \%$ of goals 20 steps away, while goal-conditioned RL has a success rate near $0 \%$. We repeated the experiment for three random seeds, retraining the policy from scratch each time. Note that there is no discernible difference between the three random seeds, plotted as transparent lines, indicating the robustness of our method to random initialization.</p>
<h1>6 Discussion and Future Work</h1>
<p>We presented SoRB, a method that combines planning via graph search and goal-conditioned RL. By exploiting the structure of goal-reaching tasks, we can obtain policies that generalize substantially better than those learned directly from RL. In our experiments, we show that SoRB can solve temporally extended navigation problems, traverse environments with image observations, and generalize to new houses in the SUNCG dataset. Our method relies heavily on goal-conditioned RL, and we expect advances in this area to make our method applicable to even more difficult tasks. While we used a stage-wise procedure, first learning the goal-conditioned policy and then applying graph search, in future work we aim to explore how graph search can improve the goal-conditioned policy itself, perhaps via policy distillation or obtaining better Q-value estimates. In addition, while the planning algorithm we use is simple (namely, Dijkstra), we believe that the key idea of using distance estimates obtained from RL algorithms for planning will open doors to incorporating more sophisticated planning techniques into RL.</p>
<p>Acknowledgements: We thank Vitchyr Pong, Xingyu Lin, and Shane Gu for helpful discussions on learning goal-conditioned value functions, Aleksandra Faust and Brian Okorn for feedback on connections to planning, and Nikolay Savinov for feedback on the SPTM baseline. RS is supported by NSF grant IIS1763562, ONR grant N000141812861, AFRL CogDeCON, and Apple. Any opinions, findings and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of NSF, AFRL, ONR, or Apple.</p>
<h2>References</h2>
<p>Agrawal, P., Nair, A. V., Abbeel, P., Malik, J., and Levine, S. (2016). Learning to poke by poking: Experiential learning of intuitive physics. In Advances in Neural Information Processing Systems, pages 5074-5082.</p>
<p>Amos, B., Jimenez, I., Sacks, J., Boots, B., and Kolter, J. Z. (2018). Differentiable mpc for end-to-end planning and control. In Advances in Neural Information Processing Systems, pages 8289-8300.</p>
<p>Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., and van den Hengel, A. (2018). Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages $3674-3683$.</p>
<p>Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, O. P., and Zaremba, W. (2017). Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048-5058.</p>
<p>Bacon, P.-L., Harb, J., and Precup, D. (2017). The option-critic architecture. In Thirty-First AAAI Conference on Artificial Intelligence.</p>
<p>Bellemare, M. G., Dabney, W., and Munos, R. (2017). A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 449-458. JMLR. org.</p>
<p>Bickel, P. J., Freedman, D. A., et al. (1981). Some asymptotic theory for the bootstrap. The annals of statistics, $9(6): 1196-1217$.</p>
<p>Chiang, H.-T. L., Faust, A., Fiser, M., and Francis, A. (2019). Learning navigation behaviors end-to-end with autorl. IEEE Robotics and Automation Letters, 4(2):2007-2014.</p>
<p>Choset, H. M., Hutchinson, S., Lynch, K. M., Kantor, G., Burgard, W., Kavraki, L. E., and Thrun, S. (2005). Principles of robot motion: theory, algorithms, and implementation. MIT press.</p>
<p>Chua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages $4759-4770$.</p>
<p>Drummond, C. (2002). Accelerating reinforcement learning by composing solutions of automatically identified subtasks. Journal of Artificial Intelligence Research, 16:59-104.</p>
<p>Faust, A., Ramirez, O., Fiser, M., Oslund, K., Francis, A., Davidson, J., and Tapia, L. (2018). Prm-rl: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning. In Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pages 5113-5120, Brisbane, Australia.</p>
<p>Finn, C. and Levine, S. (2017). Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE.</p>
<p>Florensa, C., Degrave, J., Heess, N., Springenberg, J. T., and Riedmiller, M. (2019). Self-supervised learning of image embedding for continuous control. arXiv preprint arXiv:1901.00943.</p>
<p>Fox, R., Krishnan, S., Stoica, I., and Goldberg, K. (2017). Multi-level discovery of deep options. arXiv preprint arXiv:1703.08294.</p>
<p>Frans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J. (2017). Meta learning shared hierarchies. arXiv preprint arXiv:1710.09767.</p>
<p>Gupta, S., Davidson, J., Levine, S., Sukthankar, R., and Malik, J. (2017). Cognitive mapping and planning for visual navigation. arXiv preprint arXiv:1702.03920, 3.</p>
<p>Hadar, J. and Russell, W. R. (1969). Rules for ordering uncertain prospects. The American Economic Review, 59(1):25-34.</p>
<p>Kaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results. In Proceedings of the tenth international conference on machine learning, volume 951, pages 167-173.</p>
<p>Kaelbling, L. P. (1993b). Learning to achieve goals. In IJCAI, pages 1094-1099. Citeseer.
Kavraki, L., Svestka, P., and Overmars, M. H. (1996). Probabilistic roadmaps for path planning in highdimensional configuration spaces. IEEE transactions on robotics and automation, 12(4):566-580.</p>
<p>Kulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenenbaum, J. (2016). Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pages 3675-3683.</p>
<p>Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. (2018). Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592.</p>
<p>Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6402-6413.</p>
<p>Lau, M. and Kuffner, J. J. (2005). Behavior planning for character animation. In Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 271-280. ACM.</p>
<p>LaValle, S. M. (2006). Planning algorithms. Cambridge university press.
Lee, L., Parisotto, E., Chaplot, D. S., Xing, E., and Salakhutdinov, R. (2018). Gated path planning networks. arXiv preprint arXiv:1806.06408.</p>
<p>Lenz, I., Knepper, R. A., and Saxena, A. (2015). Deepmpc: Learning deep latent features for model predictive control. In Robotics: Science and Systems. Rome, Italy.</p>
<p>Levine, S., Lee, Y., Koltun, V., and Popović, Z. (2011). Space-time planning with parameterized locomotion controllers. ACM Transactions on Graphics (TOG), 30(3):23.</p>
<p>Levy, A., Platt, R., and Saenko, K. (2019). Hierarchical reinforcement learning with hindsight. In International Conference on Learning Representations.</p>
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.</p>
<p>Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine, S., and Sermanet, P. (2019). Learning latent plans from play. arXiv preprint arXiv:1903.01973.</p>
<p>Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., et al. (2016). Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.</p>
<p>Nachum, O., Gu, S. S., Lee, H., and Levine, S. (2018). Data-efficient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems, pages 3307-3317.</p>
<p>Nagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2018). Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559-7566. IEEE.</p>
<p>Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2018). Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 6292-6299. IEEE.</p>
<p>Oh, J., Guo, X., Lee, H., Lewis, R. L., and Singh, S. (2015). Action-conditional video prediction using deep networks in atari games. In Advances in neural information processing systems, pages 2863-2871.</p>
<p>Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. (2016). Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pages 4026-4034.</p>
<p>Pardo, F., Tavakoli, A., Levdik, V., and Kormushev, P. (2017). Time limits in reinforcement learning. arXiv preprint arXiv:1712.00378.</p>
<p>Parr, R. and Russell, S. J. (1998). Reinforcement learning with hierarchies of machines. In Advances in neural information processing systems, pages 1043-1049.</p>
<p>Pong, V., Gu, S., Dalal, M., and Levine, S. (2018). Temporal difference models: Model-free deep rl for model-based control. arXiv preprint arXiv:1802.09081.</p>
<p>Precup, D. (2000). Temporal abstraction in reinforcement learning. University of Massachusetts Amherst.
Racanière, S., Weber, T., Reichert, D., Buesing, L., Guez, A., Rezende, D. J., Badia, A. P., Vinyals, O., Heess, N., Li, Y., et al. (2017). Imagination-augmented agents for deep reinforcement learning. In Advances in neural information processing systems, pages 5690-5701.</p>
<p>Savinov, N., Dosovitskiy, A., and Koltun, V. (2018a). Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653.</p>
<p>Savinov, N., Raichuk, A., Marinier, R., Vincent, D., Pollefeys, M., Lillicrap, T., and Gelly, S. (2018b). Episodic curiosity through reachability. arXiv preprint arXiv:1810.02274.</p>
<p>Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators. In International Conference on Machine Learning, pages 1312-1320.</p>
<p>Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015a). Trust region policy optimization. In International Conference on Machine Learning, pages 1889-1897.</p>
<p>Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.</p>
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
<p>Shah, P., Fiser, M., Faust, A., Kew, J. C., and Hakkani-Tur, D. (2018). Follownet: Robot navigation by following natural language directions with deep reinforcement learning. arXiv preprint arXiv:1805.06150.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484.</p>
<p>Şimşek, Ö., Wolfe, A. P., and Barto, A. G. (2005). Identifying useful subgoals in reinforcement learning by local graph partitioning. In Proceedings of the 22nd international conference on Machine learning, pages 816-823. ACM.</p>
<p>Song, S., Yu, F., Zeng, A., Chang, A. X., Savva, M., and Funkhouser, T. (2017). Semantic scene completion from a single depth image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages $1746-1754$.</p>
<p>Srinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018). Universal planning networks. arXiv preprint arXiv:1804.00645.</p>
<p>Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine Learning Proceedings 1990, pages 216-224. Elsevier.</p>
<p>Sutton, R. S., Precup, D., and Singh, S. (1999). Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211.</p>
<p>Tamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In Advances in Neural Information Processing Systems, pages 2154-2162.</p>
<p>Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K. (2017). Feudal networks for hierarchical reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3540-3549. JMLR. org.</p>
<p>Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279-292.
Watter, M., Springenberg, J., Boedecker, J., and Riedmiller, M. (2015). Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pages $2746-2754$.</p>
<p>Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Wu, Y., Tucker, G., and Nachum, O. (2018). The laplacian in rl: Learning representations with efficient approximations. arXiv preprint arXiv:1810.04586.</p>
<p>Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., and Farhadi, A. (2017). Target-driven visual navigation in indoor scenes using deep reinforcement learning. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 3357-3364. IEEE.</p>
<h1>A Efficient Shortest Path Computation</h1>
<p>Our policy solves a shortest path problem every time it recomputes a new waypoint. Naïvely running Dijkstra's algorithm to compute a shortest path among the states in our active set $\mathcal{B}$ requires $O\left(|\mathcal{B}|^{2}\right)$ queries of our value function. While the search algorithm itself is fast, it is expensive to evaluate the value function on each pair of states at every time step. In our implementation (Algorithm 2), we amortize this computation across many calls to the policy. We periodically periodically evaluate the value function on each pair of nodes in the replay buffer, and then used the Floyd Warshall algorithm to compute the shortest path between all pairs. This takes $O\left(|\mathcal{B}|^{3}\right)$</p>
<p>Algorithm 2 Inputs are the current state $s$, the goal state $g$, the replay buffer $\mathcal{B}$, and the value function $V$. Returns the length and first waypoint of the shortest path.</p>
<div class="codehilite"><pre><span></span><code>function \(\operatorname{ShortestPath}\left(s, s_{g}, \mathcal{B}, V\right)\)
    // Matrices: \(D_{\pi}, D_{\mathcal{B} \rightarrow \mathcal{B}}, D_{s \rightarrow s_{g}} \in \mathbb{R}^{|\mathcal{B}| \times|\mathcal{B}|}\)
    // Vectors: \(D_{s \rightarrow \mathcal{B}}, D_{\mathcal{B} \rightarrow g} \in \mathbb{R}^{|\mathcal{B}|}\)
    \(D_{\pi} \leftarrow-V(\mathcal{B}, \mathcal{B}) \quad \triangleright\) cached
    \(D_{\mathcal{B} \rightarrow \mathcal{B}} \leftarrow \operatorname{FLOYDWARSHALL}\left(D_{\pi}\right) \quad \triangleright\) cached
    \(D_{s \rightarrow \mathcal{B}} \leftarrow-V(s, \mathcal{B})\)
    \(D_{\mathcal{B} \rightarrow g} \leftarrow-V(\mathcal{B}, g)\)
    \(D_{s \rightarrow g} \leftarrow D_{s \rightarrow \mathcal{B}}+D_{\mathcal{B} \rightarrow \mathcal{B}}+\left(D_{\mathcal{B} \rightarrow g}\right)^{T}\)
    \(s_{w_{1}} \leftarrow \arg \min <span class="ge">_{u, v \in \mathcal{B}} D_</span>{s \rightarrow g}\)
    return \(s_{w_{1}}\)
</code></pre></div>

<p>time, but only $O\left(|\mathcal{B}|^{2}\right)$ calls to the value function. Let $D \in \mathbb{R}^{|\mathcal{B}| \times|\mathcal{B}|}$ be the resulting matrix storing the shortest path distances between all pairs of states in the active set. Now, given a start state $s$ and goal state $g$, the shortest path distance is</p>
<p>$$
d_{\mathrm{sp}}(s, g)=\min \left(\min _{u, v \in T} d(s, u)+D[u, v]+d(v, g), d(s, g)\right)
$$</p>
<p>This computation requires $O(|\mathcal{B}|)$ calls to the value function, substantially better than the $O\left(|\mathcal{B}|^{2}\right)$ calls required with the naïve implementation.</p>
<h2>B Environments</h2>
<p>We used two simple navigation environments, Point-U and Point-FourRooms, shown in Figure 4a. In both environments, the observations are the location of the agent, $s=(x, y) \in \mathbb{R}^{2}$. The agent's actions $a=(d x, d y) \in[-1,1]^{2}$ are added to the agents current position at every time step. We tuned the environments so that the goal-conditioned algorithm (which we will use as a baseline) would perform as well as possible. Observing that the agent would get stuck at corners, we modified the environment to automatically add Gaussian noise to the agents action. The resulting dynamics were</p>
<p>$$
s_{t+1}=\operatorname{proj}\left(s_{t}+a_{t}+\epsilon_{t}\right) \quad \text { where } \quad \epsilon_{t} \sim \mathcal{N}\left(0, \sigma^{2}\right)
$$</p>
<p>where proj () handles collisions with walls by projecting the state to the nearest free state. We used $\sigma^{2}=1.0$ for Point-U, and $\sigma^{2}=0.1$ for the (larger) Point-FourRooms environment.</p>
<h2>B. 1 Visual Navigation</h2>
<p>We ran most experiments on SUNCG house 0bda523d58df2ce52d0a1d90ba21f95c. We repeated all experiments on SUNCG house 0601a680273d980b791505cab993096a, with nearly identical results. We manually choose houses using the following criteria (1) single story, (2) no humans, and (3) included multiple rooms to make planning challenging. During training, we sampled "nearby" goal states (within 4 steps) for $80 \%$ of episodes, and sampled goals uniformly at random for the remaining $20 \%$ of episodes. We tuned these parameters to make goal-conditioned RL work as well as possible. We implemented goal-relabelling (Andrychowicz et al., 2017; Kaelbling, 1993b), choosing between the (1) originally sampled goal, the (2) current state, and (3) a future state in the same trajectory, each with probability $33 \%$. The agent's actions space was to move North/South/East/West. Observations were panoramic images, created by concatenating the firstperson views from each of the cardinal directions. We used ensembles of 3 value functions, each with entirely independent weights. For all neural networks conditioned on both the current observation and the goal observation, we concatenated the current observation and goal observation along their last channel. For RGB images, this resulted in an input with dimensions $H \times W \times 6$. For depth images, the concatenated input had dimension $H \times W \times 2$.</p>
<h1>C Ablation Experiments</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Sensitivity to Hyperparameters: (Left) While we used a buffer of 1000 observations for most of our experiments, decreasing the buffer size has little effect on the method's success rate. (Right) When constructing our graph, we ignore edges that are longer than some distance, MAXDIST. We find that this hyperparameter is important to the success of our method.</p>
<p>Because SoRB plans over a fixed replay buffer, one potential concern is that performance might degrade if the replay buffer is too small. To test this concern, we ran an experiment varying the size of the replay buffer. As shown in Figure 10a, decreasing the replay buffer by a factor of 10x led to no discernible drop on performance. While we do expect performance to drop if we further decrease the size of the replay buffer, the requirement of storing 100 states (even high-resolution images) seems relatively minor. In a second ablation experiment, we varied the MAXDIST hyperparameter that governs when we stop adding new edges to the graph. As shown in Figure 10b, SoRB is sensitive to this hyperparameter, with values too large and too smaller leading to worse performance. When the MAXDIST parameter is too small, graph search fails to find a path to the goal state. As we increase MAXDIST, we increase the probability of underestimating the distance between pairs of states. We expect that improvements in uncertainty quantification in RL will improve the stability of our method w.r.t. this hyperparameter.</p>
<h2>D Tricks for Learning Distances with RL</h2>
<ol>
<li>Small learning rates: Especially for the image-based tasks, we found that RL completely failed with using a critic learning rate larger than 1e-4. Smaller learning rates work too, but take longer to converge.</li>
<li>Distributional RL: The value function update for distributional RL has a particularly nice form when values correspond to distances. Additionally, distributional RL implicitly clips the values, preventing the critic to predict that unreachable states are infinitely far away.</li>
<li>Termination Condition: Carefully consider whether to set done = True at the end of each episode. In our setting the agent received a reward of -1 at each time step, so the value of each state was negative. An optimal agent therefore attempts to terminate the episode as quickly as possible. We only set done = True when the agent reached the goal state, not when the maximum number of time steps was reached or when it reached some other absorbing state.</li>
<li>Ensembles of Value Functions: Predicted distances from a single value function can be inaccurate for unseen (state, goal) pairs. When performing search using these predicted distances, these inaccurately-short predictions result in "wormholes" through the environment, where the agent mistakenly believes that two distant states are actually nearby. To mitigate this, we trained multiple, independent critics in parallel on the same data, and then aggregated predictions from each before doing search. Surprisingly, we found that taking the average predicted distance over the ensemble worked as well as taking the maximum predicted distance. We tried accelerating training by using shared convolutional layers for all critics in the ensemble, but found that this resulted in highly-correlated distant predictions that exhibited the "wormhole" problem.</li>
<li>Normalizing Observations: For the visual navigation experiments, we normalized the observations to be in the interval $[0,1]$ by dividing by the maximum pixel intensity ( 32 for depth, 255 for RGB). Normalization was most important for the generalization experiment with RGB observations.</li>
</ol>
<h1>E Failed Experiments</h1>
<ol>
<li>Goal Relabelling: As mentioned above, we tried to combine our method with off-policy goal relabelling (Andrychowicz et al., 2017; Pong et al., 2018). Surprisingly, we found that this hurt performance of the non-search policy, and had no effect on the search policy.</li>
<li>Lower-bounds on $Q$-values: We attempted to use the search path to obtain a lower bound on the target Q-values during training. In the Bellman update, we replaced the distance predicted by the target Q-values with the minimum of (1) the distance predicted by the target Q-network and (2) the distance of the shortest path found by search. This can be interpreted as a generalization of the single-step lower bound from Kaelbling (1993b). Initial experiments showed this approach slowed down learning, and in some cases prevented the algorithm from converging. We hypothesize that Q-learning is must more sensitive to error in the relative values of two actions, rather than the absolute value of any particular action. While our lower-bound method likely decreased the absolute error, it did not decrease the relative error (and may have even increased it).</li>
<li>TD3-style Ensemble Aggregation: In our main experiments, we aggregated distance predictions from the ensemble of distributional critics by first computing the expected distance of each critic, and then taking the maximum predicted distance. This approach ignores the fact that our critics are distributional. Inspired by the stability of TD3, we attempted to apply a similar approach to aggregating predictions from the ensemble of distributional critics. The naïve approach of taking the minimum for each atom does not work because the resulting distribution will not sum to one. Instead, we first compute the cumulative density function (CDF) of each critic and then take the pointwise maximum over the CDFs. Note that critics correspond to negative distance, so the maximum corresponds to being pessimistic. Finally, we convert the resulting CDF back into a PDF and return the corresponding expected distance. While this method has neat connections to second-order stochastic dominance and risk-averse expected utility maximizers (Hadar and Russell, 1969), we found that it worked poorly in practice.</li>
</ol>
<h2>F Hyperparameters</h2>
<p>Unless otherwise noted, all baselines use the same hyperparameters as our method. Unless otherwise noted, parameters were not tuned.</p>
<h2>F. 1 Search on the Replay Buffer</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">Value</th>
<th style="text-align: center;">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">Lower values also work, but training takes longer. Same for actor and critic.</td>
</tr>
<tr>
<td style="text-align: center;">training iterations</td>
<td style="text-align: center;">1 e environment steps</td>
<td style="text-align: center;">Performance changed little after 200k steps.</td>
</tr>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">train steps per environment step</td>
<td style="text-align: center;">$1: 1$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">random steps at start of training</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NN architecture (images)</td>
<td style="text-align: center;">$\operatorname{Conv}(16,8,4)+\operatorname{Conv}(32,4,4)+\operatorname{FC}(256)$</td>
<td style="text-align: center;">Same for depth and RGB images.</td>
</tr>
<tr>
<td style="text-align: center;">optimizer</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;">We used the default Tensorflow settings for $\beta_{A}, \beta_{B}, \epsilon$. Same for actor and critic.</td>
</tr>
<tr>
<td style="text-align: center;">MaxDist</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">See Figure 10</td>
</tr>
<tr>
<td style="text-align: center;">replay buffer size (training)</td>
<td style="text-align: center;">100 k</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">replay buffer size (search)</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">See Figure 10</td>
</tr>
<tr>
<td style="text-align: center;">gamma / discount</td>
<td style="text-align: center;">$\varepsilon$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\epsilon$</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">Exploration parameter for discrete actions, used for visual navigation.</td>
</tr>
<tr>
<td style="text-align: center;">OU-stddes, OU-damping</td>
<td style="text-align: center;">$1.0,2.0$</td>
<td style="text-align: center;">Exploration parameters for continuous actions, used for didactic 2D navigation</td>
</tr>
<tr>
<td style="text-align: center;">reward scale factor</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">Timed for the DDPG baseline on the 2D navigation task.</td>
</tr>
<tr>
<td style="text-align: center;">target network update frequency</td>
<td style="text-align: center;">every 5 steps</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">target network update rate $(\tau)$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Hyperparameters for SoRB</p>
<h1>F. 2 Value Iteration Networks</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
<th style="text-align: left;">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">number of iterations</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">Tuned over $[1,2,5,10,20,50]$. Little effect.</td>
</tr>
<tr>
<td style="text-align: left;">hidden units in VI block</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">Tuned over $[10,30,100,300]$. Little effect</td>
</tr>
</tbody>
</table>
<p>Table 2: Hyperparameters for VIN (Tamar et al., 2016)</p>
<h2>F. 3 Semi-Parametric Topological Memory</h2>
<p>We first tuned the $l$ parameter on goal-reaching without search. Setting $l$ to the best found value, we performed a massive (over 1000 experiments) grid search over $M, s_{\text {reach }}$, and the threshold for adding edges.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
<th style="text-align: left;">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">threshold for adding edges</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">Tuned over $[0.1,0.2,0.5,0.7,0.9]$</td>
</tr>
<tr>
<td style="text-align: left;">$s_{\text {reach }}$, threshold for choosing the <br> next waypoint along the shortest <br> path</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">Tuned over $[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7$, <br> $0.8,0.9,0.95,1.0]$</td>
</tr>
<tr>
<td style="text-align: left;">NN architecture</td>
<td style="text-align: left;">$\operatorname{Conv}(16,8,4)+\operatorname{Conv}(32,4,4)+\operatorname{FC}(256)$</td>
<td style="text-align: left;">Same architecture (but different weights) for the <br> retrieval and locomotor networks.</td>
</tr>
<tr>
<td style="text-align: left;">$l$, threshold for sampling nearby <br> states in trajectory</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">Tuned over $[1,2,4,8]$</td>
</tr>
<tr>
<td style="text-align: left;">$M$, margin between "close" and <br> "far" states</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Tuned over $[1,2,4]$</td>
</tr>
</tbody>
</table>
<p>Table 3: Hyperparameters for SPTM (Savinov et al., 2018a)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We negate the distance prediction from SoRB before computing the precision recall curve because small distances indicate that the policy should be more successful.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
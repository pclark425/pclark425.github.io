<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2238 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2238</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2238</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-278237367</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.00009v1.pdf" target="_blank">Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation</a></p>
                <p><strong>Paper Abstract:</strong> Pre-trained language models (PLMs) demonstrate remarkable intelligence but struggle with emerging tasks unseen during training in real-world applications. Training separate models for each new task is usually impractical. Multi-task learning (MTL) addresses this challenge by transferring shared knowledge from source tasks to target tasks. As an dominant parameter-efficient fine-tuning method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that captures task-specific knowledge, which acts as a prefix to the original prompt that preserves shared knowledge, while keeping PLM parameters frozen. However, PT struggles to effectively capture the heterogeneity of task-specific knowledge due to its limited representational capacity. To address this challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL method built on PT, employing the low-rank representation to model task heterogeneity and a fast-slow weights mechanism where the slow weight encodes shared knowledge, while the fast weight captures task-specific nuances, avoiding the mixing of shared and task-specific knowledge, caused by training low-rank representations from scratch. Moreover, a zero-initialized attention mechanism is introduced to minimize the disruption of immature low-rank components on original prompts during warm-up epochs. Experiments on 16 tasks demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and few-shot settings while maintaining superior parameter efficiency.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2238.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2238.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TA-LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-Adaptive Low-Rank Representation (TA-LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task prompt-tuning method that decomposes adaptable prompt vectors into a shared slow weight and task-specific fast low-rank components (rank-1 per task), using zero-initialized attention and inter-task orthogonalization to disentangle shared vs task-specific knowledge and improve generalization and parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TA-LoRA (method) on Qwen2.5 backbone</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adds a low-rank representation module in the final L layers of a frozen PLM: a shared slow matrix B and per-task fast matrices A_i (implemented as outer products u_i ⊗ v_i, rank-1) added to a shared prompt θ0; uses zero-initialized attention with a learnable gating factor and orthogonality regularization between task-specific components.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Qwen2.5 backbone used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-adaptive low-rank decomposition (shared slow matrix B + task-specific fast rank-1 matrices A_i), plus per-task adaptable prompt vectors; zero-initialized attention gating to isolate adaptable prompt influence during warmup.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language understanding / multi-task NLP (16 tasks: semantic matching, sentiment, reading comprehension, NLI, relation extraction, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Paper reports TA-LoRA outperforms vanilla prompt tuning (PT) by ~13.6% on Unseen Data and ~11.4% on Unseen Task averaged across benchmarks; achieves state-of-the-art across baselines and in some benchmarks exceeds full fine-tuning (FT) in Unseen Data setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared against baselines (FT, Adapter, BitFit, PT, SPoT, ATTEMPT, MPT); TA-LoRA surpasses SPoT, ATTEMPT, and MPT while using a comparable parameter scale. Exact per-task numbers are reported in Tables I/II of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Very parameter-efficient: TA-LoRA uses only 0.1857% of FT's parameters (reported). Fast weights A_i constrained to rank-1 to reduce complexity; separate learning rates for B and A to balance training.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Paper states TA-LoRA maintains same task-specific parameter scale as vanilla prompt tuning and is comparable in parameter cost to SPoT/ATTEMPT/MPT; exact FLOPs/memory/inference-time not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Few-shot experiments (k = 16, 32, 64) show TA-LoRA consistently outperforms PT and MPT; in some datasets 32-shot TA-LoRA achieves performance comparable to full-data training (paper's Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Unseen Task evaluation: TA-LoRA consistently outperforms FT and other baselines, demonstrating improved generalization to out-of-distribution/novel tasks (paper claims state-of-the-art in Unseen Task setting).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated on 16 tasks (8 source, 8 target); TA-LoRA improves average performance across both Unseen Data and Unseen Task settings, indicating task-specific adaptations help multi-task transfer rather than hurt it.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Parameter-efficiency result: consumes 0.1857% of FT parameters while matching/exceeding many baselines; no explicit latency/FLOPs tradeoffs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Task-adaptive low-rank prompt components (shared slow + per-task fast rank-1) substantially improve multi-task transfer and few-shot generalization compared to uniform or single-prompt baselines while remaining highly parameter-efficient (~0.1857% of FT).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical results show task-aligned low-rank adaptations improve performance and generalization vs uniform single-prompt or naive prompt tuning, supporting the idea that task-aligned abstractions aid transfer and efficiency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2238.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2238.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Prompt Tuning (PT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parameter-efficient fine-tuning method that prepends a learnable continuous prompt vector as a prefix to the model input while keeping the PLM's parameters frozen; used as a baseline and as the starting adaptable prompt in TA-LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vanilla Prompt Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fixed-length learnable prompt vectors inserted as prefixes; each task can have its own prompt vector (task-specific static parameters) but with limited representational capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Per-task learnable continuous prompt vectors (fixed-length, prefix-based); uniform in the sense of a single vector per task (no dynamic per-input allocation).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task NLP (same benchmarks as paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Parameter-efficient; paper indicates TA-LoRA maintains the same task-specific parameter scale as PT (so similar parameter footprint), but exact fraction of FT for PT not individually reported (implied comparable to TA-LoRA).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Used as baseline in few-shot; TA-LoRA outperforms PT across k-shot settings per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>PT performs worse than TA-LoRA on both Unseen Data and Unseen Task settings; specific per-task numbers are in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>PT is less effective at capturing task heterogeneity and shows substantially lower average performance than TA-LoRA (paper reports ~13.6% lower on Unseen Data, ~11.4% lower on Unseen Task on average).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Prompt tuning provides a task-specific but capacity-limited representation; TA-LoRA retains PT's parameter-efficiency while addressing its limited expressivity via task-adaptive low-rank components.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>PT is task-specific but limited; TA-LoRA shows that augmenting PT with task-adaptive low-rank representations yields clear performance gains, indicating PT alone is insufficient.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2238.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2238.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MPT (Multitask Prompt Tuning / 'MPT' referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that learns a single transferable prompt by extracting knowledge from multiple task-specific source prompts, used as a baseline (represents a more uniform single-prompt transfer approach).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MPT (single transferable prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns one prompt intended to transfer across tasks by aggregating information from multiple task-specific prompts; functions as a uniform transferable prompt baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Single transferable prompt (uniform prompt shared across tasks rather than task-adaptive components).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task NLP (same benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Comparable parameter scale to TA-LoRA per paper statement; exact metrics not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>TA-LoRA consistently outperforms MPT in few-shot settings (k = 16, 32, 64) per Table II.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>TA-LoRA surpasses MPT on Unseen Task evaluations, indicating single uniform prompts are less effective for cross-task generalization than task-adaptive low-rank components.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>MPT is outperformed by TA-LoRA across averaged multi-task metrics reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>A single transferable prompt (uniform) is less effective than task-adaptive low-rank prompt decompositions for multi-source transfer and few-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Paper's empirical comparisons show task-adaptive TA-LoRA outperforms uniform single-prompt MPT, supporting task-aligned abstraction benefits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2238.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2238.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPoT (Soft Prompt Transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that selects a prefix prompt from source prompts per target instance or task using similarity metrics — a task-aligned prompt selection baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Selects or transfers soft prompt(s) from source tasks to target tasks based on similarity metrics; effectively retrieves task-aligned prompts rather than learning a single uniform prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Prompt retrieval/selection mechanism (per-task or per-instance selection of prefix prompts based on similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task NLP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Comparable parameter scale to TA-LoRA per paper; exact cost not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>TA-LoRA outperforms SPoT while using a comparable parameter scale (paper claim).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>SPoT is a competitive baseline but is outperformed by TA-LoRA on averaged metrics in both Unseen Data and Unseen Task settings.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Task-aligned prompt retrieval (SPoT) helps but is outperformed by the proposed low-rank task-adaptive decomposition which better disentangles shared vs task-specific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>SPoT's task-aligned selection is beneficial, but TA-LoRA's explicit low-rank task-specific components deliver stronger gains, reinforcing the importance of dedicated task-specific adaptations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2238.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2238.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATTEMPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ATTEMPT (Attentional Mixtures of Soft Prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-source prompt method that interpolates prompts from large-scale source tasks using instance attention produced by a lightweight subnetwork; represents a dynamic mixture of task prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ATTEMPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interpolates multiple source prompts per instance via learned instance attention weights from a subnetwork, producing a dynamic task-adaptive prompt mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Attentional mixture of soft prompts (dynamic interpolation per instance/task via attention).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task NLP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Comparable parameter scale to TA-LoRA per the paper; exact efficiency metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>TA-LoRA surpasses ATTEMPT in reported experiments while maintaining similar parameter scale.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>ATTEMPT provides dynamic per-instance prompt mixtures but is outperformed by TA-LoRA's low-rank fast-slow decomposition in the paper's multi-task benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Dynamic mixtures of prompts via attention are useful for task-adaptive behavior, but a structured low-rank decomposition with explicit shared and task-specific components can yield superior transfer and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>ATTEMPT supports the value of dynamic, per-instance adaptation; TA-LoRA's superior empirical performance shows that structured low-rank task-specific modules are a particularly effective form of task alignment.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2238.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2238.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adapter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parameter-efficient fine-tuning approach that injects small tunable adapter modules into transformer layers to adapt PLMs to downstream tasks; used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adapter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adds tunable bottleneck modules (adapters) inside transformer layers to adapt pretrained models with few additional parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Per-task adapter modules inserted into layers (task-specific parameters per task).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task NLP (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Adapters are parameter-efficient compared to FT; paper notes adapters show competitive performance but TA-LoRA outperforms them while offering superior parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>TA-LoRA outperforms Adapter baseline on the evaluated multi-task benchmarks per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Adapter is competitive but inferior to TA-LoRA across aggregated metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adapters provide task-specific parameters and are competitive, yet TA-LoRA's low-rank task decomposition yields better multi-task transfer per the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Adapters demonstrate that explicit task-specific modules help; TA-LoRA's stronger results further support task-aligned specialization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2238.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2238.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Fine-Tuning (FT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard approach of updating all pretrained model parameters for each downstream task; used as a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Full fine-tuning (FT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>All parameters of the PLM are updated for adaptation to downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (when applied to Qwen2.5 backbone in paper comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Uniform model parameter adaptation (no explicit task-specific modularization in the prompt-sense used by the paper's PEFT methods).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task NLP (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>High parameter and computational cost (all model parameters); TA-LoRA uses only 0.1857% of FT's parameters while matching or exceeding FT in some Unseen Task settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>TA-LoRA outperforms FT on the Unseen Task setting according to the paper; in Unseen Data TA-LoRA's average surpasses FT though a few points FT is marginally better.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>FT serves as a strong baseline; TA-LoRA shows advantages in cross-task generalization and parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>FT is resource intensive; TA-LoRA achieves similar/better performance using far fewer tunable parameters (0.1857% of FT's parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Although FT optimizes all parameters, task-adaptive low-rank prompt methods can match or surpass FT on cross-task generalization while being far more parameter-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Demonstrates that structured, task-specific low-parameter adaptations can outperform uniform full-model adaptation for transfer/generalization under parameter constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2238.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2238.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA / Low-rank tuning (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoRA / Low-rank Representation (concept referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Technique that approximates parameter updates via low-rank matrices to enable parameter-efficient fine-tuning; TA-LoRA extends this concept to explicitly separate shared (slow) and task-specific (fast) low-rank components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Low-rank parameter adaptation (LoRA concept)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approximates full parameter updates as products of low-rank matrices to reduce tunable degrees of freedom; used as a foundational idea for TA-LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Low-rank matrix factorization of parameter updates (B * A), applied as shared and task-specific factors in TA-LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>parameter-efficient adaptation for NLP models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Low-rank factors reduce number of tunable parameters and calculation compared to full parameter updates; TA-LoRA further constrains A_i to rank-1 for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Low-rank parameterizations are an effective building block for task-adaptive parameter-efficient tuning; TA-LoRA demonstrates how to split low-rank factors into shared and task-specific components for improved transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>The paper builds on low-rank tuning ideas and shows an explicit decomposition benefits multi-task transfer and efficiency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spot: Better frozen model adaptation through soft prompt transfer <em>(Rating: 2)</em></li>
                <li>Attempt: Parameterefficient multi-task tuning via attentional mixtures of soft prompts <em>(Rating: 2)</em></li>
                <li>Multitask prompt tuning enables parameter-efficient transfer learning <em>(Rating: 2)</em></li>
                <li>Llamaadapter: Efficient fine-tuning of large language models with zeroinitialized attention <em>(Rating: 2)</em></li>
                <li>The power of scale for parameter-efficient prompt tuning <em>(Rating: 2)</em></li>
                <li>Efficient pareto manifold learning with low-rank structure <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2238",
    "paper_id": "paper-278237367",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "TA-LoRA",
            "name_full": "Task-Adaptive Low-Rank Representation (TA-LoRA)",
            "brief_description": "A multi-task prompt-tuning method that decomposes adaptable prompt vectors into a shared slow weight and task-specific fast low-rank components (rank-1 per task), using zero-initialized attention and inter-task orthogonalization to disentangle shared vs task-specific knowledge and improve generalization and parameter efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TA-LoRA (method) on Qwen2.5 backbone",
            "model_description": "Adds a low-rank representation module in the final L layers of a frozen PLM: a shared slow matrix B and per-task fast matrices A_i (implemented as outer products u_i ⊗ v_i, rank-1) added to a shared prompt θ0; uses zero-initialized attention with a learnable gating factor and orthogonality regularization between task-specific components.",
            "model_size": "7B (Qwen2.5 backbone used in experiments)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task-adaptive low-rank decomposition (shared slow matrix B + task-specific fast rank-1 matrices A_i), plus per-task adaptable prompt vectors; zero-initialized attention gating to isolate adaptable prompt influence during warmup.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "language understanding / multi-task NLP (16 tasks: semantic matching, sentiment, reading comprehension, NLI, relation extraction, etc.)",
            "performance_task_aligned": "Paper reports TA-LoRA outperforms vanilla prompt tuning (PT) by ~13.6% on Unseen Data and ~11.4% on Unseen Task averaged across benchmarks; achieves state-of-the-art across baselines and in some benchmarks exceeds full fine-tuning (FT) in Unseen Data setting.",
            "performance_uniform_baseline": "Compared against baselines (FT, Adapter, BitFit, PT, SPoT, ATTEMPT, MPT); TA-LoRA surpasses SPoT, ATTEMPT, and MPT while using a comparable parameter scale. Exact per-task numbers are reported in Tables I/II of the paper.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Very parameter-efficient: TA-LoRA uses only 0.1857% of FT's parameters (reported). Fast weights A_i constrained to rank-1 to reduce complexity; separate learning rates for B and A to balance training.",
            "computational_efficiency_baseline": "Paper states TA-LoRA maintains same task-specific parameter scale as vanilla prompt tuning and is comparable in parameter cost to SPoT/ATTEMPT/MPT; exact FLOPs/memory/inference-time not reported.",
            "sample_efficiency_results": "Few-shot experiments (k = 16, 32, 64) show TA-LoRA consistently outperforms PT and MPT; in some datasets 32-shot TA-LoRA achieves performance comparable to full-data training (paper's Table II).",
            "transfer_generalization_results": "Unseen Task evaluation: TA-LoRA consistently outperforms FT and other baselines, demonstrating improved generalization to out-of-distribution/novel tasks (paper claims state-of-the-art in Unseen Task setting).",
            "interpretability_results": null,
            "multi_task_performance": "Evaluated on 16 tasks (8 source, 8 target); TA-LoRA improves average performance across both Unseen Data and Unseen Task settings, indicating task-specific adaptations help multi-task transfer rather than hurt it.",
            "resource_constrained_results": "Parameter-efficiency result: consumes 0.1857% of FT parameters while matching/exceeding many baselines; no explicit latency/FLOPs tradeoffs provided.",
            "key_finding_summary": "Task-adaptive low-rank prompt components (shared slow + per-task fast rank-1) substantially improve multi-task transfer and few-shot generalization compared to uniform or single-prompt baselines while remaining highly parameter-efficient (~0.1857% of FT).",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical results show task-aligned low-rank adaptations improve performance and generalization vs uniform single-prompt or naive prompt tuning, supporting the idea that task-aligned abstractions aid transfer and efficiency.",
            "uuid": "e2238.0"
        },
        {
            "name_short": "PT",
            "name_full": "Vanilla Prompt Tuning (PT)",
            "brief_description": "Parameter-efficient fine-tuning method that prepends a learnable continuous prompt vector as a prefix to the model input while keeping the PLM's parameters frozen; used as a baseline and as the starting adaptable prompt in TA-LoRA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vanilla Prompt Tuning",
            "model_description": "Fixed-length learnable prompt vectors inserted as prefixes; each task can have its own prompt vector (task-specific static parameters) but with limited representational capacity.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Per-task learnable continuous prompt vectors (fixed-length, prefix-based); uniform in the sense of a single vector per task (no dynamic per-input allocation).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "multi-task NLP (same benchmarks as paper)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Parameter-efficient; paper indicates TA-LoRA maintains the same task-specific parameter scale as PT (so similar parameter footprint), but exact fraction of FT for PT not individually reported (implied comparable to TA-LoRA).",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Used as baseline in few-shot; TA-LoRA outperforms PT across k-shot settings per the paper.",
            "transfer_generalization_results": "PT performs worse than TA-LoRA on both Unseen Data and Unseen Task settings; specific per-task numbers are in paper tables.",
            "interpretability_results": null,
            "multi_task_performance": "PT is less effective at capturing task heterogeneity and shows substantially lower average performance than TA-LoRA (paper reports ~13.6% lower on Unseen Data, ~11.4% lower on Unseen Task on average).",
            "resource_constrained_results": null,
            "key_finding_summary": "Prompt tuning provides a task-specific but capacity-limited representation; TA-LoRA retains PT's parameter-efficiency while addressing its limited expressivity via task-adaptive low-rank components.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "PT is task-specific but limited; TA-LoRA shows that augmenting PT with task-adaptive low-rank representations yields clear performance gains, indicating PT alone is insufficient.",
            "uuid": "e2238.1"
        },
        {
            "name_short": "MPT",
            "name_full": "MPT (Multitask Prompt Tuning / 'MPT' referenced in paper)",
            "brief_description": "A method that learns a single transferable prompt by extracting knowledge from multiple task-specific source prompts, used as a baseline (represents a more uniform single-prompt transfer approach).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MPT (single transferable prompt)",
            "model_description": "Learns one prompt intended to transfer across tasks by aggregating information from multiple task-specific prompts; functions as a uniform transferable prompt baseline.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Single transferable prompt (uniform prompt shared across tasks rather than task-adaptive components).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "multi-task NLP (same benchmarks)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Comparable parameter scale to TA-LoRA per paper statement; exact metrics not reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "TA-LoRA consistently outperforms MPT in few-shot settings (k = 16, 32, 64) per Table II.",
            "transfer_generalization_results": "TA-LoRA surpasses MPT on Unseen Task evaluations, indicating single uniform prompts are less effective for cross-task generalization than task-adaptive low-rank components.",
            "interpretability_results": null,
            "multi_task_performance": "MPT is outperformed by TA-LoRA across averaged multi-task metrics reported in the paper.",
            "resource_constrained_results": null,
            "key_finding_summary": "A single transferable prompt (uniform) is less effective than task-adaptive low-rank prompt decompositions for multi-source transfer and few-shot generalization.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Paper's empirical comparisons show task-adaptive TA-LoRA outperforms uniform single-prompt MPT, supporting task-aligned abstraction benefits.",
            "uuid": "e2238.2"
        },
        {
            "name_short": "SPoT",
            "name_full": "SPoT (Soft Prompt Transfer)",
            "brief_description": "A method that selects a prefix prompt from source prompts per target instance or task using similarity metrics — a task-aligned prompt selection baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SPoT",
            "model_description": "Selects or transfers soft prompt(s) from source tasks to target tasks based on similarity metrics; effectively retrieves task-aligned prompts rather than learning a single uniform prompt.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Prompt retrieval/selection mechanism (per-task or per-instance selection of prefix prompts based on similarity).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task NLP",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Comparable parameter scale to TA-LoRA per paper; exact cost not reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "TA-LoRA outperforms SPoT while using a comparable parameter scale (paper claim).",
            "interpretability_results": null,
            "multi_task_performance": "SPoT is a competitive baseline but is outperformed by TA-LoRA on averaged metrics in both Unseen Data and Unseen Task settings.",
            "resource_constrained_results": null,
            "key_finding_summary": "Task-aligned prompt retrieval (SPoT) helps but is outperformed by the proposed low-rank task-adaptive decomposition which better disentangles shared vs task-specific knowledge.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "SPoT's task-aligned selection is beneficial, but TA-LoRA's explicit low-rank task-specific components deliver stronger gains, reinforcing the importance of dedicated task-specific adaptations.",
            "uuid": "e2238.3"
        },
        {
            "name_short": "ATTEMPT",
            "name_full": "ATTEMPT (Attentional Mixtures of Soft Prompts)",
            "brief_description": "A multi-source prompt method that interpolates prompts from large-scale source tasks using instance attention produced by a lightweight subnetwork; represents a dynamic mixture of task prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ATTEMPT",
            "model_description": "Interpolates multiple source prompts per instance via learned instance attention weights from a subnetwork, producing a dynamic task-adaptive prompt mixture.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Attentional mixture of soft prompts (dynamic interpolation per instance/task via attention).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task NLP",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Comparable parameter scale to TA-LoRA per the paper; exact efficiency metrics not provided.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "TA-LoRA surpasses ATTEMPT in reported experiments while maintaining similar parameter scale.",
            "interpretability_results": null,
            "multi_task_performance": "ATTEMPT provides dynamic per-instance prompt mixtures but is outperformed by TA-LoRA's low-rank fast-slow decomposition in the paper's multi-task benchmarks.",
            "resource_constrained_results": null,
            "key_finding_summary": "Dynamic mixtures of prompts via attention are useful for task-adaptive behavior, but a structured low-rank decomposition with explicit shared and task-specific components can yield superior transfer and efficiency.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "ATTEMPT supports the value of dynamic, per-instance adaptation; TA-LoRA's superior empirical performance shows that structured low-rank task-specific modules are a particularly effective form of task alignment.",
            "uuid": "e2238.4"
        },
        {
            "name_short": "Adapter",
            "name_full": "Adapter",
            "brief_description": "Parameter-efficient fine-tuning approach that injects small tunable adapter modules into transformer layers to adapt PLMs to downstream tasks; used as a baseline in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adapter",
            "model_description": "Adds tunable bottleneck modules (adapters) inside transformer layers to adapt pretrained models with few additional parameters.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Per-task adapter modules inserted into layers (task-specific parameters per task).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "multi-task NLP (baseline)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Adapters are parameter-efficient compared to FT; paper notes adapters show competitive performance but TA-LoRA outperforms them while offering superior parameter efficiency.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "TA-LoRA outperforms Adapter baseline on the evaluated multi-task benchmarks per the paper.",
            "interpretability_results": null,
            "multi_task_performance": "Adapter is competitive but inferior to TA-LoRA across aggregated metrics reported.",
            "resource_constrained_results": null,
            "key_finding_summary": "Adapters provide task-specific parameters and are competitive, yet TA-LoRA's low-rank task decomposition yields better multi-task transfer per the experiments.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Adapters demonstrate that explicit task-specific modules help; TA-LoRA's stronger results further support task-aligned specialization.",
            "uuid": "e2238.5"
        },
        {
            "name_short": "Full Fine-Tuning",
            "name_full": "Full Fine-Tuning (FT)",
            "brief_description": "Standard approach of updating all pretrained model parameters for each downstream task; used as a strong baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Full fine-tuning (FT)",
            "model_description": "All parameters of the PLM are updated for adaptation to downstream tasks.",
            "model_size": "7B (when applied to Qwen2.5 backbone in paper comparisons)",
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Uniform model parameter adaptation (no explicit task-specific modularization in the prompt-sense used by the paper's PEFT methods).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "multi-task NLP (baseline)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "High parameter and computational cost (all model parameters); TA-LoRA uses only 0.1857% of FT's parameters while matching or exceeding FT in some Unseen Task settings.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "TA-LoRA outperforms FT on the Unseen Task setting according to the paper; in Unseen Data TA-LoRA's average surpasses FT though a few points FT is marginally better.",
            "interpretability_results": null,
            "multi_task_performance": "FT serves as a strong baseline; TA-LoRA shows advantages in cross-task generalization and parameter efficiency.",
            "resource_constrained_results": "FT is resource intensive; TA-LoRA achieves similar/better performance using far fewer tunable parameters (0.1857% of FT's parameters).",
            "key_finding_summary": "Although FT optimizes all parameters, task-adaptive low-rank prompt methods can match or surpass FT on cross-task generalization while being far more parameter-efficient.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Demonstrates that structured, task-specific low-parameter adaptations can outperform uniform full-model adaptation for transfer/generalization under parameter constraints.",
            "uuid": "e2238.6"
        },
        {
            "name_short": "LoRA / Low-rank tuning (concept)",
            "name_full": "LoRA / Low-rank Representation (concept referenced)",
            "brief_description": "Technique that approximates parameter updates via low-rank matrices to enable parameter-efficient fine-tuning; TA-LoRA extends this concept to explicitly separate shared (slow) and task-specific (fast) low-rank components.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Low-rank parameter adaptation (LoRA concept)",
            "model_description": "Approximates full parameter updates as products of low-rank matrices to reduce tunable degrees of freedom; used as a foundational idea for TA-LoRA.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Low-rank matrix factorization of parameter updates (B * A), applied as shared and task-specific factors in TA-LoRA.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "parameter-efficient adaptation for NLP models",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "Low-rank factors reduce number of tunable parameters and calculation compared to full parameter updates; TA-LoRA further constrains A_i to rank-1 for efficiency.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Low-rank parameterizations are an effective building block for task-adaptive parameter-efficient tuning; TA-LoRA demonstrates how to split low-rank factors into shared and task-specific components for improved transfer.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "The paper builds on low-rank tuning ideas and shows an explicit decomposition benefits multi-task transfer and efficiency.",
            "uuid": "e2238.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spot: Better frozen model adaptation through soft prompt transfer",
            "rating": 2
        },
        {
            "paper_title": "Attempt: Parameterefficient multi-task tuning via attentional mixtures of soft prompts",
            "rating": 2
        },
        {
            "paper_title": "Multitask prompt tuning enables parameter-efficient transfer learning",
            "rating": 2
        },
        {
            "paper_title": "Llamaadapter: Efficient fine-tuning of large language models with zeroinitialized attention",
            "rating": 2
        },
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning",
            "rating": 2
        },
        {
            "paper_title": "Efficient pareto manifold learning with low-rank structure",
            "rating": 1
        }
    ],
    "cost": 0.017836499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation
20 Apr 2025</p>
<p>Xiao Zhang 
School of Computer and Communication Engineering
University of Science and Technology Beijing
BeijingChina</p>
<p>Kangsheng Wang 
School of Computer and Communication Engineering
University of Science and Technology Beijing
BeijingChina</p>
<p>Tianyu Hu 
School of Computer and Communication Engineering
University of Science and Technology Beijing
BeijingChina</p>
<p>Huimin Ma 
School of Computer and Communication Engineering
University of Science and Technology Beijing
BeijingChina</p>
<p>Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation
20 Apr 20252683BD56DC4A997968567EBDD0523B22arXiv:2505.00009v1[cs.CL]multi-task learningprompt tuninglow-rank representationfast-slow weights mechanism
Pre-trained language models (PLMs) demonstrate remarkable intelligence but struggle with emerging tasks unseen during training in real-world applications.Training separate models for each new task is usually impractical.Multi-task learning (MTL) addresses this challenge by transferring shared knowledge from source tasks to target tasks.As an dominant parameter-efficient fine-tuning method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that captures task-specific knowledge, which acts as a prefix to the original prompt that preserves shared knowledge, while keeping PLM parameters frozen.However, PT struggles to effectively capture the heterogeneity of task-specific knowledge due to its limited representational capacity.To address this challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL method built on PT, employing the low-rank representation to model task heterogeneity and a fast-slow weights mechanism where the slow weight encodes shared knowledge, while the fast weight captures task-specific nuances, avoiding the mixing of shared and task-specific knowledge, caused by training lowrank representations from scratch.Moreover, a zero-initialized attention mechanism is introduced to minimize the disruption of immature low-rank components on original prompts during warm-up epochs.Experiments on 16 tasks demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and fewshot settings while maintaining superior parameter efficiency. 1</p>
<p>I. INTRODUCTION</p>
<p>Pre-trained language models (PLMs) exhibit human-like intelligence by capturing general knowledge from large-scale corpora [1].However, their applications remain largely confined to applications such as chatbots due to the complexity of real-world implementation, which often requires handling multiple downstream tasks, such as writing and reasoning, alongside emerging tasks unseen during training [2]- [5].The approach employed by mainstream PLM providers-training new models for new tasks such as data analysis and numerical prediction-is both costly and somewhat impractical [6], [7].Moreover, it does not effectively leverage crosstask knowledge.To address this, multi-task learning (MTL) offers a promissing solution by utilizing shared knowledge from source tasks (those available during training) to enhance generalization to target tasks (those involving unseen data).</p>
<p>Prompt tuning (PT) [8], a parameter-efficient fine-tuning (PEFT) method, reduces the cost of adapting PLMs to downstream tasks by introducing a learnable continuous prompt vector as a prefix to the original prompt, while keeping the pretrained parameters frozen.The adaptable prompt encodes taskspecific knowledge, whereas the original prompt retains shared knowledge, aligning naturally with the objectives of MTL.Recent studies [9], [10] have explored fine-tuning adaptable vectors on multi-source tasks to generalize to target tasks, allowing PLMs to benefit from the knowledge transfer.However, even when learnable, adaptable prompts often struggle to capture the heterogeneity within source task distributions, possibly due to the limitations in expressive capacity imposed by their form and length [11]- [13].Heterogeneity refers to the differences among tasks, highlighting the unique feature distribution of each task.Ineffectively capturing heterogeneity can obscure the distinction between shared and task-specific knowledge, hinder the abstraction of shared knowledge, and limit the generalization to unseen tasks.</p>
<p>To address these challenges, we introduce Task-Adaptive Low-Rank Representation (TA-LoRA), a novel MTL method built on PT, effectively separating shared knowledge from heterogeneity by leveraging the consistent direction of adaptable vectors in the parameter space.We employ low-rank representations, expressed as the product of low-rank matrices, to approximate heterogeneity, as the finite degrees of freedom inherent in low-rank structures can effectively capture the primary directions of variation in parameter updates [14].These directions represent the evolving trends of task-specific feature distributions within the model's parameter space.</p>
<p>However, the low-rank matrices trained from scratch above result in the entanglement of shared and task-specific knowledge.To address this, we propose a fast-slow weights mechanism where the slow and fast weights represent shared and task-specific, respectively.Furthermore, we mitigate the disruption of low-rank representations to the original prompt using a zero-initialized attention mechanism [15] to ensure stability during warm-up epochs.This design allows the lowrank representation weights to gradually amplify via a learn-Low-rank Repr.</p>
<p>[CLS]</p>
<p>token
[MASK] Adaptable Frozen � 1 � 2 ... � � � 0 ... � 1 � 1 � � � � Shared � Initialize Original Prompt
...</p>
<p>Forward Backward</p>
<p>Multi-task Input Partition in parameter space</p>
<p>Low-rank representation
� 1 � 2 � � prompt tuning
Zero-initialized attn.Self-attn.able gating factor.By efficiently decoupling shared and taskspecific knowledge in multi-source tasks, TA-LoRA effectively generalizes shared knowledge to target tasks.
Transform Layers × � − � � � � � � Self-attn. Transform Layers × � � � � � � � � �
Extensive experiments were conducted on 16 NLP tasks, following the setup of [16].The results on unseen data and unseen tasks demonstrate that TA-LoRA outperforms strong baselines while maintaining superior parameter efficiency.In specific benchmark scenarios, the proposed method even outperforms full fine-tuning.Notably, TA-LoRA remains effective in few-shot settings, achieving strong performance with only 32-shot on target tasks.Our main contributions are as follows.</p>
<p>• We propose TA-LoRA, the first multi-task prompt tuning framework to leverage the low-rank representations for capturing task heterogeneity, enabling refined taskspecific learning and enhanced shared knowledge abstraction for target task generalization.</p>
<p>II. BACKGROUND</p>
<p>Parameter-efficient fine-tuning.Parameter-efficient finetuning is proposed to improve the efficiency of full fine-tuning by adjusting only a few parameters, while freezing the PLMs' parameters to retain pre-trained knowledge [17].BitFit [18] adjusts only bias terms to enhance update efficiency, while LST [19] trains a small side network with shortcut connections from the backbone's intermediate activations.Methods like Adapter [20], [21] and its variants [22] inject tunable parameters into PLMs for downstream tasks.Among these, LoRA, which employs low-rank matrices to approximate model pa-rameters, has gained significant attention.Another approach, Prompt tuning [8], [23], introduces learnable prompt vectors, with Vanilla prompt tuning [8] adding prompt vectors before key-value matrices and P-Tuning [23] modifying the embedding matrix.TA-LORA improves by addressing the sensitivity to the initialization of adaptable prompts by applying a zeroinitialized attention mechanism.Multi-task prompt tuning.Prompt tuning is particularly suitable for MTL compared to other PEFT methods because the adaptable prompt naturally disentangles shared and taskspecific knowledge, with the position of the adaptable prompt relative to the original prompt in the model reflecting these two types of knowledge [24], [25].This makes it inherently capable of representing heterogeneity across multiple source tasks, a property proven effective in NLP.Common methods like SPoT [9] use similarity metrics to select a prefix prompt for the current task, while ATTEMPT [26] interpolates prompts from large-scale source tasks with a newly initialized target task prompt, using instance attention from a lightweight subnetwork trained on multiple target tasks.MPT [10] learns a single transferable prompt by extracting knowledge from multiple task-specific source prompts.The proposed approach is the first to employ low-rank representations to capture task heterogeneity within PT, while avoiding the entanglement of shared and task-specific knowledge during training low-rank matrices from scratch, through a fast-slow weights mechanism.</p>
<p>III. METHODOLOGY</p>
<p>A. Problem Formulation</p>
<p>Given a set of multi-source tasks T = {T 1 , ..., T t } exhibiting heterogeneity, to enable cross-task knowledge transfer in T , we insert the adaptable continuous prompt vectors {θ 1 , ..., θ t } initialized by PT on each task, as prefixes to original prompts.We propose TA-LoRA based on low-rank representation to capture the heterogeneity across T , as illustrated in Figure 1.We decompose the adaptable continuous prompt vectors in the parameter space into θ 0 (shared knowledge) and parameters capturing the heterogeneity of multi-source data, which enables the separation of shared and task-specific knowledge.The task-specific parameters are approximated via low-rank representation using a low-rank structure BA.</p>
<p>Moreover, we introduce a fast-slow weights mechanism, splitting the low-rank representation matrices into a shared matrix B and task-specific matrices {A i } t i=1 .To reduce computational complexity, we represent each A i as a rank-1 matrix obtained from the outer product of two vectors u i and v i .After multiplying the shared and task-specific lowrank matrices, they are added to θ 0 , resulting in a low-rank representation.At the l-th layer, the adaptable prompt P l a serves as a prefix to the original prompt embedding P l f .The prediction for the masked token is denoted as t l .Finally, TA-LoRA separately calculates the fine-tuned adaptable vector attention score using a zero-initialized attention mechanism to mitigate sensitivity to initialization.</p>
<p>B. Low-rank Heterogeneity Representation</p>
<p>We treat the adaptable vectors θ = {θ 1 , ..., θ t } across tasks as a means to fine-tune PLMs and base models for capturing heterogeneous knowledge.Given t base models, we average their parameters in the parameter space, decomposing the parameter set of each base model into two components: θ 0 = 1 t t i=1 θ i , which represents global task knowledge, and θ i − θ 0 , which captures task-specific knowledge, as shown in (1).This distinction is made because global task knowledge is shared across all tasks and can be represented as a consistent direction in the parameter space.
θ = t i=1   θ 0 + 1 t t j=1 (θ i − θ j )  (1)
where represents the concatenation operation along axis 0.</p>
<p>Due to the limited representational capacity of parameters of PT, capturing the differences between multi-source heterogeneous tasks becomes challenging.To address this, we introduce a low-rank representation in TA-LoRA to approximate the task-specific knowledge captured by these differences, as shown in (2).Therefore, (1) is reformulated as (3).The lowrank representation approximates the task-specific knowledge, which is then combined with the shared knowledge θ 0 by a broadcasting mechanism to fine-tune PLMs.
1 t t j=1 (θ i − θ j ) ≈ B i A i(2)θ = θ 0 + s t i=1 B i A i(3)
where B ∼ N (0, σ 2 ) and A ∼ N (0, σ 2 ) represent lowrank matrices; s is a scaling factor that controls the magnitude of the task-specific knowledge.</p>
<p>When representing the heterogeneity of T using a low-rank approximation, we drew inspiration from [11]: initially, the similarity between base models is nearly zero, but increases significantly as training progresses, particularly in lower-level layers.To demonstrate the suitability of Qwen (PLM with 28 decoder layers) [27] used in the proposed framework, we calculated the similarity of the 1st, 14th, and 28th layers, as shown in Appendix A 1 , verifying the above hypothesis, which led us to deconstruct the low-rank representation in the later L layers of Qwen.The high similarity in the first 28 − L layers suggests that they capture shared features, while the later L layers focus on task-specific features.
� 1 � 1 � 2 � 2 Shared � � 1 � 2 Task � 1 -specific Task � 2 -specific � 1 ~�(0, � 2 ) �~�(0, � 2 ) � 2 ~�(0, � 2 ) Partition Initialization Multiply Low-rank representation Loss 푙 1 푙 2 푙 2</p>
<p>C. Slow-Fast Weights Mechanism</p>
<p>Low-rank representation presents a challenge: capturing heterogeneity through low-rank approximation often causes the low-rank matrices, trained from scratch, to learn representations that entangle both cross-task knowledge and taskspecific knowledge [10].This process is not affected by the previous parameter decomposition.To address this and reduce the computational complexity, we propose a fast-slow weights mechanism where B is treated as the slow weight shared across tasks, while A = {A i } t i=1 serves as the fast weight to encode task-specific knowledge in the low-rank subspace for each task, as illustrated in Figure 2. B and A are assigned with different learning rates to balance the varying gradient scales.To reduce the computational complexity introduced by A and to achieve task-specific knowledge specialization, we propose a compact representation method that decomposes the fast weight A i for task T i into two components: u i and v i .The outer product of the task-specific vectors u i and v i forms the fast weight A i = u i ⊗ v i , yielding a rank-1 matrix.Accordingly, (2) is reformulated as follows.
1 t t j=1 (θ i − θ j ) ≈ B(A i ) = B(u i ⊗ v i )(4)
This approach effectively captures the core features of each task while minimizing the number of degrees of freedom.In MTL, the rank-1 matrix constraint regulates the model's capacity, encouraging it to prioritize the extraction of essential task features while avoiding overfitting on excessive detail or noise in the training data [28], [29].</p>
<p>D. Zero-Initialized Attention</p>
<p>Low-rank representation poses an additional challenge: when randomly initialized, the low-rank matrices, being part of the prompt, may interfere with the original tokens, potentially leading to unreliable adapter prompts that disrupt attention calculation.Thus, we introduce a zero-initialized attention mechanism designed to minimize the disruption of unreliable low-rank matrices on the original prompt tokens during warmup epochs.Specifically, we adapt the self-attention mechanism in the final L layers to its variant of zero-initialized attention for the adaptable vector.The embedding [P l a , P l f ] are processed through multiple linear projection layers to generate the queries, keys, and values, as outlined below.
Q l = W l q • t l(5)K l = W l k • <a href="6">P l a , P l f </a>V l = W l v • <a href="7">P l a , P l f </a>
where W l q , W l k , and W l v are the linear projection matrices for queries, keys, and values, respectively.The attention score is calculated as follows.
S l = softmax Q l (K l ) T √ H = <a href="8">S l a , S l f </a>
where H denotes the feature dimension of PLMs; S l a and S l f are the attention scores for the adaptable and the original prompt tokens, respectively.At this stage, to ensure that the original tokens are not affected by unreliable low-rank matrices during the warm-up epochs, we set their attention scores to zero and gradually increase them as training progresses.We introduce a learnable gating factor, denoted as g l in (9), which controls the attention scores of adaptable vectors.To prevent mutual interference between adaptable and original tokens, the softmax(•) function is applied independently to respective attention components S l a and S l f .
S l a ← tanh(g l ) • S l a(9)</p>
<p>E. Source Optimization and Target Generalization</p>
<p>The optimization process for the source task consists of two steps.First, we train a single-source adapter prompt for each task, which serves as our base model.The base model is represented using (3), and the attention scores are calculated based on (8).Building on this, we optimize the proposed TA-LoRA framework by incorporating inter-task parameter orthogonalization as a regularization term into the loss function of the PLM, as detailed in (10).
L = E (x,y)∈T L PLM + λ t i=1 j̸ =i ∥A ⊤ i A j − I∥ 2 2(10)
Here, L PLM represents the pre-training loss [27] of the PLM, while the second term serves as a regularization component to enforce orthogonality among the task-specific low-rank matrices.The A ⊤ i A j constraint is applied specifically to the final L layers of the PLM decoder.The regularization coefficient λ determines the relative weight of this term.Additionally, B and A are optimized with distinct learning rates to ensure balanced and effective training.</p>
<p>To achieve effective generalization for target tasks in a fewshot setting, we initialize a task-specific rank-1 A i = (u i ⊗v i ) for each target task, and optimize it through L PLM .Furthermore, we posit that if multiple target tasks are involved, the base model parameters for these tasks can be updated using the same approach as for the source tasks.</p>
<p>IV. EXPERIMENTS</p>
<p>We evaluated TA-LoRA on a diverse set of NLP datasets intentionally selected to be independent of widely used benchmarks.This careful selection minimizes the risk of data leakage from the large-scale training corpora of PLMs, ensuring a fair assessment of the proposed method.Experimental results show that TA-LoRA consistently outperforms strong baselines in both full-data (Table I) and few-shot (Table II) settings, while maintaining superior performance efficiency.</p>
<p>A. Experimental Setup</p>
<p>Datasets.We curated and preprocessed 16 NLP task datasets from [16], designating AFQMC, Amazon, THUCNews, BQ, CMNLI, CMRC-2018, SanWen, and COTE-MFW as source tasks, and ChnSent, TNews, OCNLI, LCQMC, DRCD, C3, COTE-BD, and FinRE as target tasks.Detailed descriptions of these datasets are provided in Appendix B 1 .To evaluate TA-LoRA, we used two evaluation strategies: Unseen Data and Unseen Task.The former is a test set from the same distribution as the training data, evaluating the model's ability to capture heterogeneity, while the latter includes outof-distribution data, assessing the generalization of shared knowledge to unknown tasks.Backbone.We use the PLM of Qwen2.5 [27] with 7B parameters and employ an adaptable prompt of length 20 and introduce our low-rank representation in the last 14 layers.Baselines.We evaluate TA-LoRA against the following baselines: (1) Full fine-tuning (FT), where the entire PLM is finetuned; (2) Adapter [20], where a tunable adapter is applied; (3) BitFit [18], where only the bias parameters are fine-tuned; (4) Vanilla prompt tuning (PT) [8], where single-source prompts are fine-tuned; (5) SPoT [9], where single-source adaptable prompts are applied; (6) ATTEMPT [26], where multi-source adaptable prompts are used; (7) MPT [10], where multi-source adaptable prompts decomposition is employed.</p>
<p>B. Main Results</p>
<p>Full-data adaptation.Table I presents the performance of different methods across 16 tasks under two settings: Unseen Data and Unseen Task.The results demonstrate the superior performance and high parameter efficiency of TA-LoRA.Compared to PT, TA-LoRA achieves great improvements in performance while maintaining the same taskspecific parameter scale.Specifically, TA-LoRA outperforms PT by approximately 13.6% on unseen data and 11.4% on unseen tasks, underscoring the effectiveness of extracting   shared and task-specific knowledge from multi-source tasks.While Adapters also show competitive performance, the proposed method consistently outperforms it and offers superior parameter efficiency.In the Unseen Data setting, TA-LoRA achieves an average performance that surpasses FT.While its performance is marginally lower than FT on certain data points, TA-LoRA utilizes only 0.1857% of FT's parameters.</p>
<p>In the Unseen Task setting, TA-LoRA consistently outperforms FT, further demonstrating the benefits of leveraging cross-task knowledge transfer.Moreover, TA-LoRA achieves state-of-the-art results, surpassing SPoT, ATTEMPT, and MPT with a comparable parameter scale.Few-shot adaptation.We conducted few-shot experiments (k = 16, 32, 64) to evaluate the generalization ability of TA-LoRA to unseen tasks with limited training examples.</p>
<p>Table II presents the results of our method compared to other baselines.The findings demonstrate that TA-LoRA consistently outperforms PT and MPT across various k-shot settings.Notably, in some datasets, TA-LoRA with 32-shot achieves performance comparable to the full dataset setting.These findings clearly indicate that TA-LoRA effectively leverages cross-task knowledge from source tasks to generalize to target tasks with only a few labeled samples.</p>
<p>C. Ablation Studies</p>
<p>Fast-slow weights mechanism.To verify the effectiveness of the fast-slow weights mechanism, we compare TA-LoRA with a version of the proposed method without this mechanism, as illustrated in Table III.Moreover, we treat the shared and task-specific matrices as having the same learning rate, and the corresponding results (2nd row of Table III) indicate that the baseline (1st row of Table III) achieves an average performance improvement of 1.8% and 1.7% under the two settings compared to the ablation results.Additionally, the 3rd row of Table III shows that removing the shared B mechanism significantly reduces the performance of the proposed method.</p>
<p>Zero-initialized attention.We further examined the disruption of zero-initialized attention on TA-LoRA, as shown in the 4th row of Table III.When the attention mechanism in TA-LoRA was replaced with the default multi-head self-attention, we observed a performance decline.These findings confirm that zero-initialized attention enhances the performance of TA-LoRA, aligning with the results reported by [15], which demonstrated that zero-initialized attention mitigates the sensitivity of adaptable prompts to initialization.</p>
<p>V. CONCLUSION</p>
<p>The proposed method aims to capture heterogeneity across source tasks by leveraging the low-rank representation, effectively disentangling shared and task-specific knowledge.To achieve this, we propose TA-LoRA, a novel MTL approach.Moreover, we propose a fast-slow weights mechanism, where the slow weight encodes shared knowledge, and the fast weight captures task-specific knowledge, reducing their entanglement during training.Task-specific knowledge is regularized through interactions with other tasks, preventing overfitting to details and noise in datasets and enhancing robustness, while shared knowledge acts as prior information to facilitate generalization to target tasks.To address initialization sensitivity, we introduce a zero-initialized attention with a gating mechanism that separates the attention computation for adaptable and original prompts, preventing early-stage prompts from interfering with the original ones.Experimental results demonstrate that TA-LoRA outperforms existing methods in both full-data and few-shot scenarios, achieving superior generalization to target tasks while maintaining parameter efficiency.</p>
<p>Inspired by the findings in [11], where the similarity between base models was observed to be nearly zero initially but increased significantly during training, particularly in the lower-level layers, we further analyzed this phenomenon in the context of the proposed TA-LoRA framework.Specifically, we evaluated the similarity between base models at different layers of Qwen (a PLM with 28 decoder layers) [27] to demonstrate its suitability within our framework.We calculated the similarity of the 1st, 14th, and 28th layers, as shown in Figure 3.At the onset of training, the similarity between the base models is negligible, with values close to zero (e.g., -0.0015% at the 14th layer, 0.0018% at the 21st layer, and -0.0017% at the 27th layer).For the analysis presented in Figure 3, these similarities are approximated as zero for simplicity.Our results indicate that the similarity between base models in the 1st layer is almost negligible, reflecting the early layers' focus on general, shared knowledge across tasks.However, as training progresses, the similarity increases significantly in the 14th and 28th layers.This trend is particularly pronounced in the 28th layer, suggesting that these later layers prioritize task-specific features while still benefiting from shared representations learned earlier.</p>
<p>This observation aligns with the hierarchical nature of PLMs, where earlier layers capture more general features, and deeper layers specialize in task-specific nuances.It also underscores the effectiveness of the Qwen architecture in our framework, as the model's structure allows for a smooth transition from shared knowledge to task-specific representation, effectively capturing heterogeneity in the feature space.The increasing similarity in later layers reflects the ability of the low-rank representations in TA-LoRA to disentangle shared and task-specific knowledge, ensuring both effective crosstask knowledge transfer and specialization for individual tasks.This supports the rationale behind using Qwen in our proposed method and highlights its capability to address the challenges of multi-source task adaptation.</p>
<p>APPENDIX B DATASET DETAILS</p>
<p>This section provides detailed information about the datasets used for training, unseen data evaluation, and unseen task evaluation.The datasets span diverse tasks, domains, and sizes, offering a comprehensive benchmark for assessing the performance of TA-LoRA.</p>
<p>B.1 Training Data</p>
<p>We utilize the training sets of eight source tasks for multitask learning: AFQMC, Amazon, THUCNews, BQ, CMNLI, CMRC-2018, SanWen, and COTE-MFW.These datasets encompass various tasks such as semantic matching, sentiment analysis, reading comprehension, and text classification.The diversity of these tasks enables the model to learn shared representations across tasks while capturing task-specific nuances.Additionally, small-scale datasets are augmented using techniques like up-sampling and data enhancement methods, including synonym substitution and random addition or deletion.For large-scale datasets, down-sampling is applied to achieve balance.</p>
<p>-Task Types: These tasks are designed to evaluate different aspects of language understanding, such as recognizing semantic equivalence (e.g., AFQMC, BQ), opinion mining (e.g., COTE-MFW), and information extraction (e.g., CMRC-2018).</p>
<p>-Domains: The datasets cover a wide range of domains, including financial text, shopping reviews, and general literature, ensuring that the model is exposed to diverse linguistic styles and vocabularies.</p>
<p>-Dataset Statistics: As shown in Table IV, the sizes of the training sets vary significantly, ranging from 11.9K (CMRC-2018) to 4.1M (Amazon).This variability provides an opportunity to test the model's robustness across tasks of different scales.</p>
<p>B.2 Target Tasks of Unseen Data</p>
<p>To evaluate the model's ability to generalize to unseen data within the same tasks, we use the validation sets of the eight source tasks (AFQMC, Amazon, THUCNews, BQ, CMNLI, CMRC-2018, SanWen, and COTE-MFW).These validation sets are excluded from the training process and serve as test sets for the unseen data evaluation strategy.</p>
<p>-Purpose: This evaluation assesses how well the model performs on held-out data from tasks it has already encountered during training.It focuses on the model's ability to generalize without overfitting to the training data.</p>
<p>-Dataset Characteristics: The validation sets are smaller in size compared to the training sets, making them suitable for evaluating the model's precision on limited data.For example, AFQMC's validation set consists of 38K samples, while COTE-MFW includes 37K samples.</p>
<p>B.3 Target Tasks of Unseen Tasks</p>
<p>To assess the model's capability to generalize across tasks, we employ the training sets of eight downstream tasks: ChnSent, TNews, OCNLI, LCQMC, DRCD, C3, COTE-BD, and FinRE.These tasks are intentionally chosen to be distinct from the source tasks, covering new task types and domains.</p>
<p>-Task Types: The unseen tasks include sentiment analysis (e.g., ChnSent), natural language inference (e.g., OCNLI), and machine reading comprehension (e.g., DRCD, C3), which differ from the source tasks in both format and objective.</p>
<p>-Domains: These datasets span financial text (e.g., FinRE, ChnSent), biomedical text (e.g., OCNLI), and general knowledge (e.g., TNews, LCQMC), allowing the evaluation of domain transfer capabilities.</p>
<p>-Dataset Statistics: As shown in Table IV, these datasets vary in size from 8K (COTE-BD) to 250K (LCQMC), providing a challenging benchmark for cross-task generalization.</p>
<p>B.4 Analysis and Implications</p>
<p>The dataset selection in Table IV ensures a balance between task and domain diversity, as well as variability in data size.This design serves several key purposes:</p>
<p>-Diversity in Learning: By exposing the model to tasks from different domains and linguistic structures, we ensure that TA-LoRA learns robust shared representations that generalize well across tasks.</p>
<p>-Scalability Testing: The wide range of dataset sizes allows us to test the scalability of TA-LoRA, ensuring that it performs consistently regardless of the amount of data available for a task.</p>
<p>-Cross-Domain Generalization: The inclusion of tasks from distinct domains (e.g., financial, biomedical, and general text) highlights the model's ability to adapt to unseen contexts, a crucial feature for practical applications.</p>
<p>The comprehensive evaluation using these datasets demonstrates the effectiveness of TA-LoRA in leveraging multitask learning to achieve superior generalization, as detailed in Section IV.</p>
<p>Fig. 1 .
1
Fig. 1.The framework of TA-LoRA serves as a plugin in the final L layers of the PLM."Repr."and "attn."stand for representation and attention, respectively.</p>
<p>Fig. 2 .
2
Fig. 2. Decomposition of the slow weight B and the fast weight A i for T i , with further decomposition of fast weight A i into u i and v i ."lr" stands for the learning rate.</p>
<p>Fig. 3 .
3
Fig. 3.The similarity between base models in different layers obtained by PT on AFQMC.</p>
<p>TABLE II FEW
II
-SHOT PERFORMANCE COMPARISON OF TA-LORA WITH BASELINES ON SETTING OF UNSEEN TASK.
Unseen Taskk-shotMethodChnSentTNewsOCNLILCQMCDRCDC3COTE-BDFinREAvg.PT83.951.960.378.875.434.086.170.067.616MPT90.555.251.776.688.750.392.377.272.8Ours92.761.371.486.785.344.693.178.876.7PT82.354.360.581.776.138.687.671.869.132MPT92.258.554.479.488.255.294.580.375.3Ours94.063.572.986.988.560.393.480.480.0PT84.757.362.883.578.840.689.974.671.564MPT92.659.357.280.289.756.394.681.776.5Ours96.362.773.486.390.461.593.785.781.3</p>
<p>TABLE III ABLATION
III
RESULTS ON F&amp;SW AND ZERO-INITIALIZED ATTN.F&amp;SW INDICATES FAST-SLOW WEIGHTS MECHANISM.
Unseen DataUnseen TaskSettingBQCMNLICMRC-2018SanWenAvg.ChnSentLCQMCDRCDCOTE-BDAvg.Ours78.473.383.190.481.391.780.982.693.287.1Ours w/o F&amp;SW LR78.271.680.089.479.887.284.679.391.485.6Ours w/o F&amp;SW B69.468.379.686.676.089.279.778.590.284.4Ours w/o Zero-initialized attn.74.768.881.388.178.291.680.381.989.085.7</p>
<p>TABLE IV OVERVIEW
IV
OF THE DATASETS UTILIZED FOR TRAINING, UN S E E N DA T A , AND UN S E E N TA S K .EACH DATASET SPANS DIVERSE TASKS AND DOMAINS, SERVING AS A COMPREHENSIVE BENCHMARK FOR ASSESSING THE PERFORMANCE OF TA-LORA.
IDDatasetTask TypeDomainSizeSource/Link1AFQMCSemantic MatchingFinancial38KXu et al. (2020)2AmazonSentiment AnalysisShopping Reviews4.1MGithub3THUCNewsText ClassificationGeneral55KGithub4BQSemantic MatchingFinancial110KChen et al. (2018)5CMNLINatural Language InferenceGeneral404KXu et al. (2020)6CMRC-2018Reading ComprehensionGeneral11.9KXu et al. (2020)7SanWenRelation ExtractionLiterature16KXu et al. (2020)8COTE-MFWOpinion MiningShopping Reviews37KLi et al. (2018)9ChnSentSentiment AnalysisFinancial12KGithub10TNewsText ClassificationGeneral63KGithub11OCNLINatural Language InferenceBiomedical53KHu et al. (2020)12LCQMCQuestion MatchingGeneral250KLiu et al. (2018)13DRCDReading ComprehensionGeneral10KShao et al. (2018)14C3Multi-choice QAGeneral11KSun et al. (2018)15COTE-BDAspect-based Sentiment AnalysisHistory8KLi et al. (2018)16FinRERelation ExtractionFinancial14KLi et al. (2019)
This work was supported in part by National Nature Science Foundation of China (No. 62172036) and National Science and Technology Major Project (2022ZD0116305) 1 https://anonymous.4open.science/r/TA-LoRA-EB27
Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, Neural Information Processing Systems. 2020</p>
<p>Pre-trained language models and their applications. H Wang, J Li, H Wu, E Hovy, Y Sun, 2023Engineering</p>
<p>Enhancing autonomous driving through dual-process learning with behavior and reflection integration. Xiao Zhang, Kangsheng Wang, Tianyu Hu, Huimin Ma, ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2025</p>
<p>Autonomous driving system based on dual process theory and deliberate practice theory. Tianyu Hu, Xiao Zhang, Huimin Ma, PREPRINT (Version 1). mar 2025Research Square</p>
<p>Credes: Causal reasoning enhancement and dual-end searching for solving long-range reasoning problems using llms. Kangsheng Wang, Xiao Zhang, Hao Liu, Songde Han, Huimin Ma, Tianyu Hu, arXiv:2410.016962024arXiv preprint</p>
<p>Introducing openai o1-preview: A new reasoning model. Openai, 2024</p>
<p>Introducing canvas: A new way to write and code with chatgpt. Openai, 2024</p>
<p>The power of scale for parameterefficient prompt tuning. L Brian, A Rami, C Noah, Empirical Methods in Natural Language Processing. 2021</p>
<p>Spot: Better frozen model adaptation through soft prompt transfer. T Vu, B Lester, N Constant, R Al-Rfou, ' , D Cer, 2022Association for Computational Linguistics</p>
<p>Multitask prompt tuning enables parameter-efficient transfer learning. Zhen Wang, Rameswar Panda, Leonid Karlinsky, International Conference on Learning Representations. 2023</p>
<p>Efficient pareto manifold learning with low-rank structure. W Chen, J Kwok, International Conference on Machine Learning. 2024</p>
<p>Universality and limitations of prompt tuning. Y Wang, J Chauhan, W Wang, C Hsieh, Neural Information Processing Systems. 2024</p>
<p>When do prompting and prefixtuning work? a theory of capabilities and limitations. P Aleksandar, T P Hs, B Adel, arXiv:2310.196982023in arXiv preprintunpublished</p>
<p>Towards a unified view of parameter-efficient transfer learning. J He, C Zhou, X Ma, T Berg-Kirkpatrick, G Neubig, International Conference on Learning Representations. 2022</p>
<p>Llamaadapter: Efficient fine-tuning of large language models with zeroinitialized attention. R Zhang, J Han, C Liuand, P Gao, A Zhou, Z Hu, International Conference on Learning Representations. 2024</p>
<p>Multitask pre-training of modular prompt for Chinese few-shot learning. T Sun, Z He, Q Zhu, X Qiu, X Huang, 2023Association for Computational Linguistics</p>
<p>Effective and efficient few-shot fine-tuning for vision transformers. J Yang, H Wu, J Zhang, L Gao, J Song, IEEE International Conference on Multimedia and Expo. 2024</p>
<p>Bitfit: Simple parameterefficient fine-tuning for transformer-based masked language-models. B Z Elad, G Yoav, R Shauli, 2022Association for Computational Linguistics</p>
<p>Lst: ladder side-tuning for parameter and memory efficient transfer learning. Y Sung, J Cho, M Bansal, Neural Information Processing Systems. 2024</p>
<p>Parameter-efficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, Q D Laroussilhe, A Gesmundo, International Conference on Machine Learning. 2019</p>
<p>Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks. K M Rabeeh, R Sebastian, D Mostafa, H James, 2021Association for Computational Linguistics</p>
<p>Adapterfusion: Non-destructive task composition for transfer learning. P Jonas, K Aishwarya, Andreas R , C Kyunghyun, G Iryna, 2020Association for Computational Linguistics</p>
<p>P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. X Liu, K Ji, Y Fu, W Tam, Z Du, Z Yang, J Tang, 2022Association for Computational Linguistics</p>
<p>Bartender: A simple baseline model for task-level heterogeneous federated learning. Y Yang, Y Lu, S Huang, S Sirejiding, C Liu, M Yi, Z Xie, Y Ding, H Lu, IEEE International Conference on Multimedia and Expo. 2024</p>
<p>Enhancing multimodal sentiment analysis via learning from large language model. N Pang, W Wu, Y Hu, K Xu, Q Yin, L Qin, IEEE International Conference on Multimedia and Expo. 2024</p>
<p>Attempt: Parameterefficient multi-task tuning via attentional mixtures of soft prompts. A Asai, M Salehi, M Peters, H Hajishirzi, Empirical Methods in Natural Language Processing. 2022</p>
<p>A Yang, B Yang, B Hui, B Zheng, B Yu, C Zhou, arXiv:2412.15115Qwen2 technical report. 2024arXiv preprintunpublished</p>
<p>Omni-granularity embedding network for text-to-image person retrieval. C Wang, Z Luo, S Li, IEEE International Conference on Multimedia and Expo. 2024</p>
<p>Csce: Boosting llm reasoning by simultaneous enhancing of causal significance and consistency. Kangsheng Wang, Xiao Zhang, Juntao Lyu, Tianyu Hu, Huimin Ma, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
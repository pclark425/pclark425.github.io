<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2313 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2313</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2313</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-234789992</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2105.09506v1.pdf" target="_blank">Physics-informed neural networks (PINNs) for fluid mechanics: A review</a></p>
                <p><strong>Paper Abstract:</strong> Despite the significant progress over the last 50 years in simulating flow problems using numerical discretization of the Navier-Stokes equations (NSE), we still cannot incorporate seamlessly noisy data into existing algorithms, mesh-generation is complex, and we cannot tackle high-dimensional problems governed by parametrized NSE. Moreover, solving inverse flow problems is often prohibitively expensive and requires complex and expensive formulations and new computer codes. Here, we review flow physics-informed learning, integrating seamlessly data and mathematical models, and implementing them using physics-informed neural networks (PINNs). We demonstrate the effectiveness of PINNs for inverse problems related to three-dimensional wake flows, supersonic flows, and biomedical flows.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2313.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2313.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PINNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural networks trained with loss terms that penalize PDE residuals (via automatic differentiation) and data/BC/IC mismatch, enabling simultaneous solution and parameter inference for PDE-governed physical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Fluid mechanics (incompressible/compressible flows) and biomedical fluid-structure interaction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Forward and inverse problems governed by partial differential equations (Navier–Stokes, Euler, Cahn–Hilliard, etc.), including reconstruction of high-dimensional spatio-temporal flow fields from sparse/partial observations and inference of material/PDE parameters (e.g., permeability of thrombus).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically limited/scattered observational data (e.g., a few 2D2C velocity planes, Schlieren density-gradient fields, sparse pressure sensors, phase-field snapshots); supplemented by many randomly sampled PDE residual points (e.g., N_f up to 3×10^6) — mixed availability: sparse labeled measurements plus unlimited coordinate samples for PDE residuals.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Multimodal spatio-temporal continuous fields: pointwise sensor measurements, 2D planar vector fields (2D2C), image-like density-gradient data (Schlieren), time series (snapshots), phase-field scalar fields; high-dimensional gridded fields used for evaluation but training uses scattered points.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: nonlinear PDEs (Navier–Stokes/Euler/Cahn–Hilliard), multi-physics coupling, 3D unsteady flows, parameter estimation (inverse problems), high-dimensional input (space+time); optimization over large nonconvex parameter space of deep networks; examples use networks up to 8–9 layers, hundreds of neurons, and millions of residual samples.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature computational fluid dynamics (CFD) and continuum models exist, but inverse and data-assimilation scenarios remain challenging; there is strong prior knowledge (PDEs) but practical experimental data are often sparse/noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - models explicitly enforce mechanistic constraints (PDE residuals, conservation laws) and are used to obtain physically interpretable fields and parameters (e.g., pressure, permeability).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-informed neural networks (PINNs) — fully-connected feedforward networks with automatic differentiation</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A fully-connected feedforward NN maps space-time coordinates (x,t) to field variables (e.g., u,v,w,p,φ). Automatic differentiation computes derivatives to form PDE residuals which are added to the loss alongside data/BC/IC losses. Training uses gradient-based optimizers (ADAM) with mini-batching, weighted loss terms, techniques such as sinusoidal or tanh activations, dynamic loss weighting, adaptive activations; network sizes and training schedules vary by problem (examples: 8 hidden layers × 200 neurons, sin activation, ADAM with staged learning rates; or 9 layers × 20 neurons in biomedical case). PDE parameters (λ) can be learned jointly with network weights by minimizing total loss.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / hybrid physics-ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for inverse problems and data-constrained scenarios where integrating PDE structure with sparse observations is crucial. Less appropriate for standard forward problems where high-order CFD is more accurate and efficient; scalability and optimization can limit applicability without domain decomposition or parallelization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>3D incompressible reconstruction: streamwise velocity error often <2% in some setups (Case 3 streamwise velocity mostly <2%); thrombus permeability inference: inferred parameters a=7.10 vs true 6.90, b=0.0003 vs 0.0, yielding κ(core)=0.0011 vs 0.001 and κ(shell)=1.0003 vs 1.0; compressible bow-shock case: qualitative good agreement with CFD (no numeric error reported).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>PINNs accurately reconstruct hidden velocity/pressure fields from sparse multi-plane data and infer material parameters from phase-field data; they integrate multimodal information naturally and handle ill-posed problems that are difficult for traditional CFD. Limitations include optimization challenges (nonconvex loss), slower convergence/accuracy compared to high-order CFD for pure forward problems, sensitivity near temporal boundaries when initial data missing, and larger relative errors for small-magnitude components.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for experimental diagnostics, inverse design, and scenarios with sparse/noisy measurements: enables reconstructing full fields from partial observations, noninvasive parameter inference (e.g., thrombus properties), and replacing expensive data assimilation pipelines. Potential for industrial-scale applications with multi-GPU parallelization and domain-decomposition variants (XPINN/SimNet).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to classical CFD: PINNs are less accurate/efficient for canonical forward problems but outperform CFD for inverse/data-constrained problems because they natively integrate data and PDE constraints and avoid mesh generation. No direct numerical performance benchmarks vs CFD reported beyond qualitative agreement and selected error measures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit mechanistic constraints via PDE residuals and automatic differentiation; availability of even sparse but informative measurements (e.g., 2D2C planes, Schlieren); careful loss term weighting, network architecture choices, training strategies (adaptive activations, dynamic weights), and sufficient PDE residual sampling; domain decomposition and parallel implementations improve scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Enforcing governing PDEs as soft constraints in neural-network training allows accurate reconstruction and parameter inference from sparse, multimodal observations (where traditional CFD struggles), but practical success depends on optimization strategies, informative data placement, and scalable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2313.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-informed GPs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Gaussian Processes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gaussian process regression methods that incorporate differential operator structure or PDE constraints into the kernel/prior to solve or infer differential equations from data with probabilistic uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning of linear differential equations using Gaussian processes.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>PDE learning and data-driven solution of differential equations (general scientific computing)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learning solutions or parameters of linear differential equations by combining limited observations with prior covariance structure that respects differential operators.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed for small-data regimes (limited labeled observations) where GPs excel; the paper references their use for integrating multifidelity data but gives no per-case data quantities here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Continuous functional data / pointwise observations over space and/or time; typically low-dimensional inputs compared to deep networks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate for linear PDEs; complexity increases with dimensionality and nonlinearity (GPs scale poorly with large datasets), so suitability is mainly for small to moderate problem sizes and linear operators.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established statistical methodology (GPs) with growing extensions to PDEs and physics-informed versions; prior work by the authors established usage.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — approach encodes mechanistic (differential) knowledge in priors and provides interpretable probabilistic estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gaussian process regression with physics-informed priors</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Construct kernels or priors informed by differential operators so that GP predictions satisfy (in expectation) linear differential constraints; enables closed-form posterior predictive distributions and analytic derivative evaluation for linear operators.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / probabilistic ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for small-data linear PDE problems and for providing uncertainty quantification; less suitable for very high-dimensional nonlinear PDEs or large datasets due to computational scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Highlighted as an earlier physics-informed learning approach complementary to PINNs; useful for uncertainty-aware inference in low-data regimes, but not emphasized for large-scale nonlinear fluid problems.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful for uncertainty-aware small-data inference and as a complementary approach to PINNs where probabilistic outputs and small dataset performance matter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Implied complementarity with PINNs: GPs advantageous for small data and explicit UQ, while PINNs scale better to high-dimensional nonlinear PDEs (but lack built-in probabilistic uncertainties).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strengths of GP priors for small data and analytic derivative availability; limitations include cubic scaling with data and difficulty handling strong nonlinearity/high dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Physics-informed Gaussian processes provide a probabilistic, small-data-friendly route to incorporate mechanistic PDE structure, complementary to PINNs which scale better to high-dimensional nonlinear problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2313.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XPINN / domain decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended Physics-Informed Neural Networks (XPINNs) / domain-decomposition PINNs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-decomposition extension of PINNs that splits the computational domain into subdomains with separate neural networks (or variational PINNs) to improve scalability, parallelism, and handle multiscale/multiphysics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extended Physics-Informed Neural Networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Large-scale PDEs in fluid mechanics and multiphysics/multiscale problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Scaling PINNs to industrial-scale, multiscale, or multiphysics PDE problems via domain decomposition to parallelize training and relax optimizer/pathology issues.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not specified for specific cases; intended to operate with the same mixed sparse measurement + residual sampling PINN paradigm but enabling more residual samples distributed across subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal continuous fields partitioned into subdomains; supports heterogeneous data densities across subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Targets very high complexity: large spatial domains, multiscale behavior, and high-dimensional parameter spaces; reduces per-network complexity by decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging within PINN literature with active development; implementations and theoretical results are being produced.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — maintains enforcement of PDEs locally and enforces interface continuity/consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Domain-decomposed physics-informed neural networks (XPINN/hp-VPINN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Decompose domain into subdomains, train separate neural networks per subdomain with local PDE residuals and interface conditions; enables parallel training, reduces stiffness in optimization, and allows heterogenous architectures per region.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / hybrid parallel architectures</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Proposed as appropriate and necessary for scaling PINNs to industrial/computationally large problems and multiscale physics; paper cites XPINN and hp-VPINN as solutions to scalability/accuracy issues.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Described as a key recent advancement addressing PINN optimization/pathology and enabling parallel implementations; specific empirical results not presented in this review's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — essential for enabling PINN use on large industrial CFD problems by allowing multi-GPU parallelism and targeted local modeling of complex features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to vanilla PINNs: XPINN aims to mitigate training pathologies, improve accuracy, and achieve parallel speedup; concrete quantitative comparisons left to cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Domain decomposition reducing optimization difficulty per subnetwork, parallel hardware utilization, targeted architecture per physics region, and enforcement of interface conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Domain decomposition (XPINN/hp-VPINN) is a promising strategy to overcome scalability and optimization challenges of PINNs for large, multiscale PDE problems by splitting complexity across parallel subnetworks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2313.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>B-PINNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Physics-Informed Neural Networks (B-PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bayesian extensions of PINNs that provide probabilistic posterior distributions over solutions/parameters to quantify uncertainty when solving forward and inverse PDE problems with noisy data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Uncertainty quantification in PDE-constrained inference and reconstruction (fluid flows, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Inferring PDE solutions and parameters while quantifying epistemic and aleatoric uncertainty arising from limited/noisy observations and modeling errors.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed to handle noisy and sparse datasets; specifics depend on the application but paper references noisy-data scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Pointwise sensor data and PDE residual samples; probabilistic treatment requires modeling likelihoods/noise.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Adds statistical inference layer on top of PINNs; computationally heavier due to posterior approximation (e.g., variational inference or MCMC) but provides UQ.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging; Bayesian approaches to PINNs recently proposed and under active research.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — retains PDE constraints while requiring probabilistic interpretability of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Bayesian physics-informed neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Introduce Bayesian treatment over network weights and/or PDE parameters to obtain posterior distributions; incorporate PDE residuals in likelihood or prior terms; inference via variational Bayes or sampling approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / probabilistic Bayesian methods</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited where uncertainty quantification is important (noisy sensors, safety-critical predictions); computational cost and scalability remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Not experimentally demonstrated in this review's case studies, but cited as an approach to quantify uncertainty in PINN-inferred solutions and parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Important for reliable deployment in experimental/clinical settings where uncertainty estimates guide decisions (e.g., biomedical parameter inference).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers probabilistic UQ compared to deterministic PINNs; trade-off is increased computational and implementation complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Correct probabilistic modeling of noise and priors, efficient approximate inference methods, and leveraging PDE constraints to reduce posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Bayesian PINNs extend the deterministic PINN framework to provide principled uncertainty quantification for PDE-constrained inference, addressing noisy/sparse-data scenarios at increased computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2313.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSFnets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NSFnets (Navier–Stokes flow nets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of PINN architectures specialized for incompressible Navier–Stokes equations, used to learn velocity and pressure fields from limited measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Incompressible fluid flow simulation and inference</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learning velocity and pressure fields that satisfy Navier–Stokes equations from observations (e.g., PIV data, flow visualizations).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically sparse flow measurements (e.g., 2D2C planes or concentration fields); the PINN framework augments with PDE residual points.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal flow fields (velocity components, pressure); may include image-derived measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Nonlinear incompressible Navier–Stokes PDEs; 2D/3D unsteady flows with vortex dynamics; high-dimensional function approximation required.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Built on growing literature of PINNs for Navier–Stokes; actively used in research for flow reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — enforces incompressibility and momentum conservation explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>NSFnets (specialized PINN for Navier–Stokes)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>PINN tailored to Navier–Stokes physics: network outputs velocity and pressure, loss includes momentum and divergence residuals, uses AD for derivatives; may include measured velocities as data loss.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to flow reconstruction and parameter inference in incompressible flows where partial observations exist; demonstrated in literature for cylinder wake and other canonical flows.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as a relevant methodology for incompressible flow inference and closely related to the 3D incompressible case studies in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables noninvasive recovery of pressure and velocity fields from limited experiments, aiding diagnostics and design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually against classical CFD which requires full boundary/initial conditions and meshes; NSFnets avoid mesh generation and assimilate sparse data directly.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong prior (NS equations), quality and placement of observed measurements, and training strategies to handle PDE stiffness and optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Specialized PINN variants (NSFnets) effectively exploit Navier–Stokes structure to reconstruct hidden flow quantities from sparse measurements, bridging experiments and simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2313.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NVIDIA SimNet (AI-accelerated multiphysics simulation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance library implementing PINN-like approaches for multiphysics PDE simulation with GPU acceleration and parallelization features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NVIDIA SimNetˆ{TM}: an AI-accelerated multiphysics simulation framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Large-scale multiphysics PDE simulation (e.g., fluid mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Efficiently implement PINN training and inference at scale using optimized GPU-accelerated infrastructure and algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Supports the same mixed sparse observational + residual-sampling PINN paradigm; designed for large datasets and many residual samples leveraging GPU compute.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Arbitrary PDE-oriented spatio-temporal datasets and residual sampling; supports heterogeneous data modalities for multiphysics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Targets industrial-scale PDE problems requiring large compute and parallelism; handles large numbers of residual/data points.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Commercial/industrial-grade implementation built on PINN concepts; emerging as a practical tool.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — enforces PDEs similarly to PINNs and provides infrastructure to scale enforcement across GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>GPU-accelerated PINN framework (SimNet)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Software stack that implements PINN training with domain decomposition, efficient automatic differentiation, parallelization across GPUs, and performance optimizations for multiphysics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / scalable ML infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Intended for large-scale engineering and industrial problems where PINN methodologies need to be scaled to many residual samples and multi-GPU hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as an efficient implementation enabling PINN application at scale; specific performance numbers not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables practical deployment of PINN-based approaches for industrial complexity simulations by leveraging hardware acceleration and parallelism.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides hardware-accelerated PINN capabilities compared to ad-hoc single-GPU/CPU PINN implementations; aims to close gap toward industrial applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Efficient AD implementation, parallelization, domain-decomposition compatibility, and engineering optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>High-performance frameworks like SimNet are critical for scaling physics-informed ML to industrially relevant PDE problems by exploiting GPU parallelism and software optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2313.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity & composite networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-fidelity composite neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural architectures that combine data of different fidelities (e.g., low-accuracy/cheap simulations and high-accuracy/expensive measurements) to improve approximation and inverse PDE inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Surrogate modeling and inverse PDE problems across computational physics and engineering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Leverage cheap low-fidelity data and scarce high-fidelity measurements jointly to learn accurate models and infer PDE parameters efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Explicitly designed for situations with mixed data abundance: abundant low-fidelity labelled data and scarce high-fidelity labels.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured function evaluations from simulations and pointwise high-fidelity observations; multimodal fidelity levels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: combining heterogeneous data sources and learning cross-fidelity mappings, potentially for PDE-constrained inverse problems.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging within physics-informed ML; active area of research.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-high — can combine physical constraints with fidelity models and is useful when some mechanistic modeling exists.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Composite multi-fidelity neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Architectures composed of sub-networks that learn from different fidelity datasets with coupling layers or residual connections to transfer information; can be integrated with PDE residual losses.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Hybrid / supervised + physics-informed</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate where hierarchical data sources are available (cheap sim + expensive experiments); improves parameter inference and reduces need for many high-fidelity labels.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as a methodology that improves PINN performance with multi-fidelity data; specifics not demonstrated in this review's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for reducing experimental costs and improving surrogate accuracy by leveraging abundant low-fidelity data combined with scarce high-fidelity observations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers better sample efficiency than single-fidelity learning; trade-offs include architectural complexity and potential bias transfer from low-fidelity data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality and correlation between fidelity levels, architecture design for information transfer, and appropriate weighting between fidelity losses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Multi-fidelity composite networks can significantly increase data efficiency for PDE inference by exploiting abundant low-fidelity simulations alongside scarce high-fidelity measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2313.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive training heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive activations and dynamic loss weighting (training techniques for PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Techniques like adaptive activation functions and dynamically adjusted loss weights to accelerate convergence and mitigate optimization pathologies in PINN training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaptive activation functions accelerate convergence in deep and physics-informed neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Training deep neural networks for PDE-constrained scientific computing (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve convergence speed/stability and balance competing loss terms (PDE residual vs data/BC/IC) during PINN optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applies regardless of data amount; aims to improve training behavior with the available sparse/heterogeneous data common in PINN applications.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Applies to network training on spatio-temporal coordinate-based datasets and residual samples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Targets optimization complexity (ill-conditioning, gradient pathologies) rather than problem physics complexity per se.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent empirical advances with evidence of improved convergence in PINN literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — methods are optimizer/architecture-level heuristics not altering mechanistic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Adaptive activations & dynamic loss weighting</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Adaptive activations change activation scaling/parameters during training (layer-wise adaptation); dynamic loss weighting adjusts weights of PDE/data/BC losses during optimization to balance gradients and avoid dominated loss terms.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Training/optimization techniques for physics-informed ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Widely applicable across PINN problems to mitigate slow convergence and loss imbalance; used in the review's compressible-flow example and others cited.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to accelerate convergence and improve solution quality in cited literature; used as part of successful PINN implementations in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Crucial for practical training of PINNs, improving robustness and enabling solutions that would otherwise be hampered by optimization issues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Complementary to other optimization strategies; dynamic weighting addresses multi-term loss imbalance better than fixed heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate calibration of adaptation schemes, monitoring of loss components, and integration with optimizers like ADAM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Training heuristics such as adaptive activations and dynamic loss weighting are essential practical tools to overcome PINN optimization pathologies and realize their potential on complex PDE problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2313.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2313.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement learning (flow control)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning for active flow control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of reinforcement learning agents to learn control policies for active manipulation of fluid flows (e.g., bluff-body flow control) in simulations and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforcement learning for bluff body active flow control in experiments and simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Active flow control in fluid mechanics (aerodynamics/hydrodynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn control strategies (actuation policies) to modify flow behavior (e.g., reduce drag, suppress vortex shedding) from interactions with simulation or experimental environments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires interaction data from simulations or experiments; quantity depends on learning algorithm (can be large for model-free RL); not detailed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series of state observations and reward signals; may involve high-dimensional sensor/image observations or low-dimensional flow descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: delayed rewards, partial observability, continuous action/state spaces, and real-time control constraints; sample complexity can be large.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Active research area with successful demonstrations in both simulations and experiments but still emerging for robust industrial deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — interpretable control policies sometimes desirable, but black-box policies can be acceptable if performance validated experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Reinforcement learning (policy learning for flow actuation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>RL agents interact with flow simulations or experiments, receive observations and rewards, and update policies (e.g., deep RL) to optimize control objectives; can be trained in simulation and transferred to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and demonstrated for active control of flows (cited work); complementary to PINNs which focus on inference and surrogate modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as an avenue for active flow control and as a different AI paradigm from PINNs; specific outcomes are reported in the cited work but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant for experimental and industrial flow control tasks, potentially reducing energy consumption or improving performance via learned control laws.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to model-based control, RL can discover novel control strategies without explicit models but may require more data and careful transfer to real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality simulation environments, reward shaping, transfer learning strategies, and sample-efficient RL algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Reinforcement learning offers a complementary AI approach for learning control policies in fluid mechanics and can be combined with physics-aware models for better sample efficiency and transfer to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-informed neural networks (PINNs) for fluid mechanics: A review', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. <em>(Rating: 2)</em></li>
                <li>Machine learning of linear differential equations using Gaussian processes. <em>(Rating: 2)</em></li>
                <li>Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. <em>(Rating: 2)</em></li>
                <li>Physics-informed neural networks for high-speed flows. <em>(Rating: 2)</em></li>
                <li>Noninvasive inference of thrombus material properties with physics-informed neural networks. <em>(Rating: 2)</em></li>
                <li>Extended Physics-Informed Neural Networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. <em>(Rating: 2)</em></li>
                <li>B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data. <em>(Rating: 2)</em></li>
                <li>NVIDIA SimNetˆ{TM}: an AI-accelerated multiphysics simulation framework. <em>(Rating: 2)</em></li>
                <li>NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations. <em>(Rating: 2)</em></li>
                <li>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2313",
    "paper_id": "paper-234789992",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "PINNs",
            "name_full": "Physics-Informed Neural Networks",
            "brief_description": "Neural networks trained with loss terms that penalize PDE residuals (via automatic differentiation) and data/BC/IC mismatch, enabling simultaneous solution and parameter inference for PDE-governed physical systems.",
            "citation_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Fluid mechanics (incompressible/compressible flows) and biomedical fluid-structure interaction",
            "problem_description": "Forward and inverse problems governed by partial differential equations (Navier–Stokes, Euler, Cahn–Hilliard, etc.), including reconstruction of high-dimensional spatio-temporal flow fields from sparse/partial observations and inference of material/PDE parameters (e.g., permeability of thrombus).",
            "data_availability": "Typically limited/scattered observational data (e.g., a few 2D2C velocity planes, Schlieren density-gradient fields, sparse pressure sensors, phase-field snapshots); supplemented by many randomly sampled PDE residual points (e.g., N_f up to 3×10^6) — mixed availability: sparse labeled measurements plus unlimited coordinate samples for PDE residuals.",
            "data_structure": "Multimodal spatio-temporal continuous fields: pointwise sensor measurements, 2D planar vector fields (2D2C), image-like density-gradient data (Schlieren), time series (snapshots), phase-field scalar fields; high-dimensional gridded fields used for evaluation but training uses scattered points.",
            "problem_complexity": "High: nonlinear PDEs (Navier–Stokes/Euler/Cahn–Hilliard), multi-physics coupling, 3D unsteady flows, parameter estimation (inverse problems), high-dimensional input (space+time); optimization over large nonconvex parameter space of deep networks; examples use networks up to 8–9 layers, hundreds of neurons, and millions of residual samples.",
            "domain_maturity": "Mature computational fluid dynamics (CFD) and continuum models exist, but inverse and data-assimilation scenarios remain challenging; there is strong prior knowledge (PDEs) but practical experimental data are often sparse/noisy.",
            "mechanistic_understanding_requirements": "High - models explicitly enforce mechanistic constraints (PDE residuals, conservation laws) and are used to obtain physically interpretable fields and parameters (e.g., pressure, permeability).",
            "ai_methodology_name": "Physics-informed neural networks (PINNs) — fully-connected feedforward networks with automatic differentiation",
            "ai_methodology_description": "A fully-connected feedforward NN maps space-time coordinates (x,t) to field variables (e.g., u,v,w,p,φ). Automatic differentiation computes derivatives to form PDE residuals which are added to the loss alongside data/BC/IC losses. Training uses gradient-based optimizers (ADAM) with mini-batching, weighted loss terms, techniques such as sinusoidal or tanh activations, dynamic loss weighting, adaptive activations; network sizes and training schedules vary by problem (examples: 8 hidden layers × 200 neurons, sin activation, ADAM with staged learning rates; or 9 layers × 20 neurons in biomedical case). PDE parameters (λ) can be learned jointly with network weights by minimizing total loss.",
            "ai_methodology_category": "Physics-informed ML / hybrid physics-ML",
            "applicability": "Well-suited for inverse problems and data-constrained scenarios where integrating PDE structure with sparse observations is crucial. Less appropriate for standard forward problems where high-order CFD is more accurate and efficient; scalability and optimization can limit applicability without domain decomposition or parallelization.",
            "effectiveness_quantitative": "3D incompressible reconstruction: streamwise velocity error often &lt;2% in some setups (Case 3 streamwise velocity mostly &lt;2%); thrombus permeability inference: inferred parameters a=7.10 vs true 6.90, b=0.0003 vs 0.0, yielding κ(core)=0.0011 vs 0.001 and κ(shell)=1.0003 vs 1.0; compressible bow-shock case: qualitative good agreement with CFD (no numeric error reported).",
            "effectiveness_qualitative": "PINNs accurately reconstruct hidden velocity/pressure fields from sparse multi-plane data and infer material parameters from phase-field data; they integrate multimodal information naturally and handle ill-posed problems that are difficult for traditional CFD. Limitations include optimization challenges (nonconvex loss), slower convergence/accuracy compared to high-order CFD for pure forward problems, sensitivity near temporal boundaries when initial data missing, and larger relative errors for small-magnitude components.",
            "impact_potential": "High for experimental diagnostics, inverse design, and scenarios with sparse/noisy measurements: enables reconstructing full fields from partial observations, noninvasive parameter inference (e.g., thrombus properties), and replacing expensive data assimilation pipelines. Potential for industrial-scale applications with multi-GPU parallelization and domain-decomposition variants (XPINN/SimNet).",
            "comparison_to_alternatives": "Compared to classical CFD: PINNs are less accurate/efficient for canonical forward problems but outperform CFD for inverse/data-constrained problems because they natively integrate data and PDE constraints and avoid mesh generation. No direct numerical performance benchmarks vs CFD reported beyond qualitative agreement and selected error measures.",
            "success_factors": "Explicit mechanistic constraints via PDE residuals and automatic differentiation; availability of even sparse but informative measurements (e.g., 2D2C planes, Schlieren); careful loss term weighting, network architecture choices, training strategies (adaptive activations, dynamic weights), and sufficient PDE residual sampling; domain decomposition and parallel implementations improve scalability.",
            "key_insight": "Enforcing governing PDEs as soft constraints in neural-network training allows accurate reconstruction and parameter inference from sparse, multimodal observations (where traditional CFD struggles), but practical success depends on optimization strategies, informative data placement, and scalable implementations.",
            "uuid": "e2313.0",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Physics-informed GPs",
            "name_full": "Physics-Informed Gaussian Processes",
            "brief_description": "Gaussian process regression methods that incorporate differential operator structure or PDE constraints into the kernel/prior to solve or infer differential equations from data with probabilistic uncertainty quantification.",
            "citation_title": "Machine learning of linear differential equations using Gaussian processes.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "PDE learning and data-driven solution of differential equations (general scientific computing)",
            "problem_description": "Learning solutions or parameters of linear differential equations by combining limited observations with prior covariance structure that respects differential operators.",
            "data_availability": "Designed for small-data regimes (limited labeled observations) where GPs excel; the paper references their use for integrating multifidelity data but gives no per-case data quantities here.",
            "data_structure": "Continuous functional data / pointwise observations over space and/or time; typically low-dimensional inputs compared to deep networks.",
            "problem_complexity": "Moderate for linear PDEs; complexity increases with dimensionality and nonlinearity (GPs scale poorly with large datasets), so suitability is mainly for small to moderate problem sizes and linear operators.",
            "domain_maturity": "Established statistical methodology (GPs) with growing extensions to PDEs and physics-informed versions; prior work by the authors established usage.",
            "mechanistic_understanding_requirements": "High — approach encodes mechanistic (differential) knowledge in priors and provides interpretable probabilistic estimates.",
            "ai_methodology_name": "Gaussian process regression with physics-informed priors",
            "ai_methodology_description": "Construct kernels or priors informed by differential operators so that GP predictions satisfy (in expectation) linear differential constraints; enables closed-form posterior predictive distributions and analytic derivative evaluation for linear operators.",
            "ai_methodology_category": "Physics-informed ML / probabilistic ML",
            "applicability": "Appropriate for small-data linear PDE problems and for providing uncertainty quantification; less suitable for very high-dimensional nonlinear PDEs or large datasets due to computational scaling.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Highlighted as an earlier physics-informed learning approach complementary to PINNs; useful for uncertainty-aware inference in low-data regimes, but not emphasized for large-scale nonlinear fluid problems.",
            "impact_potential": "Useful for uncertainty-aware small-data inference and as a complementary approach to PINNs where probabilistic outputs and small dataset performance matter.",
            "comparison_to_alternatives": "Implied complementarity with PINNs: GPs advantageous for small data and explicit UQ, while PINNs scale better to high-dimensional nonlinear PDEs (but lack built-in probabilistic uncertainties).",
            "success_factors": "Strengths of GP priors for small data and analytic derivative availability; limitations include cubic scaling with data and difficulty handling strong nonlinearity/high dimensions.",
            "key_insight": "Physics-informed Gaussian processes provide a probabilistic, small-data-friendly route to incorporate mechanistic PDE structure, complementary to PINNs which scale better to high-dimensional nonlinear problems.",
            "uuid": "e2313.1",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "XPINN / domain decomposition",
            "name_full": "Extended Physics-Informed Neural Networks (XPINNs) / domain-decomposition PINNs",
            "brief_description": "A domain-decomposition extension of PINNs that splits the computational domain into subdomains with separate neural networks (or variational PINNs) to improve scalability, parallelism, and handle multiscale/multiphysics problems.",
            "citation_title": "Extended Physics-Informed Neural Networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Large-scale PDEs in fluid mechanics and multiphysics/multiscale problems",
            "problem_description": "Scaling PINNs to industrial-scale, multiscale, or multiphysics PDE problems via domain decomposition to parallelize training and relax optimizer/pathology issues.",
            "data_availability": "Not specified for specific cases; intended to operate with the same mixed sparse measurement + residual sampling PINN paradigm but enabling more residual samples distributed across subdomains.",
            "data_structure": "Spatio-temporal continuous fields partitioned into subdomains; supports heterogeneous data densities across subdomains.",
            "problem_complexity": "Targets very high complexity: large spatial domains, multiscale behavior, and high-dimensional parameter spaces; reduces per-network complexity by decomposition.",
            "domain_maturity": "Emerging within PINN literature with active development; implementations and theoretical results are being produced.",
            "mechanistic_understanding_requirements": "High — maintains enforcement of PDEs locally and enforces interface continuity/consistency.",
            "ai_methodology_name": "Domain-decomposed physics-informed neural networks (XPINN/hp-VPINN)",
            "ai_methodology_description": "Decompose domain into subdomains, train separate neural networks per subdomain with local PDE residuals and interface conditions; enables parallel training, reduces stiffness in optimization, and allows heterogenous architectures per region.",
            "ai_methodology_category": "Physics-informed ML / hybrid parallel architectures",
            "applicability": "Proposed as appropriate and necessary for scaling PINNs to industrial/computationally large problems and multiscale physics; paper cites XPINN and hp-VPINN as solutions to scalability/accuracy issues.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Described as a key recent advancement addressing PINN optimization/pathology and enabling parallel implementations; specific empirical results not presented in this review's experiments.",
            "impact_potential": "High — essential for enabling PINN use on large industrial CFD problems by allowing multi-GPU parallelism and targeted local modeling of complex features.",
            "comparison_to_alternatives": "Compared conceptually to vanilla PINNs: XPINN aims to mitigate training pathologies, improve accuracy, and achieve parallel speedup; concrete quantitative comparisons left to cited works.",
            "success_factors": "Domain decomposition reducing optimization difficulty per subnetwork, parallel hardware utilization, targeted architecture per physics region, and enforcement of interface conditions.",
            "key_insight": "Domain decomposition (XPINN/hp-VPINN) is a promising strategy to overcome scalability and optimization challenges of PINNs for large, multiscale PDE problems by splitting complexity across parallel subnetworks.",
            "uuid": "e2313.2",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "B-PINNs",
            "name_full": "Bayesian Physics-Informed Neural Networks (B-PINNs)",
            "brief_description": "Bayesian extensions of PINNs that provide probabilistic posterior distributions over solutions/parameters to quantify uncertainty when solving forward and inverse PDE problems with noisy data.",
            "citation_title": "B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Uncertainty quantification in PDE-constrained inference and reconstruction (fluid flows, etc.)",
            "problem_description": "Inferring PDE solutions and parameters while quantifying epistemic and aleatoric uncertainty arising from limited/noisy observations and modeling errors.",
            "data_availability": "Designed to handle noisy and sparse datasets; specifics depend on the application but paper references noisy-data scenarios.",
            "data_structure": "Pointwise sensor data and PDE residual samples; probabilistic treatment requires modeling likelihoods/noise.",
            "problem_complexity": "Adds statistical inference layer on top of PINNs; computationally heavier due to posterior approximation (e.g., variational inference or MCMC) but provides UQ.",
            "domain_maturity": "Emerging; Bayesian approaches to PINNs recently proposed and under active research.",
            "mechanistic_understanding_requirements": "High — retains PDE constraints while requiring probabilistic interpretability of outputs.",
            "ai_methodology_name": "Bayesian physics-informed neural networks",
            "ai_methodology_description": "Introduce Bayesian treatment over network weights and/or PDE parameters to obtain posterior distributions; incorporate PDE residuals in likelihood or prior terms; inference via variational Bayes or sampling approximations.",
            "ai_methodology_category": "Physics-informed ML / probabilistic Bayesian methods",
            "applicability": "Well-suited where uncertainty quantification is important (noisy sensors, safety-critical predictions); computational cost and scalability remain challenges.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Not experimentally demonstrated in this review's case studies, but cited as an approach to quantify uncertainty in PINN-inferred solutions and parameters.",
            "impact_potential": "Important for reliable deployment in experimental/clinical settings where uncertainty estimates guide decisions (e.g., biomedical parameter inference).",
            "comparison_to_alternatives": "Offers probabilistic UQ compared to deterministic PINNs; trade-off is increased computational and implementation complexity.",
            "success_factors": "Correct probabilistic modeling of noise and priors, efficient approximate inference methods, and leveraging PDE constraints to reduce posterior uncertainty.",
            "key_insight": "Bayesian PINNs extend the deterministic PINN framework to provide principled uncertainty quantification for PDE-constrained inference, addressing noisy/sparse-data scenarios at increased computational cost.",
            "uuid": "e2313.3",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "NSFnets",
            "name_full": "NSFnets (Navier–Stokes flow nets)",
            "brief_description": "A class of PINN architectures specialized for incompressible Navier–Stokes equations, used to learn velocity and pressure fields from limited measurements.",
            "citation_title": "NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Incompressible fluid flow simulation and inference",
            "problem_description": "Learning velocity and pressure fields that satisfy Navier–Stokes equations from observations (e.g., PIV data, flow visualizations).",
            "data_availability": "Typically sparse flow measurements (e.g., 2D2C planes or concentration fields); the PINN framework augments with PDE residual points.",
            "data_structure": "Spatio-temporal flow fields (velocity components, pressure); may include image-derived measurements.",
            "problem_complexity": "Nonlinear incompressible Navier–Stokes PDEs; 2D/3D unsteady flows with vortex dynamics; high-dimensional function approximation required.",
            "domain_maturity": "Built on growing literature of PINNs for Navier–Stokes; actively used in research for flow reconstruction.",
            "mechanistic_understanding_requirements": "High — enforces incompressibility and momentum conservation explicitly.",
            "ai_methodology_name": "NSFnets (specialized PINN for Navier–Stokes)",
            "ai_methodology_description": "PINN tailored to Navier–Stokes physics: network outputs velocity and pressure, loss includes momentum and divergence residuals, uses AD for derivatives; may include measured velocities as data loss.",
            "ai_methodology_category": "Physics-informed ML",
            "applicability": "Applicable to flow reconstruction and parameter inference in incompressible flows where partial observations exist; demonstrated in literature for cylinder wake and other canonical flows.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as a relevant methodology for incompressible flow inference and closely related to the 3D incompressible case studies in this review.",
            "impact_potential": "Enables noninvasive recovery of pressure and velocity fields from limited experiments, aiding diagnostics and design.",
            "comparison_to_alternatives": "Compared conceptually against classical CFD which requires full boundary/initial conditions and meshes; NSFnets avoid mesh generation and assimilate sparse data directly.",
            "success_factors": "Strong prior (NS equations), quality and placement of observed measurements, and training strategies to handle PDE stiffness and optimization.",
            "key_insight": "Specialized PINN variants (NSFnets) effectively exploit Navier–Stokes structure to reconstruct hidden flow quantities from sparse measurements, bridging experiments and simulations.",
            "uuid": "e2313.4",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "SimNet",
            "name_full": "NVIDIA SimNet (AI-accelerated multiphysics simulation framework)",
            "brief_description": "A high-performance library implementing PINN-like approaches for multiphysics PDE simulation with GPU acceleration and parallelization features.",
            "citation_title": "NVIDIA SimNetˆ{TM}: an AI-accelerated multiphysics simulation framework.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Large-scale multiphysics PDE simulation (e.g., fluid mechanics)",
            "problem_description": "Efficiently implement PINN training and inference at scale using optimized GPU-accelerated infrastructure and algorithms.",
            "data_availability": "Supports the same mixed sparse observational + residual-sampling PINN paradigm; designed for large datasets and many residual samples leveraging GPU compute.",
            "data_structure": "Arbitrary PDE-oriented spatio-temporal datasets and residual sampling; supports heterogeneous data modalities for multiphysics.",
            "problem_complexity": "Targets industrial-scale PDE problems requiring large compute and parallelism; handles large numbers of residual/data points.",
            "domain_maturity": "Commercial/industrial-grade implementation built on PINN concepts; emerging as a practical tool.",
            "mechanistic_understanding_requirements": "High — enforces PDEs similarly to PINNs and provides infrastructure to scale enforcement across GPUs.",
            "ai_methodology_name": "GPU-accelerated PINN framework (SimNet)",
            "ai_methodology_description": "Software stack that implements PINN training with domain decomposition, efficient automatic differentiation, parallelization across GPUs, and performance optimizations for multiphysics problems.",
            "ai_methodology_category": "Physics-informed ML / scalable ML infrastructure",
            "applicability": "Intended for large-scale engineering and industrial problems where PINN methodologies need to be scaled to many residual samples and multi-GPU hardware.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as an efficient implementation enabling PINN application at scale; specific performance numbers not provided in this review.",
            "impact_potential": "Enables practical deployment of PINN-based approaches for industrial complexity simulations by leveraging hardware acceleration and parallelism.",
            "comparison_to_alternatives": "Provides hardware-accelerated PINN capabilities compared to ad-hoc single-GPU/CPU PINN implementations; aims to close gap toward industrial applicability.",
            "success_factors": "Efficient AD implementation, parallelization, domain-decomposition compatibility, and engineering optimizations.",
            "key_insight": "High-performance frameworks like SimNet are critical for scaling physics-informed ML to industrially relevant PDE problems by exploiting GPU parallelism and software optimizations.",
            "uuid": "e2313.5",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Multi-fidelity & composite networks",
            "name_full": "Multi-fidelity composite neural networks",
            "brief_description": "Neural architectures that combine data of different fidelities (e.g., low-accuracy/cheap simulations and high-accuracy/expensive measurements) to improve approximation and inverse PDE inference.",
            "citation_title": "A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Surrogate modeling and inverse PDE problems across computational physics and engineering",
            "problem_description": "Leverage cheap low-fidelity data and scarce high-fidelity measurements jointly to learn accurate models and infer PDE parameters efficiently.",
            "data_availability": "Explicitly designed for situations with mixed data abundance: abundant low-fidelity labelled data and scarce high-fidelity labels.",
            "data_structure": "Structured function evaluations from simulations and pointwise high-fidelity observations; multimodal fidelity levels.",
            "problem_complexity": "Moderate-to-high: combining heterogeneous data sources and learning cross-fidelity mappings, potentially for PDE-constrained inverse problems.",
            "domain_maturity": "Emerging within physics-informed ML; active area of research.",
            "mechanistic_understanding_requirements": "Medium-high — can combine physical constraints with fidelity models and is useful when some mechanistic modeling exists.",
            "ai_methodology_name": "Composite multi-fidelity neural networks",
            "ai_methodology_description": "Architectures composed of sub-networks that learn from different fidelity datasets with coupling layers or residual connections to transfer information; can be integrated with PDE residual losses.",
            "ai_methodology_category": "Hybrid / supervised + physics-informed",
            "applicability": "Appropriate where hierarchical data sources are available (cheap sim + expensive experiments); improves parameter inference and reduces need for many high-fidelity labels.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as a methodology that improves PINN performance with multi-fidelity data; specifics not demonstrated in this review's experiments.",
            "impact_potential": "High for reducing experimental costs and improving surrogate accuracy by leveraging abundant low-fidelity data combined with scarce high-fidelity observations.",
            "comparison_to_alternatives": "Offers better sample efficiency than single-fidelity learning; trade-offs include architectural complexity and potential bias transfer from low-fidelity data.",
            "success_factors": "Quality and correlation between fidelity levels, architecture design for information transfer, and appropriate weighting between fidelity losses.",
            "key_insight": "Multi-fidelity composite networks can significantly increase data efficiency for PDE inference by exploiting abundant low-fidelity simulations alongside scarce high-fidelity measurements.",
            "uuid": "e2313.6",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Adaptive training heuristics",
            "name_full": "Adaptive activations and dynamic loss weighting (training techniques for PINNs)",
            "brief_description": "Techniques like adaptive activation functions and dynamically adjusted loss weights to accelerate convergence and mitigate optimization pathologies in PINN training.",
            "citation_title": "Adaptive activation functions accelerate convergence in deep and physics-informed neural networks.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Training deep neural networks for PDE-constrained scientific computing (PINNs)",
            "problem_description": "Improve convergence speed/stability and balance competing loss terms (PDE residual vs data/BC/IC) during PINN optimization.",
            "data_availability": "Applies regardless of data amount; aims to improve training behavior with the available sparse/heterogeneous data common in PINN applications.",
            "data_structure": "Applies to network training on spatio-temporal coordinate-based datasets and residual samples.",
            "problem_complexity": "Targets optimization complexity (ill-conditioning, gradient pathologies) rather than problem physics complexity per se.",
            "domain_maturity": "Relatively recent empirical advances with evidence of improved convergence in PINN literature.",
            "mechanistic_understanding_requirements": "Low — methods are optimizer/architecture-level heuristics not altering mechanistic constraints.",
            "ai_methodology_name": "Adaptive activations & dynamic loss weighting",
            "ai_methodology_description": "Adaptive activations change activation scaling/parameters during training (layer-wise adaptation); dynamic loss weighting adjusts weights of PDE/data/BC losses during optimization to balance gradients and avoid dominated loss terms.",
            "ai_methodology_category": "Training/optimization techniques for physics-informed ML",
            "applicability": "Widely applicable across PINN problems to mitigate slow convergence and loss imbalance; used in the review's compressible-flow example and others cited.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to accelerate convergence and improve solution quality in cited literature; used as part of successful PINN implementations in this review.",
            "impact_potential": "Crucial for practical training of PINNs, improving robustness and enabling solutions that would otherwise be hampered by optimization issues.",
            "comparison_to_alternatives": "Complementary to other optimization strategies; dynamic weighting addresses multi-term loss imbalance better than fixed heuristics.",
            "success_factors": "Appropriate calibration of adaptation schemes, monitoring of loss components, and integration with optimizers like ADAM.",
            "key_insight": "Training heuristics such as adaptive activations and dynamic loss weighting are essential practical tools to overcome PINN optimization pathologies and realize their potential on complex PDE problems.",
            "uuid": "e2313.7",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Reinforcement learning (flow control)",
            "name_full": "Reinforcement learning for active flow control",
            "brief_description": "Application of reinforcement learning agents to learn control policies for active manipulation of fluid flows (e.g., bluff-body flow control) in simulations and experiments.",
            "citation_title": "Reinforcement learning for bluff body active flow control in experiments and simulations.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Active flow control in fluid mechanics (aerodynamics/hydrodynamics)",
            "problem_description": "Learn control strategies (actuation policies) to modify flow behavior (e.g., reduce drag, suppress vortex shedding) from interactions with simulation or experimental environments.",
            "data_availability": "Requires interaction data from simulations or experiments; quantity depends on learning algorithm (can be large for model-free RL); not detailed in the review.",
            "data_structure": "Time-series of state observations and reward signals; may involve high-dimensional sensor/image observations or low-dimensional flow descriptors.",
            "problem_complexity": "High: delayed rewards, partial observability, continuous action/state spaces, and real-time control constraints; sample complexity can be large.",
            "domain_maturity": "Active research area with successful demonstrations in both simulations and experiments but still emerging for robust industrial deployment.",
            "mechanistic_understanding_requirements": "Medium — interpretable control policies sometimes desirable, but black-box policies can be acceptable if performance validated experimentally.",
            "ai_methodology_name": "Reinforcement learning (policy learning for flow actuation)",
            "ai_methodology_description": "RL agents interact with flow simulations or experiments, receive observations and rewards, and update policies (e.g., deep RL) to optimize control objectives; can be trained in simulation and transferred to experiments.",
            "ai_methodology_category": "Reinforcement learning",
            "applicability": "Applicable and demonstrated for active control of flows (cited work); complementary to PINNs which focus on inference and surrogate modeling.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as an avenue for active flow control and as a different AI paradigm from PINNs; specific outcomes are reported in the cited work but not detailed here.",
            "impact_potential": "Significant for experimental and industrial flow control tasks, potentially reducing energy consumption or improving performance via learned control laws.",
            "comparison_to_alternatives": "Compared to model-based control, RL can discover novel control strategies without explicit models but may require more data and careful transfer to real experiments.",
            "success_factors": "High-quality simulation environments, reward shaping, transfer learning strategies, and sample-efficient RL algorithms.",
            "key_insight": "Reinforcement learning offers a complementary AI approach for learning control policies in fluid mechanics and can be combined with physics-aware models for better sample efficiency and transfer to experiments.",
            "uuid": "e2313.8",
            "source_info": {
                "paper_title": "Physics-informed neural networks (PINNs) for fluid mechanics: A review",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Machine learning of linear differential equations using Gaussian processes.",
            "rating": 2,
            "sanitized_title": "machine_learning_of_linear_differential_equations_using_gaussian_processes"
        },
        {
            "paper_title": "Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations.",
            "rating": 2,
            "sanitized_title": "hidden_fluid_mechanics_learning_velocity_and_pressure_fields_from_flow_visualizations"
        },
        {
            "paper_title": "Physics-informed neural networks for high-speed flows.",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_for_highspeed_flows"
        },
        {
            "paper_title": "Noninvasive inference of thrombus material properties with physics-informed neural networks.",
            "rating": 2,
            "sanitized_title": "noninvasive_inference_of_thrombus_material_properties_with_physicsinformed_neural_networks"
        },
        {
            "paper_title": "Extended Physics-Informed Neural Networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations.",
            "rating": 2,
            "sanitized_title": "extended_physicsinformed_neural_networks_xpinns_a_generalized_spacetime_domain_decomposition_based_deep_learning_framework_for_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data.",
            "rating": 2,
            "sanitized_title": "bpinns_bayesian_physicsinformed_neural_networks_for_forward_and_inverse_pde_problems_with_noisy_data"
        },
        {
            "paper_title": "NVIDIA SimNetˆ{TM}: an AI-accelerated multiphysics simulation framework.",
            "rating": 2,
            "sanitized_title": "nvidia_simnetˆtm_an_aiaccelerated_multiphysics_simulation_framework"
        },
        {
            "paper_title": "NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations.",
            "rating": 2,
            "sanitized_title": "nsfnets_navierstokes_flow_nets_physicsinformed_neural_networks_for_the_incompressible_navierstokes_equations"
        },
        {
            "paper_title": "A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems.",
            "rating": 1,
            "sanitized_title": "a_composite_neural_network_that_learns_from_multifidelity_data_application_to_function_approximation_and_inverse_pde_problems"
        }
    ],
    "cost": 0.02231,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Physics-informed neural networks (PINNs) for fluid mechanics: A review
2021</p>
<p>Shengze Cai 
Zhiping Mao 
Zhicheng Wang 
· Minglang Yin 
George Em Karniadakis 
Physics-informed neural networks (PINNs) for fluid mechanics: A review</p>
<p>Acta Mechanica Sinica
202110.1007/s10409-021-0xxxx-xRESEARCH PAPERPhysics-informed learning · PINNs · Inverse problems · Supersonic flows · Biomedical flows
Despite the significant progress over the last 50 years in simulating flow problems using numerical discretization of the Navier-Stokes equations (NSE), we still cannot incorporate seamlessly noisy data into existing algorithms, mesh-generation is complex, and we cannot tackle high-dimensional problems governed by parametrized NSE. Moreover, solving inverse flow problems is often prohibitively expensive and requires complex and expensive formulations and new computer codes. Here, we review flow physics-informed learning, integrating seamlessly data and mathematical models, and implementing them using physics-informed neural networks (PINNs). We demonstrate the effectiveness of PINNs for inverse problems related to three-dimensional wake flows, supersonic flows, and biomedical flows.</p>
<p>spectral, and even meshless methods [1,2,3,4]. Yet, for real-world applications, we still cannot incorporate seamlessly (multi-fidelity) data into existing algorithms, and for industrial-complexity problems the mesh generation is time consuming and still an art. Moreover, solving inverse problems, e.g., for unknown boundary conditions or conductivities [5], etc., is often prohibitively expensive and requires different formulations and new computer codes. Finally, computer programs such as OpenFOAM [6] have more than 100,000 lines of code, making it almost impossible to maintain and update them from one generation to the next.</p>
<p>Physics-informed learning [7], introduced in a series of papers by Karniadakis's group both for Gaussian-process regression [8,9] and physics-informed neural networks (PINNs) [10], can seamlessly integrate multifidelity/multimodality experimental data with the various Navier-Stokes formulations for incompressible flows [11,12] as well as compressible flows [13] and biomedical flows [14]. PINNs use automatic differentiation to represent all the differential operators and hence there is no explicit need for a mesh generation. Instead, the Navier-Stokes equations and any other kinematic or thermodynamic constraints can be directly incorporated in the loss function of the neural network (NN) by penalizing deviations from the target values (e.g., zero residuals for the conservation laws) and are properly weighted with any given data, e.g., partial measurements of the surface pressure. PINNs are not meant to be a replacement of the existing CFD codes, and in fact the current generation of PINNs is not as accurate or as efficient as high-order CFD codes [2] for solving the standard forward problems. This limitation is associated with the minimization of the loss function, which is a high-dimensional non-convex function, a limitation which is a grand challenge of all neural networks for even commercial machine learning. However, PINNs perform much more accurately and more efficiently than any CFD solver if any scattered partial spatio-temporal 2 Shengze Cai 1 et al. data are available for the flow problem under consideration. Moreover, the forward and inverse PINN formulations are identical so there is no need for expensive data assimilation schemes that have stalled progress especially for optimization and design applications of flow problems in the past.</p>
<p>In this paper we first review the basic principles of PINNs and recent extensions using domain decomposition for multiphysics and multiscale flow problems. We then present new results for a three-dimensional (3D) wake formed in incompressible flow behind a circular cylinder. We also show results for a two-dimensional (2D) supersonic flow past a blunt body, and finally we infer material parameters in simulating thrombus deformation in a biomedical flow.</p>
<p>PINNs: Physics-Informed Neural Networks</p>
<p>In this section we first review the basic PINN concept and subsequently discuss more recent advancements in incompressible, compressible and biomedical flows.</p>
<p>PINNs: Basic Concepts</p>
<p>We consider a parametrized partial differential equation (PDE) system given by:
f (x,t,û, ∂ xû , ∂ tû , . . . , λ) = 0, x ∈ Ω , t ∈ [0, T ] u(x,t 0 ) = g 0 (x) x ∈ Ω , u(x,t) = g Γ (t), x ∈ ∂ Ω , t ∈ [0, T ],(1)
where x ∈ R d is the spatial coordinate and t is the time; f denotes the residual of the PDE, containing the differential operators (i.e., [∂ xû , ∂ tû , . . . ]); λ = [λ 1 , λ 2 , . . . ] are the PDE parameters;û(x,t) is the solution of the PDE with initial condition g 0 (x) and boundary condition g Γ (t) (which can be Dirichlet, Neumann or mixed boundary condition); Ω and ∂ Ω represent the spatial domain and the boundary, respectively.</p>
<p>In the context of the vanilla PINNs [15], a fully-connected feed-forward neural network, which is composed of multiple hidden layers, is used to approximate the solution of the PDEû by taking the space and time coordinates (x,t) as inputs, as shown in the blue panel in Fig. 1. Let the hidden variable of the k th hidden layer be denoted by z k , then the neural network can be expressed as
z 0 = (x,t), z k = σ (W k z k−1 + b k ), 1 ≤ k ≤ L − 1 z k = W k z k−1 + b k , k = L,(2)
where the output of the last layer is used to approximate the true solution, namelyû ≈ z L . W k and b k denote the weight matrix and bias vector of the k th layer; σ (·) is a nonlinear activation function. All the trainable model parameters, i.e., weights and biases, are denoted by θ in this paper.</p>
<p>In PINNs, solving a PDE system (denoted by equ. 1) is converted into an optimization problem by iteratively updating θ with the goal to minimize the loss function L:
L = ω 1 L PDE + ω 2 L data + ω 3 L IC + ω 4 L BC ,(3)
where ω 1−4 are the weighting coefficients for different loss terms. The first term L PDE in equ. 3 penalizes the residual of the governing equations. The other terms are imposed to satisfy the model predictions for the measurements L data , the initial condition L IC , and the boundary condition L BC , respectively. In general, the mean square error (MSE), taking the L 2 -norm of the sampling points, is employed to compute the losses in equ. 3. The sampling points are defined as a data set
{x i ,t i } N i=1
, where the number of points (denoted by N) for different loss terms can be different. Generally, we use the ADAM optimizer [16], an adaptive algorithm for gradient-based first-order optimization, to optimize the model parameters θ .</p>
<p>Remark 1: We note that the definition of the loss function shown in equ. 3 is problem-dependent, hence some terms may disappear for different types of the problem. For example, when we solve a forward problem in fluid mechanics with the known parameters (λ ) and the initial/boundary conditions of the PDEs, the data loss L data is not necessarily required. However, in the cases where the model parameters or the initial/boundary conditions are unknown (namely, inverse problems), the data measurements should be taken into account in order to make the optimization problem solvable. We also note that the PINN framework can be employed to solve an "over-determined" system, e.g., well-posed in a classical sense with initial and boundary conditions known and additionally some measurements inside the domain or at boundaries (e.g., pressure measurements).</p>
<p>One of the key procedures to construct the PDE loss in equ. 3 is the computation of partial derivatives, which is addressed by using automatic differentiation (AD). Relying on the combination of the derivatives for a sequence of operations by using the chain rule, AD calculates the derivatives of the outputs with respect to the network inputs directly in the computational graph. The computation of partial derivatives can be calculated with an explicit expression, hence avoiding introducing truncation errors in conventional numerical approximations. At the present time, AD has been implemented in various deep learning frameworks [17,18], which makes it convenient for the development of PINNs.</p>
<p>A schematic of PINNs is shown in Fig. 1, where the key elements (e.g., neural network, AD, loss function) are indicated in different colors. Here, we consider a Physics-informed neural networks (PINNs) for fluid mechanics: A review 3 Fig. 1: Schematic of a physics-informed neural network (PINN). A fully-connected neural network, with time and space coordinates (t, x) as inputs, is used to approximate the multi-physics solutionsû = [u, v, p, φ ]. The derivatives ofû with respect to the inputs are calculated using automatic differentiation (AD) and then used to formulate the residuals of the governing equations in the loss function, that is generally composed of multiple terms weighted by different coefficients. The parameters of the neural network θ and the unknown PDE parameters λ can be learned simultaneously by minimizing the loss function.</p>
<p>multi-physics problem, where the solutions include the velocity (u, v), pressure p and a scalar field φ , which are coupled in a PDE system f . The schematic in Fig. 1 represents most of the typical problems in fluid mechanics. For instance, the PDEs considered here can be the Boussinesq approximation of the Navier-Stokes equations, where φ is the temperature. Following the paradigm in Fig. 1, we will describe the governing equations, the loss function and the neural network configurations of PINNs case-by-case in the rest of this paper.</p>
<p>Recent Advances of PINNs</p>
<p>First proposed in [19,20], see also [15], PINNs have attracted a lot of attention in the scientific computing community as well as the fluid mechanics community. Here, we review some related works regarding the methodology and the application to fluid mechanics.</p>
<p>Beneficial due to the high flexibility and the expressive ability in function approximation, PINNs have been extended to solve various classes of PDEs, e.g., integro-differential equations [21], fractional equations [21], surfaces PDEs [22] and stochastic differential equations [23]. A variational formulation of PINNs based on the Galerkin method (hp-VPINN) was proposed to deal with PDEs with non-smooth solutions [24]. In addition, the variational hp-VPINN considered domain decomposition, and similar pointwise versions were also studied in CPINN [25], and XPINN [26]. A general parallel implementation of PINNs with domain decomposition for flow problems is presented in [27]; the NVIDIA library SimNet [28] is also a very efficient implementation of PINNs. Another important extension is the uncertainty quantification for the PDE solutions inferred by neural networks [29,30,31,32,33]. This has been studied by using the Bayesian framework [33]. Moreover, some other researches on PINNs focused on the development of the neural network architecture and the training, e.g., using multi-fidelity framework [34], adaptive activation functions [35] and dynamic weights of the loss function [36], hard constraints [37] and CNN-based network architectures [38], which can improve the performance of PINNs on different problems. On the theoretical side, some recent works [39,40,41] have provided more guarantees and insights into the convergence of PINNs.</p>
<p>The development of the methodology has inspired a number of applications in other fields, especially in fluid mechanics where the flow phenomena can be described by the NSE. In [15], the vanilla PINN was proposed to infer the unknown parameters (e.g., the coefficient of the convection term) in the NS equations based on velocity measurements for the 2D flow over a cylinder. Following this work, PINNs were then applied to various flows [10,11,12,13,14,42,43,44,45,46,47,48,49], covering the applications on compressible flows [13], biomedical flows [14,42,50], turbulent convection flows [48], free boundary and Stefan problems [47], etc. The main 4 Shengze Cai 1 et al.  The simulation was performed by the CFD solver Nektar, which is based on the spectral/hp element method [2].</p>
<p>attractive advantage of PINNs in solving fluid mechanics problems is that a unified framework (shown in Fig. 1) can be used for both forward and inverse problems. Compared to the traditional CFD solvers, PINNs are superior at integrating the data (observations of the flow quantities) and physics (governing equations). A promising application is on the flow visualization technology [12,51], where the flow fields can be easily inferred from the observations such as concentration fields and images. On the contrary, such inverse problems are difficult for conventional CFD solvers. More relevant works on mechanics in general can be found in [52] for turbulent flow, [53] for phase-field fracture model, and [54] for inferring modulus in a nonhomogeneous material.</p>
<p>Case Study for 3D Incompressible Flows</p>
<p>In this section, we demonstrate the effectiveness of PINNs for solving inverse problems in incompressible flows. In particular, we apply PINNs to reconstruct the 3D flow fields based on a few two-dimensional and two-component (2D2C) velocity observations. The proposed algorithm is able to infer the full velocity and pressure fields very accurately with limited data, which is promising for diagnosis of complex flows when only 2D measurements (e.g., planar particle image velocimetry) are available.</p>
<p>Problem setup</p>
<p>We consider the 3D wake flow past a stationary cylinder at Reynolds number Re = 200 in this section. In order to evaluate the performance of PINNs, we generate the reference solution numerically by using the spectral/hp element method [2]. <br />
x-plane (v, w) 61 × 26 y-plane (u, w) 61 × 26 z-plane (u, v) 61 × 61
focus on a sub-domain in the wake flow, namely Ω s : 4,9], which is represented by a cube with blue edges in Fig. 2(a). The contours of the three velocity components and pressure field are shown in Fig. 2(b). An Eulerian mesh with 61 × 61 × 26 grid points is used for plotting. To demonstrate the unsteadiness of the motion, we consider 50 snapshots with ∆t = 0.2, which is about two periods of the vortex shedding cycle.
[1.5, 7.5] × [−3, 3] × [
Here, we aim to apply PINNs for reconstructing the 3D flow field from the velocity observations of a few 2D planes. As illustrated in Fig. 3, three different "experimental" setups are considered in this paper:</p>
<p>-Case 1: two x-planes (x = 1.5, 7.5), one y-plane (y = 0) and two z-planes (z = 4.0, 9.0) are observed. -Case 2: two x-planes (x = 1.5, 7.5), one y-plane (y = 0) and one z-plane (z = 6.4) are observed. -Case 3: one x-plane (x = 1.5), one y-plane (y = 0) and one z-plane (z = 6.4) are observed.</p>
<p>We note that for these cross-planes, only the projected vectors (two components) are considered known. For example, the velocities (u, v) can be observed on the z-plane, while the orthogonal component (w) is unknown.</p>
<p>The purpose of doing this is to mimic the planar particle image velocimetry in real experiments. Moreover, the resolutions of these 2D2C observations are different, which can be found in Table 1.</p>
<p>Implementation of PINNs</p>
<p>Given the observation data on the cross-planes, we train a PINNs model to approximate the flow fields over the spacetime domain. The PINN in this section take (x,t) = (x, y, z,t) as inputs and outputs the velocity and pressure (u, v, w, p). The loss function in PINN can be defined as:
L = L data + L PDE ,(4)
where
L data = 1 N u N u ∑ i u(x i data ,t i data ) − u i data 2 + 1 N v N v ∑ i v(x i data ,t i data ) − v i data 2 + 1 N w N w ∑ i w(x i data ,t i data ) − w i data 2 ,(5)
and
L PDE = 1 N f N f ∑ i 4 ∑ j f j (x i f ,t i f ) 2 ,(6)f 1,2,3 = ∂ u ∂t + (u · ∇)u + ∇p − 1 Re ∇ 2 u, f 4 = ∇ · u.
The data loss L data is composed of three components, and the number of training data (namely N u , N v and N w ) depends on the number of observed planes, the data resolution of each plane as well as the number of snapshots. On the other hand, the residual points for L PDE can be randomly selected, and here we sample N f = 3 × 10 6 points over the investigated space and time domain Ω s . Note that in this study, the boundary and initial conditions are not required unlike the classical setting. Moreover, no information about the pressure is given. The weighting coefficients for the loss terms are all equal to 1. A fully-connected neural network with 8 hidden layers and 200 neurons per layer is employed. The activation function of each neuron is σ = sin(·). We apply the ADAM optimizer with mini-batch for network training, where a batch size of N = 10 4 is used for both data and residual points. The network is trained for 150 epochs with learning rates 1 × 10 −3 , 5 × 10 −4 and 1 × 10 −4 for every 50 epochs. After training, the velocity and pressure fields are evaluated on the Eulerian grid for comparison and visualization.</p>
<p>Inference results</p>
<p>For a quantitative assessment, we first define the relative L 2norm error as the evaluation metric, which is expressed as:
ε V = V CFD −V 2 V CFD 2 × 100%,(7)
where V ∈ {u, v, w, p}; V CFD andV account for the CFD data and the output of PINNs, respectively. We compute the errors in the investigated time domain, which are shown in Fig. 4. It can be seen from the plots that PINNs can infer the 3D flow very accurately for Case 1 and Case 2; using 5 planes (Case 1) is slightly better than using 4 planes (Case 2). When only 3 cross-planes are available (Case 3), the errors become much larger. However, the result of Case 3 is still acceptable as we are able to infer the main flow features with high accuracy (the error of the streamwise velocity is mostly less than 2%). We note that the errors of w-velocity are larger than the other components since the w-velocity magnitude is relatively small. Moreover, we observe larger discrepancy for the initial and final time instants, which can be attributed to the lack of training data for computing derivatives at t &lt; 0 and t &gt; 10. This generally happens in the cases when the initial condition is not provided in the unsteady case [12].</p>
<p>To visualize more details, we emphasize the result of Case 2 and demonstrate the iso-surface of vorticity magnitude and iso-surfaces of pressure at t = 8.0 in Fig. 5, where Fig. 5(a) shows the reference CFD data and Fig. 5(b) shows the result inferred by PINNs. The vorticity value is |ω| = 1.2 and the color represents the streamwise velocity component. It can be seen that the PINNs inference result (inferred from a few 2D2C observations) is very consistent with the CFD simulation. In addition, the velocities (u, v) at a single point (x = 3, y = 0, z = 6.4) against time are plotted in Fig. 5(c), where we can find that PINNs can capture the unsteadiness of vortex shedding flow very accurately.</p>
<p>Case Study for Compressible Flows</p>
<p>PINNs have also been used in simulating high-speed flows [13]. In this section, we consider the following 2D steady compressible Euler equations:
∇ · f (U) = 0, x ∈ Ω ⊂ R 2 ,(8)
where
U = [ρ, ρu, ρv, ρE] T , f = (G 1 , G 2 ) with G 1 (U) = [ρu, p + ρu 2 , ρuv, pu + ρuE], G 2 (U) = [ρv, ρuv, p + ρv 2 , pv + ρvE].
Here, ρ is the density, p is the pressure, [u, v] are the velocity components, and E is the total energy. We use the additional equation of state, which describes the relation of the pressure and energy, to close the above Euler equations. For instance, we consider the equation of state for a polytropic gas given by
p = (γ − 1) ρE − 1 2 ρ u 2 ,(9)
where γ is the adiabatic index and u = (u, v).</p>
<p>We shall employ the PINNs to solve the inverse problem of the compressible Euler equ. 8. In particular, we shall infer the density, pressure and velocity fields by using PINNs based on the information of density gradients, Physics-informed neural networks (PINNs) for fluid mechanics: A review  </p>
<p>Problem setup</p>
<p>We consider a 2D bow shock wave problem. For traditional CFD simulations, it is crucial that the boundary conditions, which play an important role, are properly implemented. However, for the shock wave problem in high-speed flows, the boundary conditions in real experiments are usually not known and can only be estimated approximately. In the present work, instead of using most of the boundary conditions required by the traditional CFD simulation, we solve the Euler equ. 8 using PINNs based on the data of density gradients ∇ρ motivated by the Schlieren photography available experimentally; additionally, we use limited data of the surface pressure obtained by pressure sensors as well as the global constrains (mass, momentum and energy). We also use the inflow conditions here. Note that unlike the one-dimensional case, where the density gradient in the whole computational domain is used in [13], here we only use the density gradients in a sub-domain D of the computational domain Ω , i.e., D ⊂ Ω . By combining the mathematical model and the given data, we have the weighted loss function of PINN given by Loss = ω 1 Loss F + ω 2 Loss ∇ρ| D + ω 3 Loss in f low + ω 4 Loss p * +ω 5 (Loss Mass + Loss Momentum + Loss Energy ) + ω 6 Loss n·u ,</p>
<p>whre the last term corresponds to the velocity condition on the surface.</p>
<p>Inference results</p>
<p>To demonstrate the effectiveness of PINNs for compressible flows, we consider the bow shock problem with the following inlet flow conditions
M ∞ = 4, p ∞ = 101253.6Pa, ρ ∞ = 1.225kg/m 3 , u ∞ = 1360.6963m/s, v ∞ = 0, T ∞ = 288K.(11)
The data points for the pressure are located on the surface of the body. By using the above inflow conditions and CFD code, we can obtain the steady state flow. We show the density computed by CFD in the left plot of Fig. 6. We employ a 6 × 60 (6 hidden layer) neural network and train it by using layer-wise adaptive tanh activate function [35] and the Adam optimizer with the learning late being 6 × 10 −4   Here, we also use the technique of dynamic weights [36,11]. The history of the training loss is shown in the right plot of Fig. 6. The results of the PINN solutions for the pressure and velocity (u) are shown in Fig. 7. Observe that the PINN solutions are in good agreement with the CFD data. This indicates that we can reconstruct the flow fields for high-speed flows using some other available knowledge different from the boundary conditions required by the traditional CFD simulation.</p>
<p>Case Study for Biomedical Flows</p>
<p>In addition to the aforementioned flow examples, PINNs have also been used in biomedical flows [14]. In this section, we consider inferring material properties of a thrombus in arterial flow described by the Navier-Stokes and Cahn-Hilliard equations. Such equations can be used for describing the mechanical interaction between thrombus and blood flow as a fluid-structure interaction (FSI) problem. The PDE system can be written as:
ρ( ∂ u ∂t + u · ∇u) + ∇p = ∇ · (σ vis + σ coh ) − µ (1 − φ )u 2κ(φ ) ,(12)∇ · u = 0,(13)∂ ψ ∂t + u · ∇ψ = 0,(14)∂ φ ∂t + u · ∇φ = τ∆ ω,(15)ω = ∆ φ + γg(φ ),(16)
g(φ ) is the derivative of the double-well potential (φ 2 − 1) 2 /4h 2 . u(x,t), p(x,t) σ(x,t), and φ (x,t) represent the velocity, pressure, stress, and phase field. h is the interfacial length; ψ denotes the auxiliary vector and its gradients are the components of the deformation gradient tensor F as follows:
F := − ∂ ψ 1 ∂ y − ψ 2 ∂ y ∂ ψ 1 ∂ x ∂ ψ 2 ∂ x .
Physics-informed neural networks (PINNs) for fluid mechanics: A review Equ. 12 is the Navier-Stokes equation with viscous and cohesive stresses, respectively, which can be written as:
σ vis = µ∇u,(17)σ coh = λ ∇ · (∇φ ⊗ φ ).(18)
Moreover, γ, τ, and λ are the interfacial mobility, relaxation parameter, and mixing energy density, respectively. We follow the normalization in [55] and write
κ = k f a 2 f
, k f is the true permeability and a f is the fibrin radius. We set the density ρ = 1, viscosity µ = 0.1, λ = 4.2428 × 10 −5 , τ = 10 −6 , and the interface length h= 0.05. These parameters in PINNs are non-dimensionalized values so as to be consistent with the CFD solver. The velocity at inlet Γ i is set as Dirichlet boundary conditions u = g, (x,t) ∈ Γ i × (0, T ). We impose the no-slip boundary on the wall Γ w and Neumann boundary conditions, i.e., ∂ φ ∂ n = ∂ ω ∂ n = 0, x ∈ Γ w ∪ Γ i ∪ Γ o for φ and ω at all boundaries.</p>
<p>PINNs</p>
<p>We construct two fully-connected neural networks, Net U and Net W, where the outputs of Net U represents a surrogate model for the PDE solutions u, v, p, and φ and the outputs of Net W are PDE solutions ω, ψ 1 , and ψ 2 . Each network has 9 hidden layers with 20 neurons per layer. The total loss L is a combination of different losses as:
L = ω 1 L PDE + ω 2 L IC + ω 3 L BC + ω 4 L data ,(19)
where L PDE is the PDEs residual loss, L IC is the initial condition loss, L BC is the boundary condition loss, and L data is the data loss. In particular:
L PDE (θ , λ; X PDE ) = 1 |X PDE | ∑ x∈X PDE f (x, ∂ tû , ∂ xû , ...; λ) 2 2 ,(20)L BC (θ , λ; X BC ) = 1 |X BC | ∑ x∈X BC B(û, x) 2 2 ,(21)
L IC (θ , λ; X IC ) = 1
|X IC | ∑ x∈X IC û − u t 0 2 2 ,(22)
L data (θ , λ; X data ) = 1
|X data | ∑ x∈X data û − u data 2 2 ,(23)
where ω 1 , ω 2 , ω 3 , and ω 4 are the weights of each term. The training sets X PDE , X BC , and X IC are sampled from the inner spatio-temporal domain, boundaries, and initial snapshot, respectively. X data is the set that contains sensor coordinates and point measurements; |·| denotes the number of training data in the training set. In particular, B represents a combination of the Dirichlet and Neumann residuals at boundaries. Finally, we optimize the model parameters θ and the PDE parameters λ = [κ] by minimizing the total loss L(θ, λ) iteratively until the loss satisfies the stopping criteria. Minimizing the total loss is an optimization process for λ such that the outputs of the PINN satisfy the PDE system, initial/boundary conditions, and point measurements.</p>
<p>Problem setup</p>
<p>We consider a computational setup with a semi-circle permeable thrombus in a channel with a steady parabolic flow coming from the left (Fig. 8). This setup is meaningful in a sense that it presents an idealized thrombus with an impermeable core, which consists of a fibrin clot and a permeable shell (consisting of loosely-packed and partially-activated platelets). This model has been adapted as an idealized thrombus in previous works [56,57,58]. The goal is to infer the unknown permeability (for the shell and core) and velocity field based on the measurable phase field data. Fig. 8(b) presents all the types of training data, namely the initial snapshot t 0 ( ), inner spatio-temporal domain from t 1 to t n ( ), and at the boundaries ( ). Also, point measurements ( ) including their coordinates and phase field value are sampled in the spatio-temporal domain to calculate the data loss term in the total loss. We draw 1,000 points from an initial snapshot, 10,000 inner points to compute the PDE residuals, and 1,000 boundary points to compute the residuals at boundaries. Note that point measurements and inner points are drawn from the inner spatio-temporal domain; the former contains the PDE solutions whereas the latter does not. The thrombus is present in the middle of the channel as shown in Fig. 8(a) with permeability κ = 0.001 in the core (φ = −1) and κ = 1 in the outer shell layer (φ = 0). To express such spatial variation explicitly, we consider a relation between φ and κ in this case:
κ(φ ) = e aφ + b,(24)
where a and b are model parameters to be optimized in the PINN model and the true values of a and b are 6.90 and 0.0. Notice that the relation is not unique in terms of its form as long as the permeability value matches the true value for φ = 0 and 1.</p>
<p>Inference Results</p>
<p>We present the inference results in Fig. 9. Fig. 9(a) shows the history of the different losses, namely PDE loss, boundary condition loss (Loss BC), initial condition loss (Loss IC), and data loss (Loss Data) in (a). We trained the model with 300,000 epochs with PDE loss as the largest component. The other errors are lower than O(10 −3 ) Plot (b) shows the inference result for κ as a function of φ ; the inferred a and b are at 7.1 and 0.0003, indicating that the permeability at core area (κ(φ = −1) = 0.0011) and shell area (κ(φ = 0) = 1.0003) match the reference values well. As a qualitative comparison, we present the predicted fields of φ pred and v pred and their difference with the reference data at t = 0.78 in Fig. 9(c). The phase field prediction exhibits a good agreement compared with the ground truth data. More importantly, the hidden velocity field can also be accurately inferred only based on the phase field data. Notice that the errors for the phase field are mainly distributed in and around the outlet layer of the thrombus and the errors in velocity field are mainly confined within the shell layer.</p>
<p>Summary</p>
<p>PINNs offer a new approach to simulating realistic fluid flows, where some data are available from multimodality measurements whereas the boundary conditions or initial conditions may be unknown. While this is perhaps the prevailing scenario in practice, existing CFD solvers cannot handle such ill-posed problems and hence one can think of PINNs for fluid problems as a complementary approach to the plethora of existing numerical methods for CFD for idealized problems. There are several opportunities for further research, e.g., using PINNs for active flow control to replace expensive experiments and time-consuming large-scale simulations as in [59], or predict fast the flow at a new high Reynolds number using transfer learning techniques assuming we have available solutions at lower Reynolds numbers. Moreover, a new area for exploration could be the development of closure models for unresolved flow dynamics at very high Reynolds number using the automatic data assimilation method provided by PINNs. Computing flow problems at scale requires efficient multi-GPU implementations in the spirit of data parallel [28] or a hybrid data parallel and model parallel paradigms as in [27]. The parallel speed up obtained for flow simulations so far is very good, suggesting that PINNs can be used in the near future for industrial complexity problems at scale that CFD methods cannot tackle.</p>
<p>incompressible flows: illustration of simulating the 3D wake flow over a circular cylinder. (a) Iso-surface of the vorticity (x-component) in the whole domain, color coded by the streamwise velocity. The cube with blue edges represents the investigated domain in this case. (b) Velocity and pressure fields in the investigated domain. The simulation was performed by the CFD solver called Nektar, which is based on the spectral/hp element method.</p>
<p>Fig. 2 :
2Case study of PINNs for incompressible flows: illustration of simulating the 3D wake flow over a circular cylinder. (a) Iso-surface of the vorticity (x-component) in the whole domain color-coded by the streamwise velocity. The cube with blue edges represents the computational domain in this case. (b) Velocity and pressure fields in the domain.</p>
<p>The computational domain is defined as Ω : [−7.5, 28.5] × [−20, 20] × [0,12.5], where the coordinates are non-dimensionalized by the diameter of the cylinder. The center of the cylinder is located at (x, y) = (0, 0). We assume that the velocity (u = 1) is uniform at the inflow boundary where x = −7.5. A periodic boundary condition is used at the lateral boundaries where y = ±20, and the zero-pressure is prescribed at the outlet where x = 28.5. Moreover, the no-slip boundary condition is imposed on the cylinder surface. The governing equations in this case study are the dimensionless incompressible Navier-Stokes equations with Re = 200. Under this configuration, the wake flow over the cylinder is 3D and unsteady. The simulation is performed until the vortex shedding flow becomes stable. At the end, the time-dependent data are collected for PINN training and evaluation.The simulation results of the 3D flow are shown inFig. 2, whereFig. 2(a)shows the iso-surface of streamwise vorticity (ω x = −0.3) color-coded with the streamwise velocity u. In this section, we are interested in the 3D flow reconstruction problem from limited data, and we only incompressible flows: problem setup for 3D flow reconstruction from 2D2C observations. (a) Case 1: two x-planes ($x=1.5, 7.5$), one y-plane ($y=0$) and two z-planes ($z=4.0,9.0$) are observed. (b) Case 2: two x-planes ($x=1.5, 7.5$), one y-plane ($y=0$) and one z-plane ($z=6.4$) are observed. (c) Case 3: one x-plane ($x=1.5$), one y-plane ($y=0$) and one z-plane ($z=6.4$) are observed. Note that for the cross-planes, only the projected vectors (two components) are measured.</p>
<p>Fig. 3 :
3Case study of PINNs for incompressible flows: problem setup for 3D flow reconstruction from 2D2C observations. (a) Case 1: two x-planes (x = 1.5, 7.5), one y-plane (y = 0) and two z-planes (z = 4.0, 9.0) are observed. (b) Case 2: two x-planes (x = 1.5, 7.5), one y-plane (y = 0) and one z-plane (z = 6.4) are observed. (c) Case 3: one x-plane (x = 1.5), one y-plane (y = 0) and one z-plane (z = 6.4) are observed. Note that for the cross-planes, only the projected vectors are measured. The goal is to infer the 3D flow in the investigated domain using PINNs from these 2D2C observations.</p>
<p>Fig. 4 :
4Case study of PINNs for incompressible flows: relative L 2 -norm errors of velocities and pressure for different flow reconstruction setups. These three cases correspond to those shown inFig. 3. The errors are computed over the entire investigated domain.</p>
<p>incompressible flows: inference result of PINNs for Case 2. (a) Iso-surfaces of vorticity magnitude (top) and pressure (bottom) at $t=8.0$ from CFD data. (b) Iso-surfaces of vorticity magnitude (top) and pressure (bottom) at $t=8.0$ inferred by PINNs. (c) Point measurement $(x=3,y=0,z=6.4)$ of velocity $(u,v)$ against time.In this case, the 3D flow is reconstructed by PINNs from four cross-planes, namely two x-planes ($x=1.5, 7.5$), one y-plane ($y=0$) and one z-plane ($z=6.4$).</p>
<p>Fig. 5 :
5Case study of PINNs for incompressible flows: inference result of PINNs for Case 2. (a) Iso-surfaces of vorticity magnitude (top) and pressure (bottom) at t = 8.0 from CFD data. (b) Iso-surfaces of vorticity magnitude (top) and pressure (bottom) at t = 8.0 inferred by PINNs. (c) Point measurement (x = 3, y = 0, z = 6.4) of velocity (u, v) against time. In this case, the 3D flow is inferred by PINNs from four cross-planes. limited data of pressure (pressure on the surface of the body), inflow conditions and global physical constrains.</p>
<p>Fig. 6 :
6Case study of PINNs for compressible flows. Left: the density obtained by using CFD simulation with the inlet flow condition(11). Right: training loss vs. number of epochs.</p>
<p>Fig. 7 :
7Case study of PINNs for compressible flows. Comparison between the PINN solutions and the CFD solutions. Top: pressure p, Bottom: velocity component u. and 3 × 10 5 epochs.</p>
<p>9 Fig. 8 :
98Case study for 2D flow past a thrombus with phase dependent permeability. (a) A channel with walls on the top and bottom boundaries with the inlet flow u(t, y) entering from the left side; φ =1 is the fluid. A thrombus with an impermeable core φ = −1 and permeable shell φ = 0 is present at the bottom boundary. (b) Sampling points for inferring permeability include initial points ( ) at the time t 0 , inner points ( ) from t 1 to t n , boundary points ( ) on boundaries, and point measurements ( ) with PDE solutions. (Figure adapted from [14]).</p>
<p>Fig. 9 :
9Inference results for 2D flow past a thrombus with phase-dependent permeability. (a) History of network losses (Loss PDE, Loss IC, Loss BC, and Loss Data) and (b) inferred permeability κ as a function of φ . (c) Comparison of phase field and velocity field for κ(φ ) at t = 0.78 and their absolute error. The core permeability is 0.001 and the shell permeability is set 1 as the actual values. 10,000 data points are scattered in the spatio-temporal domain from 30 snapshots (t ∈ [0.03, 0.93]) as the training data to infer the permeability. Inferred a and b are 7.10 and 0.0003 compared to the true value at 6.9 and 0.0. (Figure adapted from [14])</p>
<p>Table 1 :
1Case study of PINNs for incompressible flows: details of the 2D2C observations.Cross-section 
Observed 
Observed 
velocity components spatial resolution </p>
<p>Shengze Cai 1 et al.
Acknowledgements The last author (GEK) would like to acknowledge support by the Alexander von Humboldt fellowship.
Streamline upwind/Petrov-Galerkin formulations for convection dominated flows with particular emphasis on the incompressible Navier-Stokes equations. A N Brooks, T J Hughes, Computer Methods in Applied Mechanics and Engineering. 321-3Brooks, A.N., Hughes, T.J.: Streamline upwind/Petrov-Galerkin formulations for convection dominated flows with particular emphasis on the incompressible Navier-Stokes equations. Computer Methods in Applied Mechanics and Engineering 32(1-3), 199-259 (1982)</p>
<p>G E Karniadakis, S Sherwin, Spectral/hp Element Methods for Computational Fluid Dynamics. Oxford,UKOxford University Press2nd editionKarniadakis, G.E., Sherwin, S.: Spectral/hp Element Methods for Computational Fluid Dynamics, 2nd edition. Oxford University Press, Oxford,UK (2005)</p>
<p>Meshless methods for computational fluid dynamics. A J Katz, CAStanford University StanfordKatz, A.J.: Meshless methods for computational fluid dynamics. Stanford University Stanford, CA (2009)</p>
<p>Smoothed particle hydrodynamics (SPH): an overview and recent developments. M Liu, G Liu, Archives of Computational Methods in Engineering. 171Liu, M., Liu, G.: Smoothed particle hydrodynamics (SPH): an overview and recent developments. Archives of Computational Methods in Engineering 17(1), 25-76 (2010)</p>
<p>Inverse heat conduction: Ill-posed problems. J V Beck, B Blackwell, C R S ClairJr, James BeckBeck, J.V., Blackwell, B., Clair Jr, C.R.S.: Inverse heat conduction: Ill-posed problems. James Beck (1985)</p>
<p>OpenFOAM: A C++ library for complex physics simulations. H Jasak, A Jemcov, Z Tukovic, International Workshop on Coupled Methods in Numerical Dynamics. 1000Jasak, H., Jemcov, A., Tukovic, Z., et al.: OpenFOAM: A C++ library for complex physics simulations. In: International Workshop on Coupled Methods in Numerical Dynamics, vol. 1000, pp. 1-20. IUC Dubrovnik Croatia (2007)</p>
<p>M Raissi, P Perdikaris, G E Karniadakis, Physics informed learning machine. 10540Raissi, M., Perdikaris, P., Karniadakis, G.E.: Physics informed learning machine (2021). US Patent 10,963,540</p>
<p>Machine learning of linear differential equations using Gaussian processes. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 348Raissi, M., Perdikaris, P., Karniadakis, G.E.: Machine learning of linear differential equations using Gaussian processes. Journal of Computational Physics 348, 683-693 (2017)</p>
<p>Numerical Gaussian processes for time-dependent and nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, SIAM Journal on Scientific Computing. 401Raissi, M., Perdikaris, P., Karniadakis, G.E.: Numerical Gaussian processes for time-dependent and nonlinear partial differential equations. SIAM Journal on Scientific Computing 40(1), A172- A198 (2018)</p>
<p>Deep learning of vortex-induced vibrations. M Raissi, Z Wang, M S Triantafyllou, G E Karniadakis, Journal of Fluid Mechanics. 861Raissi, M., Wang, Z., Triantafyllou, M.S., Karniadakis, G.E.: Deep learning of vortex-induced vibrations. Journal of Fluid Mechanics 861, 119-137 (2019)</p>
<p>X Jin, S Cai, H Li, G E Karniadakis, NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations. 426109951Jin, X., Cai, S., Li, H., Karniadakis, G.E.: NSFnets (Navier- Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations. Journal of Computational Physics 426, 109951 (2021)</p>
<p>Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. M Raissi, A Yazdani, G E Karniadakis, Science. 3676481Raissi, M., Yazdani, A., Karniadakis, G.E.: Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science 367(6481), 1026-1030 (2020)</p>
<p>Physics-informed neural networks for high-speed flows. Z Mao, A D Jagtap, G E Karniadakis, Computer Methods in Applied Mechanics and Engineering. 360112789Mao, Z., Jagtap, A.D., Karniadakis, G.E.: Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering 360, 112789 (2020)</p>
<p>Noninvasive inference of thrombus material properties with physicsinformed neural networks. M Yin, X Zheng, J D Humphrey, G E Karniadakis, Computer Methods in Applied Mechanics and Engineering. 375113603Yin, M., Zheng, X., Humphrey, J.D., Karniadakis, G.E.: Non- invasive inference of thrombus material properties with physics- informed neural networks. Computer Methods in Applied Mechanics and Engineering 375, 113603 (2021)</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 378Raissi, M., Perdikaris, P., Karniadakis, G.E.: Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics 378, 686-707 (2019)</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)</p>
<p>Tensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, 12th {USENIX} symposium on operating systems design and implementation. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.: Tensorflow: A system for large-scale machine learning. In: 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16), pp. 265-283 (2016)</p>
<p>A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, arXiv:1912.01703Pytorch: An imperative style, high-performance deep learning library. arXiv preprintPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703 (2019)</p>
<p>M Raissi, P Perdikaris, G E Karniadakis, arXiv:1711.10561Data-driven solutions of nonlinear partial differential equations. arXiv preprintPhysics informed deep learningRaissi, M., Perdikaris, P., Karniadakis, G.E.: Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561 (2017)</p>
<p>M Raissi, P Perdikaris, G E Karniadakis, arXiv:1711.10561Data-driven discovery of nonlinear partial differential equations. arxiv. arXiv preprintPhysics informed deep learningRaissi, M., Perdikaris, P., Karniadakis, G.E.: Physics informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations. arxiv. arXiv preprint arXiv:1711.10561 (2017)</p>
<p>fPINNs: Fractional physicsinformed neural networks. G Pang, L Lu, G E Karniadakis, SIAM Journal on Scientific Computing. 414Pang, G., Lu, L., Karniadakis, G.E.: fPINNs: Fractional physics- informed neural networks. SIAM Journal on Scientific Computing 41(4), A2603-A2626 (2019)</p>
<p>A physics-informed neural network framework for PDEs on 3D surfaces: Time independent problems. Z Fang, J Zhan, IEEE Access. 8Fang, Z., Zhan, J.: A physics-informed neural network framework for PDEs on 3D surfaces: Time independent problems. IEEE Access 8, 26328-26335 (2019)</p>
<p>Learning in modal space: Solving time-dependent stochastic PDEs using physics-informed neural networks. D Zhang, L Guo, G E Karniadakis, SIAM Journal on Scientific Computing. 422Zhang, D., Guo, L., Karniadakis, G.E.: Learning in modal space: Solving time-dependent stochastic PDEs using physics-informed neural networks. SIAM Journal on Scientific Computing 42(2), A639-A665 (2020)</p>
<p>E Kharazmi, Z Zhang, G E Karniadakis, hp-VPINNs: Variational physics-informed neural networks with domain decomposition. 374113547Kharazmi, E., Zhang, Z., Karniadakis, G.E.: hp-VPINNs: Variational physics-informed neural networks with domain decomposition. Computer Methods in Applied Mechanics and Engineering 374, 113547 (2021)</p>
<p>Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems. A D Jagtap, E Kharazmi, G E Karniadakis, Computer Methods in Applied Mechanics and Engineering. 365113028Jagtap, A.D., Kharazmi, E., Karniadakis, G.E.: Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems. Computer Methods in Applied Mechanics and Engineering 365, 113028 (2020)</p>
<p>Extended Physics-Informed Neural Networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. A D Jagtap, G E Karniadakis, Communications in Computational Physics. 285Jagtap, A.D., Karniadakis, G.E.: Extended Physics-Informed Neural Networks (XPINNs): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. Communications in Computational Physics 28(5), 2002-2041 (2020)</p>
<p>K Shukla, A D Jagtap, G E Karniadakis, arXiv:2104.10013Parallel physicsinformed neural networks via domain decomposition. arXiv preprintShukla, K., Jagtap, A.D., Karniadakis, G.E.: Parallel physics- informed neural networks via domain decomposition. arXiv preprint arXiv:2104.10013 (2021)</p>
<p>O Hennigh, S Narasimhan, M A Nabian, A Subramaniam, K Tangsali, M Rietmann, J D A Ferrandis, W Byeon, Z Fang, S Choudhry, arXiv:2012.07938NVIDIA SimNetˆ{TM}: an AI-accelerated multiphysics simulation framework. arXiv preprintHennigh, O., Narasimhan, S., Nabian, M.A., Subramaniam, A., Tangsali, K., Rietmann, M., Ferrandis, J.d.A., Byeon, W., Fang, Z., Choudhry, S.: NVIDIA SimNetˆ{TM}: an AI-accelerated multi- physics simulation framework. arXiv preprint arXiv:2012.07938 (2020)</p>
<p>Adversarial uncertainty quantification in physics-informed neural networks. Y Yang, P Perdikaris, Journal of Computational Physics. 394Yang, Y., Perdikaris, P.: Adversarial uncertainty quantification in physics-informed neural networks. Journal of Computational Physics 394, 136-152 (2019)</p>
<p>Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems. D Zhang, L Lu, L Guo, G E Karniadakis, Journal of Computational Physics. 397108850Zhang, D., Lu, L., Guo, L., Karniadakis, G.E.: Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems. Journal of Computational Physics 397, 108850 (2019)</p>
<p>Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Y Zhu, N Zabaras, P S Koutsourelakis, P Perdikaris, Journal of Computational Physics. 394Zhu, Y., Zabaras, N., Koutsourelakis, P.S., Perdikaris, P.: Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics 394, 56-81 (2019)</p>
<p>Physics-constrained Bayesian neural network for fluid flow reconstruction with sparse and noisy data. L Sun, J X Wang, Theoretical and Applied Mechanics Letters. 103Sun, L., Wang, J.X.: Physics-constrained Bayesian neural network for fluid flow reconstruction with sparse and noisy data. Theoretical and Applied Mechanics Letters 10(3), 161-169 (2020)</p>
<p>B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data. L Yang, X Meng, G E Karniadakis, Journal of Computational Physics. 425109913Yang, L., Meng, X., Karniadakis, G.E.: B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data. Journal of Computational Physics 425, 109913 (2021)</p>
<p>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems. X Meng, G E Karniadakis, Journal of Computational Physics. 401109020Meng, X., Karniadakis, G.E.: A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems. Journal of Computational Physics 401, 109020 (2020)</p>
<p>Adaptive activation functions accelerate convergence in deep and physicsinformed neural networks. A D Jagtap, K Kawaguchi, G E Karniadakis, Journal of Computational Physics. 404109136Jagtap, A.D., Kawaguchi, K., Karniadakis, G.E.: Adaptive activation functions accelerate convergence in deep and physics- informed neural networks. Journal of Computational Physics 404, 109136 (2020)</p>
<p>Understanding and mitigating gradient pathologies in physics-informed neural networks. S Wang, Y Teng, P Perdikaris, arXiv:2001.04536arXiv preprintWang, S., Teng, Y., Perdikaris, P.: Understanding and mitigating gradient pathologies in physics-informed neural networks. arXiv preprint arXiv:2001.04536 (2020)</p>
<p>L Lu, R Pestourie, W Yao, Z Wang, F Verdugo, S G Johnson, arXiv:2102.04626Physics-informed neural networks with hard constraints for inverse design. arXiv preprintLu, L., Pestourie, R., Yao, W., Wang, Z., Verdugo, F., Johnson, S.G.: Physics-informed neural networks with hard constraints for inverse design. arXiv preprint arXiv:2102.04626 (2021)</p>
<p>PhyGeoNet: physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state PDEs on irregular domain. H Gao, L Sun, J X Wang, Journal of Computational Physics. 428110079Gao, H., Sun, L., Wang, J.X.: PhyGeoNet: physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state PDEs on irregular domain. Journal of Computational Physics 428, 110079 (2021)</p>
<p>On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs. Y Shin, J Darbon, G E Karniadakis, Communications in Computational Physics. 28Shin, Y., Darbon, J., Karniadakis, G.E.: On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs. Communications in Computational Physics 28, 2042-2074 (2020)</p>
<p>S Mishra, R Molinaro, arXiv:2006.16144Estimates on the generalization error of physics informed neural networks (PINNs) for approximating PDEs. arXiv preprintMishra, S., Molinaro, R.: Estimates on the generalization error of physics informed neural networks (PINNs) for approximating PDEs. arXiv preprint arXiv:2006.16144 (2020)</p>
<p>When and why PINNs fail to train: A neural tangent kernel perspective. S Wang, X Yu, P Perdikaris, arXiv:2007.14527arXiv preprintWang, S., Yu, X., Perdikaris, P.: When and why PINNs fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527 (2020)</p>
<p>Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive 4D flow MRI data using physics-informed neural networks. G Kissas, Y Yang, E Hwuang, W R Witschey, J A Detre, P Perdikaris, Computer Methods in Applied Mechanics and Engineering. 358112623Kissas, G., Yang, Y., Hwuang, E., Witschey, W.R., Detre, J.A., Perdikaris, P.: Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive 4D flow MRI data using physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering 358, 112623 (2020)</p>
<p>Predictive large-eddysimulation wall modeling via physics-informed neural networks. X Yang, S Zafar, J X Wang, H Xiao, Physical Review Fluids. 4334602Yang, X., Zafar, S., Wang, J.X., Xiao, H.: Predictive large-eddy- simulation wall modeling via physics-informed neural networks. Physical Review Fluids 4(3), 034602 (2019)</p>
<p>Q Lou, X Meng, G E Karniadakis, arXiv:2010.09147Physics-informed neural networks for solving forward and inverse flow problems via the Boltzmann-BGK formulation. arXiv preprintLou, Q., Meng, X., Karniadakis, G.E.: Physics-informed neural networks for solving forward and inverse flow problems via the Boltzmann-BGK formulation. arXiv preprint arXiv:2010.09147 (2020)</p>
<p>Physics-informed neural networks for heat transfer problems. S Cai, Z Wang, S Wang, P Perdikaris, G E Karniadakis, Journal of Heat Transfer. 143660801Cai, S., Wang, Z., Wang, S., Perdikaris, P., Karniadakis, G.E.: Physics-informed neural networks for heat transfer problems. Journal of Heat Transfer 143(6), 060801 (2021)</p>
<p>Flow over an espresso cup: inferring 3-D velocity and pressure fields from tomographic background oriented Schlieren via physics-informed neural networks. S Cai, Z Wang, F Fuest, Y J Jeon, C Gray, G E Karniadakis, Journal of Fluid Mechanics. 915Cai, S., Wang, Z., Fuest, F., Jeon, Y.J., Gray, C., Karniadakis, G.E.: Flow over an espresso cup: inferring 3-D velocity and pressure fields from tomographic background oriented Schlieren via physics-informed neural networks. Journal of Fluid Mechanics 915 (2021)</p>
<p>Deep learning of free boundary and stefan problems. S Wang, P Perdikaris, Journal of Computational Physics. 428109914Wang, S., Perdikaris, P.: Deep learning of free boundary and stefan problems. Journal of Computational Physics 428, 109914 (2021)</p>
<p>D Lucor, A Agrawal, A Sergent, arXiv:2103.03565Physics-aware deep neural networks for surrogate modeling of turbulent natural convection. arXiv preprintLucor, D., Agrawal, A., Sergent, A.: Physics-aware deep neural networks for surrogate modeling of turbulent natural convection. arXiv preprint arXiv:2103.03565 (2021)</p>
<p>Data-driven physicsinformed constitutive metamodeling of complex fluids: A multifidelity neural network (mfnn) framework. M Mahmoudabadbozchelou, M Caggioni, S Shahsavari, W H Hartt, G E Karniadakis, S Jamali, Journal of Rheology. 652Mahmoudabadbozchelou, M., Caggioni, M., Shahsavari, S., Hartt, W.H., Karniadakis, G.E., Jamali, S.: Data-driven physics- informed constitutive metamodeling of complex fluids: A multifidelity neural network (mfnn) framework. Journal of Rheology 65(2), 179-198 (2021)</p>
<p>Uncovering nearwall blood flow from sparse data with physics-informed neural networks. A Arzani, J X Wang, R M Souza, arXiv:2104.08249arXiv preprintArzani, A., Wang, J.X., D'Souza, R.M.: Uncovering near- wall blood flow from sparse data with physics-informed neural networks. arXiv preprint arXiv:2104.08249 (2021)</p>
<p>Artificial intelligence velocimetry and microaneurysmon-a-chip for three-dimensional analysis of blood flow in physiology and disease. S Cai, H Li, F Zheng, F Kong, M Dao, G E Karniadakis, S Suresh, Proceedings of the National Academy of Sciences. 11813Cai, S., Li, H., Zheng, F., Kong, F., Dao, M., Karniadakis, G.E., Suresh, S.: Artificial intelligence velocimetry and microaneurysm- on-a-chip for three-dimensional analysis of blood flow in physiology and disease. Proceedings of the National Academy of Sciences 118(13) (2021)</p>
<p>Towards physics-informed deep learning for turbulent flow prediction. R Wang, K Kashinath, M Mustafa, A Albert, R Yu, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningWang, R., Kashinath, K., Mustafa, M., Albert, A., Yu, R.: Towards physics-informed deep learning for turbulent flow prediction. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pp. 1457-1466 (2020)</p>
<p>Transfer learning enhanced physics informed neural network for phasefield modeling of fracture. S Goswami, C Anitescu, S Chakraborty, T Rabczuk, Theoretical and Applied Fracture Mechanics. 106102447Goswami, S., Anitescu, C., Chakraborty, S., Rabczuk, T.: Transfer learning enhanced physics informed neural network for phase- field modeling of fracture. Theoretical and Applied Fracture Mechanics 106, 102447 (2020)</p>
<p>E Zhang, M Yin, G E Karniadakis, arXiv:2009.04525Physics-informed neural networks for nonhomogeneous material identification in elasticity imaging. arXiv preprintZhang, E., Yin, M., Karniadakis, G.E.: Physics-informed neural networks for nonhomogeneous material identification in elasticity imaging. arXiv preprint arXiv:2009.04525 (2020)</p>
<p>The hydraulic permeability of blood clots as a function of fibrin and platelet density. A Wufsus, N Macera, K Neeves, Biophysical journal. 1048Wufsus, A., Macera, N., Neeves, K.: The hydraulic permeability of blood clots as a function of fibrin and platelet density. Biophysical journal 104(8), 1812-1823 (2013)</p>
<p>A three-dimensional phase-field model for multiscale modeling of thrombus biomechanics in blood vessels. X Zheng, A Yazdani, H Li, J D Humphrey, G E Karniadakis, PLOS Computational Biology. 1641007709Zheng, X., Yazdani, A., Li, H., Humphrey, J.D., Karniadakis, G.E.: A three-dimensional phase-field model for multiscale modeling of thrombus biomechanics in blood vessels. PLOS Computational Biology 16(4), e1007709 (2020)</p>
<p>A multiscale model of thrombus development. Z Xu, N Chen, M M Kamocka, E D Rosen, M Alber, Journal of the Royal Society Interface. 524Xu, Z., Chen, N., Kamocka, M.M., Rosen, E.D., Alber, M.: A multiscale model of thrombus development. Journal of the Royal Society Interface 5(24), 705-722 (2008)</p>
<p>A general shear-dependent model for thrombus formation. A Yazdani, H Li, J D Humphrey, G E Karniadakis, PLoS Computational Biology. 1311005291Yazdani, A., Li, H., Humphrey, J.D., Karniadakis, G.E.: A general shear-dependent model for thrombus formation. PLoS Computational Biology 13(1), e1005291 (2017)</p>
<p>Reinforcement learning for bluff body active flow control in experiments and simulations. D Fan, L Yang, Z Wang, M S Triantafyllou, G E Karniadakis, Proceedings of the National Academy of Sciences. 11742Fan, D., Yang, L., Wang, Z., Triantafyllou, M.S., Karniadakis, G.E.: Reinforcement learning for bluff body active flow control in experiments and simulations. Proceedings of the National Academy of Sciences 117(42), 26091-26098 (2020)</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4922 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4922</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4922</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-150a301567c679ef4a7113639032b626d9a7a4ae</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/150a301567c679ef4a7113639032b626d9a7a4ae" target="_blank">Comprehensible Context-driven Text Game Playing</a></p>
                <p><strong>Paper Venue:</strong> 2019 IEEE Conference on Games (CoG)</p>
                <p><strong>Paper TL;DR:</strong> This paper uses a fast CNN to encode position-and syntax-oriented structures extracted from observed texts as states as states and augments the reward signal in a universal and practical manner to learn a superior agent.</p>
                <p><strong>Paper Abstract:</strong> In order to train a computer agent to play a text-based computer game, we must represent each hidden state of the game. A Long Short-Term Memory (LSTM) model running over observed texts is a common choice for state construction. However, a normal Deep Q-learning Network (DQN) for such an agent requires millions of steps of training or more to converge. As such, an LSTM-based DQN can take tens of days to finish the training process. Though we can use a Convolutional Neural Network (CNN) as a text-encoder to construct states much faster than the LSTM, doing so without an understanding of the syntactic context of the words being analyzed can slow convergence. In this paper, we use a fast CNN to encode position-and syntax-oriented structures extracted from observed texts as states. We additionally augment the reward signal in a universal and practical manner. Together, we show that our improvements can not only speed up the process by one order of magnitude but also learn a superior agent.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4922.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4922.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DQN agent that uses an LSTM recurrent encoder to convert the concatenated trajectory (sequence of game master/action texts) into a hidden state for Q-value prediction; the LSTM's hidden state provides a token-by-token memory of the trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep Q-Network where the state encoder is an LSTM that processes trajectory text token-by-token and outputs a final hidden state used as the game state representation for Q-value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Play Zork I and sub-tasks (egg quest, troll quest, complete game) by selecting actions from a constrained action set to maximize score.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent internal memory (LSTM hidden state) plus an external replay buffer (experience replay).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>LSTM internal hidden vectors representing token history; replay memory stores tuples (s, s', a, r) where s and s' are trajectory-encoded states (concatenated observed text).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>LSTM hidden state updated token-by-token when encoding trajectories; replay memory appended with (s,s',a,r) at every environment step.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>LSTM uses its recurrent hidden state to attend implicitly to past tokens; replay memory is sampled during training (uniform, weighted, or prioritized strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On egg quest: converged to a score of 25 but required ~7 hours of training (far fewer epochs than CNN-based models in the same time). On troll quest: could not be well-trained within 45 hours and showed poor performance there (did not reach competitive scores in available training time).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Direct comparison of LSTM encoder vs CNN encoders: LSTM is much slower (token-by-token) and requires many fewer epochs per wall-clock time; performs adequately on a small subtask (egg) but scales poorly (fails to train well on troll quest within long runtimes).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>LSTM encoder makes training prohibitively slow for full games (can take tens of days to converge); recurrence encodes long sequences sequentially which is computationally expensive; limited wall-clock training throughput compared to CNN encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>When training DQN agents on long text trajectories, avoid LSTM encoders if wall-clock training time is constrained; use faster encoders (CNN + position embeddings) unless LSTM's sequential modeling is specifically required and computational budget allows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comprehensible Context-driven Text Game Playing', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4922.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4922.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN-DQN (max-pool + pos. emb.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional Neural Network Deep Q-Network with max-pooling and position embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DQN agent that encodes concatenated trajectory text using 1-D convolutional filters, max-pooling over time (interpreted as pseudo-attention), and position embeddings to preserve token order; used as the primary encoder in experiments for speed and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Convolutional neural networks for sentence classification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CNN-DQN (max-pooling + position embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep Q-Network where states are encoded by a CNN with multiple kernel sizes, a max-pooling layer (interpreted as an auto-attention mechanism), and learned position embeddings concatenated with word embeddings to retain position information.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Play Zork I and sub-tasks (egg, troll, full game) to maximize score; experiments evaluate convergence speed and maximal achieved scores.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External experience replay buffer; no recurrent internal memory in encoder (stateless CNN), but position embeddings encode order information.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Replay buffer stores tuples (s, s', a, r) where s and s' are CNN-encoded trajectory vectors; position embeddings store token position information concatenated with word embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Replay memory appended each step; CNN encoder processes the current trajectory (trimmed to last N sentences) at each step — no recurrent state to update across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>During training, samples are retrieved from replay buffer using uniform, weighted, or prioritized sampling; within a single forward pass the max-pooling layer selects salient n-gram features (pseudo-attention) from the trajectory encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Faster convergence and higher scores: on egg quest CNN max-pool with position embeddings converged fastest (about 0.5 hours) to score 25; on troll quest max-pool + position embeddings converged in ~5 hours to higher scores; on full Zork achieved a stable score of 40 at ~1 million steps when using priority replay + position embeddings (and improved further when dependency reordering added).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Compared mean-pooling vs max-pooling: max-pooling produced more stable and higher performance (interpreted as auto-attention). Position embeddings significantly improved speed and final performance. Dependency-parser reordering of trajectories (see separate entity) further sped convergence (~0.5M steps faster). Sampling strategy impacts results: weighted sampling faster on single-quest tasks, prioritized replay necessary for multi-quest exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Vanilla CNN loses long-range syntactic relationships (local n-gram locality), which can harm decision-making unless position embeddings and dependency reordering are used; CNN is stateless so relies entirely on trajectory window and replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use CNN with max-pooling and position embeddings for faster training; interpret max-pool as pseudo-attention to inspect important tokens; for multi-quest games use prioritized experience replay; apply dependency parser reordering to group syntactically-related tokens; use reward clipping and shaping as described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comprehensible Context-driven Text Game Playing', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4922.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4922.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Replay memory (experience replay)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experience Replay Buffer (DQN replay memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external buffer storing training samples (s, s', a, r) collected during interaction, from which minibatches are sampled to train the DQN; used to decorrelate samples and stabilize training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-level control through deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN agents (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DQN-based agents store environment transitions (trajectory-encoded states and actions) in a replay buffer and sample minibatches to perform Q-learning updates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Store and sample gameplay transitions to train DQN agents to play Zork I and sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Off-policy experience replay (external buffer of past transitions).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Tuples (s, s', a, r) where s and s' are trajectory-encoded states (concatenated master/action text encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>At every environment step, the agent appends the new transition (s, s', a, r) to the replay buffer (with a fixed maximum size; older samples evicted).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Sampling strategies: uniform sampling; weighted sampling by exp(r); prioritized experience replay based on temporal-difference error magnitude |δ_i| (with importance-sampling correction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Experience replay is integral to all DQN experiments; sampling strategy affects performance: on troll quest, weighted sampling converged faster than prioritized sampling in that experiment, but for full Zork prioritized sampling was necessary to avoid getting stuck in a single sub-quest (egg).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper experiments with different sampling strategies from replay memory: uniform (bad due to reward imbalance), weighted by reward (exp(r)) which speeds convergence on single-quest tasks, and prioritized experience replay (by |δ|) which is required to explore multi-quest games and avoid local maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Replay buffer distributions are highly imbalanced (positive-reward transitions are rare), causing uniform sampling to miss important transitions; prioritized sampling can introduce bias which must be corrected by importance weights; choice of sampling strategy is game-dependent (single-quest vs multi-quest).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Clip rewards (Huber/[-1,1]) to avoid outliers; use weighted sampling for single-quest tasks with rare positive rewards; use prioritized experience replay for multi-quest games to encourage exploration of rare useful transitions and avoid collapsing to local maxima; apply importance-sampling correction when using prioritized replay.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comprehensible Context-driven Text Game Playing', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4922.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4922.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weighted sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-weighted sampling (exp(r))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling strategy for replay memory that assigns sampling probability proportional to exp(r_i), giving higher probability to positive-reward samples to counter class imbalance in stored transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN agents (with weighted sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DQN trained using replay buffer minibatches sampled by weight proportional to exp(r) (with reward clipping to limit domination by large rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I (egg quest, troll quest)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Counteract the extreme imbalance of negative/zero vs positive reward transitions in replay memory so the agent learns from rare positive rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Sampling weighting strategy applied to the external replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Same replay tuples (s,s',a,r); weights computed from instant reward r as w = exp(r) (with reward clipping applied).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Weights are computed at sampling time based on stored r (recompute or store precomputed) and used to draw minibatches.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Probability P(i) = w_i / sum_j w_j with importance of rare positive samples increased; no additional retrieval mechanism beyond weighted sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On single-quest tasks (troll and egg sub-tasks) weighted sampling led to faster convergence than uniform sampling; in the troll-quest experiment weighted sampling converged faster than prioritized experience sampling (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Empirical comparison: weighted sampling vs prioritized experience replay on troll quest — both reach similar final scores but weighted sampling converges faster in that single-quest setting; for multi-quest full Zork, weighted sampling led to local-maxima collapse (stuck in egg quest) and prioritized replay was preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Can be inappropriate for multi-quest games because over-sampling transitions for a single frequently-rewarded sub-trajectory can cause the agent to converge to local maxima (fail to explore other quests). Requires reward clipping to prevent domination by large rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use weighted sampling for single-quest tasks or when positive samples are extremely rare; use reward clipping; switch to prioritized experience replay for multi-quest environments to avoid collapsing to one subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comprehensible Context-driven Text Game Playing', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4922.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4922.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prioritized replay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Experience Replay</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A replay sampling method that prioritizes transitions with large temporal-difference errors (|δ|) to focus updates on surprising/learning-significant samples, with importance-sampling correction to reduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prioritized experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN agents (with prioritized replay)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DQN variant that stores transitions in a replay buffer and samples according to priority weights w_i = |δ_i| + ε, using a hyperparameterized sampling exponent and importance-sampling correction for gradient updates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve sampling from replay memory to accelerate learning and exploration across multiple quests in Zork I.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Priority-weighted experience replay (external buffer with TD-error-based priorities).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Transitions (s,s',a,r) with computed priority based on TD-error magnitude |δ|.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Priorities updated as TD-errors change; new transitions appended with initial small epsilon priority; importance-sampling exponent b annealed during training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Sampling probability P(i) proportional to w_i^a; gradients scaled by (1/(N*P(i)))^b importance weights during updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On full Zork, prioritized experience replay was necessary to avoid the agent converging only on the egg sub-quest; switching from weighted sampling to prioritized replay enabled exploration beyond the egg and improved final scores (see Figure 6: weighted sampling resulted in agents stuck at ~5 points, prioritized replay increased exploration and scores).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Compared weighted sampling vs prioritized replay: weighted sampling faster for single-quest but prioritized replay required for multi-quest full-game exploration. The paper also mentions using importance-sampling corrections and annealing hyperparameters (a and b).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Prioritized replay can bias training and must use importance-sampling correction; tuned hyperparameters (a, b) are needed; on single-quest problems weighted reward-based sampling may converge faster.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use prioritized experience replay for multi-quest or sparse-reward text games to avoid local maxima and improve exploration; apply importance-sampling correction and anneal parameters appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comprehensible Context-driven Text Game Playing', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4922.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4922.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dependency reordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependency-parser-based trajectory reordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing step that reorders tokens in trajectory text according to dependency-parse subtrees (breadth-first traversal) so syntactically related tokens are contiguous for CNN filters, improving CNN's ability to encode syntactic chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A fast and accurate dependency parser using neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CNN-DQN (with dependency reordering)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same CNN-DQN agent but trajectories are reordered according to dependency-parsed subtrees (with padding between sub-sentences) before convolution to make syntactic relations local for CNN kernels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Make syntactically related tokens contiguous in the trajectory encoding to improve CNN's local receptive-field representations and speed convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Preprocessing transformation of the input trajectory text; relies on the same replay buffer for transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Trajectory text reordered into dependency-subtree blocks separated by padding tokens; then CNN encodes these reordered sequences into state vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Applied to trajectories before encoding at each step; not a dynamic memory store but a transformation of input fed to the encoder each time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Not an explicit retrieval mechanism; it alters what information becomes local and thus retrievable by convolutional filters and max-pooling (pseudo-attention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Dependency parser reordering improved convergence on full Zork: with reordering the agent converged in ~1.2M steps (about 0.5M steps faster than without), and improved final performance when combined with position embeddings and prioritized replay (Figure 6 purple curve).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comparative experiment: CNN-DQN with position embeddings vs same with dependency reordering; reordering yielded faster convergence and better performance, showing that syntactic-localization benefits CNN encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires a dependency parser (additional preprocessing cost) and design choices (padding between blocks) to prevent filters striding across subtrees; may not be perfect for all sentence structures and adds complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Apply dependency-parser-based reordering when using CNN encoders on long or syntactically complex narrative text to make syntactic relationships local for convolutional kernels; combine with position embeddings and prioritized replay for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comprehensible Context-driven Text Game Playing', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4922.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4922.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Repeated-penalty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accumulated repeated-bad-tries penalty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corrective reward scheme that accumulates a small negative penalty when the agent repeats the same bad (non-rewarding) transition multiple times, to discourage loops of repeated unsuccessful actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN agents (with repeated-penalty)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DQN trained with an added mechanic that applies -0.1 (cumulative if repeated) to transitions that are repeated bad tries (same negative-reward sample seen consecutively), reducing looped behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Zork I</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prevent the agent from getting stuck repeating the same negative/no-reward actions (repeated bad tries) and accelerate exploration and convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Reward-shaping applied based on detecting repeated transitions in the replay buffer / interaction history (a form of short-term repetition memory detection).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Detection of identical negative-reward samples (transition equality) in recent steps; accumulative counter used to increase penalty if repetition persists.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>At runtime, when a repeated negative sample is observed consecutively, a -0.1 penalty is added and accumulated for immediate subsequent repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Checks for equality with the immediately previous bad sample(s) to decide whether to apply/accumulate penalty (local history matching).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Applying repeated-penalty reduced repeated bad tries from 8.73% to 3.51% of training steps and accelerated convergence: with repeated penalty convergence happened in ~0.5M training steps (compared to slower without it); combined with dependency reordering gave additive benefit and converged in ~0.25M steps for that experiment (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Measured fraction of repeated bad tries and training curves with and without penalty; the penalty lowered repetition rate and sped up convergence significantly; benefit is additive with dependency reordering.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires defining what constitutes 'same negative sample' and tuning penalty magnitude; aggressive penalties could discourage necessary repeated exploration in stochastic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Apply a small accumulative negative penalty for immediate repeated identical negative transitions to reduce stuck-loop behavior; monitor replay statistics to tune penalty magnitude so exploration is not overly discouraged.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comprehensible Context-driven Text Game Playing', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Prioritized experience replay <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Convolutional neural networks for sentence classification <em>(Rating: 1)</em></li>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>A fast and accurate dependency parser using neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4922",
    "paper_id": "paper-150a301567c679ef4a7113639032b626d9a7a4ae",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "LSTM-DQN",
            "name_full": "Long Short-Term Memory Deep Q-Network",
            "brief_description": "A DQN agent that uses an LSTM recurrent encoder to convert the concatenated trajectory (sequence of game master/action texts) into a hidden state for Q-value prediction; the LSTM's hidden state provides a token-by-token memory of the trajectory.",
            "citation_title": "Long short-term memory",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN",
            "agent_description": "Deep Q-Network where the state encoder is an LSTM that processes trajectory text token-by-token and outputs a final hidden state used as the game state representation for Q-value estimation.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Zork I",
            "task_description": "Play Zork I and sub-tasks (egg quest, troll quest, complete game) by selecting actions from a constrained action set to maximize score.",
            "memory_used": true,
            "memory_type": "Recurrent internal memory (LSTM hidden state) plus an external replay buffer (experience replay).",
            "memory_representation": "LSTM internal hidden vectors representing token history; replay memory stores tuples (s, s', a, r) where s and s' are trajectory-encoded states (concatenated observed text).",
            "memory_update_mechanism": "LSTM hidden state updated token-by-token when encoding trajectories; replay memory appended with (s,s',a,r) at every environment step.",
            "memory_retrieval_mechanism": "LSTM uses its recurrent hidden state to attend implicitly to past tokens; replay memory is sampled during training (uniform, weighted, or prioritized strategies).",
            "performance_with_memory": "On egg quest: converged to a score of 25 but required ~7 hours of training (far fewer epochs than CNN-based models in the same time). On troll quest: could not be well-trained within 45 hours and showed poor performance there (did not reach competitive scores in available training time).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Direct comparison of LSTM encoder vs CNN encoders: LSTM is much slower (token-by-token) and requires many fewer epochs per wall-clock time; performs adequately on a small subtask (egg) but scales poorly (fails to train well on troll quest within long runtimes).",
            "challenges_or_limitations": "LSTM encoder makes training prohibitively slow for full games (can take tens of days to converge); recurrence encodes long sequences sequentially which is computationally expensive; limited wall-clock training throughput compared to CNN encoders.",
            "best_practices_or_recommendations": "When training DQN agents on long text trajectories, avoid LSTM encoders if wall-clock training time is constrained; use faster encoders (CNN + position embeddings) unless LSTM's sequential modeling is specifically required and computational budget allows.",
            "uuid": "e4922.0",
            "source_info": {
                "paper_title": "Comprehensible Context-driven Text Game Playing",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "CNN-DQN (max-pool + pos. emb.)",
            "name_full": "Convolutional Neural Network Deep Q-Network with max-pooling and position embeddings",
            "brief_description": "A DQN agent that encodes concatenated trajectory text using 1-D convolutional filters, max-pooling over time (interpreted as pseudo-attention), and position embeddings to preserve token order; used as the primary encoder in experiments for speed and performance.",
            "citation_title": "Convolutional neural networks for sentence classification",
            "mention_or_use": "use",
            "agent_name": "CNN-DQN (max-pooling + position embeddings)",
            "agent_description": "Deep Q-Network where states are encoded by a CNN with multiple kernel sizes, a max-pooling layer (interpreted as an auto-attention mechanism), and learned position embeddings concatenated with word embeddings to retain position information.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Zork I",
            "task_description": "Play Zork I and sub-tasks (egg, troll, full game) to maximize score; experiments evaluate convergence speed and maximal achieved scores.",
            "memory_used": true,
            "memory_type": "External experience replay buffer; no recurrent internal memory in encoder (stateless CNN), but position embeddings encode order information.",
            "memory_representation": "Replay buffer stores tuples (s, s', a, r) where s and s' are CNN-encoded trajectory vectors; position embeddings store token position information concatenated with word embeddings.",
            "memory_update_mechanism": "Replay memory appended each step; CNN encoder processes the current trajectory (trimmed to last N sentences) at each step — no recurrent state to update across steps.",
            "memory_retrieval_mechanism": "During training, samples are retrieved from replay buffer using uniform, weighted, or prioritized sampling; within a single forward pass the max-pooling layer selects salient n-gram features (pseudo-attention) from the trajectory encoding.",
            "performance_with_memory": "Faster convergence and higher scores: on egg quest CNN max-pool with position embeddings converged fastest (about 0.5 hours) to score 25; on troll quest max-pool + position embeddings converged in ~5 hours to higher scores; on full Zork achieved a stable score of 40 at ~1 million steps when using priority replay + position embeddings (and improved further when dependency reordering added).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Compared mean-pooling vs max-pooling: max-pooling produced more stable and higher performance (interpreted as auto-attention). Position embeddings significantly improved speed and final performance. Dependency-parser reordering of trajectories (see separate entity) further sped convergence (~0.5M steps faster). Sampling strategy impacts results: weighted sampling faster on single-quest tasks, prioritized replay necessary for multi-quest exploration.",
            "challenges_or_limitations": "Vanilla CNN loses long-range syntactic relationships (local n-gram locality), which can harm decision-making unless position embeddings and dependency reordering are used; CNN is stateless so relies entirely on trajectory window and replay buffer.",
            "best_practices_or_recommendations": "Use CNN with max-pooling and position embeddings for faster training; interpret max-pool as pseudo-attention to inspect important tokens; for multi-quest games use prioritized experience replay; apply dependency parser reordering to group syntactically-related tokens; use reward clipping and shaping as described.",
            "uuid": "e4922.1",
            "source_info": {
                "paper_title": "Comprehensible Context-driven Text Game Playing",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Replay memory (experience replay)",
            "name_full": "Experience Replay Buffer (DQN replay memory)",
            "brief_description": "An external buffer storing training samples (s, s', a, r) collected during interaction, from which minibatches are sampled to train the DQN; used to decorrelate samples and stabilize training.",
            "citation_title": "Human-level control through deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "DQN agents (general)",
            "agent_description": "DQN-based agents store environment transitions (trajectory-encoded states and actions) in a replay buffer and sample minibatches to perform Q-learning updates.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Zork I",
            "task_description": "Store and sample gameplay transitions to train DQN agents to play Zork I and sub-tasks.",
            "memory_used": true,
            "memory_type": "Off-policy experience replay (external buffer of past transitions).",
            "memory_representation": "Tuples (s, s', a, r) where s and s' are trajectory-encoded states (concatenated master/action text encodings).",
            "memory_update_mechanism": "At every environment step, the agent appends the new transition (s, s', a, r) to the replay buffer (with a fixed maximum size; older samples evicted).",
            "memory_retrieval_mechanism": "Sampling strategies: uniform sampling; weighted sampling by exp(r); prioritized experience replay based on temporal-difference error magnitude |δ_i| (with importance-sampling correction).",
            "performance_with_memory": "Experience replay is integral to all DQN experiments; sampling strategy affects performance: on troll quest, weighted sampling converged faster than prioritized sampling in that experiment, but for full Zork prioritized sampling was necessary to avoid getting stuck in a single sub-quest (egg).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "The paper experiments with different sampling strategies from replay memory: uniform (bad due to reward imbalance), weighted by reward (exp(r)) which speeds convergence on single-quest tasks, and prioritized experience replay (by |δ|) which is required to explore multi-quest games and avoid local maxima.",
            "challenges_or_limitations": "Replay buffer distributions are highly imbalanced (positive-reward transitions are rare), causing uniform sampling to miss important transitions; prioritized sampling can introduce bias which must be corrected by importance weights; choice of sampling strategy is game-dependent (single-quest vs multi-quest).",
            "best_practices_or_recommendations": "Clip rewards (Huber/[-1,1]) to avoid outliers; use weighted sampling for single-quest tasks with rare positive rewards; use prioritized experience replay for multi-quest games to encourage exploration of rare useful transitions and avoid collapsing to local maxima; apply importance-sampling correction when using prioritized replay.",
            "uuid": "e4922.2",
            "source_info": {
                "paper_title": "Comprehensible Context-driven Text Game Playing",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Weighted sampling",
            "name_full": "Reward-weighted sampling (exp(r))",
            "brief_description": "A sampling strategy for replay memory that assigns sampling probability proportional to exp(r_i), giving higher probability to positive-reward samples to counter class imbalance in stored transitions.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "DQN agents (with weighted sampling)",
            "agent_description": "DQN trained using replay buffer minibatches sampled by weight proportional to exp(r) (with reward clipping to limit domination by large rewards).",
            "llm_model_name": null,
            "game_or_benchmark_name": "Zork I (egg quest, troll quest)",
            "task_description": "Counteract the extreme imbalance of negative/zero vs positive reward transitions in replay memory so the agent learns from rare positive rewards.",
            "memory_used": true,
            "memory_type": "Sampling weighting strategy applied to the external replay buffer.",
            "memory_representation": "Same replay tuples (s,s',a,r); weights computed from instant reward r as w = exp(r) (with reward clipping applied).",
            "memory_update_mechanism": "Weights are computed at sampling time based on stored r (recompute or store precomputed) and used to draw minibatches.",
            "memory_retrieval_mechanism": "Probability P(i) = w_i / sum_j w_j with importance of rare positive samples increased; no additional retrieval mechanism beyond weighted sampling.",
            "performance_with_memory": "On single-quest tasks (troll and egg sub-tasks) weighted sampling led to faster convergence than uniform sampling; in the troll-quest experiment weighted sampling converged faster than prioritized experience sampling (Figure 4).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Empirical comparison: weighted sampling vs prioritized experience replay on troll quest — both reach similar final scores but weighted sampling converges faster in that single-quest setting; for multi-quest full Zork, weighted sampling led to local-maxima collapse (stuck in egg quest) and prioritized replay was preferred.",
            "challenges_or_limitations": "Can be inappropriate for multi-quest games because over-sampling transitions for a single frequently-rewarded sub-trajectory can cause the agent to converge to local maxima (fail to explore other quests). Requires reward clipping to prevent domination by large rewards.",
            "best_practices_or_recommendations": "Use weighted sampling for single-quest tasks or when positive samples are extremely rare; use reward clipping; switch to prioritized experience replay for multi-quest environments to avoid collapsing to one subtask.",
            "uuid": "e4922.3",
            "source_info": {
                "paper_title": "Comprehensible Context-driven Text Game Playing",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Prioritized replay",
            "name_full": "Prioritized Experience Replay",
            "brief_description": "A replay sampling method that prioritizes transitions with large temporal-difference errors (|δ|) to focus updates on surprising/learning-significant samples, with importance-sampling correction to reduce bias.",
            "citation_title": "Prioritized experience replay",
            "mention_or_use": "use",
            "agent_name": "DQN agents (with prioritized replay)",
            "agent_description": "DQN variant that stores transitions in a replay buffer and samples according to priority weights w_i = |δ_i| + ε, using a hyperparameterized sampling exponent and importance-sampling correction for gradient updates.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Zork I",
            "task_description": "Improve sampling from replay memory to accelerate learning and exploration across multiple quests in Zork I.",
            "memory_used": true,
            "memory_type": "Priority-weighted experience replay (external buffer with TD-error-based priorities).",
            "memory_representation": "Transitions (s,s',a,r) with computed priority based on TD-error magnitude |δ|.",
            "memory_update_mechanism": "Priorities updated as TD-errors change; new transitions appended with initial small epsilon priority; importance-sampling exponent b annealed during training.",
            "memory_retrieval_mechanism": "Sampling probability P(i) proportional to w_i^a; gradients scaled by (1/(N*P(i)))^b importance weights during updates.",
            "performance_with_memory": "On full Zork, prioritized experience replay was necessary to avoid the agent converging only on the egg sub-quest; switching from weighted sampling to prioritized replay enabled exploration beyond the egg and improved final scores (see Figure 6: weighted sampling resulted in agents stuck at ~5 points, prioritized replay increased exploration and scores).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Compared weighted sampling vs prioritized replay: weighted sampling faster for single-quest but prioritized replay required for multi-quest full-game exploration. The paper also mentions using importance-sampling corrections and annealing hyperparameters (a and b).",
            "challenges_or_limitations": "Prioritized replay can bias training and must use importance-sampling correction; tuned hyperparameters (a, b) are needed; on single-quest problems weighted reward-based sampling may converge faster.",
            "best_practices_or_recommendations": "Use prioritized experience replay for multi-quest or sparse-reward text games to avoid local maxima and improve exploration; apply importance-sampling correction and anneal parameters appropriately.",
            "uuid": "e4922.4",
            "source_info": {
                "paper_title": "Comprehensible Context-driven Text Game Playing",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Dependency reordering",
            "name_full": "Dependency-parser-based trajectory reordering",
            "brief_description": "A preprocessing step that reorders tokens in trajectory text according to dependency-parse subtrees (breadth-first traversal) so syntactically related tokens are contiguous for CNN filters, improving CNN's ability to encode syntactic chunks.",
            "citation_title": "A fast and accurate dependency parser using neural networks",
            "mention_or_use": "use",
            "agent_name": "CNN-DQN (with dependency reordering)",
            "agent_description": "Same CNN-DQN agent but trajectories are reordered according to dependency-parsed subtrees (with padding between sub-sentences) before convolution to make syntactic relations local for CNN kernels.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Zork I",
            "task_description": "Make syntactically related tokens contiguous in the trajectory encoding to improve CNN's local receptive-field representations and speed convergence.",
            "memory_used": true,
            "memory_type": "Preprocessing transformation of the input trajectory text; relies on the same replay buffer for transitions.",
            "memory_representation": "Trajectory text reordered into dependency-subtree blocks separated by padding tokens; then CNN encodes these reordered sequences into state vectors.",
            "memory_update_mechanism": "Applied to trajectories before encoding at each step; not a dynamic memory store but a transformation of input fed to the encoder each time.",
            "memory_retrieval_mechanism": "Not an explicit retrieval mechanism; it alters what information becomes local and thus retrievable by convolutional filters and max-pooling (pseudo-attention).",
            "performance_with_memory": "Dependency parser reordering improved convergence on full Zork: with reordering the agent converged in ~1.2M steps (about 0.5M steps faster than without), and improved final performance when combined with position embeddings and prioritized replay (Figure 6 purple curve).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Comparative experiment: CNN-DQN with position embeddings vs same with dependency reordering; reordering yielded faster convergence and better performance, showing that syntactic-localization benefits CNN encoders.",
            "challenges_or_limitations": "Requires a dependency parser (additional preprocessing cost) and design choices (padding between blocks) to prevent filters striding across subtrees; may not be perfect for all sentence structures and adds complexity.",
            "best_practices_or_recommendations": "Apply dependency-parser-based reordering when using CNN encoders on long or syntactically complex narrative text to make syntactic relationships local for convolutional kernels; combine with position embeddings and prioritized replay for best results.",
            "uuid": "e4922.5",
            "source_info": {
                "paper_title": "Comprehensible Context-driven Text Game Playing",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Repeated-penalty",
            "name_full": "Accumulated repeated-bad-tries penalty",
            "brief_description": "A corrective reward scheme that accumulates a small negative penalty when the agent repeats the same bad (non-rewarding) transition multiple times, to discourage loops of repeated unsuccessful actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DQN agents (with repeated-penalty)",
            "agent_description": "DQN trained with an added mechanic that applies -0.1 (cumulative if repeated) to transitions that are repeated bad tries (same negative-reward sample seen consecutively), reducing looped behavior.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Zork I",
            "task_description": "Prevent the agent from getting stuck repeating the same negative/no-reward actions (repeated bad tries) and accelerate exploration and convergence.",
            "memory_used": true,
            "memory_type": "Reward-shaping applied based on detecting repeated transitions in the replay buffer / interaction history (a form of short-term repetition memory detection).",
            "memory_representation": "Detection of identical negative-reward samples (transition equality) in recent steps; accumulative counter used to increase penalty if repetition persists.",
            "memory_update_mechanism": "At runtime, when a repeated negative sample is observed consecutively, a -0.1 penalty is added and accumulated for immediate subsequent repeats.",
            "memory_retrieval_mechanism": "Checks for equality with the immediately previous bad sample(s) to decide whether to apply/accumulate penalty (local history matching).",
            "performance_with_memory": "Applying repeated-penalty reduced repeated bad tries from 8.73% to 3.51% of training steps and accelerated convergence: with repeated penalty convergence happened in ~0.5M training steps (compared to slower without it); combined with dependency reordering gave additive benefit and converged in ~0.25M steps for that experiment (Figure 7).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Measured fraction of repeated bad tries and training curves with and without penalty; the penalty lowered repetition rate and sped up convergence significantly; benefit is additive with dependency reordering.",
            "challenges_or_limitations": "Requires defining what constitutes 'same negative sample' and tuning penalty magnitude; aggressive penalties could discourage necessary repeated exploration in stochastic environments.",
            "best_practices_or_recommendations": "Apply a small accumulative negative penalty for immediate repeated identical negative transitions to reduce stuck-loop behavior; monitor replay statistics to tune penalty magnitude so exploration is not overly discouraged.",
            "uuid": "e4922.6",
            "source_info": {
                "paper_title": "Comprehensible Context-driven Text Game Playing",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Prioritized experience replay",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Convolutional neural networks for sentence classification",
            "rating": 1
        },
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "A fast and accurate dependency parser using neural networks",
            "rating": 1
        }
    ],
    "cost": 0.016167,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Comprehensible Context-driven Text Game Playing</h1>
<p>Xusen Yin<br>Information Sciences Institute<br>University of Southern California<br>Marina del Rey, CA<br>xusenyin@usc.edu</p>
<p>Jonathan May<br>Information Sciences Institute<br>University of Southern California<br>Marina del Rey, CA<br>jonmay@isi.edu</p>
<h4>Abstract</h4>
<p>In order to train a computer agent to play a textbased computer game, we must represent each hidden state of the game. A Long Short-Term Memory (LSTM) model running over observed texts is a common choice for state construction. However, a normal Deep Q-learning Network (DQN) for such an agent requires millions of steps of training or more to converge. As such, an LSTM-based DQN can take tens of days to finish the training process. Though we can use a Convolutional Neural Network (CNN) as a text-encoder to construct states much faster than the LSTM, doing so without an understanding of the syntactic context of the words being analyzed can slow convergence. In this paper, we use a fast CNN to encode positionand syntax-oriented structures extracted from observed texts as states. We additionally augment the reward signal in a universal and practical manner. Together, we show that our improvements can not only speed up the process by one order of magnitude but also learn a superior agent.</p>
<p>Index Terms-text-based games, trajectory, dependency parser, auto-attention</p>
<h2>I. INTRODUCTION</h2>
<p>Ever since the work of [1] in learning to play Atari games, the question of whether we could also learn to play textbased games has naturally arisen [2]-[4]. However, the goal of reaching par with human players on these games is still beyond our reach, unlike that shown for Atari games. Textbased games, especially those designed for real human players, are elaborately built and hence sophisticated. Zork [4], [5] is one such game, with more than 30 rooms to explore, and combines a maze, trivia, combat, time-sensitive tasks, puzzles, and stochastic events. Most attempts to automatically learn to play real text games can only explore a few rooms of a game, achieving about 10 percent of the total available score.</p>
<p>Unlike Atari action games where one uses the joystick to play, and thus has up to 18 different actions available (including a button press), a player uses brief natural language sentences as actions to interact with text-based games. The number of valid actions is thus theoretically infinite, and even when the vocabulary and maximum action length are limited, can still be in the hundreds or even thousands. This makes policy-based learning quite difficult.</p>
<p>The Deep Q-learning Network (DQN), first introduced by [1], is also the main method used to play text-based games. One key component of the DQN as applied to text games is its encoding of context sentences into hidden states to represent game state. As the length of context sentences can vary from a
few words to thousands of words, the LSTM [6] is seemingly a natural choice for this task [4], [7]-[9].</p>
<p>However, one important limitation of DQN learning is that it usually needs millions of training steps to converge, and this can take days with the LSTM as the encoder, even for a small quest with only tens of actions. Since full-fledged textbased games usually involve multiple quests and have many more actions available to try, training an LSTM-based DQN agent for text-based games such that it may reach a level of quality similar to that observed for Atari games is functionally impossible.
[10] shows that we can use a convolutional neural network (CNN) as a text encoder in classification tasks for faster training. [3] uses a similar CNN encoder to build a DQN agent. However, previous work of building DQN agents focuses on DQN architecture [8], [9], [11] or the generation and selection of actions [2], but lacks much analysis of the observed context sentences.</p>
<p>In this paper, we focus on the analysis of observed texts and how they work with different encoder architectures. We show that both context sentence processing and encoder selection can lead to faster convergence of the DQN training process and result in superior agents. After analyzing the training process, we also find that instant reward manipulation and sample strategies can affect the training process. Furthermore, we observe that the CNN encoder with a max-pooling layer can be treated as an auto-attention mechanism in finding key components in the context to make decisions. Our final trained agent on Zork can reach state-of-the-art scores within one million training steps, or about 10 hours. ${ }^{1}$</p>
<p>The novel contributions of our paper are:</p>
<ul>
<li>We compare different encoders for the DQN framework, and determine that using CNN with a max-pooling layer as the encoder is ideal, and has the extra benefit that max-pooling functions as an auto-attention mechanism.</li>
<li>We use position embeddings when encoding trajectories to reach the state-of-the-art result on Zork;</li>
<li>We use a dependency parser to reorder game context sentences such that syntactically related elements are close to each other, which leads to a halving of our DQN agent's convergence time. We use reward shaping methods for further better training results.
${ }^{1}$ Our code is available at https://github.com/yinxusen/dqn-zork</li>
</ul>
<h2>II. MethodS</h2>
<h2>A. Text-based games</h2>
<p>When playing a text-based game, the game first outputs a sentence to describe the current environment. Then a player inputs a sentence as the action to the game, and waits for the next game output and a cumulative score from the beginning of the game. An example of taking an egg in Zork is
$l_{1}$ : West of House You are standing in an open field west of a white house, with a boarded front door. There is a small mailbox here. (Score: 0, Moves: 0)
$l_{2}:&gt;$ go north
$l_{3}$ : North of House You are facing the north side of a white house. There is no door here, and all the windows are boarded up. To the north a narrow path winds through the trees. (Score: 0, Moves: 1)
$l_{4}:&gt;$ go north
$l_{5}$ : Forest Path This is a path winding through a dimly lit forest. The path heads north-south here. One particularly large tree with some low branches stands at the edge of the path. (Score: 0 Moves: 2)
$l_{6}:&gt;$ climb tree
$l_{7}$ : Up a Tree You are about 10 feet above the ground nestled among some large branches. The nearest branch above you is above your reach. Beside you on the branch is a small bird's nest. In the bird's nest is a large egg encrusted with precious jewels, apparently scavenged by a childless songbird. The egg is covered with fine gold inlay, and ornamented in lapis lazuli and mother-of-pearl. Unlike most eggs, this one is hinged and closed with a delicate looking clasp. The egg appears extremely fragile. (Score: 0, Moves: 3)
$l_{8}:&gt;$ take the egg
$l_{9}$ : Taken. (Score: 5, Moves: 4)
We call the game output the master, a player's input sentence the action, and the gap between two consecutive scores the instant reward. A running log of a text-based game is a composition of master-action pairs, which we call a trajectory. In this example, $l_{1}, l_{3}, l_{5}$, and $l_{7}$ each are masters, while $l_{2}$, $l_{4}, l_{6}$, and $l_{8}$ each are actions. The instant reward of taking the egg is five points. The trajectory using to make the decision of choosing take the egg is the concatenation from $l_{1}$ to $l_{5}$, to denote this we use the label $t_{1-5}$.</p>
<h2>B. DQN framework</h2>
<p>A text-based game constitutes a set of states $S$, a set of actions $A$, a transition matrix $T:(S, A) \rightarrow S$, and an instant reward matrix $R:(S, A) \rightarrow \mathcal{R}$. At a state $s$, the game accepts an action $a$, then transitions to new state $s^{\prime}$ with master $m$ and instant reward $r$. The game terminates upon reaching certain terminal states, e.g. the actor is defeated in fighting. If $S$ and $A$ are finite, we can search for a policy to play the game with (simple) Q-learning: Let $Q:(S, A) \rightarrow \mathcal{R}$ be a matrix over the (state, action) space $(S, A)$. Each value $Q[s, a]$ is the expected reward that the agent will get in the future until termination if choosing $a$ at state $s$. Given the stochastic attribute of the FST, a transition of $T[s, a]$ could result in multiple new states. E.g. Using the same action $a$ at the state $s$, with probability of 0.7 the player enters a room with no thief $\left(s_{1}^{\prime}\right)$, while with probability of 0.3 the player enters the same room by with a thief in there $\left(s_{2}^{\prime}\right)$. Computing the Q-value of the state $s$ needs to compute the weighted sum of both of the next states $s_{1}^{\prime}$ and
$s_{2}^{\prime}$. We then have an iterative update function for computing $Q$ :</p>
<p>$$
Q[s, a]=E_{T[s, a]}\left(R[s, a]+\gamma * \max _{a^{\prime}}\left(Q\left[T[s, a], a^{\prime}\right]\right]\right)\right)
$$</p>
<p>where $E_{T[s, a]}$ is the expectation according to multiple new states, and $\gamma$ is a hyperparameter that controls the importance of future rewards.</p>
<p>With the knowledge of the Q-matrix, the policy of playing a game is to choose the action with the highest Q-value at each state $s$,</p>
<p>$$
a \leftarrow \arg \max Q[s, *]
$$</p>
<p>The DQN was introduced by [1] for playing Atari video games. In video games, the FST's state is implicitly provided by video frames, while the action is explicitly encoded as a joystick movement combined with the presence or absence of a button press. [1] use a CNN to encode video frames into compact hidden states. The Q-matrix is then represented by a function $f_{D Q N}:(S, A) \rightarrow \mathcal{R}$. Unlike simple Q-learning, deep Q-learning allows for very large or even infinite $A$ and $S$.</p>
<p>According to the Q-matrix update in Equation 1, DQN training requires training samples in the form of a tuple $\left(s, s^{\prime}, a, r\right)$ consisting of the current state $s$, the action $a$, the next state $s^{\prime}=T[s, a]$, and the instant reward $r=R[s, a]$. In text-based games, the state of the game is not simply a game location such as up a tree, but is a trajectory. In our egg taking example, at both $l_{7}$ and $l_{9}$ the player is in the location up a tree, but in $l_{9}$ the player also has an egg in the inventory. Having the complete trajectory is a means for easily differentiating these states.</p>
<p>With our egg taking example, four training samples exist:
$\left(t_{1-1}, t_{1-3}\right.$, go north, 0$)$,
$\left(t_{1-3}, t_{1-5}\right.$, go north, 0$)$,
$\left(t_{1-5}, t_{1-7}\right.$, climb tree, 0$)$,
$\left(t_{1-7}, t_{1-9}\right.$, take the egg, 5$)$.
The DQN training is a process of repeatedly playing the game in an exploration-exploitation manner: At every step, the DQN agent chooses a random action to play the game with a probability $\epsilon$ (exploration), or else it chooses the action with the best Q-value (exploitation). The $\epsilon$ decays from 1 to almost zero to anneal this process.</p>
<p>The DQN collects the training sample $\left(s, s^{\prime}, a, r\right)$ at every step into a replay memory. The training process gets samples from the replay memory to update the DQN. The loss function to update the DQN is</p>
<p>$$
\left(f_{D Q N}(s, a)-\left(r+\gamma * \max <em D="D" N="N" Q="Q">{a^{\prime}} f</em>\right.
$$}\left(s^{\prime}, a^{\prime}\right)\right)^{2</p>
<p>which is the square error loss between expected Q values and predicted Q values.</p>
<p>We use the same DQN framework to play text-based games, but encode texts instead of video frames.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: An example of CNN-DQN using three size-3 convolutional filters with a max-pooling layer on a trajectory <em>go north forest path this is a path .... S</em> is the start of sentence token. We use two start of sentence tokens to make sure the vectors generated from convolutional filters have the same length with the original trajectory. Each convolutional filter (triangle) encoding the whole trajectory along the dimension of tokens generates a feature vector.</p>
<h3>C. Trajectory Encoder</h3>
<p>The purpose of the trajectory encoder is to determine the hidden states of the game. We initially use an LSTM [6] as the encoder. The LSTM encodes sentences in a recurrent way, with an inner state vector to be updated after consuming each token, yielding two types of output: an output vector for each consumed token, and a final state. [8] uses the result vector from a mean-pooling layer on the output vector as the state, while we use final state as the encoded state. Our CNN encoder is inspired by [10], but we also use different pooling layers. The CNN encoder uses multiple one-dimensional convolutional filters with different kernel sizes to encode sentences, then uses a mean-pooling layer or a max-pooling layer along the dimension of the sentence, and finally concatenates pooling results into a one-dimensional vector. An example of the max-pooling CNN-DQN is shown in Figure 1.</p>
<p>The fact that the CNN encoder encodes blocks of tokens in a parallel way makes it much faster than the token-by-token LSTM encoder. However, the vanilla CNN encoder loses track of the position relationship between tokens, since it treats sentences as bags of words. To mitigate this, we apply position embeddings together with word embeddings to keep the position information, in the same way as [12]. As we will see in experiments, the CNN encoder with the use of position embeddings results in faster training and superior agents.</p>
<h3>III. EXPERIMENT SETUP</h3>
<p>We experiment using the classic 1979 game Zork I [5]. All experiments are run on a machine with two Tesla K80 GPUs (one for training, the other for evaluation) with four CPU threads. We use Python 3.6.8 and Tensorflow 1.12.0.</p>
<p>A game log collected from an expert player [13] to win all points (350) of Zork I consists of 345 steps with 130 types of actions. We think of the log as a near-optimal solution to solve Zork and constrain our system to use these 130 actions in our experiments.</p>
<h4>A. Reward shaping and clipping</h4>
<p>Reward shaping can embed common sense into the training process, leading to better agents. For the sake of faster convergence, we add –0.1 to all instant rewards. Negative masters such as "<em>you don’t ...</em>" and "<em>you can’t ...</em>" mean there is something wrong with our chosen actions, so we add a penalty of –1 on instant rewards.</p>
<p>We find that reward clipping is important in training the DQN to play text-based games. [8] uses a reward shaping method to make the expected state more noticeable by magnifying quest final scores. In this paper, we use a similar reward clipping method as [1]. We impose a variant of Huber loss [14], clipping the reward so it falls between -1 and 1, without modification of rewards that are initially in that range. Clipping rewards in this way is robust to outliers and independent of the reward systems introduced by different games, resulting in better generalization ability to all other games.</p>
<h4>B. Hyperparameters</h4>
<p>Careful choice of hyperparameters is vital to successful stochastic DQN training. We use 50,000 observation steps, 500,000 replay memories, and linearly decay ε from 1 to 0.0001 in 2,000,000 steps. Since the near-optimal path to solving Zork is 345 steps, we set each episode to have a maximum of 600 steps.</p>
<p>We save a DQN model and run evaluation every 5,000 steps of training (as one epoch). For each evaluation, we use a fixed ε = 0.05 as other work did [1], [2], [8] and run 10 episodes with the same number of steps per episodes for training.</p>
<p>We initialize our DQN models with random word embeddings and position embeddings. We use a fixed embedding size of 64. At every training step, we draw a minibatch of 32 samples and use a learning rate of 1e – 5 with the Adam optimizer. We trim trajectories to contain no more than 21 sentences to avoid unnecessarily long concatenated strings.</p>
<h3>IV. RESULTS AND DISCUSSION</h3>
<h4>A. Uniform sampling, weight-, and gap-based sampling</h4>
<p>Uniform sampling, as first introduced in [1], is the most common method to draw samples from the replay memory to train the DQN. However, it is not a general method that could work well from game to game. From our observation, the distribution of rewards is highly biased in the replay memory: Samples with negative and zero rewards are more likely to appear while those with positive rewards are rare.</p>
<p>In an experiment using only 11 actions, where we collect 43,069 training samples, 98.7% of them receive negative or zero rewards, while only 1.3% of them are positive rewards. For another experiment using 21 actions with 58,608 samples, 99.9% of them are negative or zero samples, and only 0.1% of them are positive ones. Training a DQN agent with uniform sampling on these replay memories can easily miss important samples with positive rewards and lead to a failed agent.</p>
<p>Since uniform sampling will naturally lead to an imbalance favoring zero or negative rewards, we seek a non-uniform sampling method during DQN training. We explore two different weighted sampling strategies in our experiments, fixed-weight and priority experience sampling. Fixed-weight sampling is based on the previous observation: positive reward samples are rare but more important. Even though we could use frequency counting as weights, a reward-based weight is more general, and is amenable to scenarios where counting frequency is difficult. We treat the instant reward $r_{i}$ for the $i$-th sample as the log of weight for sampling, i.e. we let:</p>
<p>$w_{i}=exp(r_{i}),$</p>
<p>then the probability of choosing sample $i$ is</p>
<p>$P(i)=\frac{w_{i}}{\sum_{j}{w_{j}}}.$</p>
<p>Reward clipping needs to be used to avoid samples with very large rewards suppressing all other samples.</p>
<p>Priority experience sampling [15] is based on the gap $|\delta_{i}|$ between the expected Q-value and the predicted Q-value, i.e. we let:</p>
<p>$\delta_{i}=f_{DQN}(s_{i},a_{i})-\left(r_{i}+\gamma*\max_{a^{\prime}}f_{DQN}(s_{i}^{\prime},a^{\prime})\right),$</p>
<p>then</p>
<p>$w_{i}=|\delta_{i}|+e,$</p>
<p>where $e$ is a small constant to avoid zero gap. The probability of choosing sample $i$ is</p>
<p>$P(i)=\frac{w_{i}^{a}}{\sum_{j}{w_{j}^{a}}}.$</p>
<p>The hyperparameter $a$ is a choice of randomness of choosing samples. Setting $a=0$ degrades the sampling method to uniform sampling.</p>
<p>To avoid bias towards samples with high probabilities, the priority experience sampling uses importance weights on the gradient $g$ when updating parameters, i.e.</p>
<p>$g_{i}=g_{i}<em>\left(\frac{1}{N</em>P(i)}\right)^{b}.$</p>
<p>The hyperparameter $b$ is annealed from 0 to 1 during training.</p>
<p>We will compare the weighted sampling and the priority experience sampling in later sections.</p>
<h3>V-B Choosing the encoder</h3>
<p>Since we use the exploration-exploitation search method with a decaying parameter $\epsilon$, it is better to let the $\epsilon$ decay to almost zero to see a whole picture of the training process. However, a well-trained LSTM-DQN on Zork could take tens of days. In order to compare encoders faster, we consider two</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Evaluation results of LSTM-DQN, CNN-DQN, and CNN-DQN with position embeddings on the egg quest of Zork I (as defined in Section III). In this plot, CNN-based DQNs are trained around 175 epochs, while the LSTM-DQN is trained 14 epochs in the same amount of time. All DQNs converge to reach a score of 25 at the end of training except the mean-pooling CNN-DQN (blue) that jitters even after 10 hours of training. On the contratry, the ones using a max-pooling layer (red and green) show more stable convergence curves, and the red curve converges fastest in half an hour using position embeddings. The LSTM-DQN (purple) converges after 7 hours, slower than the max-pooling CNN-DQNs.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Evaluation results of LSTM-DQN, CNN-DQN, and CNN-DQN with position embeddings on the troll quest of Zork I. In this plot, CNN-DQNs are trained for 190 epochs, while the LSTM-DQN is trained 25 epochs in the same amount of time. The max-pooling CNN-DQNs (red and green) converge fast with higher scores than the others, and the red one (using position embeddings) is the fastest to converge, at 5 hours. The mean-pooling CNN-DQN (blue) shows convergence at a lower score but with more jitters. The LSTM-DQN (purple) cannot be well-trained in 45 hours of training.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Evaluation results of the troll quest with the maxpooling CNN-DQN. The yellow curve uses weighted sampling while the green one uses priority experience sampling. The weighted sampling method leads to faster convergence.
of the sub-tasks that Zork comprises, following the approach taken by [3].</p>
<p>Egg quest: From the beginning of the game, the player is required to walk to a tree, climb on the tree, and take an egg. The optimal action combination to win the task is in four steps as shown in our egg-taking example in Section II-A. Zork gives the player 5 points for getting the egg. To simplify the task, we provide 11 complete actions for the agent to choose from: eight navigation actions and three essential actions to finish the task.</p>
<p>Troll quest: From the beginning of the game, the player is required to walk into a house, take a lantern and weapons, find a secret path to a troll room, and kill the troll. This task is more complicated than the egg quest, and one error in the action combination could lead to failure. Also, the player may (stochastically) be killed by the troll, which necessitates a game restart. The optimal action combination is "go north | go east | open window | enter house | go west | take sword | take lantern | move rug | open trap door | turn on lantern | go down | go north | kill troll with sword." We select 21 complete actions for this task. Among the 21 actions, there are eight navigation actions.</p>
<p>Even though we can use the same set of hyperparameters for the egg quest and the troll quest that we use on the complete game, we choose to decrease these hyperparameters for a faster experimental cycle, based on our knowledge that these two quests are sub-tasks extracted from Zork:</p>
<ul>
<li>For the egg quest we use 5,000 observation steps, 50,000 replay memory size, and we decay $\epsilon$ from 1 to 0.0001 in 500,000 steps. Since the optimal path to solving the egg quest is four steps, we set each episode to have a maximum of 100 steps.</li>
<li>Since the troll quest has a larger search space than the egg quest, we double the observation steps, replay memory size, and $\epsilon$ decaying steps settings used in the egg quest.</li>
</ul>
<p>The optimal path to solving the troll quest is 13 steps, so we set each episode to have a maximum of 150 steps.
We compare encoders based on the egg quest and the troll quest. As shown in Figure 2 and Figure 3, we compare four types of encoders: the LSTM encoder, the CNN encoder with mean-pooling, the CNN-encoder with max-pooling, and the use of position embeddings on trajectories. Both figures show consistent results toward these four types of DQNs:</p>
<p>The LSTM-DQNs (purple) run many fewer epochs than CNN-DQNs in the same amount of time. While LSTMs can converge in the egg quest with 7 hours, they cannot be welltrained in the troll quest within 45 hours, resulting in poor results in the troll quest.</p>
<p>The mean-pooling CNN-DQNs (blue) converge with jitters, and show an inferior result in the troll quest.</p>
<p>The max-pooling CNN-DQNs (red and green) show better results in both their time to convergence and their maximum scores than other approaches. CNN-DQNs using position embeddings (red) show the best results on both quests.</p>
<p>We compare weighted sampling and the priority experience sampling on the troll quest, as shown in Figure 4. Both sampling methods converge to about the same score, but weighted sampling converges more quickly. Weighted sampling is suitable for single-quest games that focus on important positive rewards.</p>
<p>For subsequent experiments on the complete Zork, we use the max-pooling CNN-DQN.</p>
<h2>C. Pooling layer and auto-attention</h2>
<p>From Figure 2 and 3, a question arises naturally: Why does max-pooling lead to a more stable and better result than meanpooling?</p>
<p>We find out that the max-pooling layer can be thought of as a kind of attention mechanism [16]. With the max-pooling DQN, we can trace back through actions to see which part of trajectories affect the final decision most. An example of attention tokens is shown in Figure 5 (left part) with the egg quest. The bold texts are the top-3 important attention wordblocks used to make the decision of choosing each action.</p>
<h2>D. Zork results</h2>
<p>Based on our experience with the egg quest and the troll quest, we choose to use the max-pooling CNN-DQN framework to train agents on the complete Zork. Results are shown in Figure 6. Our base encoder (blue) shows bad results for Zork. In 2,000,000 steps of training, there is no evaluation result higher than 50 points, which means for each run, at best we score 5 points for finishing the egg quest. The evaluation result of the blue curve shows that our agent cannot explore Zork beyond the egg quest part.</p>
<p>Zork is a multi-quest game, which means there are multiple distinct trajectories needed to score points. It is important to use priority experience sampling to train an agent on Zork, otherwise the agent tends to converge to one of the local maxima-the egg quest part in our experiment-instead of</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Pseudo-attention via max pooling. For each action chosen (center column), we show the three most important context spans relevant to this decision, both with unmodified context (left) and dependency tree-ordered context (right). Importance is determined by following a chain of max-pooling decisions over convolutional filters. The weight of arrows shows the relative importance of each token set. Shadow areas show dependency trees, and bold texts are used by convolutional filters.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Evaluation results on Zork. The blue curve shows the result of using our base encoder: max-pooling CNN-DQN with weighted sampling. With the use of priority experience sampling we get the green curve. The red curve further improves the performance over priority experience by adding position embeddings. Finally, we use dependency parser reordering on top of priority experience to get the purple curve. The training process of two million steps of base encoder finishes in 22 hours.</p>
<p>exploring more rooms of the game, as shown in Figure 6 comparing the blue curve with the green one.</p>
<p>Furthermore, we apply position embeddings on trajectories as described when playing the egg quest and the troll quest, yielding the red curve with a stable evaluation result of 40 points for a single episode.</p>
<h3><em>E. Dependency parser reordering</em></h3>
<p>The CNN encoder, though running a magnitude order faster than the LSTM, encodes local blocks of tokens, while the LSTM encodes a whole sentence. The fact that local blocks are likely to be related is ubiquitous in the domain of image process, but not in natural language processing. In fact, a token and its syntactic dependencies (e.g., a verb and its descriptive adverbs) could appear many words apart in a sentence. Consider the example sentence: <em>you are facing the north side of a white house</em>. The subject of the sentence could be (notice the bold tokens) <em>you are facing the north side of a white house</em>. The token <em>side</em> is far from other main tokens.</p>
<p>Instead of using convolutional filters on trajectories directly, we use the Stanford dependency parser [17] to reorder each trajectory so that related tokens are right beside each other. Dependency parsers rearrange tokens of a sentence into a tree, with the subject token as the root (R), and its modifier tokens as children (C). Children of the root can themselves be roots that have children, and so on, recursively. Take the sentence</p>
<p>you are facing the north side of a white house as an example, three subtrees would be generated as</p>
<ol>
<li>facing $(R)$ you $(C)$ are $(C)$ side $(C)$;</li>
<li>side $(R)$ the $(C)$ north $(C)$ house $(C)$;</li>
<li>house $(R)$ of $(C)$ a $(C)$ white $(C)$.</li>
</ol>
<p>Reading the subtrees in a breadth-first way, the three subtrees result in three reordered sub-sentences: facing you are side, side the north house, and house of a white. To avoid size- $N$ convolutional filters striding across boundaries, we add $N-1$ padding tokens among them. E.g. with padding token "O" for size-3 convolutional filters, we add two padding tokens:
facing you are side $O$ o side the north house $O$ o house of a white</p>
<p>In this way, size-3 convolutional filters cannot stride across each meaningful block.</p>
<p>The result of using a dependency parser reordering is shown in Figure 6 (purple curve). With dependency parser reordering, the trained agent can converge in around 1.2 million steps of training, which is faster by half a million steps than the red curve.</p>
<h2>F. Repeated bad tries</h2>
<p>We observe that agents tend to repeat themselves, as has also been observed in natural language dialogue generation work [18]. Agents tend to 'get stuck in a place' and repeat the same action, e.g. "go west | you need a machete to go west | go west | you need a machete to go west | go west | you need a machete to go west," and so on. These repeated tries with no positive reward we call repeated bad tries. We determine that trained agents tend to get stuck in areas without short- or long-term positive rewards from those states.</p>
<p>In our experiment with Zork, we find out that out of 2,075,356 training steps, there are 181,209 (8.73%) repeated bad tries. This behavior gives us the intuition to add an accumulative negative reward on repeated bad samples as a corrective method. In our experiments, we add a negative reward $-0.1$ if we see a repeated sample with a negative reward, and we accumulate the penalty if we immediately see the bad sample again.</p>
<p>With the repeated penalty, we can reduce the number of repetitions to 3.51%; out of 1,596,224 training steps, there are 56,058 repeated bad tries. Compared to using repeated penalty with the previous method (Figure 7, top, yellow curve), the agent trained with repeated penalty converges much faster, in about half a million training steps. The distribution of repeated bad tries in each training process is plotted in Figure 8. We also compare using repeated penalty with dependency parser reordering, see Figure 7 (bottom). The benefit is additive; incorporating a repeated penalty can also make the convergence with dependency parser reordering faster, in a quarter million steps.</p>
<h2>V. Related Work</h2>
<p>Several works [2], [4], [7]-[9], [11], [18] also build agents for text-based games based on the DQN approach designed for
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: Evaluation results of training CNN-based DQN with repeated penalty (green) VS without repeated penalty (yellow). Both figures use a max-pooling CNN-DQN with position embeddings, however the bottom figure also includes dependency-based reordering (Section IV-E).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8: Repeated bad tries in the training process, comparing no repeated penalty (yellow) VS with repeated penalty (green). The x-axis is episode of training, the y-axis is the number of repeated bad tries appearing in each episode. The model used is a max-pooling CNN-DQN with position embeddings.</p>
<p>action video games [1]. One key consideration when learning to play text-based games is how to represent game states. Instead of using trajectories, [2], [3] use different methods to represent states. Some games allow the use of the special actions look and inventory to describe the current environment and the player's belongings, and use the combination of the two instead of the trajectory as states. Our method is more generalized, and avoids the use of look and inventory at every step, which are extra steps that, in certain games (e.g. games with fighting), could lead to a dead state.</p>
<p>Text-based games have a much larger action space to explore than video games of the type evaluated previously [1], which means that the naive application of the DQN leads to slow or even failing convergence. To reduce the action space, action elimination methods that use both reinforcement learning and NLP-related motivation have been applied. [3] use action elimination DQN framework with mathematical bounds to remove unlikely actions, an orthogonal improvement to ours that could be incorporated in future work. [2] explore affordance by using Word2Vec [19] to generate reasonable actions from words, learning, e.g., that eat apple is more reasonable than eat wheel.</p>
<p>However, previous works that attempt to play Zork can only finish a very small portion of the game, far from that achievable by human players. [3] use the max-pooling CNNDQN but without position embeddings. Our Zork evaluation result is stable at a score of 40 in one million steps, compared to [3], we get a score of 40 without using the action elimination DQN framework and compared to [7], that use the LSTMDQN framework without the action elimination method, we have a huge performance gain. The generalized method of reward shaping is important for games with multiple subquests. [20] use random network distillation to change the instant reward and get improved results on several hard Atari games that require extra exploration.</p>
<h2>VI. CONCLUSION</h2>
<p>By analyzing how the contexts of text-based games are used by our learning approaches, we find that agents make better decisions when they can learn to pay attention to a comprehensible chunk of context. The CNN-DQN with a max-pooling layer is the right tool to reveal the attention information, combining flexibility and speed. However, since linear context is not always a logical chunk of information, dependency parsing gives the CNN the ability to attend focus on syntactically valid chunks. Our trained agent on Zork can reach the state-of-the-art scores in 10 hours of training.</p>
<h2>REFERENCES</h2>
<p>[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, "Human-level control through deep reinforcement learning," Nature, vol. 518, pp. 529 EP -, 02 2015. [Online]. Available: https://doi.org/10.1038/nature14236
[2] N. Fulda, D. Ricks, B. Murdoch, and D. Wingate, "What can you do with a rock? affordance extraction via word embeddings." in IJCAI, C. Sierra, Ed. ijcai.org, 2017, pp. 1039-1045. [Online]. Available: http://dbfp.uni-trier.de/db/conf/ijcai/ijcai2017.html#FuldaRMW17
[3] T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor, "Learn what not to learn: Action elimination with deep reinforcement learning," in Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds. Curran Associates, Inc., 2018, pp. 3562-3573.
[4] M. Côté, Á. Kádár, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, M. J. Hausknecht, L. E. Aoti, M. Adada, W. Tay, and A. Trischler, "Textworld: A learning environment for text-based games," CoRR, vol. abs/1806.11532, 2018.
[5] Infocom, Zork I Manual, 2001, http://infodoc.plover.net/manuals/zork1. pdf.
[6] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation, vol. 9, pp. 1735-80, 121997.
[7] B. Kostka, J. Kwiecieli, J. Kowalski, and P. Rychlikowski, "Text-based adventures of the golovin AI agent," in CIG. IEEE, 2017, pp. 181-188.
[8] K. Narasimhan, T. Kulkarni, and R. Barzilay, "Language understanding for text-based games using deep reinforcement learning," in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2015, pp. 1-11. [Online]. Available: http://aclweb.org/anthology/D15-1001
[9] G. A. Ansari, S. J. P, S. Chandar, and B. Ravindran, "Language expansion in text-based games," CoRR, vol. abs/1805.07274, 2018. [Online]. Available: http://arxiv.org/abs/1805.07274
[10] Y. Kim, "Convolutional neural networks for sentence classification," in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2014, pp. 1746-1751. [Online]. Available: http://aclweb. org/anthology/D14-1181
[11] J. He, J. Chen, X. He, J. Gao, L. Li, L. Deng, and M. Ostendorf, "Deep reinforcement learning with a natural language action space," in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2016, pp. 1621-1630. [Online]. Available: http://aclweb.org/anthology/P16-1153
[12] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, "Convolutional sequence to sequence learning," in Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70. International Convention Centre, Sydney, Australia: PMLR, 06-11 Aug 2017, pp. 1243-1252. [Online]. Available: http://proceedings.mlr.press/v70/gehring17a.html
[13] M. Norton, Zork Transcript, http://steel.lcc.gatech.edu/ marleigh/zork/ transcript.html.
[14] P. J. Huber, "Robust estimation of a location parameter," Ann. Math. Statist., vol. 35, no. 1, pp. 73-101, 031964. [Online]. Available: https://doi.org/10.1214/aoms/1177703732
[15] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, "Prioritized experience replay," in International Conference on Learning Representations, Puerto Rico, 2016.
[16] T. Luong, H. Pham, and C. D. Manning, "Effective approaches to attention-based neural machine translation," in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2015, pp. 1412-1421. [Online]. Available: http://aclweb.org/anthology/D15-1166
[17] D. Chen and C. Manning, "A fast and accurate dependency parser using neural networks," in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics, Oct. 2014, pp. 740-750. [Online]. Available: https://www.aclweb.org/anthology/D14-1082
[18] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, "Deep reinforcement learning for dialogue generation," in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 1192-1202. [Online]. Available: https://www.aclweb.org/anthology/D16-1127
[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their compositionality," in Advances in Neural Information Processing Systems 26, C. J. C. Burgos, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 3111-3119.
[20] Y. Burda, H. Edwards, A. J. Storkey, and O. Klimov, "Exploration by random network distillation," CoRR, vol. abs/1810.12894, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
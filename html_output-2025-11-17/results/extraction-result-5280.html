<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5280 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5280</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5280</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-ed1353d705eeabc0e916caba5fbae890eefe4f84</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ed1353d705eeabc0e916caba5fbae890eefe4f84" target="_blank">MolXPT: Wrapping Molecules with Text for Generative Pre-training</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.</p>
                <p><strong>Paper Abstract:</strong> Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5280.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5280.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolXPT: Wrapping Molecules with Text for Generative Pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style generative language model jointly pretrained on SMILES, scientific text, and 'wrapped' sequences where molecular mentions are replaced by SMILES to explicitly link molecules and text; used for molecular property prediction and text↔molecule generation including zero-shot text-to-molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (GPT-like)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>350M parameters (24 layers, 1024 hidden, 16 heads)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on 30M SMILES from PubChem, 30M titles/abstracts from PubMed, and 8M 'wrapped' sequences created by NER/entity linking (BERN2 + ChEBI) replacing molecule mentions with SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular property prediction and text-guided molecule generation (text-to-molecule and molecule-to-text translation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Generative LM with prompt-based finetuning for downstream tasks; also used zero-shot generation by giving text prompts to the pretrained model without finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES sequences (special <som>/<eom> tokens wrapped around SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>For property prediction: ROC-AUC. For molecule↔text generation: BLEU, ROUGE, METEOR, Text2Mol (text↔molecule alignment); for text-to-molecule: Exact match, fingerprint similarities (MACCS, RDK, Morgan), Frechet ChemNet Distance (FCD), validity (percentage of valid SMILES). Zero-shot reported top-1/top-5 fingerprint similarities (MACCS, RDK, Morgan).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MoleculeNet (BBBP, Tox21, ClinTox, HIV, BACE, SIDER) for property prediction; CheBI-20 (33,010 pairs) for text↔molecule translation; PubChem and PubMed used for pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolXPT outperforms several GNN baselines on MoleculeNet (average ROC-AUC reported 81.9%, best across listed models) and achieves comparable molecule↔text translation performance to MolT5-large while using ~44% of its parameters. For text-to-molecule generation MolXPT: Exact=0.215, MACCS=0.859 (best), RDK=0.757 (best), Morgan=0.667, FCD=0.45 (best), Text2Mol=0.578 (best), Validity=0.983 (best). Zero-shot (no finetuning) top-1 fingerprint similarities: MACCS 0.540, RDK 0.383, Morgan 0.228, and MolXPT recovered 33 reference molecules exactly in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to MolT5-large (800M params) MolXPT (350M) attains comparable molecule↔text performance on several metrics and outperforms many GNN-based methods on property prediction; MolXPT leverages explicit 'wrapped' sequences linking text and SMILES which the authors attribute to improved text-molecule alignment. MolXPT outperforms Galactica on MoleculeNet in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors report compute/resource limitations for scaling to larger models; zero-shot generation performance drops relative to finetuned models (notably lower Exact match than MolT5-large); some molecule-generation metrics (e.g., Exact match) lag behind the largest comparator; reliance on NER/entity linking to create wrapped sequences (potentially introducing linking errors); broader ethical/privacy concerns if trained on clinical data. Training larger models is computationally costly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolXPT: Wrapping Molecules with Text for Generative Pre-training', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5280.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5280.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (translation between molecules and natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-style encoder-decoder model applied to bidirectional translation between molecular SMILES and textual descriptions; used as the primary comparison baseline for molecule↔text generation tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer (T5-like)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported variants including MolT5-large (~800M parameters); MolT5-small and MolT5-base also referenced</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on molecule–text paired data (e.g., CheBI-20 and related datasets) as reported in the MolT5 work; exact pretraining corpora not specified in this MolXPT paper (MolT5 results are cited).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text↔molecule translation (text-to-molecule generation and molecule-to-text description)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Finetuning encoder-decoder LM on paired molecule-text translation tasks (prompting/sequence-to-sequence generation)</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES and natural language text</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR, Text2Mol for molecule-to-text; Exact match, MACCS/RDK/Morgan fingerprint similarity, FCD, Text2Mol, validity for text-to-molecule.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>CheBI-20 (used in comparisons reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5-large achieves strong molecule↔text generation performance (e.g., molecule-to-text BLEU/ROUGE metrics highest for MolT5-large in tables; text-to-molecule Exact=0.311 and high fingerprint similarities and validity). In MolXPT comparisons MolT5-large sometimes outperforms MolXPT on Exact match and certain fingerprint metrics, but MolXPT matches or surpasses MolT5-large on several other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>MolT5-large is larger (~800M) and in some metrics (Exact match, Morgan fingerprint) outperforms MolXPT, while MolXPT (350M) matches or beats MolT5-large on several alignment and validity measures; tradeoff between parameter count and some performance aspects is highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Larger parameter count implies higher computational cost; specifics of MolT5 limitations are not discussed in depth in this paper beyond size/resource considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolXPT: Wrapping Molecules with Text for Generative Pre-training', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5280.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5280.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT: Molecular generation using a transformer-decoder model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-decoder model applied to molecular generation from SMILES (cited in the paper as an example of adapting language models to molecular generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molgpt: Molecular generation using a transformer-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-decoder (GPT-style) applied to SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora (general statement in the MolXPT paper; exact datasets not specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular generation / drug design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretraining a transformer-decoder on SMILES sequences to autoregressively generate molecules</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolXPT: Wrapping Molecules with Text for Generative Pre-training', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Molgpt: Molecular generation using a transformer-decoder model. <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Pretrained molecular graph representation with 3d geometry <em>(Rating: 1)</em></li>
                <li>Neural scaling of deep chemical models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5280",
    "paper_id": "paper-ed1353d705eeabc0e916caba5fbae890eefe4f84",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "MolXPT",
            "name_full": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
            "brief_description": "A GPT-style generative language model jointly pretrained on SMILES, scientific text, and 'wrapped' sequences where molecular mentions are replaced by SMILES to explicitly link molecules and text; used for molecular property prediction and text↔molecule generation including zero-shot text-to-molecule generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MolXPT",
            "model_type": "autoregressive transformer (GPT-like)",
            "model_size": "350M parameters (24 layers, 1024 hidden, 16 heads)",
            "training_data": "Pretrained on 30M SMILES from PubChem, 30M titles/abstracts from PubMed, and 8M 'wrapped' sequences created by NER/entity linking (BERN2 + ChEBI) replacing molecule mentions with SMILES.",
            "application_domain": "Drug discovery / molecular property prediction and text-guided molecule generation (text-to-molecule and molecule-to-text translation)",
            "generation_method": "Generative LM with prompt-based finetuning for downstream tasks; also used zero-shot generation by giving text prompts to the pretrained model without finetuning.",
            "output_representation": "SMILES sequences (special &lt;som&gt;/&lt;eom&gt; tokens wrapped around SMILES)",
            "evaluation_metrics": "For property prediction: ROC-AUC. For molecule↔text generation: BLEU, ROUGE, METEOR, Text2Mol (text↔molecule alignment); for text-to-molecule: Exact match, fingerprint similarities (MACCS, RDK, Morgan), Frechet ChemNet Distance (FCD), validity (percentage of valid SMILES). Zero-shot reported top-1/top-5 fingerprint similarities (MACCS, RDK, Morgan).",
            "benchmarks_or_datasets": "MoleculeNet (BBBP, Tox21, ClinTox, HIV, BACE, SIDER) for property prediction; CheBI-20 (33,010 pairs) for text↔molecule translation; PubChem and PubMed used for pretraining.",
            "results_summary": "MolXPT outperforms several GNN baselines on MoleculeNet (average ROC-AUC reported 81.9%, best across listed models) and achieves comparable molecule↔text translation performance to MolT5-large while using ~44% of its parameters. For text-to-molecule generation MolXPT: Exact=0.215, MACCS=0.859 (best), RDK=0.757 (best), Morgan=0.667, FCD=0.45 (best), Text2Mol=0.578 (best), Validity=0.983 (best). Zero-shot (no finetuning) top-1 fingerprint similarities: MACCS 0.540, RDK 0.383, Morgan 0.228, and MolXPT recovered 33 reference molecules exactly in zero-shot.",
            "comparison_to_other_methods": "Compared to MolT5-large (800M params) MolXPT (350M) attains comparable molecule↔text performance on several metrics and outperforms many GNN-based methods on property prediction; MolXPT leverages explicit 'wrapped' sequences linking text and SMILES which the authors attribute to improved text-molecule alignment. MolXPT outperforms Galactica on MoleculeNet in this study.",
            "limitations_or_challenges": "Authors report compute/resource limitations for scaling to larger models; zero-shot generation performance drops relative to finetuned models (notably lower Exact match than MolT5-large); some molecule-generation metrics (e.g., Exact match) lag behind the largest comparator; reliance on NER/entity linking to create wrapped sequences (potentially introducing linking errors); broader ethical/privacy concerns if trained on clinical data. Training larger models is computationally costly.",
            "uuid": "e5280.0",
            "source_info": {
                "paper_title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5 (translation between molecules and natural language)",
            "brief_description": "A T5-style encoder-decoder model applied to bidirectional translation between molecular SMILES and textual descriptions; used as the primary comparison baseline for molecule↔text generation tasks in this paper.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "use",
            "model_name": "MolT5",
            "model_type": "encoder-decoder transformer (T5-like)",
            "model_size": "reported variants including MolT5-large (~800M parameters); MolT5-small and MolT5-base also referenced",
            "training_data": "Trained on molecule–text paired data (e.g., CheBI-20 and related datasets) as reported in the MolT5 work; exact pretraining corpora not specified in this MolXPT paper (MolT5 results are cited).",
            "application_domain": "Text↔molecule translation (text-to-molecule generation and molecule-to-text description)",
            "generation_method": "Finetuning encoder-decoder LM on paired molecule-text translation tasks (prompting/sequence-to-sequence generation)",
            "output_representation": "SMILES and natural language text",
            "evaluation_metrics": "BLEU, ROUGE, METEOR, Text2Mol for molecule-to-text; Exact match, MACCS/RDK/Morgan fingerprint similarity, FCD, Text2Mol, validity for text-to-molecule.",
            "benchmarks_or_datasets": "CheBI-20 (used in comparisons reported in this paper)",
            "results_summary": "MolT5-large achieves strong molecule↔text generation performance (e.g., molecule-to-text BLEU/ROUGE metrics highest for MolT5-large in tables; text-to-molecule Exact=0.311 and high fingerprint similarities and validity). In MolXPT comparisons MolT5-large sometimes outperforms MolXPT on Exact match and certain fingerprint metrics, but MolXPT matches or surpasses MolT5-large on several other metrics.",
            "comparison_to_other_methods": "MolT5-large is larger (~800M) and in some metrics (Exact match, Morgan fingerprint) outperforms MolXPT, while MolXPT (350M) matches or beats MolT5-large on several alignment and validity measures; tradeoff between parameter count and some performance aspects is highlighted.",
            "limitations_or_challenges": "Larger parameter count implies higher computational cost; specifics of MolT5 limitations are not discussed in depth in this paper beyond size/resource considerations.",
            "uuid": "e5280.1",
            "source_info": {
                "paper_title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT: Molecular generation using a transformer-decoder model",
            "brief_description": "A transformer-decoder model applied to molecular generation from SMILES (cited in the paper as an example of adapting language models to molecular generation).",
            "citation_title": "Molgpt: Molecular generation using a transformer-decoder model.",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "transformer-decoder (GPT-style) applied to SMILES",
            "model_size": null,
            "training_data": "SMILES corpora (general statement in the MolXPT paper; exact datasets not specified here)",
            "application_domain": "De novo molecular generation / drug design",
            "generation_method": "Pretraining a transformer-decoder on SMILES sequences to autoregressively generate molecules",
            "output_representation": "SMILES",
            "evaluation_metrics": null,
            "benchmarks_or_datasets": null,
            "results_summary": null,
            "comparison_to_other_methods": null,
            "limitations_or_challenges": null,
            "uuid": "e5280.2",
            "source_info": {
                "paper_title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Molgpt: Molecular generation using a transformer-decoder model.",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2
        },
        {
            "paper_title": "Pretrained molecular graph representation with 3d geometry",
            "rating": 1
        },
        {
            "paper_title": "Neural scaling of deep chemical models",
            "rating": 1
        }
    ],
    "cost": 0.010149499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MolXPT: Wrapping Molecules with Text for Generative Pre-training</h1>
<p>Zequn Liu ${ }^{1}$; Wei Zhang ${ }^{2 *}$, Yingce Xia ${ }^{31}$, Lijun Wu ${ }^{3}$, Shufang Xie ${ }^{4}$, Tao Qin ${ }^{3}$, Ming Zhang ${ }^{1 \dagger}$ and Tie-Yan Liu ${ }^{3}$<br>${ }^{1}$ Peking University; ${ }^{2}$ University of Science and Technology of China<br>${ }^{3}$ Microsoft Research AI4Science; ${ }^{4}$ Renmin University of China<br>{zequnliu,mzhang_cs}@pku.edu.cn; weizhang_cs@mail.ustc.edu.cn<br>{yingce.xia, lijunwu, taoqin, tyliu}@microsoft.com<br>shufangxie@ruc.edu.cn</p>
<h4>Abstract</h4>
<p>Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.</p>
<h2>1 Introduction</h2>
<p>Generative pre-trained Transformer (GPT), like GPT-3 (Brown et al., 2020) and ChatGPT (OpenAI, 2022), have obtained great success in natural language processing. They usually have billions of parameters and are trained on large corpus (Taylor et al., 2022; Singhal et al., 2022). By witnessing their great power, people start transferring language models to chemical (Bagal et al., 2022) and biological domains (Ferruz et al., 2022). For example, a small molecule (e.g., an oral drug) can be represented using simplified molecular-input lineentry system (SMILES) (Weininger, 1988), which is a sequence obtained by traversing the molecular graph using depth-first-search and several rules</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>for branching, aromaticity, etc. After serializing molecules, people pre-train language models on SMILES (Bagal et al., 2022; Tong et al., 2021; Frey et al., 2022) and obtain promising results for molecular generation.</p>
<p>Text is the most important record for molecular science and more generally, scientific discovery (Beltagy et al., 2019). It describes detailed properties of molecules, like how to synthesize the molecule (Feng et al., 2016), whether the molecule is toxic (Juurlink et al., 2003), etc. BioGPT (Luo et al., 2022) and PubMedGPT (Bolton et al., 2022) are two language models trained on biomedical literature. Recently, a new trend is to jointly model SMILES and scientific text so as to obtain shared representations across the two modalities. MolT5 is a T5-like (Raffel et al., 2020) model, where several spans of the text/SMILES are masked in the encoder and they should be reconstructed in the decoder. Galactica (Taylor et al., 2022) is a GPTlike (Brown et al., 2020) model pre-trained on various types of inputs, like text, SMILES, protein sequences, etc. Although those models demonstrate progress in prediction and generation tasks, they do not explicitly leverage the relation between molecules and text. An intuition is that, in scientific literature, when a molecule name appears in a sentence, the surrounding context could be a description of the molecule. This should be useful information for joint training but is ignored in those models.</p>
<p>To leverage such relations, in this work, we propose a novel molecule-text language model (MolXPT), which is trained on "wrapped" sequences: Given a sentence, we detect the molecular names with named entity recognition tools, and if any, replace them to the corresponding SMILES and obtain the "wrapped" sequence between SMILES and text. We pre-train a 24-layer MolXPT (with 350 M parameters) on 8 M wrapped sequences, as well as 30M SMILES from PubChem</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Framework of MolXPT. MolXPT is pretrained on text from PubMed, SMILES from PubChem and wrapped sequences between SMILES and text. The wrapped sequences are obtained by applying NER and entity linking to text and then replacing matched molecular mentions with SMILES. MolXPT can be finetuned for various text and molecular downstream tasks, like molecular property prediction and molecule-text translation.
(Kim et al., 2022) and 30M titles and abstracts from PubMed (a popular biomedical literature search engine).</p>
<p>After pre-training, we finetune MolXPT on MoleculeNet (a benchmark about molecular property prediction) (Wu et al., 2018) and molecule-text translation (Edwards et al., 2022) using promptbased finetuning. On MoleculeNet, MolXPT outperforms strong baselines with sophisticated design like GEM (Fang et al., 2022). On text-molecule translation, MolXPT performs comparably with the state-of-the-art model, MolT5-large (Edwards et al., 2022). MolT5-large has $800 M$ parameters while MolXPT only uses $44 \%$ of its parameters. We also verify that MolXPT has the zero-shot ability on text-to-molecule generation.</p>
<h2>2 Our Method</h2>
<p>MolXPT is a language model pre-trained on heterogeneous data including scientific text, SMILES sequences, and "wrapped" sequences between SMILES and text. Due to the flexible input, we can finetune it for various text and molecular tasks. The framework of MolXPT is in Figure 1.</p>
<h3>2.1 Pre-training corpus</h3>
<p>For scientific text, we use the titles and abstracts of 30M papers from PubMed ${ }^{1}$. For molecular SMILES, we randomly choose 30M molecules from PubChem ${ }^{2}$ (Kim et al., 2022).</p>
<p>The wrapped sequences are constructed via a "detect and replace" pipeline. We first use BERN2 (Sung et al., 2022), a widely used named entity recognition (NER) tool for biomedical purpose, to detect all mentions of molecules and link them to the entities in public knowledge bases like ChEBI</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(Hastings et al., 2016). After that, we can retrieve the molecular SMILES of the matched entities. Finally, we replace the molecular mentions to their corresponding SMILES. An example is shown in the left panel of Figure 1. The wrapped sequences must contain at least one molecular SMILES. We eventually obtain 8 M wrapped sequences in total.</p>
<p>Text and SMILES are tokenized separately. For text, we use byte-pair encoding (BPE) (Sennrich et al., 2016) to split the words into subwords. The number of BPE merge operation is 40 k . For SMILES sequences (including those in wrapped sequences), we tokenize them with the regular expression from (Schwaller et al., 2018). For each SMILES sequence $S$, we add a start-of-molecule token $\langle$ som $\rangle$ at the beginning of $S$ and append an end-of-molecule token $\langle$ eom $\rangle$ at the end of $S$.</p>
<h3>2.2 Model and training</h3>
<p>Model architecture: MolXPT has the same architecture as the GPT models (Radford et al., 2019). Due to computational resource limitation, in this paper, we follow the GPT-2 ${ }<em i="i">{\text {medium }}$ configuration with 24 layers, 1024 hidden size and 16 attention heads. The maximum length of input we can process is 2048 and the vocabulary size is 44536 . In total, our model has 350 M parameters.
Pre-training: The pre-training objective function of MolXPT is the negative log-likelihood. Mathematically, let $\mathcal{D}=\left{x</em>\right}<em i="i">{i}$ denote the collection of sequences of the three types of the data, and $x</em>$ tokens. The training objective function is:}=\left(s_{i, 1}, s_{i, 2}, \cdots, s_{i, n_{i}}\right)$ is the $i$-th sequence with $n_{i</p>
<p>$$
\min -\frac{1}{|\mathcal{D}|} \sum_{i=1}^{|\mathcal{D}|} \sum_{j=1}^{n_{i}} \log P\left(s_{i, j} \mid s_{i, j-1}, s_{i, j-2}, \cdots, s_{1}\right)
$$</p>
<p>The pre-training details are left in Appendix B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">Tox21</th>
<th style="text-align: center;">ClinTox</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">SIDER</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">#Molecules</td>
<td style="text-align: center;">2039</td>
<td style="text-align: center;">7831</td>
<td style="text-align: center;">1478</td>
<td style="text-align: center;">41127</td>
<td style="text-align: center;">1513</td>
<td style="text-align: center;">1478</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">G-Contextual</td>
<td style="text-align: center;">$70.3 \pm 1.6$</td>
<td style="text-align: center;">$75.2 \pm 0.3$</td>
<td style="text-align: center;">$59.9 \pm 8.2$</td>
<td style="text-align: center;">$75.9 \pm 0.9$</td>
<td style="text-align: center;">$79.2 \pm 0.3$</td>
<td style="text-align: center;">$58.4 \pm 0.6$</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: left;">G-Motif</td>
<td style="text-align: center;">$66.4 \pm 3.4$</td>
<td style="text-align: center;">$73.2 \pm 0.8$</td>
<td style="text-align: center;">$77.8 \pm 2.0$</td>
<td style="text-align: center;">$73.8 \pm 1.4$</td>
<td style="text-align: center;">$73.4 \pm 4.0$</td>
<td style="text-align: center;">$60.6 \pm 1.1$</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">GROVER $_{\text {base }}$</td>
<td style="text-align: center;">$70.0 \pm 0.1$</td>
<td style="text-align: center;">$74.3 \pm 0.1$</td>
<td style="text-align: center;">$81.2 \pm 3.0$</td>
<td style="text-align: center;">$62.5 \pm 0.9$</td>
<td style="text-align: center;">$82.6 \pm 0.7$</td>
<td style="text-align: center;">$64.8 \pm 0.6$</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: left;">GROVER $_{\text {large }}$</td>
<td style="text-align: center;">$69.5 \pm 0.1$</td>
<td style="text-align: center;">$73.5 \pm 0.1$</td>
<td style="text-align: center;">$76.2 \pm 3.7$</td>
<td style="text-align: center;">$68.2 \pm 1.1$</td>
<td style="text-align: center;">$81.0 \pm 1.4$</td>
<td style="text-align: center;">$65.4 \pm 0.1$</td>
<td style="text-align: center;">72.3</td>
</tr>
<tr>
<td style="text-align: left;">GraphMVP</td>
<td style="text-align: center;">$72.4 \pm 1.6$</td>
<td style="text-align: center;">$75.9 \pm 0.5$</td>
<td style="text-align: center;">$79.1 \pm 2.8$</td>
<td style="text-align: center;">$77.0 \pm 1.2$</td>
<td style="text-align: center;">$81.2 \pm 0.9$</td>
<td style="text-align: center;">$63.9 \pm 1.2$</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: left;">MGSSL</td>
<td style="text-align: center;">$70.5 \pm 1.1$</td>
<td style="text-align: center;">$76.5 \pm 0.3$</td>
<td style="text-align: center;">$80.7 \pm 2.1$</td>
<td style="text-align: center;">$79.5 \pm 1.1$</td>
<td style="text-align: center;">$79.7 \pm 0.8$</td>
<td style="text-align: center;">$61.8 \pm 0.8$</td>
<td style="text-align: center;">74.8</td>
</tr>
<tr>
<td style="text-align: left;">GEM</td>
<td style="text-align: center;">$72.4 \pm 0.4$</td>
<td style="text-align: center;">$\mathbf{7 8 . 1} \pm \mathbf{0 . 1}$</td>
<td style="text-align: center;">$90.1 \pm 1.3$</td>
<td style="text-align: center;">$\mathbf{8 0 . 6} \pm \mathbf{0 . 9}$</td>
<td style="text-align: center;">$85.6 \pm 1.1$</td>
<td style="text-align: center;">$67.2 \pm 0.4$</td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: left;">KV-PLM</td>
<td style="text-align: center;">$74.6 \pm 0.9$</td>
<td style="text-align: center;">$72.7 \pm 0.6$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$74.0 \pm 1.2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$61.5 \pm 1.5$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Galactica</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">69.5</td>
</tr>
<tr>
<td style="text-align: left;">MoMu</td>
<td style="text-align: center;">$70.5 \pm 2.0$</td>
<td style="text-align: center;">$75.6 \pm 0.3$</td>
<td style="text-align: center;">$79.9 \pm 4.1$</td>
<td style="text-align: center;">$76.2 \pm 0.9$</td>
<td style="text-align: center;">$77.1 \pm 1.4$</td>
<td style="text-align: center;">$60.5 \pm 0.9$</td>
<td style="text-align: center;">73.3</td>
</tr>
<tr>
<td style="text-align: left;">MolXPT</td>
<td style="text-align: center;">$\mathbf{8 0 . 0} \pm \mathbf{0 . 5}$</td>
<td style="text-align: center;">$77.1 \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{9 5 . 3} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$78.1 \pm 0.4$</td>
<td style="text-align: center;">$\mathbf{8 8 . 4} \pm \mathbf{1 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 7} \pm \mathbf{0 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on MoleculeNet. The evaluation metric is ROC-AUC. Bold fonts indicate the best results.</p>
<p>Prompt-based finetuning: MolXPT can be finetuned for downstream tasks about molecules and text. Adding classification or regression heads to pre-trained backbone models introduces the gap between pre-training and finetuning (Brown et al., 2020; Chen et al., 2022; Gu et al., 2022). Therefore, we adopt prompt-based finetuning (Gao et al., 2021) to unify different tasks into a sequence generation task, which is consistent with the pre-training objective. Briefly, given a task, we convert the input and output into text and/or SMILES sequences, equip the sequences with task-specific prompts and finetune using language modeling loss. Prompts for MoleculeNet and text-molecule translation are introduced in the Section 3.1 and 3.2 respectively. Discussion: Some works also try to jointly model text and molecules. Zeng et al. (2022) propose KV-PLM, where SMILES sequences are appended after molecule names for pre-training. Su et al. (2022) use contrastive learning between text and molecular graphs. Our MolXPT is a generative model while the above two models are not. Both of them are built upon SciBERT (Beltagy et al., 2019), a BERT model (Devlin et al., 2019) for scientific literature. MolXPT is complementary to them.</p>
<h2>3 Experiments</h2>
<p>We evaluated MolXPT on two downstream tasks: (1) molecular property prediction on MoleculeNet (Wu et al., 2018), which is to predict whether the given molecule has specific properties; (2) the generation between text descriptions and molecules (Edwards et al., 2022), where both molecules and text should be considered. In this section, we focus on introducing task definition, prompt design
and results while leaving the detailed finetuning hyper-parameters in Appendix C.</p>
<h3>3.1 Results on MoleculeNet</h3>
<p>MoleculeNet (Wu et al., 2018) is a widely-used benchmark for molecular modeling, which has more than $700 k$ compounds for various different properties. We choose six molecular classification tasks for evaluation, which are BBBP, Tox21, ClinTox, HIV, BACE and SIDER. Details are left in Appendix A. We follow GEM (Fang et al., 2022) to split the data into training/validation/test sets based on the scaffold. For these tasks, the input is a SMILES and the output is a binary label.
Finetuning strategy: Previous molecular property prediction models mainly use SMILES sequences or molecular graphs as input, while we can use the "wrapped" sequences. For example, one task is to predict the blood-brain barrier penetration (BBBP) of a molecule. Therefore, the prompt is "We can conclude that the BBB penetration of 〈som〉〈SMILES〉 〈eom〉 is 〈tag〉", where 〈SMILES〉 denotes the molecular SMILES, and $\langle$ tag $\rangle$ denotes the classification result. For the BBBP task, we design〈tag〉 as "true" or "false", indicating whether the compound can or cannot cross BBB.</p>
<p>Different tasks have different prompts (see Appendix C.1), but we put the tags to the last token of the prompt for all tasks. Let $\left(s_{i, 1}, s_{i, 2}, \cdots, s_{i, T_{i}}\right)$ denote the $i$-th wrapped sequence for the downstream task with $T_{i}$ tokens, where $s_{i, T_{i}}$ is the tag of the sequence. Denote that there are $N$ samples for finetuning. The finetuning strategy could be either</p>
<p>$$
\min -\frac{1}{N} \sum_{i=1}^{N} \log P\left(s_{i, T_{i}} \mid s_{i,&lt;T_{i}}\right)
$$</p>
<p>indicating that we finetune the tags only, or</p>
<p>$$
\min -\frac{1}{N} \sum_{i=1}^{N} \frac{1}{T_{i}} \sum_{j=1}^{T_{i}} \log P\left(s_{i, j} \mid s_{i,&lt;j}\right)
$$</p>
<p>indicating that we finetune the full prompts. According to our exploration, Eqn.(1) achieves slightly better results and we use it for all tasks (see Appendix C. 4 for the results).</p>
<p>Let $p_{\text {true }}$ and $p_{\text {false }}$ denote the probabilities of tags "true" and "false" after encoding the prefix "We can conclude that the BBB penetration of $\langle\operatorname{som}\rangle\langle$ SMILES $\rangle\langle$ eom $\rangle$ is". The probabilities that $\langle$ SMILES $\rangle$ can and cannot cross blood-brain barrier are normalized as $p_{\text {true }} /\left(p_{\text {true }}+p_{\text {false }}\right)$ and $p_{\text {false }} /\left(p_{\text {true }}+p_{\text {false }}\right)$ respectively. The finetuning hyper-parameters are in Appendix C.2.</p>
<p>We compare MolXPT with two types of baselines: (1) pre-trained language model baselines including KV-PLM (Zeng et al., 2022), Galactica (Taylor et al., 2022) and MoMu (Su et al., 2022). (2) pre-trained Graph Neural Network (GNN) baselines including GContextual (Rong et al., 2020), G-Motif (Rong et al., 2020), GROVER ${ }<em _large="{large" _text="\text">{\text {base }}$ (Rong et al., 2020), GROVER $</em>$ (Rong et al., 2020), GraphMVP (Liu et al., 2022), MGSSL (Zhang et al., 2021) and GEM (Fang et al., 2022). The evaluation metric is the ROC-AUC score. The results are in Table 1.}</p>
<p>MolXPT outperforms the GNN baselines pretrained on pure molecular data, indicating the effectiveness of pre-training with scientific text corpus. Compared with Galactica which also uses both SMILES and text for pre-training GPT-like model, MolXPT obtains better performance. Note that Galactica does not purposely build and train on the "wrapped" sequences, whose importance is demonstrated via our empirical results. A possible explanation of the superior performance is that the SMILES describes the component and structural information of molecules, while the text describes the general properties. They are complementary to each other, and joint training on them brings more effective representations.</p>
<h3>3.2 Results on text-molecule translation</h3>
<p>We evaluated the performance of MolXPT on CheBI-20 (Edwards et al., 2021), a bidirectional text-molecule translation dataset. It consists of 33,010 molecule-description pairs. We use the data split provided by MolT5 (Edwards et al., 2022), where the training, validation and test sets account
$80 \%, 10 \%$ and $10 \%$ of total data. For molecule-totext generation, given a molecular SMILES $S$, the prompt is: "The description of $\langle\mathrm{som}\rangle S\langle\mathrm{eom}\rangle$ is: The molecule is", followed by the text description of $S$. For text-to-molecule generation, given a text description $T$, the prompt is: " $T$. The compound is $\langle\mathrm{som}\rangle$ ", and the model will generate the molecular SMILES ended with $\langle$ eom $\rangle$. We compare our method with MolT5 (Edwards et al., 2022).</p>
<p>For molecule-to-text generation, the results are evaluated by NLP metrics including BLEU (Papineni et al., 2002), Rouge (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). "Text2mol" is a deep learning based metric proposed by Edwards et al. (2022) to measure the similarity of the text-molecule pairs. For text-to-molecule generation, we evaluate the following metrics: the proportion of the generated SMILES that exactly match the reference SMILES (denoted as "Exact"); the Tanimoto similarity of three types of fingerprints: MACCS (Durant et al., 2002), RDK (Schneider et al., 2015) and Morgan (Rogers and Hahn, 2010); the FCD score (Preuer et al., 2018), which measures the molecule distances by a pretrained model; the percentage of the valid generated SMILES. The results are reported in Table 2.</p>
<p>We observe that MolXPT achieves significantly better performance than MolT5-small and MolT5-base, and has comparable performance with MolT5-large. Note that MolT5-large has $800 M$ parameters while MolXPT only uses $44 \%$ of its parameters. For both tasks, our model performs the best on Text2Mol metric, indicating that MolXPT captures the alignment between text and molecule better. We attribute it to the wrapped sequences, by which the model can learn the relation between molecule and text explicitly.</p>
<p>We further verify the zero-shot text-to-molecule generation ability of MolXPT. The pre-trained MolXPT takes the text as input and directly generates molecules without finetuning. The top-1 and top-5 fingerprint similarity is in Table 3. Indeed, compared with the full data setting, the performance drops, but still reasonable numbers. In addition, the zero-shot MolXPT successfully recovers 33 molecules based on the text (see Appendix D).</p>
<h2>4 Conclusions and Future Work</h2>
<p>We propose MolXPT, a generative model pretrained on scientific text, molecular SMILES and</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Molecule-to-text</th>
<th style="text-align: center;">BLEU-2</th>
<th style="text-align: center;">BLEU-4</th>
<th style="text-align: center;">Rouge-1</th>
<th style="text-align: center;">Rouge-2</th>
<th style="text-align: center;">Rouge-L</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">Text2Mol</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MolT5-small (77M)</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.540</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-base (250M)</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.547</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-Large (800M)</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 8}$</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.582</td>
</tr>
<tr>
<td style="text-align: left;">MolXPT (350M)</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 4}$</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">$\mathbf{0 . 6 6 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 2 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Text-to-molecule</td>
<td style="text-align: center;">Exact $\uparrow$</td>
<td style="text-align: center;">MACCS $\uparrow$</td>
<td style="text-align: center;">RDK $\uparrow$</td>
<td style="text-align: center;">Morgan $\uparrow$</td>
<td style="text-align: center;">FCD $\downarrow$</td>
<td style="text-align: center;">Text2mol $\uparrow$</td>
<td style="text-align: center;">Validity $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-small</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">2.49</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.721</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-medium</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-large</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 1}$</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 4}$</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.905</td>
</tr>
<tr>
<td style="text-align: left;">MolXPT</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 7}$</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">$\mathbf{0 . 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 3}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of molecule-to-text (top) and text-to-molecule generation (bottom). For FCD, the smaller, the better. For the remaining metrics, the larger, the better. MolT5 results are from Table 1 and 2 of (Edwards et al., 2022). MolT5 parameters are from https://github.com/blender-nlp/MolT5. Bold fonts indicate the best results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MACCS</th>
<th style="text-align: center;">RDK</th>
<th style="text-align: center;">Morgan</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-shot (Top-1)</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.228</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot (Top-5)</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.423</td>
</tr>
<tr>
<td style="text-align: left;">Full data (Top-1)</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.660</td>
</tr>
</tbody>
</table>
<p>Table 3: Zero-shot text-to-molecule generation.
their wrapped sequences. We train a 24-layer MolXPT with 350M parameters. By prompt-based finetuning, it improves strong baselines on MoleculeNet and achieves comparable results with the best model on molecule-text translation but using much fewer parameters.</p>
<p>For future work, first, we will train larger MolXPT to further verify the performances across different tasks and the zero-shot/in-context (Xie et al., 2022) learning ability. Second, how to further enhance the interaction between molecules and text (e.g., using contrastive learning to enhance consistency) should be studied. Third, how to effectively adapt MolXPT into other molecule and text tasks such as text-guided molecule optimization is another direction to explore.</p>
<h2>Limitations</h2>
<p>One limitation of our method is that when training larger models, it requires more computation resources, whose cost is relatively high. However, after pre-training, we will release our models so that readers can directly use them without pre-training again.</p>
<h2>Broader Impacts</h2>
<p>We provide a new generative pre-trained model on molecules and text. On one hand, the model can be used to speed up scientific discovery, like molecule design, drug optimization, etc. On the other hand, once the model is trained on clinical data (which also describes the usage of drug molecules), it might lead to personal information leaky. We will enhance data filtration to anonymize all personal information, and will design new models to protect the information.</p>
<h2>Acknowledgement</h2>
<p>The authors Zequn Liu and Ming Zhang are partially supported by National Natural Science Foundation of China (NSFC Grant Number 62276002).</p>
<h2>References</h2>
<p>Viraj Bagal, Rishal Aggarwal, P. K. Vinod, and U. Deva Priyakumar. 2022. Molgpt: Molecular generation using a transformer-decoder model. Journal of Chemical Information and Modeling, 62(9):2064-2076.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 36153620, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang. 2022. PubMedGPT 2.7B.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, and Yue Zhang. 2022. Adaprompt: Adaptive model training for prompt-based nlp. arXiv preprint arXiv:2202.04824.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. 2002. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):12731280 .</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and Heng Ji. 2022. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817.</p>
<p>Carl Edwards, ChengXiang Zhai, and Heng Ji. 2021. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595-607.</p>
<p>Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. 2022. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127-134.</p>
<p>Minghao Feng, Bingqing Tang, Steven H Liang, and Xuefeng Jiang. 2016. Sulfur containing scaffolds in drugs: synthesis and application in medicinal chemistry. Current topics in medicinal chemistry, 16(11):1200-1216.</p>
<p>Noelia Ferruz, Steffen Schmidt, and Birte Höcker. 2022. Protgpt2 is a deep unsupervised language model for protein design. Nature Communications, 13(1):4348.</p>
<p>Nathan Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gomez-Bombarelli, Connor Coley, and Vijay Gadepally. 2022. Neural scaling of deep chemical models. ChemRxiv.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830.</p>
<p>Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2022. Ppt: Pre-trained prompt tuning for few-shot learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8410-8423.</p>
<p>Janna Hastings, Gareth Owen, Adriano Dekker, Marcus Ennis, Namrata Kale, Venkatesh Muthukrishnan, Steve Turner, Neil Swainston, Pedro Mendes, and Christoph Steinbeck. 2016. Chebi in 2016: Improved services and an expanding collection of metabolites. Nucleic acids research, 44(D1):D1214-D1219.</p>
<p>David N Juurlink, Muhammad Mamdani, Alexander Kopp, Andreas Laupacis, and Donald A Redelmeier. 2003. Drug-drug interactions among elderly patients hospitalized for drug toxicity. Jama, 289(13):16521658 .</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Leonid Zaslavsky, Jian Zhang, and Evan E Bolton. 2022. PubChem 2023 update. Nucleic Acids Research, 51(D1):D1373-D1380.</p>
<p>Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR (Poster).</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. 2022. Pretraining molecular graph representation with 3d geometry. In International Conference on Learning Representations.</p>
<p>Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6).</p>
<p>OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. Technical blog.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In $A C L$, pages 311-318.</p>
<p>Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer. 2018. Frechet chemnet distance: a metric for generative models for molecules in drug discovery. Journal of chemical information and modeling, 58(9):17361741 .</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>David Rogers and Mathew Hahn. 2010. Extendedconnectivity fingerprints. Journal of chemical information and modeling, 50(5):742-754.</p>
<p>Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. 2020. Self-supervised graph transformer on largescale molecular data. Advances in Neural Information Processing Systems, 33:12559-12571.</p>
<p>Nadine Schneider, Roger A Sayle, and Gregory A Landrum. 2015. Get your atoms in order: An opensource implementation of a novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55(10):2111-2120.</p>
<p>Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro Laino. 2018. "found in translation": predicting outcomes of complex organic chemistry reactions using neural sequence-tosequence models. Chemical science, 9(28):60916098 .</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715-1725, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2022. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and JiRong Wen. 2022. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481.</p>
<p>Mujeen Sung, Minbyul Jeong, Yonghwa Choi, Donghyeon Kim, Jinhyuk Lee, and Jaewoo Kang. 2022. Bern2: an advanced neural biomedical named entity recognition and normalization tool. arXiv preprint arXiv:2201.02080.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Xiaochu Tong, Xiaohong Liu, Xiaoqin Tan, Xutong Li, Jiaxin Jiang, Zhaoping Xiong, Tingyang Xu, Hualiang Jiang, Nan Qiao, and Mingyue Zheng. 2021. Generative models for de novo drug design. Journal of Medicinal Chemistry, 64(19):14011-14027.</p>
<p>David Weininger. 1988. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations.</p>
<p>Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature Communications, 13(1):862.</p>
<p>Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. 2021. Motif-based graph selfsupervised learning for molecular property prediction. Advances in Neural Information Processing Systems, 34:15870-15882.</p>
<h2>Appendix</h2>
<h2>A Datasets and Baselines of MoleculeNet</h2>
<p>We choose the following tasks of MoleculeNet for evaluation:
(1) BBBP contains compounds with binary labels on blood-brain barrier penetration.
(2) Tox21 is a dataset for predicting the human toxicity of compounds on 12 different targets.
(3) ClinTox contains drugs approved by the FDA and those that have failed clinical trials for toxicity reasons.
(4) HIV aims to predict whether a drug can inhibit HIV replication.
(5) BACE describes binding results for a set of inhibitors of human $\beta$-secretase 1.
(6) SIDER has compounds used in marketed medicines with 27 categories of side effects.
We compare MolXPT with the following baselines:</p>
<p>(1) GROVER is a self-supervised pre-trained graph Transformer model. G-Contextual and G-Motif are two variants of it pre-trained with contextual property prediction task and motif prediction task.
(2) GraphMVP is a self-supervised pre-trained GNN model using both 2D topological structures and 3D geometric views of molecules.
(3) MGSSL leverages a retrosynthesis-based algorithm BRICS and additional rules to find the motifs and combines motif layers with atom layers.
(4) GEM is a geometry-enhanced pre-trained GNN model.
(5) Galactica is a GPT-like model trained on a large scientific corpus and many natural sequences like SMILES. We report the result of Galactica-120B.
(6) KV-PLM is a BERT-like model where SMILES sequences are appended after molecule names for pre-training.
(7) MoMu uses contrastive learning to jointly pretrain a BERT model for text and a GNN model for molecules.</p>
<h2>B Pre-training hyper-parameters</h2>
<p>MolXPT is pre-trained for 200k steps on eight A100 GPUs. The batchsize is 2048 tokens per GPU. The gradients are accumulated for 16 steps before updating. We use Adam (Kingma and Ba, 2015) optimizer for optimization. The peak learning rate is 0.0005 and the warm-up steps are 20000. The learning rate scheduler is inverse square root decay scheduler. The dropout is 0.1 .</p>
<h2>C Finetuning details of downstream tasks</h2>
<h2>C. 1 Prompts for finetuning MoleculeNet</h2>
<p>(1) BBBP: "We can conclude that the BBB penetration of $\langle$ som $\rangle\langle$ SMILES $\rangle\langle$ eom $\rangle$ is true/false."
(2) Tox21: "We can conclude that the 〈som〉〈SMILES〉〈eom〉 activity outcome on 〈target〉 is active/inactive. " where 〈target〉 refers to corresponding receptor or enzyme for each subtask, e.g. the 〈target〉 of subtask "AR" is "Androgen Receptor".
(3) ClinTox:"We can conclude that the clinical trial toxicity of $\langle$ som $\rangle\langle$ SMILES $\rangle\langle$ eom $\rangle$ is true/false." for subtask CT_TOX and "We can conclude that the FDA approval status of $\langle$ som $\rangle\langle$ SMILES $\rangle\langle$ eom $\rangle$ is true/false." for subtask FDA_APPROVED.
(4) HIV: "We can conclude that the screening result of ability to inhibit HIV replication of $\langle$ som $\rangle$〈SMILES〉 〈eom〉 is active/inactive."
(5) BACE: "We can conclude that the binding result on beta-secretase 1 of $\langle$ som $\rangle\langle$ SMILES $\rangle\langle$ eom $\rangle$ is true/false."
(6) SIDER:"We can conclude that the 〈som〉〈SMILES〉〈eom〉 can bring about the side effect of 〈side-effect〉 is true/false." where 〈side-effect〉 refers to corresponding side-effect for each subtask.</p>
<h2>C. 2 Details of finetuning MoleculeNet</h2>
<p>We grid search the following hyper-parameters: learning rate in $\left{3 \times 10^{-5}, 5 \times 10^{-5}\right}$; dropout in ${0.1,0.3}$; total epochs from ${30,50}$. The model is selected according to validation performance.</p>
<h2>C. 3 Details of finetuning text-molecule generation</h2>
<p>For text-molecule generation, MolXPT is finetuned for 100 steps on one P40 GPU with 1024 tokens and 16 accumulated steps per device. Models are finetuned for 100 epochs. The learning rate is 0.0001 and the dropout rate is grid searched from $[0.1,0.2,0.3,0.4,0.5]$. Setting dropout rate as 0.4 and 0.5 achieves the best validation performance on molecule-to-text generation and text-to-molecule generation respectively. We use the corresponding models for testing.</p>
<h2>C. 4 MoleculeNet finetuning strategy selection</h2>
<p>We provide two finetune strategies in Eqn.(1) and Eqn.(2). Their results are reported in Table 4. Their results are similar and Eqn.(1) is slightly better.</p>
<h2>D Zero-shot text-to-molecule generation</h2>
<p>Given $K$ generated molecule $\hat{m}<em 2="2">{1}, \hat{m}</em>$ and the reference molecule $m$, the top- $K$ fingerprint similarity is}, \cdots, \hat{m}_{K</p>
<p>$$
\max <em i="i">{i \in[K]} \operatorname{similarity}\left(m, \hat{m}</em>\right)
$$</p>
<p>MolXPT generates 33 molecules that can exactly match the reference molecules without finetuning. Figure 2 shows three of the cases.</p>
<p>Input text
The molecule is a sesquiterpene lactone and active principle of Feverfew (Tanacetum parthenium). It has a role as a nonsteroidal anti-inflammatory drug, a non-narcotic analgesic, a peripheral nervous system drug, an inhibitor and a drug allergen.</p>
<p>The molecule is the (R)-enantiomer of mevalonic acid. It is a conjugate acid of a (R)-mevalonate. It is an enantiomer of a (S)mevalonic acid.</p>
<p>The molecule is a bile acid taurine conjugate of ursocholic acid. It has a role as a human metabolite and a rat metabolite. It derives from an ursocholic acid. It is a conjugate acid of a tauroursocholate.</p>
<p>Generated molecule
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples for zero-shot text-to-molecule generation. We randomly pick up three cases that MolXPT can successfully generate the reference molecules without finetuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">Tox21</th>
<th style="text-align: center;">ClinTox</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">SIDER</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dev $_{\text {full prompt }}$</td>
<td style="text-align: center;">$98.8 \pm 0.2$</td>
<td style="text-align: center;">$78.8 \pm 0.1$</td>
<td style="text-align: center;">$98.8 \pm 0.1$</td>
<td style="text-align: center;">$82.9 \pm 1.0$</td>
<td style="text-align: center;">$78.4 \pm 0.3$</td>
<td style="text-align: center;">$67.7 \pm 0.7$</td>
<td style="text-align: center;">84.2</td>
</tr>
<tr>
<td style="text-align: left;">Dev $_{\text {tags only }}$</td>
<td style="text-align: center;">$98.9 \pm 0.3$</td>
<td style="text-align: center;">$78.8 \pm 0.2$</td>
<td style="text-align: center;">$97.7 \pm 0.1$</td>
<td style="text-align: center;">$85.3 \pm 0.2$</td>
<td style="text-align: center;">$75.8 \pm 0.8$</td>
<td style="text-align: center;">$69.4 \pm 0.6$</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: left;">Test $_{\text {full prompt }}$</td>
<td style="text-align: center;">$78.1 \pm 0.4$</td>
<td style="text-align: center;">$77.2 \pm 0.1$</td>
<td style="text-align: center;">$93.4 \pm 0.1$</td>
<td style="text-align: center;">$78.1 \pm 0.9$</td>
<td style="text-align: center;">$87.9 \pm 0.3$</td>
<td style="text-align: center;">$70.0 \pm 0.2$</td>
<td style="text-align: center;">80.8</td>
</tr>
<tr>
<td style="text-align: left;">Test $_{\text {tags only }}$</td>
<td style="text-align: center;">$80.0 \pm 0.5$</td>
<td style="text-align: center;">$77.1 \pm 0.2$</td>
<td style="text-align: center;">$95.3 \pm 0.2$</td>
<td style="text-align: center;">$78.1 \pm 0.4$</td>
<td style="text-align: center;">$88.4 \pm 1.0$</td>
<td style="text-align: center;">$71.7 \pm 0.2$</td>
<td style="text-align: center;">81.9</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of different finetuning strategies on MoleculeNet. "Dev" and "Test" denote validation set and test set respectively. Subscripts represent finetuning full prompts (Eqn.(2)) or tags only respectively (Eqn.(1)). The evaluation metric is ROC-AUC.</p>
<h1>A For every submission:</h1>
<p>A1. Did you describe the limitations of your work?
The section called "Limitations".
A2. Did you discuss any potential risks of your work?
The section called "Ethnics statement".
A3. Do the abstract and introduction summarize the paper's main claims?
Abstract and Introduction (Section 1)
\ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<p>Left blank.
$\square$ B1. Did you cite the creators of artifacts you used?
Not applicable. Left blank.
$\square$ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
$\square$ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Not applicable. Left blank.
$\square$ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Not applicable. Left blank.
$\square$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
$\square$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Not applicable. Left blank.</p>
<h2>C Did you run computational experiments?</h2>
<p>Section 2.2 and Appendix B
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Section 2.2 and Appendix B
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
Section 3.1, Section 3.2 and Appendix C
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
Section 3.1, Section 3.2
C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
Section 3.2.</p>
<h1>D Did you use human annotators (e.g., crowdworkers) or research with human participants?</h1>
<p>Left blank.
D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://ftp.ncbi.nlm.nih.gov/pubmed/
${ }^{2}$ https://pubchem.ncbi.nlm.nih.gov/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
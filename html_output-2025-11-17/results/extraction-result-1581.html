<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1581 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1581</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1581</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-201124711</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1908.08006v1.pdf" target="_blank">Evolutionary Computation, Optimization and Learning Algorithms for Data Science</a></p>
                <p><strong>Paper Abstract:</strong> A large number of engineering, science and computational problems have yet to be solved in a computationally efficient way. One of the emerging challenges is how evolving technologies grow towards autonomy and intelligent decision making. This leads to collection of large amounts of data from various sensing and measurement technologies, e.g., cameras, smart phones, health sensors, smart electricity meters, and environment sensors. Hence, it is imperative to develop efficient algorithms for generation, analysis, classification, and illustration of data. Meanwhile, data is structured purposefully through different representations, such as large-scale networks and graphs. We focus on data science as a crucial area, specifically focusing on a curse of dimensionality (CoD) which is due to the large amount of generated/sensed/collected data. This motivates researchers to think about optimization and to apply nature-inspired algorithms, such as evolutionary algorithms (EAs) to solve optimization problems. Although these algorithms look un-deterministic, they are robust enough to reach an optimal solution. Researchers do not adopt evolutionary algorithms unless they face a problem which is suffering from placement in local optimal solution, rather than global optimal solution. In this chapter, we first develop a clear and formal definition of the CoD problem, next we focus on feature extraction techniques and categories, then we provide a general overview of meta-heuristic algorithms, its terminology, and desirable properties of evolutionary algorithms.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1581.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1581.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary method that evolves computer programs represented as tree structures (function and terminal sets), using subtree crossover to recombine programs and stochastic operators to explore program space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming (tree-based)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Represents candidate solutions as parse trees composed of a function set (internal nodes) and a terminal set (leaf nodes). Evolution proceeds by selecting parent trees and performing tree-based crossover (swapping subtrees) to produce offspring; generations are typically replaced in a generational GA style. GP is used for symbolic regression, program synthesis and other program-evolution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Tree (subtree) crossover: choose a subtree in each parent program-tree and swap the subtrees to form two offspring programs (selection of crossover points is stochastic).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression / program synthesis (example: evolving expressions such as 4 * tan(x) + 2); general engineering and computational problems</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually with Genetic Algorithms (vector-based) and Evolutionary Programming; paper notes GP is generational and slower than GA</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper describes GP as the canonical method for evolving executable programs using subtree crossover; notes GP's flexible, expressive tree representation enables broader application than GA but comes with greater computational cost and slower speed. The chapter does not provide quantitative measures of novelty, diversity, or executability for GP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Computation, Optimization and Learning Algorithms for Data Science', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1581.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1581.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A population-based evolutionary optimizer that operates on vector encodings (binary or integer) using parent selection, crossover (uniform/arithmetic/k-point), and mutation to search for high-fitness solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Algorithm (binary/integer representations)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Represents candidate solutions as fixed-length vectors (binary one-hot for feature selection or integer representations). Evolution uses parent selection (roulette/tournament), crossover operators (uniform, arithmetic, k-point) to recombine parents, followed by mutation altering one or more components; can be run as steady-state (SSGA) or generational (GGA). Applied in the chapter for feature-selection/optimization contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>feature vectors / parameter vectors (not code)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Vector crossover: uniform crossover, arithmetic crossover, or k-point crossover applied to parent vectors to produce offspring; specific mechanism depends on chosen operator (e.g., k-point swaps segments at k cut points).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Component-wise mutation: with a mutation rate, one or more vector components are randomly perturbed or flipped (in binary representation flip 0/1; in integer representation change value within bounds).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Feature selection, large-scale optimization, applied problems such as electric vehicle parking allocation, resource optimization in construction; generally NP-hard optimization</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SSGA vs GGA (steady-state vs generational); GA described alongside other EAs (PSO, ABC, ACO, GWO, COA)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chapter presents GA as a central EA for feature selection and optimization; describes standard crossover and mutation operators and selection schemes, and notes steady-state variants can converge faster but risk local optima. No quantitative results on novelty/diversity/executability are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Computation, Optimization and Learning Algorithms for Data Science', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1581.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1581.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An EA variant historically used to evolve finite state machines or prediction models, characterized by relying primarily on mutation rather than crossover.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evolutionary Programming (mutation-only)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evolves candidate solutions (often non-variable program structures or finite state machines) using mutation-based variation and fitness evaluation; crossover is typically not used. EP often uses fitness functions based on training sequences and has been applied to sequence prediction and time-series problems.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>sequences / model structures (e.g., finite state machines); not described as evolving code in this chapter</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Mutation-only evolution: stochastic perturbations applied to solution representation (details not specified in chapter).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Time-series prediction and sequence problems (DNA/RNA noted), prediction tasks where fitness is based on training sequences</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually with GP and GA (EP uses mutation only and no crossover)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The chapter highlights EP's distinction of using mutation-only evolution and notes suitability for prediction/sequence problems; no quantitative information on novelty or executability is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Computation, Optimization and Learning Algorithms for Data Science', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1581.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1581.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memetic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memetic Algorithms (MAs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Population-based hybrid EAs that combine global recombination (crossover) with local search (intensification) to exploit solutions more effectively, especially for NP-hard optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Memetic Algorithms (recombination + local search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintain a population and apply recombination/crossover (to mix genetic material) together with local search (memes) applied to individuals to refine solutions. They use neighborhood relations and guiding functions within a defined search space to balance exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>feature/parameter vectors or candidate solutions (not code)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Recombination/crossover used to combine parent solutions (specific operator depends on representation; chapter mentions recombination in general).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>NP-hard optimization, feature selection, large-scale engineering problems</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to classical EAs; MAs combine population search with local search whereas traditional EAs may not</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chapter emphasizes that memetic algorithms leverage recombination (crossover) plus local search to outperform traditional EAs on many hard problems; no experimental novelty/executability/diversity metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Computation, Optimization and Learning Algorithms for Data Science', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1581.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1581.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSO+C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Particle Swarm Optimization with Crossover</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PSO variant augmented with crossover to exchange information between particles and help escape local optima; described in literature and mentioned here as an adaptation of PSO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Particle Swarm Optimization with added crossover operator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Standard PSO is a velocity/position update algorithm without genetic recombination; some works augment PSO with genetic-style crossover to mix particle information and reduce trapping in local optima, blending swarm updates with recombination operations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>feature/parameter vectors (not code)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Added crossover between particles: genetic crossover operator applied to particle position vectors to produce offspring/modified particles (specific crossover variant depends on cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Feature selection and continuous optimization; cited as a modification to classical PSO to address local optima</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Classical PSO (without crossover); paper cites Hao et al. who introduced a PSO with crossover</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chapter notes that adding crossover to PSO helps prevent premature stagnation by sharing information across particles; no quantitative measures in this chapter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Computation, Optimization and Learning Algorithms for Data Science', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1581.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1581.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BinaryPSO+Mut</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binary Particle Swarm Optimization with Mutation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary-valued PSO variant that incorporates mutation operators to enable discrete feature-selection search and improve exploration in binary spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Binary PSO augmented with mutation operator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maps PSO to binary/discrete search spaces for feature selection; includes a mutation operator (borrowed from GA) to flip bits or probabilistically perturb particle representations, improving search in high-dimensional discrete domains such as feature selection for spam detection.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>binary vectors (feature subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Binary mutation: bit flips or other probabilistic perturbations applied to particle binary encodings (specifics depend on cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Feature selection (example cited: spam detection), binary optimization</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Classical PSO and other feature-selection methods; chapter cites Zhang et al. who proposed binary PSO with mutation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chapter reports that adding mutation to binary PSO can address curse of dimensionality in feature selection tasks; no quantitative novelty/diversity/executability results are provided in the chapter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Computation, Optimization and Learning Algorithms for Data Science', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic programming: on the programming of computers by means of natural selection <em>(Rating: 2)</em></li>
                <li>A particle swarm optimization algorithm with crossover operator <em>(Rating: 2)</em></li>
                <li>Binary pso with mutation operator for feature selection using decision tree applied to spam detection <em>(Rating: 2)</em></li>
                <li>Evolutionary computation 1: Basic algorithms and operators <em>(Rating: 2)</em></li>
                <li>Artificial intelligence through a simulation of evolution <em>(Rating: 1)</em></li>
                <li>Guided evolutionary strategies: escaping the curse of dimensionality in random search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1581",
    "paper_id": "paper-201124711",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "GP",
            "name_full": "Genetic Programming",
            "brief_description": "An evolutionary method that evolves computer programs represented as tree structures (function and terminal sets), using subtree crossover to recombine programs and stochastic operators to explore program space.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Genetic Programming (tree-based)",
            "system_description": "Represents candidate solutions as parse trees composed of a function set (internal nodes) and a terminal set (leaf nodes). Evolution proceeds by selecting parent trees and performing tree-based crossover (swapping subtrees) to produce offspring; generations are typically replaced in a generational GA style. GP is used for symbolic regression, program synthesis and other program-evolution tasks.",
            "input_type": "programs",
            "crossover_operation": "Tree (subtree) crossover: choose a subtree in each parent program-tree and swap the subtrees to form two offspring programs (selection of crossover points is stochastic).",
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression / program synthesis (example: evolving expressions such as 4 * tan(x) + 2); general engineering and computational problems",
            "comparison_baseline": "Compared conceptually with Genetic Algorithms (vector-based) and Evolutionary Programming; paper notes GP is generational and slower than GA",
            "key_findings": "Paper describes GP as the canonical method for evolving executable programs using subtree crossover; notes GP's flexible, expressive tree representation enables broader application than GA but comes with greater computational cost and slower speed. The chapter does not provide quantitative measures of novelty, diversity, or executability for GP.",
            "uuid": "e1581.0",
            "source_info": {
                "paper_title": "Evolutionary Computation, Optimization and Learning Algorithms for Data Science",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "GA",
            "name_full": "Genetic Algorithm",
            "brief_description": "A population-based evolutionary optimizer that operates on vector encodings (binary or integer) using parent selection, crossover (uniform/arithmetic/k-point), and mutation to search for high-fitness solutions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Genetic Algorithm (binary/integer representations)",
            "system_description": "Represents candidate solutions as fixed-length vectors (binary one-hot for feature selection or integer representations). Evolution uses parent selection (roulette/tournament), crossover operators (uniform, arithmetic, k-point) to recombine parents, followed by mutation altering one or more components; can be run as steady-state (SSGA) or generational (GGA). Applied in the chapter for feature-selection/optimization contexts.",
            "input_type": "feature vectors / parameter vectors (not code)",
            "crossover_operation": "Vector crossover: uniform crossover, arithmetic crossover, or k-point crossover applied to parent vectors to produce offspring; specific mechanism depends on chosen operator (e.g., k-point swaps segments at k cut points).",
            "mutation_operation": "Component-wise mutation: with a mutation rate, one or more vector components are randomly perturbed or flipped (in binary representation flip 0/1; in integer representation change value within bounds).",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Feature selection, large-scale optimization, applied problems such as electric vehicle parking allocation, resource optimization in construction; generally NP-hard optimization",
            "comparison_baseline": "SSGA vs GGA (steady-state vs generational); GA described alongside other EAs (PSO, ABC, ACO, GWO, COA)",
            "key_findings": "Chapter presents GA as a central EA for feature selection and optimization; describes standard crossover and mutation operators and selection schemes, and notes steady-state variants can converge faster but risk local optima. No quantitative results on novelty/diversity/executability are provided.",
            "uuid": "e1581.1",
            "source_info": {
                "paper_title": "Evolutionary Computation, Optimization and Learning Algorithms for Data Science",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "EP",
            "name_full": "Evolutionary Programming",
            "brief_description": "An EA variant historically used to evolve finite state machines or prediction models, characterized by relying primarily on mutation rather than crossover.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Evolutionary Programming (mutation-only)",
            "system_description": "Evolves candidate solutions (often non-variable program structures or finite state machines) using mutation-based variation and fitness evaluation; crossover is typically not used. EP often uses fitness functions based on training sequences and has been applied to sequence prediction and time-series problems.",
            "input_type": "sequences / model structures (e.g., finite state machines); not described as evolving code in this chapter",
            "crossover_operation": null,
            "mutation_operation": "Mutation-only evolution: stochastic perturbations applied to solution representation (details not specified in chapter).",
            "uses_literature": false,
            "uses_code": null,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Time-series prediction and sequence problems (DNA/RNA noted), prediction tasks where fitness is based on training sequences",
            "comparison_baseline": "Compared conceptually with GP and GA (EP uses mutation only and no crossover)",
            "key_findings": "The chapter highlights EP's distinction of using mutation-only evolution and notes suitability for prediction/sequence problems; no quantitative information on novelty or executability is provided.",
            "uuid": "e1581.2",
            "source_info": {
                "paper_title": "Evolutionary Computation, Optimization and Learning Algorithms for Data Science",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Memetic",
            "name_full": "Memetic Algorithms (MAs)",
            "brief_description": "Population-based hybrid EAs that combine global recombination (crossover) with local search (intensification) to exploit solutions more effectively, especially for NP-hard optimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Memetic Algorithms (recombination + local search)",
            "system_description": "Maintain a population and apply recombination/crossover (to mix genetic material) together with local search (memes) applied to individuals to refine solutions. They use neighborhood relations and guiding functions within a defined search space to balance exploration and exploitation.",
            "input_type": "feature/parameter vectors or candidate solutions (not code)",
            "crossover_operation": "Recombination/crossover used to combine parent solutions (specific operator depends on representation; chapter mentions recombination in general).",
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "NP-hard optimization, feature selection, large-scale engineering problems",
            "comparison_baseline": "Compared conceptually to classical EAs; MAs combine population search with local search whereas traditional EAs may not",
            "key_findings": "Chapter emphasizes that memetic algorithms leverage recombination (crossover) plus local search to outperform traditional EAs on many hard problems; no experimental novelty/executability/diversity metrics are reported.",
            "uuid": "e1581.3",
            "source_info": {
                "paper_title": "Evolutionary Computation, Optimization and Learning Algorithms for Data Science",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "PSO+C",
            "name_full": "Particle Swarm Optimization with Crossover",
            "brief_description": "A PSO variant augmented with crossover to exchange information between particles and help escape local optima; described in literature and mentioned here as an adaptation of PSO.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Particle Swarm Optimization with added crossover operator",
            "system_description": "Standard PSO is a velocity/position update algorithm without genetic recombination; some works augment PSO with genetic-style crossover to mix particle information and reduce trapping in local optima, blending swarm updates with recombination operations.",
            "input_type": "feature/parameter vectors (not code)",
            "crossover_operation": "Added crossover between particles: genetic crossover operator applied to particle position vectors to produce offspring/modified particles (specific crossover variant depends on cited work).",
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Feature selection and continuous optimization; cited as a modification to classical PSO to address local optima",
            "comparison_baseline": "Classical PSO (without crossover); paper cites Hao et al. who introduced a PSO with crossover",
            "key_findings": "Chapter notes that adding crossover to PSO helps prevent premature stagnation by sharing information across particles; no quantitative measures in this chapter.",
            "uuid": "e1581.4",
            "source_info": {
                "paper_title": "Evolutionary Computation, Optimization and Learning Algorithms for Data Science",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "BinaryPSO+Mut",
            "name_full": "Binary Particle Swarm Optimization with Mutation",
            "brief_description": "A binary-valued PSO variant that incorporates mutation operators to enable discrete feature-selection search and improve exploration in binary spaces.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Binary PSO augmented with mutation operator",
            "system_description": "Maps PSO to binary/discrete search spaces for feature selection; includes a mutation operator (borrowed from GA) to flip bits or probabilistically perturb particle representations, improving search in high-dimensional discrete domains such as feature selection for spam detection.",
            "input_type": "binary vectors (feature subsets)",
            "crossover_operation": null,
            "mutation_operation": "Binary mutation: bit flips or other probabilistic perturbations applied to particle binary encodings (specifics depend on cited work).",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Feature selection (example cited: spam detection), binary optimization",
            "comparison_baseline": "Classical PSO and other feature-selection methods; chapter cites Zhang et al. who proposed binary PSO with mutation",
            "key_findings": "Chapter reports that adding mutation to binary PSO can address curse of dimensionality in feature selection tasks; no quantitative novelty/diversity/executability results are provided in the chapter.",
            "uuid": "e1581.5",
            "source_info": {
                "paper_title": "Evolutionary Computation, Optimization and Learning Algorithms for Data Science",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic programming: on the programming of computers by means of natural selection",
            "rating": 2,
            "sanitized_title": "genetic_programming_on_the_programming_of_computers_by_means_of_natural_selection"
        },
        {
            "paper_title": "A particle swarm optimization algorithm with crossover operator",
            "rating": 2,
            "sanitized_title": "a_particle_swarm_optimization_algorithm_with_crossover_operator"
        },
        {
            "paper_title": "Binary pso with mutation operator for feature selection using decision tree applied to spam detection",
            "rating": 2,
            "sanitized_title": "binary_pso_with_mutation_operator_for_feature_selection_using_decision_tree_applied_to_spam_detection"
        },
        {
            "paper_title": "Evolutionary computation 1: Basic algorithms and operators",
            "rating": 2,
            "sanitized_title": "evolutionary_computation_1_basic_algorithms_and_operators"
        },
        {
            "paper_title": "Artificial intelligence through a simulation of evolution",
            "rating": 1,
            "sanitized_title": "artificial_intelligence_through_a_simulation_of_evolution"
        },
        {
            "paper_title": "Guided evolutionary strategies: escaping the curse of dimensionality in random search",
            "rating": 1,
            "sanitized_title": "guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search"
        }
    ],
    "cost": 0.01215325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evolutionary Computation, Optimization and Learning Algorithms for Data Science
16 Aug 2019</p>
<p>Farid Ghareh Mohammadi farid.ghm@uga.edu 
Department of Computer Science
Franklin College of Arts and Sciences
University of Georgia
30601AthensGeorgia</p>
<p>M Hadi Amini 
School of Computing and Information Sciences</p>
<p>College of Engineering and Computing
Florida International University
33199MiamiFL</p>
<p>Hamid R Arabnia 
Department of Computer Science
Franklin College of Arts and Sciences
University of Georgia
30601AthensGeorgia</p>
<p>Evolutionary Computation, Optimization and Learning Algorithms for Data Science
16 Aug 2019924CF8406F0796ECEF4512E8C89513D3arXiv:1908.08006v1[cs.NE]Evolutionary AlgorithmsDimension Reduction (auto-encoder)Data ScienceHeuristic OptimizationCurse of Dimensionality (CoD)Supervised LearningData AnalyticFeature ExtractionOptimal Feature SelectionBig Data
A large number of engineering, science and computational problems have yet to be solved in a computationally efficient way.One of the emerging challenges is how evolving technologies grow towards autonomy and intelligent decision making.This leads to collection of large amounts of data from various sensing and measurement technologies, e.g., cameras, smart phones, health sensors, smart electricity meters, and environment sensors.Hence, it is imperative to develop efficient algorithms for generation, analysis, classification, and illustration of data.Meanwhile, data is structured purposefully through different representations, such as large-scale networks and graphs.Therefore, data plays a pivotal role in technologies by introducing several challenges: how to present, what to present, why to present.Researchers explored various approaches to implement a comprehensive solution to express their results in every particular domain, such that the solution enhances the performance and minimizes cost, especially time complexity.In this chapter, we focus on data science as a crucial area, specifically focusing on a curse of dimensionality (CoD) which is due to the large amount of generated/sensed/collected data, especially large sets of extracted features for a particular 1</p>
<p>Introduction</p>
<p>Overview</p>
<p>A large number of engineering, science and computational problems have yet to be solved in a more computationally efficient way.One of the emerging challenges is the evolving technologies and how they enhance towards autonomy.This leads to collection of large amount of data from various sensing and measurement technologies, such as cameras, smart phones, health sensors, and environment sensors.Hence, generation, manipulation and illustration of data grow significantly.Meanwhile, data is structured purposefully through different representations, such as large-scale networks and graphs.Therefore, data plays a pivotal role in technologies by introducing several challenges: how to present, what to present, why to present.Researchers explored various approaches to implement a comprehensive solution to express their results in every particular domain, such that the solution enhances the performance and minimizes cost, especially time complexity.In this chapter, we focus on data science as a crucial area; specifically focusing on curse of dimensionality (CoD) which is due to the large amount of generated/sensed/collected data, especially large sets of extracted features for a particular purpose.This motivates researchers to think about optimization and apply nature inspired algorithms, such as meta-heuristic and evolutionary algorithms (EAs) to solve largescale optimization problems.Building on the strategies of these algorithms, researchers solve large-scale engineering and computational problems with innovative solutions.Although these algorithms look un-deterministic, they are robust enough to reach an optimal solution.To that end, researchers try to run their algorithms more than usually suggested, around 20 or 30 times, then they compute the mean of result and report only the average of 20 / 30 runs' result.This high number of runs becomes necessary because EAs, based on their randomness initialization, converge the best result, which would not be correct if only relying on one specific run.Certainly, researchers do not adopt evolutionary algorithms unless they face a problem which is suffering from placement in local optimal solution, rather than global optimal solution.In this chapter, we first develop a clear and formal definition of the CoD problem, next we focus on feature extraction techniques and categories, then we provide a general overview of meta-heuristic algorithms, its terminology, and desirable properties of evolutionary algorithms.</p>
<p>Motivation</p>
<p>In the last twenty years, computer usage has proliferated significantly, and it is most likely that you could find technologies and computers almost anywhere you want to work and live.A large amount of data is being generated, extracted and presented through a wide variety of domains, such as business, finance, medicine, social medias, multimedia, all kinds of networks, and many others sources due to this spectacular growth.This increasingly large amount of data is often referred to as Big Data.In addition, distributed systems and networks are not performing as well as they did as in the past [1].Hence, it is imperative to leverage new approaches which optimize and learn to use these devices powerfully.Moreover, Big Data also requires that scientists propose new methods to analyze the data.Obtaining a proper result, thus, requires an unmanageable amount of time and resources.This problem is known as the curse of dimensionality (CoD) which is discussed in the next sub-section in detail.Ghareh mohammadi and Arabnia has discussed application of evolutionary algorithms on images, specifically focused on image stegnalaysis [2].But, in this study we expanded our investigation and consider large-scale engineering and science problems carefully.</p>
<p>In machine learning, the majority of problems require a fitness function which optimizes a gradient value to lead a global optimum accurately [3].This function is also known as an objective function and may have different structures for different problems.In machine learning, we work with three categories of data: one supervised, one semi-supervised and one unsupervised.These categories also have different learning processes based on their types.Supervised data sets are the most common data set and are characterized by having a ground truth with which to compare results.Supervised learning algorithms normally take a supervised data sets and then divide them into two parts: train and test.After that, one of the supervised learning algorithms learns from train data, predicts test data, and compares the result with the ground truth to ascertain the accuracy of the algorithm performance.The most common types of supervised learning algorithms are classification and regression.It is noteworthy that regression has different algorithms which mainly focus on time series problems.The only exception is that regression algorithms have a particular algorithm, Logistic regression, which is considered as a classification, rather than regression, algorithm [4].In this chapter, we focus on supervised data sets and supervised learning algorithms .</p>
<p>On the other hand, unsupervised learning algorithms follow the process of using unsupervised data sets which do not have any ground truth to compare their result, which makes classifying and evaluating the performance of the algorithm problematic.The absence of a ground truth is increasingly common through all domains such as web-based, engineering, etc data and it is necessary to address this problem.Unsupervised learning takes more steps to analyze features and find the most relevant features with the best possible positive relation.Clustering and representation learning (RL) algorithms are the most common algorithms in unsupervised learning category.K-means is an important clustering algorithm that attempts to find k clusters located close to each other.The main problem of k-means is its bias-k towards the problem.In other words, k-means needs to have k number set in advance before running the algorithms.RL also works for supervised data sets, although its nature behaves in an independent way per task [5].</p>
<p>Semi-supervised data sets fall somewhere between supervised and unsupervised data sets in terms of characteristics.This means that semi-supervised learning algorithms take a data set which provides ground truth value for some instances but not for others.Expectation maximization (EM) is the most important and robust technique for working with these data sets [6].More over, EM is also able to handle missing values of a given data set properly.Real data always involves missing values, and researchers struggle with this problem.</p>
<p>Feature extractor (FE) which is discussed in details in the next section, is almost universal techniques which are capable of applying on these three types of problems to aim for dimension reduction.Meanwhile, the majority of problems and data set have been so far used are supervised data sets.But it does not mean that FE does not apply on unsupervised or semisupervised data sets.For instance, for unsupervised data set, it is normal to use dimension reduction or auto-encoder techniques for that.</p>
<p>There has been numerous challenges in the literature regarding the deployment of evolutionary algorithms for computation, optimization and learning.These studies can be reviewed in the following major aspects: curse of dimensionality [7,8], nature-inspired computation (cite all papers from 2.4 here [9,1]), nature-inspired meta-heuristic computation (cite all papers from 2.5 here [10,11]), and nature-inspired evolutionary computation (cite all papers from 2.6 here [12,13,14,15,13]).These studies are elaborately reviewed in the following.</p>
<p>Curse of Dimensionality</p>
<p>Curse of dimensionality is related to the fact that the input data is too huge that no human being can analyze it.In Machine Learning, recently, researcher work with high-dimensional data.For instances, if we're analyzing 3 channel images, such as RGB, HSV images , sized 512x512, we're working in a space with 512<em>512</em>3 dimensions.Altman and Krzywinski [7] believe that having more data is much better than having few or nothing.This overabundance of data is called the curse of dimensionality (CoD) which causes problems in big data era such as data sparsity, multiple testing, which researchers [8] proposed a new approach to solve the problem, and most importantly over-fitting which is opposite of under-fitting.Beside these problems, CoD also brings high time complexity problem which makes scientists suffering from waiting too much time to get a result.</p>
<p>The world of Information technology CoD not only causes a wide range of problems to scientists, but also has a wide adversely affect other majors, such as engineering [16], medicine [17,18], cognitive science [19,20], bioinformatic [21], and even optimization problems [22,23,3].</p>
<p>Classification in Big Data suffers from plenty of problems and issues, one of which is considered very challenging named CoD.Traditional feature extraction techniques also are not able to solve this problem technically any more due to some limitation [23].According to the research studies have accomplished, scientist proposed a new approach to solve this problem.Researchers introduce nature-inspired computation which enable to simulate traditional feature extraction techniques in a way that improve the performance of classification.</p>
<p>Nature Inspired computation</p>
<p>Pure and basic machine learning algorithms are not capable of solving emerging challenging issues in the world of technologies any more.It is needed to adopt a new approach to face this problems and leverage decent machine learning algorithms.Finally, scientists discovered that combining machine learning algorithms in a technical way may solve the problems.This mixture of machine learning techniques is called nature-inspired computation, but it still is considered an advanced machine learning algorithms.</p>
<p>Majority of scientific and technological developments leverage inspiring from the nature towards their goal, especially robotics simulate how the nature works.In world of computer science, each tool or software development process is needed to have strong synchronization, robustness, manageability, parallelization, scalability, distributedness, redundancy , adaptability, cooperation.Indeed, the nature provides the same properties.Therefore, the nature-inspired techniques play an important role in computing environments.Concretely, the nature-inspired techniques are adopted to develop practical algorithms to solve data-driven optimization problems [9].</p>
<p>Researchers in [1,9] categorized nature-inspired computation.In [9] authors classified them into six different categories such as swarm intelligence, natural evolution, molecular biology, immune system and biological cells .But here, we provide another applicable way to express the nature-inspired computation towards solving problems.One meta-heuristic and one evolutionary computation.</p>
<p>Nature-inspired Meta-heuristic computation</p>
<p>A meta-heuristic is an advanced procedure developed to seek and generate a sufficiently tuned solution to data-driven optimization problems.[10].It involves , high level view, two types of computations.The first and foremost one is population based computation which is well-known as an evolutionary algorithms, second one is non-population computation such as Tabu search (TS), stochastic local search (SLS), iterated local search (ILS), guided local search (GLS).For more information about this classification, please refer to [11].Further, Razavi and Sajedi [24] proposed a single-based meta-heuristic algorithm, Vortex Search Algorithm (VSA), is inspired by the vortices.In this chapter, we mainly focus on the former classification, evolutionary algorithms which is discussed next sub-section properly.</p>
<p>Nature-inspired evolutionary computation</p>
<p>Evolutionary algorithms (EAs) is invented not more than 28 years and is not pretty old computational algorithm [12].Research studies have been accomplished new evolutionary algorithms in engineering and computational science [13,14,15].EAs are known as population based algorithm.Their learning process comes from interactions between multiple candidate solutions called food source or population.EAs are particular optimization type of meta-heuristics designed to solve optimization problems [13].This chapter discuss classical EAs and other popular methods including memetic algorithms (MA), particle swarm optimization (PSO), and artificial bee colony (ABC), ant colony optimization (ACO), grey wolf optimizer (GWO) and coyote optimization algorithm (COA).</p>
<p>Evolutionary-based Memetic algorithms</p>
<p>Memetic algorithms (MAs) are one of particular growing research studies within EA.Based on a population based search and local search, MAs have practically succeeded in a variety of engineering and science problem domains, in particular for NP-hard optimization problems [25,13].Memetic algorithms intrinsically exploit all available sources, however, traditional EAs fail to do that.Population based search MAs leverage recombination (or crossover operator) which is an important process within MAs.For the search process, it is essential to have three parameters ready: one neighborhood relation, one guiding function, and a search space which provides borders of the problem.</p>
<p>The search space is also important to provide comprehensive knowledge for guiding function works.The implication of search space is to influence the dynamics of the search algorithm.These dynamics stand for the relationships, which are accessible, among the configurations.Thus, these relationships depend on neighborhood function.For more information about this topic, please refer to [25].</p>
<p>Organization</p>
<p>The rest of this study is organized as follows.In section 2, we have discussed the feature extraction techniques and their categories.First, feature extraction from a sample object like image against feature extraction from given data sets are mentioned.Next, the feature extraction from data set has selected to discover it.It has three types including feature selection, auto-encoder and feature generation.Then, we introduce nature-inspired algorithms and their application, together with related pseudocode in solving large-scale engineering and science problems, particularly CoD problem.The summary of evolutionary algorithms have been discussed in this chapter is as follows: genetic algorithm (GA), artificial bee colony (ABC), ant colony optimization (ACO), grey wolf optimizer (GWO), coyote optimization algorithm (COA) and particle swarm optimization.In general, Figure 1 represents the overall structure of this study.</p>
<p>Feature Extraction Techniques</p>
<p>It is worth mentioning, in the world of science, "feature extraction" is used to refer to two completely separate applications.They are two different processes, one occurring before raw data generation and one taking place after data has generated.The process of feature extraction before having raw data works to extract features using some advancing techniques, to export information from the objects.For example, if we want to extract features from images, we need to adopt advanced image processing techniques, like a feature extractor, for that end.Therefore, based on the generated data, we will have a set of raw data.Then, in pre-processing techniques, a second type of feature extraction is used for dimension reduction.Three major differences separate these two types of feature extraction.The first difference is their input value; the input value of the first algorithm is not particular features, but the second feature extraction accepts only features of any data set.Second, the first type of feature extraction is domain specific, while the second type is not domain specific.Third, the former does not adopt machine learning algorithms, but the latter type does.Basically, both of them work with data, take values and generate outputs.The scope of the first algorithm is dynamic and would be any multimedia or social networks, etc.On the other hand, the second one has a almost stationary scope of input data.</p>
<p>General overview of testing and evaluating given data set is shown in fig 2. On the top of the figure, it clearly presents that three separate steps are required to be done in advance before generating a proper result.Pre-processing plays a main role in each problem of engineering and optimization problems.Then, A classification algorithm is selected to make a model based on the train data.finally, the classifier attempts to predict the test data based on the learned data.</p>
<p>Once data is generated and data set is ready to be evaluated, we call the data set, raw data set.This data set is needed to be converted into a standard data set which enables classifiers to examine in a professional way and obtain a higher performance.The most common problems of raw data set consist of curse of dimensionality (CoD), heterogeneous features in case of values and type, missing values, outliers.In this chapter, we discuss in detail how evolutionary algorithms (EAs) are adopted to solve the CoD problems, the bottom of the figure 2 depicts the idea where EAs are explicitly embedded into pre-processing and enhances the classifier's performance.Concretely, EAs attempts to optimize the process of feature extraction in an innovative way.</p>
<p>Feature extraction (FE), which is one of the most popular pre-processing techniques, is the process of shrinking the number of dimension (features) and the capability of having adapted diversity while considering strong mapping between features and target values.FE aims to decrease the feature dimension as minimum possible as it keeps the same performance.A feature extractor is considered as the best one which is capable of decreasing the feature dimension and meanwhile improving the performance.The better result obtained by the better FE.FE techniques are intrinsically classified into three broad groups: one auto-encoder, one feature selection (FS) and feature generation.The first two of which are the most common techniques in the scope of dimension reduction.Meanwhile researcher can leverage feature generation ( such as [26,27] )to improve a classifier performance.The former technique is also known as dimension reduction (DR) which attempts to transform given dimension to a new dimension with strong linear connectivity of original dimension.The most popular auto-encoder algorithm is principal component analysis (PCA).</p>
<p>PCA completely is used to generate a new dimension using a certain formula and convert the given data into new dimension.The idea behind PCA is that it leverages singular value decomposition (SVD) theorem to seek for the most relevant and correlated features and the relationship between each others.Although PCA is used to emphasize variation and bring out strong patterns in a data set, it may not guarantee to reach a optimal solution in some data sets.PCA fails once your special visualization of instances which leads to loss of information.It tries to convert input data into new dimension using a linear function.Circle-based and sine or cosine-based distribution of instances are the most popular situations that PCA fails.PCA failure means that the FE did not obtain a better performance while decreasing the dimension, not only that, but also it did not yield the same performance.If PCA does not yield a better result, it means that features are not correlated or have non-linear relationships.However, researchers often used to enable data easy to explore and visualize, in case for representation learning (RL) [28].</p>
<p>Feature selection (FS), the latter one, which is the process of choosing proper sets of relevant features rather than converting to a new dimension.FS covers the lack of autoencoders properly by keeping the original values of features during the process, meanwhile it is most likely to decrease the number of features / dimension.Feature selection mainly provides three kinds of categories: filter-based, wrapper-based and embedded FS.Filter-based FS is the easy technique to implement and can be adapted to each engineering problems independently.</p>
<p>It tries to examine given data set features separately non-dependently with respect to their target.It attempts to calculate the goodness of each feature separately.However, the wrapperbased feature selection relies on a set of selected features and calculated their goodness using classifiers.Wrapper-based FS is a special kind of filter-based FS such that wrapper-based FS has capability of using some hyper-parameter function for evaluation.Therefore, the pace of running filter-based is high in comparison with wrapper-based.So, it is recommended for real-time systems because of low time complexity.Furthermore, filter-based is cheaper than wrapper-based.But the wrapper-based feature selection [14,29,30] yields a better result than filter-based feature selection.By advancing technologies, wrapper-based FS also can be adopted in every system, even real-time decision making system [29].The third one, embedded feature selection which is similar to the wrapper-based feature selection to select the best subsets of features.However, it has a important drawback, which is time complexity in comparison with earlier feature selection, when it tries to train the model.One of the popular embedded feature selections is regularization which provides both training and making model section, together with automatic feature selection at the same time.Furthermore, researchers [31,32], proposed another type of feature selection, combined (hybrid) methods, which mixes evolutionary algorithms together with filter based or wrapper based algorithms.</p>
<p>Feature generation, is considered the third type of feature extractor techniques.Feature generation is a technique between feature selection and dimension reduction.It starts to examine the features and tries to generate features using the features.In this case, you first increase the feature dimension then remove irrelevant features.Unlike dimension reduction, no new dimension is generated.Feature generation keeps the original features for generating new features.Then, Feature generation can do feature selection based on the generated features [26].</p>
<p>Bio-inspired evolutionary computation</p>
<p>Engineering problems and other sensitive optimization need to reach the global optimum.However, machine learning algorithms are not useful anymore.So, it is required scientists adopt new kind of algorithms have been proved completely in nature for years.In this section, we provide general overview of nature-inspired algorithms and their terminology.Tables 1 and  2</p>
<p>Overview of evolutionary algorithms</p>
<p>Everything in EA starts to explain the problem and proper solutions.The first important step in evolutionary algorithm is representation.After that, in each step, EA works based on this representation.Figure 2 depicts a general overview of each evolutionary algorithm's procedure.</p>
<p>It is extremely necessary how to present your sample solutions.Two approaches are given: an one-hot representation and an integer representation.The former one is also known as binary representation.The number of "1" in the solution shows the number of parameters have to be involved to yield a result."1"represents that which specific features are selected and "0" stands for the features which are not considered in a specific solution.In this case, your solution's length would be as same as the input feature dimension.If feature dimension become too big, handling the food source are going to be a challenging issues which waste resources and yields high time complexity.However, the integer representation works good even with high feature dimension.But it still has a big disadvantage which you need to set the reduced length of your feature vector in initialization step.Second important step is generating a population based on the descriptive model of representation.This population mostly is generated randomly with considering the representation limitation.Third step is fitness function and evaluation process.It is important to provide a tuned fitness function (objective function) towards their application of the evolutionary algorithms.</p>
<p>The next step is to select two possible solutions as parents of new generations.Selection strategy has two broad categories.One uniform parent selection and one un-uniform parent selection.In the former one, each solution has the same chance to be selected.However, the latter one has different structures and criteria, and parents are selected based on those.The ununiform parent selection has different strategies, the most important strategies is proportional selection which is also as known as roulette wheel, ranked based selection, and tournament selection.</p>
<p>Roulette Wheel and Tournament are the most widely used selection methods in GA.Roulette consider the fitness value fore each chromosomes with respect to their probabilities, using the equation 1 where p[i] stands for the probability of selecting a specific chromosome i, f[i] goes for the fitness value of each chromosome of index i.
p[i] = f [i] f <a href="1">i</a>
Moreover, the Tournament selection is pretty simpler than Roulette wheel.The idea is that it takes k chromosomes and selects based on the fitness value of each chromosome.The best fitness value goes for the lucky chromosome to be selected.</p>
<p>After that, EA tries to reproduce new generation and updated the population.EA take two parents and regenerates new offspring based on crossover operator.The crossover or recombination, which is one of genetic operators used to recombine two chromosomes to generate new offspring.The crossover operator includes uniform crossover, arithmetic crossover and k-point crossover which is a classical one.Once crossover step is done, mutation should be done with a specific rate.The mutation may change one or more components.</p>
<p>Finally, the stall condition which is set to check once new generation produced.If the new generation met the condition, EA stops running and return the best the solution which satisfied the condition.</p>
<p>Genetic algorithm v.s genetic programming</p>
<p>It is a common mistake that to think Genetic algorithm (GA) is the same genetic programming (GP).Generally speaking, researchers have used these two algorithms interchangeably.But, from a technical point of view they are completely different techniques.In this sub-section, we provide a clear definition of each of them.</p>
<p>Genetic algorithm</p>
<p>Genetic algorithm is one of the basic but important evolutionary algorithm.It has been applied on majority of problems such as engineering, medicine, finance, etc. GA provides two kinds of approaches towards solving problems [33].One steady state genetic algorithm (SSGA) and one generational genetic algorithm (GGA).They are different based on their procedure and updating mechanism function of whole process, but they do the same process of parent selection, reproduction and population update.In the literature, some studies deployed GA as an effective tool for solving large-scale optimization problems, including optimal allocation of electric vehicle charging station and distributed renewable resource in power distribution networks [34], resource optimization in construction projects [35], and allocation of electric vehicle parking lots in smart grids [36].Algorithm 1 illustrates a pseudo code of basic GA in detail.</p>
<p>Algorithm 1 Implementation of GA algorithm for feature selection Input: S = {x 0 , x 1 , x 2 , ..., x n }, max iteration  0, t=0,  M  [0, 1], random number  [0, 1],</p>
<p>Best solution = .</p>
<p>Output: Best solution : Anoptimalsubseto f f eatures(F) , F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .
1: for t=0    max iteration do 2:
Call parent selection function</p>
<p>3:</p>
<p>Call crossover method to generate offspring 4:</p>
<p>if random number   M then 5:</p>
<p>Call mutation function
6:
Return offspring
7:
end if</p>
<p>8:</p>
<p>Call fitness function to evaluate the chromosome 9:</p>
<p>if any chromosome obtained the best score then 10:</p>
<p>Update the Best solution 11:</p>
<p>end if</p>
<p>12: end for</p>
<p>In SSGA, GA works with a stationary population which the size of that will be the same and just it's solutions get updated each iteration.Moreover, SSGA is an in-place algorithms which their population do not need another space to update.Like normal process, SSGA also starts with a problem representation and fitness function, then initialize the selection strategy, crossover and mutation operators.After that, SSGA takes another step to update the population with replacement strategy.Figure 3 depicts that how two solution are selected, crossover and mutation operators are applied and then new solution is replaced with the worse solution.From technical point of view, scientists can apply either GGA or SSGA based on the problem model and strategies.However, SSGA converges faster than GGA since parents always are selected through the same population and then replaced the worse solution with the another best solution.Hence, most of research studies are accomplished using SSA.Moreover, most Evolutionary algorithms are discussed here also use the same strategies to converge faster towards global optimum.But, SSGA still has a disadvantage that may stuck in a local optimum.</p>
<p>Genetic programming</p>
<p>Genetic programming (GP) is proposed by Koza in 1992 [37].It is noteworthy that this idea is introduced date back to 50s.GP evolves computer programs which are represented as trees.Each tree consists of two sections: a function set and second is terminal set.Both of them provides constant sets of symbols.The former one always play non-leaf nodes role and the latter one plays leaf nodes role.Figure 5 shows an example of presenting a problem 4 * tan(x) + 2 .. Similar to GA that crossover is conducted on vectors, in GP crossover is done through a tree and only needs to choose two sub-tree.Figure6 expresses that the first two tree has two subset which are selected as a parent.Second tree the below are the new offsprings which are generated based on parents.GP is mostly generational genetic algorithm.Thus, GP is not a in-place algorithm.GP is useful for solving engineering and computational problems (e.g., [38]).Genetic programming has specific advantages over genetic algorithms.Here, we address the most important characteristics of GP.Genetic programming has a wide variety of representation model which makes it pretty flexible against genetic algorithm.This flexibility of GP comes from it's tree-based properties.Another important feature of GP is its application over GA.GP has greater applications in comparison with GA.In spite of considering positive features of GP, it also has disadvantages which should bear in mind.The most disadvantages of GP is its speed which is extraordinary slow.Another point is its lack of handling a large number of input data which makes also hard to handle required related population.</p>
<p>There is still another algorithm that attracts researcher's attention called evolutionary programming (EP).Fogel et al [39] originally introduced evolutionary programming.It is classified as one of the major evolutionary algorithms.It resembles genetic programming, but it does have a non-variable structure of the program to be optimized.Classical EP develops gradually finite state machine or every structure similar to it.EO always works with mutation only and does not consider crossover at all.It worth mentioning that EP uses a fitness function based on the training sequences.This feature enables EP yields a better result for prediction in time series problem and sequence problems like DNA and RNA.</p>
<p>Artificial bee colony algorithm</p>
<p>In the bees population, the process of mating and generating new offspring, finding new food sources and gathering the nectar, sharing information in hive, allocating tasks, onlooker and scout bees; all of these have been inspired properly and nature-based evolutionary algorithms have been presented.To be specific about the algorithms, honey bee mating optimization (HBMO), bee colony optimization, bee algorithm (BA) and artificial bee colony (ABC) are the most popular research studies are accomplished based on these algorithms [40].Karaboga et al [40] presents statistical overview of using these algorithms in scientific papers.It is worth mentioning that ABC has received the highest amount of usage with respect to the its application in engineering and science problems.Among all research studies had been done, according to the [40] ABC, BA, BCO and HBMO are found the most useful application, from the highest number to the lowest number, in large scale engineering problems.ABC has been considered as the most useful algorithm in several different fields and majority of research studies leverage ABC in their problems, such as: training neural network (NN), solving electrical, mechanical, software, control and civil engineering problems, facing wireless sensor networks issues, optimizing protein structure and most importantly solving image processing problems.In this chapter, we address emerging challenges like CoD problem in Big Data and provide practical engineering solutions using ABC and other related algorithms.</p>
<p>Here, we will discuss artificial bee colony (ABC) which is inspired by a set of sequential processes such as the process of seeking for a bunch of flowers, sharing information in the hive regarding that and allocating employed, onlooker and scout bees.Karaboga introduced ABC [41] which is compatible with continues problems in 2005.Algorithm 2 presents a general procedure of given ABC.A large number of research studies have accomplished using this algorithm [42,43] and even convert that into a way that it also works with discrete problems [14,30,18].Not only those, but also ABC is applied on optimization problems as an optimizer [44,45,46,43] Artificial bee colony interact with three groups of bees to have work done.The first group is employed bees, second is onlooker and last one is scout.In initialization step the number of these group are set.The employed bees, together with onlooker bees create a population which has an equal amount of two groups.ABC starts with initialization step which has positive impact on converging in ABC.Among initialization variables, limit is important criteria and provides a condition when an employed bee converts into scout bee; at a time, we only have one scout bee.</p>
<p>Algorithm 2 Implementation of ABC algorithm for feature selection Input: S = {x 0 , x 1 , x 2 , ..., x n }, P size =2 * n, limit  0, 0  lower Bound  n/2, lower Bound  upper Bound  n, max iteration  0, t=0 , = random number  [0, 1] , = random number  [0, 1], Best solution = .</p>
<p>Output: Best solution : An optimal subset o f f eatures (F) , F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .</p>
<p>1: Call fitness function to evaluate the whole food source (S) (primary evaluation of each food)
2: for { dot=0    max iteration } 3:
Call Employed bees to update the food source regarding their evaluation</p>
<p>4:</p>
<p>Call Onlooker bees to exploit the local foods to generate new food (solution)</p>
<p>5:</p>
<p>Choose parents and generate a new food (solution) based on
V i = f i + * (f i  f j ) 6:
if limit is met then :
7:
{ Call scout bee to explore new (unseen) food source to prevent from local optimum using 8:
X i = X upper Bound + * { (X upper Bound -X lower Bound )} } return N ewSolution 9: end if 10:
Call fitness function to evaluate the Solution</p>
<p>Particle swarm optimization algorithm</p>
<p>The particle swarm optimization (PSO) is one of the population-based meta-heuristic algorithms and optimization techniques.PSO is inspired from social-psychological principles [47].In 1995 Particle swarm optimization first introduced by Kennedy and Eberhart [48].The PSO is based on the simulation of common animal social behaviors, for instances: fish schooling, bird flocking.PSO like other evolutionary algorithms searches for the global optimum rather than local optimum.However, the particle swarm trapped into local optimum easily when feature dimension grows significantly.Algorithm 3 presents a pseudo code for a standard PSO.The whole process of PSO usually initialize groups of random particles and computes fitness for each particle within iterations in order to converge into global optimum.Each particle is considered as a single solution to our problem.</p>
<p>PSO follows two simple yet essential steps to have completed optimization process to find the minimum optimum or maximum optimum.The first step is communication among particles.Each particle shares their information with other particles after moving in their direction.This process makes them find a proper way toward the goal.Each time, based on the problem (maximum / minimum optimization), particles follows the particle and consider the particle that match the problem goal.For instance, each iteration particles call fitness function to get fitness of their location.Then, among the particles, one has the best value which is set to best personal location.The best value is examined based on the problem, if it is minimum optimization then the best value goes for the particle that has the minimum value.Moreover, if the problem is maximum optimization the best value goes for the particle that has the maximum value.When this value is set, each particle updates their direction and moves toward this values.It is obvious that the one has the best value does not move unless other particles find the best value.The second step which each particle does is to learn.They can learn how to update their direction after each iteration and tune the parameters.</p>
<p>The PSO does not have parent selection, recombination and mutation steps [49]; thus, this enables PSO to behave in a particular way in comparison with other evolutionary algorithms.Concretely, each member within the population do not get updated nor removed.Hao et al [50] introduced a new PSO with added crossover operator.Zhang et al [51] proposed a binary PSO with mutation operator to address CoD problem using feature selection techniques to solve it.The crossover enables the particles does not stop in the local optimum by sharing the other particles' information.In [23] PSO is classified into three different versions: classical PSO, scale-free PSO and binary PSO.</p>
<p>Few parameters are required to adjust, and enable PSO easy to implement, make popular stochastic and yet powerful swarm-based algorithm.Inertia weight becomes more important than other due to it's ability of having a trade-off between the exploration and exploitation process within a search space.In addition, inertia weight has positive affect convergence rate in PSO [52].</p>
<p>Algorithm 3 Implementation of PSO algorithm for feature selection Input: S = {x 0 , x 1 , x 2 , ..., x n } , particles number  1, acceleration c oe f f icients(c 1 , c 2 )  [0, 1], max elocit , t=0, min w ei ht, max w ei ht = random number  [0, 1] , Best solution =  .</p>
<p>Output: Best solution : Anoptimalsubseto f f eatures(F) , F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .
1: for t=0    max iteration do 2:
for i=0    particles number do 3:</p>
<p>Call fitness (= objective) function to for the current particle 4:</p>
<p>Save the best personal location if condition met then 13:</p>
<p>Return the best global location as the global optimum 14:</p>
<p>end if</p>
<p>15: end for</p>
<p>In the literature, some studies deployed PSO as an effective tool for solving large-scale optimization problems, including optimal allocation of electric vehicle charging station and distributed renewable resource in power distribution networks [34], designing power system stabilizers [53], distribution state estimation [54], and reactive power control [55].</p>
<p>Ant colony optimization (ACO)</p>
<p>Ant colony optimization is another popular evolutionary algorithms which is presented in 1999 by Dorigo, Marco and Di Caro [56], and Socha and Dorigo introduced continues domain of it [57].Basically, ACO is one of stochastic search processes.Once Ants explored a new food source, they try to lay some pheromone to mark the way which leads to the food.The pheromone is a chemical odorous material which is produced and used by ants to communicate with other ants in an indirect way.Each ant tries to produce it and lays it on their way.So Others can follow the odorous to seek for the food, meanwhile they also produce the same amount of pheromone.On the other hand, Further, as we inspired natural behavior, this chemical material is susceptible to be evaporate.Thus, the amount of pheromone on specific path will increase by keeping ants on the same path, However, each iteration we have reduction which this amount have negative affect the total amount of pheromone on a particular path.In other words, if any ants do not select the path used to be chosen, then the path would disappear.Algorithm 4 presents an overall procedure of ACO for feature selection.</p>
<p>Algorithm 4 Implementation of ACO algorithm for feature selection [58] Input: S={x 0 , x 1 , x 2 , ..., x n } , K}  1,  and  , t = 0, best solution =.Output: Best solution : An optimal subset o f f eatures (F) , F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .for each ant  Ants(K) do</p>
<p>5:</p>
<p>Generate a subset of features Grey Wolf Optimizer (GWO) is pretty new evolutionary algorithm which has been presented not sooner than 2014 which primary works based on the concept of grey wolf society [59].Mirjalili and et al claimed that [59] GWO outperforms other evolutionary algorithms for solving large-scale engineering and science problems.The GWO algorithm inspired by the natural mechanism of animals.The most common behavior which almost wild animal inherited normally are their attitude to have a kingdom, rule others and having the same hunting mechanism.It solves the science problems through the following steps:</p>
<p>-First of all, it searches for some animal as prey.In other words, it tries to explore the area (food source); -Then, it surrounds the possible prey(s) by exploitation, doing local search to find the border of sample space; -Finally, it attacks the prey, doing local search to find the best value within a new area.'A' stands for the most important parameter in GWO and adjusts the step size towards the prey.Thus, 'A' has positive impact on convergence of this algorithm to the global optimum by tuning step size which influences both exploitation and exploration.However, GWO still suffers from stalling in local minimum, So initializing the parameter 'A' with a proper value helps it to prevent from stopping in local minimum.</p>
<p>Algorithm 5 Implementation of GWO algorithm for feature selection Input: S = {x 0 , x 1 , x 2 , ..., x n }, X i = (i = 1, 2, ...n), A, t=0 , , C, max iteration  0, Best solution  S len th .</p>
<p>Output: Best solution : An optimal subset o f f eatures (F) , F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .</p>
<p>1: Call fitness function for each search agent to evaluate the whole food source (S) (primary evaluation of each food) for each search agent do   Coyote Optimization Algorithm (COA) is another yet important population-based meta-heuristic algorithms which have been inspired from the Canis latrans species and natural coyotes' behaviour.COA has a very certain procedure that works based on the way how these animals approaching other animals (preys) for catching them.Thus, COA seems to be one particular type of Grey Wolf Optimizer (GWO) as COA just does the third step of GWO.COA is presented recently in [13]  for each c coyotes in the p pack do 12:
7: U pdatethebestpositiono f currentsearcha entusin    X t+1 =   X 1 +   X 2 +   X 3 3
Update the social condition using: Examine the new social condition using: update food source with respect to better fitness using: Birth and death using:
20: pup p,t j =              soc p,t
r 1 ,j , rnd j &lt; P s or j = j 1 soc p,t r 2 ,j , rnd j  P s + P a or j = j 2 R j , otherwise Update the coyotes' information with respect to the age call fitness function to compute the fitness using chicken if fitness of chicken ==worst f itness then 8:</p>
<p>Update the chicks position if fitness of chicken != worst f itness and fitness of chicken != best f itness then 11:</p>
<p>U pdate the Random alue 12:</p>
<p>U pdate the hens position  [64].FSA inspired from the behaviors of fish school.Algorithm 8 shows the process of feature selection using FSA.research studies have applied FSA to optimize their solution such as neighborhood feature selection [65], multi-modal benchmark functions solver [66].</p>
<p>Algorithm 8 Implementation of FSA algorithm for feature selection [13] Input: S = {x 0 , x 1 , x 2 , ..., x n }, t=0 , max iteration  0, R min , L min ,  B (D)
= |POS B (D)  | |U |
, Best ca otes = .</p>
<p>Output: best ca otes : An optimal subset of features (F) , R min = F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .for each fish K  Fish do The f ish K obtained a local reduction and break Both dimension reduction by generating new dimension of features and feature selection by eliminating irrelevant and redundant features take care of missing values and classify supervised / unsupervised data sets; all of these operations come together to solve emerging challenging Np-hard problems in engineering and the sciences.A large number of data sets, particularly Big Data, are available to work on.The main problem, here, concerns their features and dimensionality, the curse of dimensionality (CoD), which causes yet another important problem, high time complexity.In this chapter, we addressed these problems and professional approaches using advanced machine learning algorithms.The studies prove that applying nature-inspired algorithms, together with machine learning techniques, enabled researchers' attempts to solve the CoD problem, which yields a proper running time with a lowest time complexity.It is noteworthy that evolutionary algorithms are non-dependent domain specific, which provides an optimized environment for researchers who want to solve their problems or optimize their approaches.In this chapter, we have explored evolutionary algorithms and their applications in solving large scale optimization problems, especially the feature extraction process for data analytics.This chapter provides insightful information for researchers who are seeking for the application of evolutionary algorithms for engineering, optimization, and data science.Having said this, in [67], we address the emerging problem, CoD, and an evolutionary-based solution is presented to solve it.We discuss the feature extraction optimization process in detail, leveraging feature extraction and evolutionary algorithms.Then, we provide detailed and practical examples of applying evolutionary algorithms with a wide variety of domains.we also classify all research studies based on the most common challenging issues such as stego image classification, network anomalies detection, network traffics classification, sentiment analysis and supervised benchmark classification. .</p>
<p>Figure 1 :
1
Figure 1: Overall structure of this study</p>
<p>Figure 2 :
2
Figure 2: General process of evolutionary algorithms</p>
<p>Figure 3 :
3
Figure 3: SSGA (steady state genetic algorithm): Process of updating the population</p>
<p>Figure 4 :
4
Figure 4: GGA (generational genetic algorithm): The process of generating a new population (generation t+1)</p>
<p>Figure 5 :
5
Figure 5: Tree presentation of a problem</p>
<p>Figure 6 :
6
Figure 6: Crossover operator in genetic programming</p>
<p>1 : 3 :
13
Call fitness function to calculate the fitness of each feature 2: for t=0    max Iteration do</p>
<p>.</p>
<p>9 :
9
Update  , A and C.</p>
<p>10 : 7
107
Call fitness function to calculate the fitness of each search agent11: Update X  , X  , X  12:if any solution obtained the best score then Coyote optimization algorithm (COA)</p>
<ul>
<li>r 1 
1
 1 + r 2   2 14:</li>
</ul>
<p>21 :
21
Transition betweenN c andN p packs usin P e = 0.005  N 2 c 22:</p>
<p>3 :fitness of chicken ==best f itness then 4 :
34
if</p>
<p>1 :
1
R min =C , L min =C 2: for t=0    max iteration do 3:generate total fish (Fish) 4:</p>
<p>6 : 9 : 13 :
6913
Choose a f eature  k  C(randoml )7: U pdateR K , L K b R K  K and |R K |, respecti el 8:end for for each fishK  Fish do 10:R s = Search(R k ) 11: R  = Swarm(R k ) 12: R f = Follow(R k ) UpdateR K , L K by seeking for the max f itness through (R k , R  , R f ) 14: if  R k (D)  == C (D)  then 15:</p>
<p>if  R k (D)  == C (D)  andL K  L min then 18:updateR min , L min b R K andL K , respecti el 19:</p>
<p>Table 1 :
1
provide complete definitions for abbreviation which are used in this chapter.List of Abbreviations
AbbDefinitionABCArtificial bee colonyACOAR Ant colony optimization attribute reductionBABee algorithmBCOBee colony optimizationBOAButterfly optimization algorithmCNNConvolutional neural networkCOACoyote Optimization AlgorithmCoDCurse of dimensionalityCSOChicken swarm optimizationCCSO chaotic chicken swarm optimizationCROCoral reefs optimizationDADragonfly algorithmDRDimension reductionEAsEvolutionary algorithmsFEFeature extractionEMExpectation maximizationEPEvolutionary programmingFSFeature selectionFSAFish swarm algorithm</p>
<p>Table 2 :
2
List of Abbreviations (Continued)</p>
<p>2 :
2
X  = the best search agent 3: X  = the second best search agent 4: X  = the third best search agent 5: for t=0    max iteration do
6:</p>
<p>by Pierezan and Coelho in 2018 to solve large-scale optimization problems.Algorithm 6 presents a general overview of COA for feature selection.=lowerBoundj+ j  (upper Boundj  lower Boundj ), t=0 , max iteration  0, Best ca otes = .Output: Best ca otes : An optimal subset o f f eatures (F) , F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .Calculate the social tendency of the pack based on N c as follows:
Algorithm 6 Implementation of COA algorithm for feature selection [13]Input: S={x 0 , x 1 , x 2 , ..., x n }which consists o f N pN  *  andN cN  *  are initialized usin socp,t c,j1: Call fitness function to calculate the coyote's fitness using:2: f itp,t c = f (socp,t c )3: for t=0    max iteration do4:alpha p,t = {socp,t c |ar c={1,2,...,N c } min f (socp,t c )}5:6:if N c is odd then7: 8: 9:cult else : p,t j = cult p,t j =Op,t Nc 2 , jp,t (Nc +1) 2 ,j p,t O +O ( Nc 2 +1), j 210:end if11:</p>
<p>Implementation of CSO algorithm for feature selection Input: S = {x 0 , x 1 , x 2 , ..., x n }, N p  N * , N c  N * are done usin soc p,t c,j = lower Boundj + j  (upper Boundj lower Boundj ) , t=0 , rooster ratio , chicks ratio , hens ratio , f ood position C, Random alue , min iteration , max iteration , chickenSwarm size .Output: Best solution : Anoptimalsubseto f f eatures(F) , F = {x 0 , x 1 , x 2 , ..., x m } , m n , (f i  F )  S , F len th  S len th .
23: 24: 25: Meng et al [60] proposed chicken swarm optimization algorithm (CSO) in 2014. Algorithm if stop condition met then Return the Best co otes end if 7 presents well-structured pseudocode of CSo for optimized feature selection. Based on per-formance of CSO, researchers have successfully solved and optimized engineering and science problems, Directional Reader Antennas Optimization [61], Community detection in social net-works [62], parameters Optimization of a fuzzy logic system [63]. 26: end for 4.8 Other optimization algorithms Algorithm 7
1: for t=0    max iteration do 2:</p>
<p>Li et al introduced fish swarm algorithm (FSA) which is another population-based ( or swarm-based) evolutionary algorithm
13:end if14:U pdate chicken position15:if t==chickenSwarm size then16:Return the best position as the global optimum17:end if18: end for</p>
<p>Nature-inspired computing technology and applications. P Marrow, BT Technology Journal. 184Oct 2000</p>
<p>Farid Ghareh, Mohammadi Hamid R Arabnia, arXiv:1907.12914Isea: Image steganalysis using evolutionary algorithms. 2019arXiv preprint</p>
<p>Niru Maheswaranathan, Luke Metz, George Tucker, Jascha Sohl-Dickstein, arXiv:1806.10230Guided evolutionary strategies: escaping the curse of dimensionality in random search. 2018arXiv preprint</p>
<p>A review of regression procedures for randomized response data, including univariate and multivariate logistic regression, the proportional odds model and item response model, and self-protective responses. Jlf Maarten, Ulf Cruyff, Bckenholt, G M Peter, Laurence E Van Der Heijden, Frank, Handbook of Statistics. Elsevier201634</p>
<p>Supervised representation learning: Transfer learning with deep autoencoders. Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015Sinno Jialin Pan, and Qing He</p>
<p>Semi-supervised learning for discrete choice models. Jie Yang, Sergey Shebalov, Diego Klabjan, IEEE Transactions on Intelligent Transportation Systems. 2018</p>
<p>The curse (s) of dimensionality. Naomi Altman, Martin Krzywinski, Nat Methods. 152018</p>
<p>Wenge Guo, Gavin Lynch, Joseph P Romano, arXiv:1812.00258A new approach for large scale multiple testing with application to fdr control for graphically structured hypotheses. 2018arXiv preprint</p>
<p>A reminiscent study of nature inspired computation. Shilpi Gupta, Shweta Bhardwaj, Parul Kalra Bhatia, International Journal of Advances in Engineering &amp; Technology. 121172011</p>
<p>Stellar-mass black hole optimization for biclustering microarray gene expression data. Balamurugan, Natarajan, Premalatha, Applied Artificial Intelligence. 2942015</p>
<p>Metaheuristics in combinatorial optimization: Overview and conceptual comparison. Christian Blum, Andrea Roli, ACM computing surveys (CSUR). 3532003</p>
<p>Evolutionary computation 1: Basic algorithms and operators. Thomas Bck, David B Fogel, Zbigniew Michalewicz, 2018CRC press</p>
<p>Coyote optimization algorithm: a new metaheuristic for global optimization problems. Juliano Pierezan, Leandro Dos, Santos Coelho, IEEE Congress on Evolutionary Computation (CEC). 2018. 2018IEEE</p>
<p>Image steganalysis using a bee colony based feature selection algorithm. Ghareh Mohammadi, M Saniee, Abadeh , Engineering Applications of Artificial Intelligence. 312014</p>
<p>A new metaheuristic feature subset selection approach for image steganalysis. Farid Ghareh, Mohammadi , Mohammad Saniee, Abadeh , Journal of Intelligent &amp; Fuzzy Systems. 2732014</p>
<p>Uncertainty quantification in an engineering design software system. Dirk Wunsch, Rmy Nigro, Grgory Coussement, Charles Hirsch, Uncertainty Management for Robust Industrial Design in Aeronautics. Springer2019</p>
<p>Precision medicine and the cursed dimensions. L Dennis, Barbour, npj Digital Medicine. 2142019</p>
<p>Automated diagnosis system for alzheimer disease using features selected by artificial bee colony. N Shunmuga, Srinivasa Karpagam, Raghavan, Journal of Computational and Theoretical Nanoscience. 1622019</p>
<p>Do additional features help or hurt category learning? the curse of dimensionality in human learners. Wai Keen, Vong , Andrew T Hendrickson, Danielle J Navarro, Amy Perfors, Cognitive science. 433e127242019</p>
<p>P Niketu, Elie Patel, Mitchell H Sarraf, Tsai, The curse of dimensionality. Anesthesiology: The Journal of the American Society of Anesthesiologists. 2018129</p>
<p>Taxonomy-aware feature engineering for microbiome classification. Mai Oudah, Andreas Henschel, BMC bioinformatics. 1912272018</p>
<p>Stochastic shape optimization via design-space augmented dimensionality reduction and rans computations. Andrea Serani, Matteo Diez, Jeroen Wackers, Michel Visonneau, Frederick Stern, AIAA Scitech 2019 Forum. 20192218</p>
<p>Big data classification using scale-free binary particle swarm optimization. Lal Sonu, Anurag Singh Gupta, Asif Baghel, Iqbal, Harmony Search and Nature Inspired Optimization Algorithms. Springer2019</p>
<p>Svsa: a semi-vortex search algorithm for solving optimization problems. Seyedeh Fatemeh, Razavi , Hedieh Sajedi, International Journal of Data Science and Analytics. 2018</p>
<p>An accelerated introduction to memetic algorithms. Pablo Moscato, Carlos Cotta, Handbook of Metaheuristics. Springer2019</p>
<p>An efficient feature generation approach based on deep learning and feature selection techniques for traffic classification. Hongtao Shi, Hongping Li, Dan Zhang, Chaqiu Cheng, Xuanxuan Cao, Computer Networks. 1322018</p>
<p>Feature engineering for predictive modeling using reinforcement learning. Udayan Khurana, Horst Samulowitz, Deepak Turaga, Thirty-Second AAAI Conference on Artificial Intelligence. 2018</p>
<p>Network representation learning: A survey. Daokun Zhang, Jie Yin, Xingquan Zhu, Chengqi Zhang, IEEE transactions on Big Data. 2018</p>
<p>Novel wrapper-based feature selection for efficient clinical decision support system. R Vanaja, Saswati Mukherjee, International Conference on Intelligent Information Technologies. Springer2018</p>
<p>Pareto front feature selection based on artificial bee colony optimization. Emrah Hancer, Bing Xue, Mengjie Zhang, Dervis Karaboga, Bahriye Akay, Information Sciences. 4222018</p>
<p>A hybrid genetic algorithm with wrapper-embedded approaches for feature selection. Xiao-Ying Liu, Yong Liang, Sai Wang, Zi-Yi Yang, Han-Shuo Ye, IEEE Access. 62018</p>
<p>Particle swarm optimization based feature selection with novel fitness function for image steganalysis. Vahid Rostami, Azar Shahmoradi, Khiavi , Artificial Intelligence and Robotics (IRANOPEN). 2016. 2016IEEE</p>
<p>A steady-state and generational evolutionary algorithm for dynamic multiobjective optimization. Shouyong Jiang, Shengxiang Yang, IEEE Transactions on Evolutionary Computation. 2112016</p>
<p>Simultaneous allocation of electric vehicles' parking lots and distributed renewable resources in smart power distribution networks. Mohsen Hadi Amini, Orkun Parsa Moghaddam, Karabasoglu, Sustainable Cities and Society. 282017</p>
<p>Genetic algorithms in resource optimization of construction project. Gang Ly Zhang, Luo, Lu, Journal of Tianjin University (Science and Technology). 3422001</p>
<p>Allocation of electric vehicles' parking lots in distribution network. M Hadi, Amini , Arif Islam, ISGT 2014. IEEE2014</p>
<p>Genetic programming: on the programming of computers by means of natural selection. John R Koza, 1992MIT press1</p>
<p>Knowledge discovery in multiobjective optimization problems in engineering via genetic programming. Igor Ls Russo, Heder S Bernardino, J C Helio, Barbosa, Expert Systems with Applications. 992018</p>
<p>Artificial intelligence through a simulation of evolution. J Lawrence, Fogel, Proc. of the 2nd Cybernetics Science Symp. of the 2nd Cybernetics Science Symp1965. 1965</p>
<p>A comprehensive survey: artificial bee colony (abc) algorithm and applications. Dervis Karaboga, Beyza Gorkemli, Celal Ozturk, Nurhan Karaboga, Artificial Intelligence Review. 4212014</p>
<p>An idea based on honey bee swarm for numerical optimization. Dervis Karaboga, 2005Erciyes university, engineering faculty, computer . . .Technical report-tr06</p>
<p>A novel history-driven artificial bee colony algorithm for data clustering. Farzaneh Zabihi, Babak Nasiri, Applied Soft Computing. 712018</p>
<p>Xiuqin Pan, and Na Sun. An improved global best guided artificial bee colony algorithm for continuous optimization problems. Yongcun Cao, Yong Lu, Cluster computing. 2018</p>
<p>A self-adaptive artificial bee colony algorithm based on global best for global optimization. Yu Xue, Jiongming Jiang, Binping Zhao, Tinghuai Ma, Soft Computing. 2018</p>
<p>Modified multiple search cooperative foraging strategy for improved artificial bee colony optimization with robustness analysis. Fatima Harfouchi, Hacene Habbi, Celal Ozturk, Dervis Karaboga, Soft Computing. 22192018</p>
<p>An improved optimization method based on krill herd and artificial bee colony with information exchange. Heqi Wang, Jiao-Hong Yi, Memetic Computing. 1022018</p>
<p>Hybrid particle swarm optimization with spiral-shaped mechanism for feature selection. Ke Chen, Feng-Yu Zhou, Xian-Feng Yuan, Expert Systems with Applications. 2019</p>
<p>Particle swarm optimization. R James Kennedy, Eberhart, Proceedings of IEEE international conference on neural networks. IEEE international conference on neural networksIEEE Press19954</p>
<p>Particle swarm optimization. Encyclopedia of machine learning. James Kennedy, 2010</p>
<p>A particle swarm optimization algorithm with crossover operator. Zhi-Feng Hao, Zhi-Gang Wang, Han Huang, 2007 International Conference on Machine Learning and Cybernetics. IEEE20072</p>
<p>Binary pso with mutation operator for feature selection using decision tree applied to spam detection. Knowledge-Based Systems. Yudong Zhang, Shuihua Wang, Preetha Phillips, Genlin Ji, 201464</p>
<p>Particle swarm optimization with probabilistic inertia weight. Ankit Agrawal, Sarsij Tripathi, Harmony Search and Nature Inspired Optimization Algorithms. Springer2019</p>
<p>Optimal design of power-system stabilizers using particle swarm optimization. Ma Abido, IEEE transactions on energy conversion. 1732002</p>
<p>Practical distribution state estimation using hybrid particle swarm optimization. Shigenori Naka, Takamu Genji, Toshiki Yura, Yoshikazu Fukuyama, 2001 IEEE Power Engineering Society Winter Meeting. Conference Proceedings (Cat. No. 01CH37194). IEEE20012</p>
<p>A particle swarm optimization for reactive power and voltage control considering voltage security assessment. Hirotaka Yoshida, Kenichi Kawata, Yoshikazu Fukuyama, Shinichi Takayama, Yosuke Nakanishi, IEEE Transactions on power systems. 1542000</p>
<p>Ant colony optimization: a new meta-heuristic. Marco Dorigo, Gianni Di, Caro , Proceedings of the 1999 congress on evolutionary computation-CEC99 (Cat. No. 99TH8406. the 1999 congress on evolutionary computation-CEC99 (Cat. No. 99TH8406IEEE19992</p>
<p>Ant colony optimization for continuous domains. Krzysztof Socha, Marco Dorigo, European journal of operational research. 18532008</p>
<p>A new hybrid ant colony optimization algorithm for feature selection. Md Monirul Kabir, Md Shahjahan, Kazuyuki Murase, Expert Systems with Applications. 3932012</p>
<p>Grey wolf optimizer. Seyedali Mirjalili, Seyed Mohammad Mirjalili, Andrew Lewis, Advances in engineering software. 201469</p>
<p>A new bio-inspired algorithm: chicken swarm optimization. Xianbing Meng, Yu Liu, Xiaozhi Gao, Hengzhen Zhang, International conference in swarm intelligence. Springer2014</p>
<p>Optimizing directional reader antennas deployment in uhf rfid localization system by using a mpcso algorithm. Weiguang Shi, Yang Guo, Shuxia Yan, Yang Yu, Peng Luo, Jianxiong Li, IEEE Sensors Journal. 18122018</p>
<p>An adaptive approach for community detection based on chicken swarm optimization algorithm. Khaled Ahmed, Ella Aboul, Ehab Hassanien, Pei-Wei Ezzat, Tsai, International Conference on Genetic and Evolutionary Computing. Springer2016</p>
<p>Dempster-shafer based probabilistic fuzzy logic system for wind speed prediction. Xian- , Bing Meng, Han-Xiong Li, 2017 International Conference on Fuzzy Theory and Its Applications (iFUZZY). IEEE2017</p>
<p>An optimizing method based on autonomous animats: fish-swarm algorithm. Xiao-Lei Li, Systems Engineering-Theory &amp; Practice. 22112002</p>
<p>Neighborhood rough set reduction with fish swarm algorithm. Yumin Chen, Zhiqiang Zeng, Junwen Lu, Soft Computing. 21232017</p>
<p>Artificial fish swarminspired whale optimization algorithm for solving multimodal benchmark functions. Imran Rahman, Junita Mohamad-Saleh, Noorazliza Sulaiman, 10th International Conference on Robotics, Vision, Signal Processing and Power Applications. Springer2019</p>
<p>Applications of nature-inspired algorithms for dimension Reduction: Enabling efficient data analytics. Farid Ghareh, Mohammadi , M Hadi Amini, Optimization, Learning and Control for Interdependent Complex Networks. Springer2019</p>            </div>
        </div>

    </div>
</body>
</html>
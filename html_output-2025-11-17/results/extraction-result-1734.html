<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1734 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1734</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1734</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-4037cedf56587287c1321d133674facaf16ab7bf</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4037cedf56587287c1321d133674facaf16ab7bf" target="_blank">Algebraic simplification of GP programs during evolution</a></p>
                <p><strong>Paper Venue:</strong> Annual Conference on Genetic and Evolutionary Computation</p>
                <p><strong>Paper TL;DR:</strong> The results suggest that the GP system employing a simplification component can achieve superior efficiency and effectiveness to the standard system on four regression and classification problems of varying difficulty.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1734.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1734.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard GP (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Genetic Programming baseline (tree-based GP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional tree-based Genetic Programming system using LISP-S style parse trees, proportional selection, subtree crossover and subtree mutation to evolve programs for regression and classification; used here as the baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Standard Genetic Programming (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Population-based evolutionary system that evolves programs represented as parse trees (LISP-S style). Initial population generated by ramped half-and-half; selection by proportional selection; genetic operators include reproduction (elitism), crossover (swapping sections of program trees), and mutation (generated by ramped half-and-half). Function set = {+, -, *, protected division, if<0}; terminal set = feature terminals and randomly generated constants. Fitness: mean squared error for regression, classification accuracy for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Described as swapping sections of programs in parse-tree representation (subtree crossover). Crossover probability = 60% (per Table 3). Mechanism: select two parents and exchange subtree(s) (paper states 'swapping of sections of programs' in the parse-tree representation).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Mutation uses the ramped half-and-half method to generate replacement subtrees for mutation (i.e., subtree replacement mutation); mutation probability = 30% (per Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>No explicit executability/validity metric reported; functionality/executability assessed indirectly via fitness measures (mean squared error for regression; classification accuracy for classification) and by program size (node count) and CPU training time.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Reported effectiveness and efficiency per task (baseline rows in Table 5). Examples: Reg1 best fitness = 0.005 ± 0.013, generations = 28.78 ± 13.43, time = 1.221 ± 0.501 s, avg program size = 37.611 ± 5.634 nodes; Reg2 best fitness = 83.774 ± 75.283, time = 5.141 ± 1.019 s, avg program size = 104.436 ± 22.171 nodes; Coins accuracy = 0.973 ± 0.025, time = 1.657 ± 0.532 s, avg size = 44.476 ± 7.302; Faces accuracy = 0.855 ± 0.117, time = 2.646 ± 0.578 s, avg size = 37.861 ± 8.755.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Not explicitly measured; paper discusses redundancy and 'building blocks' qualitatively and reports average program size as a proxy for code bloat; no genotypic/phenotypic/behavioral diversity metric defined.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>No formal novelty/executability tradeoff analysis for the baseline alone; paper qualitatively notes that redundancy in GP can provide diversity and protect building blocks from crossover, but redundancy also causes bloat and inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression (two datasets) and multi-class object classification (coin images and Yale Faces subset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>This entry is the baseline; compared against GP with online algebraic simplification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard GP serves as baseline; it achieved comparable effectiveness to simplified GP in some cases but had larger average program size and, in several experiments, longer CPU training time due to bloat. The paper reports operator rates: crossover 60%, mutation 30%, elitism 10% and max tree depths per task. No explicit measurements of novelty or executability beyond fitness, time, and program size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algebraic simplification of GP programs during evolution', 'publication_date_yy_mm': '2006-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1734.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1734.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP+Simplify</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming with Online Algebraic Simplification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-based GP system augmented with an online algebraic simplification component that applies algebraic rewrite rules and a finite-field hash-based algebraic-equivalence test to subtrees during evolution to remove redundancy and reduce program size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming with Online Algebraic Simplification</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Same GP framework as the baseline (parse-tree programs, ramped half-and-half initialisation and mutation, proportional selection, subtree crossover and reproduction). Additionally, an online simplification algorithm is applied to programs during evolution: traverse program tree bottom-up (postfix), apply a ruleset of algebraic simplification preconditions/postconditions (constant folding, neutral elements, redundant conditionals, algebraic rearrangements), and use hashing in a finite field Z_p to estimate algebraic equivalence of subtrees to enable more aggressive simplifications. Simplification parameters include hash order p (used large prime), constant precision δ, proportion of population to simplify, and simplification frequency (applied every 0,1,2,4,6 generations in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Same as baseline: subtree crossover swapping sections of parse-tree programs; crossover probability = 60% (configured in experiments). The paper notes crossover preserves building blocks early in evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Same as baseline: mutation implemented using ramped half-and-half generation of replacement subtrees for mutation events; mutation probability = 30%. The simplification is described as sometimes generating new genetic material (akin to mutation) by reorganising program structure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>No explicit executability metric (e.g., runtime correctness tests) beyond standard fitness metrics. Functionality/executability evaluated indirectly by task performance (mean squared error for regression; classification accuracy for classification), and by program size and CPU training time as proxies for efficient/executable programs.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Performance and efficiency reported in Table 5 across simplification frequencies. Examples (selected): Reg1 Every-2-gens best fitness = 0.005 ± 0.013, time = 1.109 ± 0.486 s, avg prog size = 27.232 ± 3.667 nodes (smaller than baseline 37.611 nodes). Coins Every-2-gens accuracy = 0.974 ± 0.028, time = 1.492 ± 0.407 s, avg size = 34.720 ± 4.253. Faces Every-4-gens accuracy = 0.851 ± 0.105, time = 2.251 ± 0.441 s, avg size = 28.917 ± 5.757. Reg2 showed substantial program size reduction (baseline avg size 104.436 nodes vs simplification sizes ~74–77 nodes) and lower CPU time in several frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Not explicitly measured (no quantitative genotypic/phenotypic/behavioral diversity metric). Paper qualitatively discusses redundancy as a source of diversity and that simplification can both remove redundancy and create new genetic material.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Qualitative tradeoff described: aggressive/very frequent simplification (every generation) can reduce evolutionary opportunity and slightly hurt fitness (less novelty generation), while moderate simplification frequencies (e.g., every 2 generations) often preserve or improve effectiveness and improve efficiency. Simplification can destroy building blocks early in evolution but later can generate new genetic material and reorganise programs, acting similarly to a genetic operator (creating novel program structures) while improving executability (smaller, faster programs).</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression (simple parabola and piecewise function) and multi-class object classification (coins dataset and Yale Faces subset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against the same GP implementation without simplification (standard GP baseline). Experiments vary simplification frequency (every gen, every 2, 4, 6 gens) and measure fitness, generations to termination, CPU time, and average program size.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Online algebraic simplification reduced program size (significant reduction in average node counts), often improved training efficiency (lower CPU time) and in many cases achieved comparable or superior task performance (MSE or classification accuracy) to baseline GP. Applying simplification every generation tended to slightly harm fitness and sometimes increase CPU time due to overhead; a practical recommendation is simplification every 2 generations. Authors note simplification can act like a genetic operator (it can introduce new genetic material akin to mutation) and interacts with crossover: crossover preserves building blocks early, simplification can both remove and create building blocks, so frequency tuning is important. No formal metrics for novelty/diversity/executability beyond fitness, program size and time were used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algebraic simplification of GP programs during evolution', 'publication_date_yy_mm': '2006-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic Programming: An Introduction on the Automatic Evolution of computer programs and its Applications <em>(Rating: 2)</em></li>
                <li>Genetic programming and redundancy <em>(Rating: 2)</em></li>
                <li>Code growth in genetic programming <em>(Rating: 2)</em></li>
                <li>Determining the equivalence of algebraic expressions by hash coding <em>(Rating: 2)</em></li>
                <li>Genetic programming for image analysis <em>(Rating: 1)</em></li>
                <li>Genetic programming for multiple class object detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1734",
    "paper_id": "paper-4037cedf56587287c1321d133674facaf16ab7bf",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "Standard GP (baseline)",
            "name_full": "Standard Genetic Programming baseline (tree-based GP)",
            "brief_description": "A conventional tree-based Genetic Programming system using LISP-S style parse trees, proportional selection, subtree crossover and subtree mutation to evolve programs for regression and classification; used here as the baseline comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Standard Genetic Programming (baseline)",
            "system_description": "Population-based evolutionary system that evolves programs represented as parse trees (LISP-S style). Initial population generated by ramped half-and-half; selection by proportional selection; genetic operators include reproduction (elitism), crossover (swapping sections of program trees), and mutation (generated by ramped half-and-half). Function set = {+, -, *, protected division, if&lt;0}; terminal set = feature terminals and randomly generated constants. Fitness: mean squared error for regression, classification accuracy for classification.",
            "input_type": "programs",
            "crossover_operation": "Described as swapping sections of programs in parse-tree representation (subtree crossover). Crossover probability = 60% (per Table 3). Mechanism: select two parents and exchange subtree(s) (paper states 'swapping of sections of programs' in the parse-tree representation).",
            "mutation_operation": "Mutation uses the ramped half-and-half method to generate replacement subtrees for mutation (i.e., subtree replacement mutation); mutation probability = 30% (per Table 3).",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "No explicit executability/validity metric reported; functionality/executability assessed indirectly via fitness measures (mean squared error for regression; classification accuracy for classification) and by program size (node count) and CPU training time.",
            "executability_results": "Reported effectiveness and efficiency per task (baseline rows in Table 5). Examples: Reg1 best fitness = 0.005 ± 0.013, generations = 28.78 ± 13.43, time = 1.221 ± 0.501 s, avg program size = 37.611 ± 5.634 nodes; Reg2 best fitness = 83.774 ± 75.283, time = 5.141 ± 1.019 s, avg program size = 104.436 ± 22.171 nodes; Coins accuracy = 0.973 ± 0.025, time = 1.657 ± 0.532 s, avg size = 44.476 ± 7.302; Faces accuracy = 0.855 ± 0.117, time = 2.646 ± 0.578 s, avg size = 37.861 ± 8.755.",
            "diversity_metric": "Not explicitly measured; paper discusses redundancy and 'building blocks' qualitatively and reports average program size as a proxy for code bloat; no genotypic/phenotypic/behavioral diversity metric defined.",
            "diversity_results": null,
            "novelty_executability_tradeoff": "No formal novelty/executability tradeoff analysis for the baseline alone; paper qualitatively notes that redundancy in GP can provide diversity and protect building blocks from crossover, but redundancy also causes bloat and inefficiency.",
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression (two datasets) and multi-class object classification (coin images and Yale Faces subset).",
            "comparison_baseline": "This entry is the baseline; compared against GP with online algebraic simplification.",
            "key_findings": "Standard GP serves as baseline; it achieved comparable effectiveness to simplified GP in some cases but had larger average program size and, in several experiments, longer CPU training time due to bloat. The paper reports operator rates: crossover 60%, mutation 30%, elitism 10% and max tree depths per task. No explicit measurements of novelty or executability beyond fitness, time, and program size.",
            "uuid": "e1734.0",
            "source_info": {
                "paper_title": "Algebraic simplification of GP programs during evolution",
                "publication_date_yy_mm": "2006-07"
            }
        },
        {
            "name_short": "GP+Simplify",
            "name_full": "Genetic Programming with Online Algebraic Simplification",
            "brief_description": "A tree-based GP system augmented with an online algebraic simplification component that applies algebraic rewrite rules and a finite-field hash-based algebraic-equivalence test to subtrees during evolution to remove redundancy and reduce program size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Genetic Programming with Online Algebraic Simplification",
            "system_description": "Same GP framework as the baseline (parse-tree programs, ramped half-and-half initialisation and mutation, proportional selection, subtree crossover and reproduction). Additionally, an online simplification algorithm is applied to programs during evolution: traverse program tree bottom-up (postfix), apply a ruleset of algebraic simplification preconditions/postconditions (constant folding, neutral elements, redundant conditionals, algebraic rearrangements), and use hashing in a finite field Z_p to estimate algebraic equivalence of subtrees to enable more aggressive simplifications. Simplification parameters include hash order p (used large prime), constant precision δ, proportion of population to simplify, and simplification frequency (applied every 0,1,2,4,6 generations in experiments).",
            "input_type": "programs",
            "crossover_operation": "Same as baseline: subtree crossover swapping sections of parse-tree programs; crossover probability = 60% (configured in experiments). The paper notes crossover preserves building blocks early in evolution.",
            "mutation_operation": "Same as baseline: mutation implemented using ramped half-and-half generation of replacement subtrees for mutation events; mutation probability = 30%. The simplification is described as sometimes generating new genetic material (akin to mutation) by reorganising program structure.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "No explicit executability metric (e.g., runtime correctness tests) beyond standard fitness metrics. Functionality/executability evaluated indirectly by task performance (mean squared error for regression; classification accuracy for classification), and by program size and CPU training time as proxies for efficient/executable programs.",
            "executability_results": "Performance and efficiency reported in Table 5 across simplification frequencies. Examples (selected): Reg1 Every-2-gens best fitness = 0.005 ± 0.013, time = 1.109 ± 0.486 s, avg prog size = 27.232 ± 3.667 nodes (smaller than baseline 37.611 nodes). Coins Every-2-gens accuracy = 0.974 ± 0.028, time = 1.492 ± 0.407 s, avg size = 34.720 ± 4.253. Faces Every-4-gens accuracy = 0.851 ± 0.105, time = 2.251 ± 0.441 s, avg size = 28.917 ± 5.757. Reg2 showed substantial program size reduction (baseline avg size 104.436 nodes vs simplification sizes ~74–77 nodes) and lower CPU time in several frequencies.",
            "diversity_metric": "Not explicitly measured (no quantitative genotypic/phenotypic/behavioral diversity metric). Paper qualitatively discusses redundancy as a source of diversity and that simplification can both remove redundancy and create new genetic material.",
            "diversity_results": null,
            "novelty_executability_tradeoff": "Qualitative tradeoff described: aggressive/very frequent simplification (every generation) can reduce evolutionary opportunity and slightly hurt fitness (less novelty generation), while moderate simplification frequencies (e.g., every 2 generations) often preserve or improve effectiveness and improve efficiency. Simplification can destroy building blocks early in evolution but later can generate new genetic material and reorganise programs, acting similarly to a genetic operator (creating novel program structures) while improving executability (smaller, faster programs).",
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression (simple parabola and piecewise function) and multi-class object classification (coins dataset and Yale Faces subset).",
            "comparison_baseline": "Compared against the same GP implementation without simplification (standard GP baseline). Experiments vary simplification frequency (every gen, every 2, 4, 6 gens) and measure fitness, generations to termination, CPU time, and average program size.",
            "key_findings": "Online algebraic simplification reduced program size (significant reduction in average node counts), often improved training efficiency (lower CPU time) and in many cases achieved comparable or superior task performance (MSE or classification accuracy) to baseline GP. Applying simplification every generation tended to slightly harm fitness and sometimes increase CPU time due to overhead; a practical recommendation is simplification every 2 generations. Authors note simplification can act like a genetic operator (it can introduce new genetic material akin to mutation) and interacts with crossover: crossover preserves building blocks early, simplification can both remove and create building blocks, so frequency tuning is important. No formal metrics for novelty/diversity/executability beyond fitness, program size and time were used.",
            "uuid": "e1734.1",
            "source_info": {
                "paper_title": "Algebraic simplification of GP programs during evolution",
                "publication_date_yy_mm": "2006-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic Programming: An Introduction on the Automatic Evolution of computer programs and its Applications",
            "rating": 2
        },
        {
            "paper_title": "Genetic programming and redundancy",
            "rating": 2
        },
        {
            "paper_title": "Code growth in genetic programming",
            "rating": 2
        },
        {
            "paper_title": "Determining the equivalence of algebraic expressions by hash coding",
            "rating": 2
        },
        {
            "paper_title": "Genetic programming for image analysis",
            "rating": 1
        },
        {
            "paper_title": "Genetic programming for multiple class object detection",
            "rating": 1
        }
    ],
    "cost": 0.01044025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Algebraic Simplification of GP Programs During Evolution</h1>
<p>Phillip Wong ${ }^{1}$<br>${ }^{1}$ School of Maths, Stats and Computer Science<br>Victoria University of Wellington, P. O. Box 600, Wellington, New Zealand<br>phillip@mcs.vuw.ac.nz</p>
<h2>Mengjie Zhang ${ }^{1,2}$<br>${ }^{2}$ Artificial Intelligence Research Centre<br>M\&amp;E College, Agricultural University of Hebei<br>Baoding, China, 071001<br>mengjie@mcs.vuw.ac.nz</h2>
<h2>ABSTRACT</h2>
<p>Program bloat is a fundamental problem in the field of Genetic Programming (GP). Exponential growth of redundant and functionally useless sections of programs can quickly overcome a GP system, exhausting system resources and causing premature termination of the system before an acceptable solution can be found. Simplification is an attempt to remove such redundancies from programs. This paper looks at the effects of applying an algebraic simplification algorithm to programs during the GP evolution. The GP system with the simplification is examined and compared to a standard GP system on four regression and classification problems of varying difficulty. The results suggest that the GP system employing a simplification component can achieve superior efficiency and effectiveness to the standard system on these problems.</p>
<h2>Categories and Subject Descriptors</h2>
<p>I. 2 [Artificial Intelligence]: Miscellaneous</p>
<h2>General Terms</h2>
<p>Algorithms, Performance</p>
<h2>Keywords</h2>
<p>Genetic Programming, Algebraic Simplification, Program Simplification, Code Bloating, Online Simplification</p>
<h2>1. INTRODUCTION</h2>
<p>Genetic programming (GP) [8] is a method of automatically generating programs for solving specific tasks. Firstly, an initial group of randomly generated genetic programs, normally represented as parse trees such as LISP-S trees, is created. The process of selection based on fitness is carried out to provide a basis for the next program generation. Fitness is determined by running the programs and evaluating them on a set of criteria called fitness function. The genetic</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>operators of crossover for swapping of sections of programs, mutation for random alterations to a program, and reproduction for retaining the best programs are applied to the selected programs to create a new population of programs. The process of creating new generations is repeated until certain termination criterion is met. The "best" program in the last generation is usually used as the resulting system solution. GP can be seen as a genetic beam search through the space of possible solutions to the task.</p>
<p>GP is an emerging field in evolutionary computing and machine learning and has already been applied to many tasks, including image analysis [13], object detection [16], regression problems [8] and even control programs for walking robots [3]. GP has been very successful in solving or performing these tasks and "now routinely delivers high-return human-competitive machine intelligence" [9].</p>
<p>However, one of the current and fundamental problems in GP is that the process of genetic programming will inevitably introduce some redundancy into the evolved programs $[8,14,2]$. This redundancy is regarded as a fundamental problem of GP as it slows down the search process by consuming large amounts of memory and causes exploration of large unnecessary parts of the search space. The search process continues to slow as the programs become larger until the programs become too large for the system's memory to hold, halting the system before a "good" solution can be found. Redundancy can also result in an unnecessarily complex program, which is inefficient in its execution and difficult to interpret and comprehend.</p>
<p>On the other hand, this redundancy may aid the effectiveness of the evolutionary process by providing a more diverse selection of program fragments for the process to use, and protecting useful "building blocks" within programs from the destructive nature of the crossover operator [2].</p>
<p>Simplification is a process applied in order to reduce the complexity of an expression, as well as eliminate any superfluous details. Simplification can be implemented in various ways, including using simple algebraic techniques, translation into canonical forms or numeric hashing techniques. Typically simplification is applied at the end of the evolutionary process to remove some of the complexity of the program, reducing the resource usage and improving comprehensibility, enabling it to run faster and to be easier to interpret. The editing operation proposed in [8] is an example of this kind. But as program redundancy is a problem which also occurs during the evolutionary process, online simplification during evolution to improve performance in the whole system needs to be investigated.</p>
<h3>1.1 Goals</h3>
<p>The goal of this paper is to invent a method in GP that does program simplification during the evolutionary process. We will investigate the effect of performing online simplification of the programs during the evolutionary process, to discover whether the reduction in complexity outweighs the possible benefits of redundancy. This approach will be examined and compared with the standard GP without simplification on four regression and classification problems of increasing difficulty. Specifically, we are interested in:</p>
<ul>
<li>how the online simplification algorithm can be constructed by combining some algebraic simplification rules and hashing techniques;</li>
<li>whether the simplification improves the system efficiency of the evolutionary process; and</li>
<li>whether this approach deteriorates the classification performance compared with the standard GP method without simplification.</li>
</ul>
<h3>1.2 Structure</h3>
<p>The rest of the paper is organised as follows. Section 2 describes the simplification algorithm developed in this paper. Section 3 presents the four data sets and experiment configurations. Section 4 describes the results with discussions. Section 5 concludes and gives future work directions.</p>
<h2>2. THE PROGRAM SIMPLIFICATION APPROACH</h2>
<p>In the standard GP system, the programs are represented as a LISP-S (or similar language) expression, which is stored in a tree representation [8]. The ramped half-and-half method was used for generating programs in the initial population and for the mutation operator [1]. The proportional selection mechanism and the reproduction, crossover and mutation operators [8] were used in the learning and evolutionary process. The function set consists of the commonly used four arithmetic operators $(+,-, \times, \div)$ and an if (conditional) operator. The terminal set consists of a number of feature/variable terminals from the task and several constant terminals. Based on this setting, a genetic program looks like an algebraic expression.</p>
<p>The new approach introduced in this paper uses the same setting as the standard GP approach mentioned above. The major difference between them is that the new approach has an online program simplification algorithm to be applied to the genetic programs during the evolutionary process.</p>
<p>The task of the simplification method is to obtain a smaller program, by removing the redundancy of a program, that yields the same output as the original program. In this approach, we use the idea in the algebraic expression simplification to construct simplification rules, apply these rules using a postfix search to the genetic programs, and use hashing to estimate the algebraic equivalence to simplify the genetic programs during evolution.</p>
<p>In the rest of the section, we describe the simplification rules and the simplification process, then give an example to show how a program can be simplified, and finally summarise the simplification algorithm.</p>
<h3>2.1 The Simplification Rules</h3>
<p>As in algebraic expression simplification, we use multiple rules to simplify a given genetic program. A specific rule might only be suitable for removing/reducing a particular part of the genetic program.</p>
<p>Similarly to STRIPS operators [5], we use two parts, a precondition and a postcondition to represent the simplification rules. The precondition represents the state of the surrounding nodes in the program tree that must be present in order to be able to apply the simplification rule, and the postcondition represents the additions and deletions made to the program tree to obtain the simplified form.</p>
<p>These rules form the ruleset of the program simplification algorithm, which covers major sources of redundancy in the evolved genetic programs. For example,</p>
<ul>
<li>an arithmetic operator with only constant children (e.g. $(+32)=5)$</li>
<li>subtraction or division of self (e.g. $(-\mathrm{f} 0 \mathrm{f} 0)=0$ )</li>
<li>redundant conditionals, where the outcome is always the same (e.g. if $&lt;0(2$ f0 f1) = f1)</li>
</ul>
<p>A selection of the simplification rules used in this approach are presented in table 1. In this table, constants are represented by lower-case characters (e.g. $a, b, x, j$ ), and variables are represented by upper-case characters (e.g A, B, X, J).</p>
<p>Table 1: The simplification rules used.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Precondition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Effective Result</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">if $&lt;0(a, B, C)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">B if a $&lt;0$, else C</td>
</tr>
<tr>
<td style="text-align: center;">if $&lt;0(A, B, B)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">B</td>
</tr>
<tr>
<td style="text-align: center;">$a+b$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c, c=a+b$</td>
</tr>
<tr>
<td style="text-align: center;">$a-b$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c, c=a-b$</td>
</tr>
<tr>
<td style="text-align: center;">$a \times b$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c, c=a \times b$</td>
</tr>
<tr>
<td style="text-align: center;">$a \div b$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c, c=a \div b$</td>
</tr>
<tr>
<td style="text-align: center;">$a+(b+C)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c+C, c=a+b$</td>
</tr>
<tr>
<td style="text-align: center;">$a+(b+C)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c-C, c=a-b$</td>
</tr>
<tr>
<td style="text-align: center;">$a-(b-C)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c+C, c=a-b$</td>
</tr>
<tr>
<td style="text-align: center;">$a \times(b \times C)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c \times C, c=a \times b$</td>
</tr>
<tr>
<td style="text-align: center;">$a \times(b \div C)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c \div C, c=a \times b$</td>
</tr>
<tr>
<td style="text-align: center;">$a \div(b \div C)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$c \times C, c=a \div b$</td>
</tr>
<tr>
<td style="text-align: center;">$a+(B+c)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$b+B, b=a+c$</td>
</tr>
<tr>
<td style="text-align: center;">$a+(B-c)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$b+B, b=a-c$</td>
</tr>
<tr>
<td style="text-align: center;">$a-(B+c)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$b-B, b=a-c$</td>
</tr>
<tr>
<td style="text-align: center;">$a-(B-c)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$b-B, b=a+c$</td>
</tr>
<tr>
<td style="text-align: center;">$a \times(B \times c)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$b \times B, b=a \times c$</td>
</tr>
<tr>
<td style="text-align: center;">$a \times(B \div c)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$b \times B, b=a \div c$</td>
</tr>
<tr>
<td style="text-align: center;">$a \div(B \div c)$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$b \div B, b=a \times c$</td>
</tr>
<tr>
<td style="text-align: center;">$A \div 1$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;">$A \div A$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">$0 \div A$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$0 \times A=A \times 0$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$A \times 1=1 \times A$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;">$A+0=0+A$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;">$A-0$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;">$A-A$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$A \times \frac{1}{10}=\frac{1}{10} \times A$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">$\frac{3}{10}$</td>
</tr>
<tr>
<td style="text-align: center;">$A \times \frac{2}{10}=\frac{2}{10} \times A$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">B</td>
</tr>
</tbody>
</table>
<h3>2.2 The Simplification Process</h3>
<p>To apply the ruleset to a genetic program for simplification, we used a kind of "greedy" engine, which is a recursive algorithm. It recursively travels through the program tree in a bottom-up fashion by the postfix order traversal mode. For each node it processes, the algorithm checks the precondition for each simplification rule in the ruleset. If a rule matches, it is applied to the partial tree associated with the node to make simplification. If none of the rules can be applied at a node, the algorithm moves to the next node (either neighbouring node or parent node).</p>
<p>In this way, the algorithm guarantees that each node in the program tree is only visited once. However, as all simplification rules only look at a static and limited area, arbitrary depth levels of simplification (simplification of terms which are not neighbouring but far away) are not supported in this algorithm.</p>
<h3>2.3 Estimating Algebraic Equivalence</h3>
<p>Another important aspect of a simplification system is determining when $\mathbf{X}$ is equal to Y , providing a mechanism for evaluating rules in the ruleset. This is fairly trivial when comparing single nodes, as one needs simply to check whether the nodes are identical. However, checking whether multi-node subexpressions or subtrees are equal is more difficult.</p>
<p>Our goal is to allow for not only noticeably similar expressions (e.g. $(x+y+z)$ and $(z+z+y)$ ) to be identified as equivalent, but also seemingly dissimilar expressions, for example, $(/(+(-(<em> \mathrm{w} x)(</em> \mathrm{xy}))(<em>(-\mathrm{w}$ $\mathrm{y}) \mathrm{y}))(-(</em> \mathrm{x} \mathrm{x})(* \mathrm{y} \mathrm{y}))$ and $(/(-w y)(-\mathrm{x} y))$ as well.</p>
<p>In this approach, we use hashing to address the algebraic equivalence of two subtrees/subprograms. The hashing function is used to extend the algebraic system mentioned earlier to be capable of simplifying more expressions. In the 1970s and 1980s, [12] and [7] describe methods for achieving algebraic equivalence using hashing methods for algebraic expressions. In this approach, we use a variant of those methods to cope with all common terminals and functions in the evolved genetic programs.</p>
<p>Note that using the hashing technique to determine algebraic equivalence adds to the risk of two non-equivalent subexpressions being determined as equal and one or both being discarded (depending on the simplification rule). By using a very large number of distinct hash values, the number of collisions can be kept minimal, and probabilistically minute. In this work, $p$ is used to denote the hashing order for the hash function (i.e. the total number of possible hash values). It is important that the collection of hash values qualify as a finite field ([10]) and so $p$ should be a prime number. Note that any finite field with $p$ elements is isomorphic to $\mathbb{Z}_{p}[10]$ (integers from 0 to $p$ ).</p>
<p>In the rest of this subsection, we describe how to estimate the algebraic equivalence for feature terminals, constant terminals, the four arithmetic operators and the conditional operator.</p>
<h3>2.3.1 Feature Terminals</h3>
<p>In a GP system, a feature terminal represent inputs from the task environment, such as an image feature in object classification or a simple variable in symbolic regression. The important attribute of these terminals is that a fea-
ture terminal always keeps the same value for a particular fitness case for all genetic programs during the evolutionary process. Accordingly, in this approach, the feature terminals are assigned random hash values at the beginning of the GP system run and remain unchanged for the entire duration of the evolutionary process. Specifically, we use:</p>
<p>$$
\operatorname{Hash}\left(\text { Feature }<em p="p">{n}\right)=\text { a random value in } \mathbb{Z}</em>
$$</p>
<h3>2.3.2 Constant Terminals</h3>
<p>In a GP system, constants can be any numeric type: integers, rationals, floating point, etc. Therefore, the hash function needs to be designed to handle all these types, of which the most difficult is floating point. [12] does not describe a solution to this in his paper. In this approach, we handle this by approximating the floating point with a rational number, thus converting it to a simple division of two integers.</p>
<p>Calculating accurate and irreducible rationals can be very time consuming, so a quick approximation is used. The numerator is formed by multiplying the floating point by a predefined precision constant $(\delta)$ and truncating the leftover fractional part. Using the same precision constant as a denominator, a rational representation can be very quickly found.</p>
<p>$$
\operatorname{Hash}(c)=\frac{c \times \delta}{\delta} \bmod p=(c \times \delta) \times \frac{1}{\delta} \bmod p
$$</p>
<p>This approach of course, requires modular division which one may not be familiar with. Now, the division of two numbers $\frac{x}{a}$ is equivalent to the multiplication of the first number with the multiplicative inverse of the second number $x \times \frac{1}{a}$. So to perform division, one needs only to calculate the multiplicative inverse of $y$ and multiply by $x$.</p>
<p>The key point here is to find the integer equivalence of the inverse of $\delta \bmod p$. In this approach, this is done using the Extended Euclidean Algorithm [15, 4]. For any two integers $a$ and $b$, there exists two integers $q, r$ such that $a=b \cdot q+r$. Commonly, $q$ is called the quotient and $r$ the remainder. Starting from step 0 , the algorithm additionally calculates an auxiliary number $x_{i}$ at each division step, where $x_{0}=0, x_{1}=1$ and for the other steps $x_{i}=\left(x_{i-2}-x_{i-1} \cdot q_{i-2}\right) \bmod p$. At step $i$, the devision is performed in the format of $a_{i}=q_{i} \cdot b_{i}+r_{i}$, where $a_{i}=$ $b_{i-1}, b_{i}=r_{i-1}$ and $a_{0}=a, b_{0}=b$ at step 0 . The resulting $x$ will be the equivalence of the inverse of $a \bmod b$.</p>
<p>As an example, assuming that $\delta=10$, the constant to be hashed $c=0.6$ and the hash order $p=17$, then we have</p>
<p>$$
\operatorname{Hash}(0.6)=\frac{0.6 \times 10}{10} \bmod 17=6 \times \frac{1}{10} \bmod 17
$$</p>
<p>Now we use the extended Euclidean algorithm to find the integer equivalence of $\frac{1}{10}$ (the inverse of 10 in $\mathbb{Z}<em 0="0">{17}$ ). Here, $a</em>$, the value of $x$ at that step.}=p=17, b_{0}=\delta=10$. Each step of the algorithm is shown below to calculate $x_{i</p>
<p>Step 0: $17=1(10)+7 \quad x_{0}=0$
Step 1: $10=1(7)+3 \quad x_{1}=1$
Step 2: $7=2(3)+1 \quad x_{2}=\left(x_{0}-x_{1} \cdot q_{0}\right) \bmod p$
$=(0-1 \cdot 1) \bmod 17=16$
Step 3: $3=3(1)+0 \quad x_{3}=\left(x_{1}-x_{2} \cdot q_{1}\right) \bmod p$
$=(1-16 \cdot 1) \bmod 17=2$
Step 4:</p>
<p>$$
\begin{aligned}
&amp; x_{4}=\left(x_{2}-x_{3} \cdot q_{2}\right) \bmod p \
&amp; =(16-2 \cdot 2) \bmod 17=12
\end{aligned}
$$</p>
<p>The last value for $x$ is 12 , meaning that the integer equivalence of the inverse of $10\left(\frac{1}{10}\right.$ in $\mathbb{Z}_{17}$ ) is 12 . A quick check shows that $12 \times 10 \bmod 17=120 \bmod 17=1$, so this is indeed correct. Substituting 12 in for $\frac{1}{10}$ in equation 3, we have</p>
<p>$$
6 \times \frac{1}{10} \bmod 17=6 \times 12 \bmod 17=72 \bmod 17=4
$$</p>
<p>So the constant 0.6 hashes to the value 4 in this example.</p>
<h3>2.3.3 The Arithmetic Operators</h3>
<p>Because the hashing method takes place in a finite field, all of the standard arithmetic methods are easily handled using modulo arithmetic. Hashing of these operators is equivalent to evaluating them within the field:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Hash}(A+B)=(A+B) \bmod p \
&amp; \operatorname{Hash}(A-B)=(A-B) \bmod p \
&amp; \operatorname{Hash}(A \times B)=(A \times B) \bmod p \
&amp; \operatorname{Hash}(A \div B)=(A \div B) \bmod p
\end{aligned}
$$</p>
<p>where the division hashing follows the rule of the extended Euclidean algorithm discussed above.</p>
<h3>2.3.4 The if&lt;0 operator</h3>
<p>The if&lt;0 conditional operator is a more difficult case, as it is not an arithmetic function and so cannot simply be converted to a modulo arithmetic equivalent. Additionally, it consists of three parameters (instead of the usual two): a condition, a true branch and a false branch. All the three parameters must be considered when hashing this operator as well as the order in which they appear. The following approach was formulated to handle this operator:</p>
<p>$$
\operatorname{Hash}(\text { if }&lt;0(A, B, C))=\left(\frac{A}{B}+C\right) \bmod p
$$</p>
<p>which uses division and addition to take into account the position of the three parameters.</p>
<h3>2.3.5 Operator Closure</h3>
<p>All of the functions supported are closed, meaning that for any of the functions $\diamond \in{+,-, \times, \div$, if $&lt;0}$, $(\operatorname{Hash}(A) \diamond$ $\operatorname{Hash}(B)) \bmod p=\operatorname{Hash}(A \diamond B)$ in $\mathbb{Z}_{p}$. More specifically:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Hash}(A+B)=(\operatorname{Hash}(A)+\operatorname{Hash}(B)) \bmod p \
&amp; \operatorname{Hash}(A-B)=(\operatorname{Hash}(A)-\operatorname{Hash}(B)) \bmod p \
&amp; \operatorname{Hash}(A \times B)=(\operatorname{Hash}(A) \times \operatorname{Hash}(B)) \bmod p \
&amp; \operatorname{Hash}(A \div B)=(\operatorname{Hash}(A) \div \operatorname{Hash}(B)) \bmod p \
&amp; \operatorname{Hash}(\text { if }&lt;0(A \diamond B, C, D))=\left(\frac{\operatorname{Hash}(A \diamond B)}{\operatorname{Hash}(C)}+\operatorname{Hash}(D)\right) \bmod p
\end{aligned}
$$</p>
<p>This means that by storing already calculated hash values within the tree node structure, one does not need to recalculate the hash values of subtrees each time a tree is to be hashed, as hash values of subtrees can be combined to give correct hash values of the whole tree.</p>
<h3>2.4 An Example</h3>
<p>Now, we use an example to show the simplification process for a given genetic program. The example genetic program $(-(--0.2-0.5)$ (if&lt;0 (\% (+ f0 f1) (+ f1 f0)) 0.8 (- f0 f0))) can be represented in the tree shown in figure 1.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The original program tree.
Assume that the hashing order is 17, f0 and f1 are "randomly" assigned the values 3 and 5 respectively. Also, for presentation convenience, Table 2 reiterates the rules in the ruleset that are specifically used in this example.</p>
<p>Table 2: Simplification rules used in this example.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Precondition</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Effective Result</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$(1)(-\mathrm{a} b)$</td>
<td style="text-align: left;">$\rightarrow$</td>
<td style="text-align: left;">$\mathrm{c}, \mathrm{c}=a-b$</td>
</tr>
<tr>
<td style="text-align: left;">$(2)(\% A A)$</td>
<td style="text-align: left;">$\rightarrow$</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">$(3)(-\mathrm{A} A)$</td>
<td style="text-align: left;">$\rightarrow$</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">$(4)$ (if $&lt;0$ a $B$ C)</td>
<td style="text-align: left;">$\rightarrow$</td>
<td style="text-align: left;">C (if $a \geq 0$ )</td>
</tr>
</tbody>
</table>
<p>The algorithm traverses the program tree in a "bottomup" fashion using a post-fix traversal. This means that the algorithm processes the program nodes in the order depicted in figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Bottom-up traversal order (shown by integer values).</p>
<p>The first node inspected by the algorithm is " -0.2 ", followed by " -0.5 ". As no simplification rule exists in the ruleset that governs single nodes, these nodes (and indeed the entire bottom layer of nodes) are left unchanged. Next, the algorithm moves to the parent node of " -0.2 " and " -0.5 ", which is "-". The subtree formed by this node and its children ( - -0.2 -0.5) matches the precondition for rule (1) ( - a b). The system applies this rule, replacing the subtree with the rule's effective result: " 0.3 ".</p>
<p>Now, the subtrees (+ f0 f1) and (+ f1 f0) do not match the preconditions for any of the rules, so are left unchanged. Note however, that they both have the same algebraic equivalence hash value (shown in figure 3). Therefore, when</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Hashing of two subtrees with same value (shown by integer values).
node 10 ("\%") is inspected, the subtree (\% (+ f0 f1) (+ f1 f0)) does indeed match the precondition for rule (2) (\% A A). The entire subtree is replaced using the rule to a single node 1. Similarly, the subtree ( f0 f0) matches rule (3) (- A A) and is replaced by the single node 0 when the algorithm processes " - ".</p>
<p>Figure 4 shows the tree after processing nodes 1 through 14 .
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The program after partial processing.
At this stage, the program is already reduced to 6 nodes in size, and there are still two nodes left to be processed. Inspecting the if $&lt;0$ node, the algorithm matches it with rule (4) (if&lt;0 c A B), as the first parameter of the if $&lt;0$ operator is a constant. In this case, the constant is 1 , which will obviously never be less than 0 . The system then, following the rule, replaces this subtree with its third parameter, which is 0 .</p>
<p>Lastly the root node is processed, which again matches rule (1) (- a b). Applying it yields the final result, a single numerical constant node " 0.3 " (figure 5).</p>
<h2>2</h2>
<p>Figure 5: The final program, a single node.</p>
<h3>2.5 Summary of the Simplification Algorithm</h3>
<p>The simplification algorithm simplifies a given program in the following way:</p>
<ul>
<li>Traverse the program tree in a bottom-up fashion using a postfix mode.</li>
<li>For the terminal nodes, calculate the equivalence hash values.</li>
<li>For each non-terminal node</li>
<li>Calculate the equivalence hash value of the node, directly using the hash values of its child nodes.</li>
<li>Iterate through the set of rules. If the node (and its surrounding nodes) match the precondition of a rule, apply that rule to simplify the sub-tree associated with the node.</li>
<li>Once all nodes have been processed, output the final, simplified program tree.</li>
</ul>
<p>This algorithm is invoked on a number of programs in the GP system, replacing the original programs with their simplified counterparts.</p>
<h2>3. EXPERIMENTATION SETUP</h2>
<h3>3.1 Datasets</h3>
<p>Four datasets, two for symbolic regression tasks and two for multi-class object classification tasks, were used to examine the simplification method. The two symbolic regression tasks represent different "difficulties". The first consists of 200 data points conforming to a simple parabolic curve. The second is a much more complicated piecewise function. We also used 200 data points in $[-10,10]$ as the fitness cases in this task. An example data set for each task is shown in figure 6 .
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Plots of the two regression task "ideal" solutions.</p>
<p>The first classification task uses a coin dataset. The dataset consists of 480 70x70 pixel image cutouts of New Zealand 5 and 10 cent coins with different sides up and different orientations. These make up four distinct classes: 5 cent heads, 5 cent tails, 10 cent heads, 10 cent tails.</p>
<p>The second classification task uses a subset of the Yale Database B Face Dataset [6]. It consists of face images of 5 subjects taken from a single position under 65 different lighting conditions. This creates a set of 325 instances. Example data sets for the two classification tasks are shown in figure 7 .
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Object classification data sets. (a) Coins; (b) Faces.</p>
<h3>3.2 Terminal and Function Sets</h3>
<p>The terminal set consists of several randomly generated constant terminals as well as a number of feature terminals. The constant terminals are simply $n$ floating point numbers generated in the range of $[-1,1]$ using a uniform distribution random number generator $\left(r_{0}, r_{1}, \ldots, r_{n}\right)$. The number of feature terminals $(m)$ are task dependent. For the two</p>
<p>symbolic regressions tasks, the feature terminal corresponds to the single independent variable. In the coins classification task, there are 8 feature terminals representing the extracted pixel statistic features. In the face dataset, we use 18 feature terminals representing the extracted pixel statistic features from the various facial regions.</p>
<p>$$
\text { Terminal } \operatorname{Set}=\left{r_{0}, r_{1}, \ldots, r_{n}, f_{0}, \ldots, f_{m}\right}
$$</p>
<p>The function set used for these tasks consists of the four basic arithmetic operators, as well as a conditional if $&lt;0$ operator. The division used is the commonly used "protected division" where a divide by zero results in zero, removing the undefined case. The conditional if $&lt;0$ operator takes three parameters, a condition which will be evaluated, a true branch if the condition evaluates to $&lt;$ zero and a false branch if the condition evaluates to $\geq$ zero.</p>
<p>$$
\text { Function } \operatorname{Set}={+,-, *, \%, \text { if }&lt;0}
$$</p>
<h3>3.3 Fitness Function</h3>
<p>For each symbolic regression task, the fitness of a program is governed by the mean squared error of the desired output and the actual output of the program on all patterns of each data set.</p>
<p>For the classification tasks, the fitness of a program is governed by the accuracy of classification, that is, the number of instances correctly classified by the program as a percentage of the total number of instances in the training set. In this approach, we used the static-boundary method (first described in [16]) to translate the single output of a genetic program with a training pattern to a set of class labels. While other methods exist $[17,11]$, determining the best class translation rule is beyond the scope of this paper.</p>
<h3>3.4 Experiment Configuration</h3>
<p>Table 3 shows the common parameter values used in the standard GP system and the new GP system with simplification.</p>
<p>Table 3: Genetic programming system parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Gens</th>
<th style="text-align: center;">Pop.Size</th>
<th style="text-align: center;">Mut.</th>
<th style="text-align: center;">Elit.</th>
<th style="text-align: center;">Cross.</th>
<th style="text-align: center;">Max.Dep.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reg1</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Reg2</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Coins</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Face</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>In the new simplified GP, we also used additional "simplification parameters". As mentioned earlier, hash order refers to the number of distinct hash values that programs can be hashed to, and constant precision is the number of decimal places that are kept when hashing a floating point number. Proportion is the percentage of programs in a population to be applied to simplification. Measured in number of generations, frequency refers to how often the simplification process is applied. The parameter values used in the experiments are shown in table 4. Frequency at every 0 generation means that the standard GP without simplification is applied.</p>
<p>For the coin classification problem, we equally split the data sets into a training set, a validation set and a test set. For the face data set, due to a relatively small number of examples, we used a 10 -fold cross validation technique.</p>
<p>Both GP systems run 50 generations for all the four data sets unless it found a solution, in which case the evolution was terminated early. For the coin classification problem,</p>
<p>Table 4: Algebraic simplification: simplification parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Hash order $(p)$</td>
<td style="text-align: center;">1000077157</td>
</tr>
<tr>
<td style="text-align: center;">Constant precision $(\delta)$</td>
<td style="text-align: center;">1000000</td>
</tr>
<tr>
<td style="text-align: center;">Proportion</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Frequency</td>
<td style="text-align: center;">Every $0,1,2,4$, or 6 gens.</td>
</tr>
</tbody>
</table>
<p>the evolution was also terminated when the accuracy on the validation set started falling down.</p>
<p>All single experiments were repeated 50 runs and the means and standard deviations of the results are presented in the next section.</p>
<h2>4. RESULTS AND DISCUSSION</h2>
<p>Table 5 shows results of the two GP approaches on the four data sets in terms of the effectiveness (best fitness mean squared error for regression and classification accuracy for classification), training efficiency (number of generations and training time), and average size of all the programs in the systems in number of nodes.</p>
<h3>4.1 Effectiveness</h3>
<p>As can be seen from table 5, the GP approach with the proposed simplification at different frequencies almost always achieved comparable or even superior fitness, either mean square error or accuracy, on these data sets than the basic GP approach without simplification.</p>
<p>We hypothesised that the simplification process during evolution might destroy the existing good building blocks of the genetic programs, which might result in worse classification performance. However, these results are clearly different from the original hypothesis. After checking the evolutionary process, we identify the following reasons. At the beginning of evolution, although the simplification algorithm might destroy some potentially good building blocks, this effect was very much offset by the powerful crossover operator, which can preserve good even form larger building blocks. At the later stage, when the programs are getting larger and the GP evolution is difficult to make further improvement since the crossover operator starts to destroy good existing building blocks, the simplification algorithm actually generates new genetic materials which might contain new good building blocks by reorganising the entire genetic programs. This makes it possible to consider the simplification as a new genetic operator in the future.</p>
<p>We also observe that the GP approach with simplification in every generation achieved worse performance than every two to six generations in most cases. Although this simplification could introduce new genetic materials like mutation, the programs will not have sufficient chances for evolution if we apply the simplification too often. Applying simplification less frequently will give the GP system more chances to perform evolution than that in every generation.</p>
<h3>4.2 Efficiency</h3>
<p>According to Table 5, while the numbers of generations used for the evolutionary training process for different GP systems were fairly similar, the actual training CPU times are quite different. As expected, the GP approach with the simplification almost always improved the training efficiency and in some cases very much so. This is mainly because</p>
<p>Table 5: Results for each of the four tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Frequency</th>
<th style="text-align: center;">Best Fitness</th>
<th style="text-align: center;">Generations</th>
<th style="text-align: center;">Time(s)</th>
<th style="text-align: center;">Avg. Prog Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\operatorname{Reg}_{1}$</td>
<td style="text-align: center;">without</td>
<td style="text-align: center;">$0.005 \pm 0.013$</td>
<td style="text-align: center;">$28.781 \pm 13.427$</td>
<td style="text-align: center;">$1.221 \pm 0.501$</td>
<td style="text-align: center;">$37.611 \pm 5.634$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 1</td>
<td style="text-align: center;">$0.011 \pm 0.042$</td>
<td style="text-align: center;">$32.438 \pm 13.119$</td>
<td style="text-align: center;">$1.232 \pm 0.464$</td>
<td style="text-align: center;">$25.606 \pm 2.937$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 2</td>
<td style="text-align: center;">$0.005 \pm 0.013$</td>
<td style="text-align: center;">$31.562 \pm 14.291$</td>
<td style="text-align: center;">$1.109 \pm 0.486$</td>
<td style="text-align: center;">$27.232 \pm 3.667$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 4</td>
<td style="text-align: center;">$0.027 \pm 0.119$</td>
<td style="text-align: center;">$31.094 \pm 13.359$</td>
<td style="text-align: center;">$1.071 \pm 0.494$</td>
<td style="text-align: center;">$28.412 \pm 5.074$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 6</td>
<td style="text-align: center;">$0.003 \pm 0.009$</td>
<td style="text-align: center;">$31.688 \pm 12.228$</td>
<td style="text-align: center;">$1.070 \pm 0.359$</td>
<td style="text-align: center;">$29.200 \pm 4.581$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{Reg}_{2}$</td>
<td style="text-align: center;">Without</td>
<td style="text-align: center;">$83.774 \pm 75.283$</td>
<td style="text-align: center;">$44.875 \pm 4.756$</td>
<td style="text-align: center;">$5.141 \pm 1.019$</td>
<td style="text-align: center;">$104.436 \pm 22.171$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 1</td>
<td style="text-align: center;">$92.884 \pm 80.624$</td>
<td style="text-align: center;">$44.875 \pm 4.756$</td>
<td style="text-align: center;">$5.206 \pm 0.861$</td>
<td style="text-align: center;">$74.362 \pm 13.642$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 2</td>
<td style="text-align: center;">$67.346 \pm 59.315$</td>
<td style="text-align: center;">$44.875 \pm 4.756$</td>
<td style="text-align: center;">$4.270 \pm 0.759$</td>
<td style="text-align: center;">$74.841 \pm 13.886$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 4</td>
<td style="text-align: center;">$82.471 \pm 85.606$</td>
<td style="text-align: center;">$44.875 \pm 4.756$</td>
<td style="text-align: center;">$4.152 \pm 1.069$</td>
<td style="text-align: center;">$77.337 \pm 21.487$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 6</td>
<td style="text-align: center;">$85.301 \pm 93.883$</td>
<td style="text-align: center;">$44.875 \pm 4.756$</td>
<td style="text-align: center;">$3.989 \pm 0.627$</td>
<td style="text-align: center;">$75.549 \pm 12.840$</td>
</tr>
<tr>
<td style="text-align: center;">Coins</td>
<td style="text-align: center;">Without</td>
<td style="text-align: center;">$0.973 \pm 0.025$</td>
<td style="text-align: center;">$35.750 \pm 11.200$</td>
<td style="text-align: center;">$1.657 \pm 0.532$</td>
<td style="text-align: center;">$44.476 \pm 7.302$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 1</td>
<td style="text-align: center;">$0.964 \pm 0.039$</td>
<td style="text-align: center;">$37.469 \pm 10.992$</td>
<td style="text-align: center;">$1.700 \pm 0.452$</td>
<td style="text-align: center;">$32.539 \pm 5.622$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 2</td>
<td style="text-align: center;">$0.974 \pm 0.028$</td>
<td style="text-align: center;">$35.031 \pm 11.290$</td>
<td style="text-align: center;">$1.492 \pm 0.407$</td>
<td style="text-align: center;">$34.720 \pm 4.253$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 4</td>
<td style="text-align: center;">$0.974 \pm 0.032$</td>
<td style="text-align: center;">$36.656 \pm 10.527$</td>
<td style="text-align: center;">$1.477 \pm 0.411$</td>
<td style="text-align: center;">$34.884 \pm 5.264$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 6</td>
<td style="text-align: center;">$0.954 \pm 0.054$</td>
<td style="text-align: center;">$37.250 \pm 10.336$</td>
<td style="text-align: center;">$1.522 \pm 0.355$</td>
<td style="text-align: center;">$36.566 \pm 3.919$</td>
</tr>
<tr>
<td style="text-align: center;">Faces</td>
<td style="text-align: center;">Without</td>
<td style="text-align: center;">$0.855 \pm 0.117$</td>
<td style="text-align: center;">$46.077 \pm 3.578$</td>
<td style="text-align: center;">$2.646 \pm 0.578$</td>
<td style="text-align: center;">$37.861 \pm 8.755$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 1</td>
<td style="text-align: center;">$0.876 \pm 0.104$</td>
<td style="text-align: center;">$45.712 \pm 4.415$</td>
<td style="text-align: center;">$2.622 \pm 0.583$</td>
<td style="text-align: center;">$29.798 \pm 6.571$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 2</td>
<td style="text-align: center;">$0.867 \pm 0.117$</td>
<td style="text-align: center;">$45.885 \pm 3.717$</td>
<td style="text-align: center;">$2.367 \pm 0.460$</td>
<td style="text-align: center;">$29.364 \pm 5.966$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 4</td>
<td style="text-align: center;">$0.851 \pm 0.105$</td>
<td style="text-align: center;">$46.077 \pm 3.578$</td>
<td style="text-align: center;">$2.251 \pm 0.441$</td>
<td style="text-align: center;">$28.917 \pm 5.757$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Every 6</td>
<td style="text-align: center;">$0.866 \pm 0.075$</td>
<td style="text-align: center;">$46.077 \pm 3.578$</td>
<td style="text-align: center;">$2.288 \pm 0.464$</td>
<td style="text-align: center;">$30.081 \pm 6.274$</td>
</tr>
</tbody>
</table>
<p>the simplification process removes the redundancy, makes the genetic programs shorter, and accordingly reduces the search space.</p>
<p>Not surprisingly, the GP system with simplification at every generation generally led to a very slight increase in training time in most cases (except for the face data set where there was a slight improvement). This can be attributed to the overhead introduced into the system by the simplification component. This overhead, when occurring at every generation, usually outweighs the time saved from processing smaller simplified programs.</p>
<h3>4.3 Program Size</h3>
<p>As can be seen from the last column of table 5, the average size of the programs is significantly reduced for the GP system with simplification at all frequencies over the basic GP without simplification. The small size programs have a big advantage in that the actual computation time of the solution program will be short. This is particularly useful in the situations that has a strict time requirement such as in some industrial control and security systems.</p>
<p>To do a further analysis, we also present the average size of genetic programs at every generation for the four tasks in figure 8. While performing simplification at lower frequencies results in a higher average program size than performing at every generation, this increase in size is very small, suggesting that simplification does not need to be performed at every generation.</p>
<h3>4.4 Simplification Frequency Analysis</h3>
<p>In most data sets, applying simplification at every generation led to a slight loss in fitness and a slightly higher computational cost. This suggests that in general, simplification should not be applied to evolution at every generation.</p>
<p>While GP with different simplification frequencies results in different results, it is always possible to find a good one that can achieve better effectiveness and better efficiency than the basic GP without simplification. However, this is generally task dependent and usually needs an empirical search. But if such a search can improve the system performance significantly, this is a small price to pay. Our ex-
periments suggest that simplification at every 2 generations could serve as a starting point.</p>
<h2>5. CONCLUSIONS</h2>
<p>The goal of this paper was to develop an online program simplification approach in GP during the evolutionary process. This goal was successfully achieved by defining a set of algebraic simplification rules, traversing the program tree in an bottom-up fashion by an postfix order, and applying the simplification rules along with an algebraic equivalence component to non-terminal nodes to simplify programs directly without needing to translate it into another format.</p>
<p>The GP system with the simplification algorithm was examined and compared with the basic GP approach without simplification on four regression and classification problems of varying difficulty. The results suggest that, the new simplification approach outperformed the basic GP approach in terms of effectiveness, efficiency and program size on these data sets.</p>
<p>The results also suggest that performing simplification at every generation is not recommended and simplification at every two generations could serve as a starting point.</p>
<p>The online simplification during evolution seems to be able to reduce the search space. While it could introduce new genetic materials, it is not clear whether and/or how it destroys good building blocks in the early stage of evolution, which needs to be further investigated.</p>
<p>In the current approach, we applied the simplification to all individual programs in the population. We will investigate whether the performance can be further improved if we only simplify a proportion of programs in the population. We will also investigate what effects would be produced if we consider the simplification as a new operator and put it into the function set.</p>
<h2>6. ACKNOWLEDGEMENT</h2>
<p>This work was supported in part by the Marsden Fund of New Zealand under grant No.05-VUW-017 and the University Research Fund 06/9 at Victoria University of Wellington.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Average program size per generation. (a) $\operatorname{Reg}<em 2="2">{1}$; (b) $\operatorname{Reg}</em>$; (c) Coins; (d) Faces.</p>
<h2>7. REFERENCES</h2>
<p>[1] W. Banzhaf, P. Nordin, R. E. Keller, and F. D. Francone. Genetic Programming: An Introduction on the Automatic Evolution of computer programs and its Applications. Morgan Kaufmann Publishers, 1998.
[2] T. Blickle and L. Thiele. Genetic programming and redundancy. In J. Hopf, editor, Genetic Algorithms within the Framework of Evolutionary Computation, pages 33-38, Germany, 1994.
[3] J. Busch, J. Ziegler, C. Aue, A. Ross, D. Sawitzki, and W. Banzhaf. Automatic generation of control programs for walking robots using genetic programming. In EuroGP '02: Proceedings of the 5th European Conference on Genetic Programming, pages 258-267, London, UK, 2002.
[4] B. Cherowitzo, 2006. Lecture Notes. http://wwwmath.cudenver.edu/ wcherowi/courses/m5410/ exeucalg.html. Visited on 7 January 2006.
[5] R. Fikes and N. Nilsson. Strips: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2:189-208, 1971.
[6] A. Georghiades, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach. Intelligence, 23(6):643-660, 2001.
[7] G. H. Gonnet. Determining equivalence of expressions in random polynomial time. In Proceedings of the sixteenth annual ACM symposium on Theory of computing, pages 334-341, New York, USA, 1984.
[8] J. R. Koza. Genetic Programming: On the Programming of Computers by Means of Natural Selection. MIT Press, Cambridge, MA, USA, 1992.
[9] J. R. Koza, M. A. Keane, M. J. Streeter, W. Mydlowec, J. Yu, and G. Lanza. Genetic</p>
<p>Programming IV: Routine Human-Competitive Machine Intelligence. Kluwer Academic Publishers, 2003.
[10] R. Lidl and H. Niederreiter. Introduction to finite fields and their applications. Cambridge University Press, New York, NY, USA, 1986.
[11] T. Loveard and V. Ciesielski. Representing classification problems in genetic programming. In Proceedings of the Congress on Evolutionary Computation, volume 2, pages 1070-1077, Seoul, Korea, 2001. IEEE Press.
[12] W. A. Martin. Determining the equivalence of algebraic expressions by hash coding. $j$-J-ACM, 18(4):549-558, Oct 1971.
[13] R. Poli. Genetic programming for image analysis. In Proceedings of the First Annual Conference, pages 363-368, Stanford University, CA, USA, 28-31 July 1996. MIT Press.
[14] T. Soule, J. A. Foster, and J. Dickinson. Code growth in genetic programming. In Proceedings of the First Annual Conference, pages 215-223, Stanford University, CA, USA, 28-31 1996. MIT Press.
[15] W. Trappe and L. C. Washington. Introduction to Cryptograpy with Coding theory. Prentice-Hall, 2ed edition, 2006.
[16] M. Zhang and V. Ciesielski. Genetic programming for multiple class object detection. In Proceedings of the 12th Australian Joint Conference on Artificial Intelligence, volume 1747 of LNAI, pages 180-192, Sydney, Australia. 1999. Springer-Verlag.
[17] M. Zhang and W. Smart. Multiclass object classification using genetic programming. In Applications of Evolutionary Computing, EvoWorkshops2004, volume 3005 of LNCS, pages 369-378, Portugal. 2004. Springer Verlag.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
GECCO'06, July 8-12, 2006, Seattle, Washington, USA.
Copyright 2006 ACM 1-59593-186-4/06/0007 ... $\$ 5.00$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
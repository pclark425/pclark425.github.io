<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3965 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3965</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3965</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-d7da009f457917aa381619facfa5ffae9329a6e9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9" target="_blank">Bleu: a Method for Automatic Evaluation of Machine Translation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.</p>
                <p><strong>Paper Abstract:</strong> Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3965.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3965.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bilingual Evaluation Understudy (BLEU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic, corpus-level metric for evaluating generated text by measuring closeness to one or more human reference texts using clipped n-gram precision (up to n=4) combined with a brevity penalty and geometric averaging; shown to correlate highly with human judgments for machine translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Closeness to human references operationalized as: adequacy (word choice overlap), fluency (longer n-gram matches), and appropriate length (via a brevity penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute modified n-gram precisions p_n (clipped counts against references) for n=1..N (baseline N=4); take geometric mean (weighted log-average) of p_n; multiply by corpus-level brevity penalty BP = 1 if candidate length > effective reference length r, else exp(1 - r/c). Evaluate over a test corpus (aggregate and block-level). Compare BLEU scores across systems and correlate with human judgments (linear regression).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Test corpus used in paper: ~500 sentences drawn from ~40 general news stories with 2–4 human reference translations per sentence (experiments run with 2 and 4 references); also tested on block splits (20 blocks of 25 sentences) to estimate variance.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>BLEU score (range 0–1), modified n-gram precisions p_n (for n=1..4), brevity penalty (BP), corpus and block means and standard deviations, paired t-statistics comparing systems, linear-regression correlation coefficients between BLEU and human scores (r=0.99 monolingual, r=0.96 bilingual), normalized range scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>BLEU is presented as an automatic understudy to human judges; human reference translations are required (2–4 per sentence) to compute BLEU. Human judgments (used as gold-standard for correlation) were collected from two judge pools (10 monolingual English speakers and 10 bilingual Chinese-English speakers) who rated translations on a 1–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires human reference translations (score depends on number and variety of references); correlates reliably only when averaged over a sizable test corpus (sentence-level BLEU may disagree with human judgment); penalizes phrasing differences even if fluent (style mismatch across references); modified precision cannot enforce proper length alone (necessitates brevity penalty); geometric mean is sensitive to any zero p_n; recall-based measures are problematic because multiple references contain alternative valid choices; number of references affects absolute scores (e.g., human scored higher with 4 refs than with 2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BLEU discriminated clearly between human and machine outputs and preserved system ranking observed by human judges; reported BLEU scores on the 500-sentence corpus: S1=0.0527, S2=0.0829, S3=0.0930, H1=0.1934, H2=0.2571. Blocked evaluation and paired t-tests showed differences were statistically significant; regression showed very high correlation with monolingual human judgments (r=0.99) and bilingual judgments (r=0.96).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bleu: a Method for Automatic Evaluation of Machine Translation', 'publication_date_yy_mm': '2002-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3965.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3965.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (monolingual/bilingual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human judge evaluation (monolingual readability judges and bilingual adequacy judges)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human rating procedure used as the reference standard: two panels of 10 judges each (monolingual English and bilingual Chinese-English) rated translations on a 1–5 quality scale; monolingual judges focused on fluency/readability while bilingual judges emphasized adequacy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a Method for Automatic Evaluation of Machine Translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Readability/fluency (monolingual judges) and adequacy/faithfulness to source meaning (bilingual judges), scored on an ordinal 1–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Each judge rated 250 translation pairs (each source sentence paired with each system output) presented in randomized order; per-judge normalization across sentences was used and pairwise t-tests between adjacent systems were computed to assess significance; confidence intervals (95%) were derived from t-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Subset of the same 500-sentence test corpus (random Chinese sentence subset) producing 250 source-translation pairs per judge; judges saw the same randomized web page ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Mean opinion scores (1–5), mean differences between systems with 95% confidence intervals, paired t-statistics for block comparisons, and inter-system rank order.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High: two groups of human judges (10 monolingual English speakers; 10 bilingual Chinese-English speakers), non-professional translators; judges provided per-sentence quality ratings used to compute system rankings and to validate BLEU via correlation/regression.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Inter-judge variability (some judges more liberal), skipped judgements by some judges leading to varying N per test; human evaluation is costly and slow (motivating BLEU); monolingual judges cannot judge adequacy without source text; bilinguals focus more on adequacy which can change ranking relative to monolingual judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human panels produced system rankings that matched BLEU rankings; mean differences between systems (e.g., S2>S1 by ~0.326 on 5-point scale) were statistically significant at 95% CI; monolingual and bilingual correlations with BLEU were 0.99 and 0.96 respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bleu: a Method for Automatic Evaluation of Machine Translation', 'publication_date_yy_mm': '2002-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3965.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3965.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical validation (blocking, paired t-test, regression)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Block-based variance estimation, paired t-tests, and regression correlation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical procedures used to test reliability and significance of differences between systems: corpus split into blocks to estimate variance, paired t-tests between systems on block-level BLEU scores, and linear regression to quantify correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a Method for Automatic Evaluation of Machine Translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Statistical significance of inter-system score differences (e.g., 95% confidence based on paired t-tests) and strength of association between automatic metric and human judgments (Pearson correlation via linear regression).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Divide test corpus into 20 blocks of 25 sentences to obtain BLEU samples; compute means, standard deviations, and paired t-statistics comparing adjacent systems; perform linear regression of human mean scores on BLEU scores to obtain correlation coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>20-block partitioning of the ~500-sentence test corpus (25 sentences per block) used to produce sample distributions for statistical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Block means, standard deviations, paired t values (e.g., t=6 between S1 and S2), and Pearson correlation coefficients from regression (r=0.99 for monolingual judges, r=0.96 for bilingual judges).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human judgments provided the y-variable used in regression and significance testing of differences between systems; paired t-tests compare BLEU-derived scores across systems (no human required for t-test itself).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Block size affects variance estimates (reported variance on 25-sentence blocks is an upper bound for larger corpora); t-tests assume approximate normality (t-distribution with N degrees of freedom, N varied due to skipped judgements); corpus sampling effects (different random corpora might change rankings) require sufficiently large, stylistically varied test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using blocking and paired t-tests, differences between many systems were found to be statistically significant (paired t-statistics >= 1.7 for 95% significance); regression analysis showed very high linear correlation between BLEU and human scores (0.99 and 0.96).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bleu: a Method for Automatic Evaluation of Machine Translation', 'publication_date_yy_mm': '2002-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toward finely differentiated evaluation metrics for machine translation. <em>(Rating: 2)</em></li>
                <li>Additional mt-eval references. <em>(Rating: 2)</em></li>
                <li>The ARPA MT evaluation methodologies: evolution, lessons, and future approaches. <em>(Rating: 2)</em></li>
                <li>Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3965",
    "paper_id": "paper-d7da009f457917aa381619facfa5ffae9329a6e9",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "BLEU",
            "name_full": "Bilingual Evaluation Understudy (BLEU)",
            "brief_description": "An automatic, corpus-level metric for evaluating generated text by measuring closeness to one or more human reference texts using clipped n-gram precision (up to n=4) combined with a brevity penalty and geometric averaging; shown to correlate highly with human judgments for machine translation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Closeness to human references operationalized as: adequacy (word choice overlap), fluency (longer n-gram matches), and appropriate length (via a brevity penalty).",
            "evaluation_methods": "Compute modified n-gram precisions p_n (clipped counts against references) for n=1..N (baseline N=4); take geometric mean (weighted log-average) of p_n; multiply by corpus-level brevity penalty BP = 1 if candidate length &gt; effective reference length r, else exp(1 - r/c). Evaluate over a test corpus (aggregate and block-level). Compare BLEU scores across systems and correlate with human judgments (linear regression).",
            "benchmark_or_dataset": "Test corpus used in paper: ~500 sentences drawn from ~40 general news stories with 2–4 human reference translations per sentence (experiments run with 2 and 4 references); also tested on block splits (20 blocks of 25 sentences) to estimate variance.",
            "metrics_reported": "BLEU score (range 0–1), modified n-gram precisions p_n (for n=1..4), brevity penalty (BP), corpus and block means and standard deviations, paired t-statistics comparing systems, linear-regression correlation coefficients between BLEU and human scores (r=0.99 monolingual, r=0.96 bilingual), normalized range scores.",
            "human_involvement": "BLEU is presented as an automatic understudy to human judges; human reference translations are required (2–4 per sentence) to compute BLEU. Human judgments (used as gold-standard for correlation) were collected from two judge pools (10 monolingual English speakers and 10 bilingual Chinese-English speakers) who rated translations on a 1–5 scale.",
            "limitations_or_challenges": "Requires human reference translations (score depends on number and variety of references); correlates reliably only when averaged over a sizable test corpus (sentence-level BLEU may disagree with human judgment); penalizes phrasing differences even if fluent (style mismatch across references); modified precision cannot enforce proper length alone (necessitates brevity penalty); geometric mean is sensitive to any zero p_n; recall-based measures are problematic because multiple references contain alternative valid choices; number of references affects absolute scores (e.g., human scored higher with 4 refs than with 2).",
            "llm_theory_example": null,
            "evaluation_results": "BLEU discriminated clearly between human and machine outputs and preserved system ranking observed by human judges; reported BLEU scores on the 500-sentence corpus: S1=0.0527, S2=0.0829, S3=0.0930, H1=0.1934, H2=0.2571. Blocked evaluation and paired t-tests showed differences were statistically significant; regression showed very high correlation with monolingual human judgments (r=0.99) and bilingual judgments (r=0.96).",
            "uuid": "e3965.0",
            "source_info": {
                "paper_title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "publication_date_yy_mm": "2002-07"
            }
        },
        {
            "name_short": "Human evaluation (monolingual/bilingual)",
            "name_full": "Human judge evaluation (monolingual readability judges and bilingual adequacy judges)",
            "brief_description": "Human rating procedure used as the reference standard: two panels of 10 judges each (monolingual English and bilingual Chinese-English) rated translations on a 1–5 quality scale; monolingual judges focused on fluency/readability while bilingual judges emphasized adequacy.",
            "citation_title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
            "mention_or_use": "use",
            "evaluation_criteria": "Readability/fluency (monolingual judges) and adequacy/faithfulness to source meaning (bilingual judges), scored on an ordinal 1–5 scale.",
            "evaluation_methods": "Each judge rated 250 translation pairs (each source sentence paired with each system output) presented in randomized order; per-judge normalization across sentences was used and pairwise t-tests between adjacent systems were computed to assess significance; confidence intervals (95%) were derived from t-distribution.",
            "benchmark_or_dataset": "Subset of the same 500-sentence test corpus (random Chinese sentence subset) producing 250 source-translation pairs per judge; judges saw the same randomized web page ordering.",
            "metrics_reported": "Mean opinion scores (1–5), mean differences between systems with 95% confidence intervals, paired t-statistics for block comparisons, and inter-system rank order.",
            "human_involvement": "High: two groups of human judges (10 monolingual English speakers; 10 bilingual Chinese-English speakers), non-professional translators; judges provided per-sentence quality ratings used to compute system rankings and to validate BLEU via correlation/regression.",
            "limitations_or_challenges": "Inter-judge variability (some judges more liberal), skipped judgements by some judges leading to varying N per test; human evaluation is costly and slow (motivating BLEU); monolingual judges cannot judge adequacy without source text; bilinguals focus more on adequacy which can change ranking relative to monolingual judgments.",
            "llm_theory_example": null,
            "evaluation_results": "Human panels produced system rankings that matched BLEU rankings; mean differences between systems (e.g., S2&gt;S1 by ~0.326 on 5-point scale) were statistically significant at 95% CI; monolingual and bilingual correlations with BLEU were 0.99 and 0.96 respectively.",
            "uuid": "e3965.1",
            "source_info": {
                "paper_title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "publication_date_yy_mm": "2002-07"
            }
        },
        {
            "name_short": "Statistical validation (blocking, paired t-test, regression)",
            "name_full": "Block-based variance estimation, paired t-tests, and regression correlation analysis",
            "brief_description": "Statistical procedures used to test reliability and significance of differences between systems: corpus split into blocks to estimate variance, paired t-tests between systems on block-level BLEU scores, and linear regression to quantify correlation with human judgments.",
            "citation_title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
            "mention_or_use": "use",
            "evaluation_criteria": "Statistical significance of inter-system score differences (e.g., 95% confidence based on paired t-tests) and strength of association between automatic metric and human judgments (Pearson correlation via linear regression).",
            "evaluation_methods": "Divide test corpus into 20 blocks of 25 sentences to obtain BLEU samples; compute means, standard deviations, and paired t-statistics comparing adjacent systems; perform linear regression of human mean scores on BLEU scores to obtain correlation coefficients.",
            "benchmark_or_dataset": "20-block partitioning of the ~500-sentence test corpus (25 sentences per block) used to produce sample distributions for statistical tests.",
            "metrics_reported": "Block means, standard deviations, paired t values (e.g., t=6 between S1 and S2), and Pearson correlation coefficients from regression (r=0.99 for monolingual judges, r=0.96 for bilingual judges).",
            "human_involvement": "Human judgments provided the y-variable used in regression and significance testing of differences between systems; paired t-tests compare BLEU-derived scores across systems (no human required for t-test itself).",
            "limitations_or_challenges": "Block size affects variance estimates (reported variance on 25-sentence blocks is an upper bound for larger corpora); t-tests assume approximate normality (t-distribution with N degrees of freedom, N varied due to skipped judgements); corpus sampling effects (different random corpora might change rankings) require sufficiently large, stylistically varied test sets.",
            "llm_theory_example": null,
            "evaluation_results": "Using blocking and paired t-tests, differences between many systems were found to be statistically significant (paired t-statistics &gt;= 1.7 for 95% significance); regression analysis showed very high linear correlation between BLEU and human scores (0.99 and 0.96).",
            "uuid": "e3965.2",
            "source_info": {
                "paper_title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "publication_date_yy_mm": "2002-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toward finely differentiated evaluation metrics for machine translation.",
            "rating": 2
        },
        {
            "paper_title": "Additional mt-eval references.",
            "rating": 2
        },
        {
            "paper_title": "The ARPA MT evaluation methodologies: evolution, lessons, and future approaches.",
            "rating": 2
        },
        {
            "paper_title": "Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results.",
            "rating": 2
        }
    ],
    "cost": 0.010325999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bleu: a Method for Automatic Evaluation of Machine Translation</h1>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu<br>IBM T. J. Watson Research Center<br>Yorktown Heights, NY 10598, USA<br>{papineni,roukos,toddward,weijing}@us.ibm.com</p>
<h4>Abstract</h4>
<p>Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<h3>1.1 Rationale</h3>
<p>Human evaluations of machine translation (MT) weigh many aspects of translation, including adequacy, fidelity, and fluency of the translation (Hovy, 1999; White and O'Connell, 1994). A comprehensive catalog of MT evaluation techniques and their rich literature is given by Reeder (2001). For the most part, these various human evaluation approaches are quite expensive (Hovy, 1999). Moreover, they can take weeks or months to finish. This is a big problem because developers of machine translation systems need to monitor the effect of daily changes to their systems in order to weed out bad ideas from good ideas. We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the evaluation bottleneck. Developers would benefit from an inexpensive automatic evaluation that is quick, language-independent, and correlates highly with human evaluation. We propose such an evaluation method in this paper.</p>
<h3>1.2 Viewpoint</h3>
<p>How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the central idea behind our proposal. To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric. Thus, our MT evaluation system requires two ingredients:</p>
<ol>
<li>a numerical "translation closeness" metric</li>
<li>a corpus of good quality human reference translations</li>
</ol>
<p>We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order. The main idea is to use a weighted average of variable length phrase matches against the reference translations. This view gives rise to a family of metrics using various weighting schemes. We have selected a promising baseline metric from this family.</p>
<p>In Section 2, we describe the baseline metric in detail. In Section 3, we evaluate the performance of Bleu. In Section 4, we describe a human evaluation experiment. In Section 5, we compare our baseline metric performance with human evaluations.</p>
<h2>2 The Baseline Bleu Metric</h2>
<p>Typically, there are many "perfect" translations of a given source sentence. These translations may vary in word choice or in word order even when they use the same words. And yet humans can clearly distinguish a good translation from a bad one. For example, consider these two candidate translations of a Chinese source sentence:</p>
<h2>Example 1.</h2>
<p>Candidate 1: It is a guide to action which ensures that the military always obeys the commands of the party.
Candidate 2: It is to insure the troops forever hearing the activity guidebook that party direct.</p>
<p>Although they appear to be on the same subject, they differ markedly in quality. For comparison, we provide three reference human translations of the same sentence below.</p>
<p>Reference 1: It is a guide to action that ensures that the military will forever heed Party commands.
Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party.
Reference 3: It is the practical guide for the army always to heed the directions of the party.</p>
<p>It is clear that the good translation, Candidate 1, shares many words and phrases with these three reference translations, while Candidate 2 does not. We will shortly quantify this notion of sharing in Section 2.1. But first observe that Candidate 1 shares "It is a guide to action" with Reference 1, "which" with Reference 2, "ensures that the military" with Reference 1, "always" with References 2 and 3, "commands" with Reference 1, and finally "of the party" with Reference 2 (all ignoring capitalization). In contrast, Candidate 2 exhibits far fewer matches, and their extent is less.</p>
<p>It is clear that a program can rank Candidate 1 higher than Candidate 2 simply by comparing $n$ gram matches between each candidate translation and the reference translations. Experiments over
large collections of translations presented in Section 5 show that this ranking ability is a general phenomenon, and not an artifact of a few toy examples.</p>
<p>The primary programming task for a Bleu implementor is to compare $n$-grams of the candidate with the $n$-grams of the reference translation and count the number of matches. These matches are positionindependent. The more the matches, the better the candidate translation is. For simplicity, we first focus on computing unigram matches.</p>
<h3>2.1 Modified $n$-gram precision</h3>
<p>The cornerstone of our metric is the familiar precision measure. To compute precision, one simply counts up the number of candidate translation words (unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation. Unfortunately, MT systems can overgenerate "reasonable" words, resulting in improbable, but high-precision, translations like that of example 2 below. Intuitively the problem is clear: a reference word should be considered exhausted after a matching candidate word is identified. We formalize this intuition as the modified unigram precision. To compute this, one first counts the maximum number of times a word occurs in any single reference translation. Next, one clips the total count of each candidate word by its maximum reference count, ${ }^{2}$ adds these clipped counts up, and divides by the total (unclipped) number of candidate words.</p>
<h2>Example 2.</h2>
<p>Candidate: the the the the the the the.
Reference 1: The cat is on the mat.
Reference 2: There is a cat on the mat.
Modified Unigram Precision $=2 / 7 .{ }^{3}$
In Example 1, Candidate 1 achieves a modified unigram precision of $17 / 18$; whereas Candidate 2 achieves a modified unigram precision of $8 / 14$. Similarly, the modified unigram precision in Example 2 is $2 / 7$, even though its standard unigram precision is $7 / 7$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Modified $n$-gram precision is computed similarly for any $n$ : all candidate $n$-gram counts and their corresponding maximum reference counts are collected. The candidate counts are clipped by their corresponding reference maximum value, summed, and divided by the total number of candidate $n$ grams. In Example 1, Candidate 1 achieves a modified bigram precision of 10/17, whereas the lower quality Candidate 2 achieves a modified bigram precision of 1/13. In Example 2, the (implausible) candidate achieves a modified bigram precision of 0 . This sort of modified $n$-gram precision scoring captures two aspects of translation: adequacy and fluency. A translation using the same words (1-grams) as in the references tends to satisfy adequacy. The longer $n$-gram matches account for fluency. ${ }^{4}$</p>
<h3>2.1.1 Modified $n$-gram precision on blocks of text</h3>
<p>How do we compute modified $n$-gram precision on a multi-sentence test set? Although one typically evaluates MT systems on a corpus of entire documents, our basic unit of evaluation is the sentence. A source sentence may translate to many target sentences, in which case we abuse terminology and refer to the corresponding target sentences as a "sentence." We first compute the $n$-gram matches sentence by sentence. Next, we add the clipped $n$-gram counts for all the candidate sentences and divide by the number of candidate $n$-grams in the test corpus to compute a modified precision score, $p_{n}$, for the entire test corpus.</p>
<p>$$
\begin{aligned}
&amp; p_{n}= \
&amp; \frac{\sum_{\mathcal{C} \in{\text { Candidates }}} \sum_{n-\text { gram } \in \mathcal{C}} \text { Count }<em _mathcal_C="\mathcal{C">{\text {clip }}(n-\text { gram })}{\sum</em> .
\end{aligned}
$$}^{\prime} \in{\text { Candidates }}} \sum_{n-\text { gram }^{\prime} \in \mathcal{C}^{\prime}} \text { Count }\left(n-\text { gram }^{\prime}\right)</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.1.2 Ranking systems using only modified $n$-gram precision</h3>
<p>To verify that modified $n$-gram precision distinguishes between very good translations and bad translations, we computed the modified precision numbers on the output of a (good) human translator and a standard (poor) machine translation system using 4 reference translations for each of 127 source sentences. The average precision results are shown in Figure 1.</p>
<p>Figure 1: Distinguishing Human from Machine
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>The strong signal differentiating human (high precision) from machine (low precision) is striking. The difference becomes stronger as we go from unigram precision to 4 -gram precision. It appears that any single $n$-gram precision score can distinguish between a good translation and a bad translation. To be useful, however, the metric must also reliably distinguish between translations that do not differ so greatly in quality. Furthermore, it must distinguish between two human translations of differing quality. This latter requirement ensures the continued validity of the metric as MT approaches human translation quality.</p>
<p>To this end, we obtained a human translation by someone lacking native proficiency in both the source (Chinese) and the target language (English). For comparison, we acquired human translations of the same documents by a native English speaker. We also obtained machine translations by three commercial systems. These five "systems" - two humans and three machines - are scored against two reference professional human translations. The average modified $n$-gram precision results are shown in Figure 2 .</p>
<p>Each of these $n$-gram statistics implies the same</p>
<p>Figure 2: Machine and Human Translations
<img alt="img-1.jpeg" src="img-1.jpeg" />
ranking: H2 (Human-2) is better than H1 (Human1), and there is a big drop in quality between H1 and S3 (Machine/System-3). S3 appears better than S2 which in turn appears better than S1. Remarkably, this is the same rank order assigned to these "systems" by human judges, as we discuss later. While there seems to be ample signal in any single $n$-gram precision, it is more robust to combine all these signals into a single number metric.</p>
<h3>2.1.3 Combining the modified $n$-gram precisions</h3>
<p>How should we combine the modified precisions for the various $n$-gram sizes? A weighted linear average of the modified precisions resulted in encouraging results for the 5 systems. However, as can be seen in Figure 2, the modified $n$-gram precision decays roughly exponentially with $n$ : the modified unigram precision is much larger than the modified bigram precision which in turn is much bigger than the modified trigram precision. A reasonable averaging scheme must take this exponential decay into account; a weighted average of the logarithm of modified precisions satisifies this requirement.</p>
<p>Bleu uses the average logarithm with uniform weights, which is equivalent to using the geometric mean of the modified $n$-gram precisions. ${ }^{5,6}$ Experimentally, we obtain the best correlation with mono-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>lingual human judgments using a maximum $n$-gram order of 4 , although 3-grams and 5-grams give comparable results.</p>
<h3>2.2 Sentence length</h3>
<p>A candidate translation should be neither too long nor too short, and an evaluation metric should enforce this. To some extent, the $n$-gram precision already accomplishes this. $N$-gram precision penalizes spurious words in the candidate that do not appear in any of the reference translations. Additionally, modified precision is penalized if a word occurs more frequently in a candidate translation than its maximum reference count. This rewards using a word as many times as warranted and penalizes using a word more times than it occurs in any of the references. However, modified $n$-gram precision alone fails to enforce the proper translation length, as is illustrated in the short, absurd example below.</p>
<h2>Example 3:</h2>
<p>Candidate: of the
Reference 1: It is a guide to action that ensures that the military will forever heed Party commands.
Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party.
Reference 3: It is the practical guide for the army always to heed the directions of the party.</p>
<p>Because this candidate is so short compared to the proper length, one expects to find inflated precisions: the modified unigram precision is $2 / 2$, and the modified bigram precision is $1 / 1$.</p>
<h3>2.2.1 The trouble with recall</h3>
<p>Traditionally, precision has been paired with recall to overcome such length-related problems. However, Bleu considers multiple reference translations, each of which may use a different word choice to translate the same source word. Furthermore, a good candidate translation will only use (recall) one of these possible choices, but not all. Indeed, recalling all choices leads to a bad translation. Here is an example.</p>
<h2>Example 4:</h2>
<p>Candidate 1: I always invariably perpetually do.
Candidate 2: I always do.
Reference 1: I always do.
Reference 2: I invariably do.
Reference 3: I perpetually do.
The first candidate recalls more words from the references, but is obviously a poorer translation than the second candidate. Thus, naïve recall computed over the set of all reference words is not a good measure. Admittedly, one could align the reference translations to discover synonymous words and compute recall on concepts rather than words. But, given that reference translations vary in length and differ in word order and syntax, such a computation is complicated.</p>
<h3>2.2.2 Sentence brevity penalty</h3>
<p>Candidate translations longer than their references are already penalized by the modified $n$-gram precision measure: there is no need to penalize them again. Consequently, we introduce a multiplicative brevity penalty factor. With this brevity penalty in place, a high-scoring candidate translation must now match the reference translations in length, in word choice, and in word order. Note that neither this brevity penalty nor the modified $n$-gram precision length effect directly considers the source length; instead, they consider the range of reference translation lengths in the target language.</p>
<p>We wish to make the brevity penalty 1.0 when the candidate's length is the same as any reference translation's length. For example, if there are three references with lengths 12,15 , and 17 words and the candidate translation is a terse 12 words, we want the brevity penalty to be 1 . We call the closest reference sentence length the "best match length."</p>
<p>One consideration remains: if we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sentences would be punished harshly. Instead, we compute the brevity penalty over the entire corpus to allow some freedom at the sentence level. We first compute the test corpus' effective reference length, $r$, by summing the best match lengths for each candidate sentence in the corpus. We choose the brevity
penalty to be a decaying exponential in $r / c$, where $c$ is the total length of the candidate translation corpus.</p>
<h3>2.3 Bleu details</h3>
<p>We take the geometric mean of the test corpus' modified precision scores and then multiply the result by an exponential brevity penalty factor. Currently, case folding is the only text normalization performed before computing the precision.</p>
<p>We first compute the geometric average of the modified $n$-gram precisions, $p_{n}$, using $n$-grams up to length $N$ and positive weights $w_{n}$ summing to one.</p>
<p>Next, let $c$ be the length of the candidate translation and $r$ be the effective reference corpus length. We compute the brevity penalty BP,</p>
<p>$$
\mathrm{BP}= \begin{cases}1 &amp; \text { if } c&gt;r \ e^{(1-r / c)} &amp; \text { if } c \leq r\end{cases}
$$</p>
<p>Then,</p>
<p>$$
\mathrm{BLEU}=\mathrm{BP} \cdot \exp \left(\sum_{n=1}^{N} w_{n} \log p_{n}\right)
$$</p>
<p>The ranking behavior is more immediately apparent in the $\log$ domain,</p>
<p>$$
\log \mathrm{BLEU}=\min \left(1-\frac{r}{c}, 0\right)+\sum_{n=1}^{N} w_{n} \log p_{n}
$$</p>
<p>In our baseline, we use $N=4$ and uniform weights $w_{n}=1 / N$.</p>
<h2>3 The Bleu Evaluation</h2>
<p>The Bleu metric ranges from 0 to 1 . Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even a human translator will not necessarily score 1. It is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even "rough" comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences ( 40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references. Table 1 shows the Bleu scores of the 5 systems against two references on this test corpus.</p>
<p>The MT systems S2 and S3 are very close in this metric. Hence, several questions arise:</p>
<p>Table 1: Bleu on 500 sentences</p>
<table>
<thead>
<tr>
<th>S1</th>
<th>S2</th>
<th>S3</th>
<th>H1</th>
<th>H2</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0527</td>
<td>0.0829</td>
<td>0.0930</td>
<td>0.1934</td>
<td>0.2571</td>
</tr>
</tbody>
</table>
<p>Table 2: Paired t-statistics on 20 blocks</p>
<table>
<thead>
<tr>
<th></th>
<th>S1</th>
<th>S2</th>
<th>S3</th>
<th>H1</th>
<th>H2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean</td>
<td>0.051</td>
<td>0.081</td>
<td>0.090</td>
<td>0.192</td>
<td>0.256</td>
</tr>
<tr>
<td>StdDev</td>
<td>0.017</td>
<td>0.025</td>
<td>0.020</td>
<td>0.030</td>
<td>0.039</td>
</tr>
<tr>
<td>t</td>
<td>-</td>
<td>6</td>
<td>3.4</td>
<td>24</td>
<td>11</td>
</tr>
</tbody>
</table>
<ul>
<li>Is the difference in Bleu metric reliable?</li>
<li>What is the variance of the Bleu score?</li>
<li>If we were to pick another random set of 500 sentences, would we still judge S3 to be better than S2?</li>
</ul>
<p>To answer these questions, we divided the test corpus into 20 blocks of 25 sentences each, and computed the Bleu metric on these blocks individually. We thus have 20 samples of the Bleu metric for each system. We computed the means, variances, and paired t-statistics which are displayed in Table 2. The t-statistic compares each system with its left neighbor in the table. For example, $t=6$ for the pair S1 and S2.</p>
<p>Note that the numbers in Table 1 are the Bleu metric on an aggregate of 500 sentences, but the means in Table 2 are averages of the Bleu metric on aggregates of 25 sentences. As expected, these two sets of results are close for each system and differ only by small finite block size effects. Since a paired t-statistic of 1.7 or above is $95 \%$ significant, the differences between the systems' scores are statistically very significant. The reported variance on 25 -sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus.</p>
<p>How many reference translations do we need? We simulated a single-reference test corpus by randomly selecting one of the 4 reference translations as the single reference for each of the 40 stories. In this way, we ensured a degree of stylistic variation. The systems maintain the same rank order as with multiple references. This outcome suggests that we may use a big test corpus with a single reference
translation, provided that the translations are not all from the same translator.</p>
<h2>4 The Human Evaluation</h2>
<p>We had two groups of human judges. The first group, called the monolingual group, consisted of 10 native speakers of English. The second group, called the bilingual group, consisted of 10 native speakers of Chinese who had lived in the United States for the past several years. None of the human judges was a professional translator. The humans judged our 5 standard systems on a Chinese sentence subset extracted at random from our 500 sentence test corpus. We paired each source sentence with each of its 5 translations, for a total of 250 pairs of Chinese source and English translations. We prepared a web page with these translation pairs randomly ordered to disperse the five translations of each source sentence. All judges used this same webpage and saw the sentence pairs in the same order. They rated each translation from 1 (very bad) to 5 (very good). The monolingual group made their judgments based only on the translations' readability and fluency.</p>
<p>As must be expected, some judges were more liberal than others. And some sentences were easier to translate than others. To account for the intrinsic difference between judges and the sentences, we compared each judge's rating for a sentence across systems. We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.</p>
<h3>4.1 Monolingual group pairwise judgments</h3>
<p>Figure 3 shows the mean difference between the scores of two consecutive systems and the $95 \%$ confidence interval about the mean. We see that S2 is quite a bit better than S1 (by a mean opinion score difference of 0.326 on the 5 -point scale), while S3 is judged a little better (by 0.114 ). Both differences are significant at the $95 \%$ level. ${ }^{7}$ The human H 1 is much better than the best system, though a bit worse than human H2. This is not surprising given that H1 is not a native speaker of either Chinese or English,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>whereas H 2 is a native English speaker. Again, the difference between the human translators is significant beyond the $95 \%$ level.</p>
<p>Figure 3: Monolingual Judgments - pairwise differential comparison
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<h3>4.2 Bilingual group pairwise judgments</h3>
<p>Figure 4 shows the same results for the bilingual group. They also find that S3 is slightly better than S2 (at $95 \%$ confidence) though they judge that the human translations are much closer (indistinguishable at $95 \%$ confidence), suggesting that the bilinguals tended to focus more on adequacy than on fluency.</p>
<p>Figure 4: Bilingual Judgments - pairwise differential comparison
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h2>5 Bleu vs The Human Evaluation</h2>
<p>Figure 5 shows a linear regression of the monolingual group scores as a function of the Bleu score over two reference translations for the 5 systems. The high correlation coefficient of 0.99 indicates that Bleu tracks human judgment well. Particularly interesting is how well Bleu distinguishes between S2 and S3 which are quite close. Figure 6 shows the comparable regression results for the bilingual group. The correlation coefficient is 0.96 .</p>
<p>Figure 5: Bleu predicts Monolingual Judgments
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Bleu predicts Bilingual Judgments
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>We now take the worst system as a reference point and compare the Bleu scores with the human judg-</p>
<p>ment scores of the remaining systems relative to the worst system. We took the Bleu, monolingual group, and bilingual group scores for the 5 systems and linearly normalized them by their corresponding range (the maximum and minimum score across the 5 systems). The normalized scores are shown in Figure 7. This figure illustrates the high correlation between the Bleu score and the monolingual group. Of particular interest is the accuracy of Bleu's estimate of the small difference between S2 and S3 and the larger difference between S3 and H1. The figure also highlights the relatively large gap between MT systems and human translators. ${ }^{8}$ In addition, we surmise that the bilingual group was very forgiving in judging H 1 relative to H 2 because the monolingual group found a rather large difference in the fluency of their translations.</p>
<p>Figure 7: Bleu vs Bilingual and Monolingual Judgments
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h2>6 Conclusion</h2>
<p>We believe that Bleu will accelerate the MT R\&amp;D cycle by allowing researchers to rapidly home in on effective modeling ideas. Our belief is reinforced by a recent statistical analysis of Bleu's correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)! Bleu's strength is that it correlates highly with human judg-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.</p>
<p>Finally, since MT and summarization can both be viewed as natural language generation from a textual context, we believe Bleu could be adapted to evaluating summarization or similar NLG tasks.</p>
<p>Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No. N66001-99-2-8916. The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.</p>
<p>We gratefully acknowledge comments about the geometric mean by John Makhoul of BBN and discussions with George Doddington of NIST. We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of ChineseEnglish MT systems.</p>
<h2>References</h2>
<p>E.H. Hovy. 1999. Toward finely differentiated evaluation metrics for machine translation. In Proceedings of the Eagles Workshop on Standards and Evaluation, Pisa, Italy.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, John Henderson, and Florence Reeder. 2002. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results. In Proceedings of Human Language Technology 2002, San Diego, CA. To appear.</p>
<p>Florence Reeder. 2001. Additional mt-eval references. Technical report, International Standards for Language Engineering, Evaluation Working Group. http://issco-www.unige.ch/projects/isle/taxonomy2/
J.S. White and T. O’Connell. 1994. The ARPA MT evaluation methodologies: evolution, lessons, and future approaches. In Proceedings of the First Conference of the Association for Machine Translation in the Americas, pages 193-205, Columbia, Maryland.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Crossing this chasm for Chinese-English translation appears to be a significant challenge for the current state-of-the-art systems.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
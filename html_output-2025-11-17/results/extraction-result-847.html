<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-847 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-847</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-847</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-267759607</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.12914v1.pdf" target="_blank">Large Language Model-based Human-Agent Collaboration for Complex Task Solving</a></p>
                <p><strong>Paper Abstract:</strong> In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e847.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e847.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReHAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning-based Human-Agent Collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learnable offline RL framework that trains a collaboration policy to decide at which steps an LLM-based agent should act versus when to request human intervention, balancing task performance against human intervention cost via a reward = task_reward - λ * human_cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReHAC (policy + collaboration framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MDP-based collaboration policy π_collab that selects whether subtasks are handled by agent or human; uses REINFORCE-style offline RL with Monte-Carlo reward estimation, advantage function, importance sampling clipping, and entropy regularization; policy models implemented with LLaMA-2 (LoRA fine-tuned). Agent task policy π_task is provided by a separate LLM (ChatGPT gpt-3.5-turbo-0613 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>policy model: 7B and 13B (LLaMA-2 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA, StrategyQA (multi-hop / multi-step QA)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>HotpotQA Task Reward T ≈ 46.90% (LLaMA-7B policy) and 46.78% (LLaMA-13B policy) as reported in Table 1 (task reward uses F1 score); StrategyQA reported but per-dataset numeric breakdown not provided in paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode (interactive coding with execution feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>interactive/procedural (interactive coding with execution feedback; sequential decision-making / multi-step reasoning with tool/code execution)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>InterCode Task Reward T ≈ 62.00% (LLaMA-7B policy) and 60.00% (LLaMA-13B policy); Human Intervention Rate (HIR) very low: 4.15% (7B) and 3.10% (13B); Reward R reported: 60.08 (7B) and 58.56 (13B) in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>collaboration policy (decision module), uses ReAct and Try-Again agent frameworks for task execution, policy models fine-tuned with LoRA, agent task policy provided by external LLM (ChatGPT); no extra memory/self-reflection modules were used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Offline reinforcement learning (REINFORCE-style optimization with Monte-Carlo reward estimation, advantage function, importance sampling clipping, entropy regularization); IL and prompt baselines also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method / hybrid human-in-the-loop approach</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Train a collaboration policy π_collab to allocate subtasks to human vs agent by maximizing J(π)=E[R] where R = T(s,a) - λ C(s,a); λ trades off task reward vs human intervention cost; collect human-agent trajectories (real or GPT-4-simulated) for offline RL; use LoRA to fine-tune LLaMA policy models.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>ReHAC outperforms baselines (Agent-only, Human-only, Random, Prompt, Imitation Learning) across datasets. Examples: on HotpotQA ReHAC achieves higher reward than prompt baseline; with λ=0.06 ReHAC achieved higher reward with ~30% more human interventions vs prompt baseline, while with λ=0.1 it achieved higher reward with ~20% fewer human interventions. Across datasets ReHAC yields the highest rewards; IL overfits (high train but poor test) while ReHAC generalizes better. Numeric task rewards and HIRs are reported in Table 1 (see qa and interactive fields).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper hypothesizes that LLM-based agents, while strong at static QA and reasoning traces, have limited intelligence in dynamic, interactive environments: difficulties adapting to environment changes, handling multi-step tool interaction/execution feedback, ambiguity, and hallucinations; lack of memory/self-reflection/tool-integration in basic agent architectures contributes to worse interactive/procedural performance compared to QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-based Human-Agent Collaboration for Complex Task Solving', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e847.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e847.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 (policy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (policy model used for collaboration decision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open foundation LLM (LLaMA-2) used as the policy model π_collab in ReHAC experiments, fine-tuned via LoRA to decide when to allocate subtasks to human vs agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA-2 (7B) / LLaMA-2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM used as the collaboration policy model; fine-tuned with LoRA (r=16, α=16) via offline RL objectives in the ReHAC framework.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (used as task environment for policy evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>With ReHAC collaboration policy: Task Reward T ≈ 46.90% (7B) and 46.78% (13B) on HotpotQA (F1); Reward R ≈ 31.38 (7B) and 32.22 (13B).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>interactive coding (tool/code execution loop)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>With ReHAC policy controlling collaboration: Task Reward T ≈ 62.00% (7B) and 60.00% (13B) on InterCode; HIR 4.15% (7B) and 3.10% (13B); Reward R ≈ 60.08 (7B) and 58.56 (13B).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>policy-only transformer LLM (no extra memory or reflection modules in experiments); fine-tuned with LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>LoRA fine-tuning of LLaMA policy via offline RL (REINFORCE-style) on collected human-agent/GPT-4-agent trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (policy fine-tuning) as part of ReHAC</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use LLaMA-2 as the learnable decision-maker; fine-tune via offline RL using collected human-agent trajectories; LoRA used to keep compute low.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Increasing policy model size from 7B to 13B provided slight improvements (e.g., small changes in Reward R and HIR), but authors report LLaMA-7B performs competitively with 13B, indicating limited scaling benefit for this policy task.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Policy model capacity helps, but core gap arises from agent/environment interaction complexity rather than just policy size; lacking advanced agent features (memory, self-reflection) constrains interactive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-based Human-Agent Collaboration for Complex Task Solving', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e847.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e847.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613) as agent policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's chat-based LLM (gpt-3.5-turbo-0613) used in experiments as the agent task policy π_task to execute actions within ReAct or Try-Again frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-oriented transformer LLM used as the agent that selects task actions (e.g., Search, Lookup, Finish) and interacts with environment APIs (Wikipedia API or code interpreter) under ReAct/Try-Again frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA, StrategyQA (used as the agent in ReAct-based QA experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode (used as agent in Try-Again experiments interacting with code interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / interactive coding (tool use: code execution)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chat-oriented LLM that issues actions and reasoning traces; used inside ReAct/Try-Again agent frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>baseline agent (no new architectural intervention applied to ChatGPT itself)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as the fixed agent policy in experiments; ReHAC learns when to route subtasks to this agent vs humans.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Agent-only baseline underperforms human-agent collaboration methods; combining ChatGPT agent with learned human intervention (ReHAC) improved overall task rewards compared to agent-only.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Agents like ChatGPT are competent at reasoning traces and static QA but struggle with interactive dynamics, ambiguous queries, and sequential tool-use without human guidance; this motivates human interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-based Human-Agent Collaboration for Complex Task Solving', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e847.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e847.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (simulated human)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used to simulate human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (gpt-4-0613) was used to simulate humans in collected trajectories to cheaply generate high-quality, consistent human-agent collaboration data for offline RL training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4 (gpt-4-0613) as simulated human</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity LLM used to produce human-like interventions/answers in curated trajectories; yields lower variance and more consistent collaboration data than real annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA, StrategyQA (used to generate simulated human responses)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode (simulated human trajectories also used)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>simulation of human interventions in interactive coding and QA environments</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>none (used as data generator / simulated human actor)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>data-collection strategy (simulation substitution)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use GPT-4 to simulate human annotators when collecting human-agent collaboration trajectories; remove duplicates and use consistent GPT-4 outputs to reduce variance during offline RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Policies (ReHAC GPT-4) trained on GPT-4-agent collaboration data performed better and more stably in evaluation than policies trained on real human data in some settings (e.g., ReHAC GPT-4 outperformed ReHAC Human at λ=0.06 and 0.08), likely because GPT-4 data has lower variance and higher consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-based Human-Agent Collaboration for Complex Task Solving', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e847.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e847.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning + acting agent framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that interleaves chain-of-thought style reasoning with environment actions, allowing LLMs to incorporate observations into their reasoning trace and dynamically choose next actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct framework</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architectural pattern combining reasoning traces (chain-of-thought) with explicit action outputs to interact with tools/APIs (e.g., search/lookup) and consume observations for subsequent steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (used as the environment for ReAct-based QA experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning / web navigation (when applied)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought + action selection + environment feedback</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>baseline architectural framework</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as baseline agent architecture; ReHAC's collaboration policy decides whether actions in this framework are executed by agent or human.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>ReAct-based agents benefit from human intervention determined by ReHAC; however, paper notes current ReAct-based agents still struggle on some complex tasks, motivating human-agent collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>While ReAct incorporates environment feedback, limitations remain in dynamic adaptability, tool grounding, and higher-level planning that reduce interactive performance compared to static QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-based Human-Agent Collaboration for Complex Task Solving', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e847.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e847.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Try-Again (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Try-Again (interactive coding/execution framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive agent framework used for code-interpreter tasks where agents can execute code, observe outputs, and revise attempts iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Try-Again framework (as in Yang et al., 2023 / InterCode)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Looped interaction with a code interpreter that returns execution outputs as observations; agents can retry and modify code based on feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode (interactive coding benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>interactive coding with execution feedback (sequential decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>execution-feedback loop (code run -> observation -> retry), iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>baseline framework for interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as the interaction environment for InterCode experiments; ReHAC decides when to route code-writing steps to humans vs the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Using ReHAC to introduce human interventions in Try-Again/InterCode environment yields improved task rewards while keeping human intervention minimal (low HIR).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Interactive coding requires reliable tool-execution grounding and iterative correction; LLM agents without extensive search/reflection struggle, producing a gap versus simpler QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-based Human-Agent Collaboration for Complex Task Solving', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e847.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e847.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA_vs_Interactive_Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed performance gap between QA (static) tasks and interactive/procedural tasks for LLM-based agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents and discusses that LLMs and LLM-based agents often show strong performance on static QA and reasoning benchmarks but underperform on dynamic interactive/procedural tasks (tool use, planning, multi-step execution), motivating human-in-the-loop interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLM-based agents (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General observation across experiments and related work that agents excel at understanding/planning in static contexts but struggle with dynamic environment adaptation, ambiguity resolution, execution feedback and long action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA, StrategyQA (examples of QA/knowledge tasks where agents are applied)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode (example of interactive/procedural task showing different behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / interactive coding / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid human-in-the-loop / RL-based policy for intervention timing</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Paper evaluates prompt-based heuristics, imitation learning, and proposes RL-based ReHAC to mitigate the gap by learning when to route subtasks to humans; also discusses other possible architectural interventions (memory, reflection, tools) as future directions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>ReHAC reduces performance gap by strategically introducing humans: human-agent collaboration methods outperform agent-only and human-only baselines; numeric examples in paper show improved task rewards when human interventions are allocated dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Attributed to agents' limited ability to adapt to dynamic environments, ambiguity resolution, hallucinations, and missing modules (self-reflection, memory, robust tool grounding), making end-to-end agent automation for complex real-world tasks currently infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-based Human-Agent Collaboration for Complex Task Solving', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>InterCode: Standardizing and benchmarking interactive coding with execution feedback. <em>(Rating: 2)</em></li>
                <li>Try Again (Yang et al., 2023) (interactive coding / agent retry framework) <em>(Rating: 1)</em></li>
                <li>Llama 2: Open foundation and fine-tuned chat models. <em>(Rating: 1)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
                <li>Reinforcement learning / offline RL for language agents (e.g., Retroformer, Reflexion, Expel) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-847",
    "paper_id": "paper-267759607",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ReHAC",
            "name_full": "Reinforcement Learning-based Human-Agent Collaboration",
            "brief_description": "A learnable offline RL framework that trains a collaboration policy to decide at which steps an LLM-based agent should act versus when to request human intervention, balancing task performance against human intervention cost via a reward = task_reward - λ * human_cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ReHAC (policy + collaboration framework)",
            "model_description": "MDP-based collaboration policy π_collab that selects whether subtasks are handled by agent or human; uses REINFORCE-style offline RL with Monte-Carlo reward estimation, advantage function, importance sampling clipping, and entropy regularization; policy models implemented with LLaMA-2 (LoRA fine-tuned). Agent task policy π_task is provided by a separate LLM (ChatGPT gpt-3.5-turbo-0613 in experiments).",
            "model_size": "policy model: 7B and 13B (LLaMA-2 variants)",
            "qa_task_name": "HotpotQA, StrategyQA (multi-hop / multi-step QA)",
            "qa_performance": "HotpotQA Task Reward T ≈ 46.90% (LLaMA-7B policy) and 46.78% (LLaMA-13B policy) as reported in Table 1 (task reward uses F1 score); StrategyQA reported but per-dataset numeric breakdown not provided in paper text.",
            "interactive_task_name": "InterCode (interactive coding with execution feedback)",
            "interactive_task_type": "interactive/procedural (interactive coding with execution feedback; sequential decision-making / multi-step reasoning with tool/code execution)",
            "interactive_performance": "InterCode Task Reward T ≈ 62.00% (LLaMA-7B policy) and 60.00% (LLaMA-13B policy); Human Intervention Rate (HIR) very low: 4.15% (7B) and 3.10% (13B); Reward R reported: 60.08 (7B) and 58.56 (13B) in Table 1.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "collaboration policy (decision module), uses ReAct and Try-Again agent frameworks for task execution, policy models fine-tuned with LoRA, agent task policy provided by external LLM (ChatGPT); no extra memory/self-reflection modules were used in experiments.",
            "training_method": "Offline reinforcement learning (REINFORCE-style optimization with Monte-Carlo reward estimation, advantage function, importance sampling clipping, entropy regularization); IL and prompt baselines also evaluated.",
            "intervention_type": "training method / hybrid human-in-the-loop approach",
            "intervention_description": "Train a collaboration policy π_collab to allocate subtasks to human vs agent by maximizing J(π)=E[R] where R = T(s,a) - λ C(s,a); λ trades off task reward vs human intervention cost; collect human-agent trajectories (real or GPT-4-simulated) for offline RL; use LoRA to fine-tune LLaMA policy models.",
            "intervention_effect": "ReHAC outperforms baselines (Agent-only, Human-only, Random, Prompt, Imitation Learning) across datasets. Examples: on HotpotQA ReHAC achieves higher reward than prompt baseline; with λ=0.06 ReHAC achieved higher reward with ~30% more human interventions vs prompt baseline, while with λ=0.1 it achieved higher reward with ~20% fewer human interventions. Across datasets ReHAC yields the highest rewards; IL overfits (high train but poor test) while ReHAC generalizes better. Numeric task rewards and HIRs are reported in Table 1 (see qa and interactive fields).",
            "hypothesized_cause_of_gap": "Paper hypothesizes that LLM-based agents, while strong at static QA and reasoning traces, have limited intelligence in dynamic, interactive environments: difficulties adapting to environment changes, handling multi-step tool interaction/execution feedback, ambiguity, and hallucinations; lack of memory/self-reflection/tool-integration in basic agent architectures contributes to worse interactive/procedural performance compared to QA.",
            "uuid": "e847.0",
            "source_info": {
                "paper_title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2 (policy)",
            "name_full": "LLaMA-2 (policy model used for collaboration decision)",
            "brief_description": "Open foundation LLM (LLaMA-2) used as the policy model π_collab in ReHAC experiments, fine-tuned via LoRA to decide when to allocate subtasks to human vs agent.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA-2 (7B) / LLaMA-2 (13B)",
            "model_description": "Transformer-based LLM used as the collaboration policy model; fine-tuned with LoRA (r=16, α=16) via offline RL objectives in the ReHAC framework.",
            "model_size": "7B and 13B",
            "qa_task_name": "HotpotQA (used as task environment for policy evaluation)",
            "qa_performance": "With ReHAC collaboration policy: Task Reward T ≈ 46.90% (7B) and 46.78% (13B) on HotpotQA (F1); Reward R ≈ 31.38 (7B) and 32.22 (13B).",
            "interactive_task_name": "InterCode",
            "interactive_task_type": "interactive coding (tool/code execution loop)",
            "interactive_performance": "With ReHAC policy controlling collaboration: Task Reward T ≈ 62.00% (7B) and 60.00% (13B) on InterCode; HIR 4.15% (7B) and 3.10% (13B); Reward R ≈ 60.08 (7B) and 58.56 (13B).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "policy-only transformer LLM (no extra memory or reflection modules in experiments); fine-tuned with LoRA.",
            "training_method": "LoRA fine-tuning of LLaMA policy via offline RL (REINFORCE-style) on collected human-agent/GPT-4-agent trajectories.",
            "intervention_type": "training method (policy fine-tuning) as part of ReHAC",
            "intervention_description": "Use LLaMA-2 as the learnable decision-maker; fine-tune via offline RL using collected human-agent trajectories; LoRA used to keep compute low.",
            "intervention_effect": "Increasing policy model size from 7B to 13B provided slight improvements (e.g., small changes in Reward R and HIR), but authors report LLaMA-7B performs competitively with 13B, indicating limited scaling benefit for this policy task.",
            "hypothesized_cause_of_gap": "Policy model capacity helps, but core gap arises from agent/environment interaction complexity rather than just policy size; lacking advanced agent features (memory, self-reflection) constrains interactive performance.",
            "uuid": "e847.1",
            "source_info": {
                "paper_title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChatGPT (agent)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0613) as agent policy",
            "brief_description": "OpenAI's chat-based LLM (gpt-3.5-turbo-0613) used in experiments as the agent task policy π_task to execute actions within ReAct or Try-Again frameworks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatGPT (gpt-3.5-turbo-0613)",
            "model_description": "Chat-oriented transformer LLM used as the agent that selects task actions (e.g., Search, Lookup, Finish) and interacts with environment APIs (Wikipedia API or code interpreter) under ReAct/Try-Again frameworks.",
            "model_size": null,
            "qa_task_name": "HotpotQA, StrategyQA (used as the agent in ReAct-based QA experiments)",
            "qa_performance": null,
            "interactive_task_name": "InterCode (used as agent in Try-Again experiments interacting with code interpreter)",
            "interactive_task_type": "multi-step reasoning / interactive coding (tool use: code execution)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "chat-oriented LLM that issues actions and reasoning traces; used inside ReAct/Try-Again agent frameworks.",
            "training_method": null,
            "intervention_type": "baseline agent (no new architectural intervention applied to ChatGPT itself)",
            "intervention_description": "Used as the fixed agent policy in experiments; ReHAC learns when to route subtasks to this agent vs humans.",
            "intervention_effect": "Agent-only baseline underperforms human-agent collaboration methods; combining ChatGPT agent with learned human intervention (ReHAC) improved overall task rewards compared to agent-only.",
            "hypothesized_cause_of_gap": "Agents like ChatGPT are competent at reasoning traces and static QA but struggle with interactive dynamics, ambiguous queries, and sequential tool-use without human guidance; this motivates human interventions.",
            "uuid": "e847.2",
            "source_info": {
                "paper_title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (simulated human)",
            "name_full": "GPT-4 used to simulate human annotators",
            "brief_description": "GPT-4 (gpt-4-0613) was used to simulate humans in collected trajectories to cheaply generate high-quality, consistent human-agent collaboration data for offline RL training and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4 (gpt-4-0613) as simulated human",
            "model_description": "High-capacity LLM used to produce human-like interventions/answers in curated trajectories; yields lower variance and more consistent collaboration data than real annotators.",
            "model_size": null,
            "qa_task_name": "HotpotQA, StrategyQA (used to generate simulated human responses)",
            "qa_performance": null,
            "interactive_task_name": "InterCode (simulated human trajectories also used)",
            "interactive_task_type": "simulation of human interventions in interactive coding and QA environments",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": null,
            "architectural_features": "none (used as data generator / simulated human actor)",
            "training_method": null,
            "intervention_type": "data-collection strategy (simulation substitution)",
            "intervention_description": "Use GPT-4 to simulate human annotators when collecting human-agent collaboration trajectories; remove duplicates and use consistent GPT-4 outputs to reduce variance during offline RL training.",
            "intervention_effect": "Policies (ReHAC GPT-4) trained on GPT-4-agent collaboration data performed better and more stably in evaluation than policies trained on real human data in some settings (e.g., ReHAC GPT-4 outperformed ReHAC Human at λ=0.06 and 0.08), likely because GPT-4 data has lower variance and higher consistency.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e847.3",
            "source_info": {
                "paper_title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReAct (framework)",
            "name_full": "ReAct (reasoning + acting agent framework)",
            "brief_description": "A framework that interleaves chain-of-thought style reasoning with environment actions, allowing LLMs to incorporate observations into their reasoning trace and dynamically choose next actions.",
            "citation_title": "React: Synergizing reasoning and acting in language models.",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct framework",
            "model_description": "Architectural pattern combining reasoning traces (chain-of-thought) with explicit action outputs to interact with tools/APIs (e.g., search/lookup) and consume observations for subsequent steps.",
            "model_size": null,
            "qa_task_name": "HotpotQA (used as the environment for ReAct-based QA experiments)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "tool use / multi-step reasoning / web navigation (when applied)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "chain-of-thought + action selection + environment feedback",
            "training_method": null,
            "intervention_type": "baseline architectural framework",
            "intervention_description": "Used as baseline agent architecture; ReHAC's collaboration policy decides whether actions in this framework are executed by agent or human.",
            "intervention_effect": "ReAct-based agents benefit from human intervention determined by ReHAC; however, paper notes current ReAct-based agents still struggle on some complex tasks, motivating human-agent collaboration.",
            "hypothesized_cause_of_gap": "While ReAct incorporates environment feedback, limitations remain in dynamic adaptability, tool grounding, and higher-level planning that reduce interactive performance compared to static QA.",
            "uuid": "e847.4",
            "source_info": {
                "paper_title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Try-Again (framework)",
            "name_full": "Try-Again (interactive coding/execution framework)",
            "brief_description": "An interactive agent framework used for code-interpreter tasks where agents can execute code, observe outputs, and revise attempts iteratively.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Try-Again framework (as in Yang et al., 2023 / InterCode)",
            "model_description": "Looped interaction with a code interpreter that returns execution outputs as observations; agents can retry and modify code based on feedback.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode (interactive coding benchmark)",
            "interactive_task_type": "interactive coding with execution feedback (sequential decision-making)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "execution-feedback loop (code run -&gt; observation -&gt; retry), iterative refinement",
            "training_method": null,
            "intervention_type": "baseline framework for interactive tasks",
            "intervention_description": "Used as the interaction environment for InterCode experiments; ReHAC decides when to route code-writing steps to humans vs the agent.",
            "intervention_effect": "Using ReHAC to introduce human interventions in Try-Again/InterCode environment yields improved task rewards while keeping human intervention minimal (low HIR).",
            "hypothesized_cause_of_gap": "Interactive coding requires reliable tool-execution grounding and iterative correction; LLM agents without extensive search/reflection struggle, producing a gap versus simpler QA tasks.",
            "uuid": "e847.5",
            "source_info": {
                "paper_title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "QA_vs_Interactive_Gap",
            "name_full": "Observed performance gap between QA (static) tasks and interactive/procedural tasks for LLM-based agents",
            "brief_description": "The paper documents and discusses that LLMs and LLM-based agents often show strong performance on static QA and reasoning benchmarks but underperform on dynamic interactive/procedural tasks (tool use, planning, multi-step execution), motivating human-in-the-loop interventions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "LLM-based agents (general)",
            "model_description": "General observation across experiments and related work that agents excel at understanding/planning in static contexts but struggle with dynamic environment adaptation, ambiguity resolution, execution feedback and long action sequences.",
            "model_size": null,
            "qa_task_name": "HotpotQA, StrategyQA (examples of QA/knowledge tasks where agents are applied)",
            "qa_performance": null,
            "interactive_task_name": "InterCode (example of interactive/procedural task showing different behavior)",
            "interactive_task_type": "tool use / interactive coding / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "hybrid human-in-the-loop / RL-based policy for intervention timing",
            "intervention_description": "Paper evaluates prompt-based heuristics, imitation learning, and proposes RL-based ReHAC to mitigate the gap by learning when to route subtasks to humans; also discusses other possible architectural interventions (memory, reflection, tools) as future directions.",
            "intervention_effect": "ReHAC reduces performance gap by strategically introducing humans: human-agent collaboration methods outperform agent-only and human-only baselines; numeric examples in paper show improved task rewards when human interventions are allocated dynamically.",
            "hypothesized_cause_of_gap": "Attributed to agents' limited ability to adapt to dynamic environments, ambiguity resolution, hallucinations, and missing modules (self-reflection, memory, robust tool grounding), making end-to-end agent automation for complex real-world tasks currently infeasible.",
            "uuid": "e847.6",
            "source_info": {
                "paper_title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "InterCode: Standardizing and benchmarking interactive coding with execution feedback.",
            "rating": 2,
            "sanitized_title": "intercode_standardizing_and_benchmarking_interactive_coding_with_execution_feedback"
        },
        {
            "paper_title": "Try Again (Yang et al., 2023) (interactive coding / agent retry framework)",
            "rating": 1,
            "sanitized_title": "try_again_yang_et_al_2023_interactive_coding_agent_retry_framework"
        },
        {
            "paper_title": "Llama 2: Open foundation and fine-tuned chat models.",
            "rating": 1,
            "sanitized_title": "llama_2_open_foundation_and_finetuned_chat_models"
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Reinforcement learning / offline RL for language agents (e.g., Retroformer, Reflexion, Expel)",
            "rating": 1,
            "sanitized_title": "reinforcement_learning_offline_rl_for_language_agents_eg_retroformer_reflexion_expel"
        }
    ],
    "cost": 0.017638499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Model-based Human-Agent Collaboration for Complex Task Solving</p>
<p>Xueyang Feng xueyangfeng@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods
BeijingChina</p>
<p>Zhi-Yuan Chen 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods
BeijingChina</p>
<p>Yujia Qin 
Department of Computer Science and Technology
Tsinghua University
BeijingChina</p>
<p>Yankai Lin yankailin@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods
BeijingChina</p>
<p>† Xu Chen 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods
BeijingChina</p>
<p>Zhiyuan Liu zhiyuanc2001@ruc.edu.cn 
Department of Computer Science and Technology
Tsinghua University
BeijingChina</p>
<p>Ji-Rong Wen 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods
BeijingChina</p>
<p>Hongliang He 
Wenlin Yao 
Kaixin Ma 
Wenhao Yu 
Yong Dai 
Hongming Zhang 
Zhenzhong Lan 
Webvoyager 
Edward J Hu 
Yelong Shen 
ZeyuanPhillip Wallis 
Wenlong Huang 
Fei Xia 
Ted Xiao 
Harris Chan 
Jacky Liang 
Pete Florence 
Andy Zeng 
Julia Kiseleva 
Alexey Skrynnik 
Artem Zholus 
Shrestha Mohanty 
Negar Arabzadeh 
Marc- Alexandre Côté 
Mohammad Aliannejadi 
Milagro Teruel 
Ziming Li 
Maartje ter HoeveMikhail Burtsev 
Zoya Volovikova 
Aleksandr Panov 
Xiao Liu 
Hao Yu 
Hanchen Zhang 
Yifan Xu 
Xuanyu Lei 
Hanyu Lai 
Yu Gu 
Hangliang Ding 
Kaiwen Men 
Kejuan Yang 
Shudan Zhang 
Xiang Deng 
Ao- Han Zeng 
Zhengxiao Du 
Chenhui Zhang 
Sheng Shen 
Tianjun Zhang 
Yu Su 
Huan Sun 
Minlie Huang 
Yuxiao Dong 
Karthik Mahadevan 
Jonathan Chien 
Noah Brown 
Zhuo Xu 
Carolina Parada 
Andy Zeng 
Nikhil Mehta 
Patricio Figueroa Sanz 
Xin Deng 
Ahmed Hassan Awadallah 
Julia Kisel </p>
<p>Sirui Hong
Xiawu Zheng</p>
<p>Jonathan Chen
Jinlin Wang, Zili WangYuheng Cheng, Ceyao Zhang</p>
<p>Steven Ka Shing Yau
Liyang Zhou2023Zijuan Lin</p>
<p>Allen-Zhu
Shean WangYuanzhi Li, Lu Wang</p>
<p>Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Vineet Kosaraju, William Saunders2021Reiichiro Nakano, Shantanu Jain</p>
<p>Large Language Model-based Human-Agent Collaboration for Complex Task Solving
6F3EA787195581ABB38484C8D86B79C3arXiv:2106.09685.
In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest.Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs.In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential.In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC.This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process.We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment.Our validation tests confirm the model's effectiveness.The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention.Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.</p>
<p>Introduction</p>
<p>In today's increasingly complex world, humans are confronted with multifaceted tasks stemming from technical, social, and economic domains.Solving these complex tasks necessitates not only human interaction with the environment but also intricate decision-making processes.To alleviate human workload and enhance the automation of tasks in both professional and personal spheres, researchers have been actively developing advanced tools for human assistance (Zawacki-Richter et al.,  2019; Amershi et al., 2019).Recently, the emergence of Large Language Models (LLMs) such as LLaMA (Touvron et al., 2023), Gemini (Team et al., 2023) and GPT (Brown et al., 2020;Achiam et al., 2023) has marked a significant milestone.LLMs' remarkable abilities in task understanding, planning, and reasoning (Zhao et al., 2023b) have given rise to the development of LLM-based autonomous agents (Wang et al., 2023a;Yao et al., 2022;Shinn et al., 2023).These agents are designed to leverage the LLMs' capabilities to assist humans in solving complex tasks autonomously.The LLMs' capabilities enable them to effectively navigate and address the complexities encountered in real-world scenarios, thereby offering substantial support in human decision-making processes of task-solving.</p>
<p>Despite the remarkable progress of LLM-based agents, there remains a notable gap in their intelligence level to handle complex and dynamic realworld tasks with human-like proficiency.This limitation poses a significant challenge to their practicality in real-world applications, especially in sce-arXiv:2402.12914v1[cs.CL] 20 Feb 2024 narios where high accuracy is crucial, such as the legal or financial domains.Addressing this challenge extends beyond just enhancing the agents' capabilities.Incorporating human intuition and wisdom is equally vital for the effective management of these intricate and evolving tasks, offering a complementary approach to the limitations of current agent technologies.</p>
<p>In this work, we introduce the problem of LLMbased human-agent collaboration for complex task solving, aiming to augment the capabilities of LLM-based agents by integrating human intuition and wisdom.The idea is analogous to the evolution in autonomous driving technology, which has been categorized into varying levels of autonomy, ranging from no automation, conditional automation to full automation (Khan et al., 2022;SAE International, 2021).Referring to this framework, we define the different levels of human-agent collaboration, as illustrated in Figure 1.Applying this conditional automation mode to LLM-based agents offers a practical path for their deployment in real-world scenarios, acknowledging the current limitations in their cognitive capabilities.Instead of aiming for full automation, human-agent collaboration under the paradigm of conditional automation enables humans to intervene the complex task-solving when necessary, while agents handle most of the sub-tasks.This takes advantage of both human and machine intelligence.</p>
<p>While advancements in LLMs significantly enhance the capacity for mutual understanding in human-agent collaboration, several crucial challenges persist.These challenges include defining the division of labor between humans and agents, determining the granularity of tool execution, managing proactive interruption, and implementing multi-level intervention.However, our research specifically focuses on scenarios where humans directly replace agents in action.The key challenge we aim to address in human-agent collaboration lies in determining the optimal stages for human intervention in task-solving and minimizing such intervention to enhance efficiency.Some researchers have made preliminary attempts, by designing heuristic rules or specialized prompts to determine the stages at which agents should seek human assistance (Cai et al., 2023;Wu et al., 2022a;Mehta et al., 2023;Wang et al., 2023b).However, these rule-based or prompt-driven approaches are heavily reliant on specific application contexts and lack universality.They often demand a deep understanding of the domain and substantial experience from the designers, otherwise, suboptimal design choices can lead to reduced performance.Apart from that, a standardized formal framework and universally accepted paradigm for leveraging large language models (LLMs) in human-agent collaboration is still lacking.</p>
<p>To overcome the aforementioned challenges, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC, aimed at effectively combining human intervention with the automation capabilities of LLM-based agents.Our method, leveraging reinforcement learning, trains a policy model to dynamically identify the most advantageous moments for human input during the task-solving process.ReHAC is a learnable general framework that can be applied to various scenarios and does not require additional prior knowledge to design rules and prompts.For training this policy model, we collect a dataset comprising tasks collaboratively completed by humans and LLM-based agents, utilized for the offline training of the policy model.We conducted extensive experiments on three multi-step reasoning datasets: HotpotQA, StrategyQA, and InterCode, using two popular LLM-based agent frameworks, ReAct and "Try-again".The experimental results indicate that with a policy model learned from limited data, Re-HAC can effectively allocate human intervention in human-agent collaboration scenarios, thereby achieving a balance between effectiveness and efficiency.</p>
<p>Approach</p>
<p>In this section, we first formulate the problem of human-agent collaboration for complex task solving, and then introduce our proposed ReHAC method in detail.</p>
<p>Preliminary and Problem Formulation</p>
<p>Complex task-solving, inherently necessitating multi-step planning and reasoning, is conventionally formalized as a multi-step decision-making problem.Historically, complex task-solving was predominantly achieved through human-driven methods.These methods leveraged human cognitive capabilities to determine the suitable action in each step.Formally, considering a complex task q, it is traditionally solved via a sequence of actions (a 1 , a 2 , • • • a n ), with each action determined by human decision-making, expressed as:
a t = Human(q, s t ),(1)
where
s t = (a 1 , o 1 , • • • , a t−1 , o t−1 )
denotes the history information of task state at step t and o t is the observation after a t−1 is proceeded.The advent of LLMs has brought a paradigm shift in this arena.Their impressive understanding and reasoning abilities have prompted research into LLM-based agents for complex task-solving, thereby enhancing the level of automation in tasksolving.These agent-driven methods (e.g., Re-Act (Yao et al., 2022)), leverage LLM-based agents to supplant human decision-making.This shift is represented as:
a t = Agent(q, s t ).
(2)</p>
<p>This evolution of such AI-driven techniques provides a way to the automation of complex tasksolving.However, limited by the current intelligence level of LLMs, full automation based on agentdriven methods is not yet feasible in practical scenarios (Kiseleva et al., 2022;Mehta et al., 2023).Inspired by autonomous driving (Cui et al., 2024;Fu et al., 2024;Bastola et al., 2024), we propose the problem of LLM-based human-agent collaboration for complex task solving and explore the dynamics and efficacy of the human-agent collaborative methods for complex task solving.We first explore a specific form of human-agent collaboration: humans intervene in the complex tasksolving process when necessary.Formally, we need to determine whether a human or an agent makes decisions based on the actions' complexity and contextual changes, i.e., a t = Human(q, s t ) or Agent(q, s t ), (3) It is generally perceived that direct human intervention in decision-making, particularly in realworld scenarios, incurs higher costs and diminishes the system's automation level (Cai et al., 2023;Wang et al., 2023b).On the other hand, human intervention plays an important role in enhancing task performance and flexibility.Therefore, the objective of human-agent collaboration is to enhance the effectiveness of complex task-solving with minimal reliance on human decision-making.One key challenge is to determine the stages in the task-solving process where human intervention is most beneficial and effective, aligning with the goal of minimizing human involvement while maximizing task performance.</p>
<p>ReHAC</p>
<p>In this work, we propose a Reinforcement learningbased Human-Agent Collaboration method, Re-HAC.It formulates the human-agent collaboration problem as a Markov Decision Process (MDP) framework, represented by the tuple (S, A, P, R, γ), where S is the set of states, A is the set of actions, P : S × A × S is the state transition probabilities, R serves as the reward function, and γ the discount factor.</p>
<p>For each action a t ∈ A, we define it as a tuple (a collab t , a task t ), where a collab t indicates the subtask is allocated to an agent or a human, and a task t is the task action determined by agent or human:
a collab t ∼ π collab θ 1 (a collab t |s t ) a task t ∼ π task θ 2 (a task t |s t ), if a collab t = 0; π task Human (a task t |s t ), otherwise, (4) where π collab θ 1 is the collaboration policy model, π task θ 2
is the agent-based task policy model, and
π task
Human is the human task policy.</p>
<p>To balance the maximization of task performance and the cost of human intervention, we define the reward function as:
R(s, a) = T (s, a) − λC(s, a),(5)
where T (s, a) is the measure of expected task rewards received after taking action a in state s, C(s, a) is the number of human interventions in the trajectory after taking action a, λ is a hyperparameter that serves as a penalty coefficient of the number of human interventions.We utilize Monte-Carlo estimation to compute this reward function.</p>
<p>Optimization: Following the REINFORCE algorithm (Williams, 1992), we optimize the expected reward:
J (π θ ) = E π θ [R(s, a)],(6)
which aims to find an optimal policy π θ that ensures the maximization of task rewards while minimizing the human intervention costs, and
θ = [θ 1 , θ 2 ].
We utilize the advantage function to enhance the stability of optimization and important sampling for offline learning:
A(s, a) = R(s, a) − 1 |A| a ′ ∈A R(s, a ′ ) ∇ θ J (π θ ) = s a w(s, a)∇ θ log π θ (a|s)A(s, a), w(h, a) = Clip π θ (s, a) π beh (s, a) ,(7)
where A(s, a) is the advantage function, the clip function limits the importance sampling term to the interval 1 − ϵ to 1 + ϵ, and the behavior policy π beh represents the policy under of the offline training.Moreover, we have incorporated an entropy regularization term.This term encourages the policy to explore a variety of actions, thereby preventing the policy from becoming too deterministic and overfitting to the training data.Finally, the gradient of objective function is as follows:
∇ θ J (π θ ) = ∇ θ J (π θ ) + α∇ θ H(π θ (•|s)). (8)
3 Experiments  2021)) methods to train the policy model.In all experiments, we utilized ChatGPT (gpt-3.5turbo-0613) to simulate the agent policy π task θ 2 .More model implementation and data collection details can be found in Appendix A.1.</p>
<p>In this study, we set humans and agents to solve tasks under the ReAct framework (Yao et al., 2022) for question-answering datasets.The action space of a task is {Search[entity], Lookup[keyword], and Finish[answer]}.All actions are supported by a Wikipedia web API, following the original Re-Act implementation.For the InterCode dataset, we solve tasks under the "Try Again" framework (Yang et al., 2023).Here, agents and humans interact with the code interpreter through the action a t and receive execution outputs from the code interpreter as observations o t .The task-solving process ends if any one of the following conditions is satisfied: 1) the Finish[answer] action is executed actively by π task θ 2 for the question answering dataset.2) the task reward T (s, a) = 1 for Inter-Code dataset.3) the number of actions t exceeds a pre-defined step threshold.</p>
<p>Reward Calculation For all datasets, the final reward is computed as equation ( 5).For question answering datasets, we choose the F1 score as the task reward T (s, a).For the InterCode dataset, following Yang et al. (2023), we use Intersection over Union as the task reward T (s, a).</p>
<p>Baselines We compare our method ReHAC with the following baselines: 1) Agent-only which carries out all actions by agents.2) Human-only, which conducts all actions by humans.3) Random, which selects an agent or human randomly at a probability of 50% to perform each action.4) Prompt, which prompts the agent to actively decide whether the action is executed by itself or a human.5) Imitation Learning (IL), which trains the policy model to decide whether the action should be finished by an agent or human by the IL method.More details about baselines can be found in the Appendix A.2.</p>
<p>Overall Results</p>
<p>In this section, we verify the effectiveness of our proposed ReHAC method for human-agent collaboration on the HotpotQA dataset.</p>
<p>Human-Agent Experiments Figure 2(a) shows the evaluation results of human-agent collaboration on the HotpotQA dataset.From the figure, we can observe that all human-agent collaboration methods outperform Human-only and Agentonly methods.This underscores the importance of collaborating human and agent in complex task-solving for getting higher reward.In addition, ReHAC Human achieves the best performance compared with prompt-based and random-based method in achieving higher rewards.Specifically, when λ = 0.06, ReHAC achieves a higher reward with approximately 30% more human interventions  compared with the prompt-based baseline; when λ = 0.1, it also achieves a reward improvement with about 20% less human interventions.This indicates that our ReHAC method can dynamically introduce human intervention in real human-agent collaboration scenarios, thereby achieving a balance between effectiveness and efficiency.</p>
<p>Focusing on ReHAC Human , we observe that as λ increases, the human intervention rate 1 (HIR) of ReHAC Human gradually decreases.This trend suggests that a higher human penalty coefficient elevates our policy model's "threshold" for assigning actions to humans.Simultaneously, the decrease of the HIR correspondingly results in a deterioration of human-agent interaction performance.</p>
<p>Human Simulation Due to the high cost of hiring annotators to label real human-agent collaboration data, it is costly for us to collect human-agent collaboration data on more datasets and, as a result, validate the efficacy of our method in broader scenarios.We instead use GPT-4 (gpt-4-0613) to build a simulation environment and make it collaborate with agents to solve tasks.This setup enables us to collect more "human-agent" collaboration data at a reasonable cost.</p>
<p>To verify the feasibility of using GPT-4 to simulate humans to collect "human-agent" collaboration 1 The formula for calculating the human intervention rate is in Appendix A.3. data, we learn ReHAC on the HotpotQA GPT-4agent collaboration data, named as ReHAC GPT-4 and test its performance in the real human-agent collaboration environment.From Figure 2(a), we can see that ReHAC GPT-4 exhibits better performance compared to ReHAC Human in human-agent collaboration when λ = 0.06 and 0.08.We suppose that this is possibly attributed to individual differences among humans, leading to a distribution variance in the human-agent collaboration data, while GPT-4-agent collaboration data exhibits higher consistency and lower variance.This makes ReHAC GPT-4 learn the collaboration signal more easily, and thus is more stable and performs better.</p>
<p>To further reduce costs and observe the reward variation of ReHAC during the training process, we use GPT-4 to simulate humans in the evaluation phase.Figure 2(b) shows the evaluation results when using GPT-4 to simulate humans for collaboration.Comparing the results in Figure 2(a) and (b), we notice that the relative performance of various methods is generally consistent in both human-agent collaboration and GPT-4-agent collaboration.For example, the rewards R of ReHAC consistently surpass those of the Prompt method, and both ReHAC and the Prompt method outperform the Random method.This demonstrates the viability of using GPT-4 to simulate humans for evaluation.</p>
<p>Considering feasibility and cost-effectiveness, we will continue to use GPT-4 as a substitute for human participants in all subsequent extension experiments.</p>
<p>Learning Curves Figure 3 shows the learning curves during the training process.The curves are obtained by assessing the policy model's rewards on the trainset and testset every 5 steps.From the figure, we can observe that (1) the rewards of Re-HAC gradually increase during the training process, indicating that ReHAC can progressively identify suitable points to introduce human interventions.</p>
<p>(2) While the IL method achieves high rewards on the trainset, it performs poorly on the testset.This suggests our RL-based learning method learns a more generalized human-agent collaboration strategy compared to directly learning the optimal strategy with the imitation learning method.</p>
<p>Performance on Different Dataset</p>
<p>In this part, we train and test ReHAC method on StrategyQA, and InterCode datasets in the GPT-4 simulation environment.For all experiments, we fix the parameter λ = 0.08.Throughout the training phase, we evaluate the policy model's rewards on the trainset and testset every 5 steps.Experimental results are shown in Figure 4. From the figure, we observe that: (1) Our proposed ReHAC method achieves higher reward scores compared to other baselines on all datasets.This validates the effectiveness of our approach across a broader range of datasets.(2) Both ReHAC and IL exhibit low variance and stability during the training process.Although our method and the IL method show a continuous reward increase during the training process, ReHAC can ultimately achieve higher rewards compared to the IL method.This indicates that our reinforcement learning-based method can provide more valuable guidance to the policy model
π collab θ 1
, enabling it to determine when to introduce human interventions and consequently achieving higher rewards.</p>
<p>In summary, our method demonstrates superior performance across all datasets, affirming its ability to achieve an optimal balance between efficiency and effectiveness.</p>
<p>Scaling Analysis of Policy Model</p>
<p>In this section, we analyze the impact of the model scale on the performance of the policy model.Here, we set λ = 0.08 and conduct experiments on Hot-potQA and InterCode datasets.As shown in Table 1, the LLaMA-7B model performs competitively with the LLaMA-13B model.This suggests that the Llama2-7B model is already proficient in handling the human-agent collaboration task, and the benefit of increasing the size of the model is slight.</p>
<p>We will explore smaller policy model size in the future.</p>
<p>Case Study</p>
<p>In this part, we give a specific case on the Hot-potQA dataset, as illustrated in Figure 5, to show how human-agent collaboration helps the complex task-solving.The task is to determine which historical event, the Seven Days Battles or the Battle of Manila, occurred first.When given the entire problem, the agent accurately determines the date of the Seven Days Battles but encounters multiple entries for the Battle of Manila, resulting in ambiguity.Consequently, the agent deems the query ambiguous and opts to respond with "unknown".On the contrary, our ReHAC method requires the human intervention in this situation.Upon examining the related entries, the human observes that all mentioned dates for the Battle of Manila occurs after to July 1, 1862.Based on this insight, he conjectures that the Seven Days Battles occurred first.Although this conjecture is not absolutely certain, it represents the most likely decision based on the available information.Thus, our ReHAC method returns a correct response "Seven Days Battles".This case also highlights an insightful aspect of our research into LLM-based agents: Researchers are committed to eliminating hallucinations in large language models (LLMs) to create rigorous and accurate intelligent agents.However, many tasks require imagination and intuition, making it crucial to integrate human creative thinking through human-agent collaboration at this juncture.</p>
<p>Discussion</p>
<p>In this paper, we conduct a preliminary exploration of key aspects of human-agent collaboration, aim- ing to lay the groundwork for further research in this field.Despite progress, unresolved problems and potential challenges persist.We propose three extended research directions to enhance the effectiveness, safety, and intelligence of human-agent collaboration:</p>
<p>Multi-level Human-Agent Collaboration Our focus is on modes where humans directly replace agents in action.However, given the distinct advantages of both humans and agents, we see a need to explore more complex collaboration levels.This includes human involvement in feedback, decision modification, and planning.</p>
<p>Development Stages of LLM-based Agents</p>
<p>Inspired by the L1 to L5 grading model in autonomous driving, we suggest adapting this framework for LLM-based human-agent collaboration.</p>
<p>It offers a clear structure to assess the current development stage of human-agent technologies and guide future research.While LLM agents have not reached high or full automation, this framework is crucial for identifying key technologies and challenges.However, our research indicates a significant gap before LLM agents achieve full automation (L5).Effective human-agent collaboration could be a bridge towards this goal.</p>
<p>Safety and Super Alignment</p>
<p>Safety is paramount in human-agent collaboration, particularly in high-risk scenarios.It's vital to explore methods to secure the collaboration process and mitigate risks.Moreover, with the potential of LLM-based agents evolving into superintelligence, effective collaboration becomes increasingly crucial.This collaboration is key, as it not only allows When the agent completes the task, the third step cannot be answered due to the ambiguity of the problem identified; using our method, the first two simple retrieval tasks are assigned to the agent to complete, while the third step is assigned to humans.Humans can complete the correct answer through bold speculation humans to guide ethical and safety decisions but also ensures the alignment of LLM-based agents' objectives with human interests.</p>
<p>Related Work</p>
<p>LLM-based Agent Recent advancements in LLMs have demonstrated their capabilities in reasoning (Wei et al., 2022;Kojima et al., 2022;Hao et al., 2023;Luong et al., 2024;Yue et al., 2023) and task planning (Yao et al., 2023a;Kong et al., 2023;Shen et al., 2023;Yao et al., 2023b;Deng et al., 2023).These capabilities lay the foundation for the development of LLM-based agents (Shridhar et al., 2021;Yang et al., 2023;Liu et al., 2023b;Song et al., 2023;Wang et al., 2023a).LLM-based agents, which can interact with the environment and select subsequent actions based on environment feedback, have been applied in many domains, including web navigation (Nakano et al., 2021;Cheng et al., 2024;He et al., 2024), software engineering (Qian et al., 2023;Hong et al., 2023), and robotics (Wang et al., 2024;Mahadevan et al., 2024).By synergizing the reasoning and action abilities of LLMs, ReAct (Yao et al., 2022) incorporates environment feedback into reasoning traces and determines the next step action dynamically.Subsequent research focuses on integrating code (Wang et al., 2023b;Roziere et al., 2023;Xu et al., 2023), memory modules (Rana et al., 2023;Park et al., 2023), experience reflection (Shinn et al., 2023;Zhao et al., 2023a), and tools into LLM-based agents (Liu et al., 2023a;Patil et al., 2023;Qin et al., 2023), thereby augmenting their abilities in solving complex problems.However, current LLM-based agents still perform poorly on some complex tasks.This work aims to introduce human interventions and enable humans and agents to collaboratively address complex tasks, thereby achieving improved task performance.</p>
<p>Human-Agent Collaboration In Human-Agent Collaboration (HAC), traditional research has been centered on improving the naturalness and efficiency of human interactions with intelligent agents like robots and AI systems, effectively meeting human needs (Wang et al., 2021;Wu et al., 2022b).</p>
<p>The rise of large-scale language models (LLMbased agents) marks a significant shift in the field, underscoring the role of human feedback and reasoning in enhancing agent capabilities.This approach leverages human insights to refine performance and decision-making processes.Recent studies employ heuristic rules to direct these agents towards seeking human assistance (Cai et al., 2023;Wu et al., 2022a;Mehta et al., 2023).Furthermore, there is an increasing emphasis on developing specialized prompts that motivate LLM-based agents to proactively seek human input, thus nurturing a more interactive and collaborative dynamic in these partnerships (Huang et al., 2022;Wang et al., 2023b).However, the effectiveness of these methods relies on designing high-quality rules or prompts.This is highly dependent on the designer's domain knowledge.Poor design may result in a system that cannot accurately understand or respond to complex task requirements.Our research focuses on designing a generalised and learnable method that coordinates human to effectively work with LLM-based agents in the form of direct planning.</p>
<p>Conclusion</p>
<p>In this paper, we propose the problem of large language model-based human-agent collaboration, delving into the synergy of human intuition and expertise with the computational prowess of LLMbased agents, particularly emphasizing their application in intricate decision-making tasks.We introduce a reinforcement learning-based approach for human-agent collaboration, named ReHAC.Central to ReHAC is a learnable policy model designed to pinpoint the most critical junctures for human intervention within the task-solving trajectory.Our experimental results show that ReHAC aspects better results and is more generalizable than heuristic rule-based or prompt-based approaches in humanagent collaboration tasks.We believe that ReHAC offers a practical pathway for the application of llm-agents in real-world scenarios.</p>
<p>Ethical Considerations and Limitations</p>
<p>The objective of this work focuses on human-agent collaboration, which requires humans to interact with LLM-based agents.We acknowledge that agents are likely to output some hallucinations and misleading information, and it is unclear how these contents impact humans.Additionally, all datasets used in this work are publicly available, and therefore, there are no data privacy concerns.All data collected will be used for research purposes only The limitations of this paper can be summarised in three aspects:</p>
<p>1) The current study is confined to basic LLMbased agent architectures based on the "ReAct" and "Try Again" frameworks, while more complex architectures involving self-reflection and memory capabilities are still unexplored.</p>
<p>2) Our research primarily focuses on the use of 7B and 13B scale models as policy models for task allocation.Future work will investigate the feasibility of smaller models in carrying out these tasks, aiming to maintain performance while reducing resource consumption.</p>
<p>3) This study is based on the assumption that human performance supersedes that of agents.However, as technology advances, agents might surpass human capabilities.Future research will thus shift towards exploring human-agent collaboration models in this new context.Emphasis will be placed on assessing how human-agent collaboration can ensure the safety of agent decisions while aligning with human preferences.</p>
<p>A.2 Baselines Details</p>
<p>Random We randomly choose a human or an agent to conduct action a t at a probability of 50%.</p>
<p>Prompt We prompt an agent to actively decide action a t should be finished by itself or a human.The related prompts are shown in Table 5 and Table  6.Experimental results of Random and Prompt are averaged over three repeated experiments.</p>
<p>Imitation Learning We select the top 50% of actions that receive the highest rewards in each state s t as expert demonstrations.These expert demonstrations (state-action pairs) are then used to supervise the fine-tuning of the policy model.This approach allows the policy model to learn how to make decisions that get a higher return in a given state.</p>
<p>A.3 Human Intervention Rate</p>
<p>We denote the number of steps completed by humans and agents in the dataset by num h and num a , respectively.The Human Intervention Ratio (HIR) is calculated as
HIR = num h num h + num a .
HIR measures the rate of human intervention.Generally, a higher HIR indicates better task performance, but it also tends to increase costs.</p>
<p>Figure 1 :
1
Figure 1: Different Levels of Automation.(a) No automation: Tasks are entirely performed by humans.(b) Full automation: Tasks are completely executed by agents without human intervention.(c) Conditional automation: Humans are required only for specific subtasks, without continuous monitoring.</p>
<p>Figure 2 :
2
Figure 2: (a) Human-agent collaboration evaluation.(b) GPT-4-agent collaboration evaluation.The bars above the 0-axis represent the reward R, the bars below the 0-axis represent the human intervention cost λC, and the entire columns, composed of the bars above and below the 0-axis, represent the task reward T .Numbers within the bars means the human intervention rate (%).ReHAC GPT-4 and ReHAC Human represent the policy model trained on GPT-4-agent and human-agent collaboration datasets, respectively.ReHAC outperforms other baselines in human-agent collaboration scenarios.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Reward R variations of different methods during the training process on HotpotQA dataset.Here we set the human intervention penalty coefficient λ to 0.06, 0.08, and 0.1.Curves of ReHAC and IL are averaged over 15 points, with shadows indicating the variance.</p>
<p>Thought 3 :Figure 5 :
35
Figure5: Case Study.When the agent completes the task, the third step cannot be answered due to the ambiguity of the problem identified; using our method, the first two simple retrieval tasks are assigned to the agent to complete, while the third step is assigned to humans.Humans can complete the correct answer through bold speculation</p>
<p>Table 1 :
1
Experimental results regarding different model scales.HIR represents the human intervention rate.
Dataset ModelHIR (%) Task Reward T Reward RHotpotQALLaMA-7B LLaMA-13B 47.64 51.4646.90 46.7831.38 32.22InterCodeLLaMA-7B LLaMA-13B4.15 3.1062.00 60.0060.08 58.56</p>
<p>Table 2 :
2
Collected dataset details.Questions mean the number of questions we used for human-agent collaboration task.Trajectories mean the overall trajectory number we collected.(real) refers to the real human-agent collaboration dataset, and (sim) refers to the humanagent collaboration dataset collected by using GPT-4 to simulate humans.
DatasetTrainsetTestsetQuestions Trajectories QuestionsHotpotQA(real)1411937100HotpotQA(sim)1412135100StrategyQA(sim)2502420100InterCode(sim)1002071100
https://huggingface.co/meta-llama/Llama-2-7b-hf
https://huggingface.co/meta-llama/Llama-2-13b-hf
The GUI is as shown in Figure6.
A AppendixA.1 Experimental Details Model Implementation In our most experiments, we use Llama-2-7b-hf 2 downloaded from Huggingface as our policy model π collab θ 1 .We also conduct experiments based on Llama-2-13b-hf 3 model (see Section 3.3).We implement LoRA based onPEFT (Mangrulkar et al. (2022)) and set r LoRA = 16 and α LoRA = 16 for all experiments.Based onYao et al. (2022)andYang et al. (2023), we set the step threshold for HotpotQA, Strate-gyQA, and InterCode to 7, 5, and 8, respectively.All experiments are conducted on NVIDIA A100 GPUs with 40GB memory.Human-Agent Dataset For a real human-agent collaboration dataset, we employ a uniform sampling method where each action a t has a 50% probability of being assigned to either a human annotator or the ChatGPT.For each question, we sample as many interaction trajectories as possible.Specifically, for each time t, we aim to sample trajectories including a collab t = 0 and a collab t = 1.Considering the diversity of responses from different annotators, we permit repeated sampling of the same trajectory during uniform sampling, which means all a collab t of two trajectories are the same.To enhance the quality of annotation, annotators are allowed to reference GPT-4's answers.We recruit 14 annotators through social media, all of whom are graduate students with strong language and reasoning skills.They are asked to annotate a total of about 2000 trajectories in four days and they get paid about $10 an hour.They were explicitly told that the data would be used to train the model and made public and that all the labeled data was unrelated to any individual's privacy.To facilitate the annotation process, we develop a graphical user interface (GUI) 4 and provide one hour of training to annotators.The collected data details are in Table2.GPT-4-Agent DatasetFor the dataset constructed using GPT-4 to simulate human annotation, we adopt the same sampling method as humanagent dataset collection.However, due to the uniform or near-uniform distribution of GPT-4's responses, we skip duplicate paths during uniform sampling.Collected data details are listed in
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Guidelines for human-ai interaction. Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, Proceedings of the 2019 chi conference on human factors in computing systems. the 2019 chi conference on human factors in computing systems2019</p>
<p>Ashish Bastola, Julian Brinkley, Hao Wang, Abolfazl Razi, arXiv:2401.14571Driving towards inclusion: Revisiting in-vehicle interaction in autonomous vehicles. 2024arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Human-in-the-loop through chain-of-thought. Zefan Cai, Baobao Chang, Wenjuan Han, arXiv:2306.079322023arXiv preprint</p>
<p>Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, Zhiyong Wu, arXiv:2401.10935Seeclick: Harnessing gui grounding for advanced visual gui agents. 2024arXiv preprint</p>
<p>Drive as you speak: Enabling humanlike interaction with large language models in autonomous vehicles. Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024</p>
<p>Yang Deng, Wenxuan Zhang, Wai Lam, arXiv:2311.00262See-Kiong Ng, and Tat-Seng Chua. 2023. Plug-and-play policy planner for large language model powered dialogue agents. arXiv preprint</p>
<p>Drive like a human: Rethinking autonomous driving with large language models. Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, Yu Qiao, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23), UIST '23. New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf, 7th Annual Conference on Robot Learning. 2023</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Sae levels of driving automation refined for clarity and international audience. 2021SAE International</p>
<p>Hugging-GPT: Solving AI tasks with chatGPT and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 2023</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Advances in Neural Information Processing Systems. 2023</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Large language models for multi-modal human-robot interaction. Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger, arXiv:2401.151742024arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023aarXiv preprint</p>
<p>Mint: Evaluating llms in multi-turn interaction with tools and language feedback. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, 2023b</p>
<p>J Zijie, Dongjin Wang, Shenyu Choi, Diyi Xu, Yang, arXiv:2103.04044Putting humans in the natural language processing loop: A survey. 2021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 81992</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie , Proceedings of the 2022 CHI conference on human factors in computing systems. the 2022 CHI conference on human factors in computing systemsJun Cai. 2022a</p>
<p>A survey of human-in-the-loop for machine learning. Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Tianlong Ma, Liang He, Future Generation Computer Systems. 1352022b</p>
<p>Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, arXiv:2310.06830Lemur: Harmonizing natural language and code for language agents. 2023arXiv preprint</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Advances in Neural Information Processing Systems. 2023a</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, arXiv:2308.02151Retroformer: Retrospective large language agents with policy gradient optimization. 2023barXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Systematic review of research on artificial intelligence applications in higher education-where are the educators?. Olaf Zawacki-Richter, Victoria I Marín, Melissa Bond, Franziska Gouverneur, International Journal of Educational Technology in Higher Education. 1612019</p>
<p>Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, arXiv:2308.10144Expel: Llm agents are experiential learners. 2023aarXiv preprint</p>
<p>. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 2023b. A survey of large language models</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1792 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1792</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1792</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-bef63d4f7656393b7bceb2ec704e86577c286166</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bef63d4f7656393b7bceb2ec704e86577c286166" target="_blank">FILM: Following Instructions in Language with Modular Methods</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions.</p>
                <p><strong>Paper Abstract:</strong> Recent methods for embodied instruction following are typically trained end-to-end using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance (24.46 %) with a substantial (8.17 % absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance (26.49 %). Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1792.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1792.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (LP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (bidirectional encoder representations from transformers) used in Language Processing module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained language encoder (bert-base-uncased) fine-tuned to classify instruction type and extract instruction arguments; its outputs are used to produce structured subtask sequences for the embodied agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BERT-based Language Processing (LP) module</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Two separate bert-base-uncased classifiers (fine-tuned): one for instruction type classification and one (several small classifiers) for argument extraction (obj, recep, sliced, parent). The LP outputs are matched to hand-authored templates to produce a sequence of subtasks which are consumed by the downstream deterministic policy.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale natural language corpora (general language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>BERT (as cited) is pretrained on large unlabeled corpora (original BERT: BooksCorpus and English Wikipedia). In FILM the authors fine-tuned bert-base-uncased on ALFRED instruction labels for type/argument classification (fine-tuning learning rates and settings are reported in Section 5.1).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFRED (Embodied Instruction Following)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>ALFRED: a 3D household instruction-following benchmark in AI2-THOR where an agent receives a high-level natural language goal and must execute a sequence of navigation and interaction subtasks (PickUp, PutObject, OpenObject, ToggleOn/Off, Slice, etc.) in egocentric RGB/D simulated indoor environments; episodes can be long (often >70 steps) and success requires completing all subtasks within action/step limits.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions (high-level goals and optionally low-level step-by-step instructions) represented as token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation and interaction actions in ALFRED (e.g., MoveAhead, RotateLeft/Right, LookDown, PickUp/Object interactions, PutObject, Open/Close/Receptacle interactions, ToggleOn/Off).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Language (BERT) -> predicted instruction type and arguments -> match to a hand-authored type template that expands into an ordered sequence of (object, action) subtasks; these subtasks provide high-level goals to the deterministic navigation/interaction policy which executes low-level ALFRED actions (Fast Marching for path planning + deterministic local interaction heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Egocentric RGB input (processed into predicted depth and instance segmentation via fine-tuned models), depth prediction, instance segmentation; semantic top-down map built from these modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>FILM (with the BERT-based LP module as used throughout experiments) achieved state-of-the-art results on ALFRED: Tests Unseen Success Rate (SR) = 24.46% when only high-level instructions are provided; with low-level step-by-step instructions appended to LP inputs, Tests Unseen SR = 26.49% (Table 1). (These are end-task system numbers for the FILM agent that uses the BERT LP.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Rich language priors from pretrained encoder allowed efficient fine-tuning for instruction-type and argument extraction; structured template mapping reduces the need for end-to-end learning of multi-step programs; combining language-derived subtasks with explicit spatial maps enabled strong generalization to unseen scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretrained language encoder lacks inherent spatial grounding — success depends on downstream perceptual systems (depth/segmentation) and explicit template mapping; the perception gap (imperfect segmentation, small-object detection) and lack of learned exploration inside closed receptacles limited overall performance despite robust language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language models (BERT) can be effectively fine-tuned to convert natural language goals into structured subtask programs for 3D embodied agents; this linguistic transfer is useful and contributes to SOTA performance when combined with explicit semantic mapping and a deterministic action execution module, but remaining bottlenecks are primarily perceptual (segmentation/depth) and search/exploration rather than language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FILM: Following Instructions in Language with Modular Methods', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1792.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1792.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART (aux LP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART (denoising sequence-to-sequence transformer) used as alternate Language Processing module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained sequence-to-sequence language model (BART) fine-tuned to directly map high-level natural language instructions to sequences of subtasks, used as an ablation/alternative LP in FILM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BART-based Language Processing module (seq2seq subtask generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A fine-tuned BART model that takes a high-level instruction token sequence and outputs a tokenized sequence of subtasks (e.g., (PickupObject, Pan), (PutObject, Sink), ...), replacing the template-based LP; fine-tuned on ALFRED subtask-sequence annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale natural language corpora (denoising seq2seq pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>BART (as cited) is pretrained as a denoising sequence-to-sequence model on large text corpora (e.g., BooksCorpus, CC-News, OpenWebText, Stories in the original paper). In FILM the authors fine-tuned BART on ALFRED's mapping from high-level instruction to subtask sequences (using available subtask annotations) to produce an LP without template assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFRED (Embodied Instruction Following)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same ALFRED benchmark as above (3D simulated household tasks requiring navigation and interaction to satisfy multi-step natural language goals).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions (token sequences) as input and subtask-sequence tokens as outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete ALFRED navigation & interaction actions executed by deterministic policy after mapping from predicted subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct seq2seq mapping: BART generates an explicit ordered sequence of (object, action) subtasks from the instruction; the generated subtasks are consumed by the same deterministic navigation/interaction policy as in the template+BERT pipeline (Fast Marching-based navigation and deterministic interaction heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Egocentric RGB (processed to depth and instance segmentation), depth prediction, instance segmentation; same semantic mapping pipeline as the rest of FILM.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>FILM variant using the BART LP (no template assumption) achieved 18.03% SR on Valid Unseen (reported in Appendix A.9), which is a modest drop from the main FILM base method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>FILM base method (using template + BERT LP) achieved 20.10% SR on Valid Unseen (Table 2), i.e., the BART-based LP yielded lower SR in the authors' ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained seq2seq language priors enable direct program (subtask) generation from high-level instructions, removing manual template design; leverages language structure learned in pretraining to generalize across many instruction forms.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Direct seq2seq generation without template constraints produced lower end-task performance in this work—likely due to noisier or less constrained subtask outputs and the heavy reliance on accurate perception/search to execute generated programs; the mapping still needs strong perceptual grounding which pretrained language models do not provide.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained seq2seq language models (BART) can be fine-tuned to synthesize subtask programs from instructions, but in this work a simpler template+classifier approach using BERT performed better; language pretraining alone does not solve grounding and perception challenges in embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FILM: Following Instructions in Language with Modular Methods', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1792.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1792.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALFWorld (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALFWorld: Aligning text and embodied environments for interactive learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-grounded environment / benchmark (AlfWorld) designed to align textual interactive tasks with embodied environments; mentioned as related work on aligning text and embodied learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Alfworld: Aligning text and embodied environments for interactive learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>ALFWorld (text-based interactive environment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>ALFWorld provides text-based interactive tasks and was proposed to study alignment between text-only interactive learning and embodied environments; cited in FILM's related work and also referenced as a source of code/models used by other works (Shridhar et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Text-based interactive environments / text games / aligned textual task data</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in detail within FILM; ALFWorld (as cited) constructs text-based interactive tasks aligned with ALFRED-like embodied tasks to facilitate research on transferring between text and embodied domains.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Related to ALFRED / interactive embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>ALFWorld is intended to bridge text-only interactive tasks with 3D embodied tasks (ALFRED/AI2-THOR) so that agents or models trained in text settings can be evaluated or transferred to embodied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Text-game style discrete textual actions (e.g., go to, pick up, put object) represented as textual commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>When transferred to ALFRED-style environments, discrete navigation/interaction actions (as in ALFRED).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not described in detail in FILM (ALFWorld is cited as related work); generally ALFWorld studies mappings between text commands and embodied action sequences but FILM does not implement ALFWorld-based pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Not applicable for ALFWorld itself (text-only); when aligning to embodied tasks, perception modalities include egocentric RGB, depth, and instance segmentation in ALFRED/AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>ALFWorld's premise is that aligning textual interaction data with embodied tasks can provide useful priors for program synthesis and planning, though FILM does not evaluate ALFWorld pretraining directly.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>FILM highlights perception and spatial/search challenges that are not addressed by text-only pretraining; thus transferring from text-only environments may be limited by lack of perceptual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FILM cites ALFWorld as relevant prior work exploring text-to-embodied alignment, but does not itself perform text-environment pretraining from ALFWorld; FILM's experiments show that language-model pretraining (BERT/BART) helps language-to-subtask mapping, yet perceptual and exploration components remain dominant failure modes for embodied performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FILM: Following Instructions in Language with Modular Methods', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Alfworld: Aligning text and embodied environments for interactive learning <em>(Rating: 2)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension <em>(Rating: 2)</em></li>
                <li>A persistent spatial semantic representation for high-level natural language instruction execution <em>(Rating: 2)</em></li>
                <li>Episodic transformer for vision-and-language navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1792",
    "paper_id": "paper-bef63d4f7656393b7bceb2ec704e86577c286166",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "BERT (LP)",
            "name_full": "BERT (bidirectional encoder representations from transformers) used in Language Processing module",
            "brief_description": "A pretrained language encoder (bert-base-uncased) fine-tuned to classify instruction type and extract instruction arguments; its outputs are used to produce structured subtask sequences for the embodied agent.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_agent_name": "BERT-based Language Processing (LP) module",
            "model_agent_description": "Two separate bert-base-uncased classifiers (fine-tuned): one for instruction type classification and one (several small classifiers) for argument extraction (obj, recep, sliced, parent). The LP outputs are matched to hand-authored templates to produce a sequence of subtasks which are consumed by the downstream deterministic policy.",
            "pretraining_data_type": "Large-scale natural language corpora (general language pretraining)",
            "pretraining_data_details": "BERT (as cited) is pretrained on large unlabeled corpora (original BERT: BooksCorpus and English Wikipedia). In FILM the authors fine-tuned bert-base-uncased on ALFRED instruction labels for type/argument classification (fine-tuning learning rates and settings are reported in Section 5.1).",
            "embodied_task_name": "ALFRED (Embodied Instruction Following)",
            "embodied_task_description": "ALFRED: a 3D household instruction-following benchmark in AI2-THOR where an agent receives a high-level natural language goal and must execute a sequence of navigation and interaction subtasks (PickUp, PutObject, OpenObject, ToggleOn/Off, Slice, etc.) in egocentric RGB/D simulated indoor environments; episodes can be long (often &gt;70 steps) and success requires completing all subtasks within action/step limits.",
            "action_space_text": "Natural language instructions (high-level goals and optionally low-level step-by-step instructions) represented as token sequences.",
            "action_space_embodied": "Discrete navigation and interaction actions in ALFRED (e.g., MoveAhead, RotateLeft/Right, LookDown, PickUp/Object interactions, PutObject, Open/Close/Receptacle interactions, ToggleOn/Off).",
            "action_mapping_method": "Language (BERT) -&gt; predicted instruction type and arguments -&gt; match to a hand-authored type template that expands into an ordered sequence of (object, action) subtasks; these subtasks provide high-level goals to the deterministic navigation/interaction policy which executes low-level ALFRED actions (Fast Marching for path planning + deterministic local interaction heuristics).",
            "perception_requirements": "Egocentric RGB input (processed into predicted depth and instance segmentation via fine-tuned models), depth prediction, instance segmentation; semantic top-down map built from these modalities.",
            "transfer_successful": true,
            "performance_with_pretraining": "FILM (with the BERT-based LP module as used throughout experiments) achieved state-of-the-art results on ALFRED: Tests Unseen Success Rate (SR) = 24.46% when only high-level instructions are provided; with low-level step-by-step instructions appended to LP inputs, Tests Unseen SR = 26.49% (Table 1). (These are end-task system numbers for the FILM agent that uses the BERT LP.)",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Rich language priors from pretrained encoder allowed efficient fine-tuning for instruction-type and argument extraction; structured template mapping reduces the need for end-to-end learning of multi-step programs; combining language-derived subtasks with explicit spatial maps enabled strong generalization to unseen scenes.",
            "transfer_failure_factors": "Pretrained language encoder lacks inherent spatial grounding — success depends on downstream perceptual systems (depth/segmentation) and explicit template mapping; the perception gap (imperfect segmentation, small-object detection) and lack of learned exploration inside closed receptacles limited overall performance despite robust language understanding.",
            "key_findings": "Pretrained language models (BERT) can be effectively fine-tuned to convert natural language goals into structured subtask programs for 3D embodied agents; this linguistic transfer is useful and contributes to SOTA performance when combined with explicit semantic mapping and a deterministic action execution module, but remaining bottlenecks are primarily perceptual (segmentation/depth) and search/exploration rather than language understanding.",
            "uuid": "e1792.0",
            "source_info": {
                "paper_title": "FILM: Following Instructions in Language with Modular Methods",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "BART (aux LP)",
            "name_full": "BART (denoising sequence-to-sequence transformer) used as alternate Language Processing module",
            "brief_description": "A pretrained sequence-to-sequence language model (BART) fine-tuned to directly map high-level natural language instructions to sequences of subtasks, used as an ablation/alternative LP in FILM.",
            "citation_title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "mention_or_use": "use",
            "model_agent_name": "BART-based Language Processing module (seq2seq subtask generation)",
            "model_agent_description": "A fine-tuned BART model that takes a high-level instruction token sequence and outputs a tokenized sequence of subtasks (e.g., (PickupObject, Pan), (PutObject, Sink), ...), replacing the template-based LP; fine-tuned on ALFRED subtask-sequence annotations.",
            "pretraining_data_type": "Large-scale natural language corpora (denoising seq2seq pretraining)",
            "pretraining_data_details": "BART (as cited) is pretrained as a denoising sequence-to-sequence model on large text corpora (e.g., BooksCorpus, CC-News, OpenWebText, Stories in the original paper). In FILM the authors fine-tuned BART on ALFRED's mapping from high-level instruction to subtask sequences (using available subtask annotations) to produce an LP without template assumptions.",
            "embodied_task_name": "ALFRED (Embodied Instruction Following)",
            "embodied_task_description": "Same ALFRED benchmark as above (3D simulated household tasks requiring navigation and interaction to satisfy multi-step natural language goals).",
            "action_space_text": "Natural language instructions (token sequences) as input and subtask-sequence tokens as outputs.",
            "action_space_embodied": "Discrete ALFRED navigation & interaction actions executed by deterministic policy after mapping from predicted subtasks.",
            "action_mapping_method": "Direct seq2seq mapping: BART generates an explicit ordered sequence of (object, action) subtasks from the instruction; the generated subtasks are consumed by the same deterministic navigation/interaction policy as in the template+BERT pipeline (Fast Marching-based navigation and deterministic interaction heuristics).",
            "perception_requirements": "Egocentric RGB (processed to depth and instance segmentation), depth prediction, instance segmentation; same semantic mapping pipeline as the rest of FILM.",
            "transfer_successful": true,
            "performance_with_pretraining": "FILM variant using the BART LP (no template assumption) achieved 18.03% SR on Valid Unseen (reported in Appendix A.9), which is a modest drop from the main FILM base method.",
            "performance_without_pretraining": "FILM base method (using template + BERT LP) achieved 20.10% SR on Valid Unseen (Table 2), i.e., the BART-based LP yielded lower SR in the authors' ablation.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretrained seq2seq language priors enable direct program (subtask) generation from high-level instructions, removing manual template design; leverages language structure learned in pretraining to generalize across many instruction forms.",
            "transfer_failure_factors": "Direct seq2seq generation without template constraints produced lower end-task performance in this work—likely due to noisier or less constrained subtask outputs and the heavy reliance on accurate perception/search to execute generated programs; the mapping still needs strong perceptual grounding which pretrained language models do not provide.",
            "key_findings": "Pretrained seq2seq language models (BART) can be fine-tuned to synthesize subtask programs from instructions, but in this work a simpler template+classifier approach using BERT performed better; language pretraining alone does not solve grounding and perception challenges in embodied tasks.",
            "uuid": "e1792.1",
            "source_info": {
                "paper_title": "FILM: Following Instructions in Language with Modular Methods",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "ALFWorld (mention)",
            "name_full": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "brief_description": "A text-grounded environment / benchmark (AlfWorld) designed to align textual interactive tasks with embodied environments; mentioned as related work on aligning text and embodied learning.",
            "citation_title": "Alfworld: Aligning text and embodied environments for interactive learning",
            "mention_or_use": "mention",
            "model_agent_name": "ALFWorld (text-based interactive environment)",
            "model_agent_description": "ALFWorld provides text-based interactive tasks and was proposed to study alignment between text-only interactive learning and embodied environments; cited in FILM's related work and also referenced as a source of code/models used by other works (Shridhar et al., 2021).",
            "pretraining_data_type": "Text-based interactive environments / text games / aligned textual task data",
            "pretraining_data_details": "Not specified in detail within FILM; ALFWorld (as cited) constructs text-based interactive tasks aligned with ALFRED-like embodied tasks to facilitate research on transferring between text and embodied domains.",
            "embodied_task_name": "Related to ALFRED / interactive embodied tasks",
            "embodied_task_description": "ALFWorld is intended to bridge text-only interactive tasks with 3D embodied tasks (ALFRED/AI2-THOR) so that agents or models trained in text settings can be evaluated or transferred to embodied settings.",
            "action_space_text": "Text-game style discrete textual actions (e.g., go to, pick up, put object) represented as textual commands.",
            "action_space_embodied": "When transferred to ALFRED-style environments, discrete navigation/interaction actions (as in ALFRED).",
            "action_mapping_method": "Not described in detail in FILM (ALFWorld is cited as related work); generally ALFWorld studies mappings between text commands and embodied action sequences but FILM does not implement ALFWorld-based pretraining.",
            "perception_requirements": "Not applicable for ALFWorld itself (text-only); when aligning to embodied tasks, perception modalities include egocentric RGB, depth, and instance segmentation in ALFRED/AI2-THOR.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "ALFWorld's premise is that aligning textual interaction data with embodied tasks can provide useful priors for program synthesis and planning, though FILM does not evaluate ALFWorld pretraining directly.",
            "transfer_failure_factors": "FILM highlights perception and spatial/search challenges that are not addressed by text-only pretraining; thus transferring from text-only environments may be limited by lack of perceptual grounding.",
            "key_findings": "FILM cites ALFWorld as relevant prior work exploring text-to-embodied alignment, but does not itself perform text-environment pretraining from ALFWorld; FILM's experiments show that language-model pretraining (BERT/BART) helps language-to-subtask mapping, yet perceptual and exploration components remain dominant failure modes for embodied performance.",
            "uuid": "e1792.2",
            "source_info": {
                "paper_title": "FILM: Following Instructions in Language with Modular Methods",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Alfworld: Aligning text and embodied environments for interactive learning",
            "rating": 2
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "rating": 2
        },
        {
            "paper_title": "A persistent spatial semantic representation for high-level natural language instruction execution",
            "rating": 2
        },
        {
            "paper_title": "Episodic transformer for vision-and-language navigation",
            "rating": 1
        }
    ],
    "cost": 0.015324,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FILM: Following Instructions in Language with Modular Methods</h1>
<p>So Yeon Min ${ }^{1}$ Devendra Singh Chaplot ${ }^{2}$ Pradeep Ravikumar ${ }^{1}$<br>Yonatan Bisk ${ }^{1}$ Ruslan Salakhutdinov ${ }^{1}$<br>${ }^{1}$ Carnegie Mellon University<br>{soyeonm, pradeepr, ybisk, rsalakhu}@cs.cmu.edu<br>${ }^{2}$ Facebook AI Research<br>dchaplot@fb.com</p>
<h4>Abstract</h4>
<p>Recent methods for embodied instruction following are typically trained end-toend using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance ( $24.46 \%$ ) with a substantial ( $8.17 \%$ absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance ( $26.49 \%) .{ }^{1}$ Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions. ${ }^{2}$</p>
<h2>1 INTRODUCTION</h2>
<p>Human intelligence simultaneously processes data of multiple modalities, including but not limited to natural language and egocentric vision, in an embodied environment. Powered by the success of machine learning models in individual modalities (Devlin et al., 2018; He et al., 2016; Voulodimos et al.; Anderson et al., 2018a), there has been growing interest to build multimodal embodied agents that perform complex tasks. An incipient pursuit of such interest was to solve the task of Vision Language Navigation (VLN), for which the agent is required to navigate to the goal area given a language instruction (Anderson et al., 2018b; Fried et al., 2018; Zhu et al., 2020).</p>
<p>Embodied instruction following (EIF) presents a more complex and human-like setting than VLN or Object Goal Navigation (Gupta et al., 2017; Chaplot et al., 2020b; Du et al., 2021); beyond just navigation, agents are required to execute sequences of sub-tasks that entail both navigation and interaction actions from a language instruction (Fig. 1). The additional challenges posed by EIF are threefold - the agent has to understand compositional instructions of multiple types and subtasks, choose actions from a large action space and execute them for longer horizons, and localize objects in a fine-grained manner for interaction (Nguyen et al., 2021).</p>
<p>Most existing methods (Zhang \&amp; Chai, 2021; Kim et al., 2021; Nottingham et al., 2021) for EIF have relied on neural memory of various types (transformer embeddings, LSTM state), trained end-to-end with expert trajectories upon raw or pre-processed language/visual inputs. However, EIF remains a very challenging task for end-to-end methods as they require the neural net to simultaneously learn state-tracking, building spatial memory, exploration, long-term planning, and low-level control.</p>
<p>In this work, we propose FILM $\leftrightarrows$ (Following Instructions in Language with Modular methods). FILM consists of several modular components that each (1) processes language instructions into</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An Embodied Instruction Following (EIF) task consists of multiple subtasks. (a) FILM method overview: The agent receives the language instruction and the egocentric vision of the frame. At every time step, a semantic top-down map of the scene is updated from predicted depth and instance segmentation. Until the subgoal object is observed, a search goal (blue dot) is sampled from the semantic search policy. (b) Example trajectories: Trajectory of an existing model (HiTUT (Zhang \&amp; Chai, 2021)) is plotted in a straight green line, and that of FILM is in dotted red. While HiTUT's agent travels repeatedly over a path of closed loop (thick green line, arrow pointing in the direction of travel), FILM's semantic search allows better exploration and the agent sufficiently explores the environment and completes all subtasks.
structured forms (Language Processing), (2) converts egocentric visual input into a semantic metric map (Semantic Mapping), (3) predicts a search goal location (Semantic Search Policy), and (4) outputs subsequent navigation/ interaction actions (Deterministic Policy). FILM overcomes some of the shortcomings of previous methods by leveraging a modular design with structured spatial components. Unlike many of the existing methods for EIF, FILM does not require any input that provides sequential guidance, namely expert trajectories or low-level language instructions. While Blukis et al. (2021) recently introduced a method that uses a structured spatial memory, it comes with some limitations from the lack of explicit semantic search and the reliance on expert trajectories.</p>
<p>On the ALFRED (Shridhar et al., 2020) benchmark, FILM achieves State-of-the-Art performance (24.46\%) with a large margin ( $8 \%$ absolute) from the previous SOTA (Blukis et al., 2021). Most approaches rely on low-level instructions, and we too find that including them leads to an additional $2 \%$ improvement in success rate ( $26.49 \%$ ). FILM's strong performance and our analysis indicate that an explicit structured spatial memory coupled with a semantic search policy can provide better state-tracking and exploration, even in the absence of expert trajectories or low-level instructions.</p>
<h1>2 Related Work</h1>
<p>A plethora of works have been published on embodied vision and language tasks, such as VLN (Anderson et al., 2018b; Fried et al., 2018; Zhu et al., 2020), Embodied Question Answering (Das et al., 2018; Gordon et al., 2018), and topics of multimodal representation learning (Wang et al., 2020; Bisk et al., 2020), such as Embodied Language Grounding (Prabhudesai et al., 2020). For Visual Language Navigation, which is the most comparable to the setting of our work, methods with impressive performances (Ke et al., 2019; Wang et al., 2019; Ma et al., 2019) have been proposed since the introduction of R2R (Anderson et al., 2018b). While far from conquering VLN, these methods have shown up to $61 \%$ success rate on unseen test environments (Ke et al., 2019).</p>
<p>For the more challenging task of Embodied Instruction Following (EIF), multiple methods have been proposed with differing levels of modularity in the model structure. As a baseline, Shridhar et al. (2020) has presented a Seq2Seq model with an attention mechanism and a progress monitor, while Pashevich et al. (2021) proposed to replace to seq2seq model with an episodic transformer. These methods take the concatenation of language features, visual features, and past trajectories as input and predict the subsequent action end-to-end. On the other hand, Kim et al. (2021); Zhang</p>
<p>\&amp; Chai (2021); Nguyen et al. (2021) modularly process raw language and visual inputs into structured forms, while keeping a separate "action prediction module" that outputs low-level actions given processed language outputs. Their "action taking module" itself is trained end-to-end and relies on neural memory that "implicitly" tracks all of spatial, progressive, and states of the agent. Unlike these methods, FILM's structured language/ spatial representations make reasons for failure transparent and elucidates directions to improve individual components.</p>
<p>Recently, Blukis et al. (2021) has proposed a more modular method with a persistent and structured spatial memory. Language and visual input are transformed into respectively high-level actions and the 3D map. With the 3D map and high-level actions as input, low-level actions are predicted with a value-iteration network (VIN). Navigation goals for the VIN are sampled from a model trained on interaction pose labels from expert trajectories. Among all proposed methods for EIF, FILM necessitates the least information (neither low-level instructions nor expert trajectories are needed, although the former can be taken as an additional input). Furthermore, FILM addresses the problem of search/ exploration of goal objects.</p>
<p>Various works in visual navigation with semantic mapping are also relevant. Simultaneous Localization and Mapping (SLAM) methods, which build 2D or 3D obstacle maps, have been widely used (Fuentes-Pacheco et al., 2015; Izadi et al., 2011; Snavely et al., 2008). In contrast to these works, recent methods (Chaplot et al., 2020b;a) build semantic maps with differentiable projection operations, which restrain egocentric prediction errors amplifying in the map. The task of Chaplot et al. (2020b;a) is object goal navigation, a much simpler task compared to EIF. Furthermore, while Chaplot et al. (2020b) employs a semantic exploration policy, our and their semantic policies serve fundamentally different purposes; while their policy guides a general sense of direction among multiple rooms in the search for large objects (e.g. fridge), ours guides the search for potential locations of small and flat objects which have little chance of detection at a distance. Also, our semantic policy is conditioned on language instructions. Blukis et al. (2018a;b) also successfully utilized semantic 2D maps in grounded language navigation tasks. These works are for quadcopters, whose fields of view almost entirely cover the scene and the need for "search" or "exploration" is less crucial than for pedestrian agents. Moreover, their settings only involve navigation with a single subtask.</p>
<h1>3 TASK EXPLANATION</h1>
<p>We utilize the ALFRED benchmark. The agent has to complete household tasks given only natural language instructions and egocentric vision (Fig. 1). For example, the instruction may be given as "Put a heated apple on the counter," with low-level instructions (which FILM does not use by default) further explaining step-by-step lower level actions. In this case, one way to "succeed" in this episode is to sequentially (1) pick up the apple, (2) put the apple in the microwave, (3) toggle the microwave on/off, (4) pick up the apple again, and (4) place it on the countertop. Episodes run for a significantly longer number of steps compared to benchmarks with only single subgoals; even expert trajectories, which are maximally efficient and perform only the strictly necessary actions (without any steps to search for an object), are often longer than 70 steps.</p>
<p>There are seven types of tasks (Appendix A.1), from relatively simple types (e.g. Pick \&amp; Place) to more complex ones (e.g. Heat \&amp; Place). Furthermore, the instruction may require that an object is "sliced" (e.g. Slice bread, cook it in the microwave, put it on the counter). An episode is deemed "success" if the agent completes all sub-tasks within 10 failed low-level actions and 1000 max steps.</p>
<h2>4 MEthods</h2>
<p>FILM consists of three learned modules: (1) Language Processing (LP), (2) Semantic Mapping, and (3) Semantic Search Policy; and one purely deterministic navigation/ interaction policy module (Fig. 2). At the start of an episode, the LP module processes the language instruction into a sequence of subtasks. Every time step, the semantic mapping module receives the egocentric RGB frame and updates the semantic map. If the goal object of the current subtask is not yet observed, the semantic search policy predicts a "search goal" at a coarse time scale; until the next search goal is predicted, the agent navigates to the current search goal with the deterministic policy. If the goal is observed, the deterministic policy decides low-level controls for interaction actions (e.g. "Pick Up" object).</p>
<h3>4.1 LANGUAGE PROCESSING (LP)</h3>
<p>The language processing (LP) module transforms high-level instructions into a structured sequence of subtasks (Fig. 3). It consists of two BERT (Devlin et al., 2018) submodules that receive the in-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: FILM method overview. The "grouping" in blue, green, and yellow denote the coarseness of time scale (blue: at the beginning of the episode, green: at every time step, yellow: at a coarser time scale of every 25 steps). At the beginning of the episode, the Language Processing module processes the instruction into subtasks. At every time step, Semantic Mapping converts egocentric into RGB a top-down semantic map. The semantic search policy outputs the search goal at a coarse time scale. Finally, the Deterministic Policy decides the next action. Modules in bright green are learned; the deterministic policy (grey) is not.
struction as an input at the beginning of the episode. The first submodule (BERT type classification) receives the instruction and predicts the "type" of the instruction - one of the seven types stated in Appendix A.1. The second submodule (BERT argument classification) receives both the instruction and the predicted type as input and predicts the "arguments" - (1) "obj" for the object to be picked up, (2) "recep" for the receptacle where "obj" should be ultimately placed, (3) "sliced" for whether "obj" should be sliced, and (4) "parent" for tasks with intermediate movable receptacles (e.g. "cup" in "Put a knife in a cup on the table" of Appendix A.1). An object in ALFRED is always an instance of either "obj" or "recep"; "parent" objects are a subset of "recep" objects that are movable. We train a separate BERT model for each argument predictor. The two submodules are easily trainable with supervised learning since the type and the four arguments are provided in the training set. Models use only the CLS token for classification, and they do not share parameters; all layers of "bert-base-uncased" were fine-tuned.</p>
<p>Due to the patterned nature of instructions, we can match the predicted "type" of the instruction to a "type template" with blank arguments. For example, if the instruction is classified as the "clean \&amp; place" type, it is matched to the template "(Obj, PickUp), (SinkBasin, Put), (Faucet, ToggleOn), (Faucet, ToggleOff), (Obj, PickUp), (Recep, Put)". If the "sliced" argument is predicted to be true from argument classification, subtasks of "(Knife, PickUp), (Obj, Slice), (Sink, PutObject)" will be added at the beginning of the template (with the (Sink, PutObject) to make the agent drop the knife). Filling in the "type template" with predictions of the second model, we obtain a list of subtasks (bottom of Fig. 3b) to be completed in the current episode. The "type templates" were designed by hand in less than 20 minutes. In section 5.2, we discuss the effect of using a LP module without the template assumption, for fair comparison with other works. Appendix A. 9 contains more details.</p>
<h1>4.2 Semantic Mapping Module</h1>
<p>We designed the semantic mapping module (Appendix A.2) with inspirations from prior work (Chaplot et al., 2020b). Egocentric RGB is first processed into depth map and instance segmentation, with MaskRCNN (He et al., 2017) (and its implementation by Shridhar et al. (2021)) and the depth prediction method of Blukis et al. (2021); details of the training are explained in Section $5^{3}$ . These pre-trained, off-the-shelf models were finetuned on the training scenes of ALFRED. Once processed, the depth observation is transformed to a point cloud, of which each point is associated with the predicted semantic categories. Finally, the point cloud is binned into a voxel representation; this summed over height is the semantic map. The map is locally updated and aggregated over time.
The resulting semantic map is an allocentric $(C+2) \times M \times M$ binary grid, where $C$ is the number of object categories and each of the $M \times M$ cells represents a $5 \mathrm{~cm} \times 5 \mathrm{~cm}$ space of the scene. The $C$ channels each represent whether a particular object of interest was observed; the two extra channels denote whether obstacle exists and whether exploration happened</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The Language Processing module. (a): Two BERT models respectively predict the "type" and the "arguments" of the instruction. (b): The predicted "type" from (a) is matched with a template, and the "arguments" of the template is filled with the predicted "argument."
in a particular $5 \mathrm{~cm} \times 5 \mathrm{~cm}$ space. Thus, the $C+2$ channels are a semantic/spatial summary of the corresponding space. We use $M=240$ ( 12 meters in the physical world) and $C=$ $28+$ (number of additional subgoal objects in the current task). " 28 " is the number of "receptacle" objects (e.g. "Table", "Bathtub"), which are usually large and easily detected; in the example of Fig. 1, there is one additional subgoal object ("Apple"). Please see Appendix A. 2 on details of the dynamic handling of $C$.</p>
<h1>4.3 Semantic Search Policy</h1>
<p>The semantic search policy outputs a coarse 2D distribution for potential locations of a small subgoal object (Fig. 4), given a semantic map with the 28 receptacle objects only (e.g. "Countertop", "Shelf"). The discovery of a small object is difficult in ALFRED due to three reasons - (1) many objects are tiny (some instances of "pencil" occupies less than 200 pixels even at a very close view), (2) the field of view is small due to the camera horizon mostly being downward ${ }^{4}$, (3) semantic segmentation, despite being fine-tuned, cannot detect small objects at certain angles. The role of the semantic search policy is to predict search locations for small objects, upon the observed spatial configuration of larger ones. While existing works surmise the "implicit" learning of search locations from expert trajectories, we directly learn an explicit policy without expert trajectories.</p>
<p>The policy is trained via supervised learning. For data collection, we deploy the agent without the policy in the training set and gather the (1) semantic map with only receptacle objects and (2) the ground truth location of the subgoal object after every 25 steps. A model of 15 layers of CNN with max-pooling in between (details in Appendix A.3) outputs an $N \times N$ grid, where $N$ is smaller than the original map size $M$; this is a 2D distribution for the potential location of the subgoal object. Finally, the KL divergence between this and a pseudo-ground truth "coarse" distribution whose mass is uniformly distributed over all cells with the true location of the subgoal object is minimized $\left(\min _{p} K L(p | q)\right.$ where $p$ is the coarse ground truth and $q$ is the coarse prediction). At deployment, the "search goal" is sampled from the predicted distribution, resized to match the original map size of $M \times M$ (e.g. $240 \times 240$ ), with mass in the coarse $N \times N$ (e.g. $8 \times 8$ ) grid uniformly spread out to the $\left\lfloor\frac{M}{N}\right\rfloor \times\left\lfloor\frac{M}{N}\right\rfloor$ area centered on it. Because arriving at the search goal requires time, the policy operates at a "coarse" time scale of 25 steps; the agent navigates towards the current search goal until the next goal is sampled or the subgoal object is found (more details in Section 4.4).</p>
<p>Fig. 4 shows a visualization of the semantic search policy's outputs. The policy provides a reasonably close estimate of the true distribution; the predicted mass of "bowl" is shared around observed furniture that it can appear on, and that of "faucet" peaks around the sink/ the end of the bathtub. While we chose $N=8$ as the grid size, Appendix A. 4 provides a general bound for choosing $N$.</p>
<h3>4.4 Deterministic Policy</h3>
<p>Given (1) the predicted subtasks, (2) the most recent semantic map, and (3) the search goal sampled at a coarse time scale, the deterministic policy outputs a navigation or interaction action (Fig. 2).
Let $\left[\left(\right.\right.$ obj $<em 1="1">{1}$, action $\left.\left.</em>\right), \ldots, \quad\left(\right.$ obj $<em k="k">{k}$, action $\left.\left.</em>\right)\right]$ be the list of subtasks and the current subtask be $\left(\right.$ obj $<em i="i">{i}$, action $\left.\left.</em>$ is selected as the goal; otherwise, the sample from the semantic search policy is chosen as the goal (Section 4.3). The agent then navigates towards the goal via the Fast Marching Method (Sethian, 1996) and performs}\right)$. If $o b j_{i}$ is observed in the current semantic map, the closest $o b j_{i</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Example visualization of semantic search policy outputs. In each of (a), (b), Top left: map built from ground truth depth/ segmentation, Top right: map from learned depth/ segmentation, Bottom left: ground truth "coarse" distribution, Bottom right: predicted "coarse" distribution. (a): While the true location of the "bowl" was on the upper left coffee table, the policy distributes mass over all furniture likely to have it on. (b): The true location of the faucet is on the sink and at the end of the bathtub. While the policy puts more mass near the sink, it also allocates some to the end of the bathtub.
the required interaction actions. While this "low-level" policy could be learned with imitation or reinforcement learning, we used a deterministic one based on the findings of earlier work that observed that the Fast Marching Method performs as well as a learned local navigation policy (Chaplot et al., 2020b). When the agent successfully executes the required interaction action $_{i}$ (which can be determined by the change in the egocentric RGB), the pointer of subtasks is advanced to $i+1$ or the task is completed. More details and pseudocode are provided in Appendix A.5.</p>
<h1>5 EXPERIMENTS AND RESULTS</h1>
<p>We explain the metrics, evaluation splits, and baselines against which FILM is compared. Furthermore, we describe training details of each of the learned components of FILM.
Metrics Success Rate (SR) is a binary indicator of whether all subtasks were completed. The goalcondition success (GC) of a model is the ratio of goal-conditions completed at the end of an episode. For example, in the example of Fig. 1, there are three goal-conditions - a pan must be "cleaned", a pan should rest on a countertop, and a "clean" pan should rest on a countertop. Both SR and GC can be weighted by (path length of the expert trajectory)/ (path length taken by the agent); these are called path length weighted SR (PLWSR) and path length weighted GC (PLWGC).</p>
<p>Evaluation Splits The test set consists of "Tests Seen" (1533 epsiodes) and "Tests unseen" (1529 episodes); the scenes of the latter entirely consist of rooms that do not appear in the training set, while those of the former only consist of scenes seen during training. Similarly, the validation set is partitioned into "Valid Seen" (820 epsiodes) and "Valid Unseen" (821 epsiodes). The official leaderboard ranks all entries by the SR on Tests Unseen.</p>
<p>Baselines There are two kinds of baselines: those that use low-level sequential instructions (Kim et al., 2021; Zhang \&amp; Chai, 2021; Nguyen et al., 2021; Pashevich et al., 2021) and those that do not (Nottingham et al., 2021; Blukis et al., 2021). While FILM does not necessitate low-level instructions, we report results with and without them and compare them against methods of both kinds.</p>
<p>Training Details of Learned Components In the LP module, BERT type classification and argument classification were trained with AdamW from the Transformer (Wolf et al., 2019) package; learning rates are $1 \mathrm{e}-6$ for type classification and ${1 \mathrm{e}-4,1 \mathrm{e}-5,5 \mathrm{e}-5,5 \mathrm{e}-5}$ for each of "object", "parent", "recep", "sliced" argument classification. In the Semantic Mapping module, separate depth models for camera horizons of $45^{\circ}$ and $0^{\circ}$ were fine-tuned from an existing model of HLSM (Blukis et al., 2021), both with learning rate $1 \mathrm{e}-3$ and the AdamW optimizer (epsilon 1e-6, weight decay 1e-2). Similarly, separate instance segmentation models for small and large objects were fine-tuned, starting from their respective parameters released by Shridhar et al. (2021), with learning rate 1e-3 and the SGD optimizer (momentum 0.9 , weight decay $5 \mathrm{e}-4$ ). Finally, the semantic search policy was trained with learning rate $5 \mathrm{e}-4$ and the AdamW optimizer (epsilon 1e-6). Appendix A. 2 and A. 3 discuss more details on the architectures of semantic mapping/ semantic search policy modules. The readme of our code states protocols and commands so that readers can reproduce all expriments.</p>
<p>Table 1: Test results. Top section uses step-by-step instructions; bottom section does not. Bold numbers are top scores in each section. Blue numbers are the top SR on Tests Unseen (by which the leaderboard is ranked).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Tests Seen</th>
<th></th>
<th></th>
<th></th>
<th>Tests Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>PLWGC</td>
<td>GC</td>
<td>PLWSR</td>
<td>SR</td>
<td>PLWGC</td>
<td>GC</td>
<td>PLWSR</td>
<td>SR</td>
</tr>
<tr>
<td>Low-level Sequential Instructions + High-level Goal Instruction</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SEQ2SEQ</td>
<td>(Shridhar et al., 2020)</td>
<td>6.27</td>
<td>9.42</td>
<td>2.02</td>
<td>3.98</td>
<td>4.26</td>
<td>7.03</td>
<td>0.08</td>
</tr>
<tr>
<td>MOCA</td>
<td>(Singh et al., 2020)</td>
<td>22.05</td>
<td>28.29</td>
<td>15.10</td>
<td>22.05</td>
<td>9.99</td>
<td>14.28</td>
<td>2.72</td>
</tr>
<tr>
<td>E.T.</td>
<td>(Pashevich et al., 2021)</td>
<td>-</td>
<td>36.47</td>
<td>-</td>
<td>28.77</td>
<td>-</td>
<td>15.01</td>
<td>-</td>
</tr>
<tr>
<td>E.T. + synth. data</td>
<td>(Pashevich et al., 2021)</td>
<td>34.93</td>
<td>45.44</td>
<td>27.78</td>
<td>38.42</td>
<td>11.46</td>
<td>18.56</td>
<td>4.10</td>
</tr>
<tr>
<td>LWIT</td>
<td>(Nguyen et al., 2021)</td>
<td>23.10</td>
<td>40.53</td>
<td>43.10</td>
<td>30.92</td>
<td>16.34</td>
<td>20.91</td>
<td>5.60</td>
</tr>
<tr>
<td>HtTUT</td>
<td>(Zhang \&amp; Chai, 2021)</td>
<td>17.41</td>
<td>29.97</td>
<td>11.10</td>
<td>21.27</td>
<td>11.51</td>
<td>20.31</td>
<td>5.86</td>
</tr>
<tr>
<td>ABP</td>
<td>(Kim et al., 2021)</td>
<td>4.92</td>
<td>51.13</td>
<td>3.88</td>
<td>44.55</td>
<td>2.22</td>
<td>24.76</td>
<td>1.08</td>
</tr>
<tr>
<td>FILM W.O. SEMANTIC SEARCH</td>
<td>13.10</td>
<td>35.59</td>
<td>9.43</td>
<td>25.90</td>
<td>13.37</td>
<td>35.51</td>
<td>10.17</td>
<td>23.94</td>
</tr>
<tr>
<td>FILM $\stackrel{\text { ® }}{\text { ® }}$</td>
<td>15.06</td>
<td>38.51</td>
<td>11.23</td>
<td>27.67</td>
<td>14.30</td>
<td>36.37</td>
<td>10.55</td>
<td>26.49</td>
</tr>
<tr>
<td>High-level Goal Instruction Only</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LAV</td>
<td>(Nottingham et al., 2021)</td>
<td>13.18</td>
<td>23.21</td>
<td>6.31</td>
<td>13.35</td>
<td>10.47</td>
<td>17.27</td>
<td>3.12</td>
</tr>
<tr>
<td>HtTUT G-only</td>
<td>(Zhang \&amp; Chai, 2021)</td>
<td>-</td>
<td>21.11</td>
<td>-</td>
<td>13.63</td>
<td>-</td>
<td>17.89</td>
<td>-</td>
</tr>
<tr>
<td>HLSM</td>
<td>(Blukis et al., 2021)</td>
<td>11.53</td>
<td>35.79</td>
<td>6.69</td>
<td>25.11</td>
<td>8.45</td>
<td>27.24</td>
<td>4.34</td>
</tr>
<tr>
<td>FILM W.O. SEMANTIC SEARCH</td>
<td>12.22</td>
<td>34.41</td>
<td>8.65</td>
<td>24.72</td>
<td>12.69</td>
<td>34.00</td>
<td>9.44</td>
<td>22.56</td>
</tr>
<tr>
<td>FILM $\stackrel{\text { ® }}{\text { ® }}$</td>
<td>14.17</td>
<td>36.15</td>
<td>10.39</td>
<td>25.77</td>
<td>13.13</td>
<td>34.75</td>
<td>9.67</td>
<td>24.46</td>
</tr>
</tbody>
</table>
<h1>5.1 RESULTS</h1>
<p>Table 8 shows test results. FILM achieves state-of-the-art performance across both seen and unseen scenes in the setting where only high-level instructions are given. It achieves $8.17 \%$ absolute ( $50.15 \%$ relative) gain in SR on Tests Unseen, and $0.66 \%$ absolute ( $2.63 \%$ relative) gain in SR on Tests Seen over HLSM, the previous SOTA.</p>
<p>FILM performs competitively even compared to methods that require low-level step-by-step instructions. They can be used as additional inputs to the LP module, with the low-level instruction appended to the high-level instruction for both BERT type classification and BERT argument classification. Under this setting, FILM achieves $11.06 \%$ absolute ( $71.68 \%$ relative) gain in SR on Tests Unseen compared to ABP. Notably, FILM performs similarly across Tests Seen and Tests Unseen, which implies FILM's strong generalizability. This is in contrast to that methods that require lowlevel instructions, such as ABP, E.T., LWIT, MOCA, perform very well on Tests Seen but much less so on unseen scenes. In a Sim2Real situation, these methods will excel if the agent can be trained in the exact household it will be deployed in, with multiple low-level instructions and expert trajectories. In the more realistic and cost-efficient setting where the agent is trained in a centralized manner and has to generalize to new scenes, FILM will be more adequate.</p>
<p>It is also notable that the semantic search policy significantly increases not only SR and GC, but also their path-length weighted versions. On Tests Seen, the gap of PLWSR between FILM with/ without semantic search is larger than the corresponding gap of SR (for both with/ without low-level instructions). This suggests that the semantic policy boosts the efficiency of trajectories. The results in Appendix A. 8 show that the improvement by the semantic policy is reproduced across multiple seeds; the protocols for reproduction are explained along with the result.</p>
<h3>5.2 Ablations Studies and Error Analysis</h3>
<p>Errors due to perception and language processing. To understand the importance of FILM's individual modules, we consider ablations on the base method, the base method with low-level language, and with ground truth visual/ language inputs. Table 2 shows ablations on the development sets. While the improvement from gt depth is large in unseen scenes ( $10.64 \%$ ), it is incremental on seen scenes ( $1.48 \%$ ); on the other hand, gt segmentation significantly boosts performances in both cases ( $9.26 \%$ / $9.26 \%$ ). Thus, among visual perception, segmentation is a bottleneck in both seen/ unseen scenes, and depth is a bottleneck only in the latter. On the other hand, while a large gain in SR comes from using ground truth language ( $7.43 \% / 4.22 \%$ ), that from adding low-level language as input is rather incremental. We additionally analyze the effect of the template assumption (explained in the second paragraph of Section 4.1), by reporting the performance with a Language Processing module without this assumption. The results drop without the templates but not by a large margin. Appendix A. 9 explains the details of this auxiliary Language Processing module.</p>
<p>Error modes. Table 3 shows common error modes of FILM; the metric is the percent of episodes that failed (in SR) from a particular error out of all failed episodes. The main failures in valid unseen scenes are due to failures in (1) locating the subgoal object (due to the small field of view, imperfect segmentation, ineffective exploration), (2) locating the subgoal object because it is in a closed re-</p>
<p>Table 2: Ablation results on validation splits. Base Method is FILM with semantic search policy.</p>
<p>| Method | Val Seen |  | Val Unseen |  |
| | GC | SR | GC | SR |
| Base Method | 37.20 | 24.63 | 32.45 | 20.10 |
| + low-level language | 38.54 | 25.24 | 32.89 | 20.61 |
| + gt seg. | 45.46 | 34.02 | 42.88 | 29.35 |
| + gt depth | 38.21 | 26.59 | 42.91 | 30.73 |
| + gt depth, gt seg. | 55.54 | 43.22 | 64.31 | 55.05 |
| + gt depth, gt seg., gt lang. | 59.47 | 47.44 | 69.13 | 62.48 |
| - template assumption | 31.46 | 20.37 | 31.14 | 18.03 |</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average number of subtasks completed until failure, by task type (light green/ light blue respectively for valid seen/ unseen). Dark green/ blue: average number of total subtasks in valid seen/ unseen.</p>
<p>Table 3: Error Modes. Table showing percentage of errors due to each failure mode for FILM on the Val set.</p>
<table>
<thead>
<tr>
<th>Error mode</th>
<th>Seen</th>
<th>Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td>Goal object not found</td>
<td>23.30</td>
<td>26.07</td>
</tr>
<tr>
<td>Interaction failures</td>
<td>6.96</td>
<td>8.54</td>
</tr>
<tr>
<td>Collisions</td>
<td>6.96</td>
<td>11.00</td>
</tr>
<tr>
<td>Object in closed receptacle</td>
<td>18.44</td>
<td>16.16</td>
</tr>
<tr>
<td>Language processing error</td>
<td>18.53</td>
<td>24.54</td>
</tr>
<tr>
<td>Others</td>
<td>25.81</td>
<td>13.69</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance by task type of base model on validation.</p>
<p>| Task Type | Val Seen |  | Val Unseen |  |
| | GC | SR | GC | SR |
| Overall | 37.20 | 24.63 | 32.45 | 20.10 |
| Examine | 50.00 | 34.41 | 45.06 | 29.65 |
| Pick \&amp; Place | 27.46 | 26.92 | 16.67 | 16.03 |
| Stack \&amp; Place | 23.74 | 10.71 | 9.90 | 1.98 |
| Clean \&amp; Place | 58.56 | 44.04 | 48.89 | 33.63 |
| Cool \&amp; Place | 27.04 | 12.61 | 27.41 | 14.04 |
| Heat \&amp; Place | 40.21 | 22.02 | 37.77 | 23.02 |
| Pick 2 \&amp; Place | 40.37 | 23.77 | 29.28 | 11.84 |</p>
<p>ceptacle (cabinet, drawer, etc), (3) interaction (due to object being too far or not in field of view, bad segmentation mask), (4) navigation (collisions), (5) correctly processing language instructions, (6) others, such as the deterministic policy repeating a loop of actions from depth/ segmentation failures and 10 failed actions accruing from a mixture of different errors. A failed episode is classified to the error type that occurs "earlier" (e.g. If the subtasks were processed incorrectly and also there were 10 consecutive collisions, this episode is classified as (5) (failure in incorrectly processsing language instructions) since the LP module comes "earlier" than the collisions). More details are in Appendix A.6. As seen in Table 3, goal object not found is the most common error mode, typically due to objects being small and not visible from a distance or certain viewpoints. Results of the next subsection show that this error is alleviated by the semantic search policy in certain cases.</p>
<p>Performance over different task types. To understand FILM's strengths/ weaknesses across different types of tasks, we further ablate validation results by task type in Table 4. Figure 5 shows the average number of subtasks completed for failed episodes, by task type. First, the SR and GC for "Stack \&amp; Place" is remarkably low. Second, the number of the subtasks entailed with the task type does not strongly correlate with performance. While "Heat \&amp; Place" usually involves three more subtasks than "Pick \&amp; Place", the metrics for the former are much higher than those of the latter. Since task types inevitably occur in different kinds of scenes (e.g. "Heat \&amp; Place" only occurs in kitchens) and therefore involve different kinds of objects (e.g. "Heat \&amp; Place" involves food only), the results suggest that the success of the first PickUp action largely depends on the kinds of the scene and size and type of the subgoal objects rather than number of subtasks.</p>
<p>While the above error analysis is specific to FILM, its implications regarding visual perception may generally represent the weaknesses of existing methods for EIF, since most recent methods (ABP, HLSM, HiTUT, LWIT, E.T.) use the same family of segmentation/ detection models as FILM, such as Mask-RCNN and Fast-RCNN (Wang et al., 2017). Specifically, it could be that the inability to find a subgoal object is a major failure mode in the mentioned existing methods as well. On the other hand, FILM is not designed to search inside closed receptacles (e.g. cabinets), although subgoal objects dwell in receptacles quite frequently (Table 3); a future work to extend FILM should learn to perform a more active search.</p>
<h3>5.3 Effects of the Semantic Search Policy</h3>
<p>With Valid Unseen as the development set, we observed that the semantic search policy significantly helps to find small objects (Table 5); we use the percent of episodes in which the first goal object was found ( $\% 1$ st Goal Found) as a proxy, since it can be picked up (e.g. "Apple", "Pen")</p>
<p>Table 5: Dev set results (valid unseen) of FILM with/ without semantic search policy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">$\%$ 1st Goal Found</th>
<th style="text-align: center;">SR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">HLSM (Blukis et al., 2021)</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">11.8</td>
</tr>
<tr>
<td style="text-align: left;">FILM with Search</td>
<td style="text-align: center;">$\mathbf{8 0 . 5 1}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 0 9}$</td>
</tr>
<tr>
<td style="text-align: left;">FILM w.o. Search</td>
<td style="text-align: center;">76.12</td>
<td style="text-align: center;">19.85</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Example trajectories of FILM with and without semantic search policy. Paths near the subgoals that were traveled 3 times or more are in straight red. The goal (which can be the search goal or an observed instance of a subgoal object) is in blue.</p>
<p>Table 6: Performance with and without semantic search policy, by room size.</p>
<table>
<thead>
<tr>
<th>Room Size</th>
<th>Small</th>
<th></th>
<th>Large</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>FILM</td>
<td>FILM <br> w.o. Search</td>
<td>FILM</td>
<td>FILM <br> w.o. Search</td>
</tr>
<tr>
<td>SR</td>
<td>26.70</td>
<td>26.63</td>
<td>15.17</td>
<td>14.74</td>
</tr>
<tr>
<td>\% 1st Goal Found</td>
<td>79.32</td>
<td>81.02</td>
<td>80.13</td>
<td>73.72</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance with and without semantic search policy, by task type.</p>
<table>
<thead>
<tr>
<th>Task Type</th>
<th>Clean \&amp; Place</th>
<th></th>
<th>Other Types</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>FILM</td>
<td>FILM <br> w.o. Search</td>
<td>FILM</td>
<td>FILM <br> w.o. Search</td>
</tr>
<tr>
<td>SR</td>
<td>33.63</td>
<td>14.16</td>
<td>17.94</td>
<td>20.16</td>
</tr>
<tr>
<td>\% 1st Goal Found</td>
<td>87.61</td>
<td>79.65</td>
<td>79.38</td>
<td>75.56</td>
</tr>
<tr>
<td>\% 1st Recep Found</td>
<td>80.53</td>
<td>69.03</td>
<td>58.05</td>
<td>55.93</td>
</tr>
</tbody>
</table>
<p>and thus is usually small. Thus, we use FILM with semantic search as the "base method" (default) for all experiments/ablations.</p>
<p>To further analyze when the semantic search policy especially helps, we ablate on room sizes and task types. Table 6 shows the SR and $\% 1$ st Goal Found with and without search, by room size (details on the assignment of Room Size are in Appendix A.7). As expected, the semantic policy increases both metrics, especially so in large scenes. This is desirable since the policy makes the agent less disoriented in difficult scenarios (large scenes); the model without it is more susceptible to failing even the first subtask. Figure 6 is consistent with the trend of Table 6; it shows example trajectories of FILM with and without the semantic search policy in a large kitchen scene. Since the countertop appears in the bottom right quadrant of the map, it is desirable that the agent travels there to search for a "knife". While FILM travels to this area frequently (straight red line in Fig.6), FILM without semantic search mostly wanders in irrelevant locations (e.g. the bottom left quadrant).</p>
<p>Table 7 further shows the performance with and without search by task type. Notably, the gap of performance for the "clean \&amp; place" type is very large. In the large kitchen scene of "Valid Unseen" (Fig. 6), the "Sink" looks very flat from a distance and is hardly detected. The semantic policy induces the agent to travel near the countertop area and improves the localization of the 1st Recep ("Sink") for the "clean \&amp; place" type (Table 7). In conclusion, the semantic policy improves the localization of small and flat objects in large scenes.</p>
<h1>6 CONCLUSION</h1>
<p>We proposed FILM, a new modular method for embodied instruction following which (1) processes language instructions into structured forms (Language Processing), (2) converts egocentric vision into a semantic metric map (Semantic Mapping), (3) predicts a likely goal location (Semantic Search Policy), and (4) outputs subsequent navigation/interaction actions (Algorithmic Planning). FILM achieves the state of the art on the ALFRED benchmark without any sequential supervision.</p>
<h1>ETHICS STATEMENT</h1>
<p>This research is for building autonomous agents. While we do not perform any experiments with humans, practitioners may attempt to extend and apply this technology in environments with humans. Such potential applications of this research should take privacy concerns into consideration.</p>
<p>All learned models in this research were trained using Ai2Thor (Kolve et al., 2019). Thus, they may be biased towards North American homes.</p>
<h2>REPRODUCIbILITY STATEMENT</h2>
<p>We thoroughly explain training details and model architectures in Section 5.1 and Appendix A.2, A.3. Project webpage with code, pre-trained models, and protocols to reproduce results is released here: https://soyeonm.github.io/FILM_webpage/.</p>
<h2>REFERENCES</h2>
<p>Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprint arXiv:1807.06757, 2018a.</p>
<p>Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3674-3683, 2018b.</p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Valts Blukis, Nataly Brukhim, Andrew Bennett, Ross A Knepper, and Yoav Artzi. Following highlevel navigation instructions on a simulated quadcopter with imitation learning. In Robotics: Science and Systems (RSS), 2018a.</p>
<p>Valts Blukis, Dipendra Misra, Ross A Knepper, and Yoav Artzi. Mapping navigation instructions to continuous control actions with position-visitation prediction. In Conference on Robot Learning, pp. 505-518. PMLR, 2018b.</p>
<p>Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. A persistent spatial semantic representation for high-level natural language instruction execution. In Proceedings of the Conference on Robot Learning (CoRL), 2021.</p>
<p>Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. Learning to explore using active neural slam. arXiv preprint arXiv:2004.05155, 2020a.</p>
<p>Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33, 2020b.</p>
<p>Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-10, 2018.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Heming Du, Xin Yu, and Liang Zheng. Vtnet: Visual transformer network for object goal navigation. arXiv preprint arXiv:2105.09447, 2021.</p>
<p>Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Jorge Fuentes-Pacheco, José Ruiz-Ascencio, and Juan Manuel Rendón-Mancha. Visual simultaneous localization and mapping: a survey. Artificial intelligence review, 43(1):55-81, 2015.</p>
<p>Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4089-4098, 2018.</p>
<p>Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2616-2625, 2017.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. $770-778,2016$.</p>
<p>Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961-2969, 2017.</p>
<p>Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, et al. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pp. 559-568, 2011.</p>
<p>Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. Tactical rewind: Self-correction via backtracking in vision-andlanguage navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6741-6749, 2019.</p>
<p>Byeonghwi Kim, Suvaansh Bhambri, Kunal Pratap Singh, Roozbeh Mottaghi, and Jonghyun Choi. Agent with the big picture: Perceiving surroundings for interactive instruction following. In Embodied AI Workshop CVPR, 2021.</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai, 2019.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.</p>
<p>Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. The regretful agent: Heuristic-aided navigation through progress estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6732-6740, 2019.</p>
<p>Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani. Look wide and interpret twice: Improving performance on interactive instruction-following tasks. arXiv preprint arXiv:2106.00596, 2021.</p>
<p>Kolby Nottingham, Litian Liang, Daehyun Shin, Charless C. Fowlkes, Roy Fox, and Sameer Singh. Lav, 2021. URL https://leaderboard.allenai.org/alfred/submission/c2cm7eranqs9puf9uvjg.</p>
<p>Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-andlanguage navigation. arXiv preprint arXiv:2105.06453, 2021.</p>
<p>Mihir Prabhudesai, Hsiao-Yu Fish Tung, Syed Ashar Javed, Maximilian Sieb, Adam W Harley, and Katerina Fragkiadaki. Embodied language grounding with 3d visual feature representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. $2220-2229,2020$.</p>
<p>J A Sethian. A fast marching level set method for monotonically advancing fronts. Proceedings of the National Academy of Sciences, 93(4):1591-1595, 1996. ISSN 0027-8424. doi: 10.1073/pnas. 93.4.1591. URL https://www.pnas.org/content/93/4/1591.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.</p>
<p>Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi, and Jonghyun Choi. Moca: A modular object-centric approach for interactive instruction following. arXiv preprint arXiv:2012.03208, 2020.</p>
<p>Noah Snavely, Steven M Seitz, and Richard Szeliski. Modeling the world from internet photo collections. International journal of computer vision, 80(2):189-210, 2008.</p>
<p>Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis. Deep learning for computer vision: A brief review. Computational intelligence and neuroscience, 2018.</p>
<p>Ruocheng Wang, Jiayuan Mao, Samuel J Gershman, and Jiajun Wu. Language-mediated, objectcentric representation learning. arXiv preprint arXiv:2012.15814, 2020.</p>
<p>Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. A-fast-rcnn: Hard positive generation via adversary for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2606-2615, 2017.</p>
<p>Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6629-6638, 2019.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.</p>
<p>Yichi Zhang and Joyce Chai. Hierarchical task learning from language instructions with unified transformers and self-monitoring. arXiv preprint arXiv:2106.03427, 2021.</p>
<p>Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with selfsupervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10012-10022, 2020.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Task Definition</h2>
<p>High and low-level instructions are both available to agents. There are 7 types of tasks (Fig 7. b) and the sequence of subtasks is templated according to the task type.
(a) Instruction Goal: Drop a clean pan on the table.</p>
<p>Low Level Goal: Move forward towards the gas, pick up the pan. Turn around and head to the sink, ...
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: ALFRED overview. The goal is given in high level and low level language instructions. For and agent to achieve "success" of the goal, it needs to complete a sequence of interactions (as in the explanations in the bottom of the figure) and the entailed navigation between interactions.</p>
<h2>A. 2 Semantic Mapping Module</h2>
<p>Figure 8 is an illustration of the semantic mapping module. A depth map and instance segmentation is predicted from Egocentric RGB. Then the first and the later are respectively transformed into a point cloud and a semantic label of each point in the cloud, together producing voxels. The voxels are summed across height to produce the semantic map. Partial maps obtained at particular time steps are aggregated to the global map simply via "sum/ logical or."
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Semantic mapping module. Figure was partially taken from Chaplot et al. (2020b)</p>
<p>We dynamically control the number of objects $C$ for efficiency (because there are more than 100 objects in total). All receptacle objects (for input to the semantic policy) and all non-receptacle objects that appear in the subtasks are counted in $C$. For example, in an episode with the subtask [(Pan, PickUp), (SinkBasin, Put), (Faucet, ToggleOn), (Faucet, ToggleOff), (Pan, PickUp), (Table, Put)], all receptacle objects and "Pan", "Faucet" will be the $C$ objects indicated on the map.</p>
<h2>A. 3 Semantic Search Policy Module</h2>
<p>The map from the previous subsection is passed into 7 layers of convolutional nets, each with kernel size 3 and stride 1 . There is maxpooling between any two conv nets, and after the last layer, there is softmax over the $64(8 \times 8)$ categories, for each of the $C_{o}$ (73) channels.</p>
<p>At deployment/ validation, if the agent is currently searching for the $c$ th object, then a search location is sampled from the $c$ th channel of the outputted $8 \times 8 \times C_{o}$ grid.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Semantic search policy.</p>
<h1>A. 4 IMPACT OF GRID SIZE ON THE EFFECTIVENESS OF THE SEMANTIC SEARCH POLICY</h1>
<p>While we chose $N=8,\left\lfloor\frac{M}{N}\right\rfloor=30$ for the size of the "coarse" cell of the semantic search policy, the desirable choice of $N$ may be different if a practitioner attempts to transfer FILM to different scenes/ tasks. While a "too fine" semantic policy will be hard to train due to sparseness of labels, a "too coarse" one will spread the mass of the distribution to widely.
Let us examine the "coarse" and "actual" ground truth distributions just in one direction (e.g. the horizontal direction). Let $F_{X}(x), C_{X}(x)$ be the "actual" and "coarse" ground truth CDFs in the horizontal direction. Also, let $L=\left\lfloor\frac{M}{N}\right\rfloor$ If the goal object occurs " $k$ " times in the horizontal direction, then,</p>
<p>$$
\sup <em X="X">{x}\left|F</em>\right)
$$}(x)-C_{X}(x)\right| \leq \frac{1}{k}\left(1-\frac{1}{L</p>
<p>A similar result holds in the vertical direction. The bound above suggests that if the goal object occurs more frequently (smaller $\frac{1}{k}$ ), then a coarser $L$ (larger $1-\frac{1}{L}$ ) is tolerable. On the other hand, if the goal object occurs very infrequently (larger $\frac{1}{k}$ ), then a coarse $M$ (larger $1-\frac{1}{L}$ ) will result in $F_{X}$ and $C_{X}$ becoming too different in the worst case. Thus, it is desirable that practitioners choose $L$ (and in turn, $N$ ) based on the frequency of their goal objects, on average. Furthermore, a search policy with adaptive grid sizing should be explored as future work.</p>
<h2>A. 5 PSEUDOCODE FOR THE DETERMINISTIC POLICY</h2>
<p>Following the discussion of Section 4.4, let $\left[\left(\right.\right.$ obj $<em 1="1">{1}$, action $\left.\left.</em>\right), \ldots,\left(\right.$ obj $<em k="k">{k}$, action $\left.\left.</em>\right)\right]$ be the list of subtasks, where the current subtask is $\left(\right.$ obj $<em i="i">{i}$, action $\left.\left.</em>$ is in the current frame, the agents decides to take action $}\right)$. If $o b j_{i}$ is observed in the current semantic map, the closest $o b j_{i}$ is selected as the goal to navigate; otherwise, the sample from the semantic search policy is chosen as the goal (Section 4.3). The agent then navigates towards the closest $o b j_{i}$ via the Fast Marching Method (Sethian, 1996). Once the stop distance is reached, the agent rotates 8 times to the left (at camera horizon $0,45,90, \ldots$ ) until $o b j_{i}$ is detected in egocentric vision. Once $o b j_{i<em i="i">{i}$ if two criteria are met: whether $o b j</em>$ and fails, the agent "moves backwards" and the map gets updated.
Below, we present a pseudocode for the deterministic navigation/ interaction policy. We first present explanations of some terms.}$ is in the "center" of the frame, or whether the minimum depth towards $o b j_{i}$ is in visibility distance of 1.5 meters). Otherwise, the agent "sidesteps" to keep $o b j_{i}$ in the center frame or continue rotating to the left with horizon $0 / 45$ until $o b j_{i}$ is seen within visibility distance. If the agent executes action $_{i</p>
<ul>
<li>"visible" means that an object is in the current RGB frame, and minimum (predicted) depth from the agent to it is less than or equal to 1.5 meters (which is set by ALFRED).</li>
<li>"FMM" is Fast Marching Method (Sethian, 1996).</li>
<li>We assume that a new RGB frame is given as time_step $\leftarrow$ time_step +1</li>
<li>MoveBehind, SideStep, RotateBack are not actions in ALFRED; they are defined by us.</li>
</ul>
<p>MoveBehind - RotateRight, MoveAhead, RotateLeft
SideStep - RotateRight/Left, MoveAhead, RotateLeft/Right
RotateBack - RotateRight, RotateRight</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Navigation</span><span class="o">/</span><span class="nt">interaction</span><span class="w"> </span><span class="nt">algorithm</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">episode</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">List</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">goal</span><span class="w"> </span><span class="nt">tuples</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="o">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">o</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="nx">j_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="nx">t</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="nx">o</span><span class="w"> </span><span class="nx">n_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="o">\</span><span class="nx">ldots</span><span class="p">,</span><span class="o">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">o</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="nx">j_</span><span class="p">{</span><span class="nx">k</span><span class="p">},</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="nx">t</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="nx">o</span><span class="w"> </span><span class="nx">n_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="p">)</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="nt">Task</span><span class="w"> </span><span class="nt">Success</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="nt">True</span><span class="o">/</span><span class="nt">False</span>
<span class="w">    </span><span class="nt">timestep</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">goal_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">semantic</span><span class="w"> </span><span class="nt">search</span><span class="w"> </span><span class="nt">policy</span>
<span class="w">    </span><span class="nt">execute_interaction</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">    </span><span class="nt">stop</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">    </span><span class="nt">subtask_success</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">    </span><span class="nt">move_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">task_success</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{goal_pointer</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{goal_pointer</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="nt">goal_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leq</span><span class="w"> </span><span class="nt">k</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">while</span><span class="w"> </span><span class="nt">timestep</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leq</span><span class="w"> </span><span class="nt">1000</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">update</span><span class="w"> </span><span class="nt">semantic</span><span class="w"> </span><span class="nt">map</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="nt">stop</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">if</span><span class="w"> </span><span class="nt">execute_interaction</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                    </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">action</span>
<span class="w">                    </span><span class="nt">if</span><span class="w"> </span><span class="nt">action</span><span class="o">,</span><span class="w"> </span><span class="nt">done</span><span class="w"> </span><span class="nt">successfully</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                        </span><span class="nt">subtask_success</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">True</span>
<span class="w">            </span><span class="nt">else</span>
<span class="w">                </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">visible</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">current</span><span class="w"> </span><span class="nt">frame</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">center</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">frame</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                    </span><span class="nt">execute_interaction</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">                    </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">LookDown</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">0</span><span class="o">^</span><span class="p">{</span><span class="err">\circ</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">void</span><span class="w"> </span><span class="nt">action</span>
<span class="w">                    </span><span class="nt">else</span>
<span class="w">                    </span><span class="nt">if</span><span class="w"> </span><span class="nt">previous</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="nt">was</span><span class="w"> </span><span class="nt">OpenObject</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">CloseObject</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">subtask_success</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                    </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">MoveBehind</span>
<span class="w">                    </span><span class="nt">else</span><span class="w"> </span><span class="nt">if</span><span class="w"> </span><span class="nt">previous</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="nt">was</span><span class="w"> </span><span class="nt">PutObject</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">subtask_success</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                    </span><span class="nt">Re-dilate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">semantic</span><span class="w"> </span><span class="nt">map</span>
<span class="w">                    </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">RotateBack</span>
<span class="w">                    </span><span class="nt">else</span><span class="w"> </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">visible</span><span class="w"> </span><span class="nt">but</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">center</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">frame</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                    </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">SideStep</span>
<span class="w">                    </span><span class="nt">else</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Rotate</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">camera</span><span class="w"> </span><span class="nt">horizons</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">0</span><span class="o">^</span><span class="p">{</span><span class="err">\circ</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">45</span><span class="o">^</span><span class="p">{</span><span class="err">\circ</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">until</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">visible</span>
<span class="w">                    </span><span class="nt">if</span><span class="w"> </span><span class="nt">move_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(&lt;</span><span class="nt">4</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                        </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">RotateLeft</span>
<span class="w">                    </span><span class="nt">else</span>
<span class="w">                        </span><span class="nt">if</span><span class="w"> </span><span class="nt">move_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(==</span><span class="nt">4</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                            </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">LookDown</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">45</span><span class="o">^</span><span class="p">{</span><span class="err">\circ</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">                    </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">RotateLeft</span>
<span class="w">                    </span><span class="nt">move_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">move_pointer</span><span class="w"> </span><span class="o">+</span><span class="nt">1</span><span class="w"> </span><span class="o">(</span><span class="nt">mod</span><span class="w"> </span><span class="nt">8</span><span class="o">)</span>
<span class="w">            </span><span class="nt">else</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">found</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="o">(</span><span class="nt">RotateLeft</span><span class="o">,</span><span class="w"> </span><span class="nt">RotateRight</span><span class="o">,</span><span class="w"> </span><span class="nt">MoveAhead</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">FMM</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">else</span>
<span class="w">                </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">closest</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">semantic</span><span class="w"> </span><span class="nt">map</span>
<span class="w">                    </span><span class="nt">while</span><span class="w"> </span><span class="nt">distance</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="w"> </span><span class="err">\</span><span class="nt">geq</span><span class="w"> </span><span class="nt">0</span><span class="p">.</span><span class="nc">65</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">meters</span><span class="w"> </span><span class="nt">do</span>
<span class="w">                        </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="o">(</span><span class="nt">RotateLeft</span><span class="o">,</span><span class="w"> </span><span class="nt">RotateRight</span><span class="o">,</span><span class="w"> </span><span class="nt">MoveAhead</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">FMM</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="err">\</span><span class="o">)</span>
<span class="w">                    </span><span class="nt">if</span><span class="w"> </span><span class="nt">distance</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="o">&lt;</span><span class="nt">0</span><span class="p">.</span><span class="nc">65</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">meters</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                        </span><span class="nt">stop</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">True</span>
<span class="w">            </span><span class="nt">timestep</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">timestep</span><span class="w"> </span><span class="o">+</span><span class="nt">1</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="nt">timestep</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">equiv</span><span class="w"> </span><span class="nt">0</span><span class="o">(</span><span class="err">\</span><span class="nt">bmod</span><span class="w"> </span><span class="nt">25</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">semantic</span><span class="w"> </span><span class="nt">search</span><span class="w"> </span><span class="nt">policy</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="nt">subtask_success</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">goal_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">goal_pointer</span><span class="w"> </span><span class="o">+</span><span class="nt">1</span>
<span class="w">                    </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">o</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">j_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{goal_pointer</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{goal_pointer</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">                    </span><span class="nt">move_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span>
<span class="w">                    </span><span class="nt">execute_interaction</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">                    </span><span class="nt">stop</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">                    </span><span class="nt">subtask_success</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">False</span>
<span class="w">                    </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">semantic</span><span class="w"> </span><span class="nt">search</span><span class="w"> </span><span class="nt">policy</span>
<span class="w">                    </span><span class="nt">break</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="nt">goal_pointer</span><span class="w"> </span><span class="err">\</span><span class="o">(==</span><span class="nt">k</span><span class="o">+</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">            </span><span class="nt">task_success</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">True</span>
</code></pre></div>

<h1>A. 6 MORE Explanations on Table 3</h1>
<p>Table 3 shows common error modes and the percentage they take out of all failed episodes, with regards to SR. More specifically, it is showing the distribution of episodes into exactly one error mode, out of the $79.9 \%$ of all "Val Unseen" episodes that have failed (the episodes not in the 20.10\% of Table 2). The common error modes are failures in (1) locating the subgoal object (due to the small field of view, imperfect segmentation, ineffective exploration), (2) locating the subgoal object because it is in a closed receptacle (cabinet, drawer, etc), (3) interaction (due to object being too far or not in field of view, bad segmentation mask), (4) navigation (collisions), (5) correctly processing language instructions, (6) others, such as the deterministic policy repeating a loop of actions from depth/ segmentation failures and 10 failed actions accruing from a mixture of different errors. These errors occur in the order of (5), (1)/ (2), (3), (4) in an episode, since the LP module operates in the beginning and the object has to be first localized to be interacted with, etc. If an episode ended with errors in multiple categories, it was classified as an example of an "earlier" error in making Table 3. For example, if the language processing module made an error and later there were also 10 collisions, this episode shown as a case of error (5) in Table 3.</p>
<h2>A. 7 Assignments of Rooms into "Large" and "Small" in Valid Unseen</h2>
<p>There are 4 distinct scenes in Valid Unseen (one kitchen scene, one living room, one bed room, one bathroom). The kitchen (Large) has a significantly larger area than all the others (Small).</p>
<h2>A. 8 Protocols for Reproducing the Semantic Policy</h2>
<p>The primary result in Table 1 is from architecture tuning of the language processing, the semantic mapping, and the semantic search policy modules on the development data (validation unseen). Reviewers correctly noted that it is possible random seeds will also effect performance so the model was retrained four additional times and test results are reported here. Since components of the language processing and the semantic mapping module were trained from pre-trained weights, we report the performance of FILM with semantic search policy trained from different seeds.</p>
<p>The improvement by the semantic policy as shown in Table 1 is reproducible across multiple seeds. Table 8 shows results on Tests Unseen with semantic policy trained with different starting seeds (where SEED 1 denotes that the policy was trained with torch.manual_seed (1)). With learning rate of 0.001 and evaluation of every 50 steps, the model with the lowest test loss subject to train loss $&lt;0.62$ was chosen. The exact code and commands can be found here: https://github.com/soyeonm/FILM#train-the-semantic-policy.</p>
<p>Table 8: Results of FILM reproduced across different starting seeds of the semantic policy. The $\pm$ error bar in the AvG. row denotes the sample variance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Tests Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PLWGC</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">PLWSR</td>
<td style="text-align: center;">SR</td>
</tr>
<tr>
<td style="text-align: center;">Low-level + High-level Instructions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TABLE 1</td>
<td style="text-align: center;">15.06</td>
<td style="text-align: center;">36.37</td>
<td style="text-align: center;">10.55</td>
<td style="text-align: center;">26.49</td>
</tr>
<tr>
<td style="text-align: center;">SEED 1</td>
<td style="text-align: center;">15.12</td>
<td style="text-align: center;">38.55</td>
<td style="text-align: center;">11.34</td>
<td style="text-align: center;">27.86</td>
</tr>
<tr>
<td style="text-align: center;">SEED 2</td>
<td style="text-align: center;">13.82</td>
<td style="text-align: center;">36.58</td>
<td style="text-align: center;">10.13</td>
<td style="text-align: center;">25.96</td>
</tr>
<tr>
<td style="text-align: center;">SEED 3</td>
<td style="text-align: center;">10.47</td>
<td style="text-align: center;">37.12</td>
<td style="text-align: center;">14.05</td>
<td style="text-align: center;">25.64</td>
</tr>
<tr>
<td style="text-align: center;">SEED 4</td>
<td style="text-align: center;">14.22</td>
<td style="text-align: center;">37.37</td>
<td style="text-align: center;">10.69</td>
<td style="text-align: center;">26.62</td>
</tr>
<tr>
<td style="text-align: center;">AvG.</td>
<td style="text-align: center;">13.74</td>
<td style="text-align: center;">37.20</td>
<td style="text-align: center;">11.352</td>
<td style="text-align: center;">$26.51 \pm 0.58$</td>
</tr>
<tr>
<td style="text-align: center;">High-level Instruction Only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TABLE 1</td>
<td style="text-align: center;">13.13</td>
<td style="text-align: center;">34.75</td>
<td style="text-align: center;">9.67</td>
<td style="text-align: center;">24.46</td>
</tr>
<tr>
<td style="text-align: center;">SEED 1</td>
<td style="text-align: center;">14.05</td>
<td style="text-align: center;">36.75</td>
<td style="text-align: center;">10.47</td>
<td style="text-align: center;">25.51</td>
</tr>
<tr>
<td style="text-align: center;">SEED 2</td>
<td style="text-align: center;">12.60</td>
<td style="text-align: center;">34.59</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">23.48</td>
</tr>
<tr>
<td style="text-align: center;">SEED 3</td>
<td style="text-align: center;">12.86</td>
<td style="text-align: center;">35.02</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">23.68</td>
</tr>
<tr>
<td style="text-align: center;">SEED 4</td>
<td style="text-align: center;">13.61</td>
<td style="text-align: center;">36.10</td>
<td style="text-align: center;">10.10</td>
<td style="text-align: center;">25.18</td>
</tr>
<tr>
<td style="text-align: center;">AVG.</td>
<td style="text-align: center;">13.25</td>
<td style="text-align: center;">35.44</td>
<td style="text-align: center;">9.71</td>
<td style="text-align: center;">$24.87 \pm 0.64$</td>
</tr>
</tbody>
</table>
<h2>A. 9 A Language Processing module without the template assumption</h2>
<p>The second paragraph of section 4.1 explains the template assumption, with the tasks belonging to one of the 7 types. For direct comparison with existing methods that do not take direct advantage of this assumption, we trained a new Language Processing module that does not make use of templates</p>
<p>but makes use of the subtasks sequences annotations ALFRED provides. ${ }^{5}$ Fine-tuning a pre-trained BART (Lewis et al., 2020) model, we directly learned a mapping from a high-level instruction to a sequence of subtasks (e.g. "Drop a clean pan on the table" $\rightarrow$ "(PickupObject, Pan), (PutObject, Sink), ..."). Without any assumption on the structure of the input and the output, this model takes a sequence of tokens as input and outputs a sequence of tokens. With the new LP module, we obtained SR of $18.03 \%$ on valid unseen, which is a slight drop compared to our original $20.10 \%$, indicating that templates are only marginally helpful in performance.</p>
<p>For future research, we believe templates should be used instead of subtasks annotations, since they are much cheaper to obtain in naturalistic settings. In this work, we created the 7 templates (one for each type) by writing down an intuitive canonical set of interactions to successfully perform the task. To do so, we looked at just 7 episodes in the training set and spent less than 20 minutes creating them; these cheaply obtained templates cover all 20,000 training episodes. Even to train an agent to perform more complex tasks, it is more realistic to use templates than assume sub-task annotations.</p>
<p>On the other hand, our findings simultaneously suggest the need for a better program synthesis method from instructions to subtask sequences, for general purpose instruction following not bound to certain "types" of instructions.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Existing works(Blukis et al., 2021; Kim et al., 2021; Zhang \&amp; Chai, 2021; Pashevich et al., 2021) use subtask sequence annotations (or expert trajectories that contain the subtask annotations) as well.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
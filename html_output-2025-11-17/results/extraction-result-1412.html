<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1412 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1412</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1412</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2" target="_blank">CURL: Contrastive Unsupervised Representations for Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features and is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features.</p>
                <p><strong>Paper Abstract:</strong> We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1412.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1412.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (latent imagination / latent world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL algorithm that learns a latent-space dynamics model from pixels and performs planning or policy/value learning by imagining trajectories in that latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns an encoding from pixels to a compact latent state and a latent dynamics (forward) model; uses the learned latent model to generate (imagine) trajectories for planning/policy/value updates rather than predicting raw pixels directly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control from pixels (DeepMind Control Suite / DMControl)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Assessed indirectly via downstream task performance and sample-efficiency (episode returns at fixed environment/interaction step budgets such as DMControl100k/500k); the paper does not report predictive log-likelihoods or MSE of next-frame prediction for Dreamer within this text.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in comparisons: CURL attains 1.9x median higher performance than Dreamer at 100k environment steps on DMControl (paper's DMControl100k benchmark) and is reported as on average 4.5x more data-efficient than Dreamer (Figure 6); absolute Dreamer episode-return numbers for specific environments appear in Table 1 (e.g., Dreamer FingerSpin: 796 ± 183 at 500k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper; Dreamer is treated as a learned latent neural model (black-box) with no interpretability analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper (no latent-space visualization or disentanglement evaluation reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified numerically in this paper; discussed qualitatively as a more complex pipeline relative to the proposed CURL approach because learning/planning in latent space typically introduces additional architectural components and hyperparameters (authors note extra complexity of modeling the future in latent space).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>CURL (contrastive auxiliary + model-free RL) is more sample-efficient than Dreamer on the DMControl benchmarks reported (1.9x median higher at 100k; 4.5x data-efficiency advantage reported), indicating Dreamer uses more environment interactions to reach the same task scores in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Dreamer provides competitive model-based baseline scores shown in Table 1, but on median across the tested DMControl tasks it underperforms CURL at early-step benchmarks (100k); on 500k steps Dreamer scores are reported per-environment in Table 1 and are generally lower than CURL on the majority of the six benchmarked environments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Within this paper's empirical comparisons, high-fidelity latent predictive modeling (Dreamer) yields improved sample-efficiency over naive pixel-based model-free baselines but is outperformed by the simpler contrastive representation approach (CURL) on early-step sample-efficiency benchmarks; thus, accurate latent prediction does not guarantee superior early sample-efficiency in these settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Authors emphasize a trade-off: latent world models (like Dreamer) can provide planning advantages but increase pipeline complexity and hyperparameter burden; simpler contrastive representation learning can achieve comparable or better sample-efficiency with less architectural overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Dreamer-style design choices referenced: VAE-like encoder/decoder or latent encoder plus learned latent dynamics and an imagination/planning procedure operating in latent space (details are in the Dreamer citation, not expanded in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared here as a leading latent world-model baseline; CURL outperforms Dreamer at 100k steps (1.9x median) and is more data-efficient (4.5x) for the DMControl tasks evaluated, indicating that contrastive latent representations with model-free RL can match or exceed Dreamer's sample-efficiency in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>The paper does not prescribe an optimal configuration for Dreamer; instead it argues that for the evaluated sample-efficiency regime, simpler contrastive auxiliary objectives (as used by CURL) may be preferable due to lower complexity and comparable or better empirical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1412.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1412.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (latent dynamics planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based approach that learns a latent-space model from pixels and performs planning within that latent representation for control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a compact latent representation (via a VAE-like encoder) and a latent transition model used for planning/control rather than predicting high-dimensional pixels directly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control from pixels (DMControl suite / planning from pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Evaluated via downstream task episode returns and sample-efficiency at fixed environment-step budgets; no explicit predictive MSE or likelihood values are cited in this paper's discussion of PlaNet.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Presented as a comparison baseline in Table 1; PlaNet scores on DMControl environments are listed (e.g., finger-spin 561 ± 284 at 500k steps) and generally lower than CURL on the majority of benchmarked tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here; PlaNet is treated as a learned latent neural model without interpretability analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not numerically specified here; PlaNet-like latent-model pipelines are characterized as more complex than the minimal CURL pipeline because they require learning dynamics and planning components.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>CURL outperforms PlaNet on the DMControl tasks reported (see Table 1), implying that PlaNet requires more samples to reach similar task returns in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>PlaNet is a strong model-based baseline but obtains lower returns than CURL across the highlighted DMControl tasks in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PlaNet's latent predictive modeling provides planning capability, but in these experiments contrastive representation learning combined with model-free RL (CURL) achieves superior sample-efficiency and final task returns in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Learning and planning with a latent dynamics model can improve asymptotic control, but requires extra modeling complexity and hyperparameter tuning compared to the simpler contrastive auxiliary approach.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Latent encoder (VAE-like) + latent dynamics model + planning algorithm (as in PlaNet's design described in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared unfavorably (on sample-efficiency metrics in these experiments) with CURL; PlaNet sits alongside other latent-model planners like Dreamer but underperforms CURL on the reported DMControl benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No explicit optimal configuration for PlaNet is recommended here; the paper's broader recommendation is that contrastive auxiliary objectives may be a simpler path to sample-efficiency in similar tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1412.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1412.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SLAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SLAC (Stochastic Latent Actor-Critic / latent-space world model on VAE features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that learns a latent-space world model (often built on top of VAE features) and uses it within actor-critic frameworks for control from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SLAC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a latent representation via a VAE and trains a stochastic latent dynamics model; values/policies are learned on top of that latent representation (combining latent modeling with actor-critic methods).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control from pixels (DMControl suite)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Evaluated via downstream episode returns and sample-efficiency; paper reports SLAC variants (v1 and v2) with different gradient update ratios as baselines in Table 1 and Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>SLAC reported scores appear in Table 1 and Table 5; e.g., SLACv1 cheetah-run 640 ± 19 at 500k steps; when SLAC is run with more gradient updates per agent step (SLACv2), some scores improve but at higher compute cost (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here; SLAC is treated as a latent neural model without interpretability evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Authors note increasing the gradient updates per agent step (e.g., SLACv2 with 3:1) increases compute and wall-clock time; SLACv2 shows improved performance at the cost of 'significant compute and wall-clock time overhead' (Table 5 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>CURL often outperforms SLAC (v1) in the 1:1 gradient-update regime on the evaluated DMControl tasks; SLACv2 (3:1) can close gaps or outperform CURL on some tasks but demands more compute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>SLAC obtains competitive scores on DMControl; in the paper, CURL achieves state-of-the-art over many baselines and compares to SLAC variants in both 1:1 and 3:1 update regimes (Table 1 and Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SLAC's latent modeling is useful for sample-efficiency, but improving SLAC's performance via increased optimization (more gradients per env step) has a computational cost; CURL achieves high sample-efficiency with fewer such computational trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between improved performance (via more optimization, model complexity) and computational cost; SLACv2 improves some results but requires much greater compute and wall-clock time than the 1:1 baselines and CURL.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VAE-based latent encoding, stochastic latent dynamics, actor-critic learning on latent features; number of gradient updates per agent step is a tunable design choice that impacts compute vs performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically: CURL outperforms SLACv1 at 1:1 updates on many tasks; SLACv2 (3:1) can outperform CURL on some environments but at significant compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe an optimal SLAC configuration; it highlights that increasing gradient-update ratio can help but increases computational cost, and that CURL achieves strong sample-efficiency without these extra costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1412.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1412.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPLe (Simulated Policy Learning / model-based RL for Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL approach that learns an explicit world model (pixel or latent) and uses it in a Dyna-style fashion to generate fictive rollouts for policy learning, targeted at sample-efficiency on Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-based reinforcement learning for atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns an environment model (original SimPLe used video-prediction style models) and uses the model to generate additional training data / perform Dyna-style learning to improve sample-efficiency on Atari games.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit/world-model-based Dyna-style model (pixel/latent predictive model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (discrete control from pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Evaluated by downstream task performance (Atari100k benchmark, human-normalized scores) and sample-efficiency; predictive fidelity metrics for the world model itself are not reported in this paper's description.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the Atari100k comparisons, CURL achieves a median human-normalized score (HNS) of 17.5% vs SimPLe's 14.4% and Efficient Rainbow's 16.1%; averaged across 26 games, CURL improves over Efficient Rainbow by 1.3x and shows median improvements over SimPLe of ~1.2x (paper statement).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here; SimPLe is treated as a learned neural predictive model without interpretability analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified numerically in this paper; model-based Dyna-style approaches like SimPLe are described as more complex, requiring modeling the environment and using model rollouts which can increase implementation complexity and compute requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>CURL (contrastive + model-free) improves median performance over SimPLe on the Atari100k benchmark according to this paper (median improvement ~1.2x), indicating better sample-efficiency for many games in this specific benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>SimPLe is a competitive model-based approach on Atari100k but is outperformed in median HNS by CURL in the reported comparisons (SimPLe HNS 14.4% vs CURL 17.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Model-based rollouts can improve data-efficiency, but in these Atari100k experiments simple contrastive representation learning plus a model-free learner (Efficient Rainbow + CURL) yielded higher median performance across games, suggesting that the utility of explicit world models depends on the task and implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Model-based Dyna approaches can be sample-efficient but introduce complexity; CURL demonstrates that a simpler auxiliary representation objective can provide comparable or superior sample-efficiency in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Dyna-style use of learned predictive model (pixel/latent) to generate fictive rollouts for policy learning, choice of predictive target (pixels vs latent), and model capacity are core design variables (details in SimPLe citation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to CURL on Atari100k, SimPLe is outperformed on median HNS; CURL improves over Efficient Rainbow on 19/26 games and achieves higher median than SimPLe in this paper's reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No optimal SimPLe configuration is recommended here; the paper instead highlights that contrastive feature learning is a simple alternative to model-based rollout generation for improving sample-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1412.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1412.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of approaches popularized by Ha & Schmidhuber that build compact predictive models of environments (often VAE + RNN) and use them for planning or policy learning (imagination-based control).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (Ha & Schmidhuber style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Typically comprises a VAE-based visual encoder to compress pixels and an RNN-style dynamics or controller in latent space; the learned model can be used for policy search/planning by running the controller or planning inside the learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE + recurrent dynamics / neural simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General simulated control tasks and games (e.g., Atari-style and other simulated environments); used as an example of world-model-based RL in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Described generically; this paper references world models as predictive models used for planning but does not report specific predictive-fidelity metrics (e.g., pixel-MSE or latent prediction accuracy) for Ha & Schmidhuber's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper; referenced as prior work demonstrating planning through learned world models successfully, but no numeric fidelity values are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper; world models are treated as neural latent predictors and no interpretability evaluation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Qualitatively described as adding complexity; learning world models and planning through them introduces extra components and hyperparameters compared to the simpler contrastive auxiliary pipeline proposed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper positions world-model approaches as a mainstream method to improve sample-efficiency historically, but reports that CURL (contrastive auxiliary) outperforms many world-model baselines on early-sample-efficiency benchmarks in DMControl/Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>World-model methods have been successful in prior work (cited) but the present paper's empirical results show that contrastive representation learning can match or exceed their sample-efficiency in the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Learning an explicit predictive model can enable planning and sample-efficiency gains, but for the sample-limited early-learning regime studied in this paper, reward-agnostic contrastive representations provide a simpler alternative with competitive or superior utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Main trade-off: model-based world models can enable planning but at cost of increased architectural complexity, tuning effort, and potentially higher computational requirements; contrastive methods trade predictive modeling for representation simplicity and lower implementation overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Typical choices: compress pixels via VAE, choose latent-dynamics architecture (RNN / stochastic), and choose whether to plan in latent space or use model rollouts for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared at a high level: world models provide one route to sample-efficiency (planning/imagined rollouts), while CURL shows that contrastive auxiliary representation learning + model-free RL can reach or surpass world-model baselines in sample-efficiency on the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>This paper does not prescribe optimal configurations for 'World Models' broadly; instead it advocates that for many sample-efficiency tasks a simpler contrastive auxiliary objective combined with off-policy RL is an effective design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1412.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1412.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent-space world models (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent-space world models (general class)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A broad class of model-based RL approaches that learn a compact latent representation and a predictive dynamics model in that latent space to support planning or synthetic rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Latent-space world models (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compress observations (pixels) into a low-dimensional latent via an encoder (VAE-like or contrastive encoder), learn a latent transition model (deterministic or stochastic) and optionally a reward/prediction decoder; planning or rollouts are performed in latent space to train policies or value functions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (class)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Control from pixels (Atari, DMControl), planning and data-efficient RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>In the paper, their effectiveness is evaluated primarily via downstream task performance (episode returns) and sample-efficiency; the paper notes that many prior works also measure reconstruction/prediction losses but does not list specific predictive metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Performance varies by instantiation (PlaNet, Dreamer, SLAC, SimPLe). In the benchmarks reported here, latent-space world models are often outperformed by CURL on early-step sample-efficiency metrics (e.g., Dreamer baseline vs CURL), though some variants (with more compute or optimization) can be competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not addressed in this paper for the latent-model class; generally treated as black-box learned latent predictors without interpretability analyses here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Qualitatively higher than the minimal contrastive auxiliary approach; extra components (dynamics model, decoder, planning procedure, additional hyperparameters) increase implementation and compute burden; increased optimization (more gradient updates) increases wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>CURL (contrastive auxiliary + model-free RL) is presented as a simpler, computationally lighter alternative that achieves superior or comparable sample-efficiency in the evaluated settings; specific speedups cited in this paper are sample-efficiency numbers (e.g., 1.9x and 4.5x vs Dreamer), not raw compute-time numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Varies by instance: some latent-space models achieve good asymptotic performance and sample-efficiency in prior work; in the experiments of this paper, CURL outperforms many latent-space model baselines on sample-efficiency benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Latent-space modeling can support planning and synthetic data generation, which can be highly useful for sample-limited problems; however, this paper's results show that learning strong semantic representations via contrastive methods can provide equivalent or better utility with less modeling overhead in many standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Key trade-offs: latent models can improve sample-efficiency but require more design and compute; simpler auxiliary objectives (contrastive learning) may yield similar or better sample-efficiency with less complexity, though latent models may still be advantageous in other settings (e.g., long-horizon planning, explicit dynamics requirements).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Encoder architecture (VAE/contrastive), latent dimension size, deterministic vs stochastic dynamics, whether to learn decoders (reconstruction), planning method (MPC, latent imagination) and optimization regimes (gradient updates per env step) affect fidelity, compute and utility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Latent-space world models are explicitly compared (PlaNet, Dreamer, SLAC, SimPLe) and found in these benchmarks to be generally outperformed by CURL in early-sample-efficiency; some latent-model variants with heavier optimization/runtime can match or exceed CURL on selected tasks but at higher computational expense.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>The paper does not define a single optimal latent-world-model configuration; its empirical recommendation is that, for many sample-efficient pixel-control problems and for ease of reproducibility, contrastive representation learning is a preferable design choice unless the application specifically requires explicit predictive/world-model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Model-based reinforcement learning for atari <em>(Rating: 2)</em></li>
                <li>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model <em>(Rating: 2)</em></li>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 1)</em></li>
                <li>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming <em>(Rating: 1)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1412",
    "paper_id": "paper-9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Dreamer",
            "name_full": "Dreamer (latent imagination / latent world model)",
            "brief_description": "A model-based RL algorithm that learns a latent-space dynamics model from pixels and performs planning or policy/value learning by imagining trajectories in that latent space.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer",
            "model_description": "Learns an encoding from pixels to a compact latent state and a latent dynamics (forward) model; uses the learned latent model to generate (imagine) trajectories for planning/policy/value updates rather than predicting raw pixels directly.",
            "model_type": "latent world model",
            "task_domain": "Continuous control from pixels (DeepMind Control Suite / DMControl)",
            "fidelity_metric": "Assessed indirectly via downstream task performance and sample-efficiency (episode returns at fixed environment/interaction step budgets such as DMControl100k/500k); the paper does not report predictive log-likelihoods or MSE of next-frame prediction for Dreamer within this text.",
            "fidelity_performance": "Reported in comparisons: CURL attains 1.9x median higher performance than Dreamer at 100k environment steps on DMControl (paper's DMControl100k benchmark) and is reported as on average 4.5x more data-efficient than Dreamer (Figure 6); absolute Dreamer episode-return numbers for specific environments appear in Table 1 (e.g., Dreamer FingerSpin: 796 ± 183 at 500k steps).",
            "interpretability_assessment": "Not discussed in this paper; Dreamer is treated as a learned latent neural model (black-box) with no interpretability analysis provided here.",
            "interpretability_method": "None mentioned in this paper (no latent-space visualization or disentanglement evaluation reported here).",
            "computational_cost": "Not quantified numerically in this paper; discussed qualitatively as a more complex pipeline relative to the proposed CURL approach because learning/planning in latent space typically introduces additional architectural components and hyperparameters (authors note extra complexity of modeling the future in latent space).",
            "efficiency_comparison": "CURL (contrastive auxiliary + model-free RL) is more sample-efficient than Dreamer on the DMControl benchmarks reported (1.9x median higher at 100k; 4.5x data-efficiency advantage reported), indicating Dreamer uses more environment interactions to reach the same task scores in these experiments.",
            "task_performance": "Dreamer provides competitive model-based baseline scores shown in Table 1, but on median across the tested DMControl tasks it underperforms CURL at early-step benchmarks (100k); on 500k steps Dreamer scores are reported per-environment in Table 1 and are generally lower than CURL on the majority of the six benchmarked environments.",
            "task_utility_analysis": "Within this paper's empirical comparisons, high-fidelity latent predictive modeling (Dreamer) yields improved sample-efficiency over naive pixel-based model-free baselines but is outperformed by the simpler contrastive representation approach (CURL) on early-step sample-efficiency benchmarks; thus, accurate latent prediction does not guarantee superior early sample-efficiency in these settings.",
            "tradeoffs_observed": "Authors emphasize a trade-off: latent world models (like Dreamer) can provide planning advantages but increase pipeline complexity and hyperparameter burden; simpler contrastive representation learning can achieve comparable or better sample-efficiency with less architectural overhead.",
            "design_choices": "Dreamer-style design choices referenced: VAE-like encoder/decoder or latent encoder plus learned latent dynamics and an imagination/planning procedure operating in latent space (details are in the Dreamer citation, not expanded in this paper).",
            "comparison_to_alternatives": "Compared here as a leading latent world-model baseline; CURL outperforms Dreamer at 100k steps (1.9x median) and is more data-efficient (4.5x) for the DMControl tasks evaluated, indicating that contrastive latent representations with model-free RL can match or exceed Dreamer's sample-efficiency in these benchmarks.",
            "optimal_configuration": "The paper does not prescribe an optimal configuration for Dreamer; instead it argues that for the evaluated sample-efficiency regime, simpler contrastive auxiliary objectives (as used by CURL) may be preferable due to lower complexity and comparable or better empirical performance.",
            "uuid": "e1412.0",
            "source_info": {
                "paper_title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "PlaNet",
            "name_full": "PlaNet (latent dynamics planning)",
            "brief_description": "A model-based approach that learns a latent-space model from pixels and performs planning within that latent representation for control.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "mention",
            "model_name": "PlaNet",
            "model_description": "Learns a compact latent representation (via a VAE-like encoder) and a latent transition model used for planning/control rather than predicting high-dimensional pixels directly.",
            "model_type": "latent world model",
            "task_domain": "Continuous control from pixels (DMControl suite / planning from pixels)",
            "fidelity_metric": "Evaluated via downstream task episode returns and sample-efficiency at fixed environment-step budgets; no explicit predictive MSE or likelihood values are cited in this paper's discussion of PlaNet.",
            "fidelity_performance": "Presented as a comparison baseline in Table 1; PlaNet scores on DMControl environments are listed (e.g., finger-spin 561 ± 284 at 500k steps) and generally lower than CURL on the majority of benchmarked tasks.",
            "interpretability_assessment": "Not discussed here; PlaNet is treated as a learned latent neural model without interpretability analysis in this paper.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Not numerically specified here; PlaNet-like latent-model pipelines are characterized as more complex than the minimal CURL pipeline because they require learning dynamics and planning components.",
            "efficiency_comparison": "CURL outperforms PlaNet on the DMControl tasks reported (see Table 1), implying that PlaNet requires more samples to reach similar task returns in these benchmarks.",
            "task_performance": "PlaNet is a strong model-based baseline but obtains lower returns than CURL across the highlighted DMControl tasks in the paper's tables.",
            "task_utility_analysis": "PlaNet's latent predictive modeling provides planning capability, but in these experiments contrastive representation learning combined with model-free RL (CURL) achieves superior sample-efficiency and final task returns in many cases.",
            "tradeoffs_observed": "Learning and planning with a latent dynamics model can improve asymptotic control, but requires extra modeling complexity and hyperparameter tuning compared to the simpler contrastive auxiliary approach.",
            "design_choices": "Latent encoder (VAE-like) + latent dynamics model + planning algorithm (as in PlaNet's design described in referenced work).",
            "comparison_to_alternatives": "Compared unfavorably (on sample-efficiency metrics in these experiments) with CURL; PlaNet sits alongside other latent-model planners like Dreamer but underperforms CURL on the reported DMControl benchmarks.",
            "optimal_configuration": "No explicit optimal configuration for PlaNet is recommended here; the paper's broader recommendation is that contrastive auxiliary objectives may be a simpler path to sample-efficiency in similar tasks.",
            "uuid": "e1412.1",
            "source_info": {
                "paper_title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "SLAC",
            "name_full": "SLAC (Stochastic Latent Actor-Critic / latent-space world model on VAE features)",
            "brief_description": "An algorithm that learns a latent-space world model (often built on top of VAE features) and uses it within actor-critic frameworks for control from pixels.",
            "citation_title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "mention_or_use": "mention",
            "model_name": "SLAC",
            "model_description": "Learns a latent representation via a VAE and trains a stochastic latent dynamics model; values/policies are learned on top of that latent representation (combining latent modeling with actor-critic methods).",
            "model_type": "latent world model (stochastic latent dynamics)",
            "task_domain": "Continuous control from pixels (DMControl suite)",
            "fidelity_metric": "Evaluated via downstream episode returns and sample-efficiency; paper reports SLAC variants (v1 and v2) with different gradient update ratios as baselines in Table 1 and Table 5.",
            "fidelity_performance": "SLAC reported scores appear in Table 1 and Table 5; e.g., SLACv1 cheetah-run 640 ± 19 at 500k steps; when SLAC is run with more gradient updates per agent step (SLACv2), some scores improve but at higher compute cost (Table 5).",
            "interpretability_assessment": "Not discussed here; SLAC is treated as a latent neural model without interpretability evaluation in this paper.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Authors note increasing the gradient updates per agent step (e.g., SLACv2 with 3:1) increases compute and wall-clock time; SLACv2 shows improved performance at the cost of 'significant compute and wall-clock time overhead' (Table 5 discussion).",
            "efficiency_comparison": "CURL often outperforms SLAC (v1) in the 1:1 gradient-update regime on the evaluated DMControl tasks; SLACv2 (3:1) can close gaps or outperform CURL on some tasks but demands more compute.",
            "task_performance": "SLAC obtains competitive scores on DMControl; in the paper, CURL achieves state-of-the-art over many baselines and compares to SLAC variants in both 1:1 and 3:1 update regimes (Table 1 and Table 5).",
            "task_utility_analysis": "SLAC's latent modeling is useful for sample-efficiency, but improving SLAC's performance via increased optimization (more gradients per env step) has a computational cost; CURL achieves high sample-efficiency with fewer such computational trade-offs.",
            "tradeoffs_observed": "Trade-off between improved performance (via more optimization, model complexity) and computational cost; SLACv2 improves some results but requires much greater compute and wall-clock time than the 1:1 baselines and CURL.",
            "design_choices": "VAE-based latent encoding, stochastic latent dynamics, actor-critic learning on latent features; number of gradient updates per agent step is a tunable design choice that impacts compute vs performance.",
            "comparison_to_alternatives": "Compared empirically: CURL outperforms SLACv1 at 1:1 updates on many tasks; SLACv2 (3:1) can outperform CURL on some environments but at significant compute cost.",
            "optimal_configuration": "Paper does not prescribe an optimal SLAC configuration; it highlights that increasing gradient-update ratio can help but increases computational cost, and that CURL achieves strong sample-efficiency without these extra costs.",
            "uuid": "e1412.2",
            "source_info": {
                "paper_title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "SimPLe (Simulated Policy Learning / model-based RL for Atari)",
            "brief_description": "A model-based RL approach that learns an explicit world model (pixel or latent) and uses it in a Dyna-style fashion to generate fictive rollouts for policy learning, targeted at sample-efficiency on Atari.",
            "citation_title": "Model-based reinforcement learning for atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe",
            "model_description": "Learns an environment model (original SimPLe used video-prediction style models) and uses the model to generate additional training data / perform Dyna-style learning to improve sample-efficiency on Atari games.",
            "model_type": "explicit/world-model-based Dyna-style model (pixel/latent predictive model)",
            "task_domain": "Atari games (discrete control from pixels)",
            "fidelity_metric": "Evaluated by downstream task performance (Atari100k benchmark, human-normalized scores) and sample-efficiency; predictive fidelity metrics for the world model itself are not reported in this paper's description.",
            "fidelity_performance": "In the Atari100k comparisons, CURL achieves a median human-normalized score (HNS) of 17.5% vs SimPLe's 14.4% and Efficient Rainbow's 16.1%; averaged across 26 games, CURL improves over Efficient Rainbow by 1.3x and shows median improvements over SimPLe of ~1.2x (paper statement).",
            "interpretability_assessment": "Not discussed here; SimPLe is treated as a learned neural predictive model without interpretability analysis in this paper.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Not quantified numerically in this paper; model-based Dyna-style approaches like SimPLe are described as more complex, requiring modeling the environment and using model rollouts which can increase implementation complexity and compute requirements.",
            "efficiency_comparison": "CURL (contrastive + model-free) improves median performance over SimPLe on the Atari100k benchmark according to this paper (median improvement ~1.2x), indicating better sample-efficiency for many games in this specific benchmark.",
            "task_performance": "SimPLe is a competitive model-based approach on Atari100k but is outperformed in median HNS by CURL in the reported comparisons (SimPLe HNS 14.4% vs CURL 17.5%).",
            "task_utility_analysis": "Model-based rollouts can improve data-efficiency, but in these Atari100k experiments simple contrastive representation learning plus a model-free learner (Efficient Rainbow + CURL) yielded higher median performance across games, suggesting that the utility of explicit world models depends on the task and implementation details.",
            "tradeoffs_observed": "Model-based Dyna approaches can be sample-efficient but introduce complexity; CURL demonstrates that a simpler auxiliary representation objective can provide comparable or superior sample-efficiency in these benchmarks.",
            "design_choices": "Dyna-style use of learned predictive model (pixel/latent) to generate fictive rollouts for policy learning, choice of predictive target (pixels vs latent), and model capacity are core design variables (details in SimPLe citation).",
            "comparison_to_alternatives": "Compared to CURL on Atari100k, SimPLe is outperformed on median HNS; CURL improves over Efficient Rainbow on 19/26 games and achieves higher median than SimPLe in this paper's reported numbers.",
            "optimal_configuration": "No optimal SimPLe configuration is recommended here; the paper instead highlights that contrastive feature learning is a simple alternative to model-based rollout generation for improving sample-efficiency.",
            "uuid": "e1412.3",
            "source_info": {
                "paper_title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models",
            "brief_description": "A class of approaches popularized by Ha & Schmidhuber that build compact predictive models of environments (often VAE + RNN) and use them for planning or policy learning (imagination-based control).",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models (Ha & Schmidhuber style)",
            "model_description": "Typically comprises a VAE-based visual encoder to compress pixels and an RNN-style dynamics or controller in latent space; the learned model can be used for policy search/planning by running the controller or planning inside the learned model.",
            "model_type": "latent world model (VAE + recurrent dynamics / neural simulator)",
            "task_domain": "General simulated control tasks and games (e.g., Atari-style and other simulated environments); used as an example of world-model-based RL in related work.",
            "fidelity_metric": "Described generically; this paper references world models as predictive models used for planning but does not report specific predictive-fidelity metrics (e.g., pixel-MSE or latent prediction accuracy) for Ha & Schmidhuber's approach.",
            "fidelity_performance": "Not quantified in this paper; referenced as prior work demonstrating planning through learned world models successfully, but no numeric fidelity values are provided here.",
            "interpretability_assessment": "Not discussed in this paper; world models are treated as neural latent predictors and no interpretability evaluation is provided.",
            "interpretability_method": "None mentioned here.",
            "computational_cost": "Qualitatively described as adding complexity; learning world models and planning through them introduces extra components and hyperparameters compared to the simpler contrastive auxiliary pipeline proposed in this paper.",
            "efficiency_comparison": "Paper positions world-model approaches as a mainstream method to improve sample-efficiency historically, but reports that CURL (contrastive auxiliary) outperforms many world-model baselines on early-sample-efficiency benchmarks in DMControl/Atari.",
            "task_performance": "World-model methods have been successful in prior work (cited) but the present paper's empirical results show that contrastive representation learning can match or exceed their sample-efficiency in the evaluated benchmarks.",
            "task_utility_analysis": "Learning an explicit predictive model can enable planning and sample-efficiency gains, but for the sample-limited early-learning regime studied in this paper, reward-agnostic contrastive representations provide a simpler alternative with competitive or superior utility.",
            "tradeoffs_observed": "Main trade-off: model-based world models can enable planning but at cost of increased architectural complexity, tuning effort, and potentially higher computational requirements; contrastive methods trade predictive modeling for representation simplicity and lower implementation overhead.",
            "design_choices": "Typical choices: compress pixels via VAE, choose latent-dynamics architecture (RNN / stochastic), and choose whether to plan in latent space or use model rollouts for policy learning.",
            "comparison_to_alternatives": "Compared at a high level: world models provide one route to sample-efficiency (planning/imagined rollouts), while CURL shows that contrastive auxiliary representation learning + model-free RL can reach or surpass world-model baselines in sample-efficiency on the reported benchmarks.",
            "optimal_configuration": "This paper does not prescribe optimal configurations for 'World Models' broadly; instead it advocates that for many sample-efficiency tasks a simpler contrastive auxiliary objective combined with off-policy RL is an effective design.",
            "uuid": "e1412.4",
            "source_info": {
                "paper_title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Latent-space world models (generic)",
            "name_full": "Latent-space world models (general class)",
            "brief_description": "A broad class of model-based RL approaches that learn a compact latent representation and a predictive dynamics model in that latent space to support planning or synthetic rollouts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Latent-space world models (generic)",
            "model_description": "Compress observations (pixels) into a low-dimensional latent via an encoder (VAE-like or contrastive encoder), learn a latent transition model (deterministic or stochastic) and optionally a reward/prediction decoder; planning or rollouts are performed in latent space to train policies or value functions.",
            "model_type": "latent world model (class)",
            "task_domain": "Control from pixels (Atari, DMControl), planning and data-efficient RL",
            "fidelity_metric": "In the paper, their effectiveness is evaluated primarily via downstream task performance (episode returns) and sample-efficiency; the paper notes that many prior works also measure reconstruction/prediction losses but does not list specific predictive metrics here.",
            "fidelity_performance": "Performance varies by instantiation (PlaNet, Dreamer, SLAC, SimPLe). In the benchmarks reported here, latent-space world models are often outperformed by CURL on early-step sample-efficiency metrics (e.g., Dreamer baseline vs CURL), though some variants (with more compute or optimization) can be competitive.",
            "interpretability_assessment": "Not addressed in this paper for the latent-model class; generally treated as black-box learned latent predictors without interpretability analyses here.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Qualitatively higher than the minimal contrastive auxiliary approach; extra components (dynamics model, decoder, planning procedure, additional hyperparameters) increase implementation and compute burden; increased optimization (more gradient updates) increases wall-clock time.",
            "efficiency_comparison": "CURL (contrastive auxiliary + model-free RL) is presented as a simpler, computationally lighter alternative that achieves superior or comparable sample-efficiency in the evaluated settings; specific speedups cited in this paper are sample-efficiency numbers (e.g., 1.9x and 4.5x vs Dreamer), not raw compute-time numbers.",
            "task_performance": "Varies by instance: some latent-space models achieve good asymptotic performance and sample-efficiency in prior work; in the experiments of this paper, CURL outperforms many latent-space model baselines on sample-efficiency benchmarks.",
            "task_utility_analysis": "Latent-space modeling can support planning and synthetic data generation, which can be highly useful for sample-limited problems; however, this paper's results show that learning strong semantic representations via contrastive methods can provide equivalent or better utility with less modeling overhead in many standard benchmarks.",
            "tradeoffs_observed": "Key trade-offs: latent models can improve sample-efficiency but require more design and compute; simpler auxiliary objectives (contrastive learning) may yield similar or better sample-efficiency with less complexity, though latent models may still be advantageous in other settings (e.g., long-horizon planning, explicit dynamics requirements).",
            "design_choices": "Encoder architecture (VAE/contrastive), latent dimension size, deterministic vs stochastic dynamics, whether to learn decoders (reconstruction), planning method (MPC, latent imagination) and optimization regimes (gradient updates per env step) affect fidelity, compute and utility.",
            "comparison_to_alternatives": "Latent-space world models are explicitly compared (PlaNet, Dreamer, SLAC, SimPLe) and found in these benchmarks to be generally outperformed by CURL in early-sample-efficiency; some latent-model variants with heavier optimization/runtime can match or exceed CURL on selected tasks but at higher computational expense.",
            "optimal_configuration": "The paper does not define a single optimal latent-world-model configuration; its empirical recommendation is that, for many sample-efficient pixel-control problems and for ease of reproducibility, contrastive representation learning is a preferable design choice unless the application specifically requires explicit predictive/world-model capabilities.",
            "uuid": "e1412.5",
            "source_info": {
                "paper_title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Model-based reinforcement learning for atari",
            "rating": 2
        },
        {
            "paper_title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "rating": 2
        },
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 1
        },
        {
            "paper_title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
            "rating": 1
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2
        }
    ],
    "cost": 0.022828249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CURL: Contrastive Unsupervised Representations for Reinforcement Learning</h1>
<p>Aravind Srinivas ${ }^{\circ 1}$ Michael Laskin ${ }^{\circ 1}$ Pieter Abbeel ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs offpolicy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9 x and 1.2 x performance gains at the 100 K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://www. github.com/MishaLaskin/curl.</p>
<h2>1. Introduction</h2>
<p>Developing agents that can perform complex control tasks from high dimensional observations such as pixels has been possible by combining the expressive power of deep neural networks with the long-term credit assignment power of reinforcement learning algorithms. Notable successes include learning to play a diverse set of video games from raw pixels (Mnih et al., 2015), continuous control tasks such as controlling a simulated car from a dashboard camera (Lillicrap et al., 2015) and subsequent algorithmic developments and applications to agents that successfully navigate mazes and solve complex tasks from first-person camera observations (Jaderberg et al., 2016; Espeholt et al., 2018; Jaderberg et al., 2019); and robots that successfully grasp objects in the real world (Kalashnikov et al., 2018).</p>
<p>However, it has been empirically observed that reinforcement learning from high dimensional observations such as raw pixels is sample-inefficient (Lake et al., 2017; Kaiser</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Contrastive Unsupervised Representations for Reinforcement Learning (CURL) combines instance contrastive learning and reinforcement learning. CURL trains a visual representation encoder by ensuring that the embeddings of data-augmented versions $o_{q}$ and $o_{k}$ of observation $o$ match using a contrastive loss. The query observations $o_{q}$ are treated as the anchor while the key observations $o_{k}$ contain the positive and negatives, all constructed from the minibatch sampled for the RL update. The keys are encoded with a momentum averaged version of the query encoder. The RL policy and (or) value function are built on top of the query encoder which is jointly trained with the contrastive and reinforcement learning objectives. CURL is a generic framework that can be plugged into any RL algorithm that relies on learning representations from high dimensional images.
et al., 2019). Moreover, it is widely accepted that learning policies from physical state based features is significantly more sample-efficient than learning from pixels (Tassa et al., 2018). In principle, if the state information is present in the pixel data, then we should be able to learn representations that extract the relevant state information. For this reason, it may be possible to learn from pixels as fast as from state given the right representation.</p>
<p>From a practical standpoint, although high rendering speeds in simulated environments enable RL agents to solve complex tasks within reasonable wall clock time, learning in the real world means that agents are bound to work within the limitations of physics. Kalashnikov et al. (2018) needed a farm of robotic arms that collected large scale robot in-</p>
<p>teraction data over several months to develop their robot grasp value functions and policies. The data-efficiency of the whole pipeline thus has significant room for improvement. Similarly, in simulated worlds which are limited by rendering speeds in the absence of GPU accelerators, data efficiency is extremely crucial to have a fast experimental turnover and iteration. Therefore, improving the sample efficiency of reinforcement learning (RL) methods that operate from high dimensional observations is of paramount importance to RL research both in simulation and the real world and allows for faster progress towards the broader goal of developing intelligent autonomous agents.</p>
<p>A number of approaches have been proposed in the literature to address the sample inefficiency of deep RL algorithms. Broadly, they can be classified into two streams of research, though not mutually exclusive: (i) Auxiliary tasks on the agent's sensory observations; (ii) World models that predict the future. While the former class of methods use auxiliary self-supervision tasks to accelerate the learning progress of model-free RL methods (Jaderberg et al., 2016; Mirowski et al., 2016), the latter class of methods build explicit predictive models of the world and use those models to plan through or collect fictitious rollouts for model-free methods to learn from (Sutton, 1990; Ha \&amp; Schmidhuber, 2018; Kaiser et al., 2019; Schrittwieser et al., 2019).</p>
<p>Our work falls into the first class of models, which use auxiliary tasks to improve sample efficiency. Our hypothesis is simple: If an agent learns a useful semantic representation from high dimensional observations, control algorithms built on top of those representations should be significantly more data-efficient. Self-supervised representation learning has seen dramatic progress in the last couple of years with huge advances in masked language modeling (Devlin et al., 2018) and contrastive learning (Hénaff et al., 2019; He et al., 2019a; Chen et al., 2020) for language and vision respectively. The representations uncovered by these objectives improve the performance of any supervised learning system especially in scenarios where the amount of labeled data available for the downstream task is really low.</p>
<p>We take inspiration from the contrastive pre-training successes in computer vision. However, there are a couple of key differences: (i) There is no giant unlabeled dataset of millions of images available beforehand - the dataset is collected online from the agent's interactions and changes dynamically with the agent's experience; (ii) The agent has to perform unsupervised and reinforcement learning simultaneously as opposed to fine-tuning a pre-trained network for a specific downstream task. These two differences introduce a different challenge: How can we use contrastive learning for improving agents that can learn to control effectively and efficiently from online interactions?</p>
<p>To address this challenge, we propose CURL - Contrastive</p>
<p>Uunsupervised Representations for Reinforcement Learning. CURL uses a form of contrastive learning that maximizes agreement between augmented versions of the same observation, where each observation is a stack of temporally sequential frames. We show that CURL significantly improves sample-efficiency over prior pixel-based methods by performing contrastive learning simultaneously with an off-policy RL algorithm. CURL coupled with the Soft-Actor-Critic (SAC) (Haarnoja et al., 2018) results in $\mathbf{1 . 9 x}$ median higher performance over Dreamer, a prior state-of-the-art algorithm on DMControl environments, benchmarked at 100k environment steps and matches the performance of state-based SAC on the majority of 16 environments tested, a first for pixel-based methods. In the Atari setting benchmarked at 100k interaction steps, we show that CURL coupled with a data-efficient version of Rainbow DQN (van Hasselt et al., 2019) results in 1.2x median higher performance over prior methods such as SimPLe (Kaiser et al., 2019), improving upon Efficient Rainbow (van Hasselt et al., 2019) on 19 out of 26 Atari games, surpassing human efficiency on two games.</p>
<p>While contrastive learning in aid of model-free RL has been studied in the past by van den Oord et al. (2018) using Contrastive Predictive Coding (CPC), the results were mixed with marginal gains in a few DMLab (Espeholt et al., 2018) environments. CURL is the first model to show substantial data-efficiency gains from using a contrastive self-supervised learning objective for model-free RL agents across a multitude of pixel based continuous and discrete control tasks in DMControl and Atari.</p>
<p>We prioritize designing a simple and easily reproducible pipeline. While the promise of auxiliary tasks and learning world models for RL agents has been demonstrated in prior work, there's an added layer of complexity when introducing components like modeling the future in a latent space (van den Oord et al., 2018; Ha \&amp; Schmidhuber, 2018). CURL is designed to add minimal overhead in terms of architecture and model learning. The contrastive learning objective in CURL operates with the same latent space and architecture typically used for model-free RL and seamlessly integrates with the training pipeline without the need to introduce multiple additional hyperparameters.</p>
<p>Our paper makes the following key contributions: We present CURL, a simple framework that integrates contrastive learning with model-free RL with minimal changes to the architecture and training pipeline. Using 16 complex control tasks from the DeepMind control (DMControl) suite and 26 Atari games, we empirically show that contrastive learning combined with model-free RL outperforms the prior state-of-the-art by 1.9 x on DMControl and 1.2 x on Atari compared across leading prior pixel-based methods. CURL is also the first algorithm across both model-based</p>
<p>and model-free methods that operates purely from pixels, and nearly matches the performance and sample-efficiency of a SAC algorithm trained from the state based features on the DMControl suite. Finally, our design is simple and does not require any custom architectural choices or hyperparameters which is crucial for reproducible end-to-end training. Through these strong empirical results, we demonstrate that a contrastive objective is the preferred self-supervised auxiliary task for achieving sample-efficiency compared to reconstruction based methods, and enables model-free methods to outperform state-of-the-art model-based methods in terms of data-efficiency.</p>
<h2>2. Related Work</h2>
<p>Self-Supervised Learning: Self-Supervised Learning is aimed at learning rich representations of high dimensional unlabeled data to be useful for a wide variety of tasks. The fields of natural language processing and computer vision have seen dramatic advances in self-supervised methods such as BERT (Devlin et al., 2018), CPC, MoCo, SimCLR (Hénaff et al., 2019; He et al., 2019a; Chen et al., 2020).</p>
<p>Contrastive Learning: Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. This is often best understood as performing a dictionary lookup task wherein the positive and negatives represent a set of keys with respect to a query (or an anchor). A simple instantiation of contrastive learning is Instance Discrimination (Wu et al., 2018) wherein a query and key are positive pairs if they are data-augmentations of the same instance (example, image) and negative otherwise. A key challenge in contrastive learning is the choice of negatives which can decide the quality of the underlying representations learned. The loss functions used to contrast could be among several choices such as InfoNCE (van den Oord et al., 2018), Triplet (Wang \&amp; Gupta, 2015), Siamese (Chopra et al., 2005) and so forth.</p>
<p>Self-Supervised Learning for RL: Auxiliary tasks such as predicting the future conditioned on the past observation(s) and action(s) (Jaderberg et al., 2016; Shelhamer et al., 2016; van den Oord et al., 2018; Schmidhuber, 1990) are a few representative examples of using auxiliary tasks to improve the sample-efficiency of model-free RL algorithms. The future prediction is either done in a pixel space (Jaderberg et al., 2016) or latent space (van den Oord et al., 2018). The sample-efficiency gains from reconstruction-based auxiliary losses have been benchmarked in Jaderberg et al. (2016); Higgins et al. (2017); Yarats et al. (2019). Contrastive learning has been used to extract reward signals in the latent space (Sermanet et al., 2018; Dwibedi et al., 2018; WardeFarley et al., 2018); and study representation learning on Atari games by Anand et al. (2019).</p>
<p>World Models for sample-efficiency: While joint learning of an auxiliary unsupervised task with model-free RL is one way to improve the sample-efficiency of agents, there has also been another line of research that has tried to learn world models of the environment and use them to sample rollouts and plan. An early instantiation of the generic principle was put forth by Sutton (1990) in Dyna where fictitious samples rolled out from a learned world model are used in addition to the agent's experience for sample-efficient learning. Planning through a learned world model (Srinivas et al., 2018) is another way to improve sample-efficiency. While Jaderberg et al. (2016); van den Oord et al. (2018); Lee et al. (2019) also learn pixel and latent space forward models, the models are learned to shape the latent representations, and there is no explicit Dyna or planning. Planning through learned world models has been successfully demonstrated in Ha \&amp; Schmidhuber (2018); Hafner et al. (2018; 2019). Kaiser et al. (2019) introduce SimPLe which implements Dyna with expressive deep neural networks for the world model for sample-efficiency on Atari games.</p>
<p>Sample-efficient RL for image-based control: CURL encompasses the areas of self-supervision, contrastive learning and using auxiliary tasks for sample-efficient RL. We benchmark for sample-efficiency on the DMControl suite (Tassa et al., 2018) and Atari Games benchmarks (Bellemare et al., 2013). The DMControl suite has been used widely by Yarats et al. (2019), Hafner et al. (2018), Hafner et al. (2019) and Lee et al. (2019) for benchmarking sample-efficiency for image based continuous control methods. As for Atari, Kaiser et al. (2019) propose to use the 100k interaction steps benchmark for sample-efficiency which has been adopted in Kielak (2020); van Hasselt et al. (2019). The Rainbow DQN (Hessel et al., 2017) was originally proposed for maximum sample-efficiency on the Atari benchmark and in recent times has been adapted to a version known as DataEfficient Rainbow (van Hasselt et al., 2019) with competitive performance to SimPLe without learning world models. We benchmark extensively against both model-based and model-free algorithms in our experiments. For the DMControl experiments, we compare our method to Dreamer, PlaNet, SLAC, SAC+AE whereas for Atari experiments we compare to SimPLe, Rainbow, and OverTrained Rainbow (OTRainbow) and Efficient Rainbow (Eff. Rainbow).</p>
<h2>3. Background</h2>
<p>CURL is a general framework for combining contrastive learning with RL. In principle, one could use any RL algorithm in the CURL pipeline, be it on-policy or off-policy. We use the widely adopted Soft Actor Critic (SAC) (Haarnoja et al., 2018) for continuous control benchmarks (DM Control) and Rainbow DQN (Hessel et al., 2017; van Hasselt et al., 2019) for discrete control benchmarks (Atari). Below, we review SAC, Rainbow DQN and Contrastive Learning.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. CURL Architecture: A batch of transitions is sampled from the replay buffer. Observations are then data-augmented twice to form query and key observations, which are then encoded with the query encoder and key encoders, respectively. The queries are passed to the RL algorithm while query-key pairs are passed to the contrastive learning objective. During the gradient update step, only the query encoder is updated. The key encoder weights are the moving average (EMA) of the query weights similar to MoCo (He et al., 2019a).</p>
<h3>3.1. Soft Actor Critic</h3>
<p>SAC is an off-policy RL algorithm that optimizes a stochastic policy for maximizing the expected trajectory returns. Like other state-of-the-art end-to-end RL algorithms, SAC is effective when solving tasks from state observations but fails to learn efficient policies from pixels. SAC is an actor-critic method that learns a policy $\pi_{\psi}$ and critics $Q_{\phi_{i}}$ and $Q_{\phi_{2}}$. The parameters $\phi_{i}$ are learned by minimizing the Bellman error:</p>
<p>$$
\mathcal{L}\left(\phi_{i}, \mathcal{B}\right)=\mathbb{E}<em _phi__i="\phi_{i">{t \sim \mathcal{B}}\left[\left(Q</em>\right]
$$}}(o, a)-(r+\gamma(1-d) \mathcal{T})\right)^{2</p>
<p>where $t=\left(o, a, o^{\prime}, r, d\right)$ is a tuple with observation $o$, action $a$, reward $r$ and done signal $d, \mathcal{B}$ is the replay buffer, and $\mathcal{T}$ is the target, defined as:</p>
<p>$$
\mathcal{T}=\left(\min <em _phi__i="\phi_{i">{t=1,2} Q</em>\right)\right)
$$}}^{*}\left(o^{\prime}, a^{\prime}\right)-\alpha \log \pi_{\psi}\left(a^{\prime} \mid o^{\prime</p>
<p>In the target equation (2), $Q_{\phi_{i}}^{*}$ denotes the exponential moving average (EMA) of the parameters of $Q_{\phi_{i}}$. Using the EMA has empirically shown to improve training stability in off-policy RL algorithms. The parameter $\alpha$ is a positive entropy coefficient that determines the priority of the entropy maximization over value function optimization.
While the critic is given by $Q_{\phi_{i}}$, the actor samples actions from policy $\pi_{\psi}$ and is trained by maximizing the expected return of its actions as in:</p>
<p>$$
\mathcal{L}(\psi)=\mathbb{E}<em _psi="\psi">{a \sim \pi}\left[Q^{\pi}(o, a)-\alpha \log \pi</em>(a \mid o)\right]
$$</p>
<p>where actions are sampled stochastically from the policy $a_{\psi}(o, \xi) \sim \tanh \left(\mu_{\psi}(o)+\sigma_{\psi}(o) \odot \xi\right)$ and $\xi \sim \mathcal{N}(0, I)$ is a standard normalized noise vector.</p>
<h3>3.2. Rainbow</h3>
<p>Rainbow DQN (Hessel et al., 2017) is best summarized as multiple improvements on top of the original Nature DQN (Mnih et al., 2015) applied together. Specifically, Deep Q Network (DQN) (Mnih et al., 2015) combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning (Van Hasselt et al., 2016), Dueling Network Architectures (Wang et al., 2015), Prioritized Experience Replay (Schaul et al., 2015), and Noisy Networks (Fortunato et al., 2017). Additionally, distributional reinforcement learning (Bellemare et al., 2017) proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow also makes use of multi-step returns (Sutton et al., 1998). van Hasselt et al. (2019) propose a data-efficient version of the Rainbow which can be summarized as an improved configuration of hyperparameters that is optimized for performance benchmarked at 100 K interaction steps.</p>
<h3>3.3. Contrastive Learning</h3>
<p>A key component of CURL is the ability to learn rich representations of high dimensional data using contrastive unsupervised learning. Contrastive learning (Hadsell et al., 2006; LeCun et al., 2006; van den Oord et al., 2018; Wu et al., 2018; He et al., 2019a) can be understood as learning a differentiable dictionary look-up task. Given a query $q$ and keys $\mathbb{K}=\left{k_{0}, k_{1}, \ldots\right}$ and an explicitly known partition of $\mathbb{K}$ (with respect to $q$ ) $P(\mathbb{K})=\left(\left{k_{+}\right}, \mathbb{K} \backslash\left{k_{+}\right}\right)$, the goal of contrastive learning is to ensure that $q$ matches with $k_{+}$ relatively more than any of the keys in $\mathbb{K} \backslash\left{k_{+}\right} . q, \mathbb{K}, k_{+}$,</p>
<p>and $\mathbb{K} \backslash\left{k_{+}\right}$are also referred to as anchor, targets, positive, negatives respectively in the parlance of contrastive learning (van den Oord et al., 2018; He et al., 2019a). Similarities between the anchor and targets are best modeled with dot products $\left(q^{T} k\right)$ (Wu et al., 2018; He et al., 2019a) or bilinear products $\left(q^{T} W k\right)$ (van den Oord et al., 2018; Hénaff et al., 2019) though other forms like euclidean distances are also common (Schroff et al., 2015; Wang \&amp; Gupta, 2015). To learn embeddings that respect these similarity relations, van den Oord et al. (2018) propose the InfoNCE loss:</p>
<p>$$
\mathcal{L}<em _="+">{q}=\log \frac{\exp \left(q^{T} W k</em>
$$}\right)}{\exp \left(q^{T} W k_{+}\right)+\sum_{i=0}^{K-1} \exp \left(q^{T} W k_{i}\right)</p>
<p>The loss 4 can be interpreted as the log-loss of a $K$-way softmax classifier whose label is $k_{+}$.</p>
<h2>4. CURL Implementation</h2>
<p>CURL minimally modifies a base RL algorithm by training the contrastive objective as an auxiliary loss during the batch update. In our experiments, we train CURL alongside two model-free RL algorithms - SAC for DMControl experiments and Rainbow DQN (data-efficient version) for Atari experiments. To specify a contrastive learning objective, we need to define (i) the discrimination objective (ii) the transformation for generating query-key observations (iii) the embedding procedure for transforming observations into queries and keys and (iv) the inner product used as a similarity measure between the query-key pairs in the contrastive loss. The exact specification these aspects largely determine the quality of the learned representations.</p>
<p>We first summarize the CURL architecture, and then cover each architectural choice in detail.</p>
<h3>4.1. Architectural Overview</h3>
<p>CURL uses instance discrimination with similarities to SimCLR (Chen et al., 2020), MoCo (He et al., 2019a) and CPC (Hénaff et al., 2019). Most Deep RL architectures operate with a stack of temporally consecutive frames as input (Hessel et al., 2017). Therefore, instance discrimination is performed across the frame stacks as opposed to single image instances. We use a momentum encoding procedure for targets similar to MoCo (He et al., 2019b) which we found to be better performing for RL. Finally, for the InfoNCE score function, we use a bi-linear inner product similar to CPC (van den Oord et al., 2018) which we found to work better than unit norm vector products used in MoCo and SimCLR. Ablations for both the encoder and the similarity measure choices are shown in Figure 5. The contrastive representation is trained jointly with the RL algorithm, and the latent code receives gradients from both the contrastive ob-
jective and the Q-function. An overview of the architecture is shown in in Figure 2.</p>
<h3>4.2. Discrimination Objective</h3>
<p>A key component of contrastive representation learning is the choice of positives and negative samples relative to an anchor (Bachman et al., 2019; Tian et al., 2019; Hénaff et al., 2019; He et al., 2019a; Chen et al., 2020). Contrastive Predictive Coding (CPC) based pipelines (Hénaff et al., 2019; van den Oord et al., 2018) use groups of image patches separated by a carefully chosen spatial offset for anchors and positives while the negatives come from other patches within the image and from other images.</p>
<p>While patches are a powerful way to incorporate spatial and instance discrimination together, they introduce extra hyperparameters and architectural design choices which may be hard to adapt for a new problem. SimCLR (Chen et al., 2020) and MoCo (He et al., 2019a) opt for a simpler design where there is no patch extraction.</p>
<p>Discriminating transformed image instances as opposed to image-patches within the same image optimizes a simpler instance discrimination objective (Wu et al., 2018) with the InfoNCE loss and requires minimal architectural adjustments (He et al., 2019b; Chen et al., 2020). It is preferable to pick a simpler discrimination objective in the RL setting for two reasons. First, considering the brittleness of reinforcement learning algorithms (Henderson et al., 2018), complex discrimination may destabilize the RL objective. Second, since RL algorithms are trained on dynamically generated datasets, a complex discrimination objective may significantly increase the wall-clock training time. CURL therefore uses instance discrimination rather than patch discrimination. One could view contrastive instance discrimination setups like SimCLR and MoCo as maximizing mutual information between an image and its augmented version. The reader is encouraged to refer to van den Oord et al. (2018); Hjelm et al. (2018); Tschannen et al. (2019) for connections between contrastive learning and mutual information.</p>
<h3>4.3. Query-Key Pair Generation</h3>
<p>Similar to instance discrimination in the image setting (He et al., 2019b; Chen et al., 2020), the anchor and positive observations are two different augmentations of the same image while negatives come from other images. CURL primarily relies on the random crop data augmentation, where a random square patch is cropped from the original rendering.</p>
<p>A significant difference between RL and computer vision settings is that an instance ingested by a model-free RL algorithm that operates from pixels is not just a single image but a stack of frames (Mnih et al., 2015). For example, one typically feeds in a stack of 4 frames in Atari experiments</p>
<p>and a stack of 3 frames in DMControl. This way, performing instance discrimination on frame stacks allows CURL to learn both spatial and temporal discriminative features. For details regarding the extent to which CURL captures temporal features, see Appendix E.</p>
<p>We apply the random augmentations across the batch but consistently across each stack of frames to retain information about the temporal structure of the observation. The augmentation procedure is shown in Figure 3. For more details, refer to Appendix A.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Visually illustrating the process of generating an anchor and its positive using stochastic random crops. Our aspect ratio for cropping is 0.84 , i.e, we crop a $84 \times 84$ image from a $100 \times$ 100 simulation-rendered image. Applying the same random crop coordinates across all frames in the stack ensures time-consistent spatial jittering.</p>
<h3>4.4. Similarity Measure</h3>
<p>Another determining factor in the discrimination objective is the inner product used to measure agreement between query-key pairs. CURL employs the bi-linear inner-product $\operatorname{sim}(q, k)=q^{T} W k$, where $W$ is a learned parameter matrix. We found this similarity measure to outperform the normalized dot-product (see Figure 5 in Appendix A) used in recent state-of-the-art contrastive learning methods in computer vision like MoCo and SimCLR.</p>
<h3>4.5. Target Encoding with Momentum</h3>
<p>The motivation for using contrastive learning in CURL is to train encoders that map from high dimensional pixels to more semantic latents. InfoNCE is an unsupervised loss that learns encoders $f_{q}$ and $f_{k}$ mapping the raw anchors (query) $x_{q}$ and targets (keys) $x_{k}$ into latents $q=f_{q}\left(x_{q}\right)$ and $k=f_{k}\left(x_{k}\right)$, on which we apply the similarity dot products. It is common to share the same encoder between the anchor and target mappings, that is, to have $f_{q}=f_{k}$ (van den Oord et al., 2018; Hénaff et al., 2019).</p>
<p>From the perspective of viewing contrastive learning as building differentiable dictionary lookups over high dimensional entities, increasing the size of the dictionary and enriching the set of negatives is helpful in learning rich
representations. He et al. (2019a) propose momentum contrast (MoCo), which uses the exponentially moving average (momentum averaged) version of the query encoder $f_{q}$ for encoding the keys in $\mathbb{K}$. Given $f_{q}$ parametrized by $\theta_{q}$ and $f_{k}$ parametrized by $\theta_{k}$, MoCo performs the update $\theta_{k}=m \theta_{k}+(1-m) \theta_{q}$ and encodes any target $x_{k}$ using $\operatorname{SG}\left(f_{k}\left(x_{k}\right)\right)$ [SG : Stop Gradient].</p>
<p>CURL couples frame-stack instance discrimination with momentum encoding for the targets during contrastive learning, and RL is performed on top of the encoder features.</p>
<h3>4.6. Differences Between CURL and Prior Contrastive Methods in RL</h3>
<p>van den Oord et al. (2018) use Contactive Predictive Coding (CPC) as an auxiliary task wherein an LSTM operates on a latent space of a convolutional encoder; and both the CPC and A2C (Mnih et al., 2015) objectives are jointly optimized. CURL avoids using pipelines that predict the future in a latent space such as van den Oord et al. (2018); Hafner et al. (2019). In CURL, we opt for a simple instance discrimination style contrastive auxiliary task.</p>
<h3>4.7. CURL Contrastive Learning Pseudocode (PyTorch-like)</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># f_q, f_k: encoder networks for anchor</span>
<span class="c1"># (query) and target (keys) respectively.</span>
<span class="c1"># loader: minibatch sampler from ReplayBuffer</span>
<span class="c1"># B-batch_size, C-channels, H,W-spatial_dims</span>
<span class="c1"># x : shape : [B, C, H, W]</span>
<span class="c1"># C = c * num_frames; c=3 (R/G/B) or 1 (gray)</span>
<span class="c1"># m: momentum, e.g. 0.95</span>
<span class="c1"># z_dim: latent dimension</span>
<span class="n">f_k</span><span class="o">.</span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f_q</span><span class="o">.</span><span class="n">params</span>
<span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span><span class="w"> </span><span class="n">z_dim</span><span class="p">)</span><span class="w"> </span><span class="c1"># bilinear product.</span>
<span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">loader</span><span class="p">:</span><span class="w"> </span><span class="c1"># load minibatch from buffer</span>
<span class="n">x_q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">aug</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="c1"># random augmentation</span>
<span class="n">x_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">aug</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="c1"># different random augmentation</span>
<span class="n">z_q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f_q</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_q</span><span class="p">)</span>
<span class="n">z_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f_k</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span>
<span class="n">z_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">z_k</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="w"> </span><span class="c1"># stop gradient</span>
<span class="n">proj_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="w"> </span><span class="n">z_k</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="w"> </span><span class="c1"># bilinear product</span>
<span class="n">logits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">matmul</span><span class="p">(</span><span class="n">z_q</span><span class="p">,</span><span class="w"> </span><span class="n">proj_k</span><span class="p">)</span><span class="w"> </span><span class="c1"># B x B</span>
<span class="w"> </span><span class="c1"># subtract max from logits for stability</span>
<span class="n">logits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">logits</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">max</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">arange</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="w"> </span><span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">update</span><span class="p">(</span><span class="n">f_q</span><span class="o">.</span><span class="n">params</span><span class="p">)</span><span class="w"> </span><span class="c1"># Adam</span>
<span class="n">update</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="w"> </span><span class="c1"># Adam</span>
<span class="n">f_k</span><span class="o">.</span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m</span><span class="o">*</span><span class="n">f_k</span><span class="o">.</span><span class="n">params</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">f_q</span><span class="o">.</span><span class="n">params</span>
</code></pre></div>

<h2>5. Experiments</h2>
<h3>5.1. Evaluation</h3>
<p>We measure the data-efficiency and performance of our method and baselines at 100k and 500k environment steps on DMControl and 100k interaction steps ( 400 k environment steps with action repeat of 4) on Atari, which we will henceforth refer to as DMControl100k, DMControl500k and Atari100k for clarity. While Atari100k benchmark has</p>
<p>been common practice when investigating data-efficiency on Atari (Kaiser et al., 2019; van Hasselt et al., 2019; Kielak, 2020), the DMControl benchmark was set at 500k environment steps because state-based RL approaches asymptotic performance on many environments at this point, and 100k steps to measure the speed of initial learning. A broader motivation is that while RL algorithms can achieve superhuman performance on Atari games, they are still far less efficient than a human learner. Training for 100-500k environment steps corresponds to a few hours of human time.</p>
<p>We evaluate (i) sample-efficiency by measuring how many steps it takes the best performing baselines to match CURL performance at a fixed $T$ (100k or 500k) steps and (ii) performance by measuring the ratio of the episode returns achieved by CURL versus the best performing baseline at $T$ steps. To be explicit, when we say data or sample-efficiency we're referring to (i) and when we say performance we're referring to (ii).</p>
<h3>5.2. Environments</h3>
<p>Our primary goal for CURL is sample-efficient control from pixels that is broadly applicable across a range of environments. We benchmark the performance of CURL for both discrete and continuous control environments. Specifically, we focus on DMControl suite for continuous control tasks and the Atari Games benchmark for discrete control tasks with inputs being raw pixels rendered by the environments.</p>
<p>DeepMind Control: Recently, there have been a number of papers that have benchmarked for sample efficiency on challenging visual continuous control tasks belonging to the DMControl suite (Tassa et al., 2018) where the agent operates purely from pixels. The reason for operating in these environments is multi fold: (i) they present a reasonably challenging and diverse set of tasks; (ii) sample-efficiency of pure model-free RL algorithms operating from pixels on these benchmarks is poor; (iii) multiple recent efforts to improve the sample efficiency of both model-free and model-based methods on these benchmarks thereby giving us sufficient baselines to compare against; (iv) performance on the DM control suite is relevant to robot learning in real world benchmarks.</p>
<p>We run experiments on sixteen environments from DMControl to examine the performance of CURL on pixels relative to SAC with access to the ground truth state, shown in Figure 7. For more extensive benchmarking, we compare CURL to five leading pixel-based methods across the the six environments presented in Yarats et al. (2019): ball-incup, finger-spin, reacher-easy, cheetah-run, walker-walk, cartpole-swingup for benchmarking.</p>
<p>Atari: Similar to DMControl sample-efficiency benchmarks, there have been a number of recent papers that
have benchmarked for sample-efficiency on the Atari 2600 Games. Kaiser et al. (2019) proposed comparing various algorithms in terms of performance achieved within 100K timesteps ( 400 K frames, frame skip of 4 ) of interaction with the environments (games). The method proposed by Kaiser et al. (2019) called SimPLe is a model-based RL algorithm. SimPLe is compared to a random agent, model-free Rainbow DQN (Hessel et al., 2017) and human performance for the same amount of interaction time. Recently, van Hasselt et al. (2019) and Kielak (2020) proposed data-efficient versions of Rainbow DQN which are competitive with SimPLe on the same benchmark. Given that the same benchmark has been established in multiple recent papers and that there is a human baseline to compare to, we benchmark CURL on all the 26 Atari Games (Table 2).</p>
<h3>5.3. Baselines for benchmarking sample efficiency</h3>
<p>DMControl baselines: We present a number of baselines for continuous control within the DMControl suite: (i) SACAE (Yarats et al., 2019) where the authors attempt to use a $\beta$-VAE (Higgins et al., 2017), VAE (Kingma \&amp; Welling, 2013) and a regualrized autoencoder Vincent et al. (2008); Ghosh et al. (2019) jointly with SAC; (ii) SLAC (Lee et al., 2019) which learns a latent space world model on top of VAE features Ha \&amp; Schmidhuber (2018) and builds value functions on top; (iii) PlaNet and (iv) Dreamer (Hafner et al., 2018; 2019) both of which learn a latent space world model and explicitly plan through it; (v) Pixel SAC: Vanilla SAC operating purely from pixels (Haarnoja et al., 2018). These baselines are competitive methods for benchmarking control from pixels. In addition to these, we also present the baseline State-SAC where the assumption is that the agent has access to low level state based features and does not operate from pixels. This baseline acts as an oracle in that it approximates the upper bound of how sample-efficient a pixel-based agent can get in these environments.</p>
<p>Atari baselines: For benchmarking performance on Atari, we compare CURL to (i) SimPLe (Kaiser et al., 2019), the top performing model-based method in terms of dataefficiency on Atari and (ii) Rainbow DQN (Hessel et al., 2017), a top-performing model-free baseline for Atari, (iii) OTRainbow (Kielak, 2020) which is an OverTrained version of Rainbow for data-efficiency, (iv) Efficient Rainbow (van Hasselt et al., 2019) which is a modification of Rainbow hyperparameters for data-efficiency, (v) Random Agent (Kaiser et al., 2019), (vi) Human Performance (Kaiser et al., 2019; van Hasselt et al., 2019). All the baselines and our method are evaluated for performance after 100K interaction steps ( 400 K frames with a frame skip of 4 ) which corresponds to roughly two hours of gameplay. These benchmarks help us understand how the state-of-the-art pixel based RL algorithms compare in terms of sample efficiency and also to human efficiency. Note: Scores for SimPLe</p>
<p>Table 1. Scores achieved by CURL (mean \&amp; standard deviation for 10 seeds) and baselines on DMControl500k and 1DMControl100k. CURL achieves state-of-the-art performance on the majority ( $\mathbf{5}$ out of $\mathbf{6}$ ) environments benchmarked on DMControl500k. These environments were selected based on availability of data from baseline methods (we run CURL experiments on 16 environments in total and show results in Figure 7). The baselines are PlaNet (Hafner et al., 2018), Dreamer (Hafner et al., 2019), SAC+AE (Yarats et al., 2019), SLAC (Lee et al., 2019), pixel-based SAC and state-based SAC (Haarnoja et al., 2018). SLAC results were reported with one and three gradient updates per agent step, which we refer to as SLACv1 and SLACv2 respectively. We compare to SLACv1 since all other baselines and CURL only make one gradient update per agent step. We also ran CURL with three gradient updates per step and compare results to SLACv2 in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">500K STEP SCORES</th>
<th style="text-align: center;">CURL</th>
<th style="text-align: center;">PLANET</th>
<th style="text-align: center;">DREAMER</th>
<th style="text-align: center;">SAC+AE</th>
<th style="text-align: center;">SLACV1</th>
<th style="text-align: center;">Pixel SAC</th>
<th style="text-align: center;">State SAC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Finger, SPIN</td>
<td style="text-align: center;">$926 \pm 45$</td>
<td style="text-align: center;">$561 \pm 284$</td>
<td style="text-align: center;">$796 \pm 183$</td>
<td style="text-align: center;">$884 \pm 128$</td>
<td style="text-align: center;">$673 \pm 92$</td>
<td style="text-align: center;">$179 \pm 166$</td>
<td style="text-align: center;">$923 \pm 21$</td>
</tr>
<tr>
<td style="text-align: center;">CARTPOLE, SWINGUP</td>
<td style="text-align: center;">$841 \pm 45$</td>
<td style="text-align: center;">$475 \pm 71$</td>
<td style="text-align: center;">$762 \pm 27$</td>
<td style="text-align: center;">$735 \pm 63$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$419 \pm 40$</td>
<td style="text-align: center;">$848 \pm 15$</td>
</tr>
<tr>
<td style="text-align: center;">REACHER, EASY</td>
<td style="text-align: center;">$929 \pm 44$</td>
<td style="text-align: center;">$210 \pm 390$</td>
<td style="text-align: center;">$793 \pm 164$</td>
<td style="text-align: center;">$627 \pm 58$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$145 \pm 30$</td>
<td style="text-align: center;">$923 \pm 24$</td>
</tr>
<tr>
<td style="text-align: center;">CHEETAH, RUN</td>
<td style="text-align: center;">$518 \pm 28$</td>
<td style="text-align: center;">$305 \pm 131$</td>
<td style="text-align: center;">$570 \pm 253$</td>
<td style="text-align: center;">$550 \pm 34$</td>
<td style="text-align: center;">$640 \pm 19$</td>
<td style="text-align: center;">$197 \pm 15$</td>
<td style="text-align: center;">$795 \pm 30$</td>
</tr>
<tr>
<td style="text-align: center;">WALKER, WALK</td>
<td style="text-align: center;">$902 \pm 43$</td>
<td style="text-align: center;">$351 \pm 58$</td>
<td style="text-align: center;">$897 \pm 49$</td>
<td style="text-align: center;">$847 \pm 48$</td>
<td style="text-align: center;">$842 \pm 51$</td>
<td style="text-align: center;">$42 \pm 12$</td>
<td style="text-align: center;">$948 \pm 54$</td>
</tr>
<tr>
<td style="text-align: center;">BALL IN CUP, CATCH</td>
<td style="text-align: center;">$959 \pm 27$</td>
<td style="text-align: center;">$460 \pm 380$</td>
<td style="text-align: center;">$879 \pm 87$</td>
<td style="text-align: center;">$794 \pm 58$</td>
<td style="text-align: center;">$852 \pm 71$</td>
<td style="text-align: center;">$312 \pm 63$</td>
<td style="text-align: center;">$974 \pm 33$</td>
</tr>
<tr>
<td style="text-align: center;">100K STEP SCORES</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Finger, SPIN</td>
<td style="text-align: center;">$767 \pm 56$</td>
<td style="text-align: center;">$136 \pm 216$</td>
<td style="text-align: center;">$341 \pm 70$</td>
<td style="text-align: center;">$740 \pm 64$</td>
<td style="text-align: center;">$693 \pm 141$</td>
<td style="text-align: center;">$179 \pm 66$</td>
<td style="text-align: center;">$811 \pm 46$</td>
</tr>
<tr>
<td style="text-align: center;">CARTPOLE, SWINGUP</td>
<td style="text-align: center;">$582 \pm 146$</td>
<td style="text-align: center;">$297 \pm 39$</td>
<td style="text-align: center;">$326 \pm 27$</td>
<td style="text-align: center;">$311 \pm 11$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$419 \pm 40$</td>
<td style="text-align: center;">$835 \pm 22$</td>
</tr>
<tr>
<td style="text-align: center;">REACHER, EASY</td>
<td style="text-align: center;">$538 \pm 233$</td>
<td style="text-align: center;">$20 \pm 50$</td>
<td style="text-align: center;">$314 \pm 155$</td>
<td style="text-align: center;">$274 \pm 14$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$145 \pm 30$</td>
<td style="text-align: center;">$746 \pm 25$</td>
</tr>
<tr>
<td style="text-align: center;">CHEETAH, RUN</td>
<td style="text-align: center;">$299 \pm 48$</td>
<td style="text-align: center;">$138 \pm 88$</td>
<td style="text-align: center;">$235 \pm 137$</td>
<td style="text-align: center;">$267 \pm 24$</td>
<td style="text-align: center;">$319 \pm 56$</td>
<td style="text-align: center;">$197 \pm 15$</td>
<td style="text-align: center;">$616 \pm 18$</td>
</tr>
<tr>
<td style="text-align: center;">WALKER, WALK</td>
<td style="text-align: center;">$403 \pm 24$</td>
<td style="text-align: center;">$224 \pm 48$</td>
<td style="text-align: center;">$277 \pm 12$</td>
<td style="text-align: center;">$394 \pm 22$</td>
<td style="text-align: center;">$361 \pm 73$</td>
<td style="text-align: center;">$42 \pm 12$</td>
<td style="text-align: center;">$891 \pm 82$</td>
</tr>
<tr>
<td style="text-align: center;">BALL IN CUP, CATCH</td>
<td style="text-align: center;">$769 \pm 43$</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$246 \pm 174$</td>
<td style="text-align: center;">$391 \pm 82$</td>
<td style="text-align: center;">$512 \pm 110$</td>
<td style="text-align: center;">$312 \pm 63$</td>
<td style="text-align: center;">$746 \pm 91$</td>
</tr>
</tbody>
</table>
<p>Table 2. Scores achieved by CURL (coupled with Eff. Rainbow) and baselines on Atari benchmarked at 100k time-steps (Atari100k). CURL achieves state-of-the-art performance on 7 out of $\mathbf{2 6}$ environments. Our baselines are SimPLe (Kaiser et al., 2019), OverTrained Rainbow (OTRainbow) (Kielak, 2020), Data-Efficient Rainbow (Eff. Rainbow) (van Hasselt et al., 2019), Rainbow (Hessel et al., 2017), Random Agent and Human Performance (Human). We see that CURL implemented on top of Eff. Rainbow improves over Eff. Rainbow on $\mathbf{1 9}$ out of $\mathbf{2 6}$ games. We also run CURL with 20 random seeds given that this benchmark is susceptible to high variance across multiple runs. We also see that CURL achieves superhuman performance on JamesBond and Krull.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">GAME</th>
<th style="text-align: center;">HUMAN</th>
<th style="text-align: center;">RANDOM</th>
<th style="text-align: center;">RAINBOW</th>
<th style="text-align: center;">SIMPLE</th>
<th style="text-align: center;">OTRAINBOW</th>
<th style="text-align: center;">Eff. RAINBOW</th>
<th style="text-align: center;">CURL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ALIEN</td>
<td style="text-align: center;">7127.7</td>
<td style="text-align: center;">227.8</td>
<td style="text-align: center;">318.7</td>
<td style="text-align: center;">616.9</td>
<td style="text-align: center;">824.7</td>
<td style="text-align: center;">739.9</td>
<td style="text-align: center;">558.2</td>
</tr>
<tr>
<td style="text-align: center;">AMIDAR</td>
<td style="text-align: center;">1719.5</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">188.6</td>
<td style="text-align: center;">142.1</td>
</tr>
<tr>
<td style="text-align: center;">ASSAULT</td>
<td style="text-align: center;">742.0</td>
<td style="text-align: center;">222.4</td>
<td style="text-align: center;">231</td>
<td style="text-align: center;">527.2</td>
<td style="text-align: center;">351.9</td>
<td style="text-align: center;">431.2</td>
<td style="text-align: center;">600.6</td>
</tr>
<tr>
<td style="text-align: center;">ASTERIX</td>
<td style="text-align: center;">8503.3</td>
<td style="text-align: center;">210.0</td>
<td style="text-align: center;">243.6</td>
<td style="text-align: center;">1128.3</td>
<td style="text-align: center;">628.5</td>
<td style="text-align: center;">470.8</td>
<td style="text-align: center;">734.5</td>
</tr>
<tr>
<td style="text-align: center;">BANK HEIST</td>
<td style="text-align: center;">753.1</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">15.55</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">182.1</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">131.6</td>
</tr>
<tr>
<td style="text-align: center;">BATTLE ZONE</td>
<td style="text-align: center;">37187.5</td>
<td style="text-align: center;">2360.0</td>
<td style="text-align: center;">2360.0</td>
<td style="text-align: center;">5184.4</td>
<td style="text-align: center;">4060.6</td>
<td style="text-align: center;">10124.6</td>
<td style="text-align: center;">14870.0</td>
</tr>
<tr>
<td style="text-align: center;">BOXING</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$-24.8$</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;">BREAKOUT</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">9.84</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">CHOPPER COMMAND</td>
<td style="text-align: center;">7387.8</td>
<td style="text-align: center;">811.0</td>
<td style="text-align: center;">120.0</td>
<td style="text-align: center;">1246.9</td>
<td style="text-align: center;">1033.33</td>
<td style="text-align: center;">861.8</td>
<td style="text-align: center;">1058.5</td>
</tr>
<tr>
<td style="text-align: center;">CRAZY_CLIMBER</td>
<td style="text-align: center;">35829.4</td>
<td style="text-align: center;">10780.5</td>
<td style="text-align: center;">2254.5</td>
<td style="text-align: center;">62583.6</td>
<td style="text-align: center;">21327.8</td>
<td style="text-align: center;">16185.3</td>
<td style="text-align: center;">12146.5</td>
</tr>
<tr>
<td style="text-align: center;">DEMON_ATTACK</td>
<td style="text-align: center;">1971.0</td>
<td style="text-align: center;">152.1</td>
<td style="text-align: center;">163.6</td>
<td style="text-align: center;">208.1</td>
<td style="text-align: center;">711.8</td>
<td style="text-align: center;">508.0</td>
<td style="text-align: center;">817.6</td>
</tr>
<tr>
<td style="text-align: center;">FREEWAY</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">26.7</td>
</tr>
<tr>
<td style="text-align: center;">FROSTBITE</td>
<td style="text-align: center;">4334.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">254.7</td>
<td style="text-align: center;">231.6</td>
<td style="text-align: center;">866.8</td>
<td style="text-align: center;">1181.3</td>
</tr>
<tr>
<td style="text-align: center;">GOPHER</td>
<td style="text-align: center;">2412.5</td>
<td style="text-align: center;">257.6</td>
<td style="text-align: center;">431.2</td>
<td style="text-align: center;">771.0</td>
<td style="text-align: center;">778.0</td>
<td style="text-align: center;">349.5</td>
<td style="text-align: center;">669.3</td>
</tr>
<tr>
<td style="text-align: center;">HERO</td>
<td style="text-align: center;">30826.4</td>
<td style="text-align: center;">1027.0</td>
<td style="text-align: center;">487</td>
<td style="text-align: center;">2656.6</td>
<td style="text-align: center;">6458.8</td>
<td style="text-align: center;">6857.0</td>
<td style="text-align: center;">6279.3</td>
</tr>
<tr>
<td style="text-align: center;">JAMESBOND</td>
<td style="text-align: center;">302.8</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">125.3</td>
<td style="text-align: center;">112.3</td>
<td style="text-align: center;">301.6</td>
<td style="text-align: center;">471.0</td>
</tr>
<tr>
<td style="text-align: center;">KANGAROO</td>
<td style="text-align: center;">3035.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">323.1</td>
<td style="text-align: center;">605.4</td>
<td style="text-align: center;">779.3</td>
<td style="text-align: center;">872.5</td>
</tr>
<tr>
<td style="text-align: center;">KRULL</td>
<td style="text-align: center;">2665.5</td>
<td style="text-align: center;">1598.0</td>
<td style="text-align: center;">1468</td>
<td style="text-align: center;">4539.9</td>
<td style="text-align: center;">3277.9</td>
<td style="text-align: center;">2851.5</td>
<td style="text-align: center;">4229.6</td>
</tr>
<tr>
<td style="text-align: center;">KUNG_FU_MASTER</td>
<td style="text-align: center;">22736.3</td>
<td style="text-align: center;">258.5</td>
<td style="text-align: center;">0.</td>
<td style="text-align: center;">17257.2</td>
<td style="text-align: center;">5722.2</td>
<td style="text-align: center;">14346.1</td>
<td style="text-align: center;">14307.8</td>
</tr>
<tr>
<td style="text-align: center;">MS_PACMAN</td>
<td style="text-align: center;">6951.6</td>
<td style="text-align: center;">307.3</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">1480.0</td>
<td style="text-align: center;">941.9</td>
<td style="text-align: center;">1204.1</td>
<td style="text-align: center;">1465.5</td>
</tr>
<tr>
<td style="text-align: center;">PONG</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">$-20.7$</td>
<td style="text-align: center;">$-20.6$</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">$-19.3$</td>
<td style="text-align: center;">$-16.5$</td>
</tr>
<tr>
<td style="text-align: center;">PRIVATE EYE</td>
<td style="text-align: center;">69571.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">218.4</td>
</tr>
<tr>
<td style="text-align: center;">QBERT</td>
<td style="text-align: center;">13455.0</td>
<td style="text-align: center;">163.9</td>
<td style="text-align: center;">123.46</td>
<td style="text-align: center;">1288.8</td>
<td style="text-align: center;">509.3</td>
<td style="text-align: center;">1152.9</td>
<td style="text-align: center;">1042.4</td>
</tr>
<tr>
<td style="text-align: center;">ROAD_RUNNER</td>
<td style="text-align: center;">7845.0</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">1588.46</td>
<td style="text-align: center;">5640.6</td>
<td style="text-align: center;">2696.7</td>
<td style="text-align: center;">9600.0</td>
<td style="text-align: center;">5661.0</td>
</tr>
<tr>
<td style="text-align: center;">SEAQUEST</td>
<td style="text-align: center;">42054.7</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">131.69</td>
<td style="text-align: center;">683.3</td>
<td style="text-align: center;">286.92</td>
<td style="text-align: center;">354.1</td>
<td style="text-align: center;">384.5</td>
</tr>
<tr>
<td style="text-align: center;">UP_N_DOWN</td>
<td style="text-align: center;">11693.2</td>
<td style="text-align: center;">533.4</td>
<td style="text-align: center;">504.6</td>
<td style="text-align: center;">3350.3</td>
<td style="text-align: center;">2847.6</td>
<td style="text-align: center;">2877.4</td>
<td style="text-align: center;">2955.2</td>
</tr>
</tbody>
</table>
<p>and Human baselines have been reported differently in prior work (Kielak, 2020; van Hasselt et al., 2019). To be rigorous, we take the best reported score for each individual game reported in prior work.</p>
<h2>6. Results</h2>
<h3>6.1. DMControl</h3>
<p>Sample-efficiency results for DMControl experiments are shown in Table 1 and in Figures 4, 6, and 7. Below are the key findings:
(i) CURL is the state-of-the-art image-based RL algorithm on the majority ( 5 out of 6) DMControl environments that we benchmark on for sample-efficiency against existing pixel-based baselines. On DMControl100k, CURL achieves $\mathbf{1 . 9 x}$ higher median performance than Dreamer (Hafner et al., 2019), a leading model-based method, and is $\mathbf{4 . 5 x}$ more data-efficient shown in Figure 6.
(ii) CURL operating purely from pixels nearly matches (and sometimes surpasses) the sample efficiency of SAC operating from state on the majority of 16 DMControl environments tested shown in Figure 7 and matches the median state-based score on DMControl500k shown in Figure 4. This is a first for any image-based RL algorithm, be it model-based, model-free, with or without auxiliary tasks.
(iii) CURL solves (converges close to optimal score of 1000) on the majority of 16 DMControl experiments within $\mathbf{5 0 0 k}$ steps. It also matches the state-based median score across the 6 extensively benchmarked environments in this regime.</p>
<h3>6.2. Atari</h3>
<p>Results for Atari100k are shown in Table 2. Below are the key findings:
(i) CURL achieves a median human-normalized score (HNS) of $\mathbf{1 7 . 5 \%}$ while SimPLe and Efficient Rainbow DQN achieve $14.4 \%$ and $16.1 \%$ respectively. The mean HNS is $38.1 \%, 44.3 \%$, and $28.5 \%$ for CURL, SimPLe, and Efficient Rainbow DQN respectively.
(ii) CURL improves on top of Efficient Rainbow on 19 out of 26 Atari games. Averaged across 26 games, CURL improves on top of Efficient Rainbow by $\mathbf{1 . 3 x}$, while the median performance improvement over SimPLE and Efficient Rainbow are $\mathbf{1 . 2 x}$ and $\mathbf{1 . 1 x}$ respectively.
(iii) CURL surpasses human performance on two games JamesBond (1.6 HNS), Krull (2.5 HNS).</p>
<h2>7. Ablation Studies</h2>
<p>In Appendix E, we present the results of ablation studies carried out to answer the following questions: (i) Does
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Performance of CURL coupled to SAC averaged across 10 seeds relative to SLACv1, PlaNet, Pixel SAC and State SAC baselines. At the 500k benchmark CURL matches the median score of state-based SAC. At 100k environment steps CURL achieves a 1.9 x higher median score than Dreamer. For a direct comparison, we only compute the median across the 6 environments in 1 ( 4 for SLAC) and show learning curves for CURL across 16 DMControl experiments in 7.</p>
<p>CURL learn only visual features or does it also capture temporal dynamics of the environment? (ii) How well does the RL policy perform if CURL representations are learned solely with the contrastive objective and no signal from RL? (iii) Why does CURL match state-based RL performance on some DMControl environments but not on others?</p>
<h2>8. Conclusion</h2>
<p>In this work, we proposed CURL, a contrastive unsupervised representation learning method for RL, that achieves state-of-the-art data-efficiency on pixel-based RL tasks across a diverse set of benchmark environments. CURL is the first model-free RL pipeline accelerated by contrastive learning with minimal architectural changes to demonstrate state-of-the-art performance on complex tasks so far dominated by approaches that have relied on learning world models and (or) decoder-based objectives. We hope that progress like CURL enables avenues for real-world deployment of RL in areas like robotics where data-efficiency is paramount.</p>
<h2>9. Acknowledgements</h2>
<p>This research is supported in part by DARPA through the Learning with Less Labels (LwLL) Program and by ONR through PECASE N000141612723. We also thank Wendy Shang for her help with Section E.4; Zak Stone and Google TFRC for cloud credits; Danijar Hafner, Alex Lee, and Denis Yarats for sharing data for baselines; and Lerrel Pinto, Adam Stooke, Will Whitney, and Ankesh Anand for insightful discussions.</p>
<h2>References</h2>
<p>Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté, M.A., and Hjelm, R. D. Unsupervised state representation learning in atari. In Advances in Neural Information Processing Systems, pp. 8766-8779, 2019.</p>
<p>Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15509-15519, 2019.</p>
<p>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.</p>
<p>Bellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 449-458. JMLR. org, 2017.</p>
<p>Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations, 2020.</p>
<p>Chopra, S., Hadsell, R., and LeCun, Y. Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), volume 1, pp. 539-546. IEEE, 2005.</p>
<p>Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Randaugment: Practical automated data augmentation with a reduced search space, 2019.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Dwibedi, D., Tompson, J., Lynch, C., and Sermanet, P. Learning actionable representations from visual observations. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1577-1584. IEEE, 2018.</p>
<p>Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.</p>
<p>Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.</p>
<p>Ghosh, P., Sajjadi, M. S. M., Vergari, A., Black, M., and Schlkopf, B. From variational to deterministic autoencoders, 2019.</p>
<p>Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122, 2018.</p>
<p>Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.</p>
<p>Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), volume 2, pp. 17351742. IEEE, 2006.</p>
<p>Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.</p>
<p>Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</p>
<p>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019a.</p>
<p>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. 2019b.</p>
<p>Hénaff, O. J., Srinivas, A., De Fauw, J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v. d. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.</p>
<p>Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning, 2017.</p>
<p>Higgins, I., Pal, A., Rusu, A., Matthey, L., Burgess, C., Pritzel, A., Botvinick, M., Blundell, C., and Lerchner, A. Darla: Improving zero-shot transfer in reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1480-1490. JMLR. org, 2017.</p>
<p>Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.</p>
<p>Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.</p>
<p>Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., Beattie, C., Rabinowitz, N. C., Morcos, A. S., Ruderman, A., et al. Human-level performance in 3d multiplayer games with populationbased reinforcement learning. Science, 364(6443):859865, 2019.</p>
<p>Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018.</p>
<p>Kielak, K. Do recent advancements in model-based deep reinforcement learning really improve data efficiency?, 2020.</p>
<p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc., 2012.</p>
<p>Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017.</p>
<p>Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020.</p>
<p>LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F. A tutorial on energy-based learning. 2006.</p>
<p>Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.</p>
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): $529-533,2015$.</p>
<p>Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.</p>
<p>Schmidhuber, J. Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in nonstationary environments. Technical Report FKI-126-90, TUM, 1990.</p>
<p>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.</p>
<p>Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815-823, 2015.</p>
<p>Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S., and Brain, G. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 1134-1141. IEEE, 2018.</p>
<p>Shelhamer, E., Mahmoudieh, P., Argus, M., and Darrell, T. Loss is its own reward: Self-supervision for reinforcement learning. arXiv preprint arXiv:1612.07307, 2016.</p>
<p>Srinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018.</p>
<p>Sutton, R. S. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine learning proceedings 1990, pp. 216-224. Elsevier, 1990.</p>
<p>Sutton, R. S. et al. Introduction to reinforcement learning, volume 135. 1998.</p>
<p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In Computer Vision and Pattern Recognition (CVPR), 2015. URL http://arxiv.org/abs/1409.4842.</p>
<p>Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.</p>
<p>Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lucic, M. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.
van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on artificial intelligence, 2016.
van Hasselt, H. P., Hessel, M., and Aslanides, J. When to use parametric models in reinforcement learning? In Advances in Neural Information Processing Systems, pp. 14322-14333, 2019.</p>
<p>Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096-1103, 2008.</p>
<p>Wang, X. and Gupta, A. Unsupervised learning of visual representations using videos. In Proceedings of the IEEE International Conference on Computer Vision, pp. 27942802, 2015.</p>
<p>Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., and De Freitas, N. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.</p>
<p>Warde-Farley, D., Van de Wiele, T., Kulkarni, T., Ionescu, C., Hansen, S., and Mnih, V. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018.</p>
<p>Wu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature learning via non-parametric instance-level discrimination. arXiv preprint arXiv:1805.01978, 2018.</p>
<p>Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efficiency in modellree reinforcement learning from images. arXiv preprint arXiv:1910.01741, 2019.</p>
<h2>A. Implementation Details</h2>
<p>Below, we explain the implementation details for CURL in the DMControl setting. Specifically, we use the SAC algorithm as the RL objective coupled with CURL and build on top of the publicly released implementation from <em>Yarats et al. (2019)</em>. We present in detail the hyperparameters for the architecture and optimization. We do not use any extra hyperparameter for balancing the contrastive loss and the reinforcement learning losses. Both the objectives are weighed equally in the gradient updates.</p>
<p>Table 3. Hyperparameters used for DMControl CURL experiments. Most hyperparameters values are unchanged across environments with the exception for action repeat, learning rate, and batch size.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random crop</td>
<td>True</td>
</tr>
<tr>
<td>Observation rendering</td>
<td>$(100,100)$</td>
</tr>
<tr>
<td>Observation downsampling</td>
<td>$(84,84)$</td>
</tr>
<tr>
<td>Replay buffer size</td>
<td>100000</td>
</tr>
<tr>
<td>Initial steps</td>
<td>1000</td>
</tr>
<tr>
<td>Stacked frames</td>
<td>3</td>
</tr>
<tr>
<td>Action repeat</td>
<td>2 finger, spin; walker, walk</td>
</tr>
<tr>
<td></td>
<td>8 cartpole, swingup</td>
</tr>
<tr>
<td></td>
<td>4 otherwise</td>
</tr>
<tr>
<td>Hidden units (MLP)</td>
<td>1024</td>
</tr>
<tr>
<td>Evaluation episodes</td>
<td>10</td>
</tr>
<tr>
<td>Optimizer</td>
<td>Adam</td>
</tr>
<tr>
<td>$(\beta_{1}, \beta_{2}) \rightarrow\left(f_{\theta}, \pi_{\psi}, Q_{\phi}\right)$</td>
<td>$(.9, .999)$</td>
</tr>
<tr>
<td>$(\beta_{1}, \beta_{2}) \rightarrow(\alpha)$</td>
<td>$(.5, .999)$</td>
</tr>
<tr>
<td>Learning rate $\left(f_{\theta}, \pi_{\psi}, Q_{\phi}\right)$</td>
<td>$2 e-4$ cheetah, run</td>
</tr>
<tr>
<td></td>
<td>$1 e-3$ otherwise</td>
</tr>
<tr>
<td>Learning rate $(\alpha)$</td>
<td>$1 e-4$</td>
</tr>
<tr>
<td>Batch Size</td>
<td>512</td>
</tr>
<tr>
<td>$Q$ function EMA $\tau$</td>
<td>0.01</td>
</tr>
<tr>
<td>Critic target update freq</td>
<td>2</td>
</tr>
<tr>
<td>Convolutional layers</td>
<td>4</td>
</tr>
<tr>
<td>Number of filters</td>
<td>32</td>
</tr>
<tr>
<td>Non-linearity</td>
<td>ReLU</td>
</tr>
<tr>
<td>Encoder EMA $\tau$</td>
<td>0.05</td>
</tr>
<tr>
<td>Latent dimension</td>
<td>50</td>
</tr>
<tr>
<td>Discount $\gamma$</td>
<td>.99</td>
</tr>
<tr>
<td>Initial temperature</td>
<td>0.1</td>
</tr>
</tbody>
</table>
<p>Architecture: We use an encoder architecture that is similar to <em>(Yarats et al., 2019)</em>, which we sketch in PyTorch-like pseuodocode below. The actor and critic both use the same encoder to embed image observations. A full list of hyperparameters is displayed in Table 3.</p>
<p>For contrastive learning, CURL utilizes momentum for the key encoder <em>(He et al., 2019b)</em> and a bi-linear inner product as the similarity measure <em>(van den Oord et al., 2018)</em>. Performance curves ablating these two architectural choices are shown in Figure 5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Performance on cheetah-run environment ablated twoways: (left) using the query encoder or exponentially moving average of the query encoder for encoding keys (right) using the bi-linear inner product as in <em>(van den Oord et al., 2018)</em> or the cosine inner product as in <em>He et al. (2019b); Chen et al. (2020)</em></p>
<p>Pseudo-code for the architecture is provided below:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">def</span><span class="w"> </span><span class="nt">encode</span><span class="o">(</span><span class="nt">x</span><span class="o">,</span><span class="nt">z_dim</span><span class="o">):</span>
<span class="w">    </span><span class="nt">ConvNet</span><span class="w"> </span><span class="nt">encoder</span>
<span class="w">    </span><span class="nt">args</span><span class="o">:</span>
<span class="w">        </span><span class="nt">B-batch_size</span><span class="o">,</span><span class="w"> </span><span class="nt">C-channels</span>
<span class="w">        </span><span class="nt">H</span><span class="o">,</span><span class="nt">W-spatial_dims</span>
<span class="w">        </span><span class="nt">x</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">shape</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="cp">[</span><span class="nx">B</span><span class="p">,</span><span class="w"> </span><span class="nx">C</span><span class="p">,</span><span class="w"> </span><span class="nx">H</span><span class="p">,</span><span class="w"> </span><span class="nx">W</span><span class="cp">]</span>
<span class="w">        </span><span class="nt">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nt">num_frames</span><span class="o">;</span><span class="w"> </span><span class="nt">3</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="nt">R</span><span class="o">/</span><span class="nt">G</span><span class="o">/</span><span class="nt">B</span>
<span class="w">        </span><span class="nt">z_dim</span><span class="o">:</span><span class="w"> </span><span class="nt">latent</span><span class="w"> </span><span class="nt">dimension</span>
<span class="w">    </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>
<span class="w">    </span><span class="nt">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nt">255</span><span class="o">.</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nt">c</span><span class="o">:</span><span class="w"> </span><span class="nt">channels</span><span class="o">,</span><span class="w"> </span><span class="nt">f</span><span class="o">:</span><span class="w"> </span><span class="nt">filters</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nt">k</span><span class="o">:</span><span class="w"> </span><span class="nt">kernel</span><span class="o">,</span><span class="w"> </span><span class="nt">s</span><span class="o">:</span><span class="w"> </span><span class="nt">stride</span>
<span class="w">    </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">Conv2d</span><span class="o">(</span><span class="nt">c</span><span class="o">=</span><span class="nt">x</span><span class="p">.</span><span class="nc">shape</span><span class="cp">[</span><span class="mi">1</span><span class="cp">]</span><span class="o">,</span><span class="w"> </span><span class="nt">f</span><span class="o">=</span><span class="nt">32</span><span class="o">,</span><span class="w"> </span><span class="nt">k</span><span class="o">=</span><span class="nt">3</span><span class="o">,</span><span class="w"> </span><span class="nt">s</span><span class="o">=</span><span class="nt">2</span><span class="o">)])(</span>
<span class="w">        </span><span class="nt">x</span><span class="o">)</span>
<span class="w">    </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">ReLU</span><span class="o">(</span><span class="nt">z</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="nt">_</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">range</span><span class="o">(</span><span class="nt">num_layers</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="nt">1</span><span class="o">):</span>
<span class="w">        </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">Conv2d</span><span class="o">((</span><span class="nt">c</span><span class="o">=</span><span class="nt">32</span><span class="o">,</span><span class="w"> </span><span class="nt">f</span><span class="o">=</span><span class="nt">32</span><span class="o">,</span><span class="w"> </span><span class="nt">k</span><span class="o">=</span><span class="nt">3</span><span class="o">,</span><span class="w"> </span><span class="nt">s</span><span class="o">=</span><span class="nt">1</span><span class="o">))(</span><span class="nt">z</span><span class="o">)</span>
<span class="w">        </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">ReLU</span><span class="o">(</span><span class="nt">z</span><span class="o">)</span>
<span class="w">    </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">flatten</span><span class="o">(</span><span class="nt">z</span><span class="o">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nt">in</span><span class="o">:</span><span class="w"> </span><span class="nt">input</span><span class="w"> </span><span class="nt">dim</span><span class="o">,</span><span class="w"> </span><span class="nt">out</span><span class="o">:</span><span class="w"> </span><span class="nt">output_dim</span><span class="o">,</span><span class="w"> </span><span class="nt">h</span><span class="o">:</span>
<span class="w">        </span><span class="nt">hiddens</span>
<span class="w">    </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">mlp</span><span class="o">(</span><span class="nt">in</span><span class="o">=</span><span class="nt">z</span><span class="p">.</span><span class="nc">size</span><span class="o">(),</span><span class="nt">out</span><span class="o">=</span><span class="nt">z_dim</span><span class="o">,</span><span class="nt">h</span><span class="o">=</span><span class="nt">1024</span><span class="o">)</span>
<span class="w">    </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">LayerNorm</span><span class="o">(</span><span class="nt">z</span><span class="o">)</span>
<span class="w">    </span><span class="nt">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">tanh</span><span class="o">(</span><span class="nt">z</span><span class="o">)</span>
</code></pre></div>

<p>Terminology: A common point of confusion is the meaning "training steps." We use the term environment steps to denote the amount of times the simulator environment is stepped through and interaction steps to denote the number of times the agent steps through its policy. The terms action repeat or frame skip refer to the number of times an action</p>
<p>is repeated when it's drawn from the agent's policy. For example, if action repeat is set to 4 , then 100 k interaction steps is equivalent to 400 k environment steps.</p>
<p>Batch Updates: After initializing the replay buffer with observations extracted by a random agent, we sample a batch of observations, compute the CURL objectives, and step through the optimizer. Note that since queries and keys are generated by data-augmenting an observation, we can generate arbitrarily many keys to increase the contrastive batch size without sampling any additional observations.</p>
<p>Shared Representations: The objective of performing contrastive learning together with RL is to ensure that the shared encoder learns rich features that facilitate sample efficient control. There is a subtle coincidental connection between MoCo and off-policy RL. Both the frameworks adopt the usage of a momentum averaged (EMA) version of the underlying model. In MoCo, the EMA encoder is used for encoding the keys (targets) while in off-policy RL, the EMA version of the Q-networks are used as targets in the Bellman error (Mnih et al., 2015; Haarnoja et al., 2018). Thanks to this connection, CURL shares the convolutional encoder, momentum coefficient and EMA update between contrastive and reinforcement learning updates for the shared parameters. The MLP part of the critic that operates on top of these convolutional features has a separate momentum coefficient and update decoupled from the image encoder parameters.</p>
<p>Balancing Contrastive and RL Updates: While past work has learned hyperparameters to balance the auxiliary loss coefficient or learning rate relative to the RL objective (Jaderberg et al., 2016; Yarats et al., 2019), CURL does not need any such adjustments. We use both the contrastive and RL objectives together with equal weight and learning rate. This simplifies the training process compared to other methods, such as training a VAE jointly (Hafner et al., 2018; 2019; Lee et al., 2019), that require careful tuning of coefficients for representation learning.</p>
<p>Differences in Data Collection between Computer Vision and RL Settings: There are two key differences between contrastive learning in the computer vision and RL settings because of their different goals. Unsupervised feature learning methods built for downstream vision tasks like image classification assume a setting where there is a large static dataset of unlabeled images. On the other hand, in RL, the dataset changes over time to account for the agent's new experiences. Secondly, the size of the memory bank of labeled images and dataset of unlabeled ones in vision-based settings are 65 K and 1 M (or 1B) respectively. The goal in vision-based methods is to learn from millions of unlabeled images. On the other hand, the goal in CURL is to develop sample-efficient RL algorithms. For example, to be able to solve a task within 100 K timesteps (approximately 2 hours in real-time), an agent can only ingest 100 K image frames.</p>
<p>Therefore, unlike MoCo, CURL does not use a memory bank for contrastive learning. Instead, the negatives are constructed on the fly for every minibatch sampled from the agent's replay buffer for an RL update similar to SimCLR. The exact implementation is provided as a PyTorch-like code snippet in 4.7.</p>
<h2>Data Augmentation:</h2>
<p>Random crop data augmentation has been crucial for the performance of deep learning based computer vision systems in object recognition, detection and segmentation (Krizhevsky et al., 2012; Szegedy et al., 2015; Cubuk et al., 2019; Chen et al., 2020). However, similar augmentation methods have not seen much adoption in the field of RL even though several benchmarks use raw pixels as inputs to the model.</p>
<p>CURL adopts the random crop data augmentation as the stochastic data augmentation applied to a frame stack. To make it easier for the model to correlate spatio-temporal patterns in the input, we apply the same random crop (in terms of box coordinates) across all four frames in the stack as opposed to extracting different random crop positions from each frame in the stack. Further, unlike in computer vision systems where the aspect ratio for random crop is allowed to be as low as 0.08 , we preserve much of the spatial information as possible and use a constant aspect ratio of 0.84 between the original and cropped. In our experiments, data augmented samples for CURL are formed by cropping $84 \times 84$ frames from an input frame of $100 \times 100$.</p>
<p>DMControl: We render observations at $100 \times 100$ and randomly crop $84 \times 84$ frames. For evaluation, we render observations at $100 \times 100$ and center crop to $84 \times 84$ pixels. We found that implementing random crop efficiently was extremely important to the success of the algorithm. We provide pseudocode below:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">skimage</span><span class="w"> </span><span class="kn">import</span> <span class="n">view_as_windows</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="k">def</span><span class="w"> </span><span class="nf">random_crop</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="s2">&quot; &quot;</span> <span class="s2">&quot;</span>
    <span class="n">Vectorized</span> <span class="n">random</span> <span class="n">crop</span>
    <span class="n">args</span><span class="p">:</span>
        <span class="n">imgs</span><span class="p">:</span> <span class="kp">shape</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">)</span>
            <span class="n">out</span><span class="p">:</span> <span class="n">output</span> <span class="kp">size</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="mi">84</span><span class="p">)</span>
    <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    # n: batch size.</span>
<span class="s2">    n = imgs.shape[0]</span>
<span class="s2">    img_size = imgs.shape[-1] # e.g. 100</span>
<span class="s2">    crop_max = img_size - out</span>
<span class="s2">    imgs = np.transpose(imgs, (0, 2, 3, 1))</span>
<span class="s2">    w1 = np.random.randint(0, crop_max, n)</span>
<span class="s2">    h1 = np.random.randint(0, crop_max, n)</span>
<span class="s2">    # creates all sliding window</span>
<span class="s2">    # combinations of size (out)</span>
<span class="s2">    windows = view_as_windows(</span>
<span class="s2">        imgs, (1, out, out, 1))[..., 0,:,:, 0]</span>
</code></pre></div>

<p># selects a random window
# for each batch element
cropped $=$ windows[np.arange ( $n$ ), w1, h1]
return cropped</p>
<h2>B. Atari100k Implementation Details</h2>
<p>The flexibility of CURL allows us to apply it to discrete control setting with minimal modifications. Similar to our rationale for picking SAC as the baseline RL algorithm to couple CURL with (for continuous control), we pick the data-efficient version of Rainbow DQN (Efficient Rainbow) (van Hasselt et al., 2019) for Atari100K which performs competitively with an older version of SimPLe (most recent version has improved numbers). In order to understand specifically what the gains from CURL are without any other changes, we adopt the exact same hyperparameters specified in the paper (van Hasselt et al., 2019) (including a modified convolutional encoder that uses larger kernel size and stride of 5). We present the details in Table 4. Similar to DMControl, the contrastive objective and the RL objective are weighted equally for learning (except for Pong, Freeway, Boxing and PrivateEye for which we used a coefficient of 0.05 for the momentum contastive loss. On a large majority ( 22 out of 26 ) of the games, we do not use this adjustment. While it is standard practice to use the same hyperparameters for all games in Atari, papers proposing auxiliary losses have adopted a different practice of using game specific coefficients (Jaderberg et al., 2016).). We use the Efficient Rainbow codebase from https: //github.com/Kaixhin/Rainbow which has a reproduced version of van Hasselt et al. (2019). We evaluate with 20 random seeds and report the mean score for each game given the high variance nature of the Atari100k steps benchmark. We restrict ourselves to using grayscale renderings of image observations and use random crop of frame stack as data augmentation.</p>
<h2>C. Benchmarking Data Efficiency</h2>
<p>Tables 1 and 2 show the episode returns of DMControl100k, DMControl500k, and Atari100k across CURL and a number of pixel-based baselines. CURL outperforms all baseline pixel-based methods across experiments on both DMControl100k and DMControl500k. On Atari100k experiments, CURL coupled with Eff Rainbow outperforms the baseline on the majority of games tested (19 out of 26 games).</p>
<h2>D. Further Investigation of Data-Efficiency in Contrastive RL</h2>
<p>To further benchmark CURL's sample-efficiency, we compare it to state-based SAC on a total of 16 DMControl environments. Shown in Figure 7, CURL matches state-based</p>
<p>Table 4. Hyperparameters used for Atari100K CURL experiments. Hyperparameters are unchanged across games.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random crop</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;">Image size</td>
<td style="text-align: left;">$(84,84)$</td>
</tr>
<tr>
<td style="text-align: left;">Data Augmentation</td>
<td style="text-align: left;">Random Crop (Train)</td>
</tr>
<tr>
<td style="text-align: left;">Replay buffer size</td>
<td style="text-align: left;">100000</td>
</tr>
<tr>
<td style="text-align: left;">Training frames</td>
<td style="text-align: left;">400000</td>
</tr>
<tr>
<td style="text-align: left;">Training steps</td>
<td style="text-align: left;">100000</td>
</tr>
<tr>
<td style="text-align: left;">Frame skip</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">Stacked frames</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">Action repeat</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">Replay period every</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Q network: channels</td>
<td style="text-align: left;">32,64</td>
</tr>
<tr>
<td style="text-align: left;">Q network: filter size</td>
<td style="text-align: left;">$5 \times 5,5 \times 5$</td>
</tr>
<tr>
<td style="text-align: left;">Q network: stride</td>
<td style="text-align: left;">5,5</td>
</tr>
<tr>
<td style="text-align: left;">Q network: hidden units</td>
<td style="text-align: left;">256</td>
</tr>
<tr>
<td style="text-align: left;">Momentum (EMA for CURL) $\tau$</td>
<td style="text-align: left;">0.001</td>
</tr>
<tr>
<td style="text-align: left;">Non-linearity</td>
<td style="text-align: left;">ReLU</td>
</tr>
<tr>
<td style="text-align: left;">Reward Clipping</td>
<td style="text-align: left;">$[-1,1]$</td>
</tr>
<tr>
<td style="text-align: left;">Multi step return</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Minimum replay size for sampling</td>
<td style="text-align: left;">1600</td>
</tr>
<tr>
<td style="text-align: left;">Max frames per episode</td>
<td style="text-align: left;">108 K</td>
</tr>
<tr>
<td style="text-align: left;">Update</td>
<td style="text-align: left;">Distributional Double Q</td>
</tr>
<tr>
<td style="text-align: left;">Target Network Update Period</td>
<td style="text-align: left;">every 2000 updates</td>
</tr>
<tr>
<td style="text-align: left;">Support-of-Q-distribution</td>
<td style="text-align: left;">51 bins</td>
</tr>
<tr>
<td style="text-align: left;">Discount $\gamma$</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: left;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer: learning rate</td>
<td style="text-align: left;">0.0001</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer: $\beta 1$</td>
<td style="text-align: left;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer: $\beta 2$</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer $\epsilon$</td>
<td style="text-align: left;">0.000015</td>
</tr>
<tr>
<td style="text-align: left;">Max gradient norm</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">Exploration</td>
<td style="text-align: left;">Noisy Nets</td>
</tr>
<tr>
<td style="text-align: left;">Noisy nets parameter</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Priority exponent</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Priority correction</td>
<td style="text-align: left;">$0.4 \rightarrow 1$</td>
</tr>
<tr>
<td style="text-align: left;">Hardware</td>
<td style="text-align: left;">CPU</td>
</tr>
</tbody>
</table>
<p>data-efficiency on most of the environments, but lags behind state-based SAC on more challenging environments.</p>
<h2>E. Ablations</h2>
<h2>E.1. Learning Temporal Dynamics</h2>
<p>To gain insight as to whether CURL learns temporal dynamics across the stacked frames, we also train a variant of CURL where the discriminants are individual frames as opposed to stacked ones. This can be done by sampling stacked frames from the replay buffer but only using the first frame to update the contrastive loss:</p>
<div class="codehilite"><pre><span></span><code>f_q = x_q[:,:3,...] # (B,C,H,W), C=9.
f_k = x_k[:,:3,...]
</code></pre></div>

<p>During the actor-critic update, frames in the batch are en-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. The number of steps it takes a prior leading pixel-based method, Dreamer, to achieve the same score that CURL achieves at 100k training steps (clipped at 1M steps). On average, CURL is 4.5x more data-efficient. We chose Dreamer because the authors (Hafner et al., 2019) report performance for all of the above environments while other baselines like SLAC and SAC+AE only benchmark on 4 and 6 environments, respectively. For further comparison of CURL with these methods, the reader is referred to Table 1 and Figure 4.
coded individually into latent codes, which are then concatenated before being passed to a dense network.</p>
<div class="codehilite"><pre><span></span><code># x: (B,C,H,W), C=9.
z1 = encode(x[:,:3,...])
z2 = encode(x[:,3:6,...])
z3 = encode(x[:,6:9,...])
z = torch.cat([z1,z2,z3],-1)
</code></pre></div>

<p>Encoding each frame indiviudally ensures that the contrastive objective only has access to visual discriminants. Comparing the visual and spatiotemporal variants of CURL in Figure 8 shows that the variant trained on stacked frames outperforms the visual-only version in most environments. The only exceptions are reacher and ball-in-cup environments. Indeed, in those environments the visual signal is strong enough to solve the task optimally, whereas in other environments, such as walker and cheetah, where balance or coordination is required, visual information alone is insufficient.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. CURL compared to state-based SAC run for 3 seeds on each of 16 selected DMControl environments. For the 6 environments in 4, CURL performance is averaged over 10 seeds.</p>
<h2>E.2. Increasing Gradient Updates per Agent Step</h2>
<p>Although most baselines we benchmark against use one gradient update per agent step, it was recently empirically shown that increasing the ratio of gradients per step improves data-efficiency in RL (Kielak, 2020). This finding is also supported by SLAC (Lee et al., 2019), where results are shown with a ratio of 1:1 (SLACv1) and 3:1 (SLACv2). We</p>
<p>Table 5. Scores achieved by CURL and SLAC when run with a 3:1 ratio of gradient updates per agent step on DMControl500k and DMControl100k. CURL achieves state-of-the-art performance on the majority ( $\mathbf{3}$ out of $\mathbf{4}$ ) environments on DMControl500k. Performance of both algorithms is improved relative to the 1:1 ratio reported for all baselines in Table 1 but at the cost of significant compute and wall-clock time overhead.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DMCONTROL500K</th>
<th style="text-align: center;">CURL</th>
<th style="text-align: center;">SLACv2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Finger, SPIN</td>
<td style="text-align: center;">$\mathbf{9 2 3} \pm \mathbf{5 0}$</td>
<td style="text-align: center;">$884 \pm 98$</td>
</tr>
<tr>
<td style="text-align: left;">WALKER, WALK</td>
<td style="text-align: center;">$\mathbf{9 1 1} \pm \mathbf{3 5}$</td>
<td style="text-align: center;">$891 \pm 60$</td>
</tr>
<tr>
<td style="text-align: left;">CHEETAH, RUN</td>
<td style="text-align: center;">$545 \pm 39$</td>
<td style="text-align: center;">$\mathbf{7 9 1} \pm \mathbf{3 7}$</td>
</tr>
<tr>
<td style="text-align: left;">BALL IN CUP, CATCH</td>
<td style="text-align: center;">$\mathbf{9 4 8} \pm \mathbf{2 1}$</td>
<td style="text-align: center;">$885 \pm 154$</td>
</tr>
<tr>
<td style="text-align: left;">DMCONTROL100K</td>
<td style="text-align: center;">CURL</td>
<td style="text-align: center;">SLACv2</td>
</tr>
<tr>
<td style="text-align: left;">Finger, SPIN</td>
<td style="text-align: center;">$\mathbf{7 4 1} \pm \mathbf{1 1 8}$</td>
<td style="text-align: center;">$728 \pm 212$</td>
</tr>
<tr>
<td style="text-align: left;">WALKER, WALK</td>
<td style="text-align: center;">$428 \pm 59$</td>
<td style="text-align: center;">$\mathbf{5 1 3} \pm \mathbf{4 1}$</td>
</tr>
<tr>
<td style="text-align: left;">CHEETAH, RUN</td>
<td style="text-align: center;">$314 \pm 46$</td>
<td style="text-align: center;">$\mathbf{4 3 8} \pm \mathbf{7 6}$</td>
</tr>
<tr>
<td style="text-align: left;">BALL IN CUP, CATCH</td>
<td style="text-align: center;">$\mathbf{8 9 9} \pm \mathbf{4 7}$</td>
<td style="text-align: center;">$837 \pm 147$</td>
</tr>
</tbody>
</table>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. CURL with temporal and visual discrimination (red) compared to CURL with only visual discrimination (green). In most settings, the variant with temporal variant outperforms the purely visual variant of CURL. The two exceptions are reacher and ball in cup environments, suggesting that learning dynamics is not necessary for those two environments. Note that the walker environment was run with action repeat of 4 , whereas walker walk in the main results Table 1 and Figure 7 was run with action repeat of 2 .</p>
<h2>E.3. Decoupling Representation Learning from Reinforcement Learning</h2>
<p>Typically, Deep RL representations depend almost entirely on the reward function specific to a task. However, handcrafted representations such as the proprioceptive state are independent of the reward function. It is much more desirable to learn reward-agnostic representations, so that the same representation can be re-used across different RL tasks. We test whether CURL can learn such representations by comparing CURL to a variant where the critic gradients are backpropagated through the critic and contrastive dense feedforward networks but stopped before reaching the convolutional neural network (CNN) part of the encoder.</p>
<p>Scores displayed in Figure 9 show that for many environments, the detached CNN representations are sufficient to learn an optimal policy. The major exception is the cheetah environment, where the detached representation significantly under-performs. Though promising, we leave further exploration of task-agnostic representations for future work.</p>
<h2>E.4. Removing Data Augmentation for the Actor Critic</h2>
<p>Our main results involve the use of data augmentations to regularize both the contrastive and SAC objectives. Here, we investigate whether the contrastive representations alone are sufficient for learning effective policies. In these experiments, we only augment the data for the contrastive
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. CURL where the CNN part of the encoder receives gradients from both the contrastive loss and critic (red) compared to CURL with the convolutional part of the encoder trained only with the contrastive objective (green). The detached encoder variant is able to learn representations that enable near-optimal learning on most environments, except for cheetah. As in Figure 8, the walker environment was run with action repeat of 4 .
objective but not for the SAC agent. As a result, data augmentation is used only to learn features but does not influence the control policy. The pseudocode is shown below:</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> o = original unaugmented observation
<span class="gh">#</span> aug = augmentation
<span class="gh">#</span> contrastive = InfoNCE loss
o_anchor, o_target = aug(o), aug(o)
curl_loss = contrastive(o_anchor, o_target)
sac_loss = critic_loss(o) + actor_loss(o)
loss = curl_loss + sac_loss
params = update(params, grad(loss, params))
</code></pre></div>

<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. CURL with no data augmentations passed to the SAC agent improves the performance of the baseline pixel SAC by a mean of 2.0x / median of 1.7x on DMControl500k. For these runs we use a smaller batch size of 128 than the 512 batch size used for results in Table 4. While the constastive loss alone improves over the pixel SAC baseline, most environments benefit from data augmentation also being passed to the SAC agent.</p>
<p>DMControl500k results plotted in Figure 10 show that, on average, features learned through the contrastive loss alone improve the pixel SAC baseline by 2x. Augmenting the input passed to the SAC algorithm further improves performance.</p>
<h3>E.5 Predicting State from Pixels</h3>
<p>Despite improved sample-efficiency on most DMControl tasks, there is still a visible gap between the performance of SAC on state and SAC with CURL in some environments. Since CURL learns representations by performing instance discrimination across stacks of three frames, it’s possible that the reason for degraded sample-efficiency on more challenging tasks is due to partial-observability of the ground truth state.</p>
<p>To test this hypothesis, we perform supervised regression $(X, Y)$ from pixels $X$ to the proprioceptive state $Y$, where each data point $x \in X$ is a stack of three consecutive frames and $y \in Y$ is the corresponding state extracted from the simulator. We find that the error in predicting the state from pixels correlates with the policy performance of pixelbased methods. Test-time error rates displayed in Figure 11 show that environments that CURL solves as efficiently as state-based SAC have low error-rates in predicting the state from stacks of pixels. The prediction error increases for more challenging environments, such as cheetah-run and walker-walk. Finally, the error is highest for environments where current pixel-based methods, CURL included, make no progress at all (Tassa et al., 2018), such as humanoid and swimmer.</p>
<p>This investigation suggests that degraded policy performance on challenging tasks may result from the lack of requisite information about the underlying state in the pixel data used for learning representations. We leave further investigation for future work.</p>
<h3>E.6. CURL + Efficient Rainbow Atari runs</h3>
<p>We report the scores (Tables 6 and 7) for 20 seeds across the 26 Atari games in the Atari100k benchmark for CURL coupled with Efficient Rainbow. The variance across multiple seeds is considerably high in this benchmark. Therefore, we report the scores for each of the seeds along with the mean and standard deviation for each game.</p>
<h2>F. Document changelog</h2>
<p>This document tracks the progress and changes of CURL. In order to help readers be aware of and understand the changes, here is a brief summary:
v1 Initial version.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Test-time mean squared error for predicting the proprioceptive state from pixels on a number of DMControl environments. In DMControl, environments fall into two groups - where the state corresponds to either (a) positions and velocities of the robot joints or (b) the joint angles and angular velocities.
v2 Minor changes to DMControl to account for frame skip factor when evaluating data-efficiency of CURL and baselines. Changed action repeat for the Walker-walk task from 4 to 2 to match baseline implementations.
v3 ICML 2020 Camera Ready. For our Atari experiments, we moved to the https://github.com/Kaixhin/ Rainbow codebase for easy and clean benchmarking that directly builds on top of Efficient Rainbow without other changes. We also run 20 seeds as opposed to 3 seeds earlier given the high variance nature of the benchmark.
v4 Added in Section E. 4 - an ablation investigating whether contrastive representations alone, with no augmentations passed to the policy during training, improve the baseline SAC policy.</p>
<h2>G. Connection to work on data augmentations</h2>
<p>Recently, there have been two papers published on using data augmentations for reinforcement learning, RAD (Laskin et al., 2020) and DrQ (Kostrikov et al., 2020). These two papers present the version of CURL without an auxiliary contrastive loss but rather directly feeding in the augmented views of the image observations to the underlying value / policy network(s). Both RAD and DrQ present results on both continuous and discrete control environments, surpassing the results presented in CURL on both the DMControl and Atari benchmarks. Plenty of researchers have opined in public forums whether the results in RAD and DrQ make CURL irrelevant if the objective is to use data augmentations for data-efficient reinforcement learning. We believe that answering this question needs more nuance and present our opinions below:</p>
<ol>
<li>If one has access to a rich stream of rewards from the</li>
</ol>
<p>CURL: Contrastive Unsupervised Representations for Reinforcement Learning</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pacman</th>
<th style="text-align: center;">Frostbite</th>
<th style="text-align: center;">Asterix</th>
<th style="text-align: center;">KungFuMaster</th>
<th style="text-align: center;">Kangaroo</th>
<th style="text-align: center;">Gopher</th>
<th style="text-align: center;">RoadRunner</th>
<th style="text-align: center;">JamesBond</th>
<th style="text-align: center;">BattleZone</th>
<th style="text-align: center;">Seaquest</th>
<th style="text-align: center;">Assault</th>
<th style="text-align: center;">Krull</th>
<th style="text-align: center;">Qbert</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1287</td>
<td style="text-align: center;">2292</td>
<td style="text-align: center;">850</td>
<td style="text-align: center;">8470</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">1036</td>
<td style="text-align: center;">2820</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">18100</td>
<td style="text-align: center;">322</td>
<td style="text-align: center;">634.2</td>
<td style="text-align: center;">3404.3</td>
<td style="text-align: center;">1020</td>
</tr>
<tr>
<td style="text-align: center;">1608</td>
<td style="text-align: center;">1046</td>
<td style="text-align: center;">525</td>
<td style="text-align: center;">10870</td>
<td style="text-align: center;">2280</td>
<td style="text-align: center;">574</td>
<td style="text-align: center;">3190</td>
<td style="text-align: center;">265</td>
<td style="text-align: center;">18200</td>
<td style="text-align: center;">236</td>
<td style="text-align: center;">696.8</td>
<td style="text-align: center;">2443.5</td>
<td style="text-align: center;">650</td>
</tr>
<tr>
<td style="text-align: center;">1466</td>
<td style="text-align: center;">1209</td>
<td style="text-align: center;">655</td>
<td style="text-align: center;">10920</td>
<td style="text-align: center;">1940</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">7840</td>
<td style="text-align: center;">335</td>
<td style="text-align: center;">26800</td>
<td style="text-align: center;">352</td>
<td style="text-align: center;">655.2</td>
<td style="text-align: center;">6791.4</td>
<td style="text-align: center;">830</td>
</tr>
<tr>
<td style="text-align: center;">1430</td>
<td style="text-align: center;">255</td>
<td style="text-align: center;">565</td>
<td style="text-align: center;">7730</td>
<td style="text-align: center;">1140</td>
<td style="text-align: center;">618</td>
<td style="text-align: center;">12060</td>
<td style="text-align: center;">145</td>
<td style="text-align: center;">21300</td>
<td style="text-align: center;">386</td>
<td style="text-align: center;">443</td>
<td style="text-align: center;">3022.5</td>
<td style="text-align: center;">902.5</td>
</tr>
<tr>
<td style="text-align: center;">1114</td>
<td style="text-align: center;">426</td>
<td style="text-align: center;">715</td>
<td style="text-align: center;">17525</td>
<td style="text-align: center;">520</td>
<td style="text-align: center;">534</td>
<td style="text-align: center;">8340</td>
<td style="text-align: center;">565</td>
<td style="text-align: center;">7900</td>
<td style="text-align: center;">458</td>
<td style="text-align: center;">546</td>
<td style="text-align: center;">3892.2</td>
<td style="text-align: center;">3957.5</td>
</tr>
<tr>
<td style="text-align: center;">1083</td>
<td style="text-align: center;">2280</td>
<td style="text-align: center;">715</td>
<td style="text-align: center;">3560</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">596</td>
<td style="text-align: center;">6920</td>
<td style="text-align: center;">565</td>
<td style="text-align: center;">8100</td>
<td style="text-align: center;">224</td>
<td style="text-align: center;">564.9</td>
<td style="text-align: center;">3505.5</td>
<td style="text-align: center;">772.5</td>
</tr>
<tr>
<td style="text-align: center;">2301</td>
<td style="text-align: center;">259</td>
<td style="text-align: center;">770</td>
<td style="text-align: center;">10940</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">502</td>
<td style="text-align: center;">2230</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">12000</td>
<td style="text-align: center;">282</td>
<td style="text-align: center;">514.4</td>
<td style="text-align: center;">2564.1</td>
<td style="text-align: center;">782.5</td>
</tr>
<tr>
<td style="text-align: center;">1128</td>
<td style="text-align: center;">335</td>
<td style="text-align: center;">980</td>
<td style="text-align: center;">23420</td>
<td style="text-align: center;">900</td>
<td style="text-align: center;">998</td>
<td style="text-align: center;">4250</td>
<td style="text-align: center;">365</td>
<td style="text-align: center;">16500</td>
<td style="text-align: center;">339</td>
<td style="text-align: center;">516.6</td>
<td style="text-align: center;">4079.7</td>
<td style="text-align: center;">727.5</td>
</tr>
<tr>
<td style="text-align: center;">1184</td>
<td style="text-align: center;">1409</td>
<td style="text-align: center;">665</td>
<td style="text-align: center;">15160</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">950</td>
<td style="text-align: center;">1570</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">23900</td>
<td style="text-align: center;">526</td>
<td style="text-align: center;">661.5</td>
<td style="text-align: center;">2376.4</td>
<td style="text-align: center;">705</td>
</tr>
<tr>
<td style="text-align: center;">1510</td>
<td style="text-align: center;">258</td>
<td style="text-align: center;">610</td>
<td style="text-align: center;">15370</td>
<td style="text-align: center;">730</td>
<td style="text-align: center;">544</td>
<td style="text-align: center;">6300</td>
<td style="text-align: center;">425</td>
<td style="text-align: center;">19900</td>
<td style="text-align: center;">436</td>
<td style="text-align: center;">664.5</td>
<td style="text-align: center;">4161.8</td>
<td style="text-align: center;">757.5</td>
</tr>
<tr>
<td style="text-align: center;">2343</td>
<td style="text-align: center;">335</td>
<td style="text-align: center;">905</td>
<td style="text-align: center;">22260</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">796</td>
<td style="text-align: center;">3100</td>
<td style="text-align: center;">315</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">272</td>
<td style="text-align: center;">529</td>
<td style="text-align: center;">3311.1</td>
<td style="text-align: center;">647.5</td>
</tr>
<tr>
<td style="text-align: center;">1063</td>
<td style="text-align: center;">1062</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">17320</td>
<td style="text-align: center;">880</td>
<td style="text-align: center;">522</td>
<td style="text-align: center;">1060</td>
<td style="text-align: center;">335</td>
<td style="text-align: center;">11200</td>
<td style="text-align: center;">428</td>
<td style="text-align: center;">445.2</td>
<td style="text-align: center;">2517.3</td>
<td style="text-align: center;">562.5</td>
</tr>
<tr>
<td style="text-align: center;">2040</td>
<td style="text-align: center;">1542</td>
<td style="text-align: center;">675</td>
<td style="text-align: center;">31820</td>
<td style="text-align: center;">220</td>
<td style="text-align: center;">392</td>
<td style="text-align: center;">6050</td>
<td style="text-align: center;">735</td>
<td style="text-align: center;">9700</td>
<td style="text-align: center;">358</td>
<td style="text-align: center;">573.3</td>
<td style="text-align: center;">3764.7</td>
<td style="text-align: center;">2425</td>
</tr>
<tr>
<td style="text-align: center;">1195</td>
<td style="text-align: center;">1102</td>
<td style="text-align: center;">795</td>
<td style="text-align: center;">23360</td>
<td style="text-align: center;">920</td>
<td style="text-align: center;">780</td>
<td style="text-align: center;">11810</td>
<td style="text-align: center;">950</td>
<td style="text-align: center;">23500</td>
<td style="text-align: center;">533</td>
<td style="text-align: center;">531.3</td>
<td style="text-align: center;">10150.2</td>
<td style="text-align: center;">1112.5</td>
</tr>
<tr>
<td style="text-align: center;">1343</td>
<td style="text-align: center;">2461</td>
<td style="text-align: center;">585</td>
<td style="text-align: center;">27460</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">792</td>
<td style="text-align: center;">4630</td>
<td style="text-align: center;">520</td>
<td style="text-align: center;">10500</td>
<td style="text-align: center;">968</td>
<td style="text-align: center;">663.6</td>
<td style="text-align: center;">2883.6</td>
<td style="text-align: center;">527.5</td>
</tr>
<tr>
<td style="text-align: center;">1354</td>
<td style="text-align: center;">257</td>
<td style="text-align: center;">865</td>
<td style="text-align: center;">7770</td>
<td style="text-align: center;">2300</td>
<td style="text-align: center;">454</td>
<td style="text-align: center;">2530</td>
<td style="text-align: center;">755</td>
<td style="text-align: center;">18100</td>
<td style="text-align: center;">314</td>
<td style="text-align: center;">795.3</td>
<td style="text-align: center;">5123.7</td>
<td style="text-align: center;">472.5</td>
</tr>
<tr>
<td style="text-align: center;">1925</td>
<td style="text-align: center;">513</td>
<td style="text-align: center;">730</td>
<td style="text-align: center;">8820</td>
<td style="text-align: center;">320</td>
<td style="text-align: center;">564</td>
<td style="text-align: center;">6840</td>
<td style="text-align: center;">750</td>
<td style="text-align: center;">9000</td>
<td style="text-align: center;">378</td>
<td style="text-align: center;">633</td>
<td style="text-align: center;">3652.5</td>
<td style="text-align: center;">610</td>
</tr>
<tr>
<td style="text-align: center;">1228</td>
<td style="text-align: center;">1826</td>
<td style="text-align: center;">680</td>
<td style="text-align: center;">2980</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">522</td>
<td style="text-align: center;">6580</td>
<td style="text-align: center;">795</td>
<td style="text-align: center;">8900</td>
<td style="text-align: center;">168</td>
<td style="text-align: center;">674.1</td>
<td style="text-align: center;">2376.4</td>
<td style="text-align: center;">697.5</td>
</tr>
<tr>
<td style="text-align: center;">1099</td>
<td style="text-align: center;">1889</td>
<td style="text-align: center;">965</td>
<td style="text-align: center;">10100</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">496</td>
<td style="text-align: center;">10720</td>
<td style="text-align: center;">450</td>
<td style="text-align: center;">10700</td>
<td style="text-align: center;">242</td>
<td style="text-align: center;">604.8</td>
<td style="text-align: center;">11745</td>
<td style="text-align: center;">1847.5</td>
</tr>
<tr>
<td style="text-align: center;">1608</td>
<td style="text-align: center;">2869</td>
<td style="text-align: center;">640</td>
<td style="text-align: center;">10300</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1176</td>
<td style="text-align: center;">4380</td>
<td style="text-align: center;">355</td>
<td style="text-align: center;">13100</td>
<td style="text-align: center;">467</td>
<td style="text-align: center;">665.7</td>
<td style="text-align: center;">2826</td>
<td style="text-align: center;">840</td>
</tr>
<tr>
<td style="text-align: center;">1465.5</td>
<td style="text-align: center;">1181.3</td>
<td style="text-align: center;">734.5</td>
<td style="text-align: center;">14307.8</td>
<td style="text-align: center;">872.5</td>
<td style="text-align: center;">669.3</td>
<td style="text-align: center;">5661</td>
<td style="text-align: center;">471</td>
<td style="text-align: center;">14870</td>
<td style="text-align: center;">384.5</td>
<td style="text-align: center;">600.6</td>
<td style="text-align: center;">4229.6</td>
<td style="text-align: center;">1042.4</td>
</tr>
<tr>
<td style="text-align: center;">397.5</td>
<td style="text-align: center;">856.2</td>
<td style="text-align: center;">129.8</td>
<td style="text-align: center;">7919.3</td>
<td style="text-align: center;">600.1</td>
<td style="text-align: center;">220.6</td>
<td style="text-align: center;">3289.3</td>
<td style="text-align: center;">226.2</td>
<td style="text-align: center;">5964.3</td>
<td style="text-align: center;">170.2</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">2540.6</td>
<td style="text-align: center;">828.4</td>
</tr>
</tbody>
</table>
<p>Table 6. CURL implemented on top of Efficient Rainbow - Scores reported for 20 random seeds for each of the above games, with the last two rows being the mean and standard deviation across the runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">UpNDown</th>
<th style="text-align: center;">Hero</th>
<th style="text-align: center;">CrazyClimber</th>
<th style="text-align: center;">ChopperComm.</th>
<th style="text-align: center;">DemonAttack</th>
<th style="text-align: center;">Amidar</th>
<th style="text-align: center;">Alien</th>
<th style="text-align: center;">BankHeist</th>
<th style="text-align: center;">Breakout</th>
<th style="text-align: center;">Freeway</th>
<th style="text-align: center;">Pong</th>
<th style="text-align: center;">PrivateEye</th>
<th style="text-align: center;">Boxing</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3529</td>
<td style="text-align: center;">8747.5</td>
<td style="text-align: center;">19090</td>
<td style="text-align: center;">560</td>
<td style="text-align: center;">611.5</td>
<td style="text-align: center;">150.9</td>
<td style="text-align: center;">616</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">-19.3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-0.5</td>
</tr>
<tr>
<td style="text-align: center;">772</td>
<td style="text-align: center;">3026</td>
<td style="text-align: center;">8290</td>
<td style="text-align: center;">1530</td>
<td style="text-align: center;">707.5</td>
<td style="text-align: center;">131.2</td>
<td style="text-align: center;">923</td>
<td style="text-align: center;">184</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">-16.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-11.4</td>
</tr>
<tr>
<td style="text-align: center;">5972</td>
<td style="text-align: center;">7146</td>
<td style="text-align: center;">12160</td>
<td style="text-align: center;">1390</td>
<td style="text-align: center;">843.5</td>
<td style="text-align: center;">141.5</td>
<td style="text-align: center;">467</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">-12</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">2793</td>
<td style="text-align: center;">7686</td>
<td style="text-align: center;">8920</td>
<td style="text-align: center;">1100</td>
<td style="text-align: center;">330.5</td>
<td style="text-align: center;">133.7</td>
<td style="text-align: center;">441</td>
<td style="text-align: center;">232</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">-19.6</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: center;">3546</td>
<td style="text-align: center;">7335</td>
<td style="text-align: center;">11360</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">759</td>
<td style="text-align: center;">157.1</td>
<td style="text-align: center;">716</td>
<td style="text-align: center;">187</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">-17.8</td>
<td style="text-align: center;">1357.4</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;">4552</td>
<td style="text-align: center;">7325</td>
<td style="text-align: center;">4110</td>
<td style="text-align: center;">990</td>
<td style="text-align: center;">940</td>
<td style="text-align: center;">125.4</td>
<td style="text-align: center;">453</td>
<td style="text-align: center;">367</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">-18.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">2972</td>
<td style="text-align: center;">7275.5</td>
<td style="text-align: center;">9460</td>
<td style="text-align: center;">780</td>
<td style="text-align: center;">1136</td>
<td style="text-align: center;">183.2</td>
<td style="text-align: center;">273</td>
<td style="text-align: center;">186</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">-15.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-1.7</td>
</tr>
<tr>
<td style="text-align: center;">2865</td>
<td style="text-align: center;">3115</td>
<td style="text-align: center;">20630</td>
<td style="text-align: center;">1180</td>
<td style="text-align: center;">758</td>
<td style="text-align: center;">153.6</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">-15.2</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">3098</td>
<td style="text-align: center;">7424</td>
<td style="text-align: center;">6780</td>
<td style="text-align: center;">1380</td>
<td style="text-align: center;">772.5</td>
<td style="text-align: center;">127.8</td>
<td style="text-align: center;">499</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-18.7</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">3.5</td>
</tr>
<tr>
<td style="text-align: center;">1953</td>
<td style="text-align: center;">7475</td>
<td style="text-align: center;">13570</td>
<td style="text-align: center;">970</td>
<td style="text-align: center;">820</td>
<td style="text-align: center;">149.4</td>
<td style="text-align: center;">475</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">-13.3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-0.5</td>
</tr>
<tr>
<td style="text-align: center;">1467</td>
<td style="text-align: center;">3135</td>
<td style="text-align: center;">11890</td>
<td style="text-align: center;">1200</td>
<td style="text-align: center;">784</td>
<td style="text-align: center;">125.7</td>
<td style="text-align: center;">553</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">-17.2</td>
<td style="text-align: center;">1510</td>
<td style="text-align: center;">-22.1</td>
</tr>
<tr>
<td style="text-align: center;">2912</td>
<td style="text-align: center;">5060.5</td>
<td style="text-align: center;">9160</td>
<td style="text-align: center;">1130</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">130.4</td>
<td style="text-align: center;">446</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">-20.1</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-1.8</td>
</tr>
<tr>
<td style="text-align: center;">4123</td>
<td style="text-align: center;">4409</td>
<td style="text-align: center;">10960</td>
<td style="text-align: center;">1380</td>
<td style="text-align: center;">847</td>
<td style="text-align: center;">133</td>
<td style="text-align: center;">533</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">-16.5</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: center;">2334</td>
<td style="text-align: center;">6979</td>
<td style="text-align: center;">17360</td>
<td style="text-align: center;">1230</td>
<td style="text-align: center;">771.5</td>
<td style="text-align: center;">140.5</td>
<td style="text-align: center;">968</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">-14.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: center;">2605</td>
<td style="text-align: center;">4159</td>
<td style="text-align: center;">8930</td>
<td style="text-align: center;">1350</td>
<td style="text-align: center;">907.5</td>
<td style="text-align: center;">133.8</td>
<td style="text-align: center;">499</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">-19.3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-17.6</td>
</tr>
<tr>
<td style="text-align: center;">2432</td>
<td style="text-align: center;">7560</td>
<td style="text-align: center;">11510</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">1095.5</td>
<td style="text-align: center;">191.8</td>
<td style="text-align: center;">523</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">-15.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: center;">3826</td>
<td style="text-align: center;">8587</td>
<td style="text-align: center;">22690</td>
<td style="text-align: center;">1210</td>
<td style="text-align: center;">700</td>
<td style="text-align: center;">115.5</td>
<td style="text-align: center;">616</td>
<td style="text-align: center;">276</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">-21</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">3052</td>
<td style="text-align: center;">4683.5</td>
<td style="text-align: center;">8120</td>
<td style="text-align: center;">840</td>
<td style="text-align: center;">803.5</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">475</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">-10.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5.9</td>
</tr>
<tr>
<td style="text-align: center;">3131</td>
<td style="text-align: center;">7317</td>
<td style="text-align: center;">13500</td>
<td style="text-align: center;">730</td>
<td style="text-align: center;">818</td>
<td style="text-align: center;">131.7</td>
<td style="text-align: center;">525</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">-13.3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">18.7</td>
</tr>
<tr>
<td style="text-align: center;">1169</td>
<td style="text-align: center;">7141</td>
<td style="text-align: center;">14440</td>
<td style="text-align: center;">640</td>
<td style="text-align: center;">866</td>
<td style="text-align: center;">122.4</td>
<td style="text-align: center;">622</td>
<td style="text-align: center;">273</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">-13.1</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: center;">2955.2</td>
<td style="text-align: center;">6279.3</td>
<td style="text-align: center;">12146.5</td>
<td style="text-align: center;">1058.5</td>
<td style="text-align: center;">817.6</td>
<td style="text-align: center;">142.1</td>
<td style="text-align: center;">558.2</td>
<td style="text-align: center;">131.6</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">-16.5</td>
<td style="text-align: center;">218.4</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;">1181.1</td>
<td style="text-align: center;">1871.5</td>
<td style="text-align: center;">4765.6</td>
<td style="text-align: center;">299.1</td>
<td style="text-align: center;">176.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">160.3</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">417.9</td>
<td style="text-align: center;">10.0</td>
</tr>
</tbody>
</table>
<p>Table 7. CURL implemented on top of Efficient Rainbow - Scores reported for 20 random seeds for each of the above games, with the last two rows being the mean and standard deviation across the runs.
underlying environment and is interested in optimizing the performance in terms of average reward, RAD and DrQ are likely to work better than CURL. The reason for this is simply that RAD and DrQ directly optimize for the objective one cares about, while CURL introduces an additional auxiliary consistency objective.
2. If one does not have access to a rich stream of rewards and
is interested in learning good latent spaces in a task agnostic manner that can allow for data-efficient controllers across multiple tasks, CURL is the only option since the contrastive objective in CURL is reward independent. Our ablation on the detached encoder with the CURL objective present evidence that one could build simple MLPs on top of the CURL features without fine-tuning the underlying encoder and still be data-efficient on many of the DMControl tasks.</p>
<ol>
<li>Future work in data-efficient reinforcement learning, particularly for real world settings, is likely to require approaches that do not rely on reward functions. In such scenarios, CURL is likely to be the more preferred approach. Further, one could potentially use CURL in a scenario where unsupervised pre-training without reward functions is initially performed before fine-tuning to the RL objective across multiple tasks.</li>
</ol>
<p>Given the above reasons, there isn't a straightforward answer as to which is the better algorithm and the answer really depends on what the researcher / practioner wants to solve. We also emphasize that CURL was the first approach that used data augmentations effectively to significantly improve the data-efficiency of model-free reinforcement learning methods with very simple changes and showed improvement over relatively more complex model-based methods. The augmentations and results in CURL inspired future work in the form of RAD and DrQ. We hope that the analysis and results presented in CURL encourage researchers to employ data augmentations, contrastive losses and unsupervised pre-training for future reinforcement learning research.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\circ}$ Equal contribution ${ }^{1}$ University of California, Berkeley, BAIR. Correspondence to: Aravind Srinivas, Michael Laskin <aravind_srinivas, mlaskin@berkeley.edu $>$.</p>
<p>Proceedings of the $37^{\text {th }}$ International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
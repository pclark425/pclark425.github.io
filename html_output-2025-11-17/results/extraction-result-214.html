<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-214 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-214</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-214</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-c6247a6de28885098477084f779d268f6e619ddb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c6247a6de28885098477084f779d268f6e619ddb" target="_blank">Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a specialized training framework called Reasoning Paths Optimization (RPO), which enables learning to reason and explore from diverse paths in language model reasoning, and significantly enhances the reasoning performance of large language models.</p>
                <p><strong>Paper Abstract:</strong> Advanced models such as OpenAI o1 exhibit impressive problem-solving capabilities through step-by-step reasoning. However, they may still falter on more complex problems, making errors that disrupt their reasoning paths. We attribute this to the expansive solution space, where each step has the risk of diverging into mistakes. To enhance language model reasoning, we introduce a specialized training framework called Reasoning Paths Optimization (RPO), which enables learning to reason and explore from diverse paths. Our approach encourages favorable branches at each reasoning step while penalizing unfavorable ones, enhancing the model's overall problem-solving performance. Reasoning Paths Optimization does not rely on large-scale human-annotated rationales or outputs from closed-source models, making it scalable and data-efficient. We focus on multi-step reasoning tasks, such as math word problems and science-based exam questions. The experiments demonstrate that our framework significantly enhances the reasoning performance of large language models, with up to 3.1% and 4.3% improvement on GSM8K and MMLU (STEM) respectively. Our data and code can be found at https://reasoning-paths.github.io.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e214.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e214.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard supervised fine-tuning where the model is trained to map inputs (questions) to ground-truth final answers without step-level supervision or contrastive preference signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B, Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA-3-8B), 7B (Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Question + ground-truth final answers (no chain-of-thought/rationale training). Uses the original train splits (final-answer supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>GSM8K: 7473 answers used (per Table 6, both models); MMLU-STEM: 3000 training examples (dataset split described in Appendix A.2)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>No step-by-step reasoning chains; single final-answer supervision; simple/short-answer format; does not encode intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (primary); Inter. F1 reported elsewhere for reasoning quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Varies by dataset; examples: MMLU (LLaMA-3-8B) accuracy 63.7% (Table 8); CSQA 82.7%, Winogrande 86.0% (Table 8). GSM8K final-answer accuracy not listed as a single SFT number in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>SFT using only final-answer labels performs worse on multi-step reasoning benchmarks than methods trained on self-generated reasoning paths or preference-based objectives; single-step supervision is often insufficient for complex multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e214.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e214.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection-sampling Fine-Tuning (RFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuning variant that uses model self-sampled chain-of-thought (CoT) reasoning paths as training targets; incorrect sampled paths are rejected and only acceptable sampled paths are used for supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B, Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA-3-8B), 7B (Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Self-generated chain-of-thought reasoning paths produced via chain-of-thought prompting (temperature sampling); then used as supervised targets for LM fine-tuning (rejection sampling keeps only sampled paths that reach the correct answer).</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>GSM8K: 6417 reasoning paths for LLaMA-3-8B, 5922 for Mistral (Table 6). MMLU-STEM: filtered samples where model can generate at least one correct reasoning path (size not separately enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Model-generated CoT rationales; contains step-by-step reasoning; filtered to accepted (correct final answer) paths; diversity arises from temperature sampling and repeated attempts (up to 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (primary) and Inter. F1 for reasoning consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Inter. F1 on GSM8K: 77.5 (Table 7). Example accuracies (LLaMA-3-8B, Table 8): CSQA 72.3%, Winogrande 64.2%, MMLU 59.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Compared to SFT (e.g., SFT MMLU 63.7%), RFT can be lower on some general QA benchmarks but improves reasoning consistency vs plain SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Improves reasoning-path alignment relative to SFT (higher Inter. F1 vs SFT not explicitly tabulated), but not always an accuracy win; context-dependent (sometimes lower accuracy vs SFT on certain non-multi-step tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Using self-generated correct reasoning paths as supervised targets (RFT) improves process-level alignment (Inter. F1) but does not uniformly improve or surpass preference-optimization methods on final-answer accuracy across datasets; effectiveness depends on model's ability to self-generate correct paths and the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e214.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e214.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preference-optimization method that trains the model to assign higher likelihood to preferred responses than to rejected ones without training an explicit reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Direct preference optimization: Your language model is secretly a reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B, Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA-3-8B), 7B (Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Preference pairs (favored vs disfavored responses) produced by model self-sampling; training pushes model to prefer the accepted response over the rejected one.</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>GSM8K: 5667 reasoning path pairs for LLaMA-3-8B, 5535 for Mistral (Table 6). Only samples where model can generate at least one correct path and one incorrect path are used.</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Pairwise preference supervision; full-response comparisons (DPO treats responses in entirety rather than step-local differences); sampled via CoT prompting and temperature sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (primary); Inter. F1 for reasoning quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Inter. F1 on GSM8K: 79.0 (Table 7). Example accuracies (LLaMA-3-8B, Table 8): CSQA 72.7%, Winogrande 57.9%, MMLU 56.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Compared to SFT and RFT baselines; DPO sometimes improves final-answer accuracy but can be less effective on deep multi-step reasoning according to the authors' observations and related work references.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Mixed: DPO provides contrastive signal and can help alignment, but prior/this work notes preference optimization methods may be less effective or even detrimental on in-depth reasoning tasks when they indiscriminately compare whole responses.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Global preference optimization (DPO) gives contrastive learning signal but may overlook step-local errors in multi-step reasoning; comparing full responses can miss the fact that errors are often localized to specific steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e214.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e214.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Odds-Ratio Preference Optimization (ORPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preference optimization objective that formulates the branch-pair preference using an odds-ratio (log-odds) objective to push the model to prefer favorable outputs over unfavorable ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Orpo: Monolithic preference optimization without reference model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B, Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA-3-8B), 7B (Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Preference pairs (favored vs disfavored) sampled by model; objective uses odds-ratio between favorable and unfavorable responses (log-odds) for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>GSM8K: 5667 reasoning path pairs for LLaMA-3-8B, 5535 for Mistral (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Pairwise preference supervision; compares full responses via an odds-ratio formulation; samples require at least one correct and one incorrect path.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (primary); Inter. F1 for reasoning quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>GSM8K (text) accuracy: 61.7% (Table 2). GSM8K (code) 61.6%. Inter. F1 on GSM8K: 82.7 (Table 7). Example accuracies (LLaMA-3-8B, Table 8): CSQA 76.8%, Winogrande 67.0%, MMLU 59.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>State-of-the-art preference-based baseline in the paper prior to RPO (best-performing baseline on GSM8K among preference methods).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Serves as the strongest baseline among preference methods; RPO shows consistent improvements over ORPO (e.g., GSM8K text: ORPO 61.7% → RPO 64.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Odds-ratio preference optimization is an effective preference objective for reasoning tasks, but a method that provides step-local contrast (exploring branches at intermediate steps) can further improve performance on longer multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e214.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e214.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning Paths Optimization (RPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's training framework that (1) generates reference CoT reasoning paths by prompting, (2) explores diverse branches at each intermediate step to obtain favorable and unfavorable branches, and (3) optimizes the model with a combination of reference (supervised) loss and contrastive branch-pair loss (odds-ratio or DPO-style).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B, Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA-3-8B), 7B (Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Model-generated reference chain-of-thought paths (via 4-shot CoT prompting) plus explored branch pairs sampled at each step (favorable branches that reach correct answer and unfavorable branches that do not). Optimization uses supervised causal LM loss on reference paths plus a contrastive exploration loss (log-odds / log-sigmoid over branch pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>GSM8K: 5752 reference paths for LLaMA-3-8B and 5600 for Mistral (Table 6). MMLU-STEM dataset split: 3000 training examples (Appendix A.2). CoT demonstrations: 4-shot (4 ground-truth CoT examples).</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Diverse, step-local branch pairs (favorable vs unfavorable); requires verification of final answer (A in final step); branch pairs exclude common correct prefixes; limits on sampling attempts (up to 10) to form pairs; designed to capture localized errors along reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (primary); Inter. F1 for reasoning-path quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>GSM8K (text) accuracy: 64.2% (RPO) vs 61.7% (ORPO) (Table 2) — absolute +2.5 points; GSM8K (code) 63.4% vs ORPO 61.6% (+1.8). Reported maximum improvements: up to +3.1% on GSM8K and +4.3% on MMLU-STEM compared to the highest-performing baseline (main text). Inter. F1 on GSM8K: 80.9% (RPO) vs ORPO 79.5% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Best-performing baseline prior to RPO often ORPO (e.g., GSM8K ORPO 61.7%); other baselines include DPO, RFT, and SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>+2–4.3 percentage points absolute improvement on reported benchmarks (examples: +3.1% GSM8K, +4.3% MMLU-STEM; also reported +2.5 for GSM8K text in Table 2). Improves Inter. F1 by ~1–3 points over baselines in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Combining a small number of ground-truth CoT demonstrations for prompting with model-generated reference paths and explicit exploration of favorable/unfavorable branches (contrastive, step-local supervision) yields consistent gains in final-answer accuracy and reasoning-path quality, especially for longer multi-step reasoning and STEM exam questions; balancing the supervised reference loss and the exploration (contrastive) loss (λ) is important for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct preference optimization: Your language model is secretly a reward model. <em>(Rating: 2)</em></li>
                <li>Orpo: Monolithic preference optimization without reference model. <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>Learning to summarize with human feedback. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-214",
    "paper_id": "paper-c6247a6de28885098477084f779d268f6e619ddb",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "SFT",
            "name_full": "Supervised Fine-Tuning",
            "brief_description": "Standard supervised fine-tuning where the model is trained to map inputs (questions) to ground-truth final answers without step-level supervision or contrastive preference signals.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B, Mistral-7B",
            "model_size": "8B (LLaMA-3-8B), 7B (Mistral-7B)",
            "training_stage": "SFT",
            "task_type": "multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)",
            "is_scientific_domain": true,
            "data_type": "Question + ground-truth final answers (no chain-of-thought/rationale training). Uses the original train splits (final-answer supervision).",
            "data_size": "GSM8K: 7473 answers used (per Table 6, both models); MMLU-STEM: 3000 training examples (dataset split described in Appendix A.2)",
            "data_properties": "No step-by-step reasoning chains; single final-answer supervision; simple/short-answer format; does not encode intermediate reasoning.",
            "performance_metric": "accuracy (primary); Inter. F1 reported elsewhere for reasoning quality",
            "performance_with_data": "Varies by dataset; examples: MMLU (LLaMA-3-8B) accuracy 63.7% (Table 8); CSQA 82.7%, Winogrande 86.0% (Table 8). GSM8K final-answer accuracy not listed as a single SFT number in main tables.",
            "performance_baseline": null,
            "performance_lift": null,
            "compares_data_types": true,
            "key_finding": "SFT using only final-answer labels performs worse on multi-step reasoning benchmarks than methods trained on self-generated reasoning paths or preference-based objectives; single-step supervision is often insufficient for complex multi-step reasoning.",
            "uuid": "e214.0",
            "source_info": {
                "paper_title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RFT",
            "name_full": "Rejection-sampling Fine-Tuning (RFT)",
            "brief_description": "A supervised fine-tuning variant that uses model self-sampled chain-of-thought (CoT) reasoning paths as training targets; incorrect sampled paths are rejected and only acceptable sampled paths are used for supervised training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B, Mistral-7B",
            "model_size": "8B (LLaMA-3-8B), 7B (Mistral-7B)",
            "training_stage": "SFT",
            "task_type": "multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)",
            "is_scientific_domain": true,
            "data_type": "Self-generated chain-of-thought reasoning paths produced via chain-of-thought prompting (temperature sampling); then used as supervised targets for LM fine-tuning (rejection sampling keeps only sampled paths that reach the correct answer).",
            "data_size": "GSM8K: 6417 reasoning paths for LLaMA-3-8B, 5922 for Mistral (Table 6). MMLU-STEM: filtered samples where model can generate at least one correct reasoning path (size not separately enumerated).",
            "data_properties": "Model-generated CoT rationales; contains step-by-step reasoning; filtered to accepted (correct final answer) paths; diversity arises from temperature sampling and repeated attempts (up to 10).",
            "performance_metric": "accuracy (primary) and Inter. F1 for reasoning consistency",
            "performance_with_data": "Inter. F1 on GSM8K: 77.5 (Table 7). Example accuracies (LLaMA-3-8B, Table 8): CSQA 72.3%, Winogrande 64.2%, MMLU 59.6%.",
            "performance_baseline": "Compared to SFT (e.g., SFT MMLU 63.7%), RFT can be lower on some general QA benchmarks but improves reasoning consistency vs plain SFT.",
            "performance_lift": "Improves reasoning-path alignment relative to SFT (higher Inter. F1 vs SFT not explicitly tabulated), but not always an accuracy win; context-dependent (sometimes lower accuracy vs SFT on certain non-multi-step tasks).",
            "compares_data_types": true,
            "key_finding": "Using self-generated correct reasoning paths as supervised targets (RFT) improves process-level alignment (Inter. F1) but does not uniformly improve or surpass preference-optimization methods on final-answer accuracy across datasets; effectiveness depends on model's ability to self-generate correct paths and the dataset.",
            "uuid": "e214.1",
            "source_info": {
                "paper_title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DPO",
            "name_full": "Direct Preference Optimization (DPO)",
            "brief_description": "A preference-optimization method that trains the model to assign higher likelihood to preferred responses than to rejected ones without training an explicit reward model.",
            "citation_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B, Mistral-7B",
            "model_size": "8B (LLaMA-3-8B), 7B (Mistral-7B)",
            "training_stage": "DPO",
            "task_type": "multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)",
            "is_scientific_domain": true,
            "data_type": "Preference pairs (favored vs disfavored responses) produced by model self-sampling; training pushes model to prefer the accepted response over the rejected one.",
            "data_size": "GSM8K: 5667 reasoning path pairs for LLaMA-3-8B, 5535 for Mistral (Table 6). Only samples where model can generate at least one correct path and one incorrect path are used.",
            "data_properties": "Pairwise preference supervision; full-response comparisons (DPO treats responses in entirety rather than step-local differences); sampled via CoT prompting and temperature sampling.",
            "performance_metric": "accuracy (primary); Inter. F1 for reasoning quality",
            "performance_with_data": "Inter. F1 on GSM8K: 79.0 (Table 7). Example accuracies (LLaMA-3-8B, Table 8): CSQA 72.7%, Winogrande 57.9%, MMLU 56.5%.",
            "performance_baseline": "Compared to SFT and RFT baselines; DPO sometimes improves final-answer accuracy but can be less effective on deep multi-step reasoning according to the authors' observations and related work references.",
            "performance_lift": "Mixed: DPO provides contrastive signal and can help alignment, but prior/this work notes preference optimization methods may be less effective or even detrimental on in-depth reasoning tasks when they indiscriminately compare whole responses.",
            "compares_data_types": true,
            "key_finding": "Global preference optimization (DPO) gives contrastive learning signal but may overlook step-local errors in multi-step reasoning; comparing full responses can miss the fact that errors are often localized to specific steps.",
            "uuid": "e214.2",
            "source_info": {
                "paper_title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ORPO",
            "name_full": "Odds-Ratio Preference Optimization (ORPO)",
            "brief_description": "A preference optimization objective that formulates the branch-pair preference using an odds-ratio (log-odds) objective to push the model to prefer favorable outputs over unfavorable ones.",
            "citation_title": "Orpo: Monolithic preference optimization without reference model.",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B, Mistral-7B",
            "model_size": "8B (LLaMA-3-8B), 7B (Mistral-7B)",
            "training_stage": "DPO",
            "task_type": "multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)",
            "is_scientific_domain": true,
            "data_type": "Preference pairs (favored vs disfavored) sampled by model; objective uses odds-ratio between favorable and unfavorable responses (log-odds) for optimization.",
            "data_size": "GSM8K: 5667 reasoning path pairs for LLaMA-3-8B, 5535 for Mistral (Table 6).",
            "data_properties": "Pairwise preference supervision; compares full responses via an odds-ratio formulation; samples require at least one correct and one incorrect path.",
            "performance_metric": "accuracy (primary); Inter. F1 for reasoning quality",
            "performance_with_data": "GSM8K (text) accuracy: 61.7% (Table 2). GSM8K (code) 61.6%. Inter. F1 on GSM8K: 82.7 (Table 7). Example accuracies (LLaMA-3-8B, Table 8): CSQA 76.8%, Winogrande 67.0%, MMLU 59.7%.",
            "performance_baseline": "State-of-the-art preference-based baseline in the paper prior to RPO (best-performing baseline on GSM8K among preference methods).",
            "performance_lift": "Serves as the strongest baseline among preference methods; RPO shows consistent improvements over ORPO (e.g., GSM8K text: ORPO 61.7% → RPO 64.2%).",
            "compares_data_types": true,
            "key_finding": "Odds-ratio preference optimization is an effective preference objective for reasoning tasks, but a method that provides step-local contrast (exploring branches at intermediate steps) can further improve performance on longer multi-step reasoning.",
            "uuid": "e214.3",
            "source_info": {
                "paper_title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RPO",
            "name_full": "Reasoning Paths Optimization (RPO)",
            "brief_description": "This paper's training framework that (1) generates reference CoT reasoning paths by prompting, (2) explores diverse branches at each intermediate step to obtain favorable and unfavorable branches, and (3) optimizes the model with a combination of reference (supervised) loss and contrastive branch-pair loss (odds-ratio or DPO-style).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B, Mistral-7B",
            "model_size": "8B (LLaMA-3-8B), 7B (Mistral-7B)",
            "training_stage": "multiple",
            "task_type": "multi-step reasoning QA (math benchmarks GSM8K, MATH; STEM exam questions MMLU-STEM)",
            "is_scientific_domain": true,
            "data_type": "Model-generated reference chain-of-thought paths (via 4-shot CoT prompting) plus explored branch pairs sampled at each step (favorable branches that reach correct answer and unfavorable branches that do not). Optimization uses supervised causal LM loss on reference paths plus a contrastive exploration loss (log-odds / log-sigmoid over branch pairs).",
            "data_size": "GSM8K: 5752 reference paths for LLaMA-3-8B and 5600 for Mistral (Table 6). MMLU-STEM dataset split: 3000 training examples (Appendix A.2). CoT demonstrations: 4-shot (4 ground-truth CoT examples).",
            "data_properties": "Diverse, step-local branch pairs (favorable vs unfavorable); requires verification of final answer (A in final step); branch pairs exclude common correct prefixes; limits on sampling attempts (up to 10) to form pairs; designed to capture localized errors along reasoning paths.",
            "performance_metric": "accuracy (primary); Inter. F1 for reasoning-path quality",
            "performance_with_data": "GSM8K (text) accuracy: 64.2% (RPO) vs 61.7% (ORPO) (Table 2) — absolute +2.5 points; GSM8K (code) 63.4% vs ORPO 61.6% (+1.8). Reported maximum improvements: up to +3.1% on GSM8K and +4.3% on MMLU-STEM compared to the highest-performing baseline (main text). Inter. F1 on GSM8K: 80.9% (RPO) vs ORPO 79.5% (Table 7).",
            "performance_baseline": "Best-performing baseline prior to RPO often ORPO (e.g., GSM8K ORPO 61.7%); other baselines include DPO, RFT, and SFT.",
            "performance_lift": "+2–4.3 percentage points absolute improvement on reported benchmarks (examples: +3.1% GSM8K, +4.3% MMLU-STEM; also reported +2.5 for GSM8K text in Table 2). Improves Inter. F1 by ~1–3 points over baselines in reported experiments.",
            "compares_data_types": true,
            "key_finding": "Combining a small number of ground-truth CoT demonstrations for prompting with model-generated reference paths and explicit exploration of favorable/unfavorable branches (contrastive, step-local supervision) yields consistent gains in final-answer accuracy and reasoning-path quality, especially for longer multi-step reasoning and STEM exam questions; balancing the supervised reference loss and the exploration (contrastive) loss (λ) is important for best performance.",
            "uuid": "e214.4",
            "source_info": {
                "paper_title": "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Orpo: Monolithic preference optimization without reference model.",
            "rating": 2,
            "sanitized_title": "orpo_monolithic_preference_optimization_without_reference_model"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Learning to summarize with human feedback.",
            "rating": 1,
            "sanitized_title": "learning_to_summarize_with_human_feedback"
        }
    ],
    "cost": 0.019015999999999998,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths</h1>
<p>Yew Ken Chia ${ }^{\wedge 1, \text { 固 }}$ Guizhen Chen ${ }^{\wedge 1,2}$ Weiwen Xu ${ }^{11}$<br>Luu Anh Tuan ${ }^{2}$ Soujanya Poria ${ }^{2}$ Lidong Bing ${ }^{1}$<br>${ }^{1}$ Singapore University of Technology and Design<br>${ }^{1}$ DAMO Academy, Alibaba Group, Singapore<br>${ }^{2}$ Nanyang Technological University, Singapore<br>{yewken_chia, sporia}@sutd.edu.sg {guizhen001, anhtuan.luu}@ntu.edu.sg<br>{yewken.chia, guizhen.chen, xuweiwen.xww, l.bing}@alibaba-inc.com</p>
<h4>Abstract</h4>
<p>Advanced models such as OpenAI o1 exhibit impressive problem-solving capabilities through step-by-step reasoning. However, they may still falter on more complex problems, making errors that disrupt their reasoning paths. We attribute this to the expansive solution space, where each step has the risk of diverging into mistakes. To enhance language model reasoning, we introduce a specialized training framework called Reasoning Paths Optimization (RPO), which enables learning to reason and explore from diverse paths. Our approach encourages favorable branches at each reasoning step while penalizing unfavorable ones, enhancing the model's overall problem-solving performance. Reasoning Paths Optimization does not rely on large-scale human-annotated rationales or outputs from closed-source models, making it scalable and data-efficient. We focus on multi-step reasoning tasks, such as math word problems and science-based exam questions. The experiments demonstrate that our framework significantly enhances the reasoning performance of large language models, with up to $3.1 \%$ and $4.3 \%$ improvement on GSM8K and MMLU (STEM) respectively. Our data and code can be found at https: //reasoning-paths.github.io.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown remarkable proficiency in following instructions and reasoning (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023b; Jiang et al., 2023). Analogous to human cognitive processes, chain-ofthought prompting guides models to reason step-by-step before producing the final answer (Wei et al., 2022), significantly boosting their reasoning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>capabilities and demonstrating exceptional performance across a wide array of tasks (Wang et al., 2023b; Chung et al., 2024). Despite these advancements, LLMs still exhibit limitations in scenarios that require more complex reasoning (Zhong et al., 2024).</p>
<p>As shown in Figure 1, the step-by-step reasoning path of the model is at risk of diverging to unfavorable branches that contain mistakes, thus reducing the chance of reaching the correct solution. While such mistakes may not immediately lead to the wrong answer, they can compound and derail the reasoning process (Ling et al., 2023). Furthermore, this challenge is amplified for more complex problems such as competition-level math questions (Hendrycks et al., 2021b) that require long reasoning paths to solve. Hence, there is a need to address this challenge by encouraging the models to generate the correct reasoning path while avoiding the unfavorable branches.</p>
<p>To ensure a trustworthy answer derivation process, prior studies have explored a range of methods, encompassing both prompting and fine-tuning techniques. Prompting methods repeatedly sample from LLMs for the same question and employ a voting mechanism to select the most accurate reasoning step among several alternatives. Such voting mechanisms can be applied at the final stage of the process, as demonstrated in Self-Consistency (Wang et al., 2023c), or at every intermediate step, as illustrated in Tree-of-Thought (Yao et al., 2023a). Yao et al. (2023b) shows that leveraging external environmental feedback could remind LLMs of some potential errors within their reasoning process, which potentially prevents these errors from affecting subsequent steps. However, the prompting methods generally demand extensive token usage to explore multiple reasoning paths from LLMs and integrate feedback from the environment. This causes a significant computational cost and huge execution latency.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of how the reasoning path of the model can easily diverge to unfavorable branches that fail to reach the correct solution. While we show a simplified example here, the challenge is amplified for more complex questions that require longer reasoning paths.</p>
<p>Alternatively, fine-tuning methods can directly enhance the reasoning capability of LLMs without exhaustive prompting engineering. Among these methods, reinforcement learning from human feedback (RLHF) <em>Christiano et al. (2017a); Stiennon et al. (2020); Ouyang et al. (2022)</em>, which involves training a reward model to optimize LLMs, has shown considerable effectiveness in aligning LLMs. This approach further spurs the development of subsequent works focused on preference optimization, such as DPO <em>Rafailov et al. (2023)</em> and SimPO <em>Meng et al. (2024)</em>, which has gained widespread practical adoption due to its simplicity and stability. However, it has been observed that these preference optimization algorithms may be less effective or even detrimental to tasks requiring in-depth reasoning <em>Meng et al. (2024)</em>. We hypothesize that these optimization methods may indiscriminately target the entire reasoning path as problematic, whereas, as indicated in Figure 2, errors in reasoning often occur at specific steps and affect only the subsequent erroneous branches.</p>
<p>To address the challenge of LLMs committing mistakes that can derail their reasoning paths, we introduce Reasoning Paths Optimization, a novel framework designed to explore and learn from varied reasoning paths. As illustrated in Figure 2, our approach initiates by generating a reference reasoning path for each question that can reach the correct answer via chain-of-thought prompting. Following this, we explore various solution branches emanating from each step in the reference path. With the reference reasoning paths and the potential solution branches explored, we optimize the model from two critical angles: (1) The model should generate the reference reasoning path with a high probability. (2) The model should favor all potential branches leading to the correct answer over those that do not. To achieve the optimization, we propose a reference loss that maximizes the likelihood of generating the reference reasoning path and an exploration loss that provides contrastive feedback over each pair of favorable and unfavorable branches. As a result, we can explore the diverse mistakes the model is liable to produce, and reduce their occurrence by aligning the models to the correct reasoning path.</p>
<p>Experimental results on math-based reasoning tasks such as GSM8K <em>Cobbe et al. (2021a)</em> and MATH <em>Hendrycks et al. (2021b)</em> demonstrate the effectiveness of our approach compared to strong baselines. In addition, we show that Reasoning Paths Optimization can generalize beyond math tasks to improve reasoning performance on the science, technology, engineering, and math (STEM) subset of the MMLU <em>Hendrycks et al. (2021a)</em> exam question dataset. Notably, the experiments show up to 3.1% and 4.3% improvement compared to the high-performing baseline on GSM8K and MMLU (STEM) datasets respectively.</p>
<h2>2 Reasoning Paths Optimization</h2>
<h3>2.1 Task Formulation</h3>
<p>In this work, we focus on problems that require multiple steps to arrive at the final answer or produce the final result, such as math word problems <em>Cobbe et al. (2021b); Hendrycks et al. (2021b)</em>. Thus, we provide a concrete task formulation in this section. Given a question <em>Q</em> posed in natural text, the goal is to produce the final answer <em>A</em> in natural text. The model is assumed to go through several reasoning steps <em>S</em><sup>1</sup>, <em>S</em><sup>2</sup>, . . . , <em>S</em><sup>n</sup> to arrive at</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of our Reasoning Paths Optimization framework for exploring and learning over diverse reasoning paths.</p>
<p>The final answer <em>A</em>. The reasoning path <em>P</em> is defined as the sequence of these steps:</p>
<p>$$P = (S_1, S_2, \dots, S_n) \tag{1}$$</p>
<p>where each <em>S<sub>i</sub></em> is a natural language sentence and the last step <em>S<sub>n</sub></em> should contain the answer <em>A</em>. Although the model could generate the correct final answer either devoid of a reasoning path or containing faulty reasoning steps, the findings and analysis presented in Section 3 suggest that such an outcome is improbable. To determine the separation points between different steps in the generation, we utilize the punctuation-based sentence splitting tool from NLTK, as the reasoning process follows a natural language structure.</p>
<h3>2.2 Framework Overview</h3>
<p>Large language models are capable of reasoning step-by-step to enhance their problem-solving abilities. However, they often fall short when faced with more challenging problems, committing mistakes that derail their reasoning paths. We believe this issue arises from the large solution space, where multiple reasoning paths can lead to the correct final answer, but each step carries the risk of branching into errors. To address this, we propose a specialized training framework that jointly considers diverse reasoning paths for a given problem. Our approach encourages favorable branches at each reasoning step while penalizing unfavorable ones. This framework, which we call Reasoning Paths Optimization (RPO), consists of three main stages as shown in Figure 2:</p>
<ol>
<li>
<p><strong>Generation</strong>: The generation stage aims to elicit correct reasoning steps from the base model to serve as reference reasoning paths. This eliminates the need for acquiring ground-truth reasoning path annotations.</p>
</li>
<li>
<p><strong>Exploration</strong>: To effectively explore the potential solution space to a given problem, this stage progressively creates branches from each step along reference reasoning paths. As a result, we can obtain multiple favorable and unfavorable reasoning branches, which will be used to provide contrastive feedback to the model.</p>
</li>
<li>
<p><strong>Optimization</strong>: This final stage aggregates and optimizes according to the reference reasoning paths and explored branches to enhance the innate reasoning ability of the base model. Thus, our framework aims to improve the overall reasoning ability of large language models.</p>
</li>
</ol>
<h3>2.3 Reasoning Generation</h3>
<p>While training with explanations or step-by-step reasoning paths (Mukherjee et al., 2023) can im</p>
<p>prove the reasoning performance of language models, it is labor-intensive and costly to annotate such data. Hence, our framework begins with a reasoning generation stage that automatically generates the reference reasoning paths. Concretely, given a problem question $Q$, we use chain-of-thought prompting (Wei et al., 2022) to generate reasoning paths. The chain-of-thought demonstration input $D_{C o T}$ consists of $m$ ground-truth examples, where each example is a pair consisting of a problem question and its corresponding reasoning path.</p>
<p>Let $M$ be the base model, and we sample the reference reasoning path $P$ by prompting the model with the chain-of-thought demonstration $D_{C o T}$ and the given question $Q$. We use temperature sampling (Fan et al., 2018) with a fixed temperature $T$ :</p>
<p>$$
P \sim M\left(D_{C o T}, Q \mid T\right)
$$</p>
<p>We consider the generated path as correct if it concludes with a correct answer. Therefore, we define the following function $\mathcal{F}$ to verify if the last step $S_{n} \in P$ contains the ground-truth answer $A$ :</p>
<p>$$
\mathcal{F}(P)= \begin{cases}1 &amp; \text { if } A \in S_{n} \ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>If the outputs are incorrect, i.e., $\mathcal{F}\left(P_{i}\right)=0$, we repeat the sampling and verification process until $\mathcal{F}\left(P_{i}\right)=1$ with a cap of 10 attempts, i.e., $i \leq 10$. If no suitable path is obtained after multiple attempts, we deem that this problem is far beyond the ability of the model and remove it from the training set. Thus, we obtain an initial dataset $D_{\text {init }}$ which contains the original questions, the ground-truth answers, and model-generated reference paths.</p>
<h3>2.4 Reasoning Exploration</h3>
<p>To consider potential mistakes that can occur from each reasoning step, the exploration stage of our framework explores multiple branches at each step. Concretely, given the problem $Q$, chain-of-thought demonstration, and previous steps of the generated reasoning path $P_{1: i-1}=\left(S_{1}, S_{2}, \ldots, S_{i-1}\right)$, we use temperature sampling (Fan et al., 2018) to obtain diverse branches from the current point in the reasoning path:</p>
<p>$$
B_{i} \sim M\left(D_{C o T}, Q, P_{1: i-1} \mid T\right)
$$</p>
<p>where each branch $B_{i}=\left(S_{i}^{\prime}, S_{i+1}^{\prime}, \ldots, S_{l}^{\prime}\right)$ should contain the current step up to the final step. We aim to obtain a favorable branch $B_{i}^{+}$and an unfavorable branch $B_{i}^{-}$where the favorable branch leads to the correct final answer, and the unfavorable branch does not:</p>
<p>$$
\mathcal{F}\left(B_{i}^{+}\right)=1, \quad \mathcal{F}\left(B_{i}^{-}\right)=0
$$</p>
<p>To achieve this, we iteratively sample multiple branches starting at each step $S_{i}^{\prime}$ and verify each one using the function $\mathcal{F}$, until we obtain one favorable branch and one unfavorable branch, thus forming a reasoning branch pair $\left(B_{i}^{+}, B_{i}^{-}\right)$. However, if we are unable to form a branch pair after sampling at most ten branches, the problem is removed from the training set. This ensures that the training data only includes problems where the model can potentially learn from contrasting between the favorable and unfavorable branches of the reasoning path.</p>
<h3>2.5 Reasoning Optimization</h3>
<p>To optimize the base model $M$, we consider both the reference reasoning path $P$ generated in Sec. 2.3 and the reasoning branch pairs $\left(B_{i}^{+}, B_{i}^{-}\right)$explored in Sec 2.4. Concretely, we encourage the model to produce a higher likelihood over the reference reasoning path. This is achieved by applying standard causal language modeling loss (Bengio et al., 2000) on the reference reasoning path $P$, conditioned on the input question $Q$ :</p>
<p>$$
\mathcal{L}<em M="M">{r e f}=-\log \operatorname{Pr}</em>(P \mid Q)
$$</p>
<p>Regarding the branch pair, the comparison between them may reveal the proper direction that guides the model's optimization. Therefore, we define a branch pair loss that provides contrastive feedback between the favorable and unfavorable branches. To formulate the branch pair loss in our framework, we can leverage preference-based objectives from existing work, such as the direct preference (Rafailov et al., 2023) or the odds-ratio objective (Hong et al., 2024). In this work, we mainly focus on the objective proposed by Hong et al. (2024) due to its simplicity and empirical effectiveness. Concretely, the branch pair loss $\mathcal{L}<em i="i">{b p, i}$ at the $i$-th step can be computed as the log odd-ratio between the favorable branch $B</em>$, conditioned on the input question $Q$ and reference path $P$ :}^{+}$and unfavorable branch $B_{i}^{-</p>
<p>$$
\mathcal{L}<em M="M">{b p, i}=\log \frac{\operatorname{odds}</em>}\left(B_{i}^{+} \mid Q, P\right)}{\operatorname{odds<em i="i">{M}\left(B</em>
$$}^{-} \mid Q, P\right)</p>
<p>The odds of generating a branch can be computed as the ratio between the probability of generating the branch and the probability of not generating it, conditioned on the input question $Q$ and the previous steps $P_{1:i-1}$ of the reference path:</p>
<p>$\operatorname{odds}<em i="i">{M}\left(B</em>} \mid Q, P\right)=\frac{\operatorname{Pr<em i="i">{M}\left(B</em>} \mid Q, P_{1: i-1}\right)}{1-\operatorname{Pr<em i="i">{M}\left(B</em>$} \mid Q, P_{1: i-1}\right)</p>
<p>Thus, we can aggregate the loss over the previously explored branch pairs corresponding to each step in the reasoning path:</p>
<p>$$
\mathcal{L}<em i="1">{\text {exp }}=\frac{1}{n-1} \sum</em>\right)
$$}^{n}-\log \sigma\left(\mathcal{L}_{b p, i</p>
<p>where there are $n$ steps in the reasoning path. We follow Hong et al. (2024) to apply the log-sigmoid function $\log \sigma$ on the log odds-ratio for optimization purposes. Finally, the overall loss $\mathcal{L}<em _ref="{ref" _text="\text">{R P O}$ in our framework is represented as the combination of the reference path loss $\mathcal{L}</em>$ which provides contrastive feedback over the explored branch pairs:}}$ and the exploration loss $\mathcal{L}_{\text {exp }</p>
<p>$$
\mathcal{L}<em e="e" f="f" r="r">{R P O}=\mathcal{L}</em>
$$}+\lambda \cdot \mathcal{L}_{e x p</p>
<p>where $\lambda$ is a hyperparameter weight, which intuitively balances between optimizing on the reference reasoning path, and the explored branches.</p>
<p>We would like to clarify that we compute the loss only on the output tokens. In this case, the output tokens only consist of the incorrect last part, while the correct prefixes serve as the input tokens, which are excluded from the loss calculation as shown in Figure 2. Specifically, the reasoning exploration stage in our framework first collect branch pairs from each step along a reference path, then aggregates the branch pair losses conditioned on the input question and the previous steps of the reference path. Therefore, the common prefix between the favorable and unfavorable branch is excluded in the loss calculation.</p>
<h2>3 Experiments</h2>
<h3>3.1 Datasets</h3>
<p>As we focus on enhancing the step-by-step reasoning ability of large language models, we evaluate our approach on datasets of various difficulty levels, including GSM8K (Cobbe et al., 2021b) for math word problems and MATH (Hendrycks et al., 2021b) for competition-level mathematics. We use the original training, validation, and testing data splits for our training and evaluation setup. On the other hand, we also include the MMLU (Hendrycks et al., 2021a) exam question dataset to evaluate the effectiveness of our approach in other domains. However, as many of the exam questions focus on world-knowledge and do not require multi-step reasoning, we extract a subset covering 3375 questions in the science, technology, engineering, and math (STEM) domains, and denote this as the MMLUSTEM dataset. The dataset details can be found in Appendix A.2.</p>
<p>Note that our Reasoning Paths Optimizationframework does not necessitate large-scale annotated reasoning paths for training LLMs. On the contrary, for each task, we only need a small number of reasoning demonstrations for implementing CoT prompting, which is easy to obtain. Specifically, we randomly select four questions from the training data and use their ground-truth reasoning path as CoT demonstrations during the reasoning generation stage. For the remaining procedure, Reasoning Paths Optimizationonly involves the ground-truth answer to verify the correctness of the explored branch. We include the prompt examples in Appendix A.3.</p>
<h3>3.2 Implementations</h3>
<p>To evaluate our approach, we implement Mistral7B and LLaMA-3-8B as our base models, which are recent and popular foundation large language models in the Mistral (Jiang et al., 2023) and LLaMA (Touvron et al., 2023a) model families respectively. To our knowledge, these are the leading foundation models in this parameter size category at the time of writing. To investigate how our approach affects models of different training stages, we also include experiments show that our framework also benefits the LLaMA-3-8B-Instruct version in Appendix A.4, which has undergone general instruction-tuning (Touvron et al., 2023a) to enhance performance in many aspects. Due to computational resource constraints, we are unfortunately unable to train larger model versions such as LLaMA-3-70B in this work. To avoid potential confounding factors, we do not evaluate on models that already have extensive math-specific training, such as Llemma (Azerbayev et al., 2024). To train</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Main results showing the evaluation accuracy (%) of different training methods on math reasoning questions in GSM8K and MATH, and science-based exam questions in MMLU-STEM. We also indicate the improvement of our method compared to the highest-performing baseline.</p>
<p>The models we use LoRA fine-tuning [hu2022lora] with a fixed batch size of 8 and a learning rate of 5e-5. More training details and hyperparameters can be found in the Appendix A.1. To sample multiple outputs from the models, we use a fixed sampling temperature of 0.5. For evaluation, we use greedy decoding for generation, and the accuracy metric for scoring.</p>
<h3>3.3 Comparison Methods</h3>
<p>To demonstrate the effectiveness of our approach, we compare against strong baselines including reasoning-specific training methods and preference-based optimization methods:</p>
<ol>
<li><strong>Supervised Fine-Tuning (SFT)</strong>: As a supervised baseline, we consider the case of not using any reasoning paths for training, and only training the model to directly generate the ground-truth final answer.</li>
<li><strong>Rejection Sampling Fine-Tuning (RFT) (yuan2024rejection)</strong>: We include RFT as a strong baseline for supervised training, which leverages the model to self-generate reasoning paths for training. We note that this approach is analogous to the reasoning generation stage in our framework, which aims to overcome the data limitation of not having ground-truth reasoning paths.</li>
<li><strong>Direct Preference Optimization (DPO) (rafailov2023direct)</strong>: As our method contrasts the favorable and unfavorable reasoning branches, it is similar in motivation to DPO which provides the model with contrastive feedback.</li>
<li><strong>Odds-Ratio Preference Optimization (ORPO) (hong2024orpo)</strong>: Lastly, we compare against ORPO which proposed the odds ratio objective for preference-based optimization. The main difference between our approach and ORPO is that Reasoning Paths Optimization is a holistic framework specifically designed for reasoning-based tasks; We consider that reasoning mistakes are liable to occur at any step in the reasoning path, and hence explore the possible solution paths which are necessary to provide contrastive feedback over diverse reasoning branch pairs.</li>
</ol>
<p>To ensure a fair comparison between different methods, we implement the data setting such that each method uses all viable training samples. For instance, SFT uses all the training samples as the data setting stipulates that all samples contain the question and ground-truth final answers. On the other hand, RFT uses only the samples for which the model can generate at least one correct reasoning path, and the preference-based methods DPO and ORPO use only the samples for which the model can generate at least one correct reasoning path and one incorrect reasoning path. Similar to our approach, the baselines other than SFT use a fixed temperature for sampling reasoning paths with chain-of-thought prompting. If the model is unable to generate a correct reasoning path after sampling a maximum of ten times, the given question is removed from the training set.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The effect of exploration loss weight on the MATH dataset performance for LLaMA-3-8B.</p>
<h3>3.4 Main Results</h3>
<p>To demonstrate the effectiveness of Reasoning Paths Optimization, we compare with strong baselines as shown in Figure 3. We observe that our approach shows consistent improvements in performance on different datasets and models. Particularly when trained on top of Mistral-7B, our approach can achieve up to 3.1% and 4.3% improvement compared to the highest-performing baseline on GSM8K and MMLU-STEM respectively. Given that MATH is a relatively difficult task, the base models may struggle to generate the correct paths, thereby limiting the effectiveness of path-based methods. Nevertheless, our approach can still improve other baselines, which shows that our approach can more effectively learn from the explored reasoning paths. On the other hand, we find that SFT performance is lower compared to the other methods trained on self-explored reasoning paths. This indicates that while it is possible for the model to directly generate the answer without any reasoning steps, it is less effective for more complex reasoning problems.</p>
<p>We further investigate the performance of our method on commonsense and general reasoning tasks in Appendix A.5. These tasks typically consist of straightforward questions that do not require lengthy reasoning steps, which may possibly contribute to the high SFT performance. Nevertheless, when the model is prompted to engage in step-by-step reasoning, our framework outperforms other preference optimization approaches, demonstrating its effectiveness in multi-step thinking.</p>
<h3>3.5 Effect of Exploration Weight</h3>
<p>To investigate the effect of reasoning exploration within our framework, we conduct an analysis on</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance with respect to reasoning path length on the MATH dataset for LLaMA-3-8B.</p>
<p>the loss weight λ. Specifically, a lower value of λ would place greater emphasis on the supervised loss over the reference path which leads to the correct answer. On the other hand, a higher value would place greater weight on the explored branches during training, which contrasts between the favorable and unfavorable branches arising from each reasoning step. As shown in Figure 4, we find that having a very low λ value leads to sub-optimal results, as it does not place enough emphasis on the reasoning exploration. On the other hand, we also find that placing too much weight on exploration also does not benefit the training, as it is still necessary to ground the model sufficiently in the reference reasoning path. Thus, this shows that we need to balance between the optimization of the reference reasoning path and the other possible exploration branches.</p>
<h3>3.6 Analysis On Reasoning Path Length</h3>
<p>To investigate the effectiveness of our approach with respect to the reasoning complexity, we analyze the evaluation accuracy in Figure 5. Compared to ORPO (Hong et al., 2024) which is the highest-performing baseline, we observe benefits from Reasoning Paths Optimization for longer reasoning paths. This may indicate that our approach can effectively reduce the occurrence of mistakes when solving questions that require more complex reasoning. In future work, we believe that potential directions include more diverse applications and tasks, such as code generation (Chen et al., 2021; Austin et al., 2021), and code-augmented reasoning (Gao et al., 2023; Li et al., 2023a).</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>It takes John 5 minutes to get to the first side of the Rubik’s cube. The second and third sides each take twice that long. The last 2</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>sides each take half as long as the first, and once you get the 5th side you have all 6 sides. How long does the Rubik’s cube take?</td>
</tr>
<tr>
<td>SFT</td>
<td>25</td>
</tr>
<tr>
<td>RFT</td>
<td>$5+2 * 5+2 * 5+2 * 5+2 * 5+2 * 5=5+10+10+10+10+10=55$ minutes. So the answer is 55 minutes.</td>
</tr>
<tr>
<td>DPO</td>
<td>$5+10+10+5+5=35$ minutes. So the answer is 35 minutes.</td>
</tr>
<tr>
<td>ORPO</td>
<td>$5+2 * 5+2 * 5+2 * 5+2 * 5+2 * 5=5+10+10+10+10+10=55$ minutes. So the answer is 55 minutes.</td>
</tr>
<tr>
<td>RPO</td>
<td>The first side takes 5 minutes. The second and third sides each take 10 minutes. The last 2 sides each take 2.5 minutes. The total</td>
</tr>
<tr>
<td></td>
<td>time is $5+10+10+2.5+2.5=30$ minutes. So the answer is 30 minutes.</td>
</tr>
</tbody>
</table>
<p>Table 1: A qualitative comparison between GSM8K outputs after training LLaMA-3-8B with different methods.</p>
<table>
<thead>
<tr>
<th>Training Method</th>
<th>GSM8K (Text)</th>
<th>GSM8K (Code)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ORPO</td>
<td>61.7</td>
<td>61.6</td>
</tr>
<tr>
<td>RPO (Ours)</td>
<td>$\mathbf{6 4 . 2}$</td>
<td>$\mathbf{6 3 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Analysis of performance benefits for text-based reasoning as our main setting, and code-based reasoning through python programs. Experiments are conducted using LLaMA-3-8B.</p>
<h3>3.7 Case Study</h3>
<p>To examine the qualitative benefits, Table 1 shows an example of outputs produced by different methods for the same question. While SFT provides an incorrect and over-simplified answer without showing any working, RFT, DPO, and ORPO directly calculate without explanation and thus make mistakes at the very first step. In contrast, the detailed breakdown of steps shows that RPO not only arrives at the correct answer but does so through a coherent process.</p>
<h3>3.8 Code-Based Reasoning</h3>
<p>Beyond reasoning in natural language works such as PAL <em>Gao et al. (2023)</em> have shown that large language models can be prompted to solve reasoning problems with code. To this end, we have conducted an analysis to show that our framework can also generalize to code-based reasoning. Concretely, in our reasoning generation stage, instead of generating text-based reasoning paths, we prompt the model with code demonstrations to generate a python program, which is executed to obtain the output answer. As shown in Table 2, we find similar benefits for text-based reasoning and codebased reasoning compared to ORPO, which is our strongest baseline.</p>
<h3>3.9 Effect of Contrastive Objectives</h3>
<p>To demonstrate the robustness of our framework, we have conducted additional experiments using different objectives to contrast between favorable</p>
<p>Table 3: Performance comparison on GSM8K, MATH, and MMLU-STEM datasets for different contrastive objectives in our framework using LLaMA-3-8B.</p>
<table>
<thead>
<tr>
<th>Exploration</th>
<th>GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>ORPO</td>
<td>61.7</td>
</tr>
<tr>
<td>Ours (w/ first correct as reference path)</td>
<td>64.2</td>
</tr>
<tr>
<td>Ours (w/ random one correct as reference path)</td>
<td>63.5</td>
</tr>
<tr>
<td>Ours (w/ random three correct as reference paths)</td>
<td>65.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance comparison on GSM8K for different reference paths using LLaMA-3-8B.
and unfavorable paths. Specifically, we show that the odds-ratio objective <em>Hong et al. (2024)</em> in our branch pair loss can be easily replaced with the direct preference objective <em>Rafailov et al. (2023)</em> for the branch pair loss in Equation 7. As shown in Table 3, the consistent benefit across different objectives demonstrates that our framework is robust and outperforms the respective baselines.</p>
<h3>3.10 Effect of Reference Paths</h3>
<p>In our exploration stage, we use the first reasoning path with the correct answer as the reference path. However, the correct answer can often be achieved via different paths. To analyse the effect of different reference paths, we select a random path with the correct answer after sampling 10 times. Results in Table 4 show that our method remains effective even with this variation, demonstrating its robustness across different reference paths. In addition, we analyse the effect of using more reference paths, eg, three correct reference paths. The results show that our approach can scale to multiple reference paths to further enhance performance.</p>
<p>4 Related Work</p>
<h2>Alignment and Preference-Based Optimization</h2>
<p>Reinforcement learning from human feedback (RLHF) (Christiano et al., 2017b; Ouyang et al., 2022; Xu et al., 2024) is a popular technique that aligns large language models with human preferences and to follow instructions (Ghosal et al., 2023; Chia et al., 2024a). During RLHF, a separate reward model is trained to provide scalar value feedback, which is passed to fine-tune LLMs with PPO algorithm (Schulman et al., 2017; Ziegler et al., 2019). However, PPO is known to be complex and unstable (Zheng et al., 2023), and the multi-stage training of a reward model and a policy model is also challenging (Meng et al., 2024). Recently, several techniques, including DPO (Rafailov et al., 2023; Pang et al., 2024), IPO (Azar et al., 2023), SimPO (Meng et al., 2024), and ORPO (Hong et al., 2024), have been proposed to eliminate the need for a reward model, which significantly stabilize and simplify the training process. They make pairwise comparisons between two responses generated by the models and push the model to assign a higher likelihood to the favorable response over the unfavorable one. However, these preference optimization methods indiscriminately compare the two responses in their entirety, overlooking the fact that errors in multi-step reasoning tasks arise only at specific steps and their subsequent branches. In this work, we propose Reasoning Paths Optimization which considers each intermediate step.</p>
<h2>Multi-step Reasoning in Language Models</h2>
<p>Large language models are capable of solving reasoning tasks by generating solutions in a step-bystep manner (Nye et al., 2022; Wei et al., 2022; Kojima et al., 2022; Fu et al., 2023; Chu et al., 2024). For example, Wei et al. (2022) and Kojima et al. (2022) demonstrate that by guiding the model to generate the reasoning steps before generating the final answer, the multi-step reasoning capabilities of LLMs could be effectively elicited, even in multimodal settings (Chia et al., 2024b; Zhang et al., 2024). However, LLMs are prone to producing errors during the reasoning process, especially for complex multi-step reasoning tasks (Li et al., 2024; Chia et al., 2023). To mitigate mistakes in the reasoning steps, a straightforward way is to verify the reasoning paths step-by-step. This encourages further investigations on process supervision. Uesato et al. (2022) and Lightman et al. (2024) collect human feedback labels for step-level solutions to verify the intermediate steps generated by reasoning models. Recent studies (Li et al., 2023b; Wang et al., 2024a,b) construct the step-wise labels automatically to prevent costly human annotations. These methods focus on training the verifiers (i.e., reward models). In contrast, we apply process supervision to preference optimization methods, without requiring a separate reward model.</p>
<p>Path Exploration in Artificial Intelligence The exploration of diverse paths has been widely used to improve the performance of complex tasks in the field of artificial intelligence. AlphaGo (Silver et al., 2016) uses Monte Carlo Tree Search (Kocsis and Szepesvári, 2006) to explore a large space of possible moves. Similarly, Yao et al. (2023a) leverage Tree-of-Thought prompting to explore possible solution space from LLMs. Other works (Feng et al., 2023; Xie et al., 2023) also design tree-based decoding strategies to search for the optimal solution. In the area of reasoning tasks, previous works have explored using self-sampled solutions for training (Ni et al., 2023) and tree search for path generation (Golovneva et al., 2023). Inspired by these works, we explore the diverse solution space generated by language models. Furthermore, we optimize the models with contrastive feedback from both favorable and unfavorable branches during training. Inspired by these works, we explore the diverse solution space generated by the models. Furthermore, we optimize LLMs with both favorable and unfavorable branches during training.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we introduced a novel training framework called Reasoning Paths Optimization (RPO) to enhance the step-by-step reasoning capabilities of LLMs. Our approach addresses the challenge of complex problem-solving tasks, where each reasoning step carries the risk of diverging into errors. RPO considers diverse reasoning branch pairs and encourages favorable branches at each reasoning step while penalizing unfavorable ones. Our framework is scalable, as it does not rely on large-scale human-annotated rationales. Instead, it leverages the model's own generated reasoning paths, making it adaptable to multi-step reasoning tasks such as math word problems. Through extensive experiments on datasets of varying difficulties, our framework provides an effective approach to enhance reasoning, paving the way for more reliable and accurate problem-solving in complex scenarios.</p>
<h2>Acknowledgment</h2>
<p>This work was substantially supported by DAMO Academy through DAMO Academy Research Intern Program.</p>
<h2>Limitations</h2>
<p>Our framework relies on the model's ability to generate correct reasoning paths during the training phase. If the base model is significantly underperforming, it may struggle to generate the necessary correct paths, thereby limiting the effectiveness of our approach. To provide performance insights beyond accuracy, we also report the Inter. F1 metric in Appendix A.6, which demonstrates that the reasoning paths generated after training with our method is more consistent with the groundtruth reasoning paths in GSM8K. Although the process of generating and exploring multiple reasoning paths for each problem is more computationally intensive, we note that this is a one-time cost during training. Hence, we believe that this is a worthwhile trade-off to enhance performance, which can be amortized over many inference cases.</p>
<h2>References</h2>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. Preprint, arXiv:2108.07732.</p>
<p>Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. 2023. A general theoretical paradigm to understand learning from human preferences. Preprint, arXiv:2310.12036.</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2024. Llemma: An open language model for mathematics. Preprint, arXiv:2310.10631.</p>
<p>Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. In Advances in Neural Information Processing Systems, volume 13. MIT Press.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.</p>
<p>Yew Ken Chia, Guizhen Chen, Anh Tuan Luu, Soujanya Poria, and Lidong Bing. 2023. Contrastive chain-ofthought prompting. ArXiv, abs/2311.09277.</p>
<p>Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2024a. InstructEval: Towards holistic evaluation of instruction-tuned large language models. In Proceedings of the First edition of the Workshop on the Scaling Behavior of Large Language Models (SCALE-LLM 2024), pages 35-64, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Yew Ken Chia, Vernon Toh, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. 2024b. PuzzleVQA: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. In Findings of the Association for Computational Linguistics ACL 2024, pages 16259-16273, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017a. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017b. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 4299-4307.</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2024. Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future. Preprint, arXiv:2309.15402.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi</p>
<p>Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1-53.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. In NeurIPS 2023 Foundation Models for Decision Making Workshop.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: Program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10764-10799. PMLR.</p>
<p>Deepanway Ghosal, Yew Ken Chia, Navonil Majumder, and Soujanya Poria. 2023. Flacuna: Unleashing the problem solving power of vicuna using flan finetuning. Preprint, arXiv:2307.02053.</p>
<p>Olga Golovneva, Sean O’Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023. Pathfinder: Guided search over multi-step reasoning paths. Preprint, arXiv:2312.05180.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021a. Measuring massive multitask language understanding. In International Conference on Learning Representations.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).</p>
<p>Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo: Monolithic preference optimization without reference model. Preprint, arXiv:2403.07691.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Levente Kocsis and Csaba Szepesvári. 2006. Bandit based monte-carlo planning. In Proceedings of the 17th European Conference on Machine Learning, ECML'06, page 282-293, Berlin, Heidelberg. Springer-Verlag.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Chengshu Li, Jacky Liang, Fei Xia, Andy Zeng, Sergey Levine, Dorsa Sadigh, Karol Hausman, Xinyun Chen, Li Fei-Fei, and brian ichter. 2023a. Chain of code: Reasoning with a language model-augmented code interpreter. In NeurIPS 2023 Foundation Models for Decision Making Workshop.</p>
<p>Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2024. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. In The Twelfth International Conference on Learning Representations.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023b. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315-5333, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Let's verify step by step. In The Twelfth International Conference on Learning Representations.</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. 2023. Deductive verification of chain-of-thought reasoning. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with a reference-free reward. Preprint, arXiv:2405.14734.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. Preprint, arXiv:2306.02707.</p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. 2023. Learning math reasoning from self-sampled correct and partially-correct solutions. In The Eleventh International Conference on Learning Representations.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2022. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems.</p>
<p>Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. 2024. Iterative reasoning preference optimization. CoRR, abs/2404.19733.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande. Communications of the ACM, 64:99 - 106.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. ArXiv preprint, abs/1707.06347.</p>
<p>David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya</p>
<p>Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the game of go with deep neural networks and tree search. Nature, 529:484503 .</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023a. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, L. Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process- and outcomebased feedback. ArXiv, abs/2211.14275.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2023a. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Proceedings</p>
<p>of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2717-2739, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, and Ming Gao. 2023b. Boosting language models reasoning with chain-of-knowledge prompting. arXiv preprint arXiv:2306.06427.</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 2024a. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Preprint, arXiv:2312.08935.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. 2024b. Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision. Preprint, arXiv:2402.02658.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. Self-evaluation guided beam search for reasoning. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Weiwen Xu, Deng Cai, Zhisong Zhang, Wai Lam, and Shuming Shi. 2024. Reasons to reject? aligning language models with judgments. In Findings of the Association for Computational Linguistics ACL 2024, pages 12288-12304, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2024. Scaling relationship on learning mathematical reasoning with large language models.
Zhuosheng Zhang, Aston Zhang, Mu Li, hai zhao, George Karypis, and Alex Smola. 2024. Multimodal chain-of-thought reasoning in language models.</p>
<p>Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. 2023. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964.</p>
<p>Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yihen Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Lichao Sun, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Ninghao Liu, Bei Jiang, Linglong Kong, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, and Tianming Liu. 2024. Evaluation of openai o1: Opportunities and challenges of agi. Preprint, arXiv:2409.18486.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. ArXiv preprint, abs/1909.08593.</p>
<h2>A Appendix</h2>
<h2>A. 1 Training and Hyperparameter Details</h2>
<p>We list the hyperparameter and training details in Table 5. Note that we use the validation accuracy of MATH with LLaMA-3-8B to select the loss weight $\lambda \in{0.1,0.3,0.5,0.7,0.9}$ and use it for all datasets. To ensure the diversity in our reasoning exploration stage, we remove duplicate reasoning paths and branches before training. To maintain fairness between different training methods, we ensure that each method uses at most one accepted or reference reasoning path that reaches the correct answer for each question. For preference-based methods, we ensure that each method uses at most one accepted reasoning path, and one rejected reasoning path that reaches the wrong answer for each question. Similarly, our approach uses at most one reference reasoning path for each question. To be fair to DPO which typically follows an SFT training stage, we include the SFT loss over the accepted reasoning path during training, which is a joint loss</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Main results showing the evaluation accuracy (%) of different training methods on math reasoning datasets. We also indicate the improvement of our method compared to the highest-performing baseline.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Additional results showing the evaluation accuracy on science, technology, engineering, and math questions in MMLU (Hendrycks et al., 2021a). We also indicate the improvement of our method compared to the highest-performing baseline.</p>
<p>with the DPO objective. Hence, all methods in this paper utilize a single training stage.</p>
<p>As shown in Table 6, all training methods use a similar amount of training data in terms of reasoning paths. Note that the number of samples used for each model is different as the samples are filtered based on the correctness of model outputs.</p>
<h3>A.2 Dataset Details</h3>
<p>For GSM8K and MATH, we use the original training and testing splits. For MMLU (STEM), we specifically use the STEM subset for the following subcategories of questions:</p>
<ul>
<li>abstract_algebra</li>
<li>astronomy</li>
<li>college_biology</li>
<li>college_chemistry</li>
<li>college_computer_science</li>
<li>college_mathematics</li>
<li>college_physics</li>
<li>computer_security</li>
<li>conceptual_physics</li>
<li>electrical_engineering</li>
<li>elementary_mathematics</li>
<li>high_school_biology</li>
<li>high_school_chemistry</li>
<li>high_school_computer_science</li>
<li>high_school_mathematics</li>
<li>high_school_physics</li>
<li>high_school_statistics</li>
<li>machine_learning</li>
</ul>
<p>We thus create a train-test split of the STEM questions, contain 3000 training and 375 testing samples.</p>
<h3>A.3 Prompting</h3>
<p>For our reasoning generation stage as well as the baselines of RFT, DPO, and ORPO, we use chain-of-thought prompting to generate training reasoning paths. Note that we use 4-shot prompting for all settings and methods as shown below:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Loss weight $\lambda$</th>
<th style="text-align: right;">0.3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lora rank</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: right;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Training epochs</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Hardware</td>
<td style="text-align: right;">Single A800 (80GB)</td>
</tr>
</tbody>
</table>
<p>Table 5: Hyperparameter and training details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">LLaMA-3-8B</th>
<th style="text-align: center;">Mistral</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">7473 answers</td>
<td style="text-align: center;">7473 answers</td>
</tr>
<tr>
<td style="text-align: left;">RFT</td>
<td style="text-align: right;">6417 reasoning paths</td>
<td style="text-align: center;">5922 reasoning paths</td>
</tr>
<tr>
<td style="text-align: left;">DPO</td>
<td style="text-align: right;">5667 reasoning path pairs</td>
<td style="text-align: center;">5535 reasoning path pairs</td>
</tr>
<tr>
<td style="text-align: left;">ORPO</td>
<td style="text-align: right;">5667 reasoning path pairs</td>
<td style="text-align: center;">5535 reasoning path pairs</td>
</tr>
<tr>
<td style="text-align: left;">RPO</td>
<td style="text-align: right;">5752 reference paths</td>
<td style="text-align: center;">5600 reference paths</td>
</tr>
</tbody>
</table>
<p>Table 6: Training data comparison for different methods on GSM8K dataset using different models.</p>
<h2>GSM8K CoT Prompt</h2>
<p>Question: There are 180 days in a school year. A senior can skip their final exams if they miss $5 \%$ or less of the school year. Hazel has missed 6 days of school due to illness. How many more days can she miss and still not have to take her exams?</p>
<p>Answer: There are 180 days in the school year and she can miss up to $5 \%$ so that's $180 * .05=9$ days\nHazel has been sick 6 days already and she can only miss 9 days or less so she can miss $9-6=3$ more days. So the answer is \boxed{3} days.</p>
<p>Question: Several birds were sitting in the branches of a crape myrtle tree. There were three times more cardinals than bluebirds, but half as many swallows as bluebirds. If there were 2 swallows, what is the total number of birds in the crape myrtle tree?</p>
<p>Answer: With half as many swallows as bluebirds, there are $2 * 2=4$ bluebirds. With three times more cardinals than bluebirds, there are $3 * 4=12$ cardinals, If there were 2 swallows, then the total number of birds in the crape myrtle tree is $2+4+12=18$ birds. So the answer is \boxed{18}.</p>
<p>Question: Barry goes to a shop to buy a shirt he'd been admiring for quite some</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RFT</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: left;">DPO</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: left;">ORPO</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: left;">RPO (Ours)</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">80.9</td>
</tr>
</tbody>
</table>
<p>Table 7: Analysis of the reasoning quality through Inter. Precision, Recall, and F1 metrics on GSM8K.
time. He tells the attendant that it's his birthday so she decides to give him a $15 \%$ special discount. The price tag on the shirt says $\$ 80$. How much is he supposed to pay now, considering the special discount?</p>
<p>Answer: $15 \%$ of $\$ 80=(15 / 100) * \$ 80=\$ 12$ The dollar amount of the discount is $\$ 12$ so he is supposed to pay just $\$ 80-\$ 12=$ \$68. So the answer is \boxed(\$68).</p>
<p>Question: Nancy wanted to make peanut butter cookies for a family gathering, but her cousin is allergic to peanuts. She decided to make almond butter cookies instead. A jar of almond butter costs three times the amount that a jar of peanut butter does. It takes half a jar to make a batch of cookies. A jar of peanut butter costs $\$ 3$. How many dollars more does it cost per batch to make almond butter cookies instead of peanut butter cookies?</p>
<p>Answer: A jar of almond butter costs 3 * $3=\$ 9$. \nIt takes half a jar to make a batch of cookies, so it costs $9 / 2=$ $\$ 4.50$ to use almond butter. \nIt costs 3 / $2=\$ 1.50$ to use peanut butter. \nThus, it costs $4.50-1.50=\$ 3$ more to make a batch of almond butter cookies than peanut butter cookies. So the answer is $\backslash \backslash$ boxed ${\$ 3}$.</p>
<h2>MATH CoT Prompt</h2>
<p>Question: Find the domain of the expression $\$ \backslash \backslash f r a c(\backslash s q r t{x-2}}{\backslash s q r t{5-x}} \$$.</p>
<p>Answer: The expressions inside each square root must be non-negative. Therefore, $\$ x-2$</p>
<p>\ge 0\$, so \$x\ge2\$, and $\$ 5-x \operatorname{ge} 0 \$$, so $\$ x$ \le 5\$. Also, the denominator cannot be equal to zero, so $\$ 5-x&gt;0 \$$, which gives $\$ x&lt;5 \$$. Therefore, the domain of the expression is $\$ \backslash$ boxed{[2,5)}.\nSo the final answer is \boxed{[2,5)}.</p>
<p>Question: If $\$ \backslash$ det $\backslash$ mathbf{A} $=2 \$$ and $\$ \backslash$ det $\backslash$ mathbf ${B}=12, \$$ then find $\$ \backslash$ det $(\operatorname{mathbf}{A} \backslash \operatorname{mathbf}{B}) . \$$</p>
<p>Answer: We have that $\$ \backslash$ det (\mathbf{A} \mathbf{B}) = (\det \mathbf{A})( \det \mathbf{B}) = (2)(12) = \boxed{24}.\$ So the final answer is \boxed{24}.</p>
<p>Question: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?</p>
<p>Answer: If Terrell lifts two 20-pound weights 12 times, he lifts a total of \$2\cdot 12\cdot20=480\$ pounds of weight. If he lifts two 15-pound weights instead for $\$ n \$$ times, he will lift a total of \$2\cdot15\cdot $n=30 n \$$ pounds of weight. Equating this to 480 pounds, we can solve for \$n\$:\n\begin{align<em>}\n30n\&amp;=480\\ \rightarrow\qquad $n \&amp;=480 / 30=\backslash$ boxed{16} \end{align</em>} So the final answer is \boxed{16}.</p>
<p>Question: If the system of equations \n\begin{align<em>}\n6x-4y\&amp;=a, \\ 6y-9x \&amp;=b. \n\end{align</em>}\nhas a solution $\$(x, y) \$$ where $\$ x \$$ and $\$ y \$$ are both nonzero, find $\$ \backslash \backslash$ frac{a}{b}, $\$$ assuming $\$ b \$$ is nonzero.</p>
<p>Answer: If we multiply the first equation by $\$-\backslash \backslash$ frac ${3}{2} \$$, we obtain $\$ \$ 6 y-9 x=$ $\backslash \backslash$ frac ${3}{2}$ a. $\$ \$ \backslash$ nSince we also know that $\$ 6 y-9 x=b \$$, we have $\$ \$-\backslash \backslash$ frac ${3}$ ${2} a=b \backslash$ Rightarrow $\backslash \backslash$ frac ${a}{b}=$ \boxed{-\frac{2}{3}}.\$\$
So the final answer is
$\backslash$ boxed{-\frac{2}{3}}.</p>
<h2>MMLU (STEM Subset) CoT Prompt</h2>
<p>Question: Which of the following adaptive
features would least likely be found in an animal living in a hot arid environment? \n(A) Long loops of Henle to maximize water reabsorption\n(B) Storage of water in fatty tissues $\backslash \mathrm{n}(\mathrm{C})$ Large ears to aid in heat dispersion $\backslash \mathrm{n}(\mathrm{D})$ Short loops of Henle to maximize water secretion</p>
<p>Answer: To determine which adaptive feature would be least likely found in an animal living in a hot arid environment, let's analyze each option in the context of such an environment: (A) Long loops of Henle to maximize water reabsorption: In a hot arid environment, water is scarce. Animals would benefit from adaptations that help conserve water. Long loops of Henle in the kidneys are an adaptation that allows for maximum reabsorption of water, which is crucial for survival in such conditions. This feature is likely to be found in animals living in arid environments. (B) Storage of water in fatty tissues: Some animals have evolved to store water in their bodies to survive long periods without access to water. Storing water in fatty tissues is an effective way to ensure a supply of water during droughts or dry periods. This is another adaptation that would be beneficial in a hot arid environment. (C) Large ears to aid in heat dispersion: In hot environments, animals need to dissipate excess body heat to avoid overheating. Large ears can help with this by increasing the surface area through which heat can be lost. This adaptation is also likely to be found in animals living in hot arid environments. (D) Short loops of Henle to maximize water secretion: Short loops of Henle would result in less water reabsorption and more water being excreted. In a hot arid environment where water is scarce, this would be a disadvantage because it would lead to greater water loss. Therefore, this feature is least likely to be found in animals living in such conditions. So the correct answer option is (D) Short loops of Henle to maximize water secretion</p>
<p>Question: A survey of 1000 registered voters revealed that 450 people would vote for candidate A in an upcoming election. If 220,000 people vote in the election, how many votes would the survey takers predict candidate A should receive?\n(A) $44,500 \backslash n(B) 48,900 \backslash n(C) 95,000 \backslash n(D) 99,000$</p>
<p>Answer: To predict the number of votes candidate A should receive based on the survey, we can use a proportion. The survey indicates that 450 out of 1000 registered voters would vote for candidate A. We can set up the following proportion to find the predicted number of votes out of 220,000 voters: \[\frac{450}{1000} = \frac{x}{220,000} \ ] To solve for \( x \%), we can cross-multiply and then divide: \[450 \times 220,000 = 1000 \times x\] \[99,000,000 = 1000x\] \[x = \frac{99,000,000}{1000} \ ] \[x = 99,000\] Therefore, the survey takers would predict that candidate A should receive \( 99,000 \) votes. So the correct answer option is (D) 99,000</p>
<p>Question: A cannonball is fired horizontally at $10 \mathrm{~m} / \mathrm{s}$ from a cliff. Its speed one second after being fired is about $\backslash \mathrm{n}(\mathrm{A}) 10 \mathrm{~m} / \mathrm{s} \backslash \mathrm{n}(\mathrm{B}) 14 \mathrm{~m} / \mathrm{s} \backslash \mathrm{n}(\mathrm{C}) 16$ $\mathrm{m} / \mathrm{s} \backslash \mathrm{n}(\mathrm{D}) 20 \mathrm{~m} / \mathrm{s}$</p>
<p>Answer: To determine the speed of the cannonball one second after being fired horizontally, we need to consider both the horizontal and vertical components of its motion. 1. Horizontal Motion: - The horizontal velocity ( $\mathrm{v}<em -="-">{-} \mathrm{x}$ ) remains constant because there is no horizontal acceleration (assuming air resistance is negligible). Given: $v</em>} \mathrm{x}=10 \mathrm{~m} / \mathrm{s} 2$. Vertical Motion: The vertical velocity ( $\mathrm{v<em -="-">{-} \mathrm{y}$ ) changes due to the acceleration caused by gravity ( $\mathrm{g}=$ $9.8 \mathrm{~m} / \mathrm{s}^{\wedge} 2$ ). - Initially, the vertical velocity is zero because the cannonball is fired horizontally. - After one second, the vertical velocity can be calculated using the equation: $v</em> 3$. Resultant Speed: - The resultant speed (v) is found by combining the horizontal and
vertical components using the Pythagorean theorem: $v=\operatorname{sqrt}\left(v_{-} x^{\wedge} 2+v_{-} y^{\wedge} 2\right) v=$ $\operatorname{sqrt}\left((10 \mathrm{~m} / \mathrm{s})^{\wedge} 2+(9.8 \mathrm{~m} / \mathrm{s})^{\wedge} 2\right) \mathrm{v}=\operatorname{sqrt}(100$ $+96.04) \mathrm{v}=\operatorname{sqrt}(196.04) \mathrm{v} \backslash \operatorname{approx} 14 \mathrm{~m} / \mathrm{s}$ Therefore, the speed of the cannonball one second after being fired is about $14 \mathrm{~m} / \mathrm{s}$. So the correct answer option is (B) $14 \mathrm{~m} / \mathrm{s}$} y=g * t$ where $t=$ 1 s. $v_{-} y=9.8 \mathrm{~m} / \mathrm{s}^{\wedge} 2 * 1 \mathrm{~s}=9.8 \mathrm{~m} / \mathrm{s</p>
<p>Question: A 3-microfarad capacitor is connected in series with a 6-microfarad capacitor. When a 300-volt potential difference is applied across this combination, the total energy stored in the two capacitors is $\backslash \mathrm{n}(\mathrm{A}) 0.09 \mathrm{~J} \backslash \mathrm{n}(\mathrm{B})$ $0.18 \mathrm{~J} \backslash \mathrm{n}(\mathrm{C}) 0.27 \mathrm{~J} \backslash \mathrm{n}(\mathrm{D}) 0.41 \mathrm{~J}$</p>
<p>Answer: To find the total energy stored in the two capacitors connected in series, we need to follow these steps: 1.
Calculate the equivalent capacitance of the series combination: For capacitors in series, the reciprocal of the equivalent capacitance $\backslash(C_{-}(\backslash t e x t{e q}) \backslash)$ is the sum of the reciprocals of the individual capacitances: [ \frac{1}{C_{-}(\text { eq })} } = \frac{1}{C_1}+\frac{1}{C_2} ] Given (C_{-} 1=3 \backslash, \backslash m u \backslash t e x t{F} \backslash) and (C_{-} 2=$ $6 \backslash, \backslash m u \backslash t e x t{F} \backslash): \backslash[\backslash f r a c{1}$
${C_{-}(\text { text } e q})=\backslash f r a c{1}{3 \backslash$, $\backslash m u \backslash t e x t{F}+\backslash f r a c{1}{6 \backslash, \backslash m u \backslash t e x t{F}}+\backslash f r a c{1}{6$ $\backslash, \backslash m u \backslash t e x t{F}=\backslash f r a c{3}{6 \backslash$, $\backslash m u \backslash t e x t{F}=\backslash f r a c{1}{2 \backslash, \backslash m u \backslash t e x t{F}}$ } Therefore, $\backslash\left[C_{-}(\backslash t e x t{e q}=2 \backslash\right.$, $\backslash m u \backslash t e x t{F} \backslash]$ 2. Calculate the total energy stored in the equivalent capacitor: The energy $\backslash(E \backslash)$ stored in a capacitor is given by: $\backslash\left[E=\backslash f r a c{1}{2} C V^{\wedge} 2 \backslash\right]$ Here, $\backslash\left(C=C_{-}(\backslash t e x t{e q}=2 \backslash\right.$, $\backslash m u \backslash t e x t{F}=2$ \times $10^{\wedge}{-6} \backslash, \backslash t e x t{F}$ } and $\backslash(\mathrm{V}=300 \backslash, \backslash t e x t{\mathrm{~V}) \backslash): \backslash[\mathrm{E}=$ $\backslash f r a c{1}{2} \backslash t i m e s 2 \backslash t i m e s 10^{\wedge}{-6} \backslash$, $\backslash t e x t{F} \backslash t i m e s(300 \backslash, \backslash t e x t{V})^{\wedge} 2 \backslash] \backslash[$ $E=\backslash f r a c{1}{2} \backslash t i m e s 2 \backslash t i m e s 10^{{-6}}$ \times 90000 ] \I E = 1 \times 10^{{-6}}$ \times 90000 ] \I E = 0.09 \, \text{J} } Therefore, the total energy stored in the two capacitors is $\backslash(\backslash$ boxed{0.09 \, $\backslash$ ttext{J}}). So the correct answer option is (A) 0.09</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>CSQA</th>
<th>Winogrande</th>
<th>MMLU</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA-3-8B</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SFT</td>
<td>82.7</td>
<td>86.0</td>
<td>63.7</td>
</tr>
<tr>
<td>RFT</td>
<td>72.3</td>
<td>64.2</td>
<td>59.6</td>
</tr>
<tr>
<td>DPO</td>
<td>72.7</td>
<td>57.9</td>
<td>56.5</td>
</tr>
<tr>
<td>ORPO</td>
<td>76.8</td>
<td>67.0</td>
<td>59.7</td>
</tr>
<tr>
<td>Ours</td>
<td>79.7</td>
<td>73.6</td>
<td>62.0</td>
</tr>
<tr>
<td>LLaMA-3-8B-Instruct</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SFT</td>
<td>82.3</td>
<td>84.8</td>
<td>63.4</td>
</tr>
<tr>
<td>RFT</td>
<td>73.8</td>
<td>66.5</td>
<td>63.3</td>
</tr>
<tr>
<td>DPO</td>
<td>70.4</td>
<td>63.7</td>
<td>67.9</td>
</tr>
<tr>
<td>ORPO</td>
<td>74.0</td>
<td>67.7</td>
<td>63.9</td>
</tr>
<tr>
<td>Ours</td>
<td>77.7</td>
<td>71.2</td>
<td>65.6</td>
</tr>
</tbody>
</table>
<p>Table 8: Additional evaluation results on commonsense and general reasoning tasks.</p>
<h3>A.4 Instruction-Tuned Model Experiments</h3>
<p>To investigate how our approach affects models of different training stages, we also include experiments show that our framework also benefits the LLaMA-3-8B-Instruct version in Figure 6 and Figure 7, which has undergone general instruction-tuning <em>Touvron et al. (2023a)</em> to enhance performance in many aspects. Notably, we observe improvements on both the base and the instruction-tuned model versions, which suggests that our approach may generalize well even to well-trained models.</p>
<h3>A.5 Commonsense and General Reasoning</h3>
<p>We additionally study the performance of our method on commonsense and general reasoning tasks, specifically evaluating it on CommonsenseQA <em>Talmor et al. (2019)</em>, Winogrande <em>Sakaguchi et al. (2019)</em>, and the full MMLU <em>Hendrycks et al. (2021a)</em> dataset as presented in Table 8. Despite the strong SFT baseline, which suffices for most questions requiring only one or two reasoning steps, we demonstrate that our method surpasses other preference optimization methods in terms of multi-step reasoning when the model is prompted to think step-by-step. Notably, on the Winogrande dataset, our framework achieves a significant improvement of 6.6% over the strongest preference optimization baseline, ORPO.</p>
<h3>A.6 Evaluation of Reasoning Quality</h3>
<p>To quantitatively measure the reasoning quality after training with different methods, we report the Inter. F1 metrics <em>Wang et al. (2023a)</em> which compares the numerical objects that are consistent between the generated reasoning path and ground-truth reasoning path. We report the results for LLaMA-3-8B on GSM8K as shown in Table 7. The results demonstrate that our framework not only improves the final reasoning benchmark score, but also enhances the reasoning quality as measure by the Inter. F1 metric.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Equal contribution. Yew Ken and Guizhen are students under the Joint PhD Program between Alibaba and their corresponding university.
${ }^{1}$ Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
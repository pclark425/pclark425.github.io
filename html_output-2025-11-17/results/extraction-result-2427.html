<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2427 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2427</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2427</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-264072630</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.04557v1.pdf" target="_blank">An Artificial Intelligence (AI) workflow for catalyst design and optimization</a></p>
                <p><strong>Paper Abstract:</strong> In the pursuit of novel catalyst development to address pressing environmental concerns and energy demand, conventional design and optimization methods often fall short due to the complexity and vastness of the catalyst parameter space. The advent of Machine Learning (ML) has ushered in a new era in the field of catalyst optimization, offering potential solutions to the shortcomings of traditional techniques. However, existing methods fail to effectively harness the wealth of information contained within the burgeoning body of scientific literature on catalyst synthesis. To address this gap, this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization. Our methodology combines advanced language understanding with robust optimization strategies, effectively translating knowledge extracted from diverse literature into actionable parameters for practical experimentation and optimization. In this article, we demonstrate the application of this AI workflow in the optimization of catalyst synthesis for ammonia production. The results underscore the workflow's ability to streamline the catalyst development process, offering a swift, resource-efficient, and high-precision alternative to conventional methods.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2427.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2427.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI workflow (LLM+BO+AL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integrated AI workflow combining Large Language Models, Bayesian Optimization, and an Active Learning loop for catalyst design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that (1) uses an LLM (ChatGPT) to mine literature and construct a multidimensional chemical search space, (2) represents that space as vectors for input to Bayesian optimization with a Gaussian Process surrogate, and (3) runs an iterative active-learning experimental loop to propose, synthesize, evaluate, and incorporate experimental results to converge on optimal synthesis parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Integrated LLM + Bayesian Optimization + Active Learning workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system first harvests experimental procedures and process variables from literature using ChatGPT and keyword filtering, organizes the extracted variables into a graph (Neo4j) and vectorizes them (min-max normalization for continuous variables, OHE for categorical). A Gaussian Process surrogate with a constant mean and Matern-5/2 kernel models the mapping from synthesis parameters to performance. Acquisition functions (EI for single-objective, EHVI for multi-objective) select candidate experiments. Selected conditions are physically synthesized and evaluated; the measured performance is fed back to update the GP and re-run acquisition in an iterative active learning loop until convergence or resource limits.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Catalyst discovery and optimization (ammonia synthesis) / materials synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects next experiments by maximizing an acquisition function (Expected Improvement or Expected Hypervolume Improvement) computed from the GP surrogate's posterior mean and variance; initial coverage ensured by Latin Hypercube Sampling; termination can be triggered by performance thresholds, a maximum number of experiments, or practical resource constraints (time/materials).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement (EI) for single objective; Expected Hypervolume Improvement (EHVI) for multi-objective; GP predictive variance and associated uncertainty estimates underpin expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balanced via acquisition functions: EI trades off predictive mean (exploitation) and predictive variance (exploration); EHVI extends this to multi-objective Pareto-front hypervolume improvement, guiding sampling toward points that either improve predicted performance or reduce uncertainty near the Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity enforced through (a) initial Latin Hypercube Sampling to cover the space uniformly, (b) inclusion of many categorical options from literature in the vectorized space, and (c) EHVI's tendency to explore diverse Pareto-optimal trade-offs rather than a single optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time and material resource constraints; maximum allowable number of experiments (practical lab budget).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget considerations are incorporated into termination criteria (stop when threshold performance reached or when experiment budget is exhausted); the acquisition function selects maximally informative experiments under the running budget but the paper gives no formal cost-penalized acquisition implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>For the case study, breakthrough is operationalized as high ammonia concentration; generally measured by Expected Improvement (EI) or increase in dominated hypervolume (EHVI) indicating movement toward Pareto-optimal, high-impact solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Predicted optimized ammonia concentration: 730.43 ppm for the suggested optimal point (activation temp 787.782 °C, duration 286.235 min, heating rate 14.239 °C/min, pressure 5.002 MPa). Dataset extraction: 603 articles, 774 distinct catalysts, 737 unique procedure steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively/visually to Random Sampling for initialization; contrasted with traditional OFAT and DoE approaches in discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Authors report that Latin Hypercube Sampling provides more uniform initial coverage than Random Sampling (visual/qualitative); no numerical comparison of optimization performance vs OFAT/DoE/random search is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses the exploration/exploitation tradeoff (via EI/EHVI) and multi-objective tradeoffs (e.g., activity vs stability on a Pareto front). It also notes practical tradeoffs from time and material resource limits influencing termination, but does not provide a quantitative tradeoff surface linking computation/experiment cost to information gain or diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Practical recommendations: construct diverse search space from literature using LLMs; initialize with Latin Hypercube Sampling; use GP surrogate with Matern-5/2 kernel and constant mean; use EI for single-objective and EHVI for multi-objective acquisition; iterate experiments in an active learning loop and terminate when performance thresholds or experiment budgets are reached.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence (AI) workflow for catalyst design and optimization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2427.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2427.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian optimization (BO) + GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization with Gaussian Process surrogate modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning optimization method that uses a GP surrogate to model an unknown objective function and an acquisition function to propose the next experimental points balancing exploration and exploitation; adopted here with Matern-5/2 kernel and constant mean for catalyst synthesis parameter optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimization with Gaussian Process surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A GP surrogate models f(x) (mapping from synthesis parameters to performance) with a constant mean and Matern-5/2 covariance; the GP yields predictive mean and variance used by acquisition functions (EI / EHVI) to rank candidate experiments. Categorical variables are represented via one-hot encoding, continuous variables normalized by min-max. Initial training points chosen via LHS.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design / catalyst synthesis optimization (materials science)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experimental runs by maximizing acquisition functions computed from the GP posterior; initially sample broadly (LHS) then iteratively propose points maximizing expected improvement or hypervolume gain.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive variance and acquisition functions (EI / EHVI) which use expected improvement/hypervolume improvement as the utility metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions combine predictive mean (exploitation) and predictive uncertainty (exploration); EI inherently trades off these terms; EHVI extends to multi-objective contexts to target Pareto improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises from initial LHS and from EHVI's multi-objective optimization across the Pareto front; one-hot encoding enables representation of diverse categorical choices available from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experiment budget / time and material limits (discussed qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Termination by pre-established thresholds (performance) or experimental budget caps; GP + acquisition function prioritize the most informative experiments per iteration but no explicit cost-aware acquisition rule is described.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected Improvement or Expected Hypervolume Improvement (for multi-objective) and domain-specific performance metrics such as ammonia concentration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Suggested optimal point predicted ammonia concentration = 730.43 ppm; GP uncertainty reduced across iterations (qualitative, shown in surrogate evolution figures).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random Sampling (initialization comparison), OFAT and DoE discussed as conventional alternatives in background.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LHS initialization yields better space coverage than Random Sampling; BO is described as more efficient than OFAT/DoE in literature discussion but the paper does not report direct numerical head-to-head comparisons in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Describes qualitatively how BO balances exploration vs exploitation and handles noisy, high-dimensional objectives; multi-objective extension (EHVI) handles tradeoffs between conflicting metrics like activity and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use GP with Matern-5/2 kernel and constant mean in low-data catalyst optimization tasks; initialize with LHS; employ EI/EHVI acquisition functions and iterate in an active learning loop until resource-driven stopping criteria are met.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence (AI) workflow for catalyst design and optimization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2427.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2427.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI / EHVI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement (EI) and Expected Hypervolume Improvement (EHVI) acquisition functions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>EI is a single-objective acquisition that selects next experiments maximizing expected improvement over current best; EHVI generalizes this to multi-objective problems by selecting experiments expected to increase the dominated hypervolume of the Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Acquisition functions: Expected Improvement (EI) and Expected Hypervolume Improvement (EHVI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EI uses the GP predictive mean and variance to compute expected improvement over the incumbent and selects the point maximizing this expectation. EHVI computes the expected increase in dominated hypervolume in multi-objective space if a candidate point were sampled, accounting for correlations between objectives under the GP posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active experimental design for single- and multi-objective optimization (catalyst synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate individual experimental runs to the point that maximizes EI (single-objective) or EHVI (multi-objective), thereby attempting to maximize expected utility per experimental cost (implicitly per run).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement (EI) and expected hypervolume improvement (EHVI) serve as the information/utility metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>EI balances exploitation (high predicted mean) and exploration (high variance) through its analytic form; EHVI extends this trade-off across objectives, encouraging sampling that either improves objectives or fills gaps on the Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>EHVI promotes exploration along different regions of the Pareto front, indirectly encouraging diverse solutions; no explicit diversity-penalized acquisition is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Implicit: number of experiments and lab resources; not formalized in acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition selects the most promising points per iteration; explicit cost-aware acquisition (e.g., per-experiment cost weighting) is not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement in objective value (EI) or increase in Pareto-front dominated hypervolume (EHVI); domain-specific breakthroughs measured like high ammonia concentration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative performance gains for EI/EHVI vs alternatives are provided beyond their adoption in the workflow; EHVI described as capturing correlations between objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>EI/EHVI contrasted conceptually with single-point greedy selection or random sampling; no numerical baseline comparison in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper explains conceptually how EI/EHVI embody the exploration/exploitation tradeoff and how EHVI is appropriate for multi-objective Pareto exploration (activity vs stability), but no numeric tradeoff curves are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use EI for single-objective optimization and EHVI for multi-objective catalyst optimization to prioritize experiments expected to maximally improve performance or Pareto hypervolume.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence (AI) workflow for catalyst design and optimization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2427.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2427.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active learning loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative experimental active learning loop bridging prediction and physical experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative cycle where acquisition-selected synthesis parameters are experimentally realized, performance metrics measured and fed back to update the GP surrogate, repeating until termination criteria (performance threshold, max experiments, or resource constraints) are met.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active learning experimental loop</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The loop: (1) acquisition function proposes parameter set(s), (2) laboratory executes synthesis and performance evaluation (activity, selectivity, stability), (3) results are added to the dataset, (4) GP surrogate is updated, (5) acquisition re-computed. Loop stops when predefined performance thresholds or budget limits are reached or when additional improvement is negligible.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Closed-loop experimental design for catalyst synthesis and optimization</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Experimental budget allocated iteratively to conditions that the acquisition function deems maximally informative/beneficial; termination decisions consider resource constraints and urgency.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via acquisition functions (EI/EHVI) that use GP predictive uncertainty to estimate expected utility/information gain of each candidate experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Controlled by choice of acquisition function; the loop updates model uncertainty and focuses subsequent experiments on high-utility regions identified by EI/EHVI.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initial diversity via LHS; active loop naturally explores different regions due to acquisition-driven uncertainty reduction and EHVI multi-objective exploration, but no explicit diversity-regularized acquisition is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experiment count, time, materials; practical lab resource constraints mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Termination criteria include resource constraints; the loop is intended to be resource-efficient by selecting highly informative experiments but no explicit cost-per-info optimization is implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same as acquisition: EI/EHVI and domain-specific metrics (e.g., ammonia concentration).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors show surrogate evolution from few to many datapoints qualitatively; recommended optimal point predicted ammonia concentration 730.43 ppm is an example endpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to one-shot screening or brute-force high-throughput approaches; LHS and Random Sampling used for initial-sampling comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative claims of higher resource-efficiency vs OFAT/DoE and better coverage from LHS vs Random Sampling; no quantitative experiment-count reductions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Describes qualitative tradeoffs (explore vs exploit, multi-objective Pareto tradeoffs) and practical tradeoffs imposed by time and material budgets, but lacks a quantitative allocation tradeoff analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Iterative active learning with BO is recommended to maximize value per experiment; initialize broadly, then use GP/EI or EHVI to allocate follow-up experiments until resource- or performance-based stopping conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence (AI) workflow for catalyst design and optimization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2427.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2427.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LHS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latin Hypercube Sampling (LHS) initial sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stratified sampling technique used to select an initial set of points that are well-distributed across the high-dimensional parameter space, improving initial coverage relative to naive random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Latin Hypercube Sampling (LHS) for initialization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LHS divides each input dimension into equal-probability intervals and samples once from each interval, ensuring that initial design points collectively cover the space with reduced clustering compared to pure random sampling; used here to seed the GP surrogate before iterative BO.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Initial-design sampling for active learning and Bayesian optimization in experimental sciences (catalyst synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates the initial experimental budget to maximize uniform coverage of the parameter space, reducing the risk of missing important regions early in optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit: aims to maximize representativeness/coverage which supports later information gain when used to train the initial surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>LHS is purely exploratory for initialization; subsequent exploitation/exploration is governed by BO acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Primary mechanism to ensure diversity via stratified sampling across each dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Initial experimental budget (number of initial runs); no further formal budget model provided.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Distributes the available initial sample budget evenly across the parameter space to maximize early coverage; no cost-weighting applied.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report qualitatively that LHS yields more uniformly dispersed coverage than Random Sampling (visual comparison in Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LHS shown qualitatively to reduce clustering and provide broader coverage than Random Sampling; no numerical metric given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Used to prioritize exploratory coverage at initialization, improving later BO performance; no quantitative tradeoff analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use LHS to initialize BO/surrogate when dealing with high-dimensional catalyst synthesis parameter spaces to avoid early neglect of important regions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence (AI) workflow for catalyst design and optimization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2427.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2427.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM literature mining (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model (ChatGPT) based automated literature mining for chemical space construction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ChatGPT and text-mining pipelines (Elsevier Text Mining API, keyword filters) to automatically extract catalyst preparation procedures, process variables, and numeric values from literature and build a conceptual/graph-based representation of the chemical search space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT-based literature extraction and chemical-space construction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated pipeline: query ScienceDirect via Elsevier Text Mining API to retrieve articles, segment method sections using keyword filters, feed extracted paragraphs to ChatGPT with structured prompts to extract variables (temperature, pressure, durations, etc.) and units; store extracted parameters and relations in Neo4j and vectorize for optimization input.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Knowledge extraction / search-space construction for experimental design in chemistry and materials science</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Provides a way to allocate experimental attention by defining a literature-informed search space of variables and candidate categorical values; does not itself select experiments but shapes the set of hypotheses the BO will allocate experiments to.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not applicable to the LLM extraction step; it supplies diverse candidate variables to the downstream BO which handles exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Promotes diversity indirectly by extracting many variable types and categorical options from a broad literature corpus and emphasizing diverse input data when constructing the search space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Not directly applicable; potential limits from API access and document availability noted qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Practical constraints: authors limited corpus to ScienceDirect XML-accessible articles due to API availability; no explicit computational-cost tradeoffs discussed for LLM usage.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Extraction produced 603 XML-accessible articles, 774 distinct catalysts, 737 unique procedure steps; key parameters extracted spanned activation pressure 3–10 MPa, activation duration 0.5–8 h (expanded in experiments up to 286.235 min for an optimum suggestion), activation temperature 200–700 °C (expanded experimentally up to 900 °C in search space).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual extraction / expert-curated search spaces (discussed as current-conventional practice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Authors argue LLM-derived search-space construction is more scalable and can leverage wider literature than manual expert construction; no direct quantitative comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes limitations from data reliability in literature and API access constraints; does not quantify computational cost vs information gain of LLM extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use LLMs to construct a comprehensive and diverse search space from literature before optimization; ensure expert QA filters on extracted parameters to maintain realism of experimental bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence (AI) workflow for catalyst design and optimization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Practical bayesian optimization of machine learning algorithms <em>(Rating: 2)</em></li>
                <li>Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories <em>(Rating: 2)</em></li>
                <li>AlphaFlow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning <em>(Rating: 2)</em></li>
                <li>A multi-objective active learning platform and web app for reaction optimization <em>(Rating: 2)</em></li>
                <li>Bayesian optimization for adaptive experimental design: A review. <em>(Rating: 2)</em></li>
                <li>Bayesian reaction optimization as a tool for chemical synthesis. <em>(Rating: 2)</em></li>
                <li>Parallel bayesian optimization of multiple noisy objectives with expected hypervolume improvement <em>(Rating: 2)</em></li>
                <li>Fast calculation of multiobjective probability of improvement and expected improvement criteria for Pareto optimization <em>(Rating: 2)</em></li>
                <li>Is GPT-3 all you need for low-data discovery in chemistry? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2427",
    "paper_id": "paper-264072630",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "AI workflow (LLM+BO+AL)",
            "name_full": "Integrated AI workflow combining Large Language Models, Bayesian Optimization, and an Active Learning loop for catalyst design",
            "brief_description": "A pipeline that (1) uses an LLM (ChatGPT) to mine literature and construct a multidimensional chemical search space, (2) represents that space as vectors for input to Bayesian optimization with a Gaussian Process surrogate, and (3) runs an iterative active-learning experimental loop to propose, synthesize, evaluate, and incorporate experimental results to converge on optimal synthesis parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Integrated LLM + Bayesian Optimization + Active Learning workflow",
            "system_description": "The system first harvests experimental procedures and process variables from literature using ChatGPT and keyword filtering, organizes the extracted variables into a graph (Neo4j) and vectorizes them (min-max normalization for continuous variables, OHE for categorical). A Gaussian Process surrogate with a constant mean and Matern-5/2 kernel models the mapping from synthesis parameters to performance. Acquisition functions (EI for single-objective, EHVI for multi-objective) select candidate experiments. Selected conditions are physically synthesized and evaluated; the measured performance is fed back to update the GP and re-run acquisition in an iterative active learning loop until convergence or resource limits.",
            "application_domain": "Catalyst discovery and optimization (ammonia synthesis) / materials synthesis",
            "resource_allocation_strategy": "Selects next experiments by maximizing an acquisition function (Expected Improvement or Expected Hypervolume Improvement) computed from the GP surrogate's posterior mean and variance; initial coverage ensured by Latin Hypercube Sampling; termination can be triggered by performance thresholds, a maximum number of experiments, or practical resource constraints (time/materials).",
            "computational_cost_metric": null,
            "information_gain_metric": "Expected Improvement (EI) for single objective; Expected Hypervolume Improvement (EHVI) for multi-objective; GP predictive variance and associated uncertainty estimates underpin expected utility.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balanced via acquisition functions: EI trades off predictive mean (exploitation) and predictive variance (exploration); EHVI extends this to multi-objective Pareto-front hypervolume improvement, guiding sampling toward points that either improve predicted performance or reduce uncertainty near the Pareto front.",
            "diversity_mechanism": "Diversity enforced through (a) initial Latin Hypercube Sampling to cover the space uniformly, (b) inclusion of many categorical options from literature in the vectorized space, and (c) EHVI's tendency to explore diverse Pareto-optimal trade-offs rather than a single optimum.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Time and material resource constraints; maximum allowable number of experiments (practical lab budget).",
            "budget_constraint_handling": "Budget considerations are incorporated into termination criteria (stop when threshold performance reached or when experiment budget is exhausted); the acquisition function selects maximally informative experiments under the running budget but the paper gives no formal cost-penalized acquisition implementation.",
            "breakthrough_discovery_metric": "For the case study, breakthrough is operationalized as high ammonia concentration; generally measured by Expected Improvement (EI) or increase in dominated hypervolume (EHVI) indicating movement toward Pareto-optimal, high-impact solutions.",
            "performance_metrics": "Predicted optimized ammonia concentration: 730.43 ppm for the suggested optimal point (activation temp 787.782 °C, duration 286.235 min, heating rate 14.239 °C/min, pressure 5.002 MPa). Dataset extraction: 603 articles, 774 distinct catalysts, 737 unique procedure steps.",
            "comparison_baseline": "Compared qualitatively/visually to Random Sampling for initialization; contrasted with traditional OFAT and DoE approaches in discussion.",
            "performance_vs_baseline": "Authors report that Latin Hypercube Sampling provides more uniform initial coverage than Random Sampling (visual/qualitative); no numerical comparison of optimization performance vs OFAT/DoE/random search is provided.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper discusses the exploration/exploitation tradeoff (via EI/EHVI) and multi-objective tradeoffs (e.g., activity vs stability on a Pareto front). It also notes practical tradeoffs from time and material resource limits influencing termination, but does not provide a quantitative tradeoff surface linking computation/experiment cost to information gain or diversity.",
            "optimal_allocation_findings": "Practical recommendations: construct diverse search space from literature using LLMs; initialize with Latin Hypercube Sampling; use GP surrogate with Matern-5/2 kernel and constant mean; use EI for single-objective and EHVI for multi-objective acquisition; iterate experiments in an active learning loop and terminate when performance thresholds or experiment budgets are reached.",
            "uuid": "e2427.0",
            "source_info": {
                "paper_title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Bayesian optimization (BO) + GP",
            "name_full": "Bayesian optimization with Gaussian Process surrogate modeling",
            "brief_description": "An active learning optimization method that uses a GP surrogate to model an unknown objective function and an acquisition function to propose the next experimental points balancing exploration and exploitation; adopted here with Matern-5/2 kernel and constant mean for catalyst synthesis parameter optimization.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian Optimization with Gaussian Process surrogate",
            "system_description": "A GP surrogate models f(x) (mapping from synthesis parameters to performance) with a constant mean and Matern-5/2 covariance; the GP yields predictive mean and variance used by acquisition functions (EI / EHVI) to rank candidate experiments. Categorical variables are represented via one-hot encoding, continuous variables normalized by min-max. Initial training points chosen via LHS.",
            "application_domain": "Experimental design / catalyst synthesis optimization (materials science)",
            "resource_allocation_strategy": "Allocate experimental runs by maximizing acquisition functions computed from the GP posterior; initially sample broadly (LHS) then iteratively propose points maximizing expected improvement or hypervolume gain.",
            "computational_cost_metric": null,
            "information_gain_metric": "GP predictive variance and acquisition functions (EI / EHVI) which use expected improvement/hypervolume improvement as the utility metric.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions combine predictive mean (exploitation) and predictive uncertainty (exploration); EI inherently trades off these terms; EHVI extends to multi-objective contexts to target Pareto improvements.",
            "diversity_mechanism": "Diversity arises from initial LHS and from EHVI's multi-objective optimization across the Pareto front; one-hot encoding enables representation of diverse categorical choices available from literature.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experiment budget / time and material limits (discussed qualitatively).",
            "budget_constraint_handling": "Termination by pre-established thresholds (performance) or experimental budget caps; GP + acquisition function prioritize the most informative experiments per iteration but no explicit cost-aware acquisition rule is described.",
            "breakthrough_discovery_metric": "Expected Improvement or Expected Hypervolume Improvement (for multi-objective) and domain-specific performance metrics such as ammonia concentration.",
            "performance_metrics": "Suggested optimal point predicted ammonia concentration = 730.43 ppm; GP uncertainty reduced across iterations (qualitative, shown in surrogate evolution figures).",
            "comparison_baseline": "Random Sampling (initialization comparison), OFAT and DoE discussed as conventional alternatives in background.",
            "performance_vs_baseline": "LHS initialization yields better space coverage than Random Sampling; BO is described as more efficient than OFAT/DoE in literature discussion but the paper does not report direct numerical head-to-head comparisons in this study.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Describes qualitatively how BO balances exploration vs exploitation and handles noisy, high-dimensional objectives; multi-objective extension (EHVI) handles tradeoffs between conflicting metrics like activity and stability.",
            "optimal_allocation_findings": "Use GP with Matern-5/2 kernel and constant mean in low-data catalyst optimization tasks; initialize with LHS; employ EI/EHVI acquisition functions and iterate in an active learning loop until resource-driven stopping criteria are met.",
            "uuid": "e2427.1",
            "source_info": {
                "paper_title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "EI / EHVI",
            "name_full": "Expected Improvement (EI) and Expected Hypervolume Improvement (EHVI) acquisition functions",
            "brief_description": "EI is a single-objective acquisition that selects next experiments maximizing expected improvement over current best; EHVI generalizes this to multi-objective problems by selecting experiments expected to increase the dominated hypervolume of the Pareto front.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Acquisition functions: Expected Improvement (EI) and Expected Hypervolume Improvement (EHVI)",
            "system_description": "EI uses the GP predictive mean and variance to compute expected improvement over the incumbent and selects the point maximizing this expectation. EHVI computes the expected increase in dominated hypervolume in multi-objective space if a candidate point were sampled, accounting for correlations between objectives under the GP posterior.",
            "application_domain": "Active experimental design for single- and multi-objective optimization (catalyst synthesis)",
            "resource_allocation_strategy": "Allocate individual experimental runs to the point that maximizes EI (single-objective) or EHVI (multi-objective), thereby attempting to maximize expected utility per experimental cost (implicitly per run).",
            "computational_cost_metric": null,
            "information_gain_metric": "Expected improvement (EI) and expected hypervolume improvement (EHVI) serve as the information/utility metrics.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "EI balances exploitation (high predicted mean) and exploration (high variance) through its analytic form; EHVI extends this trade-off across objectives, encouraging sampling that either improves objectives or fills gaps on the Pareto front.",
            "diversity_mechanism": "EHVI promotes exploration along different regions of the Pareto front, indirectly encouraging diverse solutions; no explicit diversity-penalized acquisition is described.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Implicit: number of experiments and lab resources; not formalized in acquisition.",
            "budget_constraint_handling": "Acquisition selects the most promising points per iteration; explicit cost-aware acquisition (e.g., per-experiment cost weighting) is not detailed.",
            "breakthrough_discovery_metric": "Improvement in objective value (EI) or increase in Pareto-front dominated hypervolume (EHVI); domain-specific breakthroughs measured like high ammonia concentration.",
            "performance_metrics": "No quantitative performance gains for EI/EHVI vs alternatives are provided beyond their adoption in the workflow; EHVI described as capturing correlations between objectives.",
            "comparison_baseline": "EI/EHVI contrasted conceptually with single-point greedy selection or random sampling; no numerical baseline comparison in paper.",
            "performance_vs_baseline": "Not quantified in this paper.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper explains conceptually how EI/EHVI embody the exploration/exploitation tradeoff and how EHVI is appropriate for multi-objective Pareto exploration (activity vs stability), but no numeric tradeoff curves are provided.",
            "optimal_allocation_findings": "Use EI for single-objective optimization and EHVI for multi-objective catalyst optimization to prioritize experiments expected to maximally improve performance or Pareto hypervolume.",
            "uuid": "e2427.2",
            "source_info": {
                "paper_title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Active learning loop",
            "name_full": "Iterative experimental active learning loop bridging prediction and physical experiments",
            "brief_description": "An iterative cycle where acquisition-selected synthesis parameters are experimentally realized, performance metrics measured and fed back to update the GP surrogate, repeating until termination criteria (performance threshold, max experiments, or resource constraints) are met.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Active learning experimental loop",
            "system_description": "The loop: (1) acquisition function proposes parameter set(s), (2) laboratory executes synthesis and performance evaluation (activity, selectivity, stability), (3) results are added to the dataset, (4) GP surrogate is updated, (5) acquisition re-computed. Loop stops when predefined performance thresholds or budget limits are reached or when additional improvement is negligible.",
            "application_domain": "Closed-loop experimental design for catalyst synthesis and optimization",
            "resource_allocation_strategy": "Experimental budget allocated iteratively to conditions that the acquisition function deems maximally informative/beneficial; termination decisions consider resource constraints and urgency.",
            "computational_cost_metric": null,
            "information_gain_metric": "Implicit via acquisition functions (EI/EHVI) that use GP predictive uncertainty to estimate expected utility/information gain of each candidate experiment.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Controlled by choice of acquisition function; the loop updates model uncertainty and focuses subsequent experiments on high-utility regions identified by EI/EHVI.",
            "diversity_mechanism": "Initial diversity via LHS; active loop naturally explores different regions due to acquisition-driven uncertainty reduction and EHVI multi-objective exploration, but no explicit diversity-regularized acquisition is described.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Experiment count, time, materials; practical lab resource constraints mentioned.",
            "budget_constraint_handling": "Termination criteria include resource constraints; the loop is intended to be resource-efficient by selecting highly informative experiments but no explicit cost-per-info optimization is implemented.",
            "breakthrough_discovery_metric": "Same as acquisition: EI/EHVI and domain-specific metrics (e.g., ammonia concentration).",
            "performance_metrics": "Authors show surrogate evolution from few to many datapoints qualitatively; recommended optimal point predicted ammonia concentration 730.43 ppm is an example endpoint.",
            "comparison_baseline": "Compared conceptually to one-shot screening or brute-force high-throughput approaches; LHS and Random Sampling used for initial-sampling comparison.",
            "performance_vs_baseline": "Qualitative claims of higher resource-efficiency vs OFAT/DoE and better coverage from LHS vs Random Sampling; no quantitative experiment-count reductions reported.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Describes qualitative tradeoffs (explore vs exploit, multi-objective Pareto tradeoffs) and practical tradeoffs imposed by time and material budgets, but lacks a quantitative allocation tradeoff analysis.",
            "optimal_allocation_findings": "Iterative active learning with BO is recommended to maximize value per experiment; initialize broadly, then use GP/EI or EHVI to allocate follow-up experiments until resource- or performance-based stopping conditions.",
            "uuid": "e2427.3",
            "source_info": {
                "paper_title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LHS",
            "name_full": "Latin Hypercube Sampling (LHS) initial sampling",
            "brief_description": "A stratified sampling technique used to select an initial set of points that are well-distributed across the high-dimensional parameter space, improving initial coverage relative to naive random sampling.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Latin Hypercube Sampling (LHS) for initialization",
            "system_description": "LHS divides each input dimension into equal-probability intervals and samples once from each interval, ensuring that initial design points collectively cover the space with reduced clustering compared to pure random sampling; used here to seed the GP surrogate before iterative BO.",
            "application_domain": "Initial-design sampling for active learning and Bayesian optimization in experimental sciences (catalyst synthesis)",
            "resource_allocation_strategy": "Allocates the initial experimental budget to maximize uniform coverage of the parameter space, reducing the risk of missing important regions early in optimization.",
            "computational_cost_metric": null,
            "information_gain_metric": "Implicit: aims to maximize representativeness/coverage which supports later information gain when used to train the initial surrogate.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "LHS is purely exploratory for initialization; subsequent exploitation/exploration is governed by BO acquisition.",
            "diversity_mechanism": "Primary mechanism to ensure diversity via stratified sampling across each dimension.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Initial experimental budget (number of initial runs); no further formal budget model provided.",
            "budget_constraint_handling": "Distributes the available initial sample budget evenly across the parameter space to maximize early coverage; no cost-weighting applied.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Authors report qualitatively that LHS yields more uniformly dispersed coverage than Random Sampling (visual comparison in Figure 6).",
            "comparison_baseline": "Random Sampling",
            "performance_vs_baseline": "LHS shown qualitatively to reduce clustering and provide broader coverage than Random Sampling; no numerical metric given.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Used to prioritize exploratory coverage at initialization, improving later BO performance; no quantitative tradeoff analysis provided.",
            "optimal_allocation_findings": "Use LHS to initialize BO/surrogate when dealing with high-dimensional catalyst synthesis parameter spaces to avoid early neglect of important regions.",
            "uuid": "e2427.4",
            "source_info": {
                "paper_title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM literature mining (ChatGPT)",
            "name_full": "Large Language Model (ChatGPT) based automated literature mining for chemical space construction",
            "brief_description": "Use of ChatGPT and text-mining pipelines (Elsevier Text Mining API, keyword filters) to automatically extract catalyst preparation procedures, process variables, and numeric values from literature and build a conceptual/graph-based representation of the chemical search space.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT-based literature extraction and chemical-space construction",
            "system_description": "Automated pipeline: query ScienceDirect via Elsevier Text Mining API to retrieve articles, segment method sections using keyword filters, feed extracted paragraphs to ChatGPT with structured prompts to extract variables (temperature, pressure, durations, etc.) and units; store extracted parameters and relations in Neo4j and vectorize for optimization input.",
            "application_domain": "Knowledge extraction / search-space construction for experimental design in chemistry and materials science",
            "resource_allocation_strategy": "Provides a way to allocate experimental attention by defining a literature-informed search space of variables and candidate categorical values; does not itself select experiments but shapes the set of hypotheses the BO will allocate experiments to.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Not applicable to the LLM extraction step; it supplies diverse candidate variables to the downstream BO which handles exploration/exploitation.",
            "diversity_mechanism": "Promotes diversity indirectly by extracting many variable types and categorical options from a broad literature corpus and emphasizing diverse input data when constructing the search space.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Not directly applicable; potential limits from API access and document availability noted qualitatively.",
            "budget_constraint_handling": "Practical constraints: authors limited corpus to ScienceDirect XML-accessible articles due to API availability; no explicit computational-cost tradeoffs discussed for LLM usage.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Extraction produced 603 XML-accessible articles, 774 distinct catalysts, 737 unique procedure steps; key parameters extracted spanned activation pressure 3–10 MPa, activation duration 0.5–8 h (expanded in experiments up to 286.235 min for an optimum suggestion), activation temperature 200–700 °C (expanded experimentally up to 900 °C in search space).",
            "comparison_baseline": "Manual extraction / expert-curated search spaces (discussed as current-conventional practice).",
            "performance_vs_baseline": "Authors argue LLM-derived search-space construction is more scalable and can leverage wider literature than manual expert construction; no direct quantitative comparison provided.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper notes limitations from data reliability in literature and API access constraints; does not quantify computational cost vs information gain of LLM extraction.",
            "optimal_allocation_findings": "Use LLMs to construct a comprehensive and diverse search space from literature before optimization; ensure expert QA filters on extracted parameters to maintain realism of experimental bounds.",
            "uuid": "e2427.5",
            "source_info": {
                "paper_title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bayesian optimization",
            "rating": 2,
            "sanitized_title": "bayesian_optimization"
        },
        {
            "paper_title": "Practical bayesian optimization of machine learning algorithms",
            "rating": 2,
            "sanitized_title": "practical_bayesian_optimization_of_machine_learning_algorithms"
        },
        {
            "paper_title": "Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories",
            "rating": 2,
            "sanitized_title": "chimera_enabling_hierarchy_based_multiobjective_optimization_for_selfdriving_laboratories"
        },
        {
            "paper_title": "AlphaFlow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning",
            "rating": 2,
            "sanitized_title": "alphaflow_autonomous_discovery_and_optimization_of_multistep_chemistry_using_a_selfdriven_fluidic_lab_guided_by_reinforcement_learning"
        },
        {
            "paper_title": "A multi-objective active learning platform and web app for reaction optimization",
            "rating": 2,
            "sanitized_title": "a_multiobjective_active_learning_platform_and_web_app_for_reaction_optimization"
        },
        {
            "paper_title": "Bayesian optimization for adaptive experimental design: A review.",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_for_adaptive_experimental_design_a_review"
        },
        {
            "paper_title": "Bayesian reaction optimization as a tool for chemical synthesis.",
            "rating": 2,
            "sanitized_title": "bayesian_reaction_optimization_as_a_tool_for_chemical_synthesis"
        },
        {
            "paper_title": "Parallel bayesian optimization of multiple noisy objectives with expected hypervolume improvement",
            "rating": 2,
            "sanitized_title": "parallel_bayesian_optimization_of_multiple_noisy_objectives_with_expected_hypervolume_improvement"
        },
        {
            "paper_title": "Fast calculation of multiobjective probability of improvement and expected improvement criteria for Pareto optimization",
            "rating": 2,
            "sanitized_title": "fast_calculation_of_multiobjective_probability_of_improvement_and_expected_improvement_criteria_for_pareto_optimization"
        },
        {
            "paper_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "rating": 1,
            "sanitized_title": "is_gpt3_all_you_need_for_lowdata_discovery_in_chemistry"
        }
    ],
    "cost": 0.01618125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Artificial Intelligence (AI) workflow for catalyst design and optimization</p>
<p>Nung Siong 
Department of Chemical Engineering
Tsinghua University
100084BeijingChina</p>
<p>Yi Shen Tew 
Xialin Zhong 
Department of Chemical Engineering
Tsinghua University
100084BeijingChina</p>
<p>Jun Yin 
Department of Chemical and Biomolecular Engineering
National University of Singapore
117576Singapore</p>
<p>Jiali Li 
Department of Chemical and Biomolecular Engineering
National University of Singapore
117576Singapore</p>
<p>Binhang Yan 
Department of Chemical Engineering
Tsinghua University
100084BeijingChina</p>
<p>Xiaonan Wang wangxiaonan@tsinghua.edu.cn 
Department of Chemical Engineering
Tsinghua University
100084BeijingChina</p>
<p>An Artificial Intelligence (AI) workflow for catalyst design and optimization
3750250064D2B70CCCA5EFCF503682CDCatalystsLarge Language ModelsActive LearningBayesian OptimizationAmmonia Synthesis
In the pursuit of novel catalyst development to address pressing environmental concerns and energy demand, conventional design and optimization methods often fall short due to the complexity and vastness of the catalyst parameter space.The advent of Machine Learning (ML) has ushered in a new era in the field of catalyst optimization, offering potential solutions to the shortcomings of traditional techniques.However, existing methods fail to effectively harness the wealth of information contained within the burgeoning body of scientific literature on catalyst synthesis.To address this gap, this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization.Our methodology combines advanced language understanding with robust optimization strategies, effectively translating knowledge extracted from diverse literature into actionable parameters for practical experimentation and optimization.In this article, we demonstrate the application of this AI workflow in the optimization of catalyst synthesis for ammonia production.The results underscore the workflow's ability to streamline the catalyst development process, offering a swift, resource-efficient, and highprecision alternative to conventional methods.</p>
<p>Introduction</p>
<p>The development of novel catalysts to address increasing energy demand and consumption has become an urgent task in the realm of renewable energy 1 .The impetus for this development is illustrated by the growth in the global catalyst market, which was valued at USD 29.7 billion in 2022 and is projected to grow at a compound annual growth rate (CAGR) of 4.6% from 2023 to 2030.This surge is driven not only by escalating demands from applications in process optimization, yield improvement, and energy saving but also by a heightened awareness and concern for environmental issues, particularly the increase in carbon dioxide emissions.The proposition of carbon neutrality has illustrated the transformation path for the chemical industry over the coming decades, and set forth new challenges regarding the use of renewable energy and the catalytic conversion of carbon dioxide into high-value chemical products 2,3 .Undeniably, the development of novel catalysts is crucial in addressing our energy needs and environmental concerns, yet it presents an arduous task, owing to the multifaceted nature of the problem at hand 4 .</p>
<p>The path to new catalyst development is beset with three salient challenges 5 .First, the traditional catalyst development process, especially in the context of solid catalysts, requires extensive preliminary efforts.The sequence of catalyst synthesis, activity testing, characterization, and industrial scale-up form a complex and time-consuming process 6,7 .Identifying optimal synthesis methods and process parameters requires extensive experimental data.The second challenge arises from the limitations imposed by time and material resources 7 .These constraints restrict chemists to exploring only the tip of the iceberg of the vast, high-dimensional chemical parameter space, thereby leaving potentially superior catalysts undiscovered 8 .The complexity of navigating this parameter space is underscored in the multi-step catalyst development process, where the crucial interrelation between catalytic activity and process variables is often neglected 9,10 .Lastly, the advancement of parallel screening and high-throughput experimental technologies, while promising, presents its challenges.These methods, though more efficient than traditional trial-anderror techniques, demand greater resources in terms of cost and data volume 11 , while given the lack of extensive data on new catalysts in lab conditions, the importance of small-scale data optimization is further highlighted 12 .The rapid synthesis of catalysts under various conditions, subsequent performance-based screening and optimization, and catalyst characterization all contribute to these escalating demands.</p>
<p>Several optimization strategies are conventionally employed to identify the optimal set of condition parameters, thereby enhancing the performance of the catalyst.The 'One Factor At a Time' (OFAT) method is frequently employed as an alternative technique for chemical process optimization and comprehension 13 .The OFAT approach often misinterprets chemical processes due to its inefficiency, inaccuracy, and neglect of synergistic effects and nonlinear responses among experimental variables 14 .Design of Experiments (DoE), a robust and extensively utilized optimization technique especially within the pharmaceutical and fine chemical sectors, serves to address these shortcomings 15 .DoE comprises a suite of statistical methodologies that endeavor to construct a mathematical model capable of describing the output of a chemical reaction (e.g., reaction yield, purity, etc.) based on the experimental inputs (e.g., temperature, reaction time) associated with the reaction.While these conventional optimization methods and their advancements have undeniably made significant contributions to the field, certain gaps persist that limit their full potential in optimizing catalyst synthesis.The predominant reliance on the empirical knowledge and intuition of seasoned chemists, while invaluable, is not systematically scalable and transferable.Techniques like OFAT and DoE, though statistically rigorous, are often unable to keep pace with the sheer complexity and vastness of the catalyst parameter space, leaving much of it unexplored and underutilized.These methods can struggle to account for the nuanced interdependencies among experimental variables and the nonlinear nature of chemical reactions 16,17 .</p>
<p>With the advent of machine learning (ML), the field of catalyst optimization is entering a new era.</p>
<p>ML techniques are gaining traction due to their capability to model complex, nonlinear systems and find correlations in vast datasets that might otherwise be overlooked 18,19 .Active learning based on Bayesian optimization (BO) 20 , a type of ML, presents a promising solution to the shortcomings of conventional optimization techniques.This strategy is based on constructing a probabilistic model for the objective function and then using it to select the most promising parameters to evaluate in the actual objective function, based on a balance between exploration and exploitation 21 .This allows for more efficient sampling of the parameter space compared to OFAT and DoE, which can accelerate the optimization process and potentially discover better solutions.In a recent study, Rossmeisl et al. demonstrated a novel application of Bayesian optimization in catalyst design, aiming to optimize multinary metal alloy catalysts for fuel cells 22 .</p>
<p>They employed a computational framework combining density functional theory (DFT) calculations, machine learning-driven kinetic modeling, and Bayesian optimization, efficiently identifying optimal catalyst compositions in vast multi-metallic spaces, thereby accelerating catalyst optimization processes.Kazuki and Yuta demonstrate the effective combination of Bayesian optimization and density functional theory calculations to find the optimal binary alloy catalyst for nitrogen activation, a critical step in ammonia synthesis 23 .Their study showcased how Bayesian optimization surpasses random search in efficiency, highlighting the benefits of data science and computational chemistry in accelerating catalyst research, and underscores plans for future exploration of multi-objective optimization for ammonia synthesis.</p>
<p>However, conventional techniques such as OFAT, DoE, and Bayesian optimization cannot efficiently leverage and synthesize the wealth of information embedded in the rapidly expanding body of scientific literature on catalyst synthesis.To bridge the gap mentioned above, many scientists have utilized the power of Artificial Intelligence (AI) to retrieve information from broad corpora of scientific literature.The Large Language Model (LLM) is a type of transformer model that is capable of modeling the probability of a sequence of tokens in texts 24 .By leveraging largescale data and massive models, it has effectively overcome several long-standing challenges in the field of Natural Language Processing (NLP), such as occasional fluency issues, lack of common knowledge, and unable to remember the context from previous sentences.ChatGPT and its successor, GPT-4, presented by OpenAI, are leading LLMs that are capable of understanding, generating, and translating human language, performing sentiment analysis, and answering questions 25 .Approaching human-level ability across many expert domains, GPT-4 can accomplish complex tasks in chemistry purely from human instructions, potentially setting the stage for transformative advancements in the field of catalyst synthesis.Jablonka et al.'s work demonstrated GPT-3's impressive capability to match conventional machine learning models in chemical property prediction and molecule design tasks, despite using fewer data points, by effectively identifying correlations within textual data 26 .On the other hand, researchers successfully utilized LLMs and particularly developed MolReGPT, which leverages prompts and retrieval methods for translation between molecules and text descriptions, surpassing other models in performance without the need for fine-tuning 27 .</p>
<p>By addressing the limitations of traditional chemical space construction and leveraging the strengths of active learning and Bayesian Optimization, the proposed AI workflow to expedite catalyst optimization at the particle scale could prove transformative, offering avenues for swift, resource-efficient, and high-precision catalyst synthesis.This workflow hinges on the convergence of advanced language understanding, Bayesian optimization, and active learning loop methodologies, operating in harmony to ascertain optimal solutions for synthesis parameters that will affect the structure of catalyst particles, consequently affecting their activity.This AI workflow enables the effective integration of knowledge extracted from a wide range of literature with practical experimentation.By fusing the text-understanding capabilities of LLMs, parameter optimization of BO, and the adaptive sampling of an active learning loop, this workflow offers a highly adaptive, robust, and efficient approach to optimizing the synthesis of catalysts.This article is structured as follows: detailed methodology can be found in Section 2; the results of a demonstrative case study are presented and discussed in Section 3; and conclusions are provided in Section 4.</p>
<p>Methodology</p>
<p>An illustration of the proposed AI-driven workflow is shown in Figure 1.The backbone of this workflow consists of two key components: the utilization of a large language model for constructing the chemical space and the application of Bayesian active learning for multi-objective optimization within that space.The large language model, ChatGPT trained on an extensive corpus, including scientific literature, is capable of understanding, analyzing, and extracting crucial data and concepts from a vast number of research papers pertinent to catalyst synthesis.This capability enables the AI to build a comprehensive and multi-dimensional search space for catalyst synthesis, encompassing various catalyst types, synthesis methods, reaction conditions, and their corresponding performance metrics.The Bayesian optimization methodology is brought to bear, which uses the information gathered by the large language model to direct the exploration and exploitation of the search space.Bayesian optimization techniques exploit probabilistic models, primarily Gaussian Processes, to approximate the unknown and complex function that maps catalyst synthesis parameters to performance metrics.The AI workflow is then augmented by an active learning loop, which functions to iteratively improve the optimization model.The loop initiates with the Bayesian optimization proposing an optimal set of condition parameters based on the current understanding of the objective function.These suggested parameters are then employed in real-life experiments to synthesize and evaluate catalysts.The performance metrics gathered from these experiments are fed back into the AI model, updating the understanding of the search space and the performance mapping function.Consequently, the loop allows for the AI system to learn dynamically from each experiment, constantly refining its predictions and progressively converging to the global optimum.Typically, the search space in Bayesian optimization is designed by experts according to their experiences.However, scientific literature contains rich and in-depth information about catalyst preparation and associated variables, which we aim to utilize for the construction of our search space.We place particular emphasis on the diversity of the input data to ensure a comprehensive search space.</p>
<p>Large Language Model (LLM) in Chemical Space Construction</p>
<p>The Elsevier Text Mining API was utilized to search the Science Direct database for academic articles relevant to the study.A generalized query string template, TITLE-ABS-KEY('Reaction Type' AND 'catalyst'), was used to capture a diverse range of articles.This template was applied using the demonstrative example of TITLE-ABS-KEY('ammonia synthesis' AND 'catalyst'), which initially yielded 2410 entries.Although the broad search term allows for a comprehensive collection, it may also include articles of varying relevance to the core research questions.The primary criterion for inclusion was the availability of articles in XML format to facilitate more efficient and precise text cleaning and segmentation, reducing the dataset to 603 articles that met this XML criterion.Additionally, while ACS Publications indeed hosts a large number of relevant articles, this study relied solely on Elsevier's Science Direct due to limitations in accessing ACS's text mining API.It should be noted that no manual screening was applied; instead, keyword-based automated filtering identified articles related to catalyst preparation.</p>
<p>After automatically gathering articles on ammonia synthesis catalysts, each literature is segmented into sections that focus on methods and experimental procedures.The filtering process was conducted using a list of keywords to detect catalyst preparation-related phrases, which was designed by sampling the section title from the literature dataset.</p>
<p>Designing Prompts for Data Extraction</p>
<p>Prompts, in the context of LLMs, are structured inputs designed to guide the model in generating specific and desirable outputs.Given that LLMs require guidance to understand the type and structure of the information to be extracted, we created specific prompts to assist in this process.</p>
<p>The design of these prompts was influenced by a thorough literature review and expert consultations.Each step in this process was tailored to set a context and provide clear instructions to guide the LLMs, in our case the ChatGPT model.</p>
<p>• Context Setting: You are a researcher in the field of [ammonia catalyst synthesis] who is good at extracting information from text.</p>
<p>• Text Introduction: This is the text which may or may not include the catalyst preparation methods.+ <input text> • Task Instructions: A comprehensive multi-step guide directs the model to analyze the text and extract relevant information, structured depending on the text content.</p>
<p>• Action Clarification: The model's capabilities and tasks are reiterated, clearly defining its role in identifying, extracting, and organizing relevant information.</p>
<p>• Output Structure Provision: A sample table is provided, offering a template for the desired output, guiding the structure and organization of the extracted information.</p>
<p>An example of the prompt used is shown below:</p>
<p>"You are a researcher in the field of [ammonia catalyst synthesis] who is good at extracting information from text.This is the text which may or may not include the catalyst preparation methods."+ <paragraph> + "Please follow these steps to extract information from the given text: Start:</p>
<ol>
<li>
<p>Analyze the text whether it is about catalyst preparation or not.</p>
</li>
<li>
<p>IF the text is about catalyst preparation THEN proceed to step 3 ELSE go to step 6.</p>
</li>
<li>
<p>Identify the process variables: temperature, pressure, duration of each step, rate of temperature change, reactant proportions, order of reactant addition, stirring speed, pH, concentrations, and additional parameters.4. Extract these values along with their units.5. Organize these into a table with the following columns: Catalyst Name, Specific Procedure Step, Process Variable, Numeric Value, and Unit.Then, END.</p>
</li>
<li>
<p>IF the text is not about catalyst preparation, output a blank table.</p>
</li>
</ol>
<p>END</p>
<p>You can perform the following actions:</p>
<p>• Identify whether the text is about preparing or synthesizing catalysts.</p>
<p>• Extract related information from the given text and given sample table format.</p>
<p>• Organize related information into the desired format.</p>
<p>• Extract the number and unit from the related information.</p>
<p>• Fill in the table with the desired information.</p>
<p>• Write down the condition of extracted properties in the condition column.</p>
<p>• Output the table.Recognizing the critical role of variable relationships in catalyst preparation, we organized the extracted data into a conceptual map.This map was created using Neo4j, a graph database management system, and parameters were selected based on frequency analysis and expert opinion 28 .The map effectively illustrates relationships between catalyst types, preparation procedures, procedure variables, numeric values, units, and applied conditions, as shown in Figure 2.</p>
<p>Sample Table:</p>
<p>Quality Control</p>
<p>At present, the Large Language Models (LLMs) are in the initial stages of fully understanding the context of catalyst preparation procedures and associated variables.To ensure the reliability and applicability of the information generated by ChatGPT, domain experts meticulously review extracted parameters, such as Activation Temperature (AT) and Activation Duration (AD).Their expertise aids in determining the range and feasibility of these parameters in real-world chemical processes, thereby aligning the AI-extracted data with current scientific understanding.To enhance the robustness of the information extraction process, two distinct filters have been designed to systematically categorize procedures and variables into their relevant categories.For a more concrete understanding, the following examples are provided.</p>
<p>For the process of melting iron oxides with structural promoters such as AlO, and CaO, and an activating promoter like KO, the specific procedure is classified as 'Combustion'.Similarly, in the 'Wet impregnation method', the specific procedure is 'Impregnation', and the variables such as 'Calcination temperature' and 'Drying temperature' are classified as 'Temperature', while 'Stirring time' is classified as 'Duration'.</p>
<p>Vectorization for Optimization</p>
<p>Given the need for a mathematical representation of our data to input into Bayesian optimization models, we converted our structural data into a multi-dimensional vector space.The min-max normalization method was used for continuous variables to define the range of space.For discrete variables, we included all options mentioned in the literature.This strategy facilitates an exhaustive representation of potential solutions within the "chemical space" associated with our catalyst.</p>
<p>Bayesian Active Learning</p>
<p>Active learning, a crucial aspect of machine learning, is particularly instrumental in the area of optimal experimental design 29,30 .The proposed Bayesian optimization loop is shown in Figure 3.This systematic approach helps identify the most beneficial experiments to be conducted next based on predefined objectives.One technique that has gained considerable attention recently is Bayesian Optimization (BO), an active learning technique that guides experimentalists in the lab to optimize unknown functions 31 .BO balances the exploration of the unknown function with experiments that exploit prior knowledge to pinpoint extrema.They operate with minimal data and use heuristic-based searches for the most informative observations 32 .This approach is versatile and can be applied to diverse search spaces, including arbitrary parameterized reaction domains, making it well-suited for optimizing the variables of catalyst synthesis.The strength of BO lies in its capacity to handle noise, high dimensionality, and the nonlinearity of the objective function, making it highly suitable for optimizing complex processes such as catalyst synthesis.</p>
<p>Initial Sampling Strategies</p>
<p>In the realm of Bayesian Optimization, the selection of initial sampling points plays a pivotal role in determining the overall effectiveness and efficiency of the optimization process 30 .These initial points from the search space form the foundation for the subsequent iterative learning and optimization steps, essentially guiding the model's exploration of the search space.This leads to the importance of employing robust initial sampling strategies, as they can significantly influence the optimization outcome.</p>
<p>Among the various initial sampling strategies available, Latin Hypercube Sampling (LHS) 33 has been utilized in this work.LHS, a stratified sampling technique, stands out due to its capability to generate a well-distributed set of initial points across the entire search space.Unlike other methods that randomly select initial points, LHS ensures that the sampled points span the search space as evenly as possible, which ensures a balanced exploration of the search space right from the outset, thus increasing the probability of identifying the optimal solution.Given the vastness and the highdimensional nature of the catalyst parameter space, a well-distributed initial sampling, as afforded by LHS, ensures a broad and diverse representation of the search space 34 .This is especially advantageous when dealing with complex, multi-step catalyst synthesis processes that involve numerous interrelated parameters.</p>
<p>Surrogate Model of BO</p>
<p>The implementation of a surrogate model is a fundamental facet of BO, with the Gaussian Process (GP) model adopted in this research because of its proficiency in managing uncertainty and the multifaceted nature inherent to catalyst synthesis optimization 35 .
[ f(x 1 ) ⋮ f(x n ) ] ~N ([ m(x 1 ) ⋮ m(x n ) ] , [ k(x 1 , x 1 ) ⋯ k(x 1 , x n ) ⋮ ⋱ ⋮ k(x n , x 1 ) ⋯ k(x n , x n ) ])(1)
A GP surrogate model offers a non-parametric, probabilistic technique to model the elusive function connecting the parameters of catalyst synthesis to performance metrics 21 .GP is formally defined as eq (1), where X = {x 1 , … , x n } is the vector of process variables.In this formulation, m(X) represents the mean function, encapsulating the expected value of GP across all elements of X. k (X, X ′ )is the covariance matrix, providing a measure of interrelation for all possible pairs within the set of process variables (X, X ′ ).The GP model's distinct advantage is its capability to estimate prediction-associated uncertainty, making it ideal for Bayesian Optimization that intrinsically requires balancing exploration and exploitation.This quantification of uncertainty enables the intelligent selection of subsequent data points, harmonizing the necessity for further exploration with optimization goals.The choice of mean function and covariance (or kernel) function in the GP model is instrumental in shaping the model's properties and subsequent performance.In this study, a constant mean function was selected for our Gaussian Process surrogate model, premising that the average value across the entire search space remains constant.</p>
<p>This assumption, while seemingly simple, is often a practical and effective starting point, particularly when undertaking new catalyst development projects.In such scenarios, the prior knowledge about the catalyst's performance landscape is typically sparse or even absent, making it challenging to substantiate more complex assumptions about the mean function.Therefore, a constant mean function provides a reasonable and unbiased baseline from which to start our optimization process and allows the model to learn and update its understanding of the objective function as more data is gathered.
k (X, X ′ ) = σ 2 (1 + √5r ℓ + 5r 2 5ℓ 2 ) exp (− √5r ℓ )(2)
Additionally, for the covariance function, a comparative assessment was carried out among a selection of Matern functions characterized by differing degrees of smoothness, denoted by ν.The Matern 5/2 kernel, expressed in eq 2, where r is the Euclidean distance between the points X and Moreover, GP predominantly operates on continuous variables, necessitating the conversion of categorical variables into a continuous format 36,37 .During the initial stages of optimization catalyst synthesis, the volume of experimental data is typically limited, especially in the field of the development of new catalysts.In these low-data regimes, simpler representation methods such as one-hot encodings (OHE) often yield significant results, or even surpass, the performance of more complex and resource-intensive descriptors 38,39 .The handling of categorical variables which are identified by the first step of the AI workflow as some of the important variables, using the method of one-hot encoding adds to the flexibility of the GP model, broadening its applicability to a wider range of scenarios.For example, in the synthesis of catalysts, where various solvents and metals are used, these categorical variables can be suitably represented in binary form via onehot encoding.This binary representation designates the presence or absence of specific solvents and metals.Beyond a binary representation, one-hot encoding can also accommodate the representation of multiple types of solvents and metals.Therefore, it does not just provide binary inclusion-exclusion information but extends to symbolize a multitude of categories within each variable.This way, the use of one-hot encoding contributes to an increasingly flexible Gaussian Process model, accommodating a broader span of input scenarios and enhancing its performance within the complex and multifaceted landscape of catalyst synthesis.This incorporation further bolsters the robustness of our GP-based Bayesian optimization framework.
X ′ , r = ‖X − X ′ ‖ 2 ,
Hence, the integration of the Gaussian Process surrogate model, a constant mean function, the Matern 5/2 covariance function, and effective handling of categorical inputs generate a robust, adaptable, and potent framework to model the intricate, high-dimensional, and uncertain search space intrinsic to catalyst synthesis.This firm foundation enhances the effectiveness of subsequent optimization stages, thus improving the likelihood of converging towards the global optimum.</p>
<p>Acquisition Function of MOBO</p>
<p>Bayesian optimization methodology centers around the acquisition function, a key component that determines the next set of experimental parameters to investigate.For single-objective optimization, the Expected Improvement (EI) acquisition function is often used due to its ability to balance exploration, targeting areas with significant predictive uncertainty, and exploitation, focusing on regions predicted to offer high performance 40 .This balance ensures effective and systematic use of resources.However, catalyst synthesis represents a complex, multi-objective optimization task, which is influenced by a variety of performance factors such as catalyst activity, selectivity, stability, and adaptability under various reaction conditions.The relationship between these often conflicting parameters is well-captured by the concept of Pareto optimality, which represents a set of non-dominated solutions where improvement in one objective invariably affects others 41 .By aiming to find solutions on or close to the Pareto front, a set of optimal solutions can be found where a decision-maker can choose depending on their specific preferences or requirements 40 .This provides a more comprehensive view of the possible optimal solutions, rather than a single "best" solution, which might not exist in multi-objective problems due to the conflicting nature of the objectives.To elaborate, activity and stability are the two parameters to be optimized.Any solution that increases the activity without decreasing the stability, or increases the stability without decreasing the activity, is considered Pareto optimal.</p>
<p>When dealing with multi-objective optimization, a more nuanced approach to the acquisition function is needed.This approach must balance multiple objectives, either by assigning specific weights to the objectives or by applying the principle of Pareto optimality.This is where the Expected Hypervolume Improvement (EHVI), a multi-objective acquisition function, comes into play 42 .The EHVI function considers the entire Pareto front, providing a comprehensive view of all optimal solutions.It is similar to the improvement in the EI acquisition function used for singleobjective optimization, the difference is that in EHVI, the improvement refers to the expected increase in the dominated hypervolume if a new sampling point were to be incorporated 43,44 .This means that EHVI considers not just the mean and variance predictions of the underlying Gaussian Process model, but also the correlation between multiple objectives.It guides the exploration through the high-dimensional parameter space to find the next promising point for exploration.</p>
<p>Active Learning Loop</p>
<p>In the Bayesian optimization framework applied to catalyst synthesis parameter optimization, the active learning loop is the crucial phase that bridges the gap between theoretical predictions and practical implementation 44 .Here, the selected set of parameters provided by the acquisition function is translated into a tangible experimental setup.Initially, experiments are conducted using the suggested combination of parameters.This step embodies the process of exploration and exploitation as indicated by the Gaussian Process surrogate model's predictive framework, which provides statistical estimates based on accumulated knowledge.This stage entails meticulous catalyst synthesis according to stipulated parameters and conditions, offering practical validation for the theoretical predictions of the model.</p>
<p>Once the catalyst is synthesized, its performance is subjected to a comprehensive evaluation.</p>
<p>Various performance metrics, such as catalyst activity, selectivity, stability, and adaptability under diverse reaction conditions, are measured.The precision in determining these metrics is of significant importance as they are incorporated back into the model, thereby directly influencing future predictions and optimization steps.Following the catalyst performance evaluation, the GP surrogate model is updated with the newly obtained experimental data.By accommodating this fresh information, the model's capability to predict the performance of different parameters is continually refined.A defining aspect of the active learning loop is the decision of when to terminate the process.The loop can conclude when the optimization process reaches a predetermined threshold of catalyst performance or a specific level of improvement.Alternatively, the loop might cease when the maximum allowable number of experiments is reached.The convergence criteria, while generally pre-established, should also incorporate practical factors such as resource constraints and the urgency of achieving the desired performance.</p>
<p>The active learning loop thus forms the backbone of the Bayesian optimization methodology.It fosters a systematic and efficient trajectory through the complex, high-dimensional parameter space of the catalyst synthesis process.By facilitating a robust cycle of prediction, experimentation, and learning, it paves the way for optimizing catalyst performance, offering significant potential to enhance the effectiveness of catalyst synthesis.</p>
<p>Result and Discussion</p>
<p>In this section, we explore the practical implementation and implications of the delineated AI Workflow for Catalyst Synthesis.We demonstrate the method's effectiveness and potential in new catalyst design through an Ammonia Synthesis case study.</p>
<p>The rising clean energy trend has spurred intense research into green hydrogen generation and its conversion into green ammonia.The drive towards decentralized green ammonia production necessitates equipment miniaturization and a reduction in reaction temperature and pressure This need aligns with contemporary catalyst requirements that prioritize milder operating conditions, robustness to potential catalyst poisoning from trace water oxygen in green hydrogen, and costeffectiveness.Notably, nitride catalysts are emerging as significant players in ammonia synthesis.</p>
<p>For instance, efforts have been made to construct a Ni/LaN system, leveraging the nitrogen vacancies of LaN to weaken N-N triple bonds, promoting N 2 dissociation assisted by Ni's facilitation of H 2 dissociation, a synergic approach albeit hindered by Ni's weak interaction with N 2 , restricting further enhancement of the catalytic activity of the system 45 .Besides, the Co 3 Mo 3 N catalyst stands out, exhibiting promising attributes aided by Cs or K promoters, nearing the performance of commercial Fe-based catalysts, although grappling with cost impediments and gaps in high-pressure activity data and resilience against water and oxygen 46 .In this study, we focused on the synthesis of the CoMo bimetallic nitride catalyst known for its higher ammonia synthesis activity 47 .This initiative involves the optimization of the activation step parameters to find an optimal set, promising a rich yield of ammonia.The process, characterized by a multitude of phases contributing to the catalyst's activity, requires parameter tuning to find the optimal parameter combination.Addressing these high-dimensional tasks necessitates the utilization of data-driven approaches and advanced machine-learning models.In this regard, the AI Workflow outlined in this study could be instrumental in overcoming these challenges.Such a method has the potential to streamline the design and synthesis of new catalysts, thereby improving the efficiency and reducing the time and cost associated with catalyst development.</p>
<p>Catalyst Preparation</p>
<p>The genesis of the Co 3 Mo 3 N (CMN) catalyst, central to ammonia production, begins with the preparation of CoMo bimetallic oxide, CoMoO 4 precursor through the hydrothermal method.We find the Co/Mo catalyst preparation process at a feed ratio of 1:1 to be a fitting example.Precisely</p>
<p>Data Extraction for Search Space Construction</p>
<p>Our study encompasses a diverse dataset by extracting information from literature using ChatGPT, that accounts for 774 distinct catalysts and 737 unique procedure steps, offering a comprehensive overview of various catalysts and procedures involved in ammonia synthesis processes.Table 1 and Table 2 serve as an illustrative summary of the dataset.</p>
<p>As shown in Table 1, the analysis of catalyst frequencies reveals that "Ammonia Catalyst" appears to be the most prevalent, finding use in 437 instances.This is succeeded by "Ru/MgO" and "Ammonia Synthesis Catalyst", appearing in 47 and 43 instances, respectively.Despite the considerable variety in the dataset, certain catalysts emerge as common across processes, with these three showing the highest frequency of usage.</p>
<p>Similarly, Table 2 outlines the frequency of different procedural steps.The "Impregnation method" stands as the most common, with 240 instances.This is followed by the "Synthesis" (187 instances), and "Reduction" (104 instances) methods, underscoring their significance in the analyzed chemical processes.Among the less frequent procedures, "Fixed-bed flow reactor" and "Synthesis with aqueous solutions of MnSO• HO and KMnO" appear in 24 instances each.Our data analysis from 603 articles on ammonia synthesis catalysts yielded significant insights into the multidimensional parameters shaping catalyst synthesis.These parameters included Activation Pressure (AP), Activation Duration (AD), Activation Temperature (AT), and Heating Rate (HR).</p>
<p>The optimization procedure targets key variables within this reduction step: activation pressure, activation temperature, heating rate, and activation duration, which emerged as dominant forces shaping the catalyst's structure and consequently, its performance in ammonia synthesis.These variables were identified as impactful parameters through the application of the large language model ChatGPT, which intelligently parses through extensive literature to discern the most influential factors in the catalyst synthesis process.</p>
<p>We observed that the Activation Pressure ranged from 3 to 10 MPa, which underscores the importance of accurate pressure control for optimal catalyst synthesis.Activation Duration, another crucial variable, presented a broader range from 0.5 to 8 hours, reflecting a wide temporal span for catalyst activation.The parameter of Activation Temperature showcased substantial variation, ranging from 200°C to 700°C.Meanwhile, the presence of lower temperatures suggests that certain catalysts can be synthesized and activated under relatively mild conditions.The Heating Rate, presented in °C/min, had a narrower range of values, primarily between 1 and 10.</p>
<p>Due to the limited amount of literature collected, the results extracted do not fully represent the entire search space.Enlarging the search space in terms of Activation Temperature is conceptualized with a foresight that higher temperatures could foster the formation of different phase structures, which in turn might influence the catalytic activity positively.This exploration promises the discovery of potentially favorable conditions that were not contemplated within the original bounds, possibly leading to groundbreaking insights into the activation parameters and their roles in synthesizing efficient catalysts.On a parallel note, initial rounds of experimentation suggested potential improvements in catalytic performance at different Heating Rates However, initial rounds of experimentation suggested potential improvements in catalytic performance at higher Activation temperatures and different Heating Rates.Therefore, in our effort to explore a larger space, we have appropriately expanded the search parameters.Specifically, we have increased the Activation Temperature up to 900°C and the Heating Rate up to 20 °C/min.This expansion will hopefully provide a broader understanding of catalyst activation, beyond the constraints of our initial findings.From this graphical representation, it is apparent that LHS ensures a more uniformly dispersed coverage across the parameter space in contrast to Random Sampling.This superior attribute of LHS for the initialization step effectively minimizes the clustering of sample points, reducing the potential for neglecting important regions of the parameter space.By generating a comprehensive initial exploration of the space, we set a robust foundation for the subsequent steps in the Bayesian Optimization process, enhancing its efficacy and efficiency in locating the optimum.The number The algorithm works towards finding the set of parameters that yield the highest concentration, effectively navigating the balance between the exploitation of areas of the parameter space known to yield good results and the exploration of less known areas that may offer further improvements.</p>
<p>The final result of this process is an optimized set of conditions that yield the maximum concentration of NH 3 , given the constraints of the problem.7a, in the initial iteration, the surrogate model provides a basic estimation of the system's behavior.Given the sparse data, the model is limited in its ability to capture the complexity of the parameter interactions.Despite this limitation, the GP model offers a good starting point for understanding the system's dynamics, focusing on areas that the model perceives as having the most potential for improvement based on the available data.As we move to a later iteration that incorporates a larger number of data points (Figure 7b), the surrogate model becomes significantly more refined.With this additional data, the model is better equipped to capture the system's behavior accurately and construct a more intricate understanding of the underlying relationships between parameters.Consequently, the surrogate model gains increased confidence in its predictions, as indicated by a decrease in the uncertainty around the estimated output.</p>
<p>Figure 7c provides a multi-dimensional representation of the parametric space of the ammonia production process.This visual representation elucidates the correlations between various operating conditions-namely activation temperature, activation duration, and heating rate-and the resultant ammonia concentration.The colors indicate the heating rate of the catalyst, while the size of each data point is proportional to the concentration of NH 3 produced under those conditions.</p>
<p>The star symbol in red represents the suggested optimal point proposed by the Bayesian optimization algorithm.The proposed optimal point, as determined by the Bayesian optimization algorithm, is graphically depicted within the explored parameter space.This point, characterized by an activation temperature of 787.782 ℃, an activation duration of 286.235 minutes, a heating rate of 14.239 ℃/min,, and an activation pressure of 5.002, is predicted by the surrogate model to yield an ammonia concentration of 730.43 ppm.Importantly, while this point is indeed suggested based on the algorithm's current understanding of the chemical space, its effectiveness, and true performance remain to be validated through further experimental testing.</p>
<p>Conclusion</p>
<p>In conclusion, the AI workflow proposed for catalyst synthesis optimization represents a compelling intersection of advanced language understanding, Bayesian optimization, and an active learning loop.ChatGPT, as a representative Large language model, has been used to construct a comprehensive and multidimensional search space.We then employed Bayesian optimization for effective multi-objective optimization within this space.Together, these methods demonstrated the potential to streamline the design and synthesis process for new catalysts.</p>
<p>In practical terms, this study has revealed substantial insights into catalyst synthesis for ammonia productiona burgeoning area of clean energy research.Through our analysis of 603 articles on ammonia synthesis catalysts, we have illuminated the significant role of factors such as Activation Pressure, Activation Duration, Activation Temperature, and Heating Rate in the synthesis process.</p>
<p>We have also showcased the expansive nature of the catalyst and procedural steps used in ammonia synthesis, reflecting the potential of our methodology to accommodate a diverse range of catalysts and synthesis processes.The practical implications of our work extend far beyond the mere understanding of catalyst design and synthesis.Our results have given us unique insights into the optimization of the ammonia production process.The Bayesian optimization algorithm proposed a set of conditions that, according to the GP surrogate model, and EI as an acquisition function, could result in an enhanced ammonia concentration.Specifically, the model suggested a combination of the parameters, forecasting an ammonia concentration of 730.43 ppm.This prediction, while yet to be verified experimentally, underscores the potential of AI-driven methods in guiding future experimental design.</p>
<p>While this study marks a considerable advancement in utilizing AI for catalyst synthesis, it is imperative to acknowledge the potential challenges and limitations encountered in the current framework.Future work should look at the reliability of published data, which forms the foundation of the LLM model.Any inconsistencies in data could potentially affect the output.</p>
<p>Therefore, validations should be made beyond the domain knowledge of the chemist.In addition, there is a clear necessity to expand the AI workflow to deal with multi-objective optimization problems that are frequently encountered in catalyst synthesis.This expansion should also include the ability to handle categorical input parameters, a feature critical for navigating the complexity of chemical spaces.Nevertheless, the AI workflow showcased in this study is a substantial leap forward in the quest for streamlined catalyst design.By effectively distilling insights from an extensive corpus of literature, this workflow demonstrates the potential of AI to supplement, if not replace, the domain knowledge traditionally required in this field.As we venture further into the era of clean energy, AI stands poised to play an increasingly prominent role in catalyzing advancements in catalyst synthesis and beyond.</p>
<p>Looking ahead, our goal is to evolve this AI workflow into an automatic loop capable of performing smart synthesis of new catalysts.This would leverage the power of the iterative active learning process coupled with the usage of large language models like ChatGPT.In this envisioned framework, once an initial set of parameters and a target objective are defined, the system would autonomously navigate through multiple iterations of experimentation and learning.At each stage, the AI system would analyze the outcomes of past iterations, discern trends and correlations, and generate predictions for the next most promising sets of parameters to test.This is where the integration of large language models comes into play.By constantly ingesting new scientific literature and research findings in real-time, the AI system could continually expand its knowledge base and refine its understanding of the catalyst synthesis process.This continual learning would allow the system to make increasingly informed and sophisticated decisions about the direction of experimentation, effectively "learning" its way toward optimal synthesis parameters.With such a system in place, the optimization process becomes a continual, autonomous loop of learning and refining, capable of swiftly identifying optimal catalyst synthesis strategies.This could substantially accelerate the discovery of novel catalysts and streamline their deployment in addressing our global energy challenges.</p>
<p>Figure 1 .
1
Figure 1.Schematic diagram of the proposed AI-driven workflow for catalyst synthesis optimization.</p>
<p>Figure 2 .
2
Figure 2. Process flow diagram of information extraction</p>
<p>|</p>
<p>Catalyst Name | Specific Procedure Step/Method | Process Variable | Numeric Value | Unit | | ZnCrOx and MnZnCrOx | Co-precipitation method | Temperature | 343 | K | | ZnCrOx and MnZnCrOx | Co-precipitation method | Duration | 30 | min | | ZnCrOx and MnZnCrOx | Co-precipitation method | pH | 8 | | | ZnCrOx and MnZnCrOx | Co-precipitation method | Aging | 3 | h | | ZnCrOx and MnZnCrOx | Co-precipitation method | Calcination temperature | 500 | °C | | MnZnCrOx | Addition of Mn(NO3)2 aqueous solution | Mass of Mn(NO3)2 | 0.26 | g | | MnZnCrOx | Addition of Mn(NO3)2 aqueous solution | Concentration of Mn(NO3)2 solution | 50 | wt% | | ZSM-5 zeolite | Hydrothermal method | Temperature | 180 | °C | | ZSM-5 zeolite | Hydrothermal method | Duration | 48 | h | | ZSM-5 zeolite | Hydrothermal method | Calcination temperature | 550 | °C | | OX-ZEO catalysts | Dual-bed mixing method | Mass ratio of oxide/zeolite | 1 | | | OX-ZEO catalysts | Granule mixing method | Mass ratio of oxide/zeolite | 1 | | | OX-ZEO catalysts | Powder mixing method | Mixing duration | 30 | min | | OX-ZEO catalysts | Powder mixing method | Pellet size | 20-40 | mesh |" 2.1.3.Conceptual Mapping of Extracted Information</p>
<p>Figure 3 .
3
Figure 3. Detailed depiction of the Bayesian optimization loop utilized within the proposed AI workflow.</p>
<p>σ 2 is the signal variance and ℓ is the length scale.Kernel emerged as the optimal choice for this particular catalyst synthesis optimization issue, striking an effective balance between model adaptability and computational efficiency.The Matern 5/2 kernel, with its intermediate level of smoothness, tends to perform well in practice, especially when handling noisy data or navigating high-dimensional search spaces.The parameter ν = 0.5, 1.5, 2.5 determines the smoothness of the function.The ν = 0.5 case corresponds to an absolute exponential (Laplace) kernel, representing a very non-smooth process.On the other end, infinity corresponds to the Radial Basis Function (RBF) kernel, representing an infinitely smooth process.The other two cases (ν = 1.5 and ν = 2.5) offer a balance between smoothness and flexibility, with the Matérn ν = 2.5 kernel being particularly highlighted due to its robustness in handling noisy data and highdimensional spaces.</p>
<p>weigh 5 .
5
8026 g of Co(NO 3 ) 2 • 6H 2 O and 4.8390 g of NaMoO 4 • 2H 2 O .Each is dissolved separately in 60 mL of deionized water, then stirred for 5 minutes at a speed of 500 rpm.Following this, the two solutions are combined and stirred continuously for an additional 10 minutes at the same speed.The mixed solution is then transferred into two 100 mL hydrothermal kettles, each containing approximately 60 mL of the solution.Place the hydrothermal kettles in a blast drying oven to undergo a hydrothermal reaction at 160 °C for 6 hours.Remove the kettles once they have completely cooled.The precipitate slurry obtained from the hydrothermal kettles is subjected to solid-liquid separation using a centrifuge operating at 5000 rpm for 5 minutes.Following centrifugation, discard the supernatant and add a certain amount of deionized water to mix evenly with the precipitate at the bottom of the centrifuge tube.Disperse ultrasonically for 5 minutes to thoroughly wash the precipitate before centrifuging again.Repeat this washing step three times.Transfer the thoroughly washed precipitate into a blast drying oven and dry for 12 hours at a set temperature of 100 °C.After drying, grind the violet blocky solid into a powder.Transfer this powder into a crucible for calcination in a muffle furnace at 450 °C for 6 hours, with a heating rate of 10 °C/min.After calcination, allow it to cool naturally to room temperature in the open air to obtain a light violet catalyst powder.Transfer the solid from the crucible to a sample bottle, marking the completion of the catalyst precursor preparation.</p>
<p>Figure 4
4
Figure4showcases the multi-stage synthesis process of the CoMo bimetallic nitride catalyst.Each phase, marked by distinct parameters illustrated in the figure, holds a role in determining the overall catalyst performance, albeit not equally.It should be noted that while there are other parameters influencing the synthesis process, including those at the hydrothermal stage, we strategically elected to focus on the activation step because modifying parameters in the hydrothermal stage generally necessitates a substantially extended timeframe to observe the resultant activity alterations, from several days to weeks.Consequently, it induces an exponential increase in both time and material costs.the activation step is where nitride is incorporated into the catalyst, a process in determining the structure of the resultant catalyst.The parameters chosen for optimization in our study are intrinsically linked to the control of this nitride introduction process, thereby holding a direct influence over the catalyst's final structure.</p>
<p>Figure 4 .
4
Figure 4. Schematic diagram illustrating the multi-stage Co 3 Mo 3 N(CMN) catalyst synthesis process for ammonia production.</p>
<p>Figure 5 .
5
Figure 5. Statistical analysis of catalyst synthesis parameters collected from the literature</p>
<p>Figure 6 .
6
Figure 6.Pairwise distribution of four critical parameters: Activation Pressure (MPa), Activation Duration (mins), Heating Rate (°C/min), and Activation Temperature (°C) visualized through two sampling strategies: Random Sampling (RS, teal) and Latin Hypercube Sampling (LHS, blue).</p>
<p>Figure 7 .
7
Figure 7. a.The Surrogate model's initial state incorporates just a few data points.b.Transitions to a later stage in the optimization process, where the model has been refined by a significantly larger set of data points.c.Suggested optimal point as indicated by the Bayesian optimization algorithm, along with the set of experimental data points obtained thus far.</p>
<p>Table 1
1
Frequency of Catalyst Names Extracted by ChatGPT from Literature
Catalyst Name</p>
<p>Table 2
2
Frequency of Procedure Steps Extracted by ChatGPT from Literature
Procedure Step
ACKNOWLEDGEMENTThis work is supported by the National Key R&amp;D Program of China (No. 2022ZD0117501).
. J S Hargreaves, L Li, Heterogeneous Catalysis for Sustainable Energy</p>
<p>A statistical review of considerations on the implementation path of China's "double carbon" goal. Sustainability. J Hao, L Chen, N Zhang, 20221411274</p>
<p>A quantitative roadmap for China towards carbon neutrality in 2060 using methanol and ammonia as energy carriers. Y Li, S Lan, M Ryberg, J Pérez-Ramírez, X Wang, Iscience. 2021624</p>
<p>The past, present and future of heterogeneous catalysis. I Fechete, Y Wang, J C Védrine, Catalysis Today. 18912012</p>
<p>Handbook of heterogeneous catalysis. G Ertl, H Knözinger, J Weitkamp, 1997VCH Weinheim</p>
<p>Innovation in process development: From catalyst to industrial process. J.-F Joly, F Giroudière, F Bertoncini, Catalysis today. 2182013</p>
<p>Developing Catalysts via Structure-Property Relations Discovered by Machine Learning: An Industrial Perspective. H Joshi, N Wilde, T S Asche, D Wolf, Chemie Ingenieur Technik. 202211</p>
<p>Bayesian optimization of high-entropy alloy compositions for electrocatalytic oxygen reduction. J K Pedersen, C M Clausen, O A Krysiak, B Xiao, T A Batchelor, T Löffler, V A Mints, L Banko, M Arenz, A Savan, Angewandte Chemie. 202145</p>
<p>Dynamic programming. R Bellman, Science. 15337311966</p>
<p>AlphaFlow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning. A A Volk, R W Epps, D T Yonemoto, B S Masters, F N Castellano, K G Reyes, M Abolhasani, Nature Communications. 202311403</p>
<p>Microscale high-throughput experimentation as an enabling technology in drug discovery: Application in the discovery of (Piperidinyl) pyridinyl-1 H-benzimidazole diacylglycerol acyltransferase 1 inhibitors. T Cernak, N J Gesmundo, K Dykstra, Y Yu, Z Wu, Z.-C Shi, P Vachal, D Sperbeck, S He, B A Murphy, Journal of Medicinal Chemistry. 6092017</p>
<p>Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories. F Häse, L M Roch, A Aspuru-Guzik, Chemical science. 9392018</p>
<p>DOE (design of experiments) in development chemistry: Potential obstacles. D Lendrem, M Owen, S Godbert, Organic Process Research &amp; Development. 532001</p>
<p>A Brief Introduction to Chemical Reaction Optimization. C J Taylor, A Pomberger, K C Felton, R Grainger, M Barecka, T W Chamberlain, R A Bourne, C N Johnson, A A Lapkin, Chemical Reviews. 20236</p>
<p>Efficiency by design: optimisation in process research. M R Owen, C Luscombe, S Lai; Godbert, D L Crookes, D Emiabata-Smith, Organic Process Research &amp; Development. 532001</p>
<p>. R Panday, Modeling, </p>
<p>Stability of micro dry wire EDM: OFAT and DOE method. A Banu, M Y Ali, M A Rahman, M Konneh, The International Journal of Advanced Manufacturing Technology. 1062020</p>
<p>Machine learning: the new AI. E Alpaydin, 2016MIT press</p>
<p>Machine-learning atomic simulation for heterogeneous catalysis. D Chen, C Shang, Z.-P Liu, npj Computational Materials. 20231</p>
<p>Practical bayesian optimization of machine learning algorithms. J Snoek, H Larochelle, R P Adams, 201225Advances in neural information processing systems</p>
<p>Bayesian optimization. R Garnett, </p>
<p>Towards the computational design of solid catalysts. J K Nørskov, T Bligaard, J Rossmeisl, C H Christensen, Nature chemistry. 112009</p>
<p>Exploring the Optimal Alloy for Nitrogen Activation by Combining Bayesian Optimization with Density Functional Theory Calculations. K Okazawa, Y Tsuji, K Kurino, M Yoshida, Y Amamoto, K Yoshizawa, ACS omega. 202249</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv [cs.CL] 2023A Survey of Large Language Models. </p>
<p>. Introducing Openai, Chatgpt, 2022. 2023 2023-06-25</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry? ChemRxiv Theoretical and Computational Chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.26434/chemrxiv-2023-fw8n42023-05-16</p>
<p>Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective. J Li, Y Liu, W Fan, X.-Y Wei, H Liu, J Tang, Q Li, arXiv [cs.CL] 2023</p>
<p>Neo4j graph database realizes efficient storage performance of oilfield ontology. F Gong, Y Ma, W Gong, X Li, C Li, X Yuan, 10.1371/journal.pone.0207595PLOS ONE. 1311e02075952018-11-16. 2023-05-20</p>
<p>A multi-objective active learning platform and web app for reaction optimization. J A G Torres, S H Lau, P Anchuri, J M Stevens, J E Tabora, J Li, A Borovika, R P Adams, A G Doyle, 10.1021/jacs.2c08592Journal of the American Chemical Society. 144432022-11-02</p>
<p>Bayesian optimization for adaptive experimental design: A review. S Greenhill, S Rana, S Gupta, P Vellanki, S Venkatesh, IEEE. 2020</p>
<p>Bayesian reaction optimization as a tool for chemical synthesis. B J Shields, J Stevens, J Li, M Parasram, F Damani, J I M Alvarado, J M Janey, R P Adams, A G Doyle, 10.1038/s41586-021-03213-yNature. 20217844</p>
<p>Multi-objective materials bayesian optimization with active learning of design constraints: Design of ductile refractory multiprincipal-element alloys. D Khatamsaz, B Vela, P Singh, D D Johnson, D Allaire, R Arróyave, Acta Materialia. 2361181332022</p>
<p>On Latin hypercube sampling. The annals of statistics. W.-L Loh, 199624</p>
<p>Large sample properties of simulations using Latin hypercube sampling. M Stein, Technometrics. 2921987</p>
<p>Gaussian processes for machine learning. M Seeger, 200414International journal of neural systems</p>
<p>Bayesian optimisation over multiple continuous and categorical inputs. B Ru, A Alvi, V Nguyen, M A Osborne, S Roberts, International Conference on Machine Learning. 2020PMLR</p>
<p>Dealing with categorical and integer-valued variables in bayesian optimization with gaussian processes. E C Garrido-Merchán, D Hernández-Lobato, Neurocomputing. 3802020</p>
<p>Equipping data-driven experiment planning for Self-driving Laboratories with semantic memory: case studies of transfer learning in chemical reaction optimization. R J Hickman, J Ruža, H Tribukait, L M Roch, A García-Durán, Reaction Chemistry &amp; Engineering. 2023</p>
<p>The effect of chemical representation on active machine learning towards closed-loop optimization. A Pomberger, A P Mccarthy, A Khan, S Sung, C Taylor, M Gaunt, L Colwell, D Walz, A Lapkin, Reaction Chemistry &amp; Engineering. 20226</p>
<p>Fast calculation of multiobjective probability of improvement and expected improvement criteria for Pareto optimization. I Couckuyt, D Deschrijver, T Dhaene, Journal of Global Optimization. 602014</p>
<p>A review of multi-objective optimization: Methods and its applications. N Gunantara, Cogent Engineering. 5115022422018</p>
<p>Parallel bayesian optimization of multiple noisy objectives with expected hypervolume improvement. S Daulton, M Balandat, E Bakshy, Advances in Neural Information Processing Systems. 202134</p>
<p>Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization. S Daulton, M Balandat, E Bakshy, Advances in Neural Information Processing Systems. 202033</p>
<p>Bayesian optimization with active learning of design constraints using an entropy-based approach. D Khatamsaz, B Vela, P Singh, D D Johnson, D Allaire, R Arróyave, Computational Materials. 2023149</p>
<p>Vacancy-enabled N2 activation for ammonia synthesis on an Ni-loaded catalyst. T.-N Ye, S.-W Park, Y Lu, J Li, M Sasase, M Kitano, T Tada, H Hosono, Nature. 20207816</p>
<p>Development and recent progress on ammonia synthesis catalysts for Haber-Bosch process. J Humphreys, R Lan, S Tao, Advanced Energy and Sustainability Research. 202112000043</p>
<p>Dual functionalized interstitial N atoms in Co3Mo3N enabling CO2 activation. K Feng, J Tian, J Zhang, Z Li, Y Chen, K H Luo, B Yang, B Yan, ACS Catalysis. 20228</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2917 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2917</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2917</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264128263</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.08922v1.pdf" target="_blank">LLaMA-Rider: Spurring Large Language Models to Explore the Open World</a></p>
                <p><strong>Paper Abstract:</strong> Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments and try to align the LLMs’ knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model’s performance. Besides, we integrate sub-task rela-beling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experiences. By evaluation in Minecraft, an open-ended sandbox world, we demonstrate that our approach LLaMA-Rider enhances the efficiency of the LLM in exploring the environment, and effectively improves the LLM’s ability to accomplish more tasks through fine-tuning with merely 1.3k instances of collected data, showing minimal training costs compared to the baseline using reinforcement learning. The code is available at https: //github.com/PKU-RL/LLaMA-Rider .</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2917.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2917.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-Rider</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-Rider (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage exploration-and-learning framework that spurs a large language model to explore an open-world (Minecraft) via a feedback-revision loop, collects successful and partial subtask trajectories, and then finetunes the LLM on those experiences to improve multi-task planning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLaMA-Rider</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The agent uses LLaMA-2-70B-chat as a policy that, given a textual observation, a task description, and short history, outputs a high-level skill. During exploration it applies a feedback-revision loop (environment feedback is converted to text and re-prompted up to T times) and subtask relabeling (replace task prompt with current subtask until completion). Collected successful and partial subtask trajectories are converted into a supervised dataset and used to finetune the same LLM via QLoRA/LoRA (SFT). Action selection uses noun-first matching + embedding retrieval to map LLM outputs to available skill descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>LLaMA-2-70B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Minecraft (MineDojo benchmark; Plan4MC tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Open-ended sandbox environment (Minecraft) provided via MineDojo. Tasks are multi-step crafting/harvest/interaction tasks (Plan4MC's 30 difficult tasks: log-based, cobblestone-based, mob-based), long-horizon, partially observable, with compositional subtasks and a large action/skill space (55 semantic skills).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>mixed: short-term working memory (recent action history) + subtask-level memory + experience (episodic) replay (supervised dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>1) Short-term working memory implemented by including the last three actions (h_t = last 3 actions) in the prompt; 2) Subtask relabeling: when the agent is working on a subtask τ_s, the prompt's task field is replaced with τ_s and kept until τ_s is completed (this enforces focused subtask context across steps); 3) Collected experiences (successful full-task trajectories and partial subtask trajectories) are stored offline as a supervised dataset (1.3k instances) used to finetune the LLM via QLoRA/LoRA (i.e., an experience-replay style static memory used to change model weights).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>During online action selection: prompt inclusion (recent actions and current subtask are included directly in prompt). Action-space retrieval uses noun-matching followed by embedding similarity to map LLM output to a canonical skill. Offline memory (experience dataset) is accessed by finetuning (no runtime retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Online short-term prompt memory: last 3 actions; Subtask label persisted until subtask completes; Offline experience memory: dataset of ~1.3k decision instances used for SFT. Exact unbounded long-term memory not used at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Last three executed skills (action history); current subtask identifier used in prompt (subtask label); collected trajectories including state-text observations, task/subtask labels, past-three-actions context, and chosen skill actions (successful full-task trajectories and partial subtask trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>After finetuning on collected experiences (i.e., using the offline experience memory), LLaMA-Rider (ours) achieved an average success rate of ~34% across the 30 Plan4MC tasks and successfully accomplished 25 out of 30 tasks (reported aggregate in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>LLaMA-Rider Base (before finetuning, relying on only prompts + short history during exploration) had an average success rate of ~20% across the same 30 tasks and accomplished 16 out of 30 tasks. Additionally, exploration with the feedback-revision loop and CoT was essential: without feedback-revision + CoT the agent failed most tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>+14 percentage points average success-rate (from ~20% to ~34% after SFT on collected experiences) and +9 more tasks successfully achieved (from 16 to 25 out of 30).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>1) Short prompt-based working memory (last 3 actions) plus the feedback-revision loop materially improves exploration efficiency; 2) Subtask relabeling (keeping subtask context in the prompt) substantially improves ability to complete long-horizon tasks and generalization via compositionality; 3) Storing collected successful and partial subtask trajectories and finetuning the LLM (experience replay via SFT) yields large gains in downstream task success and enables generalization to unseen harder tasks; 4) The feedback-revision mechanism is critical because it conveys environment-specific constraints to the LLM enabling efficient correction of invalid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Memory usage is limited: only last three actions are included in the prompt (short horizon), subtask relabeling is a simple label replacement (no structured memory graph), offline experience memory is modest (1.3k instances). Authors note limited utilization of richer environmental information and remaining retrieval inaccuracies (action retrieval via noun matching + embeddings can still produce errors). They also highlight no runtime long-term retrieval mechanism beyond prompt inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMA-Rider: Spurring Large Language Models to Explore the Open World', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2917.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2917.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GITM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GITM (Ghost in the Minecraft)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that integrates LLMs with text-based knowledge and memory to build generally capable agents in Minecraft; cited by this paper as an LLM-based Minecraft agent that includes memory components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GITM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as an approach that integrates LLMs with text-based knowledge and memory to operate in Minecraft; the paper cites it as an example of memory-augmented LLM agents but does not describe its architecture or experiments in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Minecraft (open-world MineDojo / similar)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Open-ended sandbox Minecraft environment; large, multi-step tasks requiring planning and environment grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>text-based knowledge and memory (as described in citation; details not provided here)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as an example of integrating text-based knowledge and memory with LLMs for Minecraft, but this paper does not provide architecture or performance details from GITM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMA-Rider: Spurring Large Language Models to Explore the Open World', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory <em>(Rating: 2)</em></li>
                <li>REACT: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
                <li>Language models meet world models: Embodied experiences enhance language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2917",
    "paper_id": "paper-264128263",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "LLaMA-Rider",
            "name_full": "LLaMA-Rider (this paper)",
            "brief_description": "A two-stage exploration-and-learning framework that spurs a large language model to explore an open-world (Minecraft) via a feedback-revision loop, collects successful and partial subtask trajectories, and then finetunes the LLM on those experiences to improve multi-task planning and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLaMA-Rider",
            "agent_description": "The agent uses LLaMA-2-70B-chat as a policy that, given a textual observation, a task description, and short history, outputs a high-level skill. During exploration it applies a feedback-revision loop (environment feedback is converted to text and re-prompted up to T times) and subtask relabeling (replace task prompt with current subtask until completion). Collected successful and partial subtask trajectories are converted into a supervised dataset and used to finetune the same LLM via QLoRA/LoRA (SFT). Action selection uses noun-first matching + embedding retrieval to map LLM outputs to available skill descriptions.",
            "base_llm_model": "LLaMA-2-70B-chat",
            "base_llm_size": "70B",
            "text_game_name": "Minecraft (MineDojo benchmark; Plan4MC tasks)",
            "text_game_description": "Open-ended sandbox environment (Minecraft) provided via MineDojo. Tasks are multi-step crafting/harvest/interaction tasks (Plan4MC's 30 difficult tasks: log-based, cobblestone-based, mob-based), long-horizon, partially observable, with compositional subtasks and a large action/skill space (55 semantic skills).",
            "uses_memory": true,
            "memory_type": "mixed: short-term working memory (recent action history) + subtask-level memory + experience (episodic) replay (supervised dataset)",
            "memory_architecture": "1) Short-term working memory implemented by including the last three actions (h_t = last 3 actions) in the prompt; 2) Subtask relabeling: when the agent is working on a subtask τ_s, the prompt's task field is replaced with τ_s and kept until τ_s is completed (this enforces focused subtask context across steps); 3) Collected experiences (successful full-task trajectories and partial subtask trajectories) are stored offline as a supervised dataset (1.3k instances) used to finetune the LLM via QLoRA/LoRA (i.e., an experience-replay style static memory used to change model weights).",
            "memory_retrieval_mechanism": "During online action selection: prompt inclusion (recent actions and current subtask are included directly in prompt). Action-space retrieval uses noun-matching followed by embedding similarity to map LLM output to a canonical skill. Offline memory (experience dataset) is accessed by finetuning (no runtime retrieval).",
            "memory_capacity": "Online short-term prompt memory: last 3 actions; Subtask label persisted until subtask completes; Offline experience memory: dataset of ~1.3k decision instances used for SFT. Exact unbounded long-term memory not used at runtime.",
            "what_is_stored_in_memory": "Last three executed skills (action history); current subtask identifier used in prompt (subtask label); collected trajectories including state-text observations, task/subtask labels, past-three-actions context, and chosen skill actions (successful full-task trajectories and partial subtask trajectories).",
            "performance_with_memory": "After finetuning on collected experiences (i.e., using the offline experience memory), LLaMA-Rider (ours) achieved an average success rate of ~34% across the 30 Plan4MC tasks and successfully accomplished 25 out of 30 tasks (reported aggregate in paper).",
            "performance_without_memory": "LLaMA-Rider Base (before finetuning, relying on only prompts + short history during exploration) had an average success rate of ~20% across the same 30 tasks and accomplished 16 out of 30 tasks. Additionally, exploration with the feedback-revision loop and CoT was essential: without feedback-revision + CoT the agent failed most tasks.",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "+14 percentage points average success-rate (from ~20% to ~34% after SFT on collected experiences) and +9 more tasks successfully achieved (from 16 to 25 out of 30).",
            "key_findings_about_memory": "1) Short prompt-based working memory (last 3 actions) plus the feedback-revision loop materially improves exploration efficiency; 2) Subtask relabeling (keeping subtask context in the prompt) substantially improves ability to complete long-horizon tasks and generalization via compositionality; 3) Storing collected successful and partial subtask trajectories and finetuning the LLM (experience replay via SFT) yields large gains in downstream task success and enables generalization to unseen harder tasks; 4) The feedback-revision mechanism is critical because it conveys environment-specific constraints to the LLM enabling efficient correction of invalid actions.",
            "memory_limitations": "Memory usage is limited: only last three actions are included in the prompt (short horizon), subtask relabeling is a simple label replacement (no structured memory graph), offline experience memory is modest (1.3k instances). Authors note limited utilization of richer environmental information and remaining retrieval inaccuracies (action retrieval via noun matching + embeddings can still produce errors). They also highlight no runtime long-term retrieval mechanism beyond prompt inclusion.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2917.0",
            "source_info": {
                "paper_title": "LLaMA-Rider: Spurring Large Language Models to Explore the Open World",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GITM",
            "name_full": "GITM (Ghost in the Minecraft)",
            "brief_description": "Prior work that integrates LLMs with text-based knowledge and memory to build generally capable agents in Minecraft; cited by this paper as an LLM-based Minecraft agent that includes memory components.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "mention_or_use": "mention",
            "agent_name": "GITM",
            "agent_description": "Mentioned in related work as an approach that integrates LLMs with text-based knowledge and memory to operate in Minecraft; the paper cites it as an example of memory-augmented LLM agents but does not describe its architecture or experiments in detail.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Minecraft (open-world MineDojo / similar)",
            "text_game_description": "Open-ended sandbox Minecraft environment; large, multi-step tasks requiring planning and environment grounding.",
            "uses_memory": true,
            "memory_type": "text-based knowledge and memory (as described in citation; details not provided here)",
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as an example of integrating text-based knowledge and memory with LLMs for Minecraft, but this paper does not provide architecture or performance details from GITM.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2917.1",
            "source_info": {
                "paper_title": "LLaMA-Rider: Spurring Large Language Models to Explore the Open World",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "rating": 2,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_enviroments_via_large_language_models_with_textbased_knowledge_and_memory"
        },
        {
            "paper_title": "REACT: Synergizing reasoning and acting in language models",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Language models meet world models: Embodied experiences enhance language models",
            "rating": 1,
            "sanitized_title": "language_models_meet_world_models_embodied_experiences_enhance_language_models"
        }
    ],
    "cost": 0.011970999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLAMA RIDER: SPURRING LARGE LANGUAGE MODELS TO EXPLORE THE OPEN WORLD
13 Oct 2023</p>
<p>Yicheng Feng 
School of Computer Science
Peking University</p>
<p>Yuxuan Wang 
School of Computer Science
Peking University</p>
<p>Jiazheng Liu 
School of Computer Science
Peking University</p>
<p>Sipeng Zheng spzheng@baai.ac.cn 
Beijing Academy of Artificial Intelligence</p>
<p>Zongqing Lu zongqing.lu@pku.edu.cn 
School of Computer Science
Peking University</p>
<p>Beijing Academy of Artificial Intelligence</p>
<p>LLAMA RIDER: SPURRING LARGE LANGUAGE MODELS TO EXPLORE THE OPEN WORLD
13 Oct 2023577BCA4C00ABE005E1F96FFCFC2C4354arXiv:2310.08922v1[cs.LG]
Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments, and try to align the LLMs' knowledge with the world conditions.Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain.In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities.In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment.This facilitates exploration and enhances the model's performance.Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experiences.By evaluation in Minecraft, an open-ended sandbox world, we demonstrate that our approach LLaMA-Rider enhances the efficiency of the LLM in exploring the environment, and effectively improves the LLM's ability to accomplish more tasks through finetuning with merely 1.3k instances of collected data, showing minimal training costs compared to the baseline using reinforcement learning.</p>
<p>INTRODUCTION</p>
<p>Recently, significant advancements and successes have been achieved in the performance of Large Language Models (LLMs) in attaining human-like intelligence (OpenAI, 2023).Given the powerful capability of LLMs, many research works have started utilizing their abilities to assist intelligent agents in decision-making in the environments (Yao et al., 2023;Huang et al., 2022a;Li et al., 2022;Singh et al., 2023), and have found that LLMs possess a certain level of abilities for planning and accomplishing various tasks (Wang et al., 2023b).However, the knowledge that LLMs rely on comes from the language corpus used during pre-training, and there may be discrepancies between this knowledge and specific environments (Ahn et al., 2022).</p>
<p>To ground LLMs to environments, some studies design specific mechanisms through prompt engineering to provide information from environments for LLMs (Wang et al., 2023c;Yao et al., 2023;Wu et al., 2023;Zhu et al., 2023;Liu et al., 2022).However, LLMs do not improve or acquire new knowledge in environments.Additionally, for more complex tasks, more complicated mechanisms and prompts are required, Preprint which results in high costs of LLM generation and reliance on strong models like GPT-4 (OpenAI, 2023) with enough knowledge (Wang et al., 2023a).Some other studies ground LLMs with finetuning (Yao et al., 2022;Deng et al., 2023;Xiang et al., 2023), but they usually require task-dependent datasets.Reinforcement Learning (RL) methods are also studied in the literature (Carta et al., 2023), but these methods train LLMs as task-specific policies, and we found that RL methods are difficult to scale up to larger models or more complex tasks (see Section 5.2.2).</p>
<p>In this paper, we aim to enhance LLMs through their exploration in open-ended environments (Figure 1), like humans can adapt to new situations through practice.Previous studies have tried to update LLMs in embodied environments like BabyAI (Chevalier-Boisvert et al., 2019) and Virtu-alHome (Puig et al., 2018), but these world sizes are rather limited.Whether LLMs can improve their knowledge in more complicated open-ended worlds like Minecraft is still unknown (Fan et al., 2022;Guss et al., 2019).We think there are two major challenges here.First, in an environment like Minecraft, tasks are often complex and may involve many sub-tasks.At the same time, these long-horizon tasks often require each step to be carried out precisely, and a single error in the middle sometimes can negate previous progress.Besides, due to the high level of freedom, the action space can be large, while many actions may be invalid in different states.These reasons make it hard to collect successful task trajectories in the environment using random exploration as in previous works (Xiang et al., 2023;Li et al., 2022).The second challenge is that there can be a significant amount of tasks in such an open world, so training policies for specific tasks are not applicable in these environments.We hope that LLMs have the ability to perform multiple tasks and generalize to new tasks.</p>
<p>In response to these challenges, we propose LLaMA-Rider , a two-stage learning framework consisting of an exploration stage and a learning stage (Figure 2).We investigate how to spur LLMs to explore the environment themselves and collect successful experiences for learning.Compared to random exploration or search methods that can hardly work in complex environments, allowing LLMs to explore on their own in the environment can harness the inherent capabilities of the models, thereby enabling more effective discovery of successful experiences.We propose a multi-round feedback mechanism, which allows the LLM to revise its decisions by providing information about failed actions in the environment.This feedback-revision exploration mechanism is more efficient due to the capability of LLMs, as the draft decisions made are often related to task completion at first, and LLMs can effectively understand feedback information.Additionally, we use sub-task relabeling to help LLMs maintain consistency in sub-task planning.</p>
<p>In the learning stage, we process the collected experiences into datasets and use supervised finetuning (SFT) to train the LLM.In addition to the experience gained from successful tasks, we also collect experiences from partially completed sub-tasks, as some tasks are too difficult to accomplish in the environment in the exploration stage.Numerous tasks in open-ended environments often have compositionality, which means experiences from past tasks can frequently assist in completing other tasks.We propose to use sub-task relabeling of the collected experiences to improve data utilization while helping LLMs learn the compositionality between tasks.</p>
<p>We evaluate our method in MineDojo (Fan et al., 2022), a simulation platform for Minecraft.We use the basic skills trained by Plan4MC (Yuan et al., 2023) as the action space since the skills possess more semantics compared with primitive actions and are better aligned with LLMs.We use LLaMA-2-70B-chat (Touvron et al., 2023) in our experiments.Our experiments show that LLaMA-Rider can explore the environment efficiently with our feedback-revision mechanism, and can learn to complete tasks more effectively by finetuning on a collected dataset of only 1.3k in size, demonstrating much higher sample efficiency compared to RL methods.We also show the generalization ability of LLaMA-Rider in novel hard tasks.</p>
<p>RELATED WORK</p>
<p>LLM-BASED AGENTS</p>
<p>There is a large body of recent studies on LLM-based agents, which have delved into the capacities of LLMs for decision-making and are well summarized in the survey papers (Wang et al., 2023b;Xi et al., 2023).There are basically three ways to integrate LLMs into decision-making problems.First, using the code generation capabilities of LLMs, LLMs take in information from the environment and produce code that can interact directly within the environment (Liang et al., 2023;Singh et al., 2023).The second way is to employ LLMs for planning, following a concept similar to hierarchical RL (Ahn et al., 2022;Huang et al., 2022b;Wang et al., 2023c;Dasgupta et al., 2023).The third approach involves continually prompting LLMs or introducing memory modules to generate outputs that can execute better strategies directly within a textual environment (Wei et al., 2022;Yao et al., 2023;Kim et al., 2023).</p>
<p>Minecraft, as a popular and challenging open-world benchmark, has also attracted substantial attention for the studies of LLM-based agents.DEPS (Wang et al., 2023c) introduces the descriptor, explainer, and selector for plan generation with the help of LLM.Plan4MC (Yuan et al., 2023) constructs a skill graph with the help of LLM and proposes a skill search algorithm for planning over the basic skills pretrained by reinforcement learning (RL).Moreover, to build LLM-based agents in Minecraft, Voyager (Wang et al., 2023a) leverages the code generation of LLMs, while GITM (Zhu et al., 2023) integrates LLMs with texted-based knowledge and memory.However, in the aforementioned studies, LLMs do not update themselves from their interactions with the environment, so they can neither learn from nor adapt to the environment.Consequently, their potential applicability in specific environments is limited, as they can solely depend on the knowledge and capabilities gained during pre-training.</p>
<p>FINETUNING LANGUAGE MODELS IN ENVIRONMENTS</p>
<p>There are studies that ground Language Models (LMs) to environments with finetuning.PIGLeT (Zellers et al., 2021) integrates a neural symbolic dynamics model with an LM to learn natural language meaning grounded in physical interactions.Also focusing on the decision-making of LMs in embodied environments, LID (Li et al., 2022) uses expert trajectories to finetune a model that concatenates an LM with action decoders.They also propose active data gathering to collect experiences that mix random actions and policy-generated actions for exploration.Similarly, E2WM (Xiang et al., 2023) uses supervised learning to finetune LMs with the data collected by Monte Carlo Tree Search and random exploration.Additionally, GLAM (Carta et al., 2023) ground LMs in environments with online RL, but they train the LM into a task-specific policy, and the RL method suffers from low sample efficiency and high cost of training.Our work is different from existing work in that we spur the LLM itself to explore with feedback from the environment, and we target multi-task and generalization abilities in the open world.</p>
<p>PRELIMINARIES</p>
<p>LARGE LANGUAGE MODELS</p>
<p>LMs, which predict the probability of the ith token given inputs and the previously generated tokens P i = P (s i |inputs, s 1 , s 2 , • • • , s i−1 ), are used to generate a series of tokens by sampling from the probability of the token sequences P (x) = Π n i=1 P i , where x can be considered as a random variable representing n tokens in the token library.LLMs often have billions of weights and are trained from billions of tokens to enable them to achieve remarkable performance on generative tasks.</p>
<p>To finetune LLMs with full parameters requires remarkable compute resources.Fortunately, some techniques can help with efficient finetuning.Low-Rank Adaptation (LoRA) (Hu et al., 2022) involves the process of keeping the pretrained model weights fixed while introducing trainable rank decomposition matrices into every layer of LLMs.Original pretrained weights W 0 ∈ R d×k are augmented to W 0 + ∆W = W 0 + BA, where B ∈ R d×r and A ∈ R r×k .The matrices A and B are both trainable, with A initialized to a normal distribution and B initialized to zero.Moreover, QLoRA (Dettmers et al., 2023) adds quantization and paged optimizers to further reduce training compute costs.Quantization aims to transform input from a high-information representation into a low-information representation, such as converting FP32 to int8 to reduce memory usage.</p>
<p>PROBLEM STATEMENT</p>
<p>We consider an environment that can be formalized as a Partially Observable Markov Decision Process (POMDP) defined by tuple M = (S, O, A, T , R, γ), where S is the environment state, .The framework consists of two stages.In the exploration stage, the LLM explores to accomplish tasks with the help of the feedback-revision mechanism and subtask relabeling.In the learning stage, the collected trajectories are formatted into a supervised dataset to finetune the LLM.</p>
<p>A is the action space, O is the observation space, T is the transition function, R is the reward function, and γ is the discount factor.Since we use LLMs as embodied agents, we assume a language vocabulary V and we can encode the observations and actions from the environment into natural language.Besides, we assume a goal space G and we can sample a task τ = (g, K), g ∈ G, where g is the goal of the task and K is the task information including task-relevant knowledge.We can also encode the task τ into task description τ text ∈ V N .</p>
<p>In this study, we explore the Minecraft simulator provided by MineDojo (Fan et al., 2022), which is an open-ended sandbox world.There is rich information in the observation space, but a big portion of it cannot be comprehended by LLMs such as game visuals.We extract the items in the agent's inventory and field of view, along with their quantities, and encode them into natural language sentences as the observations for LLMs: o text = (inv, f ov) ∈ V N .Primitive actions in the environment (e.g., move forward, turn right, click) have insufficient semantics which hampers the planning capability of LLMs.We use skill descriptions as the action space of the LLM agent noted with a text ∈ V N .</p>
<p>SKILLS AND TASKS IN PLAN4MC</p>
<p>We use the basic skills and tasks in Plan4MC (Yuan et al., 2023) in our experiments in MineDojo, since the basic skills have more semantic meaning than primitive actions.Plan4MC uses RL to train three types of basic skills: finding-skills, manipulation-skills, and crafting-skills.They then define 40 difficult tasks that can be completed with the trained skills.We define the action space of the LLM agent A text as the descriptions of these basic skills.</p>
<p>METHODOLOGY</p>
<p>Our method is illustrated in Figure 2, which is a two-stage framework.We introduce the exploration stage and the learning stage respectively in the following.</p>
<p>EXPLORATION WITH FEEDBACK</p>
<p>Prompt mechanism.Unlike previous studies such as Voyager (Wang et al., 2023a) and GITM (Zhu et al., 2023) which use complex prompts to tweak LLMs to accomplish various tasks in open-ended worlds like Minecraft, our approach employs a straightforward prompt that makes LLMs provide the next action given input information about observation and task.This brings two advantages.First, it makes finetuning LLMs to learn from past experiences easy, considering the context-length limit of LLMs.Second, it reduces the cost of LLM generation.</p>
<p>Formally, the LLM serves as the policy π(a text t |o text t , τ text , h t ).We provide the textual observation o text , the task description τ text and the history information h in the input prompt to feed the LLM at each time step t, and the output of the LLM is the chosen action a text .We find that if there are too Preprint Algorithm 1. Feedback-revision
Require: o text t , τ text , h t , π LLM , E, T Ensure: a text t 1: a text t ∼ π LLM (•|o text t , τ text , h t ) 2: f t = E(s t , a t ) 3: for i = 0 to T do 4: if f t = 0 then 5: return a text t 6: end if 7: f t → f text t 8: a text t ∼ π LLM (•|o text t , τ text , h t , f text t ) 9: f t = E(s t , a t ) 10: end for 11: if f t = 0 then 12:
return a text t 13: end if 14: return 0 many tokens of history information in the prompt, it will affect the output of the LLM.Therefore, in our experiments, we set h to be the last three actions performed h t = (a text t−3 , a text t−2 , a text t−1 ).Feedback-revision mechanism.LLMs possess rich knowledge of the real world, but there is often a gap between the knowledge of LLMs and the specific environment to which they are applied.For example, which actions can be performed in the environment?What are the prerequisites for each action before execution?What conditions need to be satisfied for the completion of different tasks in the environment?What are the names of various items in the environment?LLMs often lack understanding of these questions, leading to decision-making errors.Previous studies ground LLMs to environments by searching through the action space (Xiang et al., 2023) or mix policy with random actions (Li et al., 2022) to collect experiences, or train LLMs with reinforcement learning (Carta et al., 2023).But these methods can hardly scale up to worlds with long-horizon tasks.They all do not provide environmental knowledge to LLMs but make LLMs explore through trial and error.We propose to spur LLMs to explore the world themselves with their reasoning capabilities by feeding them environmental feedback information and letting LLMs revise their decisions.LLMs can access environmental knowledge during this process, and the method makes use of LLMs' inherent ability to enhance the efficiency of exploration.</p>
<p>Formally, after the LLM produces an action a text t ∼ π(•|o text t , τ text , h t ), a feedback information is generated by the environment f t = E(s t , a t ), where E denotes the environment, s t denotes the state, and a t denotes the primitive actions corresponding to a text t .If f t ̸ = 0, which means the action causes an error, the feedback is processed by a prompt into f text t and fed back to the LLM together with the previous input information, and the LLM would make a revision to produce a new action
a text ′ t ∼ π(•|o text t , τ text , h t , f text t ). Then a new feedback is generated f t = E(s t , a ′ t )
. This feedback-revision procedure can be repeated until f t = 0 or the maximum number of allowed revisions T has been reached which means the exploration has failed and the episode ends.The formalized approach of the feedback-revision mechanism can be seen in Algorithm 1.</p>
<p>Subtask relabeling.Long-horizon tasks in an open world are often composed of many subtasks.Since our input prompt is brief, limited information is provided.So the LLM planner may forget what subtask it is currently working on and opt to start completing other subtasks, resulting in failure to consistently complete one subtask.To solve this problem, whenever the LLM's output skill is accomplishing a subtask τ s of the task τ , we replace the task information τ text in the input prompt with τ text s and keep it until τ s is completed.This subtask relabeling provides another important benefit: some subtasks may have been met in the collected experiences as a simpler task or as a subtask of another task, so this method helps LLMs make use of previously learned experiences to solve new tasks.</p>
<p>Action retrieval.To match the output of the LLM with the action space, there are two major ways: feed the action list to the LLM or retrieve the action list based on the output.We find that feeding a lengthy list of actions as input to the LLM would affect its output to generate more unreasonable actions unrelated to the current task.Therefore, we use action retrieval to select an action from Preprint the action space that is closest to the output of the LLM.Additionally, we find that querying with token embeddings could cause retrieval errors since the action description often consists of only a few words, e.g., "craft wooden planks" may be matched to "craft wooden sword" instead of "craft planks".We propose to use noun matching before embedding matching to alleviate this problem.Details of action retrieval can be found in Appendix C.</p>
<p>Chain-of-thought (CoT) prompting.In our experiments in Minecraft, we find that the LLM often makes decision mistakes due to insensitivity to the relationships between numbers.To enhance the efficiency of exploration, we integrate in-context learning and chain-of-thought prompting (Wei et al., 2022) that make the LLM compare the item numbers in the inventory and the requirements before making decisions.The prompt can be seen in Appendix B.3, and we only use it in the exploration stage for Minecraft.</p>
<p>FINETUNING LLMS WITH EXPERIENCES</p>
<p>Dataset construction.We compile task experiences of all tasks collected by the LLM from the environment into a supervised dataset, with the input be the task information and the observation x = (o text t , τ text , h t ), and the label be the action y = a text t .In addition to success trajectories, we also include partial trajectories where a subtask is completed, since some tasks are too hard to accomplish during exploration, and the subtask experience may help the LLM to accomplish the whole task more easily.Besides, subtask experiences may also help the LLM solve some other tasks due to the compositionality.To better make use of the subtask information and encourage combinatorial generalization, we also use subtask relabeling to construct the dataset.Namely, if the LLM is solving a subtask τ s of task τ at time step t in a trajectory, we add the data
(x = (o text t , τ text s , h t ), y = a text t ) into the dataset.
Training.With the dataset including experiences of various tasks in the environment, we train the LLM with supervised finetuning (SFT).We use QLoRA (Dettmers et al., 2023) to reduce memory usage, and more details can be found in Appendix A.</p>
<p>EXPERIMENTS</p>
<p>EXPERIMENTAL SETUP</p>
<p>MineDojo environment.We evaluate our proposed method on Minecraft based on the MineDojo (Fan et al., 2022) simulator.We use 30 difficult tasks in Plan4MC (Yuan et al., 2023) including three types: 10 log-based tasks, 10 cobblestone-based tasks, and 10 mob-based tasks.The minimum number of planning steps provided by Plan4MC required for these tasks ranges from 2 to 30, with an average minimum of 11.5 steps.More details about the tasks can be found in Appendix D. We use 55 basic skills trained by Plan4MC and convert them to skill descriptions in natural language as the action space of the LLM.Note that the skill policies do not guarantee success, and the success rates of all the skills are provided in Appendix D. For each task τ = (g, K), the goal g is the target item of the task and the knowledge K is the requirement to achieve target g in MineDojo.The feedback information f t from the environment is the requirements that are not met to execute skill a t in MineDojo.The prompt template for the LLM's input and the feedback can be found in Appendix B.</p>
<p>We define the subtasks of a task τ as the tasks τ s = (g s , K s ) whose goal g s is one of the requirements to achieve task τ .For example, the task "craft bowl" has two subtasks "craft planks" and "place crafting table nearby".Note that some subtasks are simple so are not among the 30 difficult tasks for evaluation.</p>
<p>LLM agent.We use LLaMA-2-70B-chat (Touvron et al., 2023) as our LLM agent, which was recently released and has strong question-answering and instruction-following abilities.These abilities are important for the LLM to actively explore in the environment, and conversely, our method can also effectively make good use of its strong abilities to do something beyond question answering, namely exploring new environments.</p>
<p>Baselines.We compare with three baselines in our experiments.The first is ChatGPT planner (Ouyang et al., 2022), the interactive LLM baseline in Plan4MC, which uses a carefully designed Table 1.Success rates in all tasks.LLaMA-Rider Exploration is tested for 5 episodes in log-based tasks and 10 episodes in other tasks.All other methods are tested for 30 episodes.Results for ChatGPT planner and Plan4MC are from the report of Plan4MC (Yuan et al., 2023 prompt mechanism to make ChatGPT (GPT-3.5)propose skill plans.This baseline also uses the LLM to choose skills trained in Plan4MC for accomplishing tasks in Minecraft.Since ChatGPT possesses more accurate knowledge about Minecraft than LLaMA-2-70B-chat, by comparing with this baseline, we show whether our exploration-learning framework can enable an LLM to adapt to a new environment and outperform a stronger language model.The second is RL where we use the training framework proposed in GLAM (Carta et al., 2023) and use their default language model T5 (Chung et al., 2022).We try our best to fit GLAM into the Minedojo environment but we have to constrain the action space to include only the necessary actions to reduce sample complexity.The detailed implementation is described in the Appendix E. The third is Plan4MC, where they construct a skill graph and use depth-first search (DFS) for planning over basic skills.This baseline Preprint ensures that the planning is correct.Thus, it can be seen as an upper bound of our method.However, we note that our method may outperform Plan4MC in some tasks.We speculate this is because Plan4MC does not always generate the optimal plan in terms of planning steps, though the plan is correct.</p>
<p>EVALUATION</p>
<p>We set the maximum number of revisions as T = 5 for which we find can best balance the efficiency and success rate of the LLM's exploration for all tasks.Since the log-based tasks are easier, we only perform 5 episodes of exploration, where we make the LLaMA-Rider explore for 10 episodes for the rest 20 tasks, so that the experience collected from different tasks be in similar quantities.For the task "craft stick " and "place crafting table</p>
<p>nearby", we change the biome to forest in the exploration stage to improve the chance of finding logs .The results are shown in Table 1.</p>
<p>EXPLORATION OF LLAMA-RIDER IN MINECRAFT</p>
<p>LLaMA-Rider Exploration shows the LLM's ability to explore in Minecraft to accomplish different tasks with our designed prompt combined with the feedback-revision mechanism.Compared with ChatGPT planner which is based on a powerful LLM with more Minecraft knowledge (see Appendix F), LLaMA-Rider Exploration can obtain successful experiences more effectively without finetuning in log-based tasks and has comparable performance in the other tasks.This can be attributed to our feedback-revision mechanism, which provides more environment information for the LLM to acquire knowledge alignment, and the CoT prompt that mitigates the LLM's numerical comparison issue.Besides, the success rates in stone-based tasks and mob-based tasks demonstrate that it is difficult for LLMs to solve long-horizon complex tasks in environments just rely on prompt engineering, reflecting the importance for LLMs to update with environmental experiences to adapt.</p>
<p>ENHANCING LLM WITH ENVIRONMENTAL EXPERIENCES</p>
<p>Performance in explored tasks.We collect trajectories that the LLM achieves success in the whole tasks or subtasks and process them into a supervised dataset of 1.3k instances as described in Section 4.2.We train LLaMA-2-70B-chat on the dataset for two epochs, and then test the resulting model LLaMA-Rider on 30 tasks without CoT prompting.From the results in Table 1, the trained LLaMA-Rider outperforms the base model on various tasks, so the learning stage is effective.Besides, LLaMA-Rider outperforms ChatGPT planner in 17 out of 30 tasks, demonstrating that our exploration-learning framework allows an LLM to quickly adapt to a new environment and surpass a more advanced LLM, even with a simple prompt mechanism.</p>
<p>Compared with the performance in the exploration stage, LLaMA-Rider can accomplish more tasks (25 vs. 16) after training, proving that the model can learn the knowledge from the experiences effectively and generalize well, while also reflecting the necessity of allowing LLMs to update themselves in the environment.Without the help of CoT prompting at test time, LLaMA-Rider can still perform better, which reflects that the model acquires stronger decision-making abilities.The phenomenon that LLaMA-Rider can achieve success in tasks without successful experiences in the dataset like "craft sign " and "craft wooden shovel " proves that the model is not memorizing experiences but learning more knowledge for planning.Besides, as we show in Appendix F, LLaMA-Rider can also answer task-relevant questions better, so the model is indeed aligning with the environment.The generalization ability is probably also due to our subtask relabeling method which helps LLaMA-Rider learn compositionality among different tasks.Besides, compared with Plan4MC, our method can achieve comparable performance in several tasks and even better performance in relatively simpler log-based tasks, showing that LLaMA-Rider already demonstrates strong abilities in planning and decision-making.</p>
<p>On the other hand, RL, which also finetunes the LLM in the environment, fails in all log-based tasks.Thus, we do not conduct experiments in the rest tasks to save resources.We find that the LLM struggles to explore the world with trial and error in long-horizon tasks with a large action space.In addition to small models like T5-base, which we think may have limited decision-making abilities in the complex environment, we have also tried to train LLaMA-2-70B-chat with reinforcement learning, but we found the training unaffordable.So the RL method is difficult to scale up.In Preprint contrast, our method only requires the LLM to explore for 5 or 10 episodes in the environment and trains the LLM on a small dataset with just 1.3k instances, showing significantly lower cost and higher sample efficiency.</p>
<p>Overall, we conclude that our method LLaMA-Rider adapts to the environment efficiently and effectively and shows good multi-task ability in the open-world Minecraft.</p>
<p>Table 2. Success rates in novel iron-based tasks.Methods are tested for 30 episodes.LLaMA-Rider Base is LLaMA-Rider before finetuning.</p>
<p>Tasks</p>
<p>LLaMA-Rider Base 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 LLaMA-Rider (ours) 0.13 0.00 0.00 0.00 0.00 0.00 0.07 0.03 0.00 0.00 Generalization to novel hard tasks.Since LLaMA-Rider can complete tasks without successful experiences at training time, we also test its performance on novel tasks that it has not explored and not been trained on.We conduct the experiment on 10 iron-based tasks, which are more difficult than the previous 30 tasks with the planning steps of Plan4MC ranging from 30 to 121, 68.9 on average.The results are shown in Table 2.</p>
<p>We find that LLaMA-Rider has very poor performance before training.But after finetuned with the experiences in the previous 30 tasks, LLaMA-Rider can now achieve 3 of them.This shows that the LLM can learn to make use of past experiences to solve novel tasks that have not been explored, which demonstrates the generalization of the planning ability learned by our method.Additionally, since the experiences can help LLaMA-Rider solve more complex tasks, it is promising that LLaMA-Rider can repeat the exploration and learning procedure and explore for more challenging tasks continuously in the open world.</p>
<p>ABLATION STUDY</p>
<p>We first test the LLaMA-Rider's performance in the exploration stage without CoT prompting and feedback-revision mechanism in the 30 tasks.We find that LLaMA-Rider can only achieve success in "craft stick " with a success rate of 0.5 and fails in all other tasks (thus omitted in Table 1).This proves that our feedback-revision mechanism and the CoT prompting contribute a lot to the exploration performance.Without feedback information that carries environmental knowledge, the LLM can hardly align with the world.</p>
<p>Table 3. Success rates in stone-based tasks.Methods are tested for 30 episodes.LLaMA-Rider w/o subtask is the method without subtask relabeling at training and testing time.</p>
<p>Tasks</p>
<p>LLaMA-Rider w/o subtask 0.00 0.00 0.00 0.00 0.00 0.30 0.00 0.03 0.03 0.07</p>
<p>LLaMA-Rider (ours) 0.17 0.57 0.40 0.10 0.00 0.07 0.03 0.03 0.00 0.07</p>
<p>Then we study the contribution of our subtask relabeling.We train LLaMA-2-70B-chat with a dataset without the subtask relabeled data.At test time we also do not use subtask relabeling.We test on 10 stone-based tasks, since these tasks are more long-horizon and contain more subtasks.The results are shown in Table 3.The model performs poor in the long-horizon stone-based tasks without subtask relabeling method, while LLaMA-Rider can achieve even more tasks than those in training experiences, proving that subtask relabeling is important for both the achievement (and thus the exploration) of tasks and the generalization ability to new tasks.</p>
<p>CONCLUSION AND LIMITATIONS</p>
<p>In this paper, we introduce LLaMA-Rider, which is a learning framework that spurs the LLM to explore the open world with the feedback-revision mechanism and then use the collected experiences to update itself for task planning.We also propose to use subtask relabeling for long-horizon tasks.</p>
<p>Preprint</p>
<p>Our experiments in the open world Minecraft show the effectiveness and efficiency of our method which helps the LLM to adapt to the embodied environment and improve the capability to solve multiple tasks.We also find that LLaMA-Rider can use past experiences to solve novel hard tasks, showing a life-long exploration and learning potential.</p>
<p>Though we use Minecraft as our testbed in the experiments, LLaMA-Rider is a general learning framework that can be applied to other open worlds.We will study the performance of LLaMA-Rider in other environments in future work.</p>
<p>One limitation of this method is its relatively insufficient utilization of environmental information.Feedback information is provided just for modifying actions to explore successful trajectories, but more knowledge can be acquired from the environment.In future work, we will investigate how to integrate knowledge gained through exploration for updating the LLM.</p>
<p>A TRAINING DETAILS</p>
<p>We perform supervised finetuing (SFT) on LLaMA-2-70B-chat with our collected dataset with QLoRA (Dettmers et al., 2023).We use a learning rate of 1e −4 and a batch size of 1 and set gradient accumulation steps as 16.We set LoRA R dimension to 64 and LoRA alpha to 16, and we use 0.05 LoRA dropout.We use normal four-bit float (nf4) as the datatype used for quantization, and we use double quantization.We use paged optimizers.Training is conducted on 4 NVIDIA Tesla A100 GPUs.</p>
<p>B PROMPT DESIGN</p>
<p>C ACTION RETRIEVAL</p>
<p>To match the output of the LLM with the action space, we use an action retrieval mechanism to select an action from the action space that is closest to the output of the LLM.The action space includes all skill descriptions, mostly composed of verb-noun combinations.</p>
<p>A straightforward idea is to compare the embedding of the LLM's output with those of all skill descriptions.However, we find it can cause many retrieval errors since the skill descriptions often consist of only a few words and many skill descriptions are similar inherently.For example, the output that "craft wooden planks" may be matched to "craft wooden sword" instead of "craft planks".</p>
<p>Therefore, for our experiments, we propose to use noun matching before embedding matching to alleviate this problem, since the quantity of verbs is much less than that of nouns.Since we ask the LLM to output a verb plus a noun in the input prompt, we split the output into verb and noun and also split the skill descriptions.Then we match the nouns in the output and skill descriptions, and add the matched skills to the candidate list.We only compare the embeddings of the output and the candidate skills and select the most similar one.</p>
<p>Preprint</p>
<p>Besides, since the nouns generated by the language model will include different vocabularies that have similar meanings, we also match these nouns, such as 'wood' and 'log'.</p>
<p>The method alleviates the retrieval problems of the short actions, but can still not guarantee the accuracy of the retrieval.We may explore better methods in the future.</p>
<p>D TASK AND SKILL DETAILS IN MINECRAFT</p>
<p>In this section, we provide details about tasks and basic skills in Plan4MC used in our experiments.</p>
<p>We keep the task setup the same as Plan4MC, where in each episode the agent is randomly transported with a maximum distance of 500, and the mobs are spawned with a maximum distance of 30.We list the information of the trained basic skill policies provided in the paper of Plan4MC in Table 7.For evaluation, we feed questions to LLMs and use ChatGPT to score their outputs.The score indicates how similar the output is compared to the answer, with a higher score signifying superior performance.The results are shown in Table 8.We can find the gpt-3.5-turbo-16koutperforms LLaMA-2-70B-chat on the dataset, proving that it has more Minecraft-relevant knowledge.</p>
<p>Besides, to examine whether LLaMA-Rider can acquire more environmental knowledge other than planning capabilities after learning from exploration, we identify some questions related to the training task from the knowledge dataset and conduct a case study.The results are shown below.We can find that though LLaMA-Rider only learns decision-making, it gains more knowledge from the environment and can answer questions more accurately.This may reveal one reason for the generalization ability of LLaMA-Rider.</p>
<p>Question</p>
<p>In Minecraft, What drops do dairy cows provide when killed?Answer 0-2 Leather and 1-3 Raw Beef (Cooked Beef if killed while on fire)</p>
<p>LLaMA-2-70B-chat Dairy cows provide 3-5 milk buckets when killed.</p>
<p>LLaMA-Rider</p>
<p>The answer is 'beef'.</p>
<p>Question</p>
<p>In Minecraft, Can a bowl be crafted with only one plank?</p>
<p>Answer</p>
<p>No, three planks are required to craft a bowl.</p>
<p>LLaMA-2-70B-chat Yes.</p>
<p>LLaMA-Rider</p>
<p>A bowl cannot be crafted with only one plank in Minecraft.It requires three planks arranged in a bowl shape to craft a bowl.In Minecraft, Can a bowl be crafted with only one plank?</p>
<p>LLaMA-Rider</p>
<p>The answer is no.Cows do not drop raw meat, they drop leather and beef.</p>
<p>Figure 1 .
1
Figure 1.Spurring LLaMA to explore the open world.</p>
<p>Task Name: craft crafting table Inventory: 2 planks; 4 sticks Surround: 1 log nearby Past skills: craft planks; craft stick; find log nearby Require: 4 planks need 4 planks to craft table "get planks" Feedback Revision "find logs" need 1 log to craft planks Task Name: craft wooden pickaxe Inventory: 2 planks; 4 sticks Surround: Nothing Past skills: harvest
log; craft planks;craft stickRequire:3 planks, 2 sticks, 1crafting table nearby</p>
<dl>
<dt>Subtask Relabeling Task Success planks: need 1; sticks: satisfied crafting table nearby: need 1 "get crafting table" LLaMA-Rider MC MC LLaMA-Rider LLaMA-Rider AR AR AR t-1 t … … Exploration Stage Env Execution Action Retrieval Minecraft Feedback Task</dt>
<dd>
<p>craft wooden pickaxe Info: … Next skill: find log nearby Supervised Dataset Task: craft crafting table Info: … Next skill: find log nearby</p>
</dd>
</dl>
<p>Subtask Relabeling Learning Stage COT craft crafting table craft planks find log nearby MC AR COT Chain of Thought …</p>
<p>finetuning Figure 2. Overview of LLaMA-Rider</p>
<p>). LLaMA-Rider Base is LLaMA-Rider before finetuning.The bold results are the best among LLaMA-Rider, LLaMA-Rider Base, ChatGPT planner and RL.We do not compare with LLaMA-Rider Exploration due to the different test episode numbers.
TaskLLaMA-Rider ExplorationLLaMA-Rider BaseChatGPT plannerRLLLaMA-Rider (ours)Plan4MC0.900.230.300.000.430.301.000.370.170.000.670.300.800.730.070.000.970.470.600.670.000.000.770.230.600.570.030.000.570.370.000.670.000.000.600.430.800.00.200.000.370.530.600.770.470.000.600.370.800.070.630.000.100.470.000.030.730.000.270.700.400.000.00-0.170.370.100.000.20-0.570.470.100.000.03-0.400.530.200.000.13-0.100.570.000.000.00-0.000.370.000.130.00-0.070.100.000.000.00-0.030.170.000.000.07-0.030.070.000.000.13-0.000.100.100.000.10-0.070.200.700.600.57-0.600.830.300.500.76-0.570.530.000.100.00-0.030.170.000.100.00-0.070.130.300.500.37-0.430.370.000.000.00-0.000.070.000.030.43-0.030.430.000.000.03-0.000.200.000.000.30-0.030.330.000.000.00-0.000.13based0.610.410.260.000.540.42based0.090.010.07-0.140.30based0.130.180.25-0.180.32Total average0.280.200.19-0.290.34Achieved tasks #161620-2530</p>
<p>Your goal is to complete a task in Minecraft.Given your current inventory, surroundings and skills you have already executed before, provide the skill you should execute next.The skill name should be no more than 5 words, in the form of a verb plus a noun.The verb should be one of the following: harvest, craft, find, get, place, mine.Here's the feedback from the environment: Your inventory or surroundings does not meet the requirements to perform the skill {{retrieved skill}} Speculated reason: {{feedback information}} Based on the information, please output the next skill you need to do.Your goal is to complete a task in Minecraft.Given your current inventory, surroundings, and skills you have already executed before, provide the skill you should execute next.Last three skills you have just already executed: {{past skills}} Recipe: The requirements to {{task}} in Minecraft is: {{requirement}} Your output:
PreprintKeyExampletaskcraft wooden pickaxeinventory4.0 plankssurrounding 1.0 log nearbyrequirement 3 planks, 2 stick, 1 crafting table nearbyB.4 SFT DATA FORMATFor the collected trajectories, we process each decision step into a supervised data instance as fol-lows.Input Template:B.1 DECISION-MAKING PROMPTTemplate:Now the information:Task: {{task}}Inventory: {{inventory}}Please provide your output in the following format: Surroundings: {{surrounding}}Next skill: skill nameNow the information:Task: {{task}} Inventory: {{inventory}} Output Template:Surroundings: {{surrounding}}Last three skills you have just already executed: {{past skills}} Next skill: {{skill name}}Recipe: The requirements to {{task}} in Minecraft is: {{requirement}}Your output:KeyExampleKey task inventorycraft wooden pickaxe Example 4.0 plankstask surrounding 1.0 log nearby craft wooden pickaxeinventory past skills4.0 planks harvest log; craft planks; find log nearbysurrounding 1.0 log nearby requirement 3 planks, 2 stick, 1 crafting table nearbypast skills skill nameharvest log; craft planks; find log nearby harvest logrequirement 3 planks, 2 stick, 1 crafting table nearbyB.2 FEEDBACK-REVISION PROMPTTemplate:...Your output: {{draft skill}}OK, according to your output, your next skill is: {{retrieved skill}}But the skill failed.Please find out the reason why the skill failed, and make a revision.Here's your inventory: {{inventory}}Here's your surroundings: {{surrounding}}Revised skill:</p>
<p>Table 4 .
4
Settings for log-based tasks at test time.Max steps refers to maximum environmental steps.
Task iconTask descriptionBiome Max stepscraft stickplains3000place crafting table nearby plains3000craft bowlforest3000craft chestforest3000craft trapdoorforest3000craft signforest3000craft wooden pickaxeforest3000craft wooden axeforest3000craft wooden swordforest3000craft wooden shovelforest3000</p>
<p>Table 5 .
5
Settings for stone-based tasks and mob-based tasks at test time.Initial tools are provided in the agent's inventory at task beginning.Max steps refers to maximum environmental steps..5-turbo-16k to generate question-answer pairs with short and precise answers based on the collected data.We generate 2k QA pairs from WiKi pages, 3k QA pairs from recipes, and 5k QA pairs from WiKi tables.
Task iconTask descriptionInitial toolsBiomeMax stepsget furnace nearby<em>10extreme hills5000craft stone stairs</em>10extreme hills5000craft stone slab<em>10extreme hills3000craft cobblestone wall</em>10extreme hills5000craft torch<em>10extreme hills5000craft lever</em>1forest hills5000craft stone pickaxe<em>1forest hills10000craft stone axe</em>1forest hills10000craft stone sword<em>1forest hills10000craft stone shovel</em>1forest hills10000harvest milk<em>1, </em>3plains3000harvest wool<em>1, </em>2plains3000craft bed<em>1, </em>1plains10000craft painting<em>1, </em>1plains10000craft carpet<em>1plains3000craft item frame</em>1, <em>1plains10000harvest beef</em>1plains3000harvest cooked beef<em>1, </em>1plains10000harvest mutton<em>1plains3000harvest cooked mutton</em>1, *1plains10000</p>
<p>Table 8 .
8
Minecraft knowledge test for gpt-3.5-turbo-16kand LLaMA-2-70B-chat.
ModelWiki Page Recipe Wiki Table Averagegpt-3.5-turbo-16k7.267.977.157.42LLaMA-2-70B-chat 6.917.236.977.04</p>
<p>If you answer correctly, you get 20 tokens.You can use these tokens to redeem rewards.If you answer incorrectly, you lose 4 tokens.You have 5 lives.Go ahead and answer now.(Note: Please answer only 'yes' or 'no'.)
QuestionIn Minecraft, Do cows drop raw meat in Minecraft?AnswerYes.LLaMA-2-70B-chat
We mostly retain the content in Appendix B.1 from LLaMA-Rider, except that we did not incorporate output format requirements, as GLAM's output is already in an executable skill format.E.2 TRAINING DETAILSWe used T5-base(Chung et al., 2022)as our base model.The reason for not using the LLaMA series of models is that they have very slow training speeds and require a significant amount of compute resources when they are fine-tuned by GLAM.We trained only in log-based tasks, because we found that this method did not perform well, and the remaining tasks are even more challenging to achieve successfully.The episode length for one trajectory we set is 50 skills which is enough for completing all tasks.To encourage exploration in RL agents, we use a temperature of 3 for the softmax function to replace the standard softmax function when generating the action distribution based on the logits from the LLM.We also add QLoRA for efficient finetuning.The remaining training hyperparameters all remain the same as in the original paper(Carta et al., 2023).F MINECRAFT KNOWLEDGE TESTAs stated in Section 5.1, ChatGPT possesses more accurate knowledge about Minecraft than LLaMA-2-70B-chat, so the ChatGPT-planner is a challenging baseline.To verify this, we construct a Minecraft knowledge dataset.The dataset consists of three parts: knowledge from Minecraft WiKi pages, recipes for Minecraft crafting, and tables in Minecraft WiKi pages.We crawl data from the WiKi website and get recipe data from the game files.We then use
Do as I can, not as I say: Grounding language in robotic affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Nicolas Sermanet, Clayton Sievers, Alexander Tan, Vincent Toshev, Fei Vanhoucke, Ted Xia, Peng Xiao, Sichun Xu, Mengyuan Xu, Andy Yan, Zeng, CoRL. 2022</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, ICML. 2023</p>
<p>Babyai: A platform to study the sample efficiency of grounded language learning. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio, ICLR2019</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, arXiv:2210.114162022arXiv preprint</p>
<p>Collaborating with language models for embodied reasoning. Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, Rob Fergus, arXiv:2302.007632023arXiv preprint</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, arXiv:2306.06070Mind2web: Towards a generalist agent for the web. 2023arXiv preprint</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. NeurIPS2022</p>
<p>Minerl: A large-scale dataset of minecraft demonstrations. H William, Brandon Guss, Nicholay Houghton, Phillip Topin, Cayden Wang, Manuela Codel, Ruslan Veloso, Salakhutdinov, IJCAI. 2019</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR2022</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Preprint, Pieter Huang, Deepak Abbeel, Igor Pathak, Mordatch, ICML. 2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, CoRL. 2022b</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, Yuke Zhu, NeurIPS. 2022</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, ICRA. 2023</p>
<p>Mind's eye: Grounded language model reasoning through simulation. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M Dai, arXiv:2210.053592022arXiv preprint</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 2022NeurIPS</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, CVPR. 2018</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, ICRA. 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Open foundation and fine-tuned chat models. 20232arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023barXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023carXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, NeurIPS2022</p>
<p>Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li, arXiv:2305.15486Spring: Gpt-4 out-performs rl algorithms by studying papers and reasoning. 2023arXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Language models meet world models: Embodied experiences enhance language models. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu, arXiv:2305.106262023arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Preprint, Howard Yao, John Chen, Karthik Yang, Narasimhan, 2022NeurIPS</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, ICLR2023</p>
<p>Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, Zongqing Lu, arXiv:2303.165632023arXiv preprint</p>
<p>Piglet: Language grounding through neuro-symbolic interaction in a 3d world. Rowan Zellers, Ari Holtzman, Matthew E Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, Yejin Choi, ACL. 2021</p>
<p>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.171442023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
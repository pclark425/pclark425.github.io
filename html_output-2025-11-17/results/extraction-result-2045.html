<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2045 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2045</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2045</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-279243862</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.04287v1.pdf" target="_blank">Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Training large language model (LLM) agents to acquire necessary skills and perform diverse tasks within an environment is gaining interest as a means to enable open-endedness. However, creating the training dataset for their skill acquisition faces several challenges. Manual trajectory collection requires significant human effort. Another approach, where LLMs directly propose tasks to learn, is often invalid, as the LLMs lack knowledge of which tasks are actually feasible. Moreover, the generated data may not provide a meaningful learning signal, as agents often already perform well on the proposed tasks. To address this, we propose a novel automatic skill discovery framework EXIF for LLM-powered agents, designed to improve the feasibility of generated target behaviors while accounting for the agents'capabilities. Our method adopts an exploration-first strategy by employing an exploration agent (Alice) to train the target agent (Bob) to learn essential skills in the environment. Specifically, Alice first interacts with the environment to retrospectively generate a feasible, environment-grounded skill dataset, which is then used to train Bob. Crucially, we incorporate an iterative feedback loop, where Alice evaluates Bob's performance to identify areas for improvement. This feedback then guides Alice's next round of exploration, forming a closed-loop data generation process. Experiments on Webshop and Crafter demonstrate EXIF's ability to effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without any human intervention, achieving substantial performance improvements. Interestingly, we observe that setting Alice to the same model as Bob also notably improves performance, demonstrating EXIF's potential for building a self-evolving system.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2045.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2045.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EXIF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EXploration and Iterative Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven, exploration-first curriculum/data-generation framework where an exploration agent (Alice) collects environment-grounded trajectories, generates instruction–trajectory (skill) pairs to fine-tune a target agent (Bob), and iteratively conditions future exploration on natural-language feedback about Bob's failures to produce targeted curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o (primary); experiments also use Qwen2.5-7B and Llama3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Alice (an LLM) performs goal-less or weakly constrained exploration in the environment to collect feasible trajectories, then retroactively generates natural-language instructions paired with those trajectories to form a skill dataset. Bob is supervised-fine-tuned on this dataset. After evaluation, Alice analyzes Bob's successes/failures (sampled rollouts and validation tasks) and produces concise natural-language feedback (F^{(k)}). Subsequent exploration is conditioned on this feedback to bias data collection toward behaviors Bob struggles with, forming a closed-loop iterative curriculum that adapts to the student's capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Webshop (text-based web shopping) and Crafter (Minecraft-like 2D open-world)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Webshop: text-based UI navigation, attribute selection, short-horizon purchase tasks, requires grounding in product attributes and search-query formulation; Crafter: open-ended, long-horizon survival/exploration/crafting, compositional prerequisites (inventory, crafting tables, furnaces), many orthogonal skills, stochastic dynamics and moving entities.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Exploration is conditioned on iterative feedback derived from Bob's evaluation (sampled successful and failed trajectories, reward thresholds). In Crafter, Alice also uses randomized initial agent status/inventory to diversify exploration; personas are used in Webshop. Training iterations may use cumulative datasets of prior iterations (ablation studies reported).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Supervised fine-tuning (SFT) with LoRA adapters, post-hoc rationale labeling (Webshop), trajectory segmentation & rule-based classifier (Crafter), evaluation rollouts, persona injection for exploration, cumulative vs non-cumulative data usage ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Webshop (Qwen2.5-7B Bob): EXIF reward 52.6 ± 0.4 (improvement +29.4 vs base), Success Rate (SR) 12.4% ± 0.1 (+7.4). Webshop (Llama3.1-8B Bob): EXIF reward 53.7 ± 1.2 (+51.7 vs base), SR 7.0% (+7.0). Crafter (Qwen2.5-7B Bob): learned skills NS = 15/22, Average Progress (AP) = 30.4% ± 2.6 (+18.8). Crafter (Llama3.1-8B Bob): NS = 14/22, AP = 31.9% ± 3.1 (+20.5). Using same (smaller) model for Alice and Bob still yields substantial gains nearly matching larger-Alice results in Webshop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Compare-to baselines: Proposal-First (PF) and Explore-First without feedback (EF). Example: Webshop (Qwen Bob) PF/EF reward ≈ 38.6 ± 2.4 (+15.4), EXIF 52.6; Llama PF reward 27.2 ±2.2 vs EXIF 53.7. Crafter: PF and EF AP < 30% (EXIF AP ≈ 30–31.9%), EXIF yields more learned skills (NS) and higher AP than PF/EF.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Exploration-first methods produced valid skill dataset ratios of ~85% (Webshop) and ~70% (Crafter) vs <30% valid for proposal-first. Crafter: EXIF-trained Qwen discovered 15/22 predefined skills; Llama discovered 14/22. Typical per-iteration data: Webshop 250 exploratory episodes per round (retain buy-ending trajectories); Crafter yields ~1500 obs–action pairs per iteration after segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Webshop: EXIF-trained Bob generalizes to the evaluation set of 500 test tasks with large reward gains, indicating improved generalization to novel product queries and attribute constraints; Crafter: improved AP shows better composition/use of learned skills in long-horizon open-ended trials. No explicit zero-shot/few-shot breakdown beyond test-set evaluations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Proposal-first curricula frequently propose infeasible tasks (LLM unaware of environment), producing many invalid or misaligned trajectories. Even with EXIF, success rate improvement (precise attribute matching in Webshop) remains limited because identifying exact product attributes is difficult. Feedback relies on natural-language analysis and may fail in very complex environments; EXIF performance depends on quality of feedback and trajectory processing (e.g., segmentation heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Helps: In Crafter long-horizon evaluations (AP metric), EXIF substantially improves performance (AP from <12% baseline to ~30%), enabling agents to manage health, gather resources, and craft tools over extended rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>For domains requiring domain-specific prerequisites (crafting trees in Crafter), EXIF discovers prerequisite-aware skills (e.g., placing a table before crafting) and boosts ability to perform compositional tasks. No experiments on specialized scientific domains reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations: (1) Replacing a large Alice (GPT-4o) with the same smaller LLM (Qwen2.5-7B) still yields significant gains, suggesting robustness to teacher size; (2) Cumulative vs Non-Cumulative training data: Webshop gains little from cumulative data (new iteration data higher quality), whereas in Crafter cumulative data prevents forgetting and is beneficial; (3) EF (explore-first without feedback) plateaus after ~1–2 iterations while EXIF keeps improving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Tested larger Alice (GPT-4o) and smaller Alice (Qwen2.5-7B). Smaller Alice used as teacher still produced substantial improvements, almost matching larger-Alice performance in Webshop; suggests limited strict dependence on very large teacher for these domains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>An exploration-first, environment-grounded LLM curriculum that iteratively conditions on student-specific feedback (EXIF) produces far more valid training data than proposal-first LLM proposals (<30% valid), yields larger performance gains (e.g., Webshop reward improvements from ~2 to >50 for Llama3.1-8B), and scales to long-horizon, compositional domains (Crafter NS and AP gains). Feedback — not just more data — is critical: EF plateaus while EXIF continues to improve across iterations; using identical LLMs for teacher and student can produce a self-evolving curriculum with strong gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2045.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2045.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proposal-First (PF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proposal-First Task Generation (LLM proposes tasks then collects rollouts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum/data-generation approach where an LLM directly proposes tasks/goals (without environment interaction) and then collects or simulates rollouts conditioned on those proposed tasks to make instruction–trajectory pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (proposal-first)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o for task proposals in baselines (when used as Alice); experiments also compare using other LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLM is prompted to propose tasks/goals first (proposal-first). Rollouts or trajectories are then produced conditioned on those proposals to build the training set. No prior environment exploration is used to validate feasibility before proposing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Webshop and Crafter (used as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Same as EXIF domains; proposal-first fails when LLM lacks environment grounding, leading to infeasible or invalid tasks especially in environments with absent entities or specific constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Rollout generation conditioned on proposed tasks; used as baseline against exploration-first and feedback-guided methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Webshop: PF baseline yields much lower valid-data ratio (<30%) and lower downstream performance than EXIF (example: Qwen2.5-7B PF reward ≈ 38.6 vs EXIF 52.6; Llama PF 27.2 vs EXIF 53.7). Crafter PF average progress (AP) < 30%, fewer learned skills than EXIF.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Proposal-first produced <30% valid skill dataset in both Webshop and Crafter (validity = instruction feasible in environment and trajectory aligned to instruction).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>High proportion of invalid or infeasible tasks because the proposing LLM lacks direct environment interaction/grounding. Generated synthetic data often misaligned with what the student agent needs, yielding suboptimal learning signal.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Underperforms in long-horizon, compositional domains like Crafter compared to exploration-first with feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Direct LLM task proposal without environment grounding is brittle: many proposed tasks are infeasible and yield low-quality training data, producing substantially worse downstream agent performance than exploration-first + feedback approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2045.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2045.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EF (Explore-First, no feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explore-First without Iterative Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-first baseline where an LLM agent (Alice) gathers environment trajectories and relabels them into tasks/instructions, but does not receive iterative feedback based on the student's evaluation to guide subsequent exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (explore-first)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o (baseline); other LLMs also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Alice explores the environment and retroactively generates instruction–trajectory pairs from those explorations. These datasets are used to fine-tune Bob, but exploration in subsequent iterations is not conditioned on Bob's current weaknesses (no feedback loop).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Webshop and Crafter (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Same characteristics as EXIF domains; exploration-first ensures higher validity than proposal-first but lacks targeted adaptation to student capability.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Trajectory relabeling into instructions, SFT fine-tuning of Bob, persona injection for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Per-iteration performance improves initially but plateaus after 1–2 iterations. Example: In Webshop EF plateaus quickly whereas EXIF continues to gain; PF/EF both underperform EXIF overall (numerical examples: EF reward values near PF but lower than EXIF — see EXIF numbers). Crafter EF AP remains below EXIF (~<30%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Exploration-first methods (EF) yield much higher valid-data ratios than proposal-first (≈85% Webshop, 70% Crafter), but EF lacks feedback-driven targeting.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Lacks iterative targeting: additional data alone results in diminishing returns (performance plateaus). Does not prioritize generation of tasks that lie beyond Bob's current capability, so fails to guide progressive skill expansion as effectively as EXIF.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Improves over proposal-first but inferior to EXIF for long-horizon benchmarks (Crafter AP and NS metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Compared directly against EXIF: EF plateaus after early iterations while EXIF continues to improve; demonstrates importance of feedback loop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Environment-grounded exploration alone produces valid training data but without feedback-guided targeting it yields limited iterative gains; feedback is a crucial component for continued curriculum effectiveness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2045.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2045.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs as curriculum mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Related-work mentions of LLMs for curriculum/task generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related literature notes that LLMs have been used to define curricula and generate tasks based on agent behavior or 'interestingness' metrics, but such proposal-first approaches can suffer feasibility/alignment issues without environment grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (proposal/context-aware in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Prior works referenced use LLMs to propose curricula or tasks (sometimes context-aware or based on notions like interestingness). The paper contrasts these with exploration-first, feedback-conditioned generation, arguing that proposals without environment interaction lead to infeasible tasks and misaligned training data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>General (cited works cover web agents, RL goal generation, environment design)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Varies by cited work; issues arise when LLM proposals are not validated by environment interaction, particularly in environments with missing entities, strong prerequisites, or long-horizon compositional requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Mentioned prior combinations include interestingness-based curricula, context-aware proposals, and self-play approaches (in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Prior LLM-proposal approaches can generate many invalid tasks and produce synthetic datasets that are irrelevant to the agent's evolving needs; this motivated EXIF's exploration-first + feedback design.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Paper emphasizes that LLM-generated curricula must be environment-grounded and student-aware (via feedback) to be effective; otherwise proposal-first methods produce low-validity datasets and weak downstream performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Guiding pretraining in reinforcement learning with large language models <em>(Rating: 2)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Dataenvgym: Data generation agents in teacher environments with student feedback <em>(Rating: 2)</em></li>
                <li>Proposer-agent-evaluator (pae): Autonomous skill discovery for foundation model internet agents <em>(Rating: 2)</em></li>
                <li>Bagel: Bootstrapping agents by guiding exploration with language <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2045",
    "paper_id": "paper-279243862",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "EXIF",
            "name_full": "EXploration and Iterative Feedback",
            "brief_description": "An LLM-driven, exploration-first curriculum/data-generation framework where an exploration agent (Alice) collects environment-grounded trajectories, generates instruction–trajectory (skill) pairs to fine-tune a target agent (Bob), and iteratively conditions future exploration on natural-language feedback about Bob's failures to produce targeted curricula.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "GPT-4o (primary); experiments also use Qwen2.5-7B and Llama3.1-8B",
            "llm_model_size": null,
            "curriculum_description": "Alice (an LLM) performs goal-less or weakly constrained exploration in the environment to collect feasible trajectories, then retroactively generates natural-language instructions paired with those trajectories to form a skill dataset. Bob is supervised-fine-tuned on this dataset. After evaluation, Alice analyzes Bob's successes/failures (sampled rollouts and validation tasks) and produces concise natural-language feedback (F^{(k)}). Subsequent exploration is conditioned on this feedback to bias data collection toward behaviors Bob struggles with, forming a closed-loop iterative curriculum that adapts to the student's capabilities.",
            "domain_name": "Webshop (text-based web shopping) and Crafter (Minecraft-like 2D open-world)",
            "domain_characteristics": "Webshop: text-based UI navigation, attribute selection, short-horizon purchase tasks, requires grounding in product attributes and search-query formulation; Crafter: open-ended, long-horizon survival/exploration/crafting, compositional prerequisites (inventory, crafting tables, furnaces), many orthogonal skills, stochastic dynamics and moving entities.",
            "state_conditioning": true,
            "state_conditioning_details": "Exploration is conditioned on iterative feedback derived from Bob's evaluation (sampled successful and failed trajectories, reward thresholds). In Crafter, Alice also uses randomized initial agent status/inventory to diversify exploration; personas are used in Webshop. Training iterations may use cumulative datasets of prior iterations (ablation studies reported).",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Supervised fine-tuning (SFT) with LoRA adapters, post-hoc rationale labeling (Webshop), trajectory segmentation & rule-based classifier (Crafter), evaluation rollouts, persona injection for exploration, cumulative vs non-cumulative data usage ablation.",
            "performance_llm_curriculum": "Webshop (Qwen2.5-7B Bob): EXIF reward 52.6 ± 0.4 (improvement +29.4 vs base), Success Rate (SR) 12.4% ± 0.1 (+7.4). Webshop (Llama3.1-8B Bob): EXIF reward 53.7 ± 1.2 (+51.7 vs base), SR 7.0% (+7.0). Crafter (Qwen2.5-7B Bob): learned skills NS = 15/22, Average Progress (AP) = 30.4% ± 2.6 (+18.8). Crafter (Llama3.1-8B Bob): NS = 14/22, AP = 31.9% ± 3.1 (+20.5). Using same (smaller) model for Alice and Bob still yields substantial gains nearly matching larger-Alice results in Webshop.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "Compare-to baselines: Proposal-First (PF) and Explore-First without feedback (EF). Example: Webshop (Qwen Bob) PF/EF reward ≈ 38.6 ± 2.4 (+15.4), EXIF 52.6; Llama PF reward 27.2 ±2.2 vs EXIF 53.7. Crafter: PF and EF AP &lt; 30% (EXIF AP ≈ 30–31.9%), EXIF yields more learned skills (NS) and higher AP than PF/EF.",
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Exploration-first methods produced valid skill dataset ratios of ~85% (Webshop) and ~70% (Crafter) vs &lt;30% valid for proposal-first. Crafter: EXIF-trained Qwen discovered 15/22 predefined skills; Llama discovered 14/22. Typical per-iteration data: Webshop 250 exploratory episodes per round (retain buy-ending trajectories); Crafter yields ~1500 obs–action pairs per iteration after segmentation.",
            "transfer_generalization_results": "Webshop: EXIF-trained Bob generalizes to the evaluation set of 500 test tasks with large reward gains, indicating improved generalization to novel product queries and attribute constraints; Crafter: improved AP shows better composition/use of learned skills in long-horizon open-ended trials. No explicit zero-shot/few-shot breakdown beyond test-set evaluations reported.",
            "computational_cost": null,
            "failure_modes_limitations": "Proposal-first curricula frequently propose infeasible tasks (LLM unaware of environment), producing many invalid or misaligned trajectories. Even with EXIF, success rate improvement (precise attribute matching in Webshop) remains limited because identifying exact product attributes is difficult. Feedback relies on natural-language analysis and may fail in very complex environments; EXIF performance depends on quality of feedback and trajectory processing (e.g., segmentation heuristics).",
            "long_horizon_performance": "Helps: In Crafter long-horizon evaluations (AP metric), EXIF substantially improves performance (AP from &lt;12% baseline to ~30%), enabling agents to manage health, gather resources, and craft tools over extended rollouts.",
            "specialized_domain_performance": "For domains requiring domain-specific prerequisites (crafting trees in Crafter), EXIF discovers prerequisite-aware skills (e.g., placing a table before crafting) and boosts ability to perform compositional tasks. No experiments on specialized scientific domains reported.",
            "ablation_studies": "Ablations: (1) Replacing a large Alice (GPT-4o) with the same smaller LLM (Qwen2.5-7B) still yields significant gains, suggesting robustness to teacher size; (2) Cumulative vs Non-Cumulative training data: Webshop gains little from cumulative data (new iteration data higher quality), whereas in Crafter cumulative data prevents forgetting and is beneficial; (3) EF (explore-first without feedback) plateaus after ~1–2 iterations while EXIF keeps improving.",
            "model_size_scaling": "Tested larger Alice (GPT-4o) and smaller Alice (Qwen2.5-7B). Smaller Alice used as teacher still produced substantial improvements, almost matching larger-Alice performance in Webshop; suggests limited strict dependence on very large teacher for these domains.",
            "key_findings_curriculum_effectiveness": "An exploration-first, environment-grounded LLM curriculum that iteratively conditions on student-specific feedback (EXIF) produces far more valid training data than proposal-first LLM proposals (&lt;30% valid), yields larger performance gains (e.g., Webshop reward improvements from ~2 to &gt;50 for Llama3.1-8B), and scales to long-horizon, compositional domains (Crafter NS and AP gains). Feedback — not just more data — is critical: EF plateaus while EXIF continues to improve across iterations; using identical LLMs for teacher and student can produce a self-evolving curriculum with strong gains.",
            "uuid": "e2045.0"
        },
        {
            "name_short": "Proposal-First (PF)",
            "name_full": "Proposal-First Task Generation (LLM proposes tasks then collects rollouts)",
            "brief_description": "A curriculum/data-generation approach where an LLM directly proposes tasks/goals (without environment interaction) and then collects or simulates rollouts conditioned on those proposed tasks to make instruction–trajectory pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (proposal-first)",
            "llm_model_name": "GPT-4o for task proposals in baselines (when used as Alice); experiments also compare using other LLMs",
            "llm_model_size": null,
            "curriculum_description": "LLM is prompted to propose tasks/goals first (proposal-first). Rollouts or trajectories are then produced conditioned on those proposals to build the training set. No prior environment exploration is used to validate feasibility before proposing tasks.",
            "domain_name": "Webshop and Crafter (used as baseline comparisons)",
            "domain_characteristics": "Same as EXIF domains; proposal-first fails when LLM lacks environment grounding, leading to infeasible or invalid tasks especially in environments with absent entities or specific constraints.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Rollout generation conditioned on proposed tasks; used as baseline against exploration-first and feedback-guided methods.",
            "performance_llm_curriculum": "Webshop: PF baseline yields much lower valid-data ratio (&lt;30%) and lower downstream performance than EXIF (example: Qwen2.5-7B PF reward ≈ 38.6 vs EXIF 52.6; Llama PF 27.2 vs EXIF 53.7). Crafter PF average progress (AP) &lt; 30%, fewer learned skills than EXIF.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Proposal-first produced &lt;30% valid skill dataset in both Webshop and Crafter (validity = instruction feasible in environment and trajectory aligned to instruction).",
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "High proportion of invalid or infeasible tasks because the proposing LLM lacks direct environment interaction/grounding. Generated synthetic data often misaligned with what the student agent needs, yielding suboptimal learning signal.",
            "long_horizon_performance": "Underperforms in long-horizon, compositional domains like Crafter compared to exploration-first with feedback.",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Direct LLM task proposal without environment grounding is brittle: many proposed tasks are infeasible and yield low-quality training data, producing substantially worse downstream agent performance than exploration-first + feedback approaches.",
            "uuid": "e2045.1"
        },
        {
            "name_short": "EF (Explore-First, no feedback)",
            "name_full": "Explore-First without Iterative Feedback",
            "brief_description": "An exploration-first baseline where an LLM agent (Alice) gathers environment trajectories and relabels them into tasks/instructions, but does not receive iterative feedback based on the student's evaluation to guide subsequent exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (explore-first)",
            "llm_model_name": "GPT-4o (baseline); other LLMs also evaluated",
            "llm_model_size": null,
            "curriculum_description": "Alice explores the environment and retroactively generates instruction–trajectory pairs from those explorations. These datasets are used to fine-tune Bob, but exploration in subsequent iterations is not conditioned on Bob's current weaknesses (no feedback loop).",
            "domain_name": "Webshop and Crafter (baseline comparisons)",
            "domain_characteristics": "Same characteristics as EXIF domains; exploration-first ensures higher validity than proposal-first but lacks targeted adaptation to student capability.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Trajectory relabeling into instructions, SFT fine-tuning of Bob, persona injection for exploration.",
            "performance_llm_curriculum": "Per-iteration performance improves initially but plateaus after 1–2 iterations. Example: In Webshop EF plateaus quickly whereas EXIF continues to gain; PF/EF both underperform EXIF overall (numerical examples: EF reward values near PF but lower than EXIF — see EXIF numbers). Crafter EF AP remains below EXIF (~&lt;30%).",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Exploration-first methods (EF) yield much higher valid-data ratios than proposal-first (≈85% Webshop, 70% Crafter), but EF lacks feedback-driven targeting.",
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Lacks iterative targeting: additional data alone results in diminishing returns (performance plateaus). Does not prioritize generation of tasks that lie beyond Bob's current capability, so fails to guide progressive skill expansion as effectively as EXIF.",
            "long_horizon_performance": "Improves over proposal-first but inferior to EXIF for long-horizon benchmarks (Crafter AP and NS metrics).",
            "specialized_domain_performance": null,
            "ablation_studies": "Compared directly against EXIF: EF plateaus after early iterations while EXIF continues to improve; demonstrates importance of feedback loop.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Environment-grounded exploration alone produces valid training data but without feedback-guided targeting it yields limited iterative gains; feedback is a crucial component for continued curriculum effectiveness.",
            "uuid": "e2045.2"
        },
        {
            "name_short": "LLMs as curriculum mention",
            "name_full": "Related-work mentions of LLMs for curriculum/task generation",
            "brief_description": "Related literature notes that LLMs have been used to define curricula and generate tasks based on agent behavior or 'interestingness' metrics, but such proposal-first approaches can suffer feasibility/alignment issues without environment grounding.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated (proposal/context-aware in related work)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Prior works referenced use LLMs to propose curricula or tasks (sometimes context-aware or based on notions like interestingness). The paper contrasts these with exploration-first, feedback-conditioned generation, arguing that proposals without environment interaction lead to infeasible tasks and misaligned training data.",
            "domain_name": "General (cited works cover web agents, RL goal generation, environment design)",
            "domain_characteristics": "Varies by cited work; issues arise when LLM proposals are not validated by environment interaction, particularly in environments with missing entities, strong prerequisites, or long-horizon compositional requirements.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Mentioned prior combinations include interestingness-based curricula, context-aware proposals, and self-play approaches (in related work).",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Prior LLM-proposal approaches can generate many invalid tasks and produce synthetic datasets that are irrelevant to the agent's evolving needs; this motivated EXIF's exploration-first + feedback design.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Paper emphasizes that LLM-generated curricula must be environment-grounded and student-aware (via feedback) to be effective; otherwise proposal-first methods produce low-validity datasets and weak downstream performance.",
            "uuid": "e2045.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Guiding pretraining in reinforcement learning with large language models",
            "rating": 2
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2
        },
        {
            "paper_title": "Dataenvgym: Data generation agents in teacher environments with student feedback",
            "rating": 2
        },
        {
            "paper_title": "Proposer-agent-evaluator (pae): Autonomous skill discovery for foundation model internet agents",
            "rating": 2
        },
        {
            "paper_title": "Bagel: Bootstrapping agents by guiding exploration with language",
            "rating": 2
        }
    ],
    "cost": 0.016687999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback
20 Jun 2025</p>
<p>Yongjin Yang 
KAIST AI</p>
<p>Sinjae Kang 
KAIST AI</p>
<p>Juyong Lee 
KAIST AI</p>
<p>Dongjun Lee 
KAIST AI</p>
<p>Se-Young Yun yunseyoung@kaist.ac.kr 
KAIST AI</p>
<p>Kimin Lee kiminlee@kaist.ac.kr 
KAIST AI</p>
<p>Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback
20 Jun 202523576772E354D2D43BBE3A18AA75C00FarXiv:2506.04287v2[cs.AI]
Training large language model (LLM) agents to acquire necessary skills and perform diverse tasks within an environment is gaining interest as a means to enable open-endedness.However, creating the training dataset for their skill acquisition faces several challenges.Manual trajectory collection requires significant human effort.Another approach, where LLMs directly propose tasks to learn, is often invalid, as the LLMs lack knowledge of which tasks are actually feasible.Moreover, the generated data may not provide a meaningful learning signal, as agents often already perform well on the proposed tasks.To address this, we propose a novel automatic skill discovery framework-EXploration and Iterative Feedback (EXIF)-for LLM-powered agents, designed to improve the feasibility of generated target behaviors while accounting for the agents' capabilities.Our method adopts an exploration-first strategy by employing an exploration agent (Alice) to train the target agent (Bob) to learn essential skills in the environment.Specifically, Alice first interacts with the environment to retrospectively generate a feasible, environment-grounded skill dataset, which is then used to train Bob.Crucially, we incorporate an iterative feedback loop, where Alice evaluates Bob 's performance to identify areas for improvement.This feedback then guides Alice 's next round of exploration, forming a closed-loop data generation process.Experiments on Webshop and Crafter demonstrate EXIF 's ability to effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without any human intervention, achieving substantial performance improvements.Interestingly, we observe that setting Alice to the same model as Bob also notably improves performance, demonstrating EXIF 's potential for building a self-evolving system.</p>
<p>Introduction</p>
<p>Large language model (LLM)-powered agents have demonstrated remarkable capabilities in interacting with complex environments and performing user-instructed tasks, including game playing [38,13] and graphical user interface (GUI) manipulation [49,40,21,33].A significant aspiration for these agents is to achieve open-endedness: the ability to autonomously explore, learn, and continuously expand their capabilities within an environment, effectively becoming capable of tackling an evergrowing range of tasks without human intervention.This kind of open-endedness cannot be easily achieved with prompting techniques such as reasoning [43], reflection [34], and tree search [18].These in-context learning mechanisms are often insufficient for fostering continuous, autonomous learning-especially in unfamiliar settings where the agent lacks awareness of possible actions and their consequences [2,45,51], necessitating continuous learning mechanisms within the environment.(1) an explore-first strategy that enables the agent, Alice, to navigate the environment and generate feasible, valid tasks, which are then used to train another agent, Bob; and (2) an iterative feedback mechanism that produces tasks and trajectories beyond Bob 's current capabilities to expand its skills.Through multiple iterations, EXIF enables Bob to expand its skill set in the target environment without any human guidance.</p>
<p>To cultivate open-ended learning and enable agents to continuously acquire specialized skills in new environments, collecting suitable training data is a critical step.A straightforward approach is to manually collect instructions and corresponding trajectories for a multitude of potential tasks in each environment, but this is often infeasible due to high costs.Consequently, recent work harnesses the generative capabilities of LLMs to automatically synthesize instruction-trajectory datasets [25,29], reducing human annotation effort and enabling scalable data collection across diverse environments.These methods often prompt LLMs to directly propose tasks and then collect trajectories conditioned on those tasks-a process we refer to as proposal-first task generation [50,48,35].</p>
<p>However, applying this proposal-first approach to foster open-ended learning presents two critical downsides.First, without actively interacting with the environment, LLMs cannot determine which tasks are feasible when making their proposals, potentially generating a large volume of invalid tasks.Second, lacking awareness of the current agent's evolving capabilities during its training lifecycle, LLMs may produce synthetic data that is misaligned with what the agent actually needs to learn to expand its skill set effectively.Because these requirements are unmet, much of the resulting synthetic data may be irrelevant or suboptimal, failing to effectively guide the agent toward learning the essential skills in the target environment [25,11,44].</p>
<p>In this paper, we propose a novel automatic skill discovery algorithm for language agents, based on EXploration and Iterative Feedback (EXIF).Our method integrates two crucial components: (a) exploration-based skill dataset generation and (b) multi-iteration feedback.EXIF utilizes two LLM agents: Alice, which generates exploratory trajectories and corresponding instructionspairing them into a skill dataset, referring to data used to learn necessary skills in the environmentand Bob, which is trained on this dataset to effectively perform tasks in the given environment.Specifically, Alice explores the environment and converts these explorations into feasible trajectories and instructions.This ensures that the generated tasks are grounded in the environment, unlike proposal-first approaches, which risk producing infeasible tasks.Bob is then trained on the generated dataset.Subsequently, EXIF incorporates an iterative feedback loop: Alice identifies areas where Bob struggles and provides targeted feedback.Based on this feedback, Alice generates a new, tailored skill dataset to address these specific needs.As a result, EXIF iteratively improves Bob's skill repository by ensuring its skills are grounded in both the environment and Bob's own capabilities, ultimately enabling Bob to generalize to unseen tasks within the environment.</p>
<p>Through extensive experiments on two challenging benchmarks, Webshop [42] and Crafter [10], we show that EXIF results in a consistent improvement of LLM agent over the iterative training process.Specifically, in Webshop, the LLM agent trained with EXIF substantially improves its reward from 2.0 to 52.0 over training iterations, and in Crafter, it achieves performance comparable to that of GPT-4o [15] (Alice).Notably, this performance is achieved without any human intervention in the synthetic data generation process.Moreover, we demonstrate that even using the same model for both Alice and Bob yields notable performance improvement, highlighting the potential for building self-evolving systems.We believe that our method paves a way for more autonomous, self-improving AI agents that learn and adapt in complex environments with minimal human guidance, enabling a new generation of intelligent systems.</p>
<p>Method</p>
<p>In this section, we introduce EXIF, a novel algorithm for automatic skill discovery using exploration and iterative feedback.As illustrated in Figure 1, EXIF utilizes a LLM agent, hereafter referred to as Alice (parameterized by ϕ), for exploration-based trajectory generation and feedback processing.The insights gained from Alice are then used to iteratively train a target LLM agent, hereafter referred to as Bob (parameterized by θ).The process involves initial exploration and instruction generation by Alice, followed by iterative refinement of Bob based on its performance.The pseudocode is provided in Appendix C, and additional implementation details are available in Appendix E.</p>
<p>Throughout, we consider an agent interacting with an environment over discrete time steps t = 1, 2, . . ., T , receiving observation o t ∈ O and taking action a t ∈ A based on the history h t = (o t−H , a t−H , . . ., o t−1 ) and optionally a goal g.We use an LLM as a policy π ϕ (or π θ ), producing actions as
a t ∼ π ϕ (• | h t , o t , g). The full trajectory is denoted τ = (o 1 , a 1 , . . . , o T , a T ).
Specifically, our method consists of the following steps:</p>
<p>• Step 1 (Exploration &amp; skill dataset generation): Alice explores the target environment to collect diverse trajectories and then generates instructions from collected trajectories and creates synthetic instruction-trajectory pairs (skill dataset) (Section 2.1).</p>
<p>• Step 2 (Training target agent &amp; evaluation):</p>
<p>The generated skill dataset is used to fine-tune Bob, which is then evaluated in the target environment (Section 2.2).</p>
<p>• Step 3 (Feedback &amp; repeat (Steps 1-3)): Alice gives feedback on Bob's evaluation and repeats Steps 1-3, with exploration this time conditioned on feedback to inform targeted data generation for subsequent rounds of fine-tuning (Section 2.3).</p>
<p>Exploration</p>
<p>The initial phase focuses on gathering diverse behavioral data from the environment using Alice's policy π ϕ .Unlike typical goal-oriented agents, Alice operates without an explicit external goal g during this phase.This is because Alice often lacks prior knowledge of the environment, and exploring with an arbitrary goal, proposed by Alice, might lead to invalid trajectories if the goal is not achievable within the environment.exp and generates a natural language instruction I (j) that describes the demonstrated task or behavior.This yields the final skill dataset D skill = {(I (j) , τ (j) )} M j=1 , where each instruction I (j) is grounded in a corresponding trajectory τ (j) exp .</p>
<p>Fine-tuning Bob</p>
<p>The generated dataset D skill is used to train the target agent, Bob, whose policy π θ is parameterized by θ.We employ supervised fine-tuning (SFT) to teach Bob (π θ ) to execute the generated instructions I (j) by mimicking the actions a (j)</p>
<p>t in the corresponding trajectories τ (j) = (o
(j) 1 , a (j) 1 , . . . , o(j)
Tj , a</p>
<p>Tj ).Specifically, Bob (π θ ) is trained to maximize the likelihood of the actions in the trajectory given the instruction and the history.This is achieved by minimizing the SFT loss over the dataset D skill :
L SF T (θ; D skill ) = − M j=1 Tj t=1 log π θ (a (j) t |h (j) t , o (j) t , I (j) ),(1)
where h
(j) t = (o (j) t−H , a(j)
t−H , . . ., o</p>
<p>t−1 ) is the history at t with context length H within trajectory j.This initial training yields the first version of Bob's fine-tuned policy π θ (0) .</p>
<p>Feedback generation &amp; iterative process</p>
<p>EXIF incorporates an iterative refinement loop (indexed by k = 0, 1, 2, . . . ) to progressively enhance Bob 's (π θ ) capabilities by targeting areas for improvement.Each iteration involves evaluating Bob at iteration k, generating targeted data using Alice (ϕ) guided by feedback for the next iteration (k + 1), and retraining Bob (θ).</p>
<p>Feedback generation To generate feedback for iteration k + 1, the performance or behaviors of the current Bob policy π θ (k) in the target environment are evaluated.This evaluation involves executing Bob on a set of evaluation tasks or allowing it to interact within the environment, potentially attempting tasks similar to those in the training set or novel ones.Analyzing its successes and failures-such as the inability to follow certain instructions or failure to complete specific sub-tasks as reflected in the o t , a t sequences-then yields a natural language feedback signal F (k) .This signal encodes the deficiencies or areas where Bob (π θ (k) ) requires improvement.</p>
<p>Repeat the process After feedback generation, the next iteration begins: exploration and instruction generation with Alice, fine-tuning Bob, evaluation, and feedback generation.The only key difference starting from iteration 1 is that the first step-exploration-is now conditioned on the feedback signal F (k) to generate a skill dataset tailored to Bob 's current status.This iterative framework ensures that Bob expands the necessary skills at each iteration without any human intervention, supporting the goal of open-endedness.</p>
<p>Experiments</p>
<p>In this section, we present our experimental results.The goal of the experiments is to address the following four research questions: RQ1: How effective is EXIF in enabling Bob to solve more tasks in the environment by expanding its skill set without human guidance?RQ2: How important is the exploration-first approach in generating valid tasks for Bob? RQ3: How do feedback and iterative refinement influence the skill discovery process?RQ4: Can EXIF effectively enable the emergence of a self-evolving agent system?</p>
<p>Experiment settings</p>
<p>We describe our experimental settings, including environments, models, and baselines.Details are provided in Appendix B (environments), Appendix D (prompts), and Appendix E (implementation).</p>
<p>Environment To answer our research questions, we conduct experiments on two challenging benchmarks: Webshop [42] and Crafter [10], exhibiting different task properties.</p>
<p>• Webshop: Webshop is a text-based simulated e-commerce web environment where agents must navigate web pages to purchase a product specified by a natural language instruction.The observation space consists of the textual content of the web pages, and the action space involves searching queries and clicking UI elements.Key skills include grounding instructions, selecting appropriate search keywords, identifying the correct products, and clicking on the right attributes.This benchmark allows us to evaluate whether using EXIF improves Bob's generalization capability when faced with novel products and constraints.</p>
<p>• Crafter: Crafter is a Minecraft-like 2D game environment simulating 2D open world.The main objective of the agent in this environment is to survive, explore, gather resources, craft items, and defend against threatening mobs.To interface with LLM agents, we convert imagebased observations into a text format by describing the agents' status, inventory, surroundings, and directly facing entities [28].Key skills in Crafter include exploration, health management, mineral collection, and tool crafting.Within this complex, open-ended benchmark, our aim is to demonstrate that EXIF's goal-less exploration can uncover fundamental skills, like drinking water and collecting resources.Furthermore, we want to show how its iterative feedback loop is crucial for discovering more complex, compositional skills, such as crafting advanced weapons, ultimately enabling the achievement of long-horizon goals.</p>
<p>Models</p>
<p>In both experiments, we use GPT-4o-2024-08-06 [15] as the base LLM for Alice.For Bob, we employ two different base LLMs: Qwen2.5-7B[41] and Llama3.1-8B[9].We also conduct an experiment using the same LLM for both Alice and Bob, i.e., Qwen2.5-7B, to study the potential of a self-evolving system (Section 3.4).</p>
<p>Baselines We compare EXIF with several baselines: the proprietary model gpt-4o and the base Bob models before training.We also evaluate task proposal-first methods (PF), where Alice proposes tasks without exploration, and rollouts are generated conditioned on these tasks to form the skill dataset.Lastly, we include an explore-first method without a feedback mechanism, denoted as EF.</p>
<p>Exploration details In Webshop, we assign a unique persona for each episode using PersonaHub3 to encourage diversity.In each round, Alice explores for 250 episodes, ending when a purchase is made or the maximum horizon is reached.In Crafter, Alice is only instructed to survive as long as possible.</p>
<p>Each of the 50 episodes ends when the maximum horizon is reached or health points are depleted, following the benchmark's predefined termination criteria.</p>
<p>Training details As described in Section 2, Alice generates skill dataset to train Bob.In Webshop, we additionally apply post-hoc reasoning [24] to label rationales based on instructions and trajectories.In Crafter, to construct a high-quality skill dataset, we preprocess long-horizon explorative trajectories into segments to generate instructions.While segmenting, we apply a rule-based classifier to monitor changes in the agent's status, inventory, and surrounding entities, but ensure that no additional information is provided beyond the agent's observability.We, then, filter out random and uninformative behavior by retaining only the last four steps of each segment.</p>
<p>Feedback In Webshop, we use Alice to provide feedback on Bob 's validation performance.Specifically, we use task IDs 501-550 from the validation set.We randomly sample two successful and four failed trajectories, including instructions, based on a reward threshold of 0.5.Alice is then prompted to identify model shortcomings and suggest two exploration guidelines as feedback.In Crafter, we request Bob to survive in the environment as long as possible without specific goals, mirroring the standard test setup [28], due to the absence of validation tasks.Then, we prompt Alice to generate feedback on Bob's 20 rollout trials in the environment.</p>
<p>Evaluation In Webshop, we utilize the first 500 test tasks to measure the performance of Bob.Specifically, we use the environment's predefined reward and the task success rate (SR) to measure the performance.In Crafter, we adopt two metrics to thoroughly examine (1) the improvement of skill set and (2) the capability of agents in using the learned skills in a long-horizon interaction with the environment.First, we count the number of learned skills (NS) out of 22 pre-defined tasks in the benchmark.When measuring this, we provide an explicit instruction specifying each task and the necessary prerequisites (e.g., the stone pickaxe when mining iron) to the agent, and count the completed skills with at least a 0.5 success rate over 10 trials.Second, we measure the average progress (AP) of achievements accomplishments (out of the pre-defined 22 tasks) in a single rollout starting without any prerequisite item, following evaluation of prior work [28], across 20 trials.</p>
<p>Main results</p>
<p>Quantitative analysis Table 1 presents a comparison of agents trained on different datasets for the Webshop and Crafter tasks.Notably, in both tasks, EXIF significantly outperforms the base model before fine-tuning, indicating that Alice generates meaningful skill dataset for Bob.Furthermore, compared to PF and EF, EXIF achieves superior performance, highlighting the importance of both the exploration-first strategy and the feedback mechanism.</p>
<p>Specifically, in Webshop, the base Llama3.1-8Bmodel achieves only a reward value of 2.0, indicating that it fails to perform well on any tasks.In contrast, using our method, it achieves a reward value exceeding 50.0-significantly higher than that of the proprietary model GPT-4o.This suggests that GPT-4o is also unfamiliar with Webshop and lacks the capability to effectively accomplish the tasks.This also explains the poor performance of PF methods, as Alice struggles not only with achieving the proposed tasks but also with generating valid ones, highlighting the need for an exploration-first approach.Moreover, incorporating a feedback mechanism into EF-which is EXIF-boosts performance by nearly 50%, underscoring the importance of feedback in guiding the synthesis of training trajectories tailored to the agent.Specifically, as shown in Figure 2, the performance of EF plateaus after iteration 1 or 2, whereas EXIF exhibits consistent gains due to the feedback mechanism, indicating that naive scaling of data alone does not improve performance.However, the success rate does not improve significantly, as precisely identifying the correct item with all attributes is very challenging.A similar trend is observed for Qwen2.5-7B,though in this case, feedback-guided exploration also leads to an increase in success rate.</p>
<p>In Crafter, agents using both Llama3.1-8Band Qwen2.5-7Bachieve performance close to that of GPT-4o.Specifically, in the evaluation measuring the number of learned skills, the trained Qwen agent matches the base GPT-4o, achieving 15 skills out of 22 test tasks.Similarly, the Llama agent achieves 14 skills-twice as many as its untrained counterpart.When we evaluate agents by making them survive in the environment for as long as possible without any prerequisite inventory, the Llama and Qwen agents achieve AP values of 31.9% and 30.4%, respectively.This indicates that the skills discovered by EXIF are highly beneficial in long-horizon, open-ended evaluation settings.</p>
<p>Compared to the base agents, which average below 12% AP, agents trained with EXIF learn to manage health by using resources like food and water, and gradually upgrade their inventory by collecting materials and crafting tools.In contrast, both PF and EF show limited performance-with AP below 30%-highlighting the advantage of feedback-guided exploration in expanding agent capabilities.Additionally, as shown in Figure 2, the feedback mechanism in EXIF enables the agent to learn a greater number of skills (NS) and achieve larger gains in AP over training iterations compared to EF, similar to the trend observed in Webshop-highlighting the effectiveness of feedback.</p>
<p>Qualitative analysis Figure 3 shows qualitative examples demonstrating how, given the same instruction, the trained model differs in its action sequences compared to the base model.In Webshop, we observe that the base model fails to click on attributes such as "size, 21 in x 35 in," whereas after applying EXIF, the model successfully follows the instruction by learning how to correctly click attributes or conditions mentioned in the prompt.In Crafter, the base model exhibits excessive random behavior for the given instruction of "Collect iron".Due to such repetitive behavior, the agents fail to reach the target iron tile as obstructed by the stone tile.On the other hand, the model trained with EXIF learns that the skill of collecting stones is necessary to move forward and ultimately reaches the target iron, successfully completing the task.</p>
<p>Trajectory and feedback analysis</p>
<p>Proposal-first vs exploration-first A lot of tasks from the proposal-first approach are invalid, as the model proposes goals without precise knowledge of the environment, often leading to infeasible tasks or mismatched trajectories.In contrast, the exploration-first approach yields mostly valid tasks by generating trajectories first and then deriving instructions from the trajectory and final observation, ensuring better alignment.For example, tasks like "Smelt raw beef into cooked beef using coal in the furnace" or "Place a torch in a dark cave area," though seemingly plausible, are indeed invalid in Crafter due to the absence of entities.Figure 4a shows the ratio of valid skill datasets generated by   Focus on resource preparation for iron tool crafting, prioritizing materials that support smelting and tool upgrades; avoid crafting additional wooden tools as they are redundant at this stage the two approaches: PF and EF.Specifically, we consider skill data valid if the instruction is feasible in the environment and its trajectory aligns with the corresponding instruction (see Appendix E).</p>
<p>We observe that exploration-first methods yield 85% and 70% in Webshop and Crafter, respectively, while proposal-first methods result in less than 30% valid skill dataset, demonstrating the importance of the exploration-first approach for collecting trajectories.</p>
<p>Feedback analysis Table 2 presents feedback examples during EXIF.In Webshop, early iterations show Bob repeating actions and using short queries, while later iterations include feedback prompting attribute interactions (e.g., size, color).As a result, Alice adjusts its exploration, and Bob exhibits reduced action repetition, increased attribute selection, and more detailed search queries, as shown in Figure 4b.In Crafter, feedback guides Alice toward increasingly advanced skills in each round.As shown in Figure 4c, the skill distribution shifts toward tasks targeting different objectives over iterations.Early feedback focuses on basic skills like collecting wood, while later rounds emphasize crafting stone tools, enabling Bob to complete more complex tasks (please refer to Appendix F for the definition of each task type).</p>
<p>Additional studies</p>
<p>Potential of self-evolving system Figure 5a shows the result of replacing Alice (GPT-4o) with Qwen2.5-7B, the same model used for Bob.Surprisingly, this also leads to a significant performance improvement on both benchmarks compared to the base models, nearly matching the performance of a larger Alice model in Webshop.This suggests that EXIF can effectively expand the skill set within the environment even without relying on a proprietary model.It highlights the potential of EXIF towards building a self-evolving system-where two identical agents, without any human intervention, collaboratively generate data and learn to perform well, resembling a form of self-play [27].(b) Ablation on whether using data from the previous iteration, where "Cumulative" means using data from previous iterations, and "Non-Cumulative" means not using data from the previous iteration.</p>
<p>Ablation on training</p>
<p>We also conduct an ablation study on data usage to examine whether using the generated dataset from the previous round is beneficial."Cumulative" indicates using the previous dataset, while "Non-cumulative" means not using it.As shown in Figure 5b, in Webshop, using cumulative data provides limited benefit, since the next iteration produces a higher-quality skill dataset that compensates for what the previous one lacks.In contrast, in Crafter, using cumulative data is more beneficial as a way to prevent forgetting, since the task involves acquiring new skills that are orthogonal to those from earlier rounds, and each generation differs in its skill distribution.</p>
<p>Related work</p>
<p>Skills in autonomous agent The concept of skills has been studied across various agentic tasks, including locomotion and manipulation [36,1,31].A common approach defines skills via latent variables, aiming to discover all the possible skills given a distribution over state-action trajectories [6,20,30].Alternatively, skills can be represented with natural language in hierarchical frameworks, where high-level policies select language-defined skills and low-level policies execute them [14,47,17].Recently, several works have loosely defined a skill as a sequence of actions, allowing it to emerge implicitly from the policy without explicit representation [50].Following this view, we aim to discover feasible and useful skills grounded in both the environment and the target agent's training.</p>
<p>Curriculum generation for autonomous agent A line of research has explored methods for automatically generating goal states [8,32] or designing training environments [16,39,4], enabling agents to continuously learn novel behaviors in open-ended environments.Several works have also investigated self-play approaches [22,37], where agents improve their capabilities by learning to achieve challenging goals generated by their opponents.More recently, LLMs have been used to define curricula [5,26], and some studies leverage this to create training curricula based on the notion of interestingness [46,7].Additionally, there are works that use LLMs to generate tasks based on the agent behavior or introduces context-aware task proposals [17,50].In this work, we study ensuring the feasibility of the generated plans by letting the LLM explore the environment and, then, relabeling the collected exploration trajectory retroactively.</p>
<p>Dataset synthesis for LLM agent</p>
<p>To learn diverse skills, synthesizing dataset with a variety of instructions is crucial.Early approaches to collecting instructions following the trajectory for training LLM agents depend on human annotation [3,23].Due to the prohibitive cost of human annotation, AutoWebGLM [19] utilized LLMs for synthesizing instructions, and OpenWebVoyager [12] utilized LLMs to collect further trajectories that follow the instructions.To improve the quality of generated instructions, BAGEL [25] studies refining the synthesized instructions by testing an agent with the generated instructions.Furthermore, NNetnav [24] and Explorer [29] propose exploration-based dataset generation, which ensures the feasibility of the trajectory.On top of these works, our approach extends the concept of exploration-based dataset synthesis to adopt iterative interactions between the teacher and student agents, allowing for a more scalable trajectory synthesis.</p>
<p>Conclusion</p>
<p>We propose EXIF, a novel framework for automated skill discovery in LLM agents that combines an exploration-first mechanism with iterative training using feedback.Our approach collects trajectories via exploration-guided task generation, uses the explorative agent Alice to generate a skill dataset, trains the target agent Bob on this dataset, and iteratively refines the exploration strategy based on feedback about Bob 's behavior to expand its skill set.Through extensive experiments, we show that the LLM agent's performance improves over multiple iterations, acquiring diverse skills without any human demonstrations-even in a self-play setting.We believe our method represents a meaningful step toward achieving open-endedness by enabling agents to autonomously acquire diverse, environment-grounded skills through iterative exploration and feedback.</p>
<p>Plus (5.5 inch) $8.9</p>
<p>Action Space Actions consist of two distinct types: search and click.The search action allows the agent to search for items in the web environment and is only available on the initial page with the search button.Search queries can include any keywords related to various products, such as phones, tablets, shoes, clothes, and more.</p>
<p>All actions beyond the initial page are click actions.There are three types of click actions:</p>
<p>• Clicking HTML elements, mostly item IDs, to navigate to specific product pages.</p>
<p>• Clicking navigation options, where the agent can choose to go back to the previous page, proceed to the next page, return to the search page, purchase a product, etc.</p>
<p>• Selecting product attributes, such as color or size, to finalize the product details before purchase.</p>
<p>Benchmark Evaluation For Webshop, there are predefined tasks identified by task IDs.Following the original setting [42], we use task IDs 0-499 as evaluation tasks.The instruction in each evaluation task typically takes the form of a search request with specific constraints, such as: "Find me double sided, machine washable decorative pillows with printing technology with size: 28" x 28", and price lower than 30.00 dollars."Each task has a predefined reward based on how similar the selected product is to the ground-truth answer.A success is counted when the reward is 1.0, indicating a perfect match.</p>
<p>B.2 Crafter</p>
<p>We explain the details of the Crafter environment, including the observation space, action space, instruction set, and evaluation setting.</p>
<p>Observations Within our experimental setup, we convert raw image observations into structured textual representations to interface with the LLM agent.Each textual observation encodes the agent's current status, inventory, immediate surroundings, and the entity directly in its line of sight.A specific example is provided below.</p>
<p>Example of Crafter Observation</p>
<h3>Current Observation Your status:</h3>
<p>-health: Evaluation We evaluate our method in the Crafter environment using two complementary metrics that capture (1) the diversity and number of skills acquired, and (2) the agent's ability to use these skills in long-horizon interactions without task instruction.</p>
<p>• Number of learned skills (NS) : To assess the breadth of the acquired skill set, we compute the number of learned skills, denoted as NS, out of the 22 pre-defined tasks in the Crafter benchmark.</p>
<p>For each task, we provide the agent with an explicit natural language instruction that clearly specifies the goal and any necessary prerequisites.The agent is evaluated over 10 independent trials per task.A task is considered successfully learned if the agent achieves a success rate of at least 0.5 across these trials.This metric reflects the agent's ability to master individual skills when prompted with clear instructions.All trials are conducted using environment seeds 42+i, where i = 0, 1, . . ., 9.</p>
<p>• Average progress (AP) : To evaluate the agent's ability to autonomously achieve goals in an open-ended setting, we compute the average progress, denoted as AP.This metric measures the average proportion (ranging from 0 to 1) of distinct achievements accomplished in a single episode, out of the same set of 22 tasks.Following prior work, the agent is initialized without any prerequisite items (i.e., no tools and resources) and runs for one full rollout.The AP score is averaged over 20 such episodes.Unlike NS, which evaluates isolated skill execution under guided instructions, AP captures how well the agent can compose and utilize previously learned skills to make progress toward multiple goals in a long-horizon, unguided setting.All episodes are conducted using environment seeds 42+i, where i = 0, 1, . . ., 19.</p>
<p>C Algorithm</p>
<p>Algorithm 1 presents the detailed procedure of EXIF, with further explanation provided in Section 2.</p>
<p>Algorithm 1: EXIF: Automatic Skill Discovery via Exploration and Iterative Feedback 1: Initialize:
2:
LLM agent Alice (policy π ϕ parameterized by ϕ)</p>
<p>3:</p>
<p>Target LLM agent Bob (policy π θ parameterized by θ)</p>
<p>4:</p>
<p>Total number of iterations K iter if k = 0 then 12:</p>
<p>Alice explores environment:
a t ∼ π ϕ (• | h t , o t ) ▷ Initial exploration phase 13:
Collect M initial exploratory trajectories D Alice explores environment using feedback
F (k−1) : a t ∼ π ϕ (• | h t , o t , F (k−1)
) ▷ Exploration with feedback for each trajectory τ Alice analyzes τ (j) exp and generates a natural language instruction I (j) 22: Fine-tune Bob's policy parameters θ to θ (k) using D (k) skill , yielding policy π θ (k)
D (k) skill ← D (k) skill ∪ {(I (j) , τ(j)</p>
<p>27:</p>
<p>Minimize SFT loss:
L SF T (θ (k) ; D (k) skill ) = − M j=1 Tj t=1 log π θ (k) (a (j) t | h (j) t , o(j)
t , I (j) ) Evaluate Bob's current policy π θ (k) in the target environment.Let E k be the evaluation data ▷ Collect (o t , a t ), etc.</p>
<p>31:</p>
<p>Alice analyzes Bob's performance E k to generate natural language feedback F (k) ▷ F (k) for next iter.(if k &lt; K iter − 1)</p>
<p>32:</p>
<p>33: end for</p>
<p>D Exploration prompts</p>
<p>We provide the detailed prompts that are used for the experiment.We used several different types of prompts for each benchmark we used: Webshop and Crafter.The prompts comprise an exploration prompt, an instruction generation prompt, an evaluation prompt, and a feedback generation prompt.In Webshop, we additionally use a post-hoc reasoning prompt.</p>
<p>D.1 Webshop</p>
<p>D.1.1 Exploration prompt Exploration Prompt</p>
<p>You are a web-shop-agent that can interact with the webpage by taking actions.You need to buy something that you want at the end.Also, you should adopt the identity of following persona : {task_state.persona}You should take actions that are consistent with the persona you have adopted.</p>
<p>In the web environment, your actions are strictly limited to two types:</p>
<ol>
<li>search[keywords]: Use this action only when a "[button] Search [button_]" is present in the current web page content.You must replace "keywords" with any valid search query you want to search.</li>
</ol>
<p>click[HTML Element]:</p>
<p>Use this action to click on an HTML Element in the page content."HTML Element" can be any clickable element in the page represented inside "[button]" and "[button_]", such as an item id, action button, or attributes and options like color or size.Note that the 'HTML Element' must be present in the current page content.Also, do not click the attributes inside the "[clicked button]" and "[clicked button_]", "item name", and "button" iteself (e.g.click[button] is not allowed).</p>
<p>Only use search action when a "[button] Search [button_]" is present in the current web page content and otherwise, use click action (click item id, attributes like color and size, or action button).Feedback from Previous Round : {feedback_from_alice} Now here is the new page content.Read carefully the page content.Based on your persona and the current web page content, give a brief thought and provide any valid action that seems very interesting.When outputting the action, please write your action after the prompt 'Action:'.</p>
<p>D.1.2 Instruction generation prompt</p>
<p>Instruction generation Prompt</p>
<p>You are a helpful assistant trained to understand web environment and generate shopping instructions.You are given an action sequence and a final product description.Your task is to generate only an user query that will lead to the final product description.Now here are the given action sequence and final product description.Action Sequence: action_sequence Final Product Description: {final_state} Considering both search keywords and product detail, please generate an user query.Please put more weight on the search keywords than the product detail.Do not directly include the product name in the query and rather give a high-level description of the product.Note that clicked attributes in action sequence, like size, color, and options should be included in the query.(Buy now is not an attribute) Attributes without [clicked button] should not be included in the query, as they are not part of the product.You should also include the price condition in the query (e.g.price lower than XX dollars).You should not include any other text than the query.Randomly start the query with words "Find me", "Show me", "I am looking for", "I need", "I want", or similar words.</p>
<p>User Query:</p>
<p>D.1.3 Evaluation Prompt Evaluation Prompt</p>
<p>You are an agent with a strict task of completing a web shopping assignment based on the page content and the user's instructions.</p>
<p>In each step, your actions are strictly limited to two types:</p>
<ol>
<li>search[keywords]: Use this action only when a "[button] Search [button_]" is present in the current web page content.You must replace "keywords" with any valid search query you want to search.</li>
</ol>
<p>click[HTML Element]:</p>
<p>Use this action to click on an HTML Element in the page content."HTML Element" can be any clickable element in the page represented inside "[button]" and "[button_]", such as an item id, action button, or attributes and options like color or size.Note that the "HTML Element" must be present in the current page content.Also, do not click the "clicked button" or "item name".</p>
<p>Only use search action when a "[button] Search [button_]" is present in the current web page content and otherwise, use click action (click item id, attributes like color and size, or action button).Now, here is the task Task : {task_name} To complete the given task, you have taken the following actions: {action_summary} Now here is the new page content.Read carefully the page content.Based on the previous actions, the given task, and the current web page content, give a brief thought and provide a valid action.When outputting the action, please write your action after the prompt "Action:".</p>
<p>D.1.4 Feedback generation Prompt</p>
<p>Feedback Generation Prompt</p>
<p>You are an AI assistant tasked with analyzing web shopping trajectories.To get a high reward, the model needs to complete the task with the given instruction, fulfilling the task requirements of product type, price, attributes like size and color, etc.</p>
<p>Given trajectories of varying rewards, identify strengths in successful trajectories and weaknesses in failed trajectories.Provide concise feedback (2 points maximum) on what skills need improvement to achieve a high reward.</p>
<p>Using your feedback, you will explore the web shopping task on the next round, where your trajectories will be used to train the model.For example, if the model lacks detailed search queries, you need to make an initial query very detailed when the search page is shown because your search will be used as data for fine-tuning the model.Now here are the trajectories of the current model: Also, you can take up to 10 actions in the environment, so please give feedback on how to have a good and concise action sequence.<strong>*</strong>Note that during your exploration, there are "no instructions, given criteria, or requirements to follow", so you need to provide feedback on which types of actions are beneficial (as there are only two types: search and click, specify which search keywords or clicking on which elements are beneficial).</p>
<p>If you do certain actions with your interest, the models are encouraged to do more of that action.Thus, do not say something like "do something to meet criteria", "follow the criteria, instructions, or given states", or "match specific attributes".Just say what you think is good or bad.</p>
<p>The example format could be like this:</p>
<p>1.The current low reward is due to B. Refrain from B during your exploration.</p>
<ol>
<li>The current low reward is due to not clicking C. Ensure to click diverse C during your exploration.</li>
</ol>
<p>D.1.5 Post-hoc reasoning prompt</p>
<p>Post-hoc Reasoning Prompt</p>
<p>You are an AI assistant tasked with explaining actions taken in a web environment.</p>
<p>Given the instruction you need to follow and the current observation, provide a rationale for why the "last action" was taken to follow the instruction.You can also refer to the previous actions to provide a rationale.The rationale should naturally fit with "[your rationale].Thus, my action is [chosen action]."You only need to provide "your rationale" part.Be very concise and clear.Now, here are the given instruction, previous actions, current observation, and the last action.You are an intelligent agent navigating and surviving in the Crafter game world while performing the given task, learning and adapting through feedback.Below are the only valid actions you can take in the game, along with their descriptions.</p>
<h3>Valid Actions</h3>
<p>-move_left: move one tile west -move_right: move one tile east -move_up: move one tile north -move_down: move one tile south -do: interact with the tile in front (collect material, drink from lake to restore 'drink' level, attack creature, hunt cow to restore 'food' level) -sleep: sleep to restore 'energy' level -place_stone: place a stone in front -place_table: place a wooden crafting table in front, used for making tools and weapons.</p>
<p>-place_furnace: place a stone furnace in front, used for crafting advanced tools and materials.</p>
<p>-place_plant: place a plant in front -make_wood_pickaxe: craft a wood pickaxe, which requires a nearby table and wood in your inventory.</p>
<p>-make_wood_sword: craft a wood sword, which requires a nearby table and wood in your inventory.</p>
<p>-make_stone_pickaxe: craft a stone pickaxe, which requires a nearby table and both wood and stone in your inventory.</p>
<p>-make_stone_sword: craft a stone sword, which requires a nearby table and both wood and stone in your inventory.</p>
<p>-make_iron_pickaxe: craft an iron pickaxe, which requires both a nearby table and furnace, as well as wood, coal, and iron in your inventory.</p>
<p>-make_iron_sword: craft an iron sword, which requires both a nearby table and furnace, as well as wood, coal, and iron in your inventory.</p>
<h3>Instructions</h3>
<p>-Plan progressively based on your inventory: Before choosing your next action, carefully examine your current inventory.Reflect on the resources and tools you've gathered so far to determine the next meaningful step-whether it's crafting a new tool, upgrading existing gear, or preparing for a more advanced objective.</p>
<p>-Identify and avoid meaningless actions: Each turn you are shown the observation and status from the previous step.Always compare them with the current values; if they are identical, your last move was meaningless-adapt your plan so you do not repeat it.</p>
<p>-Stay alive: When any health falls below its average level, prioritize eating, drinking, sleeping, or defending as appropriate.</p>
<p>-Use the right tools: Some blocks (e.g., stone, iron, diamond) cannot be harvested with a bare hand-craft and equip the correct pickaxe before using do.</p>
<p>-Placement rules: You may place a work table, furnace, plant, or stone only when you are facing a tile of grass, path, or sand.</p>
<h3 feedback_from_alice="feedback_from_alice">Feedback from Previous Round</h3>
<p>We include the Feedback from Previous Round part without the first exploration, by replacing {feedback_from_alice} into appropriate text, such as "-Advance in the Crafter world by strategically collecting resources, crafting tools, and overcoming environmental challenges.".</p>
<p>D.2.2 Instruction generation prompt</p>
<p>Relabel Prompt</p>
<p>You are a language model trained to analyze agent behavior in the game Crafter.Your task is to infer the most likely instruction the agent was pursuing, given a sequence of environmental observations and actions.</p>
<p>Guidelines:</p>
<p>-Pay special attention to the most recent observation and action, as they reveal the agent's immediate intention.</p>
<p>-The agent can only interact with the tile it is directly facing, so consider only the facing tile when interpreting interaction actions.-The do action means the agent is trying to interact with the tile it is facing.For example: -If facing material: collect material -If facing grass: collect sapling -If facing water: drink to restore thirst -If facing hostile creature: defeat the creature -If facing cow: hunt to restore hunger -If there's a table or furnace nearby and your action starts with 'make', you're making a tool.Focus on that action.</p>
<p>-Avoid vague or generic explanations.Be precise and grounded in the recent context.</p>
<p>Your output should clearly state the inferred goal the agent was pursuing, based strictly on its behavior and what it was facing.Keep your response very brief -around 10 words maximum.</p>
<p>Here is a sequence of actions and current observation-action pair the agent took in the Crafter game.The turns are listed in chronological order, from oldest to most recent.</p>
<p>D.2.3 Evaluation prompt Evaluation Prompt</p>
<p>You are an intelligent agent navigating and surviving in the Crafter game world while performing the given task, learning and adapting through feedback.Below are the only valid actions you can take in the game, along with their descriptions.</p>
<h3>Valid Actions</h3>
<p>-move_left: move one tile west -move_right: move one tile east -move_up: move one tile north -move_down: move one tile south -do: interact with the tile in front (collect material, drink from lake to restore 'drink' level, attack creature, hunt cow to restore 'food' level) -sleep: sleep to restore 'energy' level -place_stone: place a stone in front -place_table: place a wooden crafting table in front, used for making tools and weapons.</p>
<p>-place_furnace: place a stone furnace in front, used for crafting advanced tools and materials.</p>
<p>-place_plant: place a plant in front -make_wood_pickaxe: craft a wood pickaxe, which requires a nearby table and wood in your inventory.</p>
<p>-make_wood_sword: craft a wood sword, which requires a nearby table and wood in your inventory.</p>
<p>-make_stone_pickaxe: craft a stone pickaxe, which requires a nearby table and both wood and stone in your inventory.</p>
<p>-make_stone_sword: craft a stone sword, which requires a nearby table and both wood and stone in your inventory.</p>
<p>-make_iron_pickaxe: craft an iron pickaxe, which requires both a nearby table and furnace, as well as wood, coal, and iron in your inventory.</p>
<p>-make_iron_sword: craft an iron sword, which requires both a nearby table and furnace, as well as wood, coal, and iron in your inventory.</p>
<p>-noop: do nothing ### Instructions -Plan progressively based on your inventory: Before choosing your next action, carefully examine your current inventory.Reflect on the resources and tools you've gathered so far to determine the next meaningful step-whether it's crafting a new tool, upgrading existing gear, or preparing for a more advanced objective.</p>
<p>-Identify and avoid meaningless actions: Each turn you are shown the observation and status from the previous step.Always compare them with the current values; if they are identical, your last move was meaningless-adapt your plan so you do not repeat it.</p>
<p>-Stay alive: When any health falls below its average level, prioritize eating, drinking, sleeping, or defending as appropriate.</p>
<p>-Use the right tools: Some blocks (e.g., stone, iron, diamond) cannot be harvested with a bare hand-craft and equip the correct pickaxe before using do.</p>
<p>-Placement rules: You may place a work table, furnace, plant, or stone only when you are facing a tile of grass, path, or sand.Now, here is the task Task : {task_name} For NS evaluation, the agent is prompted with a specific task name (e.g., "Make stone pickaxe"), whereas for AP evaluation, the task instruction is replaced with a general open-ended prompt: "Advance in the Crafter world by strategically collecting resources, crafting tools, and overcoming environmental challenges."</p>
<p>D.2.4 Feedback generation prompt Feedback Generation Prompt</p>
<p>You are an expert evaluator analyzing agent behavior in a survival crafting game called Crafter.You will be given a <strong>reduced version</strong> of the agent's trajectory, focusing only on segments where the agent's status and inventory have been changed.</p>
<p>F Details on skills</p>
<p>Webshop In WebShop, there are no explicit skills pre-defined in the environment.However, as explained in Section 3.3, certain high-level skills are required to perform well across diverse tasks.These include searching with detailed keywords, navigating the web, backtracking, clicking the correct product, refining search queries, reading descriptions and features, and selecting the appropriate attributes.</p>
<p>As shown in Figure 4b, EXIF effectively improves detailed search queries and selects the correct attributes while avoiding unnecessary, duplicate actions.We also expected Alice to exhibit advanced navigation behaviors, such as using the next or previous buttons, but found that these behaviors actually harmed performance.In WebShop, navigating further does not necessarily lead to better product discovery.The same holds true for backtracking.We believe that more advanced and meaningful skills will emerge in future, more challenging benchmarks using EXIF.</p>
<p>Crafter Unlike WebShop, Crafter allows us to observe explicit skills required for long-term survival through a set of predefined tasks.As shown in Figure 4c, Alice discovers more skills with each iteration, which in turn improves Bob 's performance over time.We additionally define task types to group the pre-defined skills.The full list of tasks, along with task types and their descriptions, is provided in Table 3. Defeat a zombie enemy trigger the specific crafting behavior.This indicates a lack of understanding that crafting requires an explicit "make_stone_sword" action, not a generic interaction.Finally, in Iteration 2, Bob correctly identifies both the prerequisite "placing the table" and the appropriate action, which is explicitly calling the "make_stone_sword" action.</p>
<p>Another example is shown below: In Iteration 2, Bob finds the zombie but repeatedly uses the "do" action without accounting for the zombie's movement.As a result, it fails to make effective contact and cannot defeat the zombie, reflecting a lack of adaptation to dynamic enemy behavior.In contrast, in Iteration 3, Bob 's action sequence demonstrates adaptive behavior: Bob actively adjusts its position in response to the zombie's movement, tracking the enemy until it successfully defeats it.This indicates an emerging understanding of how to engage moving entities in the environment, highlighting the effectiveness of EXIF.</p>
<p>looking for scientific equipment with a price lower than $100." "Collect stone using wood pickaxe."ValidationTask 1 -Score 1 : 0.5 Validation Task 2 -Score 2 : 0.6Trajectory Feedback : Use more skills to get monster.</p>
<p>Figure 1 :
1
Figure1: Overview of our framework for automatic skill discovery through exploration and iterative feedback (EXIF), consisting of two main components: (1) an explore-first strategy that enables the agent, Alice, to navigate the environment and generate feasible, valid tasks, which are then used to train another agent, Bob; and (2) an iterative feedback mechanism that produces tasks and trajectories beyond Bob 's current capabilities to expand its skills.Through multiple iterations, EXIF enables Bob to expand its skill set in the target environment without any human guidance.</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Performance comparison of EXIF with feedback at each iteration versus EF, which scales data by generating more samples per iteration without feedback, on Webshop and Crafter using Qwen2.5-7B.Increasing the amount of data alone does not improve performance without feedback.</p>
<p>Figure 4 :
4
Figure 4: (a) The ratio of valid skill dataset among those generated using PF and EF approaches in Webshop and Crafter.(b) The average number of repeated actions (# R), average number of clicking attributes (# C), and average number of search keywords (# SW) by Bob, normalized by 20 for display, per iteration.(c) The skill distribution discovered by Alice in each iteration in Crafter.</p>
<p>Figure 5 :
5
Figure5: (a) Performance of Bob using the Qwen2.5-7Bmodel when Alice is Gpt-4o (red) or the Qwen2.5-7B(blue) model, investigating the potential of a self-evolving system (blue).(b) Ablation on whether using data from the previous iteration, where "Cumulative" means using data from previous iterations, and "Non-Cumulative" means not using data from the previous iteration.</p>
<p>5 :FeedbackF 8 :
58
(−1) ← null ▷ No feedback for the first iteration (k = 0) for k = 0 to K iter − 1 do</p>
<p>{trajectory} -----------------Based on these trajectories, provide concise feedback (2 points maximum) on what kinds of behaviors are desirable and undesirable during exploration.Keep the points very brief.Most importantly, for each point, write a brief guide on what you need to do during your exploration of the web shopping task on the next round.</p>
<p>Instruction: {instruction}Previous actions before the last action: {previous_actions} Current observation: {current_observation} Last action taken based on the current observation: {action} Why was this last action taken?Provide a rationale:</p>
<p>Comparison of Iteration 2 and
2
Iteration 3 of EXIF in Crafter Instruction: make_stone_sword Unsuccessful Trajectory (Iteration 2) "move_right → move_right → do → do . . .(repeated)" Successful Trajectory (Iteration 3) "move_right → move_right → do → move_left → do → move_up → do"'</p>
<p>) and the current observation o t .The objective is to produce a wide range of interaction sequences or trajectories, τ exp = (o 1 , a 1 , . . ., o T , a T ), capturing various feasible behaviors within the environment's constraints.To avoid excessive random behavior, we use weak constraints such as assigning a persona during exploration or setting a vague objective like survival in the game environment.Exploration continues until a termination condition is met (e.g., reaching a maximum step count T max ).This process yields an initial dataset of exploratory trajectories D exp = {τ Instruction generation To train Bob, we convert exploratory trajectories from Alice into a skill dataset.Alice analyzes each trajectory τ
(j)(j) exp } M j=1 .
Specifically, Alice interacts with the environment over time steps t = 1, ..., T , generating actionsa t ∼ π ϕ (•|h t , o t ) based solely on the interaction history h t = (o t−H , a t−H , ..., o t−1Exploration with feedback After the first iteration, exploration is conditioned on feedback from the previous iteration k (detailed in Section 2.3).The feedback F(k)guides Alice in generating a new skill dataset for the next round, k + 1, specifically tailored to address the shortcomings identified in Bob during iteration k.Alice 's action is now conditioned on the feedback: a t ∼ π ϕ (• | h t , o t , F (k) ), steering exploration toward behaviors and states relevant to the skills Bob lacks.</p>
<p>Table 1 :
1
Performance comparison of agents using different base LLMs (GPT-4o, Llama3.1-8B,Qwen2.5-7B) and methods across the Webshop and Crafter environments.Reward is the predefined reward in Webshop; SR denotes Success Rate in Webshop; NS is the number of learned skills in Crafter; and AP indicates the average progress rate in Crafter.For Reward, SR, NS, and AP, We report values in the format mean ±standard error (improvement over the base model) across multiple evaluations.# Iter.refers to the number of iterations conducted in the training process; # Traj.indicates the number of trajectories used to train the model throughout the entire training process.
Base LLM MethodWebshopCrafter# Iter. # Traj.RewardSR (%)# Iter. # Traj. NSAP (%)GPT-4oBase--16.5±1.411.4±1.2--1535.5±2.4Base--23.2±1.25.0±0.1--911.6±1.7Qwen2.5-7BPF EF1 1000 38.6±2.4 (+15.4) 6.6±1.0 (+1.6) 3 1 1000 42.1±0.2 (+18.9) 6.6±0.1 (+1.6) 1150 10 (+1) 24.5±8.3 (+12.9) 150 11 (+2) 26.2±2.6 (+14.6)EXIF (Ours) 4 1000 52.6±0.4 (+29.4) 12.4±0.1 (+7.4) 3150 15 (+6) 30.4±2.6 (+18.8)Base--2.0±0.10.0±0.0--711.4±1.2Llama3.1-8BPF EF1 1250 27.2±2.2 (+25.2) 2.0±0.0 (+2.0) 3 500 38.1±0.9 (+36.1) 3.0±0.0 (+3.0) 1150 11 (+4) 23.5±3.3 (+12.1) 150 12 (+5) 27.0±3.3 (+15.6)EXIF (Ours) 4 1000 53.7±1.2 (+51.7) 7.0±0.0 (+7.0) 3150 14 (+7) 31.9±3.1 (+20.5)</p>
<p>Table 2 :
2
Examples of feedback at each iteration.Critical parts that lead to changes in exploration are highlighted in bold.The current low reward is due to broad search queries.Use more . . .detailed search keywords . . .during your exploration.2. The current low reward . . .Avoid clicking the same item multiple times . . .during your exploration.
TaskIter.Feedback1.1Webshop
3 1.The model's initial search query . . .generate a detailed query that specifies . . .like small/medium.2. The model underutilizes attribute selection.Actively click on diverse attributes, . . .select specific size options.Crafter 1 Focus on practicing stone tool crafting and resource collection to improve progress on currently underexplored early survival tasks 3</p>
<p>Action SpaceThe environment exposes an 17-action discrete control space that can be grouped into five functional categories.Navigation actions allow single-tile movement in the four cardinal directions, supporting spatial exploration.Interaction enables direct engagement with the forward tile, including resource collection, and combat.Placement actions let the agent deploy terrainmodifying objects-stone blocks, crafting tables, furnaces, and plants-that serve as prerequisites for later tasks.Crafting actions synthesize tools and weapons when contextual requirements (nearby table or furnace) and inventory resources are satisfied.Finally, rest/idle actions restore internal energy or deliberately suspend activity, preserving the agent's state.
-coal 5 steps to your north-eastYou are facing path at your front (east direction)• Navigation: move_left, move_right, move_up, move_down• Interaction: do• Placement: place_stone, place_table, place_furnace, place_plant• Crafting: make_wood_pickaxe, make_wood_sword, make_stone_pickaxe,make_stone_sword, make_iron_pickaxe, make_iron_sword• Rest / Idle: sleep, noop5/9-food: 8/9-drink: 9/9-energy: 8/9Your inventory:-wood_pickaxe: 1-stone: 9-stone_pickaxe: 1-coal: 3-iron: 1-wood_sword: 1-stone_sword: 1You see:-water 2 steps to your west-grass 1 steps to your south-stone 3 steps to your east-path 1 steps to your east-sand 1 steps to your west</p>
<p>Your output <strong>must</strong> be a JSON object with the following two fields: { "behavior_analysis": "Describe what the agent has accomplished so far.Mention specific achievements (e.g., placing a table) and what those imply about the agent's current progression or intent.","next_iteration_advice": "Suggest a specific, actionable next step for the agent that would likely improve its capabilities or unlock new achievements.The advice should always start with 'Focus on...' and be</p>
<p>Table 3 :
3
Task skill categories, the full list of corresponding skills under each category, and descriptions of each skill used in Crafter.
Task Type Task NameDescriptioncollect_saplingGather saplings from the grassHarvestplace_plantPlace a plant on the groundeat_plantEat a plant to recover healthwake_upWake up after sleepingStatuseat_cowHunt a cowcollect_drinkDrink water in front of the rivercollect_woodChop trees to collect woodWoodplace_table make_wood_pickaxe Craft a wooden pickaxe Place a crafting tablemake_wood_swordCraft a wooden swordcollect_stoneMine stone blocksStonemake_stone_pickaxe Craft a stone pickaxe make_stone_sword Craft a stone swordplace_stonePlace a stone block in the groundcollect_coalMine coal blocksplace_furnacePlace a furnace for crafting advanced toolsIroncollect_iron make_iron_pickaxeMine iron blocks Craft an iron pickaxemake_iron_swordCraft an iron swordcollect_diamondMine diamond blocksHuntdefeat_skeleton defeat_zombieDefeat a skeleton enemy
https://huggingface.co/datasets/proj-persona/PersonaHub
AcknowledgementsWe thank Jimin Lee for extensive discussion on developing the framework and for help in outlining the figures.We also thank Minu Kim for discussions on the ideation of automatic skill discovery for LLM agents.Automated Skill Discovery for Language Agents through Exploration and Iterative FeedbackSupplementary MaterialA Limitation &amp; Broader ImpactLimitations While the proposed EXIF framework represents a significant step toward autonomous skill discovery, it has some limitations.First, the feedback mechanism-a core component of EXIF -relies on natural language, which requires accurate identification of weaknesses.Although it performs well on the benchmarks we evaluated, it may struggle in more complex environments.Second, we have not explored a version incorporating more predefined skill sets, as in Khan et al.[17].We plan to extend our work to these more diverse feedback settings and additional environments.Broader Impact The development of EXIF and similar autonomous skill discovery methods holds considerable broader impact for the advancement of artificial intelligence.By enabling agents to autonomously explore, learn, and continuously expand their capabilities without direct human intervention, this research paves the way for a new generation of more independent and adaptive AI systems.Such systems could revolutionize various domains beyond game playing and GUI manipulation, potentially leading to breakthroughs in scientific discovery, personalized education, and complex problem-solving in dynamic real-world scenarios.The ability of agents like Bob to generalize to unseen tasks based on self-generated, environment-grounded experiences could significantly reduce the reliance on costly human-annotated datasets, accelerating the deployment of capable AI in a wider array of applications and fostering the creation of truly intelligent systems that can adapt and grow with minimal human guidance.B Environment Details B.1 WebshopWe explain the details of the Webshop environment, covering the observation space, action space, the instructions used, and how the benchmark score is calculated.Observations The observation is a text-based web page, which can be a search page, a product list page, or an item description page.An example of a product list page is detailed below: -Do not include any explanation or text outside of the JSON block.-Do not list step-by-step logs or inventory diffs -summarize behavior abstractly.-Consider the agent's current resources and abilities to suggest realistic next goals.-Make sure the 'next_iteration_advice' sentence is specific and skill-oriented, not vague.Note: This is a <strong>partial trajectory</strong>, so analyze only what is visible.E Implementation details E.1 WebshopExploration During exploration, we run 250 episodes per round.Each episode has a maximum horizon of 10 steps.We only retain trajectories that end with a "buy now" action within this limit.During exploration, we provide previous actions but omit previous observations, as they may distract Alice 's decision-making.Additionally, we exclude search keywords from the previous actions to prevent the trajectory from resembling a proposal-based approach, where Alice would try every option to match the search keywords.Training We train Bob for a maximum of 200 steps with a total batch size of 64.We use the AdamW optimizer with a learning rate of 2e−5 and a weight decay of 0.01.We utilize LoRA adapters with a rank of 64.Training is performed on NVIDIA A6000 GPUs using DeepSpeed Stage 3 configuration.In Webshop, the model is trained from scratch at each iteration, as continuing from the previous checkpoint may hinder performance-especially when increasing the number of rounds-since excessive training might lead to loss of generalizability.E.2 CrafterExploration During exploration, we run 50 episodes per round, each with a maximum horizon of 100 steps.To collect a diverse set of task-relevant trajectories, each episode is initialized with randomized agent status and inventory configurations, constrained to ensure logical consistency (e.g., we exclude states where the agent possesses a stone pickaxe without having crafted or acquired a wood pickaxe).This setup encourages the agent to explore a broad range of achievable skills without relying on unrealistic initial conditions.Processing the trajectories To construct a high-quality skill dataset, we process the trajectory collected by Alice.We first segment the long-horizon trajectory into several segments by using a rule-based classifier.The rule-based classifier monitors the changes in the agent's observation information.Second, when a change is detected at time t, we define a skill trajectory as the four most recent observation-action pairs: (o t−3 , a t−3 , . . ., o t , a t ).Alice then labels these segments with corresponding skill instructions.Each iteration yields roughly 1500 observation-action pairs for Bob's training.Training We train our model using LoRA-based supervised fine-tuning with a rank of 16.The training is conducted for a total batch size of 32 using the AdamW optimizer with a learning rate of 1e−4.We leverage NVIDIA A6000 GPUs and adopt the DeepSpeed Stage 3 configuration to enable efficient large-scale training.We also follow the training scheme in WebShop, where we train the model from scratch at iteration k using the cumulative data up to iteration k.G More examples G.1 WebshopWe provide additional examples of Bob 's performance across iterations in WebShop.For better visualization, incorrect actions at each step are highlighted in red, while correct actions are shown in green.The example is presented below:
Joshua Achiam, Harrison Edwards, Dario Amodei, Pieter Abbeel, arXiv:1807.10299Variational option discovery algorithms. 2018arXiv preprint</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, arXiv:2310.05915Fireact: Toward language agent fine-tuning. 2023arXiv preprint</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems. 202336</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine, Advances in neural information processing systems. 2020</p>
<p>Guiding pretraining in reinforcement learning with large language models. Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas, International Conference on Machine Learning. 2023</p>
<p>Diversity is all you need: Learning skills without a reward function. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, arXiv:1802.060702018arXiv preprint</p>
<p>Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code. Maxence Faldor, Jenny Zhang, Antoine Cully, Jeff Clune, arXiv:2405.155682024arXiv preprint</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, International Conference on Machine Learning. 2018</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, arXiv:2407.217832024arXiv preprint</p>
<p>Benchmarking the spectrum of agent capabilities. Danijar Hafner, International Conference on Learning Representations. 2022</p>
<p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu, Webvoyager, arXiv:2401.13919Building an end-to-end web agent with large multimodal models. 2024arXiv preprint</p>
<p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, Dong Yu, arXiv:2410.19609Openwebvoyager: Building multimodal web agents via iterative real-world exploration, feedback and optimization. 2024arXiv preprint</p>
<p>Pokéllmon: A human-parity agent for pokémon battles with large language models. Sihao Hu, Tiansheng Huang, Ling Liu, arXiv:2402.011182024arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR2022</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Illuminating generalization in deep reinforcement learning through procedural level generation. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, Sebastian Risi, arXiv:1806.107292018arXiv preprint</p>
<p>Dataenvgym: Data generation agents in teacher environments with student feedback. Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal, arXiv:2410.062152024arXiv preprint</p>
<p>Jing Yu Koh, Stephen Mcaleer, Daniel Fried, Ruslan Salakhutdinov, arXiv:2407.01476Tree search for language model agents. 2024arXiv preprint</p>
<p>Autowebglm: A large language modelbased web navigating agent. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Unsupervised reinforcement learning with contrastive intrinsic control. Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, Pieter Abbeel, Advances in Neural Information Processing Systems. 2022</p>
<p>Juyong Lee, Taywon Min, Minyong An, Dongyoon Hahm, Haeone Lee, Changyeon Kim, Kimin Lee, arXiv:2404.16660Benchmarking mobile device control agents across diverse configurations. 2024arXiv preprint</p>
<p>Hao Liu, Alexander Trott, Richard Socher, Caiming Xiong, arXiv:1902.00528Competitive experience replay. 2019arXiv preprint</p>
<p>Xing Han Lù, Zdeněk Kasner, Siva Reddy, Weblinx, arXiv:2402.05930Real-world website navigation with multi-turn dialogue. 2024arXiv preprint</p>
<p>Shikhar Murty, Dzmitry Bahdanau, Christopher D Manning, arXiv:2410.02907Nnetscape navigator: Complex demonstrations for web agents without a demonstrator. 2024arXiv preprint</p>
<p>Bagel: Bootstrapping agents by guiding exploration with language. Shikhar Murty, Christopher D Manning, Peter Shaw, Mandar Joshi, Kenton Lee, International Conference on Machine Learning. PMLR2024</p>
<p>Lift: Unsupervised reinforcement learning with foundation models as teachers. Taewook Nam, Juyong Lee, Jesse Zhang, Sung Ju Hwang, Joseph J Lim, Karl Pertsch, arXiv:2312.089582023arXiv preprint</p>
<p>Asymmetric self-play for automatic goal discovery in robotic manipulation. Openai Openai, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, D' Ruben, Arthur Sa, Henrique P Petron, Pinto, arXiv:2101.048822021arXiv preprint</p>
<p>Balrog: Benchmarking agentic llm and vlm reasoning on games. Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, International Conference on Learning Representations. 2025</p>
<p>Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents. Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, Ahmed Awadallah, arXiv:2502.113572025arXiv preprint</p>
<p>Lipschitzconstrained unsupervised skill discovery. Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, Gunhee Kim, International Conference on Learning Representations. 2022</p>
<p>Accelerating reinforcement learning with learned skill priors. Karl Pertsch, Youngwoon Lee, Joseph Lim, Conference on robot learning. 2021</p>
<p>Skew-fit: State-covering self-supervised reinforcement learning. Murtaza Vitchyr H Pong, Steven Dalal, Ashvin Lin, Shikhar Nair, Sergey Bahl, Levine, arXiv:1903.036982019arXiv preprint</p>
<p>Androidworld: A dynamic benchmarking environment for autonomous agents. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, International Conference on Learning Representations. 2025</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 2023</p>
<p>Learn-byinteract: A data-centric framework for self-adaptive agents in realistic environments. Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö Arık, arXiv:2501.108932025arXiv preprint</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 1999</p>
<p>Openended learning leads to generally capable agents. Adam Team, Anuj Stooke, Catarina Mahajan, Charlie Barros, Jakob Deck, Jakub Bauer, Maja Sygnowski, Max Trebacz, Michael Jaderberg, Mathieu, arXiv:2107.12808Open Ended Learning. 2021arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. Rui Wang, Joel Lehman, Jeff Clune, Kenneth O Stanley, arXiv:1901.017532019arXiv preprint</p>
<p>Benchmarking multimodal agents for open-ended tasks in real computer environments. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, J Toh, Zhoujun Hua, Dongchan Cheng, Fangyu Shin, Lei, Advances in Neural Information Processing Systems. 202437</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 2022</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations. 2023</p>
<p>Skill reinforcement learning and planning for open-world long-horizon tasks. Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, Zongqing Lu, arXiv:2303.165632023arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Omni: Open-endedness via models of human notions of interestingness. Jenny Zhang, Joel Lehman, Kenneth Stanley, Jeff Clune, arXiv:2306.017112023arXiv preprint</p>
<p>Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, Joseph J Lim, arXiv:2310.10021Bootstrap your own skills: Learning to solve new tasks with large language model guidance. 2023arXiv preprint</p>
<p>Boyuan Zheng, Xiaolong Michael Y Fatemi, Zora Zhiruo Jin, Apurva Wang, Yueqi Gandhi, Yu Song, Jayanth Gu, Gaowen Srinivasa, Graham Liu, Neubig, arXiv:2504.07079Web agents can self-improve by discovering and honing skills. 2025arXiv preprint</p>
<p>Webarena: A realistic web environment for building autonomous agents. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig, International Conference on Learning Representations. 2024</p>
<p>Proposer-agent-evaluator (pae): Autonomous skill discovery for foundation model internet agents. Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, Erran Li, arXiv:2412.131942024arXiv preprint</p>
<p>Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar, Archer, arXiv:2402.19446Training language model agents via hierarchical multi-turn rl. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
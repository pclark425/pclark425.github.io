<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5019 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5019</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5019</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-550f7dba7757f22afd87786749509574609d6180</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/550f7dba7757f22afd87786749509574609d6180" target="_blank">Hinge-Loss Markov Random Fields and Probabilistic Soft Logic</a></p>
                <p><strong>Paper Venue:</strong> Journal of machine learning research</p>
                <p><strong>Paper TL;DR:</strong> An algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures.</p>
                <p><strong>Paper Abstract:</strong> A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5019",
    "paper_id": "paper-550f7dba7757f22afd87786749509574609d6180",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004562999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hinge-Loss Markov Random Fields and Probabilistic Soft Logic</h1>
<p>Stephen H. Bach<br>BACH@CS.STANFORD.EDU<br>Computer Science Department<br>Stanford University<br>Stanford, CA 94305, USA<br>Matthias Broecheler<br>Matthias@DataStax.COM<br>DataStax<br>Bert Huang<br>BHUANG@VT.EDU<br>Computer Science Department<br>Virginia Tech<br>Blacksburg, VA 24061, USA<br>Lise Getoor<br>GETOOR@SOE.UCSC.EDU<br>Computer Science Department<br>University of California, Santa Cruz<br>Santa Cruz, CA 95064, USA</p>
<p>Editor: Luc De Raedt</p>
<h4>Abstract</h4>
<p>A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hingeloss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.</p>
<p>Keywords: Probabilistic graphical models, statistical relational learning, structured prediction</p>
<h1>1. Introduction</h1>
<p>In many problems in machine learning, the domains are rich and structured, with many interdependent elements that are best modeled jointly. Examples include social networks, biological networks, the Web, natural language, computer vision, sensor networks, and so on. Machine learning subfields such as statistical relational learning (Getoor and Taskar, 2007), inductive logic programming (Muggleton and De Raedt, 1994), and structured prediction (Bakir et al., 2007) all seek to represent dependencies in data induced by relational structure. With the ever-increasing size of available data, there is a growing need for models that are highly scalable while still able to capture rich structure.</p>
<p>In this paper, we introduce hinge-loss Markov random fields (HL-MRFs), a new class of probabilistic graphical models designed to enable scalable modeling of rich, structured data. HL-MRFs are analogous to discrete MRFs, which are undirected probabilistic graphical models in which probability mass is log-proportional to a weighted sum of feature functions. Unlike discrete MRFs, however, HL-MRFs are defined over continuous variables in the $[0,1]$ unit interval. To model dependencies among these continuous variables, we use linear and quadratic hinge functions, so that probability density is lost according to a weighted sum of hinge losses. As we will show, hinge-loss features capture many common modeling patterns for structured data.</p>
<p>When designing classes of models, there is generally a trade off between scalability and expressivity: the more complex the types and connectivity structure of the dependencies, the more computationally challenging inference and learning become. HL-MRFs address a crucial gap between the two extremes. By using hinge-loss functions to model the dependencies among the variables, which admit highly scalable inference without restrictions on their connectivity structure, HL-MRFs can capture a wide range of useful relationships. One reason they are so expressive is that hinge-loss dependencies are at the core of a number of scalable techniques for modeling both discrete and continuous structured data.</p>
<p>To motivate HL-MRFs, we unify three different approaches for scalable inference in structured models: (1) randomized algorithms for MAX SAT (Goemans and Williamson, 1994), (2) local consistency relaxation (Wainwright and Jordan, 2008) for discrete Markov random fields defined using Boolean logic, and (3) reasoning about continuous information with fuzzy logic. We show that all three approaches lead to the same convex programming objective. We then define HL-MRFs by generalizing this unified inference objective as a weighted sum of hinge-loss features and using them as the weighted features of graphical models. Since HL-MRFs generalize approaches that reason about relational data with weighted logical knowledge bases, they retain the same high level of expressivity. As we show in Section 6.4, they are effective for modeling both discrete and continuous data.</p>
<p>We also introduce probabilistic soft logic (PSL), a new probabilistic programming language that makes HL-MRFs easy to define and use for large, relational data sets. ${ }^{1}$ This idea has been explored for other classes of models, such as Markov logic networks (Richardson and Domingos, 2006) for discrete MRFs, relational dependency networks (Neville and Jensen, 2007) for dependency networks, and probabilistic relational models (Getoor et al., 2002) for Bayesian networks. We build on these previous approaches, as well as the connection between hinge-loss potentials and logical clauses, to define PSL. In addition to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>probabilistic rules, PSL provides syntax that enables users to easily apply many common modeling techniques, such as domain and range constraints, blocking and canopy functions, and aggregate variables defined over other random variables.</p>
<p>Our next contribution is to introduce a number of inference and learning algorithms. First, we examine MAP inference, i.e., the problem of finding a most probable assignment to the unobserved random variables. MAP inference in HL-MRFs is always a convex optimization. Although any off-the-shelf optimization toolkit could be used, such methods typically do not leverage the sparse dependency structures common in graphical models. We introduce a consensus-optimization approach to MAP inference for HL-MRFs, showing how the problem can be decomposed using the alternating direction method of multipliers (ADMM) and how the resulting subproblems can be solved analytically for hinge-loss potentials. Our approach enables HL-MRFs to easily scale beyond the capabilities of off-the-shelf optimization software or sampling-based inference in discrete MRFs. We then show how to learn HL-MRFs from training data using a variety of methods: structured perceptron, maximum pseudolikelihood, and large margin estimation. Since structured perceptron and large margin estimation rely on inference as subroutines, and maximum pseudolikelihood estimation is efficient by design, all of these methods are highly scalable for HL-MRFs. We evaluate them on core relational learning and structured prediction tasks, such as collective classification and link prediction. We show that HL-MRFs offer predictive accuracy comparable to analogous discrete models while scaling much better to large data sets.</p>
<p>This paper brings together and expands work on scalable models for structured data that can be either discrete, continuous, or a mixture of both (Broecheler et al., 2010a; Bach et al., 2012, 2013, 2015b). The effectiveness of HL-MRFs and PSL has been demonstrated on many problems, including information extraction (Liu et al., 2016) and automatic knowledge base construction (Pujara et al., 2013), extracting and evaluating natural-language arguments on the Web (Samadi et al., 2016), high-level computer vision (London et al., 2013), drug discovery (Fakhraei et al., 2014) and predicting drug-drug interactions (Sridhar et al., 2016), natural language semantics (Beltagy et al., 2014; Sridhar et al., 2015; Deng and Wiebe, 2015; Ebrahimi et al., 2016), automobile-traffic modeling (Chen et al., 2014), recommender systems (Kouki et al., 2015), information retrieval (Alshukaili et al., 2016), and predicting attributes (Li et al., 2014) and trust (Huang et al., 2013; West et al., 2014) in social networks. The ability to easily incorporate latent variables into HL-MRFs and PSL (Bach et al., 2015a) has enabled further applications, including modeling latent topics in text (Foulds et al., 2015), and predicting student outcomes in massive open online courses (MOOCs) (Ramesh et al., 2014, 2015). Researchers have also studied how to make HL-MRFs and PSL even more scalable by developing distributed implementations (Miao et al., 2013; Magliacane et al., 2015). That they are already being widely applied indicates HL-MRFs and PSL address an open need in the machine learning community.</p>
<p>The paper is organized as follows. In Section 2, we first consider models for structured prediction that are defined using logical clauses. We unify three different approaches to scalable inference in such models, showing that they all optimize the same convex objective. We then generalize this objective in Section 3 to define HL-MRFs. In Section 4, we introduce PSL, specifying the language and giving many examples of common usage. Next we introduce a scalable message-passing algorithm for MAP inference in Section 5 and a</p>
<p>number of learning algorithms in Section 6, evaluating them on a range of tasks. Finally, in Section 7, we discuss related work.</p>
<h1>2. Unifying Convex Inference for Logic-Based Graphical Models</h1>
<p>In many structured domains, propositional and first-order logics are useful tools for describing the intricate dependencies that connect the unknown variables. However, these domains are usually noisy; dependencies among the variables do not always hold. To address this, logical semantics can be incorporated into probability distributions to create models that capture both the structure and the uncertainty in machine learning tasks. One common way to do this is to use logic to define feature functions in a probabilistic model. We focus on Markov random fields (MRFs), a popular class of probabilistic graphical models. Informally, an MRF is a distribution that assigns probability mass using a scoring function that is a weighted combination of feature functions called potentials. We will use logical clauses to define these potentials. We first define MRFs more formally to introduce necessary notation:</p>
<p>Definition 1 Let $\boldsymbol{x}=\left(x_{1}, \ldots, x_{n}\right)$ be a vector of random variables and let $\boldsymbol{\phi}=\left(\phi_{1}, \ldots, \phi_{m}\right)$ be a vector of potentials where each potential $\phi_{j}(\boldsymbol{x})$ assigns configurations of the variables a real-valued score. Also, let $\boldsymbol{w}=\left(w_{1}, \ldots, w_{m}\right)$ be a vector of real-valued weights. Then, a Markov random field is a probability distribution of the form</p>
<p>$$
P(\boldsymbol{x}) \propto \exp \left(\boldsymbol{w}^{\top} \boldsymbol{\phi}(\boldsymbol{x})\right)
$$</p>
<p>In an MRF, the potentials should capture how the domain behaves, assigning higher scores to more probable configurations of the variables. If a modeler does not know how the domain behaves, the potentials should capture how it might behave, so that a learning algorithm can find weights that lead to accurate predictions. Logic provides an excellent formalism for defining such potentials in structured and relational domains.</p>
<p>We now introduce some notation to make this logic-based approach more formal. Consider a set of logical clauses $\boldsymbol{C}=\left{C_{1}, \ldots, C_{m}\right}$, i.e., a knowledge base, where each clause $C_{j} \in \boldsymbol{C}$ is a disjunction of literals and each literal is a variable $x$ or its negation $\neg x$ drawn from the variables $\boldsymbol{x}$ such that each variable $x_{i} \in \boldsymbol{x}$ appears at most once in $C_{j}$. Let $I_{j}^{+}$(resp. $I_{j}^{-}$) $\subset{1, \ldots, n}$ be the set of indices of the variables that are not negated (resp. negated) in $C_{j}$. Then $C_{j}$ can be written as</p>
<p>$$
\left(\bigvee_{i \in I_{j}^{+}} x_{i}\right) \bigvee\left(\bigvee_{i \in I_{j}^{-}} \neg x_{i}\right)
$$</p>
<p>Logical clauses of this form are expressive because they can be viewed equivalently as implications from conditions to consequences:</p>
<p>$$
\bigwedge_{i \in I_{j}^{-}} x_{i} \Longrightarrow \bigvee_{i \in I_{j}^{+}} x_{i}
$$</p>
<p>This "if-then" reasoning is intuitive and can describe many dependencies in structured data.</p>
<p>Assuming we have a logical knowledge base $\boldsymbol{C}$ describing a structured domain, we can embed it in an MRF by defining each potential $\phi_{j}$ using a corresponding clause $C_{j}$. If an assignment to the variables $\boldsymbol{x}$ satisfies $C_{j}$, then we let $\phi_{j}(\boldsymbol{x})$ equal 1 , and we let it equal 0 otherwise. For our subsequent analysis we assume $w_{j} \geq 0(\forall j=1, \ldots, m)$. The resulting MRF preserves the structured dependencies described in $\boldsymbol{C}$ but enables much more flexible modeling. Clauses no longer must always hold, and the model can express uncertainty over different possible worlds. The weights express how strongly the model expects each corresponding clause to hold; the higher the weight, the more probable that it is true according to the model.</p>
<p>This notion of embedding weighted, logical knowledge bases in MRFs is an appealing one. For example, Markov logic (Richardson and Domingos, 2006) is a popular formalism that induces MRFs from weighted first-order knowledge bases. Given a data set, the firstorder clauses are grounded using the constants in the data to create the set of propositional clauses $\boldsymbol{C}$. Each propositional clause has the weight of the first-order clause from which it was grounded. In this way, a weighted, first-order knowledge base can compactly specify an entire family of MRFs for a structured machine-learning task.</p>
<p>Although we now have a method for easily defining rich, structured models for a wide range of problems, there is a new challenge: finding a most probable assignment to the variables, i.e., MAP inference, is NP-hard (Shimony, 1994; Garey et al., 1976). This means that (unless $\mathrm{P}=\mathrm{NP}$ ) our only hope for performing tractable inference is to perform it approximately. Observe that MAP inference for an MRF defined by $\boldsymbol{C}$ is the integer linear program</p>
<p>$$
\begin{aligned}
\underset{\boldsymbol{x} \in{0,1}^{n}}{\arg \max } P(\boldsymbol{x}) &amp; \equiv \underset{\boldsymbol{x} \in{0,1}^{n}}{\arg \max } \boldsymbol{w}^{\top} \boldsymbol{\phi}(\boldsymbol{x}) \
&amp; \equiv \underset{\boldsymbol{x} \in{0,1}^{n}}{\arg \max } \sum_{C_{j} \in \boldsymbol{C}} w_{j} \min \left{\sum_{i \in I_{j}^{\wedge}} x_{i}+\sum_{i \in I_{j}^{\wedge}}\left(1-x_{i}\right), 1\right}
\end{aligned}
$$</p>
<p>While this program is intractable, it does admit convex programming relaxations.
In this section, we show how convex programming can be used to perform tractable inference in MRFs defined by weighted knowledge bases. We first discuss in Section 2.1 an approach developed by Goemans and Williamson (1994) that views MAP inference as an instance of the classic MAX SAT problem and relaxes it to a convex program from that perspective. This approach has the advantage of providing strong guarantees on the quality of the discrete solutions it obtains. However, it has the disadvantage that general-purpose convex programming toolkits do not scale well to relaxed MAP inference for large graphical models (Yanover et al., 2006). In Section 2.2 we then discuss a seemingly distinct approach, local consistency relaxation, with complementary advantages and disadvantages: it offers highly scalable message-passing algorithms but comes with no quality guarantees. We then unite these approaches by proving that they solve equivalent optimization problems with identical solutions. Then, in Section 2.3, we show that the unified inference objective is also equivalent to exact MAP inference if the knowledge base $\boldsymbol{C}$ is interpreted using ≈Åukasiewicz logic, an infinite-valued logic for reasoning about naturally continuous quantities such as similarity, vague or fuzzy concepts, and real-valued data.</p>
<h1>Bach, Broecheler, Huang, and Getoor</h1>
<p>That these three interpretations all lead to the same inference objective-whether reasoning about discrete or continuous information-is useful. To the best of our knowledge, we are the first to show their equivalence. This equivalence indicates that the same modeling formalism, inference algorithms, and learning algorithms can be used to reason scalably and accurately about both discrete and continuous information in structured domains. We generalize the unified inference objective in Section 3.1 to define hinge-loss MRFs, and in the rest of the paper we develop a probabilistic programming language and algorithms that realize the goal of a scalable and accurate framework for structured data, both discrete and continuous.</p>
<h3>2.1 MAX SAT Relaxation</h3>
<p>One approach to approximating objective (4) is to use relaxation techniques developed in the randomized algorithms community for the MAX SAT problem. Formally, the MAX SAT problem is to find a Boolean assignment to a set of variables that maximizes the total weight of satisfied clauses in a knowledge base composed of disjunctive clauses annotated with nonnegative weights. In other words, objective (4) is an instance of MAX SAT. Randomized approximation algorithms can be constructed for MAX SAT by independently rounding each Boolean variable $x_{i}$ to true with probability $p_{i}$. Then, the expected weighted satisfaction $\hat{w}<em j="j">{j}$ of a clause $C</em>$ is</p>
<p>$$
\hat{w}<em j="j">{j}=w</em>\right)
$$}\left(1-\prod_{i \in I_{j}^{+}}\left(1-p_{i}\right) \prod_{i \in I_{j}^{-}} p_{i</p>
<p>also known as a (weighted) noisy-or function, and the expected total score $\hat{W}$ is</p>
<p>$$
\hat{W}=\sum_{C_{j} \in \boldsymbol{C}} w_{j}\left(1-\prod_{i \in I_{j}^{+}}\left(1-p_{i}\right) \prod_{i \in I_{j}^{-}} p_{i}\right)
$$</p>
<p>Optimizing $\hat{W}$ with respect to the rounding probabilities would give the exact MAX SAT solution, so this randomized approach has not made the problem any easier yet, but Goemans and Williamson (1994) showed how to bound $\hat{W}$ below with a tractable linear program.</p>
<p>To approximately optimize $\hat{W}$, associate with each Boolean variable $x_{i}$ a corresponding continuous variable $\hat{y}_{i}$ with domain $[0,1]$. Then let $\hat{\boldsymbol{y}}^{\star}$ be the optimum of the linear program</p>
<p>$$
\underset{\hat{\boldsymbol{y}} \in[0,1]^{n}}{\arg \max } \sum_{C_{j} \in \boldsymbol{C}} w_{j} \min \left{\sum_{i \in I_{j}^{+}} \hat{y}<em I__j="I_{j" _in="\in" i="i">{i}+\sum</em>\right), 1\right}
$$}^{-}}\left(1-\hat{y}_{i</p>
<p>Observe that objectives (4) and (7) are of the same form, except that the variables are relaxed to the unit hypercube in objective (7). Goemans and Williamson (1994) proved that if $p_{i}$ is set to $\hat{y}<em i="i">{i}^{\star}$ for all $i$, then $\hat{W} \geq .632 Z^{\star}$, where $Z^{\star}$ is the optimal total weight for the MAX SAT problem. If each $p</em>$ is set using any function in a special class, then this</p>
<p>lower bound improves to a .75 approximation. One simple example of such a function is</p>
<p>$$
p_{i}=\frac{1}{2} \tilde{g}_{i}^{*}+\frac{1}{4}
$$</p>
<p>In this way, objective (7) leads to an expected .75 approximation of the MAX SAT solution.
The following method of conditional probabilities (Alon and Spencer, 2008) can find a single Boolean assignment that achieves at least the expected score from a set of rounding probabilities, and therefore at least .75 of the MAX SAT solution when objective (7) and function (8) are used to obtain them. Each variable $x_{i}$ is greedily set to the value that maximizes the expected weight over the unassigned variables, conditioned on either possible value of $x_{i}$ and the previously assigned variables. This greedy maximization can be applied quickly because, in many models, variables only participate in a small fraction of the clauses, making the change in expectation quick to compute for each variable. Specifically, referring to the definition of $\hat{W}(6)$, the assignment to $x_{i}$ only needs to maximize over the clauses $C_{j}$ in which $x_{i}$ participates, i.e., $i \in I_{j}^{+} \cup I_{j}^{-}$, which is usually a small set.</p>
<p>This approximation is powerful because it is a tractable linear program that comes with strong guarantees on solution quality. However, even though it is tractable, generalpurpose convex optimization toolkits do not scale well to large MAP problems. In the following subsection, we unify this approximation with a complementary one developed in the probabilistic graphical models community.</p>
<h1>2.2 Local Consistency Relaxation</h1>
<p>Another approach to approximating objective (4) is to apply a relaxation developed for Markov random fields called local consistency relaxation (Wainwright and Jordan, 2008). This approach starts by viewing MAP inference as an equivalent optimization over marginal probabilities. ${ }^{2}$ For each $\phi_{j} \in \boldsymbol{\phi}$, let $\boldsymbol{\theta}<em j="j">{j}$ be a marginal distribution over joint assignments $\boldsymbol{x}</em>}$. For example, $\theta_{j}\left(\boldsymbol{x<em j="j">{j}\right)$ is the probability that the subset of variables associated with potential $\phi</em>}$ is in a particular joint state $\boldsymbol{x<em j="j">{j}$. Also, let $x</em>$.}(i)$ denote the setting of the variable with index $i$ in the state $\boldsymbol{x}_{j</p>
<p>With this variational formulation, inference can be relaxed to an optimization over the first-order local polytope $\mathbb{L}$. Let $\boldsymbol{\mu}=\left(\mu_{1}, \ldots, \mu_{n}\right)$ be a vector of probability distributions, where $\mu_{i}(k)$ is the marginal probability that $x_{i}$ is in state $k$. The first-order local polytope is</p>
<p>$$
\mathbb{L} \triangleq\left{\begin{array}{l|l}
\left(\boldsymbol{\theta}, \boldsymbol{\mu}\right) \geq \mathbf{0} &amp; \sum_{\boldsymbol{x}<em j="j">{j} \mid x</em>}(i)=k} \theta_{j}\left(\boldsymbol{x<em i="i">{j}\right)=\mu</em>(k) &amp; \forall i, j, k \
\sum_{\boldsymbol{x}<em j="j">{j}} \theta</em>}\left(\boldsymbol{x<em k="0">{j}\right)=1 &amp; \forall j \
\sum</em>(k)=1 &amp; \forall i
\end{array}\right}
$$}^{K_{i}-1} \mu_{i</p>
<p>which constrains each marginal distribution $\boldsymbol{\theta}<em j="j">{j}$ over joint states $\boldsymbol{x}</em>$.}$ to be consistent only with the marginal distributions $\boldsymbol{\mu}$ over individual variables that participate in the potential $\phi_{j</p>
<p>MAP inference can then be approximated with the first-order local consistency relaxation:</p>
<p>$$
\underset{\left(\boldsymbol{\theta}, \boldsymbol{\mu}\right) \in \mathbb{L}}{\arg \max } \sum_{j=1}^{m} w_{j} \sum_{\boldsymbol{x}<em j="j">{j}} \theta</em>}\left(\boldsymbol{x<em j="j">{j}\right) \phi</em>\right)
$$}\left(\boldsymbol{x}_{j</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>which is an upper bound on the true MAP objective. Much work has focused on solving the first-order local consistency relaxation for large-scale MRFs, which we discuss further in Section 7. These algorithms are appealing because they are well-suited to the sparse dependency structures common in MRFs, so they can scale to large problems. However, in general, the solutions can be fractional, and there are no guarantees on the approximation quality of a tractable discretization of these fractional solutions.</p>
<p>We show that for MRFs with potentials defined by $\boldsymbol{C}$ and nonnegative weights, local consistency relaxation is equivalent to MAX SAT relaxation.</p>
<p>Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and associated nonnegative weights, the first-order local consistency relaxation of MAP inference is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Specifically, any partial optimum $\boldsymbol{\mu}^{\star}$ of objective (10) is an optimum $\hat{\boldsymbol{y}}^{\star}$ of objective (7), and vice versa.</p>
<p>We prove Theorem 2 in Appendix A. Our proof analyzes the local consistency relaxation to derive an equivalent, more compact optimization over only the variable pseudomarginals $\boldsymbol{\mu}$ that is identical to the MAX SAT relaxation. Theorem 2 is significant because it shows that the rounding guarantees of MAX SAT relaxation also apply to local consistency relaxation, and the scalable message-passing algorithms developed for local consistency relaxation also apply to MAX SAT relaxation.</p>
<h1>2.3 Lukasiewicz Logic</h1>
<p>The previous two subsections showed that the same convex program can approximate MAP inference in discrete, logic-based models, whether viewed from the perspective of randomized algorithms or variational methods. In this subsection, we show that this convex program can also be used to reason about naturally continuous information, such as similarity, vague or fuzzy concepts, and real-valued data. Instead of interpreting the clauses $\boldsymbol{C}$ using Boolean logic, we can interpret them using Lukasiewicz logic (Klir and Yuan, 1995), which extends Boolean logic to infinite-valued logic in which the propositions $\boldsymbol{x}$ can take truth values in the continuous interval $[0,1]$. Extending truth values to a continuous domain enables them to represent concepts that are vague, in the sense that they are often neither completely true nor completely false. For example, the propositions that a sensor value is high, two entities are similar, or a protein is highly expressed can all be captured in a more nuanced manner in Lukasiewicz logic. We can also use the now continuous valued $\boldsymbol{x}$ to represent quantities that are naturally continuous (scaled to $[0,1]$ ), such as actual sensor values, similarity scores, and protein expression levels. The ability to reason about continuous values is valuable, as many important applications are not entirely discrete.</p>
<p>The extension to continuous values requires a corresponding extended interpretation of the logical operators $\wedge$ (conjunction), $\vee$ (disjunction), and $\neg$ (negation). The Lukasiewicz t-norm and t-co-norm are $\wedge$ and $\vee$ operators that correspond to the Boolean logic operators for integer inputs (along with the negation operator $\neg$ ):</p>
<p>$$
\begin{aligned}
x_{1} \wedge x_{2} &amp; =\max \left{x_{1}+x_{2}-1,0\right} \
x_{1} \vee x_{2} &amp; =\min \left{x_{1}+x_{2}, 1\right} \
\neg x &amp; =1-x
\end{aligned}
$$</p>
<p>The analogous MAX SAT problem for Lukasiewicz logic is therefore</p>
<p>$$
\underset{\boldsymbol{x} \in[0,1]^{n}}{\arg \max } \sum_{C_{j} \in \boldsymbol{C}} w_{j} \min \left{\sum_{i \in I_{j}^{+}} x_{i}+\sum_{i \in I_{j}^{-}}\left(1-x_{i}\right), 1\right}
$$</p>
<p>which is identical in form to the relaxed MAX SAT objective (7). Therefore, if an MRF is defined over continuous variables with domain $[0,1]^{n}$ and the logical knowledge base $\boldsymbol{C}$ defining the potentials is interpreted using Lukasiewicz logic, then exact MAP inference is identical to finding the optimum using the unified, relaxed inference objective derived for Boolean logic in the previous two subsections. This result shows the equivalence of all three approaches: MAX SAT relaxation, local consistency relaxation, and MAX SAT using Lukasiewicz logic.</p>
<h1>3. Hinge-Loss Markov Random Fields</h1>
<p>We have shown that a specific family of convex programs can be used to reason scalably and accurately about both discrete and continuous information. In this section, we generalize this family to define hinge-loss Markov random fields (HL-MRFs), a new kind of probabilistic graphical model. HL-MRFs retain the convexity and expressivity of convex programs discussed in Section 2, and additionally support an even richer space of dependencies.</p>
<p>To begin, we define HL-MRFs as density functions over continuous variables $\boldsymbol{y}=$ $\left(y_{1}, \ldots, y_{n}\right)$ with joint domain $[0,1]^{n}$. These variables have different possible interpretations depending on the application. Since we are generalizing the interpretations explored in Section 2, HL-MRF MAP states can be viewed as rounding probabilities or pseudomarginals, or they can represent naturally continuous information. More generally, they can be viewed simply as degrees of belief, confidences, or rankings of possible states; and they can describe discrete, continuous, or mixed domains. The application domain typically determines which interpretation is most appropriate. The formalisms and algorithms described in the rest of this paper are general with respect to such interpretations.</p>
<h3>3.1 Generalized Inference Objective</h3>
<p>To define HL-MRFs, we will first generalize the unified inference objective of Section 2 in several ways, which we first restate in terms of the HL-MRF variables $\boldsymbol{y}$ :</p>
<p>$$
\underset{\boldsymbol{y} \in[0,1]^{n}}{\arg \max } \sum_{C_{j} \in \boldsymbol{C}} w_{j} \min \left{\sum_{i \in I_{j}^{+}} y_{i}+\sum_{i \in I_{j}^{-}}\left(1-y_{i}\right), 1\right}
$$</p>
<p>For now, we are still assuming that the objective terms are defined using a weighted knowledge base $\boldsymbol{C}$, but we will quickly drop this requirement. To do so, we examine one term in isolation. Observe that the maximum value of any unweighted term is 1 , which is achieved when a linear function of the variables is at least 1 . We say that the term is satisfied whenever this occurs. When a term is unsatisfied, we can refer to its distance to satisfaction, which is how far it is from achieving its maximum value. Also observe that we can rewrite</p>
<p>the optimization explicitly in terms of distances to satisfaction:</p>
<p>$$
\underset{\boldsymbol{y} \in[0,1]^{n}}{\arg \min } \sum_{C_{j} \in \boldsymbol{C}} w_{j} \max \left{1-\sum_{i \in I_{j}^{+}} y_{i}-\sum_{i \in I_{j}^{-}}\left(1-y_{i}\right), 0\right}
$$</p>
<p>so that the objective is equivalently to minimize the total weighted distance to satisfaction. Each unweighted objective term now measures how far the linear constraint</p>
<p>$$
1-\sum_{i \in I_{j}^{+}} y_{i}-\sum_{i \in I_{j}^{-}}\left(1-y_{i}\right) \leq 0
$$</p>
<p>is from being satisfied.</p>
<h1>3.1.1 Relaxed Linear Constraints</h1>
<p>With this view of each term as a relaxed linear constraint, we can easily generalize them to arbitrary linear constraints. We no longer require that the inference objective be defined using only logical clauses, and instead each term can be defined using any function $\ell_{j}(\boldsymbol{y})$ that is linear in $\boldsymbol{y}$. These functions can capture more general dependencies, such as beliefs about the range of values a variable can take and arithmetic relationships among variables.</p>
<p>The new inference objective is</p>
<p>$$
\underset{\boldsymbol{y} \in[0,1]^{n}}{\arg \min } \sum_{j=1}^{m} w_{j} \max \left{\ell_{j}(\boldsymbol{y}), 0\right}
$$</p>
<p>In this form, each term represents the distance to satisfaction of a linear constraint $\ell_{j}(\boldsymbol{y}) \leq 0$. That constraint could be defined using logical clauses as discussed above, or it could be defined using other knowledge about the domain. The weight $w_{j}$ indicates how important it is to satisfy a constraint relative to others by scaling the distance to satisfaction. The higher the weight, the more distance to satisfaction is penalized. Additionally, two relaxed inequality constraints, $\ell_{j}(\boldsymbol{y}) \leq 0$ and $-\ell_{j}(\boldsymbol{y}) \leq 0$, can be combined to represent a relaxed equality constraint $\ell_{j}(\boldsymbol{y})=0$.</p>
<h3>3.1.2 Hard Linear Constraints</h3>
<p>Now that our inference objective admits arbitrary relaxed linear constraints, it is natural to also allow hard constraints that must be satisfied at all times. Hard constraints are important modeling tools. They enable groups of variables to represent mutually exclusive possibilities, such as a multinomial or categorical variable, and functional or partial functional relationships. Hard constraints can also represent background knowledge about the domain, restricting the domain to regions that are feasible in the real world. Additionally, they can encode more complex model components such as defining a random variable as an aggregate over other unobserved variables, which we discuss further in Section 4.3.5.</p>
<p>We can think of including hard constraints as allowing a weight $w_{j}$ to take an infinite value. Again, two inequality constraints can be combined to represent an equality constraint. However, when we introduce an inference algorithm for HL-MRFs in Section 5, it</p>
<p>will be useful to treat hard constraints separately from relaxed ones, and further, treat hard inequality constraints separately from hard equality constraints. Therefore, in the definition of HL-MRFs, we will define these three components separately.</p>
<h1>3.1.3 Generalized Hinge-Loss Functions</h1>
<p>The objective terms measuring each constraint's distance to satisfaction are hinge losses. There is a flat region, on which the distance to satisfaction is 0 , and an angled region, on which the distance to satisfaction grows linearly away from the hyperplane $\ell_{j}(\boldsymbol{y})=0$. This loss function is useful-as we discuss in the previous section, it is a bound on the expected loss in the discrete setting, among other things-but it is not appropriate for all modeling situations.</p>
<p>A piecewise-linear loss function makes MAP inference "winner take all," in the sense that it is preferable to fully satisfy the most highly weighted objective terms completely before reducing the distance to satisfaction of terms with lower weights. For example, consider the following optimization problem:</p>
<p>$$
\underset{y_{1} \in[0,1]}{\arg \min } w_{1} \max \left{y_{1}, 0\right}+w_{2} \max \left{1-y_{1}, 0\right}
$$</p>
<p>If $w_{1}&gt;w_{2} \geq 0$, then the optimizer is $y_{1}=0$ because the term that prefers $y_{1}=0$ overrules the term that prefers $y_{1}=1$. The result does not indicate any ambiguity or uncertainty, but if the two objective terms are potentials in a probabilistic model, it is sometimes preferable that the result reflect the conflicting preferences. We can change the inference problem so that it smoothly trades off satisfying conflicting objective terms by squaring the hinge losses. Observe that in the modified problem</p>
<p>$$
\underset{y_{1} \in[0,1]}{\arg \min } w_{1}\left(\max \left{y_{1}, 0\right}\right)^{2}+w_{2}\left(\max \left{1-y_{1}, 0\right}\right)^{2}
$$</p>
<p>the optimizer is now $y_{1}=\frac{w_{2}}{w_{1}+w_{2}}$, reflecting the relative influence of the two loss functions.
Another advantage of squared hinge-loss functions is that they can behave more intuitively in the presence of hard constraints. Consider the problem</p>
<p>$$
\begin{array}{ll}
\underset{\left(y_{1}, y_{2}\right) \in[0,1]^{2}}{\arg \min } &amp; \max \left{0.9-y_{1}, 0\right}+\max \left{0.6-y_{2}, 0\right} \
\text { such that } &amp; y_{1}+y_{2} \leq 1
\end{array}
$$</p>
<p>The first term prefers $y_{1} \geq 0.9$, the second term prefers $y_{2} \geq 0.6$, and the constraint requires that $y_{1}$ and $y_{2}$ are mutually exclusive. Such problems are very common and arise when conflicting evidence of different strengths support two mutually exclusive possibilities. The evidence values 0.9 and 0.6 could come from many sources, including base models trained to make independent predictions on individual random variables, domain-specialized similarity functions, or sensor readings. For this problem, any solution $y_{1} \in[0.4,0.9]$ and $y_{2}=1-y_{1}$ is an optimizer. This solution set includes counterintuitive optimizers like $y_{1}=0.4$ and $y_{2}=0.6$, even though the evidence supporting $y_{1}$ is stronger. Again, squared hinge losses</p>
<p>ensure the optimizers better reflect the relative strength of evidence. For the problem</p>
<p>$$
\underset{\left(y_{1}, y_{2}\right) \in[0,1]^{2}}{\arg \min \left(\max \left{0.9-y_{1}, 0\right}\right)^{2}+\left(\max \left{0.6-y_{2}, 0\right}\right)^{2}}
$$</p>
<p>such that</p>
<p>$$
y_{1}+y_{2} \leq 1
$$</p>
<p>the only optimizer is $y_{1}=0.65$ and $y_{2}=0.35$, which is a more informative solution.
We therefore complete our generalized inference objective by allowing either hinge-loss or squared hinge-loss functions. Users of HL-MRFs have the choice of either one for each potential, depending on which is appropriate for their task.</p>
<h1>3.2 Definition</h1>
<p>We can now formally state the full definition of HL-MRFs. They are defined so that a MAP state is a solution to the generalized inference objective proposed in the previous subsection. We state the definition in a conditional form for later convenience, but this definition is fully general since the vector of conditioning variables may be empty.</p>
<p>Definition 3 Let $\boldsymbol{y}=\left(y_{1}, \ldots, y_{n}\right)$ be a vector of $n$ variables and $\boldsymbol{x}=\left(x_{1}, \ldots, x_{n^{\prime}}\right)$ a vector of $n^{\prime}$ variables with joint domain $\boldsymbol{D}=[0,1]^{n+n^{\prime}}$. Let $\boldsymbol{\phi}=\left(\phi_{1}, \ldots, \phi_{m}\right)$ be a vector of $m$ continuous potentials of the form</p>
<p>$$
\phi_{j}(\boldsymbol{y}, \boldsymbol{x})=\left(\max \left{\ell_{j}(\boldsymbol{y}, \boldsymbol{x}), 0\right}\right)^{p_{j}}
$$</p>
<p>where $\ell_{j}$ is a linear function of $\boldsymbol{y}$ and $\boldsymbol{x}$ and $p_{j} \in{1,2}$. Let $\boldsymbol{c}=\left(c_{1}, \ldots, c_{r}\right)$ be a vector of $r$ linear constraint functions associated with index sets denoting equality constraints $\mathcal{E}$ and inequality constraints $\mathcal{I}$, which define the feasible set</p>
<p>$$
\tilde{\boldsymbol{D}}=\left{(\boldsymbol{y}, \boldsymbol{x}) \in \boldsymbol{D} \mid \begin{array}{l}
c_{k}(\boldsymbol{y}, \boldsymbol{x})=0, \forall k \in \mathcal{E} \
c_{k}(\boldsymbol{y}, \boldsymbol{x}) \leq 0, \forall k \in \mathcal{I}
\end{array}\right}
$$</p>
<p>For $(\boldsymbol{y}, \boldsymbol{x}) \in \boldsymbol{D}$, given a vector of $m$ nonnegative free parameters, i.e., weights, $\boldsymbol{w}=$ $\left(w_{1}, \ldots, w_{m}\right)$, a constrained hinge-loss energy function $f_{\boldsymbol{w}}$ is defined as</p>
<p>$$
f_{\boldsymbol{w}}(\boldsymbol{y}, \boldsymbol{x})=\sum_{j=1}^{m} w_{j} \phi_{j}(\boldsymbol{y}, \boldsymbol{x})
$$</p>
<p>We now define HL-MRFs by placing a probability density over the inputs to a constrained hinge-loss energy function. Note that we negate the hinge-loss energy function so that states with lower energy are more probable, in contrast with Definition 1. This change is made for later notational convenience.</p>
<p>Definition $4 A$ hinge-loss Markov random field $P$ over random variables $\boldsymbol{y}$ and conditioned on random variables $\boldsymbol{x}$ is a probability density defined as follows: if $(\boldsymbol{y}, \boldsymbol{x}) \notin \tilde{\boldsymbol{D}}$, then $P(\boldsymbol{y} \mid \boldsymbol{x})=0$; if $(\boldsymbol{y}, \boldsymbol{x}) \in \tilde{\boldsymbol{D}}$, then</p>
<p>$$
P(\boldsymbol{y} \mid \boldsymbol{x})=\frac{1}{Z(\boldsymbol{w}, \boldsymbol{x})} \exp \left(-f_{\boldsymbol{w}}(\boldsymbol{y}, \boldsymbol{x})\right)
$$</p>
<p>where</p>
<p>$$
Z(\boldsymbol{w}, \boldsymbol{x})=\int_{\boldsymbol{y} |(\boldsymbol{y}, \boldsymbol{x}) \in \hat{D}} \exp \left(-f_{\boldsymbol{w}}(\boldsymbol{y}, \boldsymbol{x})\right) d \boldsymbol{y}
$$</p>
<p>In the rest of this paper, we will explore how to use HL-MRFs to solve a wide range of structured machine learning problems. We first introduce a probabilistic programming language that makes HL-MRFs easy to define for large, rich domains.</p>
<h1>4. Probabilistic Soft Logic</h1>
<p>In this section we introduce a general-purpose probabilistic programming language, probabilistic soft logic (PSL). PSL allows HL-MRFs to be easily applied to a broad range of structured machine learning problems by defining templates for potentials and constraints. In models for structured data, there are very often repeated patterns of probabilistic dependencies. A few of the many examples include the strength of ties between similar people in social networks, the preference for triadic closure when predicting transitive relationships, and the "exactly one active" constraints on functional relationships. Often, to make graphical models both easy to define and able to generalize across different data sets, these repeated dependencies are defined using templates. Each template defines an abstract dependency, such as the form of a potential function or constraint, along with any necessary parameters, such as the weight of the potential, each of which has a single value across all dependencies defined by that template. Given input data, an undirected graphical model is constructed from a set of templates by first identifying the random variables in the data and then "grounding out" each template by introducing a potential or constraint into the graphical model for each subset of random variables to which the template applies.</p>
<p>A PSL program is written in a declarative, first-order syntax and defines a class of HL-MRFs that are parameterized by the input data. PSL provides a natural interface to represent hinge-loss potential templates using two types of rules: logical rules and arithmetic rules. Logical rules are based on the mapping from logical clauses to hinge-loss potentials introduced in Section 2. Arithmetic rules provide additional syntax for defining an even wider range of hinge-loss potentials and hard constraints.</p>
<h3>4.1 Definition</h3>
<p>In this subsection we define PSL. Our definition covers the essential functionality that should be supported by all implementations, but many extensions are possible. The PSL syntax we describe can capture a wide range of HL-MRFs, but new settings and scenarios could motivate the development of additional syntax to make the construction of different kinds of HL-MRFs more convenient.</p>
<h3>4.1.1 Preliminaries</h3>
<p>We begin with a high-level definition of PSL programs.
Definition 5 A PSL program is a set of rules, each of which is a template for hinge-loss potentials or hard linear constraints. When grounded over a base of ground atoms, a PSL program induces a HL-MRF conditioned on any specified observations.</p>
<p>In the PSL syntax, many components are named using identifiers, which are strings that begin with a letter (from the set ${\mathrm{A}, \ldots, \mathrm{Z}, \mathrm{a}, \ldots, \mathrm{z}}$ ), followed by zero or more letters, numeric digits, or underscores.</p>
<p>PSL programs are grounded out over data, so the universe over which to ground must be defined.</p>
<p>Definition 6 A constant is a string that denotes an element in the universe over which a PSL program is grounded.</p>
<p>Constants are the elements in a universe of discourse. They can be entities or attributes. For example, the constant "person1" can denote a person, the constant "Adam" can denote a person's name, and the constant "30" can denote a person's age. In PSL programs, constants are written as strings in double or single quotes. Constants use backslashes as escape characters, so they can be used to encode quotes within constants. It is assumed that constants are unambiguous, i.e., different constants refer to different entities and attributes. ${ }^{3}$ Groups of constants can be represented using variables.</p>
<p>Definition 7 A variable is an identifier for which constants can be substituted.
Variables and constants are the arguments to logical predicates. Together, they are generically referred to as terms.</p>
<p>Definition 8 A term is either a constant or a variable.
Terms are connected by relationships called predicates.
Definition 9 A predicate is a relation defined by a unique identifier and a positive integer called its arity, which denotes the number of terms it accepts as arguments. Every predicate in a PSL program must have a unique identifier as its name.</p>
<p>We refer to a predicate using its identifier and arity appended with a slash. For example, the predicate Friends/2 is a binary predicate, i.e., taking two arguments, which represents whether two constants are friends. As another example, the predicate Name/2 can relate a person to the string that is that person's name. As a third example, the predicate EnrolledInClass/3 can relate two entities, a student and professor, with an additional attribute, the subject of the class.</p>
<p>Predicates and terms are combined to create atoms.
Definition 10 An atom is a predicate combined with a sequence of terms of length equal to the predicate's arity. This sequence is called the atom's arguments. An atom with only constants for arguments is called a ground atom.</p>
<p>Ground atoms are the basic units of reasoning in PSL. Each represents an unknown or observation of interest and can take any value in $[0,1]$. For example, the ground atom Friends("person1", "person2") represents whether "person1" and "person2" are friends. Atoms that are not ground are placeholders for sets of ground atoms. For example, the atom Friends(X, Y) stands for all ground atoms that can be obtained by substituting constants for variables X and Y .</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.1.2 Inputs</h1>
<p>As we have already stated, PSL defines templates for hinge-loss potentials and hard linear constraints that are grounded out over a data set to induce a HL-MRF. We now describe how that data set is represented and provided as the inputs to a PSL program. The first inputs are two sets of predicates: a set $\mathbb{C}$ of closed predicates, the atoms of which are completely observed, and a set $\mathbb{O}$ of open predicates, the atoms of which may be unobserved. The third input is the base $\mathcal{A}$, which is the set of all ground atoms under consideration. All atoms in $\mathcal{A}$ must have a predicate in either $\mathbb{C}$ or $\mathbb{O}$. These are the atoms that can be substituted into the rules and constraints of a PSL program, and each will later be associated with a HLMRF random variable with domain $[0,1]$. The final input is a function $\mathcal{O}: \mathcal{A} \rightarrow[0,1] \cup{\emptyset}$ that maps the ground atoms in the base to either an observed value in $[0,1]$ or a symbol $\emptyset$ indicating that it is unobserved. The function $\mathcal{O}$ is only valid if all atoms with a predicate in $\mathbb{C}$ are mapped to a $[0,1]$ value. Note that this definition makes the sets $\mathbb{C}$ and $\mathbb{O}$ redundant in a sense, since they can be derived from $\mathcal{A}$ and $\mathcal{O}$, but it will be convenient later to have $\mathbb{C}$ and $\mathbb{O}$ explicitly defined.</p>
<p>Ultimately, the method for specifying PSL's inputs is implementation-specific, since different choices make it more or less convenient for different scenarios. In this paper, we will assume that $\mathbb{C}, \mathbb{O}, \mathcal{A}$, and $\mathcal{O}$ exist, and we remain agnostic about how they were specified. However, to make this aspect of using PSL more concrete, we will describe one possible method for defining them here.</p>
<p>Our example method for specifying PSL's inputs is text-based. The first section of the text input is a definition of the constants in the universe, which are grouped into types. An example universe definition follows.</p>
<div class="codehilite"><pre><span></span><code>Person = {&quot;alexis&quot;, &quot;bob&quot;, &quot;claudia&quot;, &quot;david&quot;}
Professor = {&quot;alexis&quot;, &quot;bob&quot;}
Student = {&quot;claudia&quot;, &quot;david&quot;}
Subject = {&quot;computer science&quot;, &quot;statistics&quot;}
</code></pre></div>

<p>This universe includes six constants, four with two types ("alexis", "bob", "claudia", and "david") and two with one type ("computer science" and "statistics").</p>
<p>The next section of input is the definition of predicates. Each predicate includes the types of constants it takes as arguments and whether it is closed. For example, we can define predicates for an advisor-student relationship prediction task as follows:</p>
<div class="codehilite"><pre><span></span><code>Advises(Professor, Student)
Department(Person, Subject) (closed)
EnrolledInClass(Student, Subject, Professor) (closed)
</code></pre></div>

<p>In this case, there is one open predicate (Advises) and two closed predicates (Department and EnrolledInClass).</p>
<p>The final section of input is any associated observations. They can be specified in a list, for example:</p>
<div class="codehilite"><pre><span></span><code>Advises(&quot;alexis&quot;, &quot;david&quot;) = 1
Department(&quot;alexis&quot;, &quot;computer science&quot;) = 1
Department(&quot;bob&quot;, &quot;computer science&quot;) = 1
Department(&quot;claudia&quot;, &quot;statistics&quot;) = 1
Department(&quot;david&quot;, &quot;statistics&quot;) = 1
</code></pre></div>

<p>In addition, values for atoms with the EnrolledInClass predicate could also be specified. If a ground atom does not have a specified value, it will have a default observed value of 0 if its predicate is closed or remain unobserved if its predicate is open.</p>
<p>We now describe how this text input is processed into the formal inputs $\mathbb{C}, \mathbb{O}, \mathcal{A}$, and $\mathcal{O}$. First, each predicate is added to either $\mathbb{C}$ or $\mathbb{O}$ based on whether it is annotated with the (closed) tag. Then, for each predicate in $\mathbb{C}$ or $\mathbb{O}$, ground atoms of that predicate are added to $\mathcal{A}$ with each sequence of constants as arguments that can be created by selecting a constant of each of the predicate's argument types. For example, assume that the input file contains a single predicate definition</p>
<h1>Category(Document, Cat.Name)</h1>
<p>where the universe is Document $={$ "d1", "d2" $}$ and Cat_Name $={$ "politics", "sports" $}$. Then,</p>
<p>$$
\mathcal{A}=\left{\begin{array}{l}
\text { Category("d1", "politics"), } \
\text { Category("d1", "sports"), } \
\text { Category("d2", "politics"), } \
\text { Category("d2", "sports") }}
\end{array}\right}
$$</p>
<p>Finally, we define the function $\mathcal{O}$. Any atom in the explicit list of observations is mapped to the given value. Then, any remaining atoms in $\mathcal{A}$ with a predicate in $\mathbb{C}$ are mapped to 0 , and any with a predicate in $\mathbb{O}$ are mapped to $\emptyset$.</p>
<p>Before moving on, we also note that PSL implementations can support predicates and atoms that are defined functionally. Such predicates can be thought of as a type of closed predicate. Their observed values are defined as a function of their arguments. One of the most common examples is inequality, atoms of which can be represented with the shorthand infix operator !=. For example, the following atom has a value of 1 when two variables A and $B$ are replaced with different constants and 0 when replaced with the same constant.</p>
<p>$$
A!=B
$$</p>
<p>Such functionally defined predicates can be implemented without requiring their values over all arguments to be specified by the user.</p>
<h3>4.1.3 Rules and Grounding</h3>
<p>Before introducing the syntax and semantics of specific PSL rules, we define the grounding procedure that induces HL-MRFs in general. Given the inputs $\mathbb{C}, \mathbb{O}, \mathcal{A}$, and $\mathcal{O}$, PSL induces</p>
<p>a HL-MRF $P(\boldsymbol{y} \mid \boldsymbol{x})$ as follows. First, each ground atom $a \in \mathcal{A}$ is associated with a random variable with domain $[0,1]$. If $\mathcal{O}(a)=\emptyset$, then the variable is included in the free variables $\boldsymbol{y}$, and otherwise it is included in the observations $\boldsymbol{x}$ with a value of $\mathcal{O}(a)$.</p>
<p>With the variables in the distribution defined, each rule in the PSL program is applied to the inputs and produces hinge-loss potentials or hard linear constraints, which are added to the HL-MRF. In the rest of this subsection, we describe two kinds of PSL rules: logical rules and arithmetic rules.</p>
<h1>4.1.4 Logical Rules</h1>
<p>The first kind of PSL rule is a logical rule, which is made up of literals.
Definition 11 A literal is an atom or a negated atom.
In PSL, the prefix operator ! or - is used for negation. A negated atom has a value of one minus the value of the unmodified atom. For example, if Friends("person1", "person2") has a value of 0.7 , then !Friends("person1", "person2") has a value of 0.3 .</p>
<p>Definition 12 A logical rule is a disjunctive clause of literals. Logical rules are either weighted or unweighted. If a logical rule is weighted, it is annotated with a nonnegative weight and optionally a power of two.</p>
<p>Logical rules express logical dependencies in the model. As in Boolean logic, the negation, disjunction (written as $|$ or $|$ ), and conjunction (written as \&amp;\&amp; or \&amp;) operators obey De Morgan's Laws. Also, an implication (written as $-&gt;$ or &lt;-) can be rewritten as the negation of the body disjuncted with the head. For example</p>
<p>$$
\begin{aligned}
&amp; \text { P1(A, B) \&amp;\&amp; P2(A, B) -&gt; P3(A, B) || P4(A, B) } \
&amp; \equiv \text { ! (P1(A, B) \&amp;\&amp; P2(A, B)) || P3(A, B) || P4(A, B) } \
&amp; \equiv \text { !P1(A, B) || !P2(A, B) || P3(A, B) || P4(A, B) }
\end{aligned}
$$</p>
<p>Therefore, any formula written as an implication with (1) a literal or conjunction of literals in the body and (2) a literal or disjunction of literals in the head is also a valid logical rule, because it is equivalent to a disjunctive clause.</p>
<p>There are two kinds of logical rules: weighted or unweighted. A weighted logical rule is a template for a hinge-loss potential that penalizes how far the rule is from being satisfied. A weighted logical rule begins with a nonnegative weight and optionally ends with an exponent of two (^2). For example, the weighted logical rule</p>
<p>1 : Advisor(Prof, S) \&amp;\&amp; Department(Prof, Sub) -&gt; Department(S, Sub)
has a weight of 1 and induces potentials propagating department membership from advisors to advisees. An unweighted logical rule is a template for a hard linear constraint that requires that the rule always be satisfied. For example, the unweighted logical rule</p>
<p>Friends(X, Y) \&amp;\&amp; Friends(Y, Z) -&gt; Friends(X, Z) .
induces hard linear constraints enforcing the transitivity of the Friends/2 predicate. Note the period (.) that is used to emphasize that this rule is always enforced and disambiguate it from weighted rules.</p>
<p>A logical rule is grounded out by performing all distinct substitutions from variables to constants such that the resulting ground atoms are in the base $\mathcal{A}$. This procedure produces a set of ground rules, which are rules containing only ground atoms. Each ground rule will then be interpreted as either a potential or hard constraint in the induced HL-MRF. For notational convenience, we assume without loss of generality that all the random variables are unobserved, i.e., $\mathcal{O}(a)=\emptyset, \forall a \in \mathcal{A}$. If the input data contain any observations, the following description still applies, except that some free variables will be replaced with observations from $\boldsymbol{x}$. The first step in interpreting a ground rule is to map its disjunctive clause to a linear constraint. This mapping is based on the unified inference objective derived in Section 2. Any ground PSL rule is a disjunction of literals, some of which are negated. Let $I^{+}$be the set of indices of the variables that correspond to atoms that are not negated in the ground rule, when expressed as a disjunctive clause, and, likewise, let $I^{-}$be the indices of the variables corresponding to atoms that are negated. Then, the clause is mapped to the inequality</p>
<p>$$
1-\sum_{i \in I^{+}} y_{i}-\sum_{i \in I^{-}}\left(1-y_{i}\right) \leq 0
$$</p>
<p>If the logical rule that templated the ground rule is weighted with a weight of $w$ and is not annotated with $\sim 2$, then the potential</p>
<p>$$
\phi(\boldsymbol{y}, \boldsymbol{x})=\max \left{1-\sum_{i \in I^{+}} y_{i}-\sum_{i \in I^{-}}\left(1-y_{i}\right), 0\right}
$$</p>
<p>is added to the HL-MRF with a parameter of $w$. If the rule is weighted with a weight $w$ and annotated with $\sim 2$, then the potential</p>
<p>$$
\phi(\boldsymbol{y}, \boldsymbol{x})=\left(\max \left{1-\sum_{i \in I^{+}} y_{i}-\sum_{i \in I^{-}}\left(1-y_{i}\right), 0\right}\right)^{2}
$$</p>
<p>is added to the HL-MRF with a parameter of $w$. If the rule is unweighted, then the function</p>
<p>$$
c(\boldsymbol{y}, \boldsymbol{x})=1-\sum_{i \in I^{+}} y_{i}-\sum_{i \in I^{-}}\left(1-y_{i}\right)
$$</p>
<p>is added to the set of constraint functions and its index is included in the set $\mathcal{I}$ to define a hard inequality constraint $c(\boldsymbol{y}, \boldsymbol{x}) \leq 0$.</p>
<p>As an example of the grounding process, consider the following logical rule. As part of a program for link prediction, it is often helpful to model the transitivity of a relationship.</p>
<h1>3 : Friends(A, B) \&amp;\&amp; Friends(B, C) $\rightarrow$ Friends(C, A) ${ }^{\sim} 2$</h1>
<p>Imagine that the input data are $\mathbb{C}={ }, \mathbb{O}={\text { Friends/2 }}$,</p>
<p>$$
\mathcal{A}=\left{\begin{array}{l}
\text { Friends("p1", "p2"), } \
\text { Friends("p1", "p3"), } \
\text { Friends("p2", "p1"), } \
\text { Friends("p2", "p3"), } \
\text { Friends("p3", "p1"), } \
\text { Friends("p3", "p2") }
\end{array}\right}
$$</p>
<p>and $\mathcal{O}(a)=\emptyset, \forall a \in \mathcal{A}$. Then, the rule will induce six ground rules. One such ground rule is</p>
<p>3 : Friends("p1", "p2") \&amp;\&amp; Friends("p2", "p3") -&gt; Friends("p3", "p1") ^2
which is equivalent to the following.
3 : !Friends("p1", "p2") || !Friends("p2", "p3") || Friends("p3", "p1") ^2
If the atoms Friends("p1", "p2"), Friends("p2", "p3"), and Friends("p3", "p1") correspond to the random variables $y_{1}, y_{2}$, and $y_{3}$, respectively, then this ground rule is interpreted as the weighted hinge-loss potential</p>
<p>$$
3\left(\max \left{y_{1}+y_{2}-y_{3}-1,0\right}\right)^{2}
$$</p>
<p>Since the grounding process uses the mapping from Section 2, logical rules can be used to reason accurately and efficiently about both discrete and continuous information. They are a convenient method for constructing HL-MRFs with the unified inference objective for weighted logical knowledge bases as their MAP inference objective. They also allow the user to seamlessly incorporate some of the additional features of HL-MRFs, such as squared potentials and hard constraints. Next, we introduce an even more flexible class of PSL rules.</p>
<h1>4.1.5 Arithmetic Rules</h1>
<p>Arithmetic rules in PSL are more general templates for hinge-loss potentials and hard linear constraints. Like logical rules, they come in weighted and unweighted variants, but instead of using logical operators they use arithmetic operators. In general, an arithmetic rule relates two linear combinations of atoms with an inequality or an equality. A simple example enforces the mutual exclusivity of liberal and conservative ideologies.</p>
<p>$$
\text { Liberal }(P)+\text { Conservative }(P)=1
$$</p>
<p>Just like logical rules, arithmetic rules are grounded out by performing all possible substitutions of constants for variables to make ground atoms in the base $\mathcal{A}$. In this example, each substitution for Liberal (P) and Conservative (P) is constrained to sum to 1 . Since the rule is unweighted and arithmetic, it defines a hard constraint $c(\boldsymbol{y}, \boldsymbol{x})$ and its index will be included in $\mathcal{E}$ because it is an equality constraint.</p>
<p>To make arithmetic rules more flexible and easy to use, we define some additional syntax. The first is a generalized definition of atoms that can be substituted with sums of ground atoms, rather than just a single atom.</p>
<p>Definition $13 A$ summation atom is an atom that takes terms and/or sum variables as arguments. A summation atom represents the summations of ground atoms that can be obtained by substituting individual constants for variables and summing over all possible constants for sum variables.</p>
<p>A sum variable is represented by prepending a plus symbol $(+)$ to a variable. For example, the summation atom</p>
<p>$$
\text { Friends }(P,+F)
$$</p>
<p>is a placeholder for the sum of all ground atoms with predicate Friends/2 in $\mathcal{A}$ that share a first argument. Note that sum variables can be used at most once in a rule, i.e., each sum variable in a rule must have a unique identifier. Summation atoms are useful because they can describe dependencies without needing to specify the number of atoms that can participate. For example, the arithmetic rule</p>
<p>$$
\text { Label }(X,+L)=1
$$</p>
<p>says that labels for each constant substituted for X should sum to one, without needing to specify how many possible labels there are.</p>
<p>The substitutions for sum variables can be restricted using logical clauses as filters.
Definition 14 A filter clause is a logical clause defined for a sum variable in an arithmetic rule. The logical clause only contains atoms (1) with predicates that appear in $\mathbb{C}$ and (2) that only take as arguments (a) constants, (b) variables that appear in the arithmetic rule, and (c) the sum variable for which it is defined.</p>
<p>Filter clauses restrict the substitutions for a sum variable in the corresponding arithmetic rule by only including substitutions for which the clause evaluates to true. The filters are evaluated using Boolean logic. Each ground atom $a$ is treated as having a value of 0 if and only if $\mathcal{O}(a)=0$. Otherwise, it is treated as having a value of 1 . For example, imagine that we want to restrict the summation in the following arithmetic rule to only constants that satisfy a property Property/1.</p>
<p>$$
\operatorname{Link}(X,+Y)&lt;=1
$$</p>
<p>Then, we can add the following filter clause.</p>
<p>$$
{\mathrm{Y}: \text { Property }(\mathrm{Y})}
$$</p>
<p>Then, the hard linear constraints templated by the arithmetic rule will only sum over constants substituted for Y such that Property $(\mathrm{Y})$ is non-zero.</p>
<p>In arithmetic rules, atoms can also be modified with coefficients. These coefficients can be hard-coded. As a simple example, in the rule</p>
<p>$$
\text { Susceptible }(X)&gt;=0.5 \text { Biomarker1 }(X)+0.5 \text { Biomarker2 }(X)
$$</p>
<p>the property Susceptible/1, which represents the degree to which a patient is susceptible to a particular disease, must be at least the average value of two biomarkers.</p>
<p>PSL also supports two forms of coefficient-defining syntax. The first form of coefficient syntax is a cardinality function that counts the number of terms substituted for a sum variable. Cardinality functions enable rules that depend on the number of substitutions in order to be scaled correctly, such as when averaging. Cardinality is denoted by enclosing a sum variable, without the + , in pipes. For example, the rule</p>
<p>$$
1 /|\mathrm{Y}| \text { Friends }(\mathrm{X},+\mathrm{Y})=\text { Friendliness }(\mathrm{X})
$$</p>
<p>defines the Friendliness/1 property of a person X in a social network as the average strength of their outgoing friendship links. In cases in which Friends/2 is not symmetric, we can extend this rule to sum over both outgoing and incoming links as follows.</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">|</span><span class="n">Y1</span><span class="err">|</span><span class="w"> </span><span class="err">|</span><span class="n">Y2</span><span class="err">|</span><span class="w"> </span><span class="n">Friends</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="o">+</span><span class="n">Y1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">|</span><span class="n">Y1</span><span class="err">|</span><span class="w"> </span><span class="err">|</span><span class="n">Y2</span><span class="err">|</span><span class="w"> </span><span class="n">Friends</span><span class="p">(</span><span class="o">+</span><span class="n">Y2</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">)</span>
<span class="o">=</span><span class="w"> </span><span class="n">Friendliness</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="w"> </span><span class="mf">.</span>
</code></pre></div>

<p>The second form of coefficient syntax is built-in coefficient functions. The exact set of supported functions is implementation specific, but standard functions like maximum and minimum should be included. Coefficient functions are prepended with @ and use square brackets instead of parentheses to distinguish them from predicates. Coefficient functions can take either scalars or cardinality functions as arguments. For example, the following rule for matching two sets of constants requires that the sum of the Matched/2 atoms be the minimum of the sizes of the two sets.</p>
<p>$$
\operatorname{Matched}(+\mathrm{X},+\mathrm{Y})=\mathbb{\&amp;} \operatorname{Min}[|\mathrm{X}|,|\mathrm{Y}|] .
$$</p>
<p>Note that PSL's coefficient syntax can also be used to define constants, as in this example.
So far we have focused on using arithmetic rules to define templates for linear constraints, but they can also be used to define hinge-loss potentials. For example, the following arithmetic rule prefers that the degree to which a person X is extroverted (represented with Extroverted/1) does not exceed the average extroversion of their friends:</p>
<div class="codehilite"><pre><span></span><code><span class="mi">2</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">Extroverted</span><span class="o">(</span><span class="n">X</span><span class="o">)</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">|</span><span class="n">Y</span><span class="o">|</span><span class="w"> </span><span class="n">Extroverted</span><span class="o">(+</span><span class="n">Y</span><span class="o">)</span><span class="w"> </span><span class="o">^</span><span class="mi">2</span>
<span class="o">{</span><span class="n">Y</span><span class="o">:</span><span class="w"> </span><span class="n">Friends</span><span class="o">(</span><span class="n">X</span><span class="o">,</span><span class="w"> </span><span class="n">Y</span><span class="o">)</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">Friends</span><span class="o">(</span><span class="n">Y</span><span class="o">,</span><span class="w"> </span><span class="n">X</span><span class="o">)}</span>
</code></pre></div>

<p>This rule is a template for weighted hinge-loss potentials of the form</p>
<p>$$
2\left(\max \left{y_{i^{\prime}}-\frac{1}{|\mathcal{F}|} \sum_{i \in \mathcal{F}} y_{i}, 0\right}\right)^{2}
$$</p>
<p>where $y_{i^{\prime}}$ is the variable corresponding to a grounding of the atom Extroverted(X) and $\mathcal{F}$ is the set of the indices of the variables corresponding to Extroverted(Y) atoms of the friends Y that satisfy the rule's filter clause. Note that the weight of 2 is distinct from the coefficients in the linear constraint $\ell(\boldsymbol{y}, \boldsymbol{x}) \leq 0$ defining the hinge-loss potential. If the arithmetic rule were an equality instead of an inequality, each grounding would be two hinge-loss potentials, one using $\ell(\boldsymbol{y}, \boldsymbol{x}) \leq 0$ and one using $-\ell(\boldsymbol{y}, \boldsymbol{x}) \leq 0$. In this way, arithmetic rules can define general hinge-loss potentials.</p>
<p>For completeness, we state the full, formal definition of an arithmetic rule and define its grounding procedure.</p>
<p>Definition 15 An arithmetic rule is an inequality or equality relating two linear combinations of summation atoms. Each sum variable in an arithmetic rule can be used once. An arithmetic rule can be annotated with filter clauses for a subset of its sum variables that restrict its groundings. Arithmetic rules are either weighted or unweighted. If an arithmetic rule is weighted, it is annotated with a nonnegative weight and optionally a power of two.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>Note that ambiguous references to underlying entities can be modeled by using different constants for different references and representing whether they refer to the same underlying entity as a predicate.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
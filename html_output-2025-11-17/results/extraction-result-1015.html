<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1015 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1015</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1015</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-263826714</p>
                <p><strong>Paper Title:</strong> Generalization Ability of Deep Reinforcement Learning-based Navigation Based on Curriculum Learning</p>
                <p><strong>Paper Abstract:</strong> In this paper, curriculum learning is studied as an approach to improve generalization ability in navigation tasks, that is, improve the agent’s ability of navigating in scenarios different from those used for training. The agent is trained based on the TD3 algorithm, and curriculum learning selects different stages (i.e. different curricula) of Empty/Sparse/Normal worlds for training. Via extensive numerical comparisons with agents trained under such curricula, it is shown that properly used curriculum learning improves the agent’s ability of generalization. Furthermore, an automatic curriculum learning (Auto-CL) approach is proposed. Auto-CL is shown to have even better generalization than the standard curriculum learning, since it makes the agent able to navigate in new environments with more than 6% shorter paths in more than 21% shorter time.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1015.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1015.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard TD3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard TD3-trained navigation agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A TD3 (Twin Delayed DDPG) actor-critic agent trained in a single environment (Normal world) using LiDAR, velocity and target information for continuous control navigation in Gazebo; used as the baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Standard TD3 agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-critic deep reinforcement learning agent trained with TD3 (continuous control). Observations: down-sampled 180° front LiDAR, linear/angular velocity, distance and heading to target. Actions: continuous linear and angular velocity (normalized). Reward: +400 on reach, -100 on collision, shaped per-step reward (vx - |vθ|)/α with α=10.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (Pioneer3DX in Gazebo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Normal world (training); evaluated in Empty / Sparse / Normal evaluation map</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Three scenario types defined by obstacle density: Empty world (no obstacles), Sparse world (low obstacle density), Normal world (higher/dense obstacle density). Evaluation map used for generalization tests contains three spatial regions (left: dense, middle: sparse, right: no obstacles) and a sequence of targets to visit; environments are simulated in Gazebo with mobile Pioneer3DX model.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Categorical obstacle density of map (Empty / Sparse / Normal). In Auto-CL experiments complexity is also parameterized by number of boxes (b_num) present in environment, with b_num ∈ [initial ... b_max].</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Trained in: Normal = high complexity (dense obstacles); Evaluation includes Medium (Sparse) and Low (Empty).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is low for this agent (trained in single environment only — Normal). Variation across evaluation domains measured by testing in different obstacle-density worlds (Empty, Sparse, Normal) and an evaluation map with three regions of differing densities.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Low during training (single scenario), Variation in testing: medium-to-high (tested across three distinct densities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (reach target), average path length, average completion time (and their std / min / max over evaluation runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On baseline evaluations reported in paper: (from Table 2 at 0.42 million training steps) Empty success rate = 1.00 (but with many detours), Sparse success rate = 0.75, Normal success rate = 0.85. In multi-target evaluation (Table 4) reported metrics (units not explicitly stated): Avg path = 395.67, Avg time = 299.58 (other per-run min/max/std given in table but formatting in paper is inconsistent).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper reports that training exclusively in a single (Normal) high-complexity environment yields poor generalization to other densities; as training progressed the agent's performance in Sparse and Empty worlds often decreased to near zero, indicating overfitting to the Normal-world distribution and a sensitivity to variation in obstacle density. The authors interpret this as an inability to handle environment variation when complexity and variation distributions differ between train and test.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Normal world (trained and tested same domain): success rate ≈ 0.85 (at 0.42M steps); performance good in-domain but degrades when training steps change (see Table 2 showing unstable generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Tested in Empty world while trained only in Normal: success rate sometimes 1.00 at 0.42M steps but routes contain large detours and are sub-optimal; with other training steps success rate dropped to 0 (Table 2), indicating unreliable performance under low-complexity target but distributional mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training (standard TD3 trained in Normal world only)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Poor: agent trained only in Normal world overfits to that distribution; shows detours and sub-optimal routing on evaluation map that mixes densities, with success rates that decline sharply in Sparse/Empty test worlds as training continues.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training scale reported in experiments: examples include 0.42 million, 0.51 million, and 1.2 million training steps for the standard TD3 baseline; other TD3 hyperparams: batch size 40, replay buffer 1e6, learning rate 1e-4, discount 0.99999.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Baseline TD3 trained in a single (Normal) environment displays limited generalization to worlds with different obstacle densities; increasing training steps does not guarantee better cross-environment performance and can lead to catastrophic loss of performance in other densities (overfitting).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1015.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1015.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S-N curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse-to-Normal curriculum-trained agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A TD3 agent trained with a two-stage curriculum that first trains in Sparse (easy) environments and then in Normal (harder) environments; improves generalization across densities compared to single-environment training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>S-N curriculum-trained TD3 agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same TD3 agent architecture as baseline, but training follows a curriculum: first train in Sparse world then in Normal world (S -> N). Observations and actions identical to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (Pioneer3DX in Gazebo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Two-stage curriculum: Sparse world then Normal world; evaluated in Empty / Sparse / Normal evaluation map</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Sparse world: few obstacles; Normal world: denser obstacles. Curriculum trains agent sequentially from lower obstacle density to higher, intending to expose agent to increasing data complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Obstacle density (categorical) across stages; measured implicitly via world label (Sparse vs Normal) and average path-length metrics collected during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Training progression: low complexity (Sparse) → higher complexity (Normal).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Moderate variation introduced by switching scenario between two training stages (S then N); evaluation uses three-region map mixing densities.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Medium (two distinct training stages increases variation versus single-environment training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate, average path length (per scenario), evaluated also on multi-target route length/time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>From Table 3 and Table 4 (paper): Table 3 — Empty: success = 1.00, avg path length = 4.61; Sparse: success = 0.95, avg path = 4.78; Normal: success = 0.90, avg path = 6.95. Table 4 (multi-target evaluation): Avg path = 76.32, Max path = 83.70, Min path = 72.25, Path σ = 3.33; Avg time = 202.96 (units not explicitly stated), Max time = 225.72, Min time = 178.74, Time σ = 12.37.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit: the paper finds that training from easy (Sparse) to hard (Normal) (S-N) yields better cross-domain generalization than training only in Normal. S-N effectively reduces detours and improves success on mixed-density evaluation. The authors also compare reverse-order curricula and find that training order matters — training from hard to easy can cause forgetting of complex-domain skills.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>After S-N training, Normal world (high complexity) performance: success ≈ 0.90, avg path length ≈ 6.95 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Empty world after S-N training: success = 1.00, avg path length = 4.61 (Table 3), indicating strong performance in low complexity region despite exposure to higher complexity later.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (two-stage: Sparse → Normal)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>S-N improved generalization relative to single-environment TD3: higher success rates and shorter, more direct routes on mixed-density evaluation map; S-N is among the best performing hand-designed curricula in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained with comparable budget to other methods in the study (experiments report training steps on the order of 0.42–1.2 million steps for different agents).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Carefully-designed two-stage curriculum (easy → hard) improves navigation generalization across environments with different obstacle densities; S-N reduces detours and yields high success rates in both low- and medium-complexity test regions. Additional stages (S-N-S) did not meaningfully improve over S-N in these experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1015.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1015.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Curriculum Learning (Auto-CL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated curriculum mechanism that adapts environment complexity by incrementally adding obstacles (boxes) based on agent performance thresholds, integrated with TD3 training to improve generalization across environment densities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Auto-CL-trained TD3 agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>TD3 agent trained with an automatic curriculum: each training episode is run in an environment containing b_num boxes; after evaluation, if the agent's score exceeds score_th twice and b_num < b_max then b_num is increased by b_add, gradually increasing environment complexity during training. Algorithm parameters: b_num=4, b_add=1, b_max=36, score_th=350.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (Pioneer3DX in Gazebo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Auto-generated box environments (Auto-CL) and evaluation map with three regions (dense/sparse/empty)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Training environments are procedurally varied by adjusting the number of boxes (obstacles) placed in the map. Complexity grows automatically driven by agent performance; evaluation uses a fixed multi-region map mixing dense, sparse and empty regions (targets placed across map).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of boxes present in the environment (b_num); obstacle density categories (Empty/Sparse/Normal) used for benchmarking. Auto-CL explicitly controls b_num ∈ [4 .. 36].</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies during training from low (b_num initial = 4) to up to high (b_max = 36).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Adaptive procedural variation via incrementing b_num when performance threshold (score_th) is exceeded; evaluation tests across three fixed densities and a mixed-density evaluation map. Variation is both across episodes and across training progression.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (automatically varied complexity across training episodes according to performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate, average path length, average completion time, per-run min/max/std; reported relative improvements versus other curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>From Table 3 and Table 4: Table 3 — Empty: success = 1.00, avg path length = 4.50; Sparse: success = 0.95, avg path length = 5.15; Normal: success = 0.95, avg path length = 5.95. Table 4 (multi-target evaluation): Avg path = 71.70, Max path = 75.30, Min path = 68.29, Path σ = 2.07; Avg time = 160.24, Max time = 177.85, Min time = 151.00, Time σ = 8.62. Paper reports Auto-CL achieves >6% shorter path and >21% shorter time compared to S-N on this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper argues and empirically demonstrates that gradually increasing complexity (variation) based on agent performance (Auto-CL) yields better generalization than fixed single-environment training or hand-designed curricula. Auto-CL reduces detours and completion time across mixed-density tests, indicating a favorable trade-off: adaptive variation during training reduces overfitting to any single density while still building capability for higher-complexity regions.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Normal world after Auto-CL training: success = 0.95, avg path length = 5.95 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Empty world after Auto-CL training: success = 1.00, avg path length = 4.50 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum learning (Auto-CL): adaptive procedural complexity increase using parameters b_num, b_add, b_max and a score threshold score_th.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Auto-CL generalized better than both the standard TD3 baseline and hand-designed curricula (S-N): in the mixed-density multi-target evaluation it produced the shortest average path (≈71.70) and shortest average time (≈160.24) with smaller standard deviations; reported improvements versus S-N: ≈6.05% shorter path, ≈21.05% shorter time (Table 4). Auto-CL achieved near-perfect success rates across Empty/Sparse/Normal test sets (1.00, 0.95, 0.95 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training budget reported similarly to other agents; experiments reference training steps in the order of 0.42–1.2 million steps for different runs. Auto-CL hyperparameters controlling environment progression (b_num growth) are specified, but paper does not report a separate reduced-step training budget for Auto-CL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated incremental complexity (Auto-CL) produces better generalization to new, mixed-density environments than single-environment TD3 and most hand-designed curricula; Auto-CL achieves shorter paths and times with lower variance, and avoids some forgetting observed in reverse-order curricula. The relationship observed is that controlled increasing of complexity (variation) tied to agent competence produces a favorable trade-off: improved cross-environment performance without excessive in-domain overfitting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A two-stage reinforcement learning approach for multi-UAV collision avoidance under imperfect sensing. <em>(Rating: 2)</em></li>
                <li>Cm3: Cooperative multi-goal multi-stage multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>A survey on curriculum learning <em>(Rating: 2)</em></li>
                <li>Addressing function approximation error in actor-critic methods <em>(Rating: 1)</em></li>
                <li>Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1015",
    "paper_id": "paper-263826714",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Standard TD3",
            "name_full": "Standard TD3-trained navigation agent",
            "brief_description": "A TD3 (Twin Delayed DDPG) actor-critic agent trained in a single environment (Normal world) using LiDAR, velocity and target information for continuous control navigation in Gazebo; used as the baseline in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Standard TD3 agent",
            "agent_description": "Actor-critic deep reinforcement learning agent trained with TD3 (continuous control). Observations: down-sampled 180° front LiDAR, linear/angular velocity, distance and heading to target. Actions: continuous linear and angular velocity (normalized). Reward: +400 on reach, -100 on collision, shaped per-step reward (vx - |vθ|)/α with α=10.",
            "agent_type": "simulated robotic agent (Pioneer3DX in Gazebo)",
            "environment_name": "Normal world (training); evaluated in Empty / Sparse / Normal evaluation map",
            "environment_description": "Three scenario types defined by obstacle density: Empty world (no obstacles), Sparse world (low obstacle density), Normal world (higher/dense obstacle density). Evaluation map used for generalization tests contains three spatial regions (left: dense, middle: sparse, right: no obstacles) and a sequence of targets to visit; environments are simulated in Gazebo with mobile Pioneer3DX model.",
            "complexity_measure": "Categorical obstacle density of map (Empty / Sparse / Normal). In Auto-CL experiments complexity is also parameterized by number of boxes (b_num) present in environment, with b_num ∈ [initial ... b_max].",
            "complexity_level": "Trained in: Normal = high complexity (dense obstacles); Evaluation includes Medium (Sparse) and Low (Empty).",
            "variation_measure": "Variation is low for this agent (trained in single environment only — Normal). Variation across evaluation domains measured by testing in different obstacle-density worlds (Empty, Sparse, Normal) and an evaluation map with three regions of differing densities.",
            "variation_level": "Low during training (single scenario), Variation in testing: medium-to-high (tested across three distinct densities).",
            "performance_metric": "Success rate (reach target), average path length, average completion time (and their std / min / max over evaluation runs).",
            "performance_value": "On baseline evaluations reported in paper: (from Table 2 at 0.42 million training steps) Empty success rate = 1.00 (but with many detours), Sparse success rate = 0.75, Normal success rate = 0.85. In multi-target evaluation (Table 4) reported metrics (units not explicitly stated): Avg path = 395.67, Avg time = 299.58 (other per-run min/max/std given in table but formatting in paper is inconsistent).",
            "complexity_variation_relationship": "Yes — the paper reports that training exclusively in a single (Normal) high-complexity environment yields poor generalization to other densities; as training progressed the agent's performance in Sparse and Empty worlds often decreased to near zero, indicating overfitting to the Normal-world distribution and a sensitivity to variation in obstacle density. The authors interpret this as an inability to handle environment variation when complexity and variation distributions differ between train and test.",
            "high_complexity_low_variation_performance": "Normal world (trained and tested same domain): success rate ≈ 0.85 (at 0.42M steps); performance good in-domain but degrades when training steps change (see Table 2 showing unstable generalization).",
            "low_complexity_high_variation_performance": "Tested in Empty world while trained only in Normal: success rate sometimes 1.00 at 0.42M steps but routes contain large detours and are sub-optimal; with other training steps success rate dropped to 0 (Table 2), indicating unreliable performance under low-complexity target but distributional mismatch.",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment training (standard TD3 trained in Normal world only)",
            "generalization_tested": true,
            "generalization_results": "Poor: agent trained only in Normal world overfits to that distribution; shows detours and sub-optimal routing on evaluation map that mixes densities, with success rates that decline sharply in Sparse/Empty test worlds as training continues.",
            "sample_efficiency": "Training scale reported in experiments: examples include 0.42 million, 0.51 million, and 1.2 million training steps for the standard TD3 baseline; other TD3 hyperparams: batch size 40, replay buffer 1e6, learning rate 1e-4, discount 0.99999.",
            "key_findings": "Baseline TD3 trained in a single (Normal) environment displays limited generalization to worlds with different obstacle densities; increasing training steps does not guarantee better cross-environment performance and can lead to catastrophic loss of performance in other densities (overfitting).",
            "uuid": "e1015.0"
        },
        {
            "name_short": "S-N curriculum",
            "name_full": "Sparse-to-Normal curriculum-trained agent",
            "brief_description": "A TD3 agent trained with a two-stage curriculum that first trains in Sparse (easy) environments and then in Normal (harder) environments; improves generalization across densities compared to single-environment training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "S-N curriculum-trained TD3 agent",
            "agent_description": "Same TD3 agent architecture as baseline, but training follows a curriculum: first train in Sparse world then in Normal world (S -&gt; N). Observations and actions identical to baseline.",
            "agent_type": "simulated robotic agent (Pioneer3DX in Gazebo)",
            "environment_name": "Two-stage curriculum: Sparse world then Normal world; evaluated in Empty / Sparse / Normal evaluation map",
            "environment_description": "Sparse world: few obstacles; Normal world: denser obstacles. Curriculum trains agent sequentially from lower obstacle density to higher, intending to expose agent to increasing data complexity.",
            "complexity_measure": "Obstacle density (categorical) across stages; measured implicitly via world label (Sparse vs Normal) and average path-length metrics collected during evaluation.",
            "complexity_level": "Training progression: low complexity (Sparse) → higher complexity (Normal).",
            "variation_measure": "Moderate variation introduced by switching scenario between two training stages (S then N); evaluation uses three-region map mixing densities.",
            "variation_level": "Medium (two distinct training stages increases variation versus single-environment training).",
            "performance_metric": "Success rate, average path length (per scenario), evaluated also on multi-target route length/time.",
            "performance_value": "From Table 3 and Table 4 (paper): Table 3 — Empty: success = 1.00, avg path length = 4.61; Sparse: success = 0.95, avg path = 4.78; Normal: success = 0.90, avg path = 6.95. Table 4 (multi-target evaluation): Avg path = 76.32, Max path = 83.70, Min path = 72.25, Path σ = 3.33; Avg time = 202.96 (units not explicitly stated), Max time = 225.72, Min time = 178.74, Time σ = 12.37.",
            "complexity_variation_relationship": "Explicit: the paper finds that training from easy (Sparse) to hard (Normal) (S-N) yields better cross-domain generalization than training only in Normal. S-N effectively reduces detours and improves success on mixed-density evaluation. The authors also compare reverse-order curricula and find that training order matters — training from hard to easy can cause forgetting of complex-domain skills.",
            "high_complexity_low_variation_performance": "After S-N training, Normal world (high complexity) performance: success ≈ 0.90, avg path length ≈ 6.95 (Table 3).",
            "low_complexity_high_variation_performance": "Empty world after S-N training: success = 1.00, avg path length = 4.61 (Table 3), indicating strong performance in low complexity region despite exposure to higher complexity later.",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (two-stage: Sparse → Normal)",
            "generalization_tested": true,
            "generalization_results": "S-N improved generalization relative to single-environment TD3: higher success rates and shorter, more direct routes on mixed-density evaluation map; S-N is among the best performing hand-designed curricula in the experiments.",
            "sample_efficiency": "Trained with comparable budget to other methods in the study (experiments report training steps on the order of 0.42–1.2 million steps for different agents).",
            "key_findings": "Carefully-designed two-stage curriculum (easy → hard) improves navigation generalization across environments with different obstacle densities; S-N reduces detours and yields high success rates in both low- and medium-complexity test regions. Additional stages (S-N-S) did not meaningfully improve over S-N in these experiments.",
            "uuid": "e1015.1"
        },
        {
            "name_short": "Auto-CL",
            "name_full": "Automatic Curriculum Learning (Auto-CL)",
            "brief_description": "An automated curriculum mechanism that adapts environment complexity by incrementally adding obstacles (boxes) based on agent performance thresholds, integrated with TD3 training to improve generalization across environment densities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Auto-CL-trained TD3 agent",
            "agent_description": "TD3 agent trained with an automatic curriculum: each training episode is run in an environment containing b_num boxes; after evaluation, if the agent's score exceeds score_th twice and b_num &lt; b_max then b_num is increased by b_add, gradually increasing environment complexity during training. Algorithm parameters: b_num=4, b_add=1, b_max=36, score_th=350.",
            "agent_type": "simulated robotic agent (Pioneer3DX in Gazebo)",
            "environment_name": "Auto-generated box environments (Auto-CL) and evaluation map with three regions (dense/sparse/empty)",
            "environment_description": "Training environments are procedurally varied by adjusting the number of boxes (obstacles) placed in the map. Complexity grows automatically driven by agent performance; evaluation uses a fixed multi-region map mixing dense, sparse and empty regions (targets placed across map).",
            "complexity_measure": "Number of boxes present in the environment (b_num); obstacle density categories (Empty/Sparse/Normal) used for benchmarking. Auto-CL explicitly controls b_num ∈ [4 .. 36].",
            "complexity_level": "Varies during training from low (b_num initial = 4) to up to high (b_max = 36).",
            "variation_measure": "Adaptive procedural variation via incrementing b_num when performance threshold (score_th) is exceeded; evaluation tests across three fixed densities and a mixed-density evaluation map. Variation is both across episodes and across training progression.",
            "variation_level": "High (automatically varied complexity across training episodes according to performance).",
            "performance_metric": "Success rate, average path length, average completion time, per-run min/max/std; reported relative improvements versus other curricula.",
            "performance_value": "From Table 3 and Table 4: Table 3 — Empty: success = 1.00, avg path length = 4.50; Sparse: success = 0.95, avg path length = 5.15; Normal: success = 0.95, avg path length = 5.95. Table 4 (multi-target evaluation): Avg path = 71.70, Max path = 75.30, Min path = 68.29, Path σ = 2.07; Avg time = 160.24, Max time = 177.85, Min time = 151.00, Time σ = 8.62. Paper reports Auto-CL achieves &gt;6% shorter path and &gt;21% shorter time compared to S-N on this evaluation.",
            "complexity_variation_relationship": "Yes — the paper argues and empirically demonstrates that gradually increasing complexity (variation) based on agent performance (Auto-CL) yields better generalization than fixed single-environment training or hand-designed curricula. Auto-CL reduces detours and completion time across mixed-density tests, indicating a favorable trade-off: adaptive variation during training reduces overfitting to any single density while still building capability for higher-complexity regions.",
            "high_complexity_low_variation_performance": "Normal world after Auto-CL training: success = 0.95, avg path length = 5.95 (Table 3).",
            "low_complexity_high_variation_performance": "Empty world after Auto-CL training: success = 1.00, avg path length = 4.50 (Table 3).",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum learning (Auto-CL): adaptive procedural complexity increase using parameters b_num, b_add, b_max and a score threshold score_th.",
            "generalization_tested": true,
            "generalization_results": "Auto-CL generalized better than both the standard TD3 baseline and hand-designed curricula (S-N): in the mixed-density multi-target evaluation it produced the shortest average path (≈71.70) and shortest average time (≈160.24) with smaller standard deviations; reported improvements versus S-N: ≈6.05% shorter path, ≈21.05% shorter time (Table 4). Auto-CL achieved near-perfect success rates across Empty/Sparse/Normal test sets (1.00, 0.95, 0.95 respectively).",
            "sample_efficiency": "Training budget reported similarly to other agents; experiments reference training steps in the order of 0.42–1.2 million steps for different runs. Auto-CL hyperparameters controlling environment progression (b_num growth) are specified, but paper does not report a separate reduced-step training budget for Auto-CL.",
            "key_findings": "Automated incremental complexity (Auto-CL) produces better generalization to new, mixed-density environments than single-environment TD3 and most hand-designed curricula; Auto-CL achieves shorter paths and times with lower variance, and avoids some forgetting observed in reverse-order curricula. The relationship observed is that controlled increasing of complexity (variation) tied to agent competence produces a favorable trade-off: improved cross-environment performance without excessive in-domain overfitting.",
            "uuid": "e1015.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A two-stage reinforcement learning approach for multi-UAV collision avoidance under imperfect sensing.",
            "rating": 2,
            "sanitized_title": "a_twostage_reinforcement_learning_approach_for_multiuav_collision_avoidance_under_imperfect_sensing"
        },
        {
            "paper_title": "Cm3: Cooperative multi-goal multi-stage multi-agent reinforcement learning",
            "rating": 2,
            "sanitized_title": "cm3_cooperative_multigoal_multistage_multiagent_reinforcement_learning"
        },
        {
            "paper_title": "A survey on curriculum learning",
            "rating": 2,
            "sanitized_title": "a_survey_on_curriculum_learning"
        },
        {
            "paper_title": "Addressing function approximation error in actor-critic methods",
            "rating": 1,
            "sanitized_title": "addressing_function_approximation_error_in_actorcritic_methods"
        },
        {
            "paper_title": "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation",
            "rating": 1,
            "sanitized_title": "virtualtoreal_deep_reinforcement_learning_continuous_control_of_mobile_robots_for_mapless_navigation"
        }
    ],
    "cost": 0.0160305,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generalization Ability of Deep Reinforcement Learning-based Navigation Based on Curriculum Learning</p>
<p>Yuchen He 
School of Cyber Security
Southeast University
Nanjing</p>
<p>Simone Baldi 
School of Cyber Security
Southeast University
Nanjing</p>
<p>Generalization Ability of Deep Reinforcement Learning-based Navigation Based on Curriculum Learning
6C6A56E5F9B256948CF95CA81633C11110.1088/1742-6596/2593/1/012003mobile robotdeep reinforcement learningcurriculum learningnavigation
In this paper, curriculum learning is studied as an approach to improve generalization ability in navigation tasks, that is, improve the agent's ability of navigating in scenarios different from those used for training.The agent is trained based on the TD3 algorithm, and curriculum learning selects different stages (i.e.different curricula) of Empty/Sparse/Normal worlds for training.Via extensive numerical comparisons with agents trained under such curricula, it is shown that properly used curriculum learning improves the agent's ability of generalization.Furthermore, an automatic curriculum learning (Auto-CL) approach is proposed.Auto-CL is shown to have even better generalization than the standard curriculum learning, since it makes the agent able to navigate in new environments with more than 6% shorter paths in more than 21% shorter time.</p>
<p>Introduction</p>
<p>In recent years, mobile robots, such as sweeping robots, Automated Guided Vehicle (AGVs), unmanned underwater robots, and other unmanned robots, have been widely used for domestic and industrial tasks.As a basic ability of mobile robots, the ability of navigation and obstacle avoidance have been widely studied.Unfortunately, despite the progress in the field, traditional navigation methods still need a well-structured system and lots of tuning to ensure satisfactory performance [1].</p>
<p>With the progress in deep neural networks, deep reinforcement learning has gradually overcome traditional methods.Although some shortcomings exist like lack of generalization and sub-optimal solutions [2,3], it is widely recognized that deep reinforcement learning has the potential to be used in complex situations like navigation and obstacle avoidance.As a result, there is growing interest among scholars in utilizing deep reinforcement learning methods to address navigation and obstacle avoidance challenges [4,5].</p>
<p>Due to the diversity of network structures and the many tweaks needed to improve efficiency, several learning-based methods have been proposed for navigation and obstacle avoidance: note that these methods typically show good performance in maps similar to those used for training [4,5].Unfortunately, it can be easily verified that agents trained with these methods perform poorly in maps different from those used for training, that is, they lack the ability to generalize their knowledge in new maps.This lack of generalization will surely affect the deployment of deep reinforcement learning in the real world.An example of lack of generalization is shown in Section 4.2 in this work.This study aims to improve the generalization capability of deep reinforcement learning through the use of curriculum learning.</p>
<p>Curriculum learning (CL) is widely used in the domain of deep learning and machine learning [6,7], which is a method that has been explored to enhance the generalization and data efficiency of agents [8].The main idea of CL is to set a series of human-like courses for training and train the agent from easy to hard tasks [9].The terms "easy" and "hard" mainly refers to data complexity: in the context of navigation, data complexity is mostly determined by the map complexity.</p>
<p>In this work, agents are trained under different curricula and are compared on the ability to navigate in different maps.Furthermore, an automatic curriculum learning method (Auto-CL) that transfers a multi-stage training process into a single-stage training process by adjusting a few hyperparameters of the map automatically is proposed.The main contributions of this work are as follow:</p>
<p>• Comparing the performance of agents that are trained under different curricula with the performance of state-of-the-art deep reinforcement learning training.It is found that while the standard deep reinforcement learning approach is unable to generalize, an optimally designed two-stage curriculum learning approach can improve the agents' ability to navigate in different maps.</p>
<p>• Proposing an automatic curriculum learning method that can simplify multi-stage training process and improve the agents' ability to navigate in different maps.As compared to the standard curriculum learning, the proposed Auto-CL has 6% improved performance in terms of shorter path and more than 21% improved performance in terms of shorter time.The rest of the work is organized as follows: Section 2 reviews related works in the field, Section 3 describes the proposed methods, Section 4 gives the experimental results and Section 5 gives concluding remarks.</p>
<p>Related Works</p>
<p>Deep reinforcement learning methods have been used in different ways to solve navigation problems.They can be used based on different sensors, e.g.vision-based [10][11][12] or LiDAR-based [13][14][15], to achieve end-to-end control.In [11], the authors propose a training method of vision-based robot to decrease the gap between simulation and reality.The authors of [12] propose a structure of two networks to achieve environment exploration and target localization separately.In [14], the authors use down-sampled LiDAR data to let an agent work with cheaper LiDAR, while in [15], a high dimensional LiDAR data is used for observation with an additional fully connected layer to have a more refined control.Also, deep reinforcement learning methods can be used to replace the function of a certain module in navigation system [16] and besides local observation, the authors also use global map information in robot's field of view, which gives the agent a global view and makes local motion planning more oriented.It is worth remarking that in all aforementioned approaches, the testing maps are analogous to the training maps, thus the generalization ability of the agents is unclear.</p>
<p>The frameworks in [10,15] provide learning-based navigation training that contain different maps for various kinds of mobile robots.This is done in order to let the agent learn to navigate in different scenarios.However, even here there are no testing maps that differ substantially from the training maps.</p>
<p>Curriculum learning (CL) methods are mainly used in deep learning and machine learning [6,7]: in the robotic domain, CL is mainly used in multi-agent deep reinforcement learning navigation area and can reduce difficulty of multi-agent navigation training process with sparse reward [17][18][19].The authors of [17] use a multi-stage training process when dealing with multi-robot obstacle avoidance to reduce training difficulty, whereas [18] suggest a two-stage reinforcement learning strategy for addressing multi-UAV collision avoidance in the presence of imperfect sensing.Finally, [19] use a novel two-stage curriculum to solve multi-goal multi-agent control problems.The main issue of curriculum learning is that the curricula should be carefully selected: some trial-and-error is needed in the selection, and there is no automatic way to generate curricula in an optimal way.</p>
<p>Proposed Approach</p>
<p>Problem Formulation</p>
<p>The interaction between an agent and its environment can be modelled as a Markov Decision Process (MDP).An MDP is defined by a tuple (, , (, ), (, )), where  is the state space set,  is the action space set, (, ) represents the probability distribution of state transitions, that is, the possibility of state changing from   to  +1 after taking action   .(, ) is the reward agents can get at state   under action   .</p>
<p>(, ) is a critic function that maps state   and action   to the  value of the corresponding state-action pair.This Q value represents the expected cumulative reward of taking action   in state   under policy .Roughly speaking, higher  value in state   indicate a more optimal policy.</p>
<p>The observation space   can be described as   = (  ,   ,   ) where   represents down-sampled normalized LiDAR data within 180 degrees in front of robot,   = (  ,   ), comprises the linear and the angular velocity,   = (, ) comprises the distance between the robot and the target, and the difference between the current and the desired heading.The action space   can be described as   = (  ,   ), Let us recall that   is always non-negative because the LiDAR can only detect 180 degrees in front of the robot.Also, the actions (  and   ) is normalized to [-1, 1] during training process to improve the training performance.</p>
<p>Algorithm and Policy Parameterization</p>
<p>TD3(Twin Delayed Deep Deterministic Policy Gradients) [2] is an extension of the DDPG algorithm, it uses clipped double Q-learning to reduce overestimation bias, uses delayed policy updates to increase the convergence of policy and uses target policy smoothing to increase the stability and exploration of policy.The detail of TD3 is shown in Algorithm.1.In this study, TD3 algorithm is employed to train the agent, with a network structure architecture illustrated in Fig. 1.The actor-critic network is utilized, with FC denoting Fully Connected layer, ReLU and Tanh representing two different activation functions.The middle number in each gray block of Fig. 1 represents the number of neurons in the corresponding layer and the right number in each red block represents the vector dimension.The actor network has two fully connected layer with ReLU module followed.The last layer output two actions after Tanh module limits their range.Two critic networks are used in the TD3 algorithm whose goal is to estimate the  value: the minimum  value among these estimates is used as final estimate, so as to reduce the error caused by the overestimation of the  value during the training process.In critic network, LiDAR data and target information that are processed with two fully connected layer followed with ReLU are concatenated with action that are processed with one fully connected layer followed with ReLU, and then are fed into another fully connected layer.The last layer output the Q value.</p>
<p>The reward function used during the training process is as below:
𝑅(𝑠, 𝑎) = { 𝑟 1 𝑡𝑎𝑟𝑔𝑒𝑡 𝑟𝑒𝑎𝑐ℎ𝑒𝑑 𝑟 2 𝑐𝑜𝑙𝑙𝑖𝑠𝑖𝑜𝑛 (𝑣 𝑥 − |𝑣 𝜃 |)/𝛼 𝑒𝑎𝑐ℎ 𝑠𝑡𝑒𝑝𝑠
where  1 is the reward when the robot reaches the target,  2 is the punishment when collision occurs, and  is a hyperparameter to adjust the size of the reward.Among all these curriculum, S-N and E-N follow the idea of training from easy to hard.However, as the literature has shown that, in some cases, training from hard to easy can get a better performance [20,21], N-S and N-E is also considered for comparison.At last, S-N-S is set to show whether three training stages can give a better result than two.To further evaluate the performance of each agent under different curricula, an evaluation scenario is created in Gazebo, which is shown in Fig. 4.This evaluation map contains three regions, the left region of the map is a region with dense obstacles, the middle region has sparse obstacles and the right region has no obstacles.The map is not surrounded by walls and a series of targets (shown in Fig. 6) is set to test and evaluate agents' ability to navigate.</p>
<p>Curriculum Learning Settings</p>
<p>Problem Location</p>
<p>Reproducing the findings of [5] which use a standard TD3 approach reveals that, while the agent performs well in a single scenario, it struggles to navigate multiple scenarios effectively.This may be attributed to the fact that the agent was solely trained in the Normal world.We further test their agent in Normal world, Sparse world and Empty world and collect the results in Tab.2.The table's rows represent the standard TD3 with different training steps and the table's columns represent rate of success in reaching the target tested in different scenarios.The results show that as the training steps increase, the ability of navigation of the standard TD3 increases in Normal world, but decreases rapidly to 0 in Sparse world and Empty world.Even though the rate of success is 1.0 in Empty world for 0.42 million training steps, still there are a lot of detours and sub-optimal routes.This is shown in Fig. 5, where the green point is the target and the red points represent the routes resulting from the standard TD3, exhibiting large detours.The results show that the automatic curriculum performs the best in Empty world and Normal world and the second best in Sparse world in terms of success rate.Among the other five curricula, N-S is the best in Sparse world and S-N-S is the best in Normal world.Except E-N, all other curricula achieve perfect rate of success in Empty world.For comparison, N-S and N-E have poor successful rate in Normal world, meaning that the agent forgets the ability to navigate in Normal world after being trained in an Empty or Sparse scenario.E-N also face a similar problem: the agent forgets the ability to navigate in Empty world after being trained in the Normal scenario: this may indicate that the gap between the Normal and the Empty world is too large.S-N-S has a similar performance as S-N, which indicates that S-N is effective enough and S-N-S gives no improvement.On average path length, the performance of agents trained under five curricula and Auto-CL is comparable in Empty world, with S-N-S being the best.In Sparse world, N-S outperforms the others, while in Normal world, Auto-CL has the best performance.This supports the conclusion that N-S and Auto-CL are the most effective curricula in Sparse world and Normal world respectively.</p>
<p>Evaluation</p>
<p>To further evaluate the performance of different curricula, S-N's and Auto-CL's ability to navigate in new evaluation scenarios is compared with standard TD3's (shown in Fig. 4).Fig. 6a depicts the positions and order of the targets, with the blue point representing the starting point, the red points representing the target points, and the green lines indicating the order of the target.The order of the targets is indicated by green lines.The green arrows in Fig. 6b-d   Fig. 6b shows the route of the agent trained with standard TD3: it can be noted that there are a lot of detours, which means that the baseline method standard TD3 can hardly let agent finish navigation tasks in a map with different regions that have different obstacle densities.As compared to the standard TD3, the route of S-N shown in Fig. 6c is much better.The route of Auto-CL shown in Fig. 6d is even shorter as compared to the route of S-N, which have the best performance in the navigation task in a map with varying obstacle density.The data in Tab.4 give a quantitative comparison among the three methods.Tab.4 shows that Auto-CL has the shortest path length (more than 6%) and shortest time (more than 21%), with smaller standard deviation than the other two methods.</p>
<p>Conclusion</p>
<p>In this paper, a curriculum learning method is used to improve an agent's ability to navigate in different scenarios.Agents trained under different curricula are compared and an automatic curriculum learning method to automatically generate new training scenarios is proposed.The automatic curriculum method gives the best performance, not only on the trained scenarios but, most importantly, also on new testing scenarios, indicating its potential applicability in specific scenarios.</p>
<p>As future work, optimizing the distribution of data complexity in automatic curriculum learning can be implemented by developing a more detailed classification based on data complexity.Furthermore, offline reinforcement learning methods can be employed to enhance data efficiency and provide more refined control over the curriculum learning process.Some text.</p>
<p>Algorithm. 1 : 2 𝐵𝑓𝑜𝑟 𝑖 = 1, 2 12
122
TD3 algorithm 1 Initialize actor network parameterized , two critic networks  1 ,  2 , Empty replay buffer .Initialize target network:  ′ ← ,  1 ′ ←  1 ,  2 ′ ←  2 2 for i in episodes: 3 while not : 4 choose action based on state :  = (  () + ,   ,  ℎℎ ) and execute . 5 observe new state  ′ , reward  and the finish signal  6 save (, , ,  ′ , ) into  7 end while 8 sample a minibatch,  = {(, , ,  ′ , )} from  9 compute target action:  ′ ( ′ ) = (  ′ ( ′ ) + ,   ,  ℎℎ ) 10 choose the minimal Q of two target networks: (,  ′ , ) =  +  ×  × min =1,2    ′ ( ′ ,  ′ ( ′ )) 11 update critic networks: ∇ ϕ i 1 || ∑ (   (, ) − (,  ′ , )) if i mod d then: 13 update actor network: ∇  1 ||    (,   ()) 14 update target networks: ϕ i ′ ←   ′ + (1 − )    = 1,2  ′ ←  ′ + (1 − ) 15 end if 16 end for</p>
<p>Three training scenarios are set to get data of different complexity and are shown in Fig.2.They are called Normal world, Sparse world and Empty world.Normal world, shown on the left of Fig.2, means a world with normal obstacle density, while Sparse world and Empty world means world with sparse obstacle density and with no obstacle at all.</p>
<p>Fig. 2 .
2
Fig. 2. Training scenarios: Normal world (on the left), Sparse world (in the middle) and Empty world (on the right).Given three scenarios, different curriculum structure trains agent with different combination of these scenarios.Let N represent the training in Normal world, S represent the training in Sparse world and E represent the training in Empty world.Based on this notation, agents are trained under five curricula: S-N, N-S, E-N, N-E, S-N-S, and compare the ability of navigation in different scenarios.Among all these curriculum, S-N and E-N follow the idea of training from easy to hard.However, as the literature has shown that, in some cases, training from hard to easy can get a better performance[20,21], N-S and N-E is also considered for comparison.At last, S-N-S is set to show whether three training stages can give a better result than two.</p>
<p>Fig. 1 .Fig. 3 .
13
Fig. 1.Actor and critic structure in TD3 algorithm</p>
<p>4 .
4
Experiments and analysis 4.1.Experiment Settings A Pioneer3DX robot is used during the whole training process.Deep reinforcement learning is implemented in Gazebo on a computer equipped with python3.8,cuda11.3,pytorch1.11.0.CPU 2.6GHz, Platinum 8350C CPU, RAM 16GB, GPU RTX3080Ti, RAM 12GB.Training and validating environment use ROS Noetic for communication and use Rviz for auxiliary display.The reward  1 is chosen to be 400, reward  2 is -100 and  is 10.The robot's max linear velocity is 0.5 m/s and its max angular velocity is 1.0 rad/s.In this work, parameters in Algorithm.2are b_num= 4, b_add=1, b_max=36 and score_th=350.During the training process, the agent concludes the current episode and updates the network once the robot reaches a target or collides with an obstacle.Some parameters used during deep reinforcement learning training process are shown in Tab.1.</p>
<p>Fig. 6 .
6
Fig. 6.Target information and routes of different agents in grid map.</p>
<p>Algorithm.2: Automatic curriculum learning (Auto-CL) Initialize parameter</p>
<p>1 Initial number of boxes in environment, b_num. 2 Number of boxes to be added each time, b_add.3 Max number of boxes in environment, b_max.4 Threshold score to increase complexity, score_th.
Training process5 for i in n_episode:6 initialize and put b_num boxes into environment.7 agent interact with environment8 network updates9 evaluate and get score.10 if score &gt; score_th twice and b_num &lt; b_max:11b_num += b_add</p>
<p>Table . 1
.
. Deep reinforcement learning parameters for TD3-based training
ParameterValueLearning Rate1e-4Batch size40Discount factor 𝛾0.99999Replay buffer size1e6Policy noise0.2Noise clip0.5Freq updates actor2</p>
<p>Table . 2
.
Rate of success of standard TD3 agent under different scenarios Performance of 5 different curricula and the automatic curriculum mentioned in Algorithm.1 are compared and the results is shown in Tab.3.The table's rows represent different curricula and the table's columns represent the rate of success and average path length of reaching the same target under different scenarios.The standard TD3 is the agent trained under single stage and its performance is treated as baseline of all curricula.
Training stepsEmpty worldSparse worldNormal world0.42million steps1.000.750.850.51 million steps00.200.901.2 million steps000.954.3. Comparison of different curriculum</p>
<p>Table . 3
.
Rate of success and average path length of different curricula under different scenarios
EmptySparseNormalCurriculumSuccess rateAvg path lengthSuccess rateAvg path lengthSuccess rateAvg path lengthstandard TD31.005.650.756.670.858.10S-N1.004.610.954.780.906.95N-S1.004.440.964.410.65-E-N0.45-0.70-0.907.29N-E1.004.600.70-0.40-S-N-S1.004.390.954.810.917.10Auto-CL1.004.500.955.150.955.95</p>
<p>Table . 4
.
Evaluation Metrics of three methods
Avg pathMax pathMin pathPath 𝝈Avg timeMax timeMin timeTime 𝝈standard TD395.67107.9388.986.05299.58388.86246.0940.42S-N76.3283.7072.253.33202.96225.72178.7412.37Auto-CL71.7075.3068.292.07160.24177.85151.008.62Improvement of Auto-CL wrt S-N6.05%10.04%5.48%37.84%21.05%21.21%15.52%30.32%</p>
<p>The office marathon: Robust navigation in an. E Marder-Eppstein, E Berger, T Foote, 2010IEEE</p>
<p>Addressing function approximation error in actor. S Fujimoto, H Hoof, D Meger, PMLR2018</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, arXiv:1707.063472017arXiv preprint</p>
<p>Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation. L Tai, G Paolo, M Liu, </p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2017</p>
<p>Goal-Driven Autonomous Exploration Through Deep Reinforcement Learning. R Cimurs, I H Suh, J H Lee, </p>
<p>. IEEE Robotics and Automation Letters. 722021</p>
<p>Curriculumnet: Weakly supervised learning from large-scale web. S Guo, W Huang, H Zhang, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Self-paced learning with diversity. L Jiang, D Meng, S I Yu, Advances in neural information processing systems. 201427</p>
<p>A survey on curriculum learning. X Wang, Y Chen, W Zhu, </p>
<p>Elman J L. Learning and development in neural networks: The importance of starting small. IEEE Transactions on Pattern Analysis and Machine Intelligence. 4492021</p>
<p>. Cognition. 4811993</p>
<p>Learning and development in neural networks: The importance of starting small. J Elman, </p>
<p>. Cognition. 4811993</p>
<p>Towards monocular vision based obstacle avoidance through deep reinforcement learning. L Xie, S Wang, A Markham, arXiv:1706.098292017arXiv preprint</p>
<p>Visual navigation in real-world indoor environments using end-to-end deep reinforcement learning. J Kulhánek, E Derner, R Babuška, </p>
<p>. IEEE Robotics and Automation Letters. 632021</p>
<p>Towards generalization in target-driven visual navigation by using deep reinforcement learning. A Devo, G Mezzetti, G Costante, </p>
<p>. IEEE Transactions on Robotics. 3652020</p>
<p>Deep reinforcement learning for map-less goal-driven robot navigation. M Dobrevski, D Skočaj, International Journal of Advanced Robotic Systems. 18117298814219926212021</p>
<p>Robust RL-based map-less local planning: Using 2D point clouds as observations. F Leiva, J Ruiz-Del-Solar, </p>
<p>. IEEE Robotics and Automation Letters. 542020</p>
<p>Arena-Rosnav: Towards deployment of deep-reinforcementlearning-based obstacle avoidance into conventional autonomous navigation systems. L Kä Stner, T Buiyan, L Jiao, C]//2021</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Mobile robot path planning in dynamic environments through globally guided reinforcement learning. B Wang, Z Liu, Q Li, </p>
<p>. IEEE Robotics and Automation Letters. 542020</p>
<p>Towards optimally decentralized multi-robot collision avoidance via deep reinforcement. P Long, T Fan, X Liao, IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2018. 2018</p>
<p>A two-stage reinforcement learning approach for multi-UAV collision avoidance under imperfect sensing. D Wang, T Fan, T Han, </p>
<p>. IEEE Robotics and Automation Letters. 522020</p>
<p>Cm3: Cooperative multi-goal multi-stage multi-agent reinforcement learning. J Yang, A Nakhaei, D Isele, arXiv:1809.051882018arXiv preprint</p>
<p>Learning to teach. Y Fan, F Tian, T Qin, arXiv:1805.036432018arXiv preprint</p>
<p>Dynamically composing domain-data selection with clean-data selection by" co-curricular learning" for neural machine translation. W Wang, I Caswell, C Chelba, arXiv:1906.011302019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
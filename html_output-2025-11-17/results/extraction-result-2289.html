<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2289 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2289</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2289</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-202579608</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1909.12072v1.pdf" target="_blank">Towards Explainable Artificial Intelligence</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today's ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been generally considered"black boxes", not providing any information about what exactly makes them arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this field and makes a plea for a wider use of explainable learning algorithms in practice.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2289.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2289.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Layer-wise Relevance Propagation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A propagation-based explanation framework that attributes a model's prediction back to input features by redistributing relevance from outputs to inputs using local rules; applicable to deep networks, LSTMs and other models and designed to be computationally efficient (one forward and one backward pass).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General ML interpretability; applied across neuroscience, health, autonomous driving, drug design, physics (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide pixel-/feature-wise explanations of individual model predictions to detect spurious correlates, foster trust, and derive scientific insight from complex ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>images, time-series (e.g., EEG), sequences (text), potentially other high-dimensional inputs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — explains complex, highly non-linear deep models with many parameters and high-dimensional inputs where internal representations are nested and non-intuitive.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature ML methodology applied to established scientific domains with existing domain knowledge (neuroscience, medical imaging, physics).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - interpretability is required to verify mechanisms, detect bias, and extract scientific insights from data-driven models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Layer-wise Relevance Propagation (explainability method for neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>LRP redistributes the prediction score backward through the network using theoretically motivated local redistribution rules (deep Taylor decomposition connection), producing relevance scores per input dimension; applicable to deep nets, LSTMs, Fisher Vector classifiers and to models transformed into neural networks ('neuralization').</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Explainability / post-hoc attribution (leveraging model internals)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate for high-dimensional, non-linear models where per-feature attribution is required; leverages model structure and avoids gradient-shattering issues, but requires access to model internals or a differentiable-neuralization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported as delivering good-quality explanations at low computational cost relative to perturbation/surrogate methods; robust to gradient shattering and supports pixelwise, high-resolution explanations useful for bias detection (e.g., 'Clever Hans').</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables verification, detection of spurious dataset artifacts, fosters scientific insights and can be integrated into model development, validation, and potentially model compression/pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to gradient-based methods and perturbation/surrogate approaches; LRP is argued to be faster (one forward/backward pass) and to avoid gradient discontinuities; alternatives include Guided Backpropagation, Deconvolution, LIME, SmoothGrad and perturbation methods which tend to be more computationally expensive or less stable.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Leverages internal model structure, theoretically motivated redistribution rules, generality across architectures, and low computational cost (one forward/backward pass) facilitating practical use.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>For high-dimensional, non-linear scientific prediction tasks that require mechanistic interpretability, propagation-based attributions like LRP provide efficient, detailed explanations that better reveal model strategies and dataset artifacts than many model-agnostic or gradient-only methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2289.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Interpretable Model-agnostic Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic explanation technique that fits an interpretable surrogate model locally around a prediction by sampling the neighborhood of the input and learning a simple approximating model to explain the original model's decision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Why should i trust you?: Explaining the predictions of any classifier</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General ML interpretability; used across domains where model internals are unavailable or uninterpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Explain individual predictions of complex black-box models by approximating them with locally interpretable models to provide human-understandable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>tabular, images, text (depends on interpretable input representation used by surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate to high — targets complex, non-linear models but explanation is local and thus simplifies complexity to a local surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Method is mature as a general interpretability tool; applied in various domains but with limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - used where some interpretability is desired but full mechanistic models may be unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>LIME (surrogate model-based explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Samples perturbations around an input, queries the black-box model on these perturbed samples, and fits a simple interpretable model (e.g., linear model or decision tree) weighted by proximity to the original input to explain the prediction locally; model-agnostic and does not require access to internals.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Explainability / model-agnostic surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when model internals are inaccessible and a local, human-interpretable explanation suffices; less suitable when global explanations or high-resolution attribution are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Useful for coarse, human-interpretable explanations but has high computational cost and can be slow (minutes per explanation for large state-of-the-art models) and may miss fine-grained or global behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Medium — democratizes explanations for black-box systems but computational expense and locality limit scalability for large models or exhaustive validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to gradient- and propagation-based methods: LIME is model-agnostic (no internals needed) but slower and provides coarser explanations; alternatives like LRP leverage internals for faster, higher-resolution explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Model-agnosticism and human-interpretable surrogate models; limitations are computational expense and locality of approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Model-agnostic local surrogates are useful when internals are unavailable, but their high computational cost and coarse locality limit their utility for large-scale, high-resolution scientific explainability tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2289.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmoothGrad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmoothGrad (gradient-averaging explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explanation method that reduces noise in gradient-based saliency maps by averaging gradients computed for multiple noisy versions of the input, improving visualization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smoothgrad: removing noise by adding noise</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General interpretability for image and other continuous input domains</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve gradient-based saliency explanations by reducing visual noise and producing more stable attribution maps.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>images or other continuous high-dimensional inputs where gradients are computable</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High non-linearity and high dimensionality; gradients may suffer from fragmentation/discontinuities.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature as a gradient-based enhancement method within interpretability research.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - used to enhance visual interpretability rather than provide causal mechanistic explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>SmoothGrad (gradient-averaging)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Samples Gaussian noise around the input, computes gradients for each noisy sample, and averages them to produce a smoother saliency map; requires gradient access to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Explainability / gradient-based attribution</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable where gradients are available and saliency visualizations are desired; not model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Improves visual smoothness and reduces noisy artifacts in gradient-based heatmaps but still inherits limitations of gradient explanations (sensitivity to gradient issues).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — improves usability of gradient explanations in practice, enhancing their interpretability for human users.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to raw gradient/saliency methods, SmoothGrad typically yields less noisy maps; compared to propagation-based LRP, SmoothGrad relies on gradients and may still be affected by gradient discontinuities.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simple averaging strategy, low implementation complexity, and effective noise reduction when gradients are available.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Averaging gradients over noisy inputs yields more interpretable saliency maps, but gradient-based approaches remain limited by fundamental gradient pathologies for complex models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2289.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpRAy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spectral Relevance Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-explanation technique that clusters individual explanation heatmaps to identify distinct prediction strategies and systematic model behaviors across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Model analysis across vision datasets (e.g., PASCAL VOC) and general classifier behavior analysis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Aggregate individual explanations to find global patterns and detect systematic biases or 'Clever Hans' strategies in classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>collections of explanation heatmaps derived from image datasets</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Meta-analysis over many high-dimensional explanations; requires clustering and spectral analysis techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging technique within interpretability literature for large-scale model auditing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - aimed at uncovering global model strategies and dataset artifacts to provide mechanistic insight into model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Spectral Relevance Analysis (SpRAy)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Compute explanation heatmaps for many instances, then apply spectral clustering to group similar heatmaps into clusters representing distinct prediction strategies; analyze clusters to identify spurious correlates or typical decision modes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Explainability / meta-explanation and unsupervised analysis</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well suited to datasets where many individual explanations can be generated; particularly useful for auditing vision models and detecting dataset artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated ability to identify distinct strategies (e.g., multiple clusters for 'horse' classification including copyright watermark detection), enabling systematic detection of Clever Hans predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for large-scale model validation and dataset auditing; can guide dataset curation and model improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides a dataset-level, semi-automated analysis complementary to single-instance explanation methods; alternatives focus on individual explanations only.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Aggregation of explanations, spectral clustering to reveal patterns, and interpretability of resulting clusters for human analysts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Aggregating instance-level explanations and clustering them reveals systematic prediction strategies and dataset artifacts that single-instance explanations may hide.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2289.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MeaningfulPerturbation / Activation Max</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meaningful Perturbation and Activation Maximization (optimization-based explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Optimization-formulated explanation approaches that either find minimal perturbations that most change the prediction (meaningful perturbation) or synthesize inputs that maximize a model response (activation maximization) to reveal representative prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interpretable explanations of black boxes by meaningful perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General interpretability across vision and other domains where prototypes or minimal informative regions are useful</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Produce explanations by solving optimization tasks that identify either critical input regions or prototypical inputs for a class.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>images or input domains where optimization in input space is feasible</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Computationally demanding — involves optimization in high-dimensional input spaces and may require many forward evaluations of the model.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Methodologically established within interpretability but computationally intensive and often used in research settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — attempts to provide prototypical or causally informative input patterns, but abstractions may remain low-level.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Meaningful perturbation / Activation maximization</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Formulate explanation as optimization: meaningful perturbation finds sparse masks or perturbations that most reduce model output; activation maximization searches for inputs that strongly activate particular neurons/classes, often with regularization or generative priors to yield interpretable prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Explainability / optimization-based attribution and feature visualization</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for creating prototypes and identifying maximally-informative input regions; computational cost limits scalability for per-instance exhaustive use.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Can produce compelling visual prototypes and identify key input regions, but optimization costs and sensitivity to regularization make results variable; useful to study internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — valuable for representation analysis and prototype generation, but limited for routine large-scale auditing due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides higher-level prototypes vs. pixelwise attributions from LRP/gradients; more computationally expensive than propagation-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Formulating explanations as an optimization problem and using priors/regularization to obtain interpretable solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Optimization-based explanations can reveal prototypical inputs and minimal informative perturbations, but high computational cost and sensitivity to regularization limit routine applicability for large scientific datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2289.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepTensorNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Tensor Neural Networks (Schütt et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep tensor neural network architectures designed to learn quantum-chemical properties from molecular structure, providing interpretable quantum-chemical insights from learned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantum-chemical insights from deep tensor neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Quantum chemistry / computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict quantum-chemical properties of molecules (e.g., energies, forces) from molecular structures to accelerate simulations and enable chemical insight.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured molecular data (graphs/3D coordinates) — high-dimensional geometric descriptors</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — many degrees of freedom, complex quantum-mechanical interactions, and high-dimensional input spaces (atomic positions/types).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established scientific domain with strong theoretical foundations; ML approaches are a relatively recent augmentation of traditional methods.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - scientific insight about interactions and physically plausible predictions are important; interpretability desirable to relate learned features to chemical concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep tensor neural networks (deep learning for quantum chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural architectures that model interactions between atoms using tensor representations and aggregation over molecular structure to predict quantum-chemical quantities; details are in the cited work but not expanded in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (domain-specific architecture for molecular data)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for predicting properties from molecular structures, offering speedups over ab initio methods while requiring high-quality training data.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported in cited work to provide quantum-chemical insights and accurate property predictions; survey cites it as an example of explainable / ML-driven scientific progress.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can accelerate quantum-chemical calculations and support discovery by enabling fast property prediction and interpretability of learned chemical representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>ML approaches complement or accelerate traditional quantum-chemical computations (e.g., DFT), offering trade-offs between accuracy and computational cost; explicit comparisons are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Architecture tailored to molecular structure and physically-informed representation learning yielding interpretable chemical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Specialized deep architectures that respect molecular structure can learn accurate quantum-chemical mappings and provide interpretable representations that bridge data-driven predictions and chemical theory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2289.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML Force Fields</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learned Force Fields (Chmiela et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ML models trained to reproduce forces and energies from quantum calculations to enable accurate molecular dynamics simulations at near ab initio accuracy with reduced compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards exact molecular dynamics simulations with machine-learned force fields</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Molecular dynamics / computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Replace or accelerate force-field calculations for molecular dynamics by learning potentials/force fields from high-fidelity quantum data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>molecular configurations with associated energies/forces (structured, high-dimensional geometric data)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high — requires learning complex many-body interactions and ensuring energy/force conservation and generalization across configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established domain; ML-based force fields are an active research direction augmenting classical/quantum methods.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - physical plausibility, conservation laws and interpretability of learned potentials are important for scientific trust and use in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Machine-learned force fields (supervised ML for potentials)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train models (e.g., kernel or neural network based) to predict forces/energies from atomic geometries to enable molecular dynamics with ML-derived potentials; details are in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning for physics-informed surrogate models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for accelerating molecular dynamics given sufficient high-quality quantum-mechanical training data; must ensure physical constraints are respected.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as enabling near-exact molecular dynamics simulations when trained on quantum data, representing a major acceleration opportunity for simulation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can drastically reduce computational cost of accurate MD simulations and enable longer timescale or larger-system studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>ML force fields trade off some interpretability for speed compared to full quantum methods; the cited work aims to approach exactness while benefiting from ML scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality quantum training data, architectures/learning schemes that capture many-body physics and respect physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>With sufficient high-fidelity quantum training data and appropriate model design, ML can produce force fields that enable near-accurate molecular dynamics at drastically lower cost than ab initio simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2289.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Computational Fluorescence Microscopy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine learning-based integrated prediction of morphological and molecular tumor profiles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ML approach to predict molecular tumor profiles from morphological imaging data (fluorescence or conventional microscopy), integrating imaging and molecular information for cancer profiling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Cancer pathology / computational microscopy</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict molecular or molecular-profile information (e.g., biomarkers) from morphological image data to enable integrated tumor characterization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>imaging data (microscopy, histopathology) with associated molecular labels; multimodal (image + molecular labels)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — high-dimensional image data with complex phenotype-genotype relations and potential confounding factors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Active and emerging application of ML in pathology; domain has established clinical workflows but ML integration is relatively recent.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - clinicians and researchers require interpretable predictions to trust molecular inferences from morphology and for biomarker discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised deep learning for image-to-molecular-profile prediction (integrated ML imaging)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train ML models (likely CNNs) to map morphological image features to molecular tumor profiles; the cited work integrates morphological and molecular data for prediction (details in cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning / multimodal integration</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for tasks where paired imaging and molecular labels exist; requires careful validation to avoid overfitting to dataset artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Presented as a promising direction to computationally infer molecular profiles from images, enabling integrated tumor characterization; interpretability methods (e.g., LRP) can help validate predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could reduce need for costly molecular assays, enable large-scale retrospective analyses, and suggest morphological correlates of molecular alterations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Complementary to direct molecular assays; ML can scale and discover image-based proxies but must be validated against gold-standard molecular tests.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of paired imaging and molecular labels, model interpretability to verify biological plausibility, and robust validation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Integrating image-based ML predictions with molecular profiling can enable scalable tumor characterization, but demands interpretable models and careful validation to ensure biological validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2289.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interpretable Drug Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interpretable deep learning in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of interpretable deep learning approaches to predict molecular activity/properties in drug discovery, with emphasis on interpretability to support chemical insight and lead design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interpretable deep learning in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Drug discovery / cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict biological activity, toxicity or molecular properties of chemical compounds and provide interpretable attributions to inform medicinal chemistry decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>molecular descriptors, graphs, or fingerprints (structured), possibly high-dimensional and sparse labeled datasets</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — complex structure-activity relationships, high-dimensional chemical space, and often limited labeled examples for specific targets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established domain for QSAR and cheminformatics; interpretable deep learning is a growing area within drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - medicinal chemists require interpretable rationales for predictions to guide design and regulatory decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Interpretable deep learning models for molecular property/activity prediction</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Deep learning architectures (e.g., graph neural networks, CNNs on molecular representations) augmented with interpretability methods to attribute predictions to molecular substructures or features; specifics in cited chapter.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning with interpretability constraints</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for accelerating property prediction and lead optimization when interpretability is integrated to support actionable chemical insights.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as successful in providing interpretable predictions that can help in drug design; interpretability is emphasized as critical for adoption in drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — interpretable models can increase trust and utility of ML in lead discovery and reduce experimental costs through better-guided hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to black-box deep models and classical QSAR; interpretability methods provide an advantage for human-in-the-loop decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Combining predictive accuracy with interpretable attributions that align with chemical knowledge and enable actionable insights.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>In drug discovery, integrating interpretability into deep models is essential: predictive accuracy alone is insufficient because domain experts require mechanistic attributions to act on model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2289.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EEG Deep Learning + LRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interpretable deep neural networks for single-trial EEG classification (with LRP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of deep neural networks for single-trial EEG classification combined with Layer-wise Relevance Propagation to obtain interpretable attributions over time and channels for neuroscientific insight.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interpretable deep neural networks for single-trial eeg classification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Neuroscience / brain imaging / EEG analysis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Classify single-trial EEG signals (e.g., cognitive or clinical states) and explain which temporal/channel features drive predictions to support neuroscientific interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>time-series multichannel EEG data (high-dimensional, temporally structured)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — noisy, non-stationary signals with complex spatiotemporal patterns and relatively low signal-to-noise ratio per trial.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established domain; application of deep learning and interpretability is an active research area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - neuroscientific interpretation demands mechanistic, localized explanations (which brain regions/times contribute to decisions).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep neural networks (for EEG) + Layer-wise Relevance Propagation</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train deep models (likely convolutional or recurrent architectures) on single-trial EEG for classification tasks and apply LRP to attribute predictions to time-channel inputs to interpret model decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning with post-hoc interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for single-trial EEG classification where interpretability is required to link model outputs to neuroscience constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported as enabling interpretable single-trial classifications and identification of relevant temporal/channel features; useful for neuroscientific discovery and clinical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate to high — improves trust and scientific utility of ML in EEG analysis by surfacing interpretable feature attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over black-box deep models by providing per-trial attributions; compared implicitly to linear models (which are interpretable) but deeper models can capture more complex patterns when coupled with LRP.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Combining architectures capable of capturing temporal patterns with attribution methods that produce localized, interpretable relevances.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep models can extract richer signals from single-trial EEG, but coupling them with attribution methods (LRP) is crucial to obtain neuroscientifically meaningful and trustworthy explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2289.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gait DL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep learning for gait pattern analysis (explaining individual gait with DL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of deep learning to explain individual gait patterns, using interpretability methods to identify features that distinguish individuals' gait.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining the unique nature of individual gait patterns with deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Biomechanics / gait analysis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify and explain features of individuals' gait patterns to understand variability and distinctive signatures using deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>time-series biomechanical sensor or kinematic data (high-dimensional sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate to high — temporal, multi-sensor signals with inter-individual variability and possible noise/artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established biomechanics domain; application of DL and explainability is a growing area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high - interpretation helps understanding unique gait characteristics and potential clinical relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep learning for time-series gait data + interpretability methods</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train deep sequence or convolutional models on gait data to classify/characterize individuals and apply attribution methods to understand which temporal and sensor features drive the classification.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning with post-hoc interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for uncovering complex, subtle patterns in gait data and providing human-interpretable explanations for differences.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated ability to explain unique gait signatures; interpretability supports biomechanical insight and potential clinical use.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — can aid personalized assessment, rehabilitation planning, and biometric applications when combined with robust validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides richer, non-linear pattern detection compared to linear models, with interpretability methods mitigating black-box concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality time-series data, appropriate sequence-capable models, and attribution methods that map relevances to biomechanically meaningful features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep models reveal individual-specific gait features, but interpretable attributions are necessary to translate learned patterns into biomechanical understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2289.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PilaniaMaterialsML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accelerating materials property predictions using machine learning (Pilania et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ML to predict material properties from descriptors and thus accelerate materials discovery and screening processes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerating materials property predictions using machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Materials science / materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict materials properties rapidly to enable screening of candidate materials and accelerate discovery cycles compared to expensive experiments or simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured descriptors of materials (composition, structure), possibly high-dimensional and heterogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — complex structure-property relationships, high-dimensional descriptor spaces, and potentially limited labeled data for novel materials.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established field; ML is an active and growing tool for materials prediction and discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high - scientific adoption benefits from interpretable features linking predictions to physical/chemical mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised ML for materials property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train ML models (regressors/classifiers) on known materials descriptor-to-property mappings; methods vary (kernel methods, random forests, deep nets) and specifics are in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (regression/classification)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to accelerate property prediction when historical data or computed properties are available; generalization to novel chemistries remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as accelerating predictions and enabling larger-scale screening, supporting materials discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — reduces experimental/computational cost of screening and speeds up materials development cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>ML-based predictions trade off some interpretability and may require careful validation against experimental or high-fidelity simulation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of curated datasets, informative descriptors, appropriate ML model selection, and integration with domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>ML can substantially accelerate materials property prediction and discovery, but success depends on data quality and interpretability to connect predictions to physical insights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2289.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2289.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SupernovaCNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enhanced rotational invariant convolutional neural network for supernovae detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional neural network architecture adapted for rotational invariance to detect supernovae in astronomical imaging data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhanced rotational invariant convolutional neural network for supernovae detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Astronomy / supernova detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Detect supernovae from imaging data robustly across orientations and imaging conditions to support astronomical surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>image data (astronomical images), possibly with class labels for sources</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate to high — high-resolution images with class imbalance and need for invariances (rotation) and robustness to noise/observational variation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established astronomical detection problems; ML/ CNNs increasingly used in surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low to medium - detection tasks can tolerate black-box models, but interpretability is useful for vetting false positives and peculiar events.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Rotation-invariant convolutional neural network</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>CNN architecture augmented or designed for rotation invariance to improve detection of supernovae irrespective of orientation; details in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (CNN with architectural invariances)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for large-scale image-based detection tasks in astronomy where invariances are important.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Presented as an improved architecture for rotational robustness in supernova detection; interpretability not emphasized in the survey but potentially useful for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — enhances automation and sensitivity in astronomical surveys, enabling more efficient discovery pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over vanilla CNNs by enforcing rotational invariance, which can increase robustness and reduce data augmentation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Architectural design enforcing symmetries/invariances, sufficient labeled training data, and appropriate preprocessing for astronomical imaging.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding domain-specific invariances into deep architectures (e.g., rotation invariance) improves applicability and robustness for structured scientific image-detection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Explainable Artificial Intelligence', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unmasking clever hans predictors and assessing what machines really learn <em>(Rating: 2)</em></li>
                <li>Towards exact molecular dynamics simulations with machine-learned force fields <em>(Rating: 2)</em></li>
                <li>Quantum-chemical insights from deep tensor neural networks <em>(Rating: 2)</em></li>
                <li>Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles <em>(Rating: 2)</em></li>
                <li>Interpretable deep neural networks for single-trial eeg classification <em>(Rating: 2)</em></li>
                <li>Interpretable deep learning in drug discovery <em>(Rating: 2)</em></li>
                <li>Accelerating materials property predictions using machine learning <em>(Rating: 2)</em></li>
                <li>Explaining the unique nature of individual gait patterns with deep learning <em>(Rating: 2)</em></li>
                <li>Meaningful perturbation <em>(Rating: 1)</em></li>
                <li>Why should i trust you?: Explaining the predictions of any classifier <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2289",
    "paper_id": "paper-202579608",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "LRP",
            "name_full": "Layer-wise Relevance Propagation",
            "brief_description": "A propagation-based explanation framework that attributes a model's prediction back to input features by redistributing relevance from outputs to inputs using local rules; applicable to deep networks, LSTMs and other models and designed to be computationally efficient (one forward and one backward pass).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General ML interpretability; applied across neuroscience, health, autonomous driving, drug design, physics (as cited).",
            "problem_description": "Provide pixel-/feature-wise explanations of individual model predictions to detect spurious correlates, foster trust, and derive scientific insight from complex ML models.",
            "data_availability": null,
            "data_structure": "images, time-series (e.g., EEG), sequences (text), potentially other high-dimensional inputs",
            "problem_complexity": "High — explains complex, highly non-linear deep models with many parameters and high-dimensional inputs where internal representations are nested and non-intuitive.",
            "domain_maturity": "Mature ML methodology applied to established scientific domains with existing domain knowledge (neuroscience, medical imaging, physics).",
            "mechanistic_understanding_requirements": "High - interpretability is required to verify mechanisms, detect bias, and extract scientific insights from data-driven models.",
            "ai_methodology_name": "Layer-wise Relevance Propagation (explainability method for neural networks)",
            "ai_methodology_description": "LRP redistributes the prediction score backward through the network using theoretically motivated local redistribution rules (deep Taylor decomposition connection), producing relevance scores per input dimension; applicable to deep nets, LSTMs, Fisher Vector classifiers and to models transformed into neural networks ('neuralization').",
            "ai_methodology_category": "Explainability / post-hoc attribution (leveraging model internals)",
            "applicability": "Applicable and appropriate for high-dimensional, non-linear models where per-feature attribution is required; leverages model structure and avoids gradient-shattering issues, but requires access to model internals or a differentiable-neuralization.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported as delivering good-quality explanations at low computational cost relative to perturbation/surrogate methods; robust to gradient shattering and supports pixelwise, high-resolution explanations useful for bias detection (e.g., 'Clever Hans').",
            "impact_potential": "High — enables verification, detection of spurious dataset artifacts, fosters scientific insights and can be integrated into model development, validation, and potentially model compression/pruning.",
            "comparison_to_alternatives": "Compared qualitatively to gradient-based methods and perturbation/surrogate approaches; LRP is argued to be faster (one forward/backward pass) and to avoid gradient discontinuities; alternatives include Guided Backpropagation, Deconvolution, LIME, SmoothGrad and perturbation methods which tend to be more computationally expensive or less stable.",
            "success_factors": "Leverages internal model structure, theoretically motivated redistribution rules, generality across architectures, and low computational cost (one forward/backward pass) facilitating practical use.",
            "key_insight": "For high-dimensional, non-linear scientific prediction tasks that require mechanistic interpretability, propagation-based attributions like LRP provide efficient, detailed explanations that better reveal model strategies and dataset artifacts than many model-agnostic or gradient-only methods.",
            "uuid": "e2289.0",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "LIME",
            "name_full": "Local Interpretable Model-agnostic Explanations",
            "brief_description": "A model-agnostic explanation technique that fits an interpretable surrogate model locally around a prediction by sampling the neighborhood of the input and learning a simple approximating model to explain the original model's decision.",
            "citation_title": "Why should i trust you?: Explaining the predictions of any classifier",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General ML interpretability; used across domains where model internals are unavailable or uninterpretable.",
            "problem_description": "Explain individual predictions of complex black-box models by approximating them with locally interpretable models to provide human-understandable explanations.",
            "data_availability": null,
            "data_structure": "tabular, images, text (depends on interpretable input representation used by surrogate)",
            "problem_complexity": "Moderate to high — targets complex, non-linear models but explanation is local and thus simplifies complexity to a local surrogate.",
            "domain_maturity": "Method is mature as a general interpretability tool; applied in various domains but with limitations.",
            "mechanistic_understanding_requirements": "Medium - used where some interpretability is desired but full mechanistic models may be unavailable.",
            "ai_methodology_name": "LIME (surrogate model-based explanation)",
            "ai_methodology_description": "Samples perturbations around an input, queries the black-box model on these perturbed samples, and fits a simple interpretable model (e.g., linear model or decision tree) weighted by proximity to the original input to explain the prediction locally; model-agnostic and does not require access to internals.",
            "ai_methodology_category": "Explainability / model-agnostic surrogate",
            "applicability": "Applicable when model internals are inaccessible and a local, human-interpretable explanation suffices; less suitable when global explanations or high-resolution attribution are needed.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Useful for coarse, human-interpretable explanations but has high computational cost and can be slow (minutes per explanation for large state-of-the-art models) and may miss fine-grained or global behaviors.",
            "impact_potential": "Medium — democratizes explanations for black-box systems but computational expense and locality limit scalability for large models or exhaustive validation.",
            "comparison_to_alternatives": "Compared to gradient- and propagation-based methods: LIME is model-agnostic (no internals needed) but slower and provides coarser explanations; alternatives like LRP leverage internals for faster, higher-resolution explanations.",
            "success_factors": "Model-agnosticism and human-interpretable surrogate models; limitations are computational expense and locality of approximation.",
            "key_insight": "Model-agnostic local surrogates are useful when internals are unavailable, but their high computational cost and coarse locality limit their utility for large-scale, high-resolution scientific explainability tasks.",
            "uuid": "e2289.1",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "SmoothGrad",
            "name_full": "SmoothGrad (gradient-averaging explanation)",
            "brief_description": "An explanation method that reduces noise in gradient-based saliency maps by averaging gradients computed for multiple noisy versions of the input, improving visualization quality.",
            "citation_title": "Smoothgrad: removing noise by adding noise",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General interpretability for image and other continuous input domains",
            "problem_description": "Improve gradient-based saliency explanations by reducing visual noise and producing more stable attribution maps.",
            "data_availability": null,
            "data_structure": "images or other continuous high-dimensional inputs where gradients are computable",
            "problem_complexity": "High non-linearity and high dimensionality; gradients may suffer from fragmentation/discontinuities.",
            "domain_maturity": "Mature as a gradient-based enhancement method within interpretability research.",
            "mechanistic_understanding_requirements": "Medium - used to enhance visual interpretability rather than provide causal mechanistic explanations.",
            "ai_methodology_name": "SmoothGrad (gradient-averaging)",
            "ai_methodology_description": "Samples Gaussian noise around the input, computes gradients for each noisy sample, and averages them to produce a smoother saliency map; requires gradient access to the model.",
            "ai_methodology_category": "Explainability / gradient-based attribution",
            "applicability": "Applicable where gradients are available and saliency visualizations are desired; not model-agnostic.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Improves visual smoothness and reduces noisy artifacts in gradient-based heatmaps but still inherits limitations of gradient explanations (sensitivity to gradient issues).",
            "impact_potential": "Moderate — improves usability of gradient explanations in practice, enhancing their interpretability for human users.",
            "comparison_to_alternatives": "Compared to raw gradient/saliency methods, SmoothGrad typically yields less noisy maps; compared to propagation-based LRP, SmoothGrad relies on gradients and may still be affected by gradient discontinuities.",
            "success_factors": "Simple averaging strategy, low implementation complexity, and effective noise reduction when gradients are available.",
            "key_insight": "Averaging gradients over noisy inputs yields more interpretable saliency maps, but gradient-based approaches remain limited by fundamental gradient pathologies for complex models.",
            "uuid": "e2289.2",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "SpRAy",
            "name_full": "Spectral Relevance Analysis",
            "brief_description": "A meta-explanation technique that clusters individual explanation heatmaps to identify distinct prediction strategies and systematic model behaviors across datasets.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Model analysis across vision datasets (e.g., PASCAL VOC) and general classifier behavior analysis",
            "problem_description": "Aggregate individual explanations to find global patterns and detect systematic biases or 'Clever Hans' strategies in classifiers.",
            "data_availability": null,
            "data_structure": "collections of explanation heatmaps derived from image datasets",
            "problem_complexity": "Meta-analysis over many high-dimensional explanations; requires clustering and spectral analysis techniques.",
            "domain_maturity": "Emerging technique within interpretability literature for large-scale model auditing.",
            "mechanistic_understanding_requirements": "High - aimed at uncovering global model strategies and dataset artifacts to provide mechanistic insight into model behavior.",
            "ai_methodology_name": "Spectral Relevance Analysis (SpRAy)",
            "ai_methodology_description": "Compute explanation heatmaps for many instances, then apply spectral clustering to group similar heatmaps into clusters representing distinct prediction strategies; analyze clusters to identify spurious correlates or typical decision modes.",
            "ai_methodology_category": "Explainability / meta-explanation and unsupervised analysis",
            "applicability": "Well suited to datasets where many individual explanations can be generated; particularly useful for auditing vision models and detecting dataset artifacts.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated ability to identify distinct strategies (e.g., multiple clusters for 'horse' classification including copyright watermark detection), enabling systematic detection of Clever Hans predictors.",
            "impact_potential": "High for large-scale model validation and dataset auditing; can guide dataset curation and model improvement.",
            "comparison_to_alternatives": "Provides a dataset-level, semi-automated analysis complementary to single-instance explanation methods; alternatives focus on individual explanations only.",
            "success_factors": "Aggregation of explanations, spectral clustering to reveal patterns, and interpretability of resulting clusters for human analysts.",
            "key_insight": "Aggregating instance-level explanations and clustering them reveals systematic prediction strategies and dataset artifacts that single-instance explanations may hide.",
            "uuid": "e2289.3",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "MeaningfulPerturbation / Activation Max",
            "name_full": "Meaningful Perturbation and Activation Maximization (optimization-based explanations)",
            "brief_description": "Optimization-formulated explanation approaches that either find minimal perturbations that most change the prediction (meaningful perturbation) or synthesize inputs that maximize a model response (activation maximization) to reveal representative prototypes.",
            "citation_title": "Interpretable explanations of black boxes by meaningful perturbation",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General interpretability across vision and other domains where prototypes or minimal informative regions are useful",
            "problem_description": "Produce explanations by solving optimization tasks that identify either critical input regions or prototypical inputs for a class.",
            "data_availability": null,
            "data_structure": "images or input domains where optimization in input space is feasible",
            "problem_complexity": "Computationally demanding — involves optimization in high-dimensional input spaces and may require many forward evaluations of the model.",
            "domain_maturity": "Methodologically established within interpretability but computationally intensive and often used in research settings.",
            "mechanistic_understanding_requirements": "Medium to high — attempts to provide prototypical or causally informative input patterns, but abstractions may remain low-level.",
            "ai_methodology_name": "Meaningful perturbation / Activation maximization",
            "ai_methodology_description": "Formulate explanation as optimization: meaningful perturbation finds sparse masks or perturbations that most reduce model output; activation maximization searches for inputs that strongly activate particular neurons/classes, often with regularization or generative priors to yield interpretable prototypes.",
            "ai_methodology_category": "Explainability / optimization-based attribution and feature visualization",
            "applicability": "Applicable for creating prototypes and identifying maximally-informative input regions; computational cost limits scalability for per-instance exhaustive use.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Can produce compelling visual prototypes and identify key input regions, but optimization costs and sensitivity to regularization make results variable; useful to study internal representations.",
            "impact_potential": "Moderate — valuable for representation analysis and prototype generation, but limited for routine large-scale auditing due to cost.",
            "comparison_to_alternatives": "Provides higher-level prototypes vs. pixelwise attributions from LRP/gradients; more computationally expensive than propagation-based methods.",
            "success_factors": "Formulating explanations as an optimization problem and using priors/regularization to obtain interpretable solutions.",
            "key_insight": "Optimization-based explanations can reveal prototypical inputs and minimal informative perturbations, but high computational cost and sensitivity to regularization limit routine applicability for large scientific datasets.",
            "uuid": "e2289.4",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "DeepTensorNN",
            "name_full": "Deep Tensor Neural Networks (Schütt et al.)",
            "brief_description": "Deep tensor neural network architectures designed to learn quantum-chemical properties from molecular structure, providing interpretable quantum-chemical insights from learned representations.",
            "citation_title": "Quantum-chemical insights from deep tensor neural networks",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Quantum chemistry / computational chemistry",
            "problem_description": "Predict quantum-chemical properties of molecules (e.g., energies, forces) from molecular structures to accelerate simulations and enable chemical insight.",
            "data_availability": null,
            "data_structure": "structured molecular data (graphs/3D coordinates) — high-dimensional geometric descriptors",
            "problem_complexity": "High — many degrees of freedom, complex quantum-mechanical interactions, and high-dimensional input spaces (atomic positions/types).",
            "domain_maturity": "Established scientific domain with strong theoretical foundations; ML approaches are a relatively recent augmentation of traditional methods.",
            "mechanistic_understanding_requirements": "High - scientific insight about interactions and physically plausible predictions are important; interpretability desirable to relate learned features to chemical concepts.",
            "ai_methodology_name": "Deep tensor neural networks (deep learning for quantum chemistry)",
            "ai_methodology_description": "Neural architectures that model interactions between atoms using tensor representations and aggregation over molecular structure to predict quantum-chemical quantities; details are in the cited work but not expanded in this survey.",
            "ai_methodology_category": "Supervised deep learning (domain-specific architecture for molecular data)",
            "applicability": "Appropriate for predicting properties from molecular structures, offering speedups over ab initio methods while requiring high-quality training data.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported in cited work to provide quantum-chemical insights and accurate property predictions; survey cites it as an example of explainable / ML-driven scientific progress.",
            "impact_potential": "High — can accelerate quantum-chemical calculations and support discovery by enabling fast property prediction and interpretability of learned chemical representations.",
            "comparison_to_alternatives": "ML approaches complement or accelerate traditional quantum-chemical computations (e.g., DFT), offering trade-offs between accuracy and computational cost; explicit comparisons are in the cited paper.",
            "success_factors": "Architecture tailored to molecular structure and physically-informed representation learning yielding interpretable chemical patterns.",
            "key_insight": "Specialized deep architectures that respect molecular structure can learn accurate quantum-chemical mappings and provide interpretable representations that bridge data-driven predictions and chemical theory.",
            "uuid": "e2289.5",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "ML Force Fields",
            "name_full": "Machine-learned Force Fields (Chmiela et al.)",
            "brief_description": "ML models trained to reproduce forces and energies from quantum calculations to enable accurate molecular dynamics simulations at near ab initio accuracy with reduced compute cost.",
            "citation_title": "Towards exact molecular dynamics simulations with machine-learned force fields",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Molecular dynamics / computational chemistry",
            "problem_description": "Replace or accelerate force-field calculations for molecular dynamics by learning potentials/force fields from high-fidelity quantum data.",
            "data_availability": null,
            "data_structure": "molecular configurations with associated energies/forces (structured, high-dimensional geometric data)",
            "problem_complexity": "Very high — requires learning complex many-body interactions and ensuring energy/force conservation and generalization across configurations.",
            "domain_maturity": "Established domain; ML-based force fields are an active research direction augmenting classical/quantum methods.",
            "mechanistic_understanding_requirements": "High - physical plausibility, conservation laws and interpretability of learned potentials are important for scientific trust and use in simulations.",
            "ai_methodology_name": "Machine-learned force fields (supervised ML for potentials)",
            "ai_methodology_description": "Train models (e.g., kernel or neural network based) to predict forces/energies from atomic geometries to enable molecular dynamics with ML-derived potentials; details are in cited work.",
            "ai_methodology_category": "Supervised learning for physics-informed surrogate models",
            "applicability": "Highly applicable for accelerating molecular dynamics given sufficient high-quality quantum-mechanical training data; must ensure physical constraints are respected.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as enabling near-exact molecular dynamics simulations when trained on quantum data, representing a major acceleration opportunity for simulation workflows.",
            "impact_potential": "High — can drastically reduce computational cost of accurate MD simulations and enable longer timescale or larger-system studies.",
            "comparison_to_alternatives": "ML force fields trade off some interpretability for speed compared to full quantum methods; the cited work aims to approach exactness while benefiting from ML scalability.",
            "success_factors": "High-quality quantum training data, architectures/learning schemes that capture many-body physics and respect physical constraints.",
            "key_insight": "With sufficient high-fidelity quantum training data and appropriate model design, ML can produce force fields that enable near-accurate molecular dynamics at drastically lower cost than ab initio simulations.",
            "uuid": "e2289.6",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Computational Fluorescence Microscopy",
            "name_full": "Machine learning-based integrated prediction of morphological and molecular tumor profiles",
            "brief_description": "An ML approach to predict molecular tumor profiles from morphological imaging data (fluorescence or conventional microscopy), integrating imaging and molecular information for cancer profiling.",
            "citation_title": "Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Cancer pathology / computational microscopy",
            "problem_description": "Predict molecular or molecular-profile information (e.g., biomarkers) from morphological image data to enable integrated tumor characterization.",
            "data_availability": null,
            "data_structure": "imaging data (microscopy, histopathology) with associated molecular labels; multimodal (image + molecular labels)",
            "problem_complexity": "High — high-dimensional image data with complex phenotype-genotype relations and potential confounding factors.",
            "domain_maturity": "Active and emerging application of ML in pathology; domain has established clinical workflows but ML integration is relatively recent.",
            "mechanistic_understanding_requirements": "High - clinicians and researchers require interpretable predictions to trust molecular inferences from morphology and for biomarker discovery.",
            "ai_methodology_name": "Supervised deep learning for image-to-molecular-profile prediction (integrated ML imaging)",
            "ai_methodology_description": "Train ML models (likely CNNs) to map morphological image features to molecular tumor profiles; the cited work integrates morphological and molecular data for prediction (details in cited paper).",
            "ai_methodology_category": "Supervised deep learning / multimodal integration",
            "applicability": "Appropriate for tasks where paired imaging and molecular labels exist; requires careful validation to avoid overfitting to dataset artifacts.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Presented as a promising direction to computationally infer molecular profiles from images, enabling integrated tumor characterization; interpretability methods (e.g., LRP) can help validate predictions.",
            "impact_potential": "High — could reduce need for costly molecular assays, enable large-scale retrospective analyses, and suggest morphological correlates of molecular alterations.",
            "comparison_to_alternatives": "Complementary to direct molecular assays; ML can scale and discover image-based proxies but must be validated against gold-standard molecular tests.",
            "success_factors": "Availability of paired imaging and molecular labels, model interpretability to verify biological plausibility, and robust validation pipelines.",
            "key_insight": "Integrating image-based ML predictions with molecular profiling can enable scalable tumor characterization, but demands interpretable models and careful validation to ensure biological validity.",
            "uuid": "e2289.7",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Interpretable Drug Discovery",
            "name_full": "Interpretable deep learning in drug discovery",
            "brief_description": "Application of interpretable deep learning approaches to predict molecular activity/properties in drug discovery, with emphasis on interpretability to support chemical insight and lead design.",
            "citation_title": "Interpretable deep learning in drug discovery",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Drug discovery / cheminformatics",
            "problem_description": "Predict biological activity, toxicity or molecular properties of chemical compounds and provide interpretable attributions to inform medicinal chemistry decisions.",
            "data_availability": null,
            "data_structure": "molecular descriptors, graphs, or fingerprints (structured), possibly high-dimensional and sparse labeled datasets",
            "problem_complexity": "High — complex structure-activity relationships, high-dimensional chemical space, and often limited labeled examples for specific targets.",
            "domain_maturity": "Established domain for QSAR and cheminformatics; interpretable deep learning is a growing area within drug discovery.",
            "mechanistic_understanding_requirements": "High - medicinal chemists require interpretable rationales for predictions to guide design and regulatory decisions.",
            "ai_methodology_name": "Interpretable deep learning models for molecular property/activity prediction",
            "ai_methodology_description": "Deep learning architectures (e.g., graph neural networks, CNNs on molecular representations) augmented with interpretability methods to attribute predictions to molecular substructures or features; specifics in cited chapter.",
            "ai_methodology_category": "Supervised deep learning with interpretability constraints",
            "applicability": "Appropriate for accelerating property prediction and lead optimization when interpretability is integrated to support actionable chemical insights.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as successful in providing interpretable predictions that can help in drug design; interpretability is emphasized as critical for adoption in drug discovery.",
            "impact_potential": "High — interpretable models can increase trust and utility of ML in lead discovery and reduce experimental costs through better-guided hypotheses.",
            "comparison_to_alternatives": "Compared conceptually to black-box deep models and classical QSAR; interpretability methods provide an advantage for human-in-the-loop decisions.",
            "success_factors": "Combining predictive accuracy with interpretable attributions that align with chemical knowledge and enable actionable insights.",
            "key_insight": "In drug discovery, integrating interpretability into deep models is essential: predictive accuracy alone is insufficient because domain experts require mechanistic attributions to act on model outputs.",
            "uuid": "e2289.8",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "EEG Deep Learning + LRP",
            "name_full": "Interpretable deep neural networks for single-trial EEG classification (with LRP)",
            "brief_description": "Use of deep neural networks for single-trial EEG classification combined with Layer-wise Relevance Propagation to obtain interpretable attributions over time and channels for neuroscientific insight.",
            "citation_title": "Interpretable deep neural networks for single-trial eeg classification",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Neuroscience / brain imaging / EEG analysis",
            "problem_description": "Classify single-trial EEG signals (e.g., cognitive or clinical states) and explain which temporal/channel features drive predictions to support neuroscientific interpretation.",
            "data_availability": null,
            "data_structure": "time-series multichannel EEG data (high-dimensional, temporally structured)",
            "problem_complexity": "High — noisy, non-stationary signals with complex spatiotemporal patterns and relatively low signal-to-noise ratio per trial.",
            "domain_maturity": "Established domain; application of deep learning and interpretability is an active research area.",
            "mechanistic_understanding_requirements": "High - neuroscientific interpretation demands mechanistic, localized explanations (which brain regions/times contribute to decisions).",
            "ai_methodology_name": "Deep neural networks (for EEG) + Layer-wise Relevance Propagation",
            "ai_methodology_description": "Train deep models (likely convolutional or recurrent architectures) on single-trial EEG for classification tasks and apply LRP to attribute predictions to time-channel inputs to interpret model decisions.",
            "ai_methodology_category": "Supervised deep learning with post-hoc interpretability",
            "applicability": "Appropriate for single-trial EEG classification where interpretability is required to link model outputs to neuroscience constructs.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported as enabling interpretable single-trial classifications and identification of relevant temporal/channel features; useful for neuroscientific discovery and clinical applications.",
            "impact_potential": "Moderate to high — improves trust and scientific utility of ML in EEG analysis by surfacing interpretable feature attributions.",
            "comparison_to_alternatives": "Improves over black-box deep models by providing per-trial attributions; compared implicitly to linear models (which are interpretable) but deeper models can capture more complex patterns when coupled with LRP.",
            "success_factors": "Combining architectures capable of capturing temporal patterns with attribution methods that produce localized, interpretable relevances.",
            "key_insight": "Deep models can extract richer signals from single-trial EEG, but coupling them with attribution methods (LRP) is crucial to obtain neuroscientifically meaningful and trustworthy explanations.",
            "uuid": "e2289.9",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Gait DL",
            "name_full": "Deep learning for gait pattern analysis (explaining individual gait with DL)",
            "brief_description": "Application of deep learning to explain individual gait patterns, using interpretability methods to identify features that distinguish individuals' gait.",
            "citation_title": "Explaining the unique nature of individual gait patterns with deep learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Biomechanics / gait analysis",
            "problem_description": "Identify and explain features of individuals' gait patterns to understand variability and distinctive signatures using deep models.",
            "data_availability": null,
            "data_structure": "time-series biomechanical sensor or kinematic data (high-dimensional sequences)",
            "problem_complexity": "Moderate to high — temporal, multi-sensor signals with inter-individual variability and possible noise/artifacts.",
            "domain_maturity": "Established biomechanics domain; application of DL and explainability is a growing area.",
            "mechanistic_understanding_requirements": "Medium to high - interpretation helps understanding unique gait characteristics and potential clinical relevance.",
            "ai_methodology_name": "Deep learning for time-series gait data + interpretability methods",
            "ai_methodology_description": "Train deep sequence or convolutional models on gait data to classify/characterize individuals and apply attribution methods to understand which temporal and sensor features drive the classification.",
            "ai_methodology_category": "Supervised deep learning with post-hoc interpretability",
            "applicability": "Appropriate for uncovering complex, subtle patterns in gait data and providing human-interpretable explanations for differences.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated ability to explain unique gait signatures; interpretability supports biomechanical insight and potential clinical use.",
            "impact_potential": "Moderate — can aid personalized assessment, rehabilitation planning, and biometric applications when combined with robust validation.",
            "comparison_to_alternatives": "Provides richer, non-linear pattern detection compared to linear models, with interpretability methods mitigating black-box concerns.",
            "success_factors": "High-quality time-series data, appropriate sequence-capable models, and attribution methods that map relevances to biomechanically meaningful features.",
            "key_insight": "Deep models reveal individual-specific gait features, but interpretable attributions are necessary to translate learned patterns into biomechanical understanding.",
            "uuid": "e2289.10",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "PilaniaMaterialsML",
            "name_full": "Accelerating materials property predictions using machine learning (Pilania et al.)",
            "brief_description": "Use of ML to predict material properties from descriptors and thus accelerate materials discovery and screening processes.",
            "citation_title": "Accelerating materials property predictions using machine learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Materials science / materials discovery",
            "problem_description": "Predict materials properties rapidly to enable screening of candidate materials and accelerate discovery cycles compared to expensive experiments or simulations.",
            "data_availability": null,
            "data_structure": "structured descriptors of materials (composition, structure), possibly high-dimensional and heterogeneous",
            "problem_complexity": "High — complex structure-property relationships, high-dimensional descriptor spaces, and potentially limited labeled data for novel materials.",
            "domain_maturity": "Established field; ML is an active and growing tool for materials prediction and discovery.",
            "mechanistic_understanding_requirements": "Medium to high - scientific adoption benefits from interpretable features linking predictions to physical/chemical mechanisms.",
            "ai_methodology_name": "Supervised ML for materials property prediction",
            "ai_methodology_description": "Train ML models (regressors/classifiers) on known materials descriptor-to-property mappings; methods vary (kernel methods, random forests, deep nets) and specifics are in cited work.",
            "ai_methodology_category": "Supervised learning (regression/classification)",
            "applicability": "Well-suited to accelerate property prediction when historical data or computed properties are available; generalization to novel chemistries remains a challenge.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as accelerating predictions and enabling larger-scale screening, supporting materials discovery workflows.",
            "impact_potential": "High — reduces experimental/computational cost of screening and speeds up materials development cycles.",
            "comparison_to_alternatives": "ML-based predictions trade off some interpretability and may require careful validation against experimental or high-fidelity simulation baselines.",
            "success_factors": "Availability of curated datasets, informative descriptors, appropriate ML model selection, and integration with domain knowledge.",
            "key_insight": "ML can substantially accelerate materials property prediction and discovery, but success depends on data quality and interpretability to connect predictions to physical insights.",
            "uuid": "e2289.11",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "SupernovaCNN",
            "name_full": "Enhanced rotational invariant convolutional neural network for supernovae detection",
            "brief_description": "A convolutional neural network architecture adapted for rotational invariance to detect supernovae in astronomical imaging data.",
            "citation_title": "Enhanced rotational invariant convolutional neural network for supernovae detection",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Astronomy / supernova detection",
            "problem_description": "Detect supernovae from imaging data robustly across orientations and imaging conditions to support astronomical surveys.",
            "data_availability": null,
            "data_structure": "image data (astronomical images), possibly with class labels for sources",
            "problem_complexity": "Moderate to high — high-resolution images with class imbalance and need for invariances (rotation) and robustness to noise/observational variation.",
            "domain_maturity": "Established astronomical detection problems; ML/ CNNs increasingly used in surveys.",
            "mechanistic_understanding_requirements": "Low to medium - detection tasks can tolerate black-box models, but interpretability is useful for vetting false positives and peculiar events.",
            "ai_methodology_name": "Rotation-invariant convolutional neural network",
            "ai_methodology_description": "CNN architecture augmented or designed for rotation invariance to improve detection of supernovae irrespective of orientation; details in cited work.",
            "ai_methodology_category": "Supervised deep learning (CNN with architectural invariances)",
            "applicability": "Appropriate for large-scale image-based detection tasks in astronomy where invariances are important.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Presented as an improved architecture for rotational robustness in supernova detection; interpretability not emphasized in the survey but potentially useful for verification.",
            "impact_potential": "Moderate — enhances automation and sensitivity in astronomical surveys, enabling more efficient discovery pipelines.",
            "comparison_to_alternatives": "Improves over vanilla CNNs by enforcing rotational invariance, which can increase robustness and reduce data augmentation needs.",
            "success_factors": "Architectural design enforcing symmetries/invariances, sufficient labeled training data, and appropriate preprocessing for astronomical imaging.",
            "key_insight": "Embedding domain-specific invariances into deep architectures (e.g., rotation invariance) improves applicability and robustness for structured scientific image-detection tasks.",
            "uuid": "e2289.12",
            "source_info": {
                "paper_title": "Towards Explainable Artificial Intelligence",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unmasking clever hans predictors and assessing what machines really learn",
            "rating": 2,
            "sanitized_title": "unmasking_clever_hans_predictors_and_assessing_what_machines_really_learn"
        },
        {
            "paper_title": "Towards exact molecular dynamics simulations with machine-learned force fields",
            "rating": 2,
            "sanitized_title": "towards_exact_molecular_dynamics_simulations_with_machinelearned_force_fields"
        },
        {
            "paper_title": "Quantum-chemical insights from deep tensor neural networks",
            "rating": 2,
            "sanitized_title": "quantumchemical_insights_from_deep_tensor_neural_networks"
        },
        {
            "paper_title": "Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles",
            "rating": 2,
            "sanitized_title": "towards_computational_fluorescence_microscopy_machine_learningbased_integrated_prediction_of_morphological_and_molecular_tumor_profiles"
        },
        {
            "paper_title": "Interpretable deep neural networks for single-trial eeg classification",
            "rating": 2,
            "sanitized_title": "interpretable_deep_neural_networks_for_singletrial_eeg_classification"
        },
        {
            "paper_title": "Interpretable deep learning in drug discovery",
            "rating": 2,
            "sanitized_title": "interpretable_deep_learning_in_drug_discovery"
        },
        {
            "paper_title": "Accelerating materials property predictions using machine learning",
            "rating": 2,
            "sanitized_title": "accelerating_materials_property_predictions_using_machine_learning"
        },
        {
            "paper_title": "Explaining the unique nature of individual gait patterns with deep learning",
            "rating": 2,
            "sanitized_title": "explaining_the_unique_nature_of_individual_gait_patterns_with_deep_learning"
        },
        {
            "paper_title": "Meaningful perturbation",
            "rating": 1,
            "sanitized_title": "meaningful_perturbation"
        },
        {
            "paper_title": "Why should i trust you?: Explaining the predictions of any classifier",
            "rating": 1,
            "sanitized_title": "why_should_i_trust_you_explaining_the_predictions_of_any_classifier"
        }
    ],
    "cost": 0.0233665,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Explainable Artificial Intelligence
26 Sep 2019</p>
<p>Wojciech Samek wojciech.samek@hhi.fraunhofer.de 
Fraunhofer Heinrich Hertz Institute
10587BerlinGermany</p>
<p>Klaus-Robert Müller klaus-robert.mueller@tu-berlin.de 
Technische Universität Berlin
10587BerlinGermany</p>
<p>Korea University
Anam-dong, Seongbuk-gu02841SeoulKorea</p>
<p>Max Planck Institute for Informatics
66123SaarbrückenGermany</p>
<p>Towards Explainable Artificial Intelligence
26 Sep 201910.1007/978-3-030-28954-6_1Explainable Artificial Intelligence · Model Transparency · Deep Learning · Neural Networks · Interpretability
In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today's ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been generally considered "black boxes", not providing any information about what exactly makes them arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this field and makes a plea for a wider use of explainable learning algorithms in practice.</p>
<p>Introduction</p>
<p>Today's artificial intelligence (AI) systems based on machine learning excel in many fields. They not only outperform humans in complex visual tasks [16,53] or strategic games [56,83,61], but also became an indispensable part of our every day lives, e.g., as intelligent cell phone cameras which can recognize and track faces [71], as online services which can analyze and translate written texts [11] or as consumer devices which can understand speech and generate human-like answers [90]. Moreover, machine learning and artificial intelligence have become indispensable tools in the sciences for tasks such as prediction, simulation or exploration [78,15,89,92]. These immense successes of AI systems mainly became possible through improvements in deep learning methodology [48,47], the availability of large databases [17,34] and computational gains obtained with powerful GPU cards [52].</p>
<p>Despite the revolutionary character of this technology, challenges still exist which slow down or even hinder the prevailance of AI in some applications. Examplar challenges are (1) the large complexity and high energy demands of current deep learning models [29], which hinder their deployment in resource restricted environments and devices, (2) the lack of robustness to adversarial attacks [55], which pose a severe security risk in application such as autonomous driving 5 , and (3) the lack of transparency and explainability [76,32,18], which reduces the trust in and the verifiability of the decisions made by an AI system. This paper focuses on the last challenge. It presents recent developments in the field of explainable artificial intelligence and aims to foster awareness for the advantages-and at times-also for the necessity of transparent decision making in practice. The historic second Go match between Lee Sedol and AlphaGo [82] nicely demonstrates the power of today's AI technology, and hints at its enormous potential for generating new knowledge from data when being accessible for human interpretation. In this match AlphaGo played a move, which was classified as "not a human move" by a renowned Go expert, but which was the deciding move for AlphaGo to win the game. AlphaGo did not explain the move, but the later play unveiled the intention behind its decision. With explainable AI it may be possible to also identify such novel patterns and strategies in domains like health, drug development or material sciences, moreover, the explanations will ideally let us comprehend the reasoning of the system and understand why the system has decided e.g. to classify a patient in a specific manner or associate certain properties with a new drug or material. This opens up innumerable possibilities for future research and may lead to new scientific insights.</p>
<p>The remainder of the paper is organized as follows. Section 2 discusses the need for transparency and trust in AI. Section 3 comments on the different types of explanations and their respective information content and use in practice. Recent techniques of explainable AI are briefly summarized in Section 4, including methods which rely on simple surrogate functions, frame explanation as an optimization problem, access the model's gradient or make use of the model's internal structure. The question of how to objectively evaluate the quality of explanations is addressed in Section 5. The paper concludes in Section 6 with a discussion on general challenges in the field of explainable AI.</p>
<p>Need for Transparency and Trust in AI</p>
<p>Black box AI systems have spread to many of today's applications. For machine learning models used, e.g., in consumer electronics or online translation services, transparency and explainability are not a key requirement as long as the overall performance of these systems is good enough. But even if these systems fail, e.g., the cell phone camera does not recognize a person or the translation service produces grammatically wrong sentences, the consequences are rather unspectacular. Thus, the requirements for transparency and trust are rather low for these types of AI systems. In safety critical applications the situation is very different. Here, the intransparency of ML techniques may be a limiting or even disqualifying factor. Especially if single wrong decisions can result in danger to life and health of humans (e.g., autonomous driving, medical domain) or significant monetary losses (e.g., algorithmic trading), relying on a data-driven system whose reasoning is incomprehensible may not be an option. This intransparency is one reason why the adoption of machine learning to domains such as health is more cautious than the usage of these models in the consumer, e-commerce or entertainment industry.</p>
<p>In the following we discuss why the ability to explain the decision making of an AI system helps to establish trust and is of utmost importance, not only in medical or safety critical applications. We refer the reader to [91] for a discussion of the challenges of transparency.</p>
<p>Explanations Help to Find "Clever Hans" Predictors</p>
<p>Clever Hans was a horse that could supposedly count and that was considered a scientific sensation in the years around 1900. As it turned out later, Hans did not master the math but in about 90 percent of the cases, he was able to derive the correct answer from the questioner's reaction. Analogous behaviours have been recently observed in state-of-the-art AI systems [46]. Also here the algorithms have learned to use some spurious correlates in the training and test data and similarly to Hans predict right for the 'wrong' reason.</p>
<p>For instance, the authors of [44,46] showed that the winning method of the PASCAL VOC competition [23] was often not detecting the object of interest, but was utilizing correlations or context in the data to correctly classify an image. It recognized boats by the presence of water and trains by the presence of rails in the image, moreover, it recognized horses by the presence of a copyright watermark 6 . The occurrence of the copyright tags in horse images is a clear artifact in the dataset, which had gone unnoticed to the organizers and participants of the challenge for many years. It can be assumed that nobody has systematically checked the thousands images in the dataset for this kind of artifacts (but even if someone did, such artifacts may be easily overlooked).</p>
<p>Many other examples of "Clever Hans" predictors have been described in the literature. For instance, [73] show that current deep neural networks are distinguishing the classes "Wolf" and "Husky" mainly by the presence of snow in the image. The authors of [46] demonstrate that deep models overfit to padding artifacts when classifying airplanes, whereas [63] show that a model which was trained to distinguish between 1000 categories, has not learned dumbbells as an independent concept, but associates a dumbbell with the arm which lifts it. Such "Clever Hans" predictors perform well on their respective test sets, but will certainly fail if deployed to the real-world, where sailing boats may lie on a boat trailer, both wolves and huskies can be found in non-snow regions and horses do not have a copyright sign on them. However, if the AI system is a black box, it is very difficult to unmask such predictors. Explainability helps to detect these types of biases in the model or the data, moreover, it helps to understand the weaknesses of the AI system (even if it is not a "Clever Hans" predictor). In the extreme case, explanations allow to detect the classifier's misbehaviour (e.g., the focus on the copyright tag) from a single test image 7 . Since understanding the weaknesses of a system is the first step towards improving it, explanations are likely to become integral part of the training and validation process of future AI models.</p>
<p>Explanations Foster Trust and Verifiability</p>
<p>The ability to verify decisions of an AI system is very important to foster trust, both in situations where the AI system has a supportive role (e.g., medical diagnosis) and in situations where it practically takes the decisions (e.g., autonomous driving). In the former case, explanations provide extra information, which, e.g., help the medical expert to gain a comprehensive picture of the patient in order to take the best therapy decision. Similarly to a radiologist, who writes a detailed report explaining his findings, a supportive AI system should in detail explain its decisions rather than only providing the diagnosis to the medical expert. In cases where the AI system itself is deciding, it is even more critical to be able to comprehend the reasoning of the system in order to verify that it is not behaving like Clever Hans, but solves the problem in a robust and safe manner. Such verifications are required to build the necessary trust in every new technology.</p>
<p>There is also a social dimension of explanations. Explaining the rationale behind one's decisions is an important part of human interactions [30]. Explanations help to build trust in a relationship between humans, and should therefore be also part of human-machine interactions [3]. Explanations are not only an inevitable part of human learning and education (e.g., teacher explains solution to student), but also foster the acceptance of difficult decisions and are important for informed consent (e.g., doctor explaining therapy to patient). Thus, even if not providing additional information for verifying the decision, e.g., because the patient may have no medical knowledge, receiving explanations usually make us feel better as it integrates us into the decision-making process. An AI system which interacts with humans should therefore be explainable.</p>
<p>Explanations are a Prerequisite for New Insights</p>
<p>AI systems have the potential to discover patterns in data, which are not accessible to the human expert. In the case of the Go game, these patterns can be new playing strategies [82]. In the case of scientific data, they can be unknown associations between genes and diseases [51], chemical compounds and material properties [68] or brain activations and cognitive states [49]. In the sciences, identifying these patterns, i.e., explaining and interpreting what features the AI system uses for predicting, is often more important than the prediction itself, because it unveils information about the biological, chemical or neural mechanisms and may lead to new scientific insights.</p>
<p>This necessity to explain and interpret the results has led to a strong dominance of linear models in scientific communities in the past (e.g. [42,67]). Linear models are intrinsically interpretable and thus easily allow to extract the learned patterns. Only recently, it became possible to apply more powerful models such as deep neural networks without sacrificing interpretability. These explainable non-linear models have already attracted attention in domains such as neuroscience [87,89,20], health [33,14,40], autonomous driving [31], drug design [70] and physics [78,72] and it can be expected that they will play a pivotal role in future scientific research.</p>
<p>Explanations are Part of the Legislation</p>
<p>The infiltration of AI systems into our daily lives poses a new challenge for the legislation. Legal and ethical questions regarding the responsibility of AI systems and their level of autonomy have recently received increased attention [21,27]. But also anti-discrimination and fairness aspects have been widely discussed in the context of AI [28,19]. The EU's General Data Protection Regulation (GDPR) has even added the right to explanation to the policy in Articles 13, 14 and 22, highlighting the importance of human-understandable interpretations derived from machine decisions. For instance, if a person is being rejected for a loan by the AI system of a bank, in principle, he or she has the right to know why the system has decided in this way, e.g., in order to make sure that the decision is compatible with the anti-discrimination law or other regulations. Although it is not yet clear how these legal requirements will be implemented in practice, one can be sure that transparency aspects will gain in importance as AI decisions will more and more affect our daily lives.</p>
<p>Different Facets of an Explanation</p>
<p>Recently proposed explanation techniques provide valuable information about the learned representations and the decision-making of an AI system. These explanations may differ in their information content, their recipient and their purpose. In the following we describe the different types of explanations and comment on their usefulness in practice.</p>
<p>Recipient</p>
<p>Different recipients may require explanations with different level of detail and with different information content. For instance, for users of AI technology it may be sufficient to obtain coarse explanations, which are easy to interpret, whereas AI researchers and developers would certainly prefer explanations, which give them deeper insights into the functioning of the model.</p>
<p>In the case of image classification such simple explanations could coarsely highlight image regions, which are regarded most relevant for the model. Several preprocessing steps, e.g., smoothing, filtering or contrast normalization, could be applied to further improve the visualization quality. Although discarding some information, such coarse explanations could help the ordinary user to foster trust in AI technology. On the other hand AI researchers and developers, who aim to improve the model, may require all the available information, including negative evidence, about the AI's decision in the highest resolution (e.g., pixelwise explanations), because only this complete information gives detailed insights into the (mal)functioning of the model.</p>
<p>One can easily identify further groups of recipients, which are interested in different types of explanations. For instance, when applying AI to the medical domain these groups could be patients, doctors and institutions. An AI system which analyzes patient data could provide simple explanations to the patients, e.g., indicating too high blood sugar, while providing more elaborate explanations to the medical personal, e.g., unusual relation between different blood parameters. Furthermore, institutions such as hospitals or the FDA might be less interested in understanding the AI's decisions for individual patients, but would rather prefer to obtain global or aggregated explanations, i.e., patterns which the AI system has learned after analyzing many patients.</p>
<p>Information Content</p>
<p>Different types of explanation provide insights into different aspects of the model, ranging from information about the learned representations to the identification of distinct prediction strategies and the assessment of overall model behaviour. Depending on the recipient of the explanations and his or her intent, it may be advantageous to focus on one particular type of explanation. In the following we briefly describe four different types of explanations.</p>
<ol>
<li>Explaining learned representations: This type of explanation aims to foster the understanding of the learned representations, e.g., neurons of a deep neural network. Recent work [12,38] investigates the role of single neurons or group of neurons in encoding certain concepts. Other methods [84,93,64,65] aim to interpret what the model has learned by building prototypes that are representative of the abstract learned concept. These methods, e.g., explain what the model has learned about the category "car" by generating a prototypical image of a car. Building such a prototype can be formulated within the activation maximization framework and has been shown to be an effective tool for studying the internal representation of a deep neural network. 2. Explaining individual predictions: Other types of explanations provide information about individual predictions, e.g., heatmaps visualizing which pixels have been most relevant for the model to arrive at its decision [60] or heatmaps highlighting the most sensitive parts of an input [84]. Such explanations help to verify the predictions and establish trust in the correct functioning on the system. Layer-wise Relevance Propagation (LRP) [9,58] provides a general framework for explaining individual predictions, i.e., it is applicable to various ML models, including neural networks [9], LSTMs [7], Fisher Vector classifiers [44] and Support Vector Machines [35].  [41,37]. This type of explanations can be useful for obtaining a better understanding of the training dataset and how it influences the model. Furthermore, these representative examples can potentially help to identify biases in the data and make the model more robust to variations of the training dataset.</li>
</ol>
<p>Role</p>
<p>Besides the recipient and information content it is also important to consider the purpose of an explanation. Here we can distinguish two aspects, namely (1) the intent of the explanation method (what specific question does the explanation answer) and (2) our intent (what do we want to use the explanation for). Explanations are relative and it makes a huge difference whether their intent is to explain the prediction as is (even if it is incorrect), whether they aim to visualize what the model "thinks" about a specific class (e.g., the true class) or whether they explain the prediction relative to another alternative ("why is this image classified as car and not as truck"). Methods such as LRP allow to answer all these different questions, moreover, they also allow to adjust the amount of positive and negative evidence in the explanations, i.e., visualize what speaks for (positive evidence) and against (negative evidence) the prediction. Such finegrained explanations foster the understanding of the classifier and the problem at hand. Furthermore, there may be different goals for using the explanations beyond visualization and verification of the prediction. For instance, explanations can be potentially used to improve the model, e.g., by regularization [74]. Also since explanations provide information about the (relevant parts of the) model, they can be potentially used for model compression and pruning. Many other uses (certification of the model, legal use) of explanations can be thought of, but the details remain future work.</p>
<p>Methods of Explainable AI</p>
<p>This section gives an overview over different approaches to explainable AI, starting with techniques which are model-agnostic and rely on a simple surrogate function to explain the predictions. Then, we discuss methods which compute explanations by testing the model's response to local perturbations (e.g., by utilizing gradient information or by optimization). Subsequently, we present very efficient propagation-based explanation techniques which leverage the model's internal structure. Finally, we consider methods which go beyond individual explanations towards a meta-explanation of model behaviour.</p>
<p>This section is not meant to be a complete survey of explanation methods, but it rather summarizes the most important developments in this field. Some approaches to explainable AI, e.g., methods which find influencial examples [37], are not discussed in this section.</p>
<p>Explaining with Surrogates</p>
<p>Simple classifiers such as linear models or shallow decision trees are intrinsically interpretable, so that explaining its predictions becomes a trivial task. Complex classifiers such as deep neural networks or recurrent models on the other hand contain several layers of non-linear transformations, which largely complicates the task of finding what exactly makes them arrive at their predictions.</p>
<p>One approach to explain the predictions of complex models is to locally approximate them with a simple surrogate function, which is interpretable. A popular technique falling into this category is Local Interpretable Model-agnostic Explanations (LIME) [73]. This method samples in the neighborhood of the input of interest, evaluates the neural network at these points, and tries to fit the surrogate function such that it approximates the function of interest. If the input domain of the surrogate function is human-interpretable, then LIME can even explain decisions of a model which uses non-interpretable features. Since LIME is model agnostic, it can be applied to any classifier, even without knowing its internals, e.g., architecture or weights of a neural network classifier. One major drawback of LIME is its high computational complexity, e.g., for state-ofthe-art models such as GoogleNet it requires several minutes for computing the explanation of a single prediction [45]. Similar to LIME which builds a model for locally approximating the function of interest, the SmoothGrad method [85] samples the neighborhood of the input to approximate the gradient. Also SmoothGrad does not leverage the internals of the model, however, it needs access to the gradients. Thus, it can also be regarded as a gradient-based explanation method.</p>
<p>Explaining with Local Perturbations</p>
<p>Another class of methods construct explanations by analyzing the model's response to local changes. This includes methods which utilize the gradient information as well as perturbation-and optimization-based approaches.</p>
<p>Explanation methods relying on the gradient of the function of interest [2] have a long history in machine learning. One example is the so-called Sensitivity Analysis (SA) [62,10,84]. Although being widely used as explanation methods, SA technically explains the change in prediction instead of the prediction itself. Furthermore, SA has been shown to suffer from fundamental problems such as gradient shattering and explanation discontinuities, and is therefore considered suboptimal for explanation of today's AI models [60]. Variants of Sensitivity Analysis exist which tackle some of these problems by locally averaging the gradients [85] or integrating them along a specific path [88].</p>
<p>Perturbation-based explanation methods [94,97,25] explicitly test the model's response to more general local perturbations. While the occlusion method of [94] measures the importance of input dimensions by masking parts of the input, the Prediction Difference Analysis (PDA) approach of [97] uses conditional sampling within the pixel neighborhood of an analyzed feature to effectively remove information. Both methods are model-agnostic, i.e., can be applied to any classifier, but are computationally not very efficient, because the function of interest (e.g., neural network) needs to be evaluated for all perturbations.</p>
<p>The meaningful perturbation method of [25,26] is another model-agnostic technique to explaining with local perturbations. It regards explanation as a meta prediction task and applies optimization to synthesize the maximally informative explanations. The idea to formulate explanation as an optimization problem is also used by other methods. For instance, the methods [84,93,64] aim to interpret what the model has learned by building prototypes that are representative of the learned concept. These prototypes are computed within the activation maximization framework by searching for an input pattern that produces a maximum desired model response. Conceptually, activation maximization [64] is similar to the meaningful perturbation approach of [25]. While the latter finds a minimum perturbation of the data that makes f (x) low, activation maximization finds a minimum perturbation of the gray image that makes f (x) high. The costs of optimization can make these methods computationally very demanding.</p>
<p>Propagation-Based Approaches (Leveraging Structure)</p>
<p>Propagation-based approaches to explanation are not oblivious to the model which they explain, but rather integrate the internal structure of the model into the explanation process.</p>
<p>Layer-wise Relevance Propagation (LRP) [9,58] is a propagation-based explanation framework, which is applicable to general neural network structures, including deep neural networks [13], LSTMs [7,5], and Fisher Vector classifiers [44]. LRP explains individual decisions of a model by propagating the prediction from the output to the input using local redistribution rules. The propagation process can be theoretically embedded in the deep Taylor decomposition framework [59]. More recently, LRP was extended to a wider set of machine learning models, e.g., in clustering [36] or anomaly detection [35], by first transforming the model into a neural network ('neuralization') and then applying LRP to explain its predictions. The leveraging of the model structure together with the use of appropriate (theoretically-motivated) propagation rules, enables LRP to deliver good explanations at very low computational cost (one forward and one backward pass). Furthermore, the generality of the LRP framework allows also to express other recently proposed explanation techniques, e.g., [81,95]. Since LRP does not rely on gradients, it does not suffer from problems such as gradient shattering and explanation discontinuities [60].</p>
<p>Other popular explanation methods leveraging the model's internal structure are Deconvolution [94] and Guided Backprogagation [86]. In contrast to LRP, these methods do not explain the prediction in the sense "how much did the input feature contribute to the prediction", but rather identify patterns in input space, that relate to the analyzed network output.</p>
<p>Many other explanation methods have been proposed in the literature which fall into the "leveraging structure" category. Some of these methods use heuristics to guide the redistribution process [79], others incorporate an optimization step into the propagation process [39]. The iNNvestigate toolbox [1] provides an efficient implementation for many of these propagation-based explanation methods.</p>
<p>Meta-Explanations</p>
<p>Finally, individual explanations can be aggregated and analyzed to identify general patterns of classifier behavior. A recently proposed method, spectral relevance analysis (SpRAy) [46], computes such meta explanations by clustering individual heatmaps. This approach allows to investigate the predictions strategies of the classifier on the whole dataset in a (semi-)automated manner and to systematically find weak points in models or training datasets.</p>
<p>Another type of meta-explanation aims to better understand the learned representations and to provide interpretations in terms of human-friendly concepts. For instance, the network dissection approach of [12,96] evaluates the semantics of hidden units, i.e., quantify what concepts these neurons encode. Other recent work [38] provides explanations in terms of user-defined concepts and tests to which degree these concepts are important for the prediction.</p>
<p>Evaluating Quality of Explanations</p>
<p>The objective assessment of the quality of explanations is an active field of research. Many efforts have been made to define quality measures for heatmaps which explain individual predictions of an AI model. This section gives an overview over the proposed approaches.</p>
<p>A popular measure for heatmap quality is based on perturbation analysis [9,75,6]. The assumption of this evaluation metric is that the perturbation of relevant (according to the heatmap) input variables should lead to a steeper decline of the prediction score than the perturbation of input dimensions which are of lesser importance. Thus, the average decline of the prediction score after several rounds of perturbation (starting from the most relevant input variables) defines an objective measure of heatmap quality. If the explanation identifies the truly relevant input variables, then the decline should be large. The authors of [75] recommend to use untargeted perturbations (e.g., uniform noise) to allow fair comparison of different explanation methods. Although being very popular, it is clear that perturbation analysis can not be the only criterion to evaluate explanation quality, because one could easily design explanations techniques which would directly optimize this criterion. Examples are occlusion methods which were used in [94,50], however, they have been shown to be inferior (according to other quality criteria) to explanation techniques such as LRP [8].</p>
<p>Other studies use the 'pointing game" [95] to evaluate the quality of a heatmap. The goal of this game is to evaluate the discriminativeness of the explanations for localizing target objects, i.e., it is compared if the most relevant point of the heatmap lies on the object of designated category. Thus, these measures assume that the AI model will focus most attention on the object of interest when classifying it, therefore this should be reflected in the explanation. However, this assumption may not always be true, e.g., "Clever Hans" predictors [46] may rather focus on context than of the object itself, irrespectively of the explanation method used. Thus, their explanations would be evaluated as poor quality according to this measure although they truly visualize the model's prediction strategy.</p>
<p>Task specific evaluation schemes have also been proposed in the literature. For example, [69] use the subject-verb agreement task to evaluate explanations of a NLP model. Here the model predicts a verb's number and the explanations verify if the most relevant word is indeed the correct subject or a noun with the predicted number. Other approaches to evaluation rely on human judgment [73,66]. Such evaluation schemes relatively quickly become impractical if evaluating a larger number of explanations.</p>
<p>A recent study [8] proposes to objectively evaluate explanation for sequential data using ground truth information in a toy task. The idea of this evaluation metric is to add or subtract two numbers within an input sequence and measure the correlation between the relevances assigned to the elements of the sequence and the two input numbers. If the model is able to accurately perform the addition and subtraction task, then it must focus on these two numbers (other numbers in the sequence are random) and this must be reflected in the explanation.</p>
<p>An alternative and indirect way to evaluate the quality of explanations is to use them for solving other tasks. The authors of [6] build document-level representations from word-level explanations. The performance of these documentlevel representations (e.g., in a classification task) reflect the quality of the wordlevel explanations. Another work [4] uses explanation for reinforcement learning. Many other functionally-grounded evaluations [18] could be conceived such as using explanations for compressing or pruning the neural network or training student models in a teacher-student scenario.</p>
<p>Lastly, another promising approach to evaluate explanations is based on the fulfillment of a certain axioms [80,88,54,60,57]. Axioms are properties of an explanation that are considered to be necessary and should therefore be fulfilled. Proposed axioms include relevance conservation [60], explanation continuity [60], sensitivity [88] and implementation invariance [88]. In contrast to the other quality measures discussed in this section, the fulfillment or non-fulfillment of certain axioms can be often shown analytically, i.e., does not require empirical evaluations.</p>
<p>Challenges and Open Questions</p>
<p>Although significant progress has been made in the field of explainable AI in the last years, challenges still exist both on the methods and theory side as well as regarding the way explanations are used in practice. Researchers have already started working on some of these challenges, e.g., the objective evaluation of explanation quality or the use of explanations beyond visualization. Other open questions, especially those concerning the theory, are more fundamental and more time will be required to give satisfactory answers to them.</p>
<p>Explanation methods allow us to gain insights into the functioning of the AI model. Yet, these methods are still limited in several ways. First, heatmaps computed with today's explanation methods visualize "first-order" information, i.e., they show which input features have been identified as being relevant for the prediction. However, the relation between these features, e.g., whether they are important on their own or only whether they occur together, remains unclear. Understanding these relations is important in many applications, e.g., in the neurosciences such higher-order explanations could help us to identify groups of brain regions which act together when solving a specific task (brain networks) rather than just identifying important single voxels.</p>
<p>Another limitation is the low abstraction level of explanations. Heatmaps show that particular pixels are important without relating these relevance values to more abstract concepts such as the objects or the scene displayed in the image. Humans need to interpret the explanations to make sense them and to understand the model's behaviour. This interpretation step can be difficult and erroneous. Meta-explanations which aggregate evidence from these low-level heatmaps and explain the model's behaviour on a more abstract, more human understandable level, are desirable. Recently, first approaches to aggregate lowlevel explanations [46] and quantify the semantics of neural representations [12] have been proposed. The construction of more advanced meta-explanations is a rewarding topic for future research.</p>
<p>Since the recipient of explanations is ultimately the human user, the use of explanations in human-machine interaction is an important future research topic. Some works (e.g., [43]) have already started to investigate human factors in explainable AI. Constructing explanations with the right user focus, i.e., asking the right questions in the right way, is a prerequisite to successful human-machine interaction. However, the optimization of explanations for optimal human usage is still a challenge which needs further study.</p>
<p>A theory of explainable AI, with a formal and universally agreed definition of what explanations are, is lacking. Some works made a first step towards this goal by developing mathematically well-founded explanation methods. For instance, the authors of [59] approach the explanation problem by integrating it into the theoretical framework of Taylor decomposition. The axiomatic approaches [88,54,60] constitute another promising direction towards the goal of developing a general theory of explainable AI.</p>
<p>Finally, the use of explanations beyond visualization is a wide open challenge. Future work will show how to integrate explanations into a larger optimization process in order to, e.g., improve the model's performance or reduce its complexity.</p>
<p>https://doi.org/10.1007/978-3-030-28954-6_1. In: W. Samek et al. (Eds.) Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol. 11700, pp. 5-22. Springer, Cham (2019).</p>
<p>Explaining with representative examples: Another class of methods interpret classifiers by identifying representative training examplesSection 4 
gives an overview over recently proposed methods for computing individual 
explanations. 
3. Explaining model behaviour: This type of explanations go beyond the 
analysis of individual predictions towards a more general understanding of 
model behaviour, e.g., identification of distinct prediction strategies. The 
spectral relevance analysis (SpRAy) approach of [46] computes such meta 
explanations by clustering individual heatmaps. Each cluster then represents 
a particular prediction strategy learned by the model. For instance, the au-
thors of [46] identify four clusters when classifying "horse" images with the 
Fisher Vector classifier [77] trained on the PASCAL VOC 2007 dataset [22], 
namely (1) detect the horse and rider, 2) detect a copyright tag in portrait 
oriented images, 3) detect wooden hurdles and other contextual elements of 
horseback riding, and 4) detect a copyright tag in landscape oriented images. 
Such explanations are useful for obtaining a global overview over the learned 
strategies and detecting "Clever Hans" predictors [46]. 
4. 
The authors of[24] showed that deep models can be easily fooled by physical-world attacks. For instance, by putting specific stickers on a stop sign one can achieve that the stop sign is not recognized by the system anymore.
The PASCAL VOC images have been automatically crawled from flickr and especially the horse images were very often copyrighted with a watermark.
Traditional methods to evaluate classifier performance require large test datasets.
. Towards Explainable Artificial Intelligence</p>
<p>M Alber, S Lapuschkin, P Seegerer, M Hägele, K T Schütt, G Montavon, W Samek, K R Müller, S Dähne, P J Kindermans, iNNvestigate neural networks!. 20Alber, M., Lapuschkin, S., Seegerer, P., Hägele, M., Schütt, K.T., Montavon, G., Samek, W., Müller, K.R., Dähne, S., Kindermans, P.J.: iNNvestigate neural net- works!. Journal of Machine Learning Research 20(93), 1-8 (2019)</p>
<p>Gradient-based attribution methods. M Ancona, E Ceolini, C Öztireli, M Gross, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700Ancona, M., Ceolini, E.,Öztireli, C., Gross, M.: Gradient-based attribution meth- ods. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, Springer (2019)</p>
<p>Structuring dimensions for collaborative systems evaluation. P Antunes, V Herskovic, S F Ochoa, J A Pino, ACM Computing Surveys (CSUR). 442Antunes, P., Herskovic, V., Ochoa, S.F., Pino, J.A.: Structuring dimensions for col- laborative systems evaluation. ACM Computing Surveys (CSUR) 44(2), 8 (2012)</p>
<p>RUDDER: Return Decomposition for Delayed Rewards. J A Arjona-Medina, M Gillhofer, M Widrich, T Unterthiner, S Hochreiter, arXiv:1806.07857arXiv preprintArjona-Medina, J.A., Gillhofer, M., Widrich, M., Unterthiner, T., Hochreiter, S.: RUDDER: Return Decomposition for Delayed Rewards. arXiv preprint arXiv:1806.07857 (2018)</p>
<p>Explaining and interpreting LSTMs with LRP. L Arras, J Arjona-Medina, M Gillhofer, M Widrich, G Montavon, K R Müller, S Hochreiter, W Samek, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700211238Arras, L., Arjona-Medina, J., Gillhofer, M., Widrich, M., Montavon, G., Müller, K.R., Hochreiter, S., Samek, W.: Explaining and interpreting LSTMs with LRP. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp. 211238. Springer (2019)</p>
<p>What is relevant in a text document?": An interpretable machine learning approach. L Arras, F Horn, G Montavon, K R Müller, W Samek, PLoS ONE. 128181142Arras, L., Horn, F., Montavon, G., Müller, K.R., Samek, W.: "What is relevant in a text document?": An interpretable machine learning approach. PLoS ONE 12(8), e0181142 (2017)</p>
<p>Explaining recurrent neural network predictions in sentiment analysis. L Arras, G Montavon, K R Müller, W Samek, EMNLP'17 Workshop on Computational Approaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA). Arras, L., Montavon, G., Müller, K.R., Samek, W.: Explaining recurrent neural network predictions in sentiment analysis. In: EMNLP'17 Workshop on Computa- tional Approaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA). pp. 159-168 (2017)</p>
<p>Evaluating recurrent neural network explanations. L Arras, A Osman, K R Müller, W Samek, ACL'19 Workshop on BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Arras, L., Osman, A., Müller, K.R., Samek, W.: Evaluating recurrent neural net- work explanations. In: ACL'19 Workshop on BlackboxNLP: Analyzing and Inter- preting Neural Networks for NLP, pp. 113-126 (2019)</p>
<p>On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. S Bach, A Binder, G Montavon, F Klauschen, K R Müller, W Samek, PLoS ONE. 107130140Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.R., Samek, W.: On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLoS ONE 10(7), e0130140 (2015)</p>
<p>How to explain individual classification decisions. D Baehrens, T Schroeter, S Harmeling, M Kawanabe, K Hansen, K R Müller, Journal of Machine Learning Research. 11Baehrens, D., Schroeter, T., Harmeling, S., Kawanabe, M., Hansen, K., Müller, K.R.: How to explain individual classification decisions. Journal of Machine Learn- ing Research 11, 1803-1831 (2010)</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, International Conference on Learning Representations (ICLR. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. In: International Conference on Learning Representations (ICLR). (2015)</p>
<p>Network dissection: Quantifying interpretability of deep visual representations. D Bau, B Zhou, A Khosla, A Oliva, A Torralba, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Bau, D., Zhou, B., Khosla, A., Oliva, A., Torralba, A.: Network dissection: Quanti- fying interpretability of deep visual representations. In: IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR). pp. 6541-6549 (2017)</p>
<p>Layer-wise relevance propagation for deep neural network architectures. A Binder, S Bach, G Montavon, K R Müller, W Samek, Information Science and Applications (ICISA). Binder, A., Bach, S., Montavon, G., Müller, K.R., Samek, W.: Layer-wise relevance propagation for deep neural network architectures. In: Information Science and Applications (ICISA), pp. 913-922 (2016)</p>
<p>Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles. A Binder, M Bockmayr, M Hägele, S Wienert, D Heim, K Hellweg, A Stenzinger, L Parlow, J Budczies, B Goeppert, arXiv:1805.11178arXiv preprintBinder, A., Bockmayr, M., Hägele, M., Wienert, S., Heim, D., Hellweg, K., Sten- zinger, A., Parlow, L., Budczies, J., Goeppert, B., et al.: Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morpho- logical and molecular tumor profiles. arXiv preprint arXiv:1805.11178 (2018)</p>
<p>Towards exact molecular dynamics simulations with machine-learned force fields. S Chmiela, H E Sauceda, K R Müller, A Tkatchenko, Nature Communications. 913887Chmiela, S., Sauceda, H.E., Müller, K.R., Tkatchenko, A.: Towards exact molecular dynamics simulations with machine-learned force fields. Nature Communications 9(1), 3887 (2018)</p>
<p>A committee of neural networks for traffic sign classification. D Cireşan, U Meier, J Masci, J Schmidhuber, International Joint Conference on Neural Networks (IJCNN). Cireşan, D., Meier, U., Masci, J., Schmidhuber, J.: A committee of neural networks for traffic sign classification. In: International Joint Conference on Neural Networks (IJCNN). pp. 1918-1921 (2011)</p>
<p>Imagenet: A largescale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large- scale hierarchical image database. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 248-255 (2009)</p>
<p>F Doshi-Velez, B Kim, arXiv:1702.08608Towards a rigorous science of interpretable machine learning. arXiv preprintDoshi-Velez, F., Kim, B.: Towards a rigorous science of interpretable machine learn- ing. arXiv preprint arXiv:1702.08608 (2017)</p>
<p>F Doshi-Velez, M Kortz, R Budish, C Bavitz, S Gershman, D O&apos;brien, S Schieber, J Waldo, D Weinberger, A Wood, arXiv:1711.01134Accountability of AI under the law: The role of explanation. arXiv preprintDoshi-Velez, F., Kortz, M., Budish, R., Bavitz, C., Gershman, S., O'Brien, D., Schieber, S., Waldo, J., Weinberger, D., Wood, A.: Accountability of AI under the law: The role of explanation. arXiv preprint arXiv:1711.01134 (2017)</p>
<p>Uncovering convolutional neural network decisions for diagnosing multiple sclerosis on conventional mri using layer-wise relevance propagation. F Eitel, E Soehler, J Bellmann-Strobl, A U Brandt, K Ruprecht, R M Giess, J Kuchling, S Asseyer, M Weygandt, J D Haynes, arXiv:1904.08771arXiv preprintEitel, F., Soehler, E., Bellmann-Strobl, J., Brandt, A.U., Ruprecht, K., Giess, R.M., Kuchling, J., Asseyer, S., Weygandt, M., Haynes, J.D., et al.: Uncovering convo- lutional neural network decisions for diagnosing multiple sclerosis on conventional mri using layer-wise relevance propagation. arXiv preprint arXiv:1904.08771 (2019)</p>
<p>European Commission's High-Level Expert Group: Draft ethics guidelines for trustworthy AI. European CommissionEuropean Commission's High-Level Expert Group: Draft ethics guidelines for trustworthy AI. European Commission (2019)</p>
<p>The PASCAL Visual Object Classes Challenge: A Retrospective. M Everingham, S A Eslami, L Van Gool, C K Williams, J Winn, A Zisserman, International Journal of Computer Vision. 1111Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The PASCAL Visual Object Classes Challenge: A Retrospective. International Journal of Computer Vision 111(1), 98-136 (2015)</p>
<p>The Pascal visual object classes (VOC) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, International Journal of Computer Vision. 882Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The Pascal visual object classes (VOC) challenge. International Journal of Computer Vision 88(2), 303-338 (2010)</p>
<p>K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, C Xiao, A Prakash, T Kohno, D Song, arXiv:1707.08945Robust physical-world attacks on deep learning models. arXiv preprintEykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., Prakash, A., Kohno, T., Song, D.: Robust physical-world attacks on deep learning models. arXiv preprint arXiv:1707.08945 (2017)</p>
<p>Interpretable explanations of black boxes by meaningful perturbation. R C Fong, A Vedaldi, IEEE International Conference on Computer Vision (CVPR). Fong, R.C., Vedaldi, A.: Interpretable explanations of black boxes by meaningful perturbation. In: IEEE International Conference on Computer Vision (CVPR). pp. 3429-3437 (2017)</p>
<p>Explanations for attributing deep neural network predictions. R Fong, A Vedaldi, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700149167Fong, R., Vedaldi, A.: Explanations for attributing deep neural network predic- tions. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp. 149167. Springer (2019)</p>
<p>European union regulations on algorithmic decisionmaking and a "right to explanation. B Goodman, S Flaxman, AI Magazine. 383Goodman, B., Flaxman, S.: European union regulations on algorithmic decision- making and a "right to explanation". AI Magazine 38(3), 50-57 (2017)</p>
<p>Algorithmic bias: From discrimination discovery to fairness-aware data mining. S Hajian, F Bonchi, C Castillo, 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Hajian, S., Bonchi, F., Castillo, C.: Algorithmic bias: From discrimination discovery to fairness-aware data mining. In: 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 2125-2126 (2016)</p>
<p>Learning both weights and connections for efficient neural network. S Han, J Pool, J Tran, W Dally, Advances in Neural Information Processing Systems (NIPS). Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for efficient neural network. In: Advances in Neural Information Processing Systems (NIPS). pp. 1135-1143 (2015)</p>
<p>R L Heath, J Bryant, Human communication theory and research: Concepts, contexts, and challenges. Routledge. Heath, R.L., Bryant, J.: Human communication theory and research: Concepts, contexts, and challenges. Routledge (2013)</p>
<p>Visual scene understanding for autonomous driving using semantic segmentation. M Hofmarcher, T Unterthiner, J Arjona-Medina, G Klambauer, S Hochreiter, B Nessler, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700285296Hofmarcher, M., Unterthiner, T., Arjona-Medina, J., Klambauer, G., Hochreiter, S., Nessler, B.: Visual scene understanding for autonomous driving using semantic segmentation. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp. 285296. Springer (2019)</p>
<p>Causability and explainabilty of artificial intelligence in medicine. A Holzinger, G Langs, H Denk, K Zatloukal, H Müller, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery p. 1312Holzinger, A., Langs, G., Denk, H., Zatloukal, K., Müller, H.: Causability and explainabilty of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery p. e1312 (2019)</p>
<p>Explaining the unique nature of individual gait patterns with deep learning. F Horst, S Lapuschkin, W Samek, K R Müller, W I Schöllhorn, Scientific Reports. 92391Horst, F., Lapuschkin, S., Samek, W., Müller, K.R., Schöllhorn, W.I.: Explaining the unique nature of individual gait patterns with deep learning. Scientific Reports 9, 2391 (2019)</p>
<p>Largescale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, IEEE conference on Computer Vision and Pattern Recognition (CVPR). Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large- scale video classification with convolutional neural networks. In: IEEE conference on Computer Vision and Pattern Recognition (CVPR). pp. 1725-1732 (2014)</p>
<p>Towards explaining anomalies: A deep Taylor decomposition of one-class models. J Kauffmann, K R Müller, G Montavon, arXiv:1805.06230arXiv preprintKauffmann, J., Müller, K.R., Montavon, G.: Towards explaining anomalies: A deep Taylor decomposition of one-class models. arXiv preprint arXiv:1805.06230 (2018)</p>
<p>J Kauffmann, M Esders, G Montavon, W Samek, K R Müller, arXiv:1906.07633From clustering to cluster explanations via neural networks. arXiv preprintKauffmann, J., Esders, M., Montavon, G., Samek, W., Müller, K.R.: From cluster- ing to cluster explanations via neural networks. arXiv preprint arXiv:1906.07633 (2019)</p>
<p>R Khanna, B Kim, J Ghosh, O Koyejo, arXiv:1810.10118Interpreting black box predictions using fisher kernels. arXiv preprintKhanna, R., Kim, B., Ghosh, J., Koyejo, O.: Interpreting black box predictions using fisher kernels. arXiv preprint arXiv:1810.10118 (2018)</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). B Kim, M Wattenberg, J Gilmer, C Cai, J Wexler, F Viegas, R Sayres, International Conference on Machine Learning (ICML). Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., Sayres, R.: Interpretability beyond feature attribution: Quantitative testing with concept acti- vation vectors (TCAV). In: International Conference on Machine Learning (ICML). pp. 2673-2682, (2018)</p>
<p>Learning how to explain neural networks: Patternnet and patternattribution. P J Kindermans, K T Schütt, M Alber, K R Müller, D Erhan, B Kim, S Dähne, International Conference on Learning Representations (ICLR). Kindermans, P.J., Schütt, K.T., Alber, M., Müller, K.R., Erhan, D., Kim, B., Dähne, S.: Learning how to explain neural networks: Patternnet and patternattri- bution. In: International Conference on Learning Representations (ICLR). (2018)</p>
<p>Scoring of tumor-infiltrating lymphocytes: From visual estimation to machine learning. F Klauschen, K R Müller, A Binder, M Bockmayr, M Hägele, P Seegerer, S Wienert, G Pruneri, S De Maria, S Badve, Seminars in Cancer Biology. 522Klauschen, F., Müller, K.R., Binder, A., Bockmayr, M., Hägele, M., Seegerer, P., Wienert, S., Pruneri, G., de Maria, S., Badve, S., et al.: Scoring of tumor-infiltrating lymphocytes: From visual estimation to machine learning. Seminars in Cancer Biology 52(2), 151-157 (2018)</p>
<p>Understanding black-box predictions via influence functions. P W Koh, P Liang, International Conference on Machine Learning (ICML). Koh, P.W., Liang, P.: Understanding black-box predictions via influence functions. In: International Conference on Machine Learning (ICML). pp. 1885-1894 (2017)</p>
<p>Information-based functional brain mapping. N Kriegeskorte, R Goebel, P Bandettini, Proceedings of the National Academy of Sciences. 10310Kriegeskorte, N., Goebel, R., Bandettini, P.: Information-based functional brain mapping. Proceedings of the National Academy of Sciences 103(10), 3863-3868 (2006)</p>
<p>An evaluation of the human-interpretability of explanation. I Lage, E Chen, J He, M Narayanan, B Kim, S Gershman, F Doshi-Velez, arXiv:1902.00006arXiv preprintLage, I., Chen, E., He, J., Narayanan, M., Kim, B., Gershman, S., Doshi-Velez, F.: An evaluation of the human-interpretability of explanation. arXiv preprint arXiv:1902.00006 (2019)</p>
<p>Analyzing classifiers: Fisher vectors and deep neural networks. S Lapuschkin, A Binder, G Montavon, K R Müller, W Samek, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Lapuschkin, S., Binder, A., Montavon, G., Müller, K.R., Samek, W.: Analyzing classifiers: Fisher vectors and deep neural networks. In: IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR). pp. 2912-2920 (2016)</p>
<p>Opening the Machine Learning Black Box with Layer-wise Relevance Propagation. S Lapuschkin, Technische Universität BerlinPh.D. thesisLapuschkin, S.: Opening the Machine Learning Black Box with Layer-wise Rele- vance Propagation. Ph.D. thesis, Technische Universität Berlin (2019)</p>
<p>Unmasking clever hans predictors and assessing what machines really learn. S Lapuschkin, S Wäldchen, A Binder, G Montavon, W Samek, K R Müller, Nature Communications. 101096Lapuschkin, S., Wäldchen, S., Binder, A., Montavon, G., Samek, W., Müller, K.R.: Unmasking clever hans predictors and assessing what machines really learn. Nature Communications 10, 1096 (2019)</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436-444 (2015)</p>
<p>Y A Lecun, L Bottou, G B Orr, K R Müller, Neural networks: Tricks of the trade. SpringerEfficient backpropLeCun, Y.A., Bottou, L., Orr, G.B., Müller, K.R.: Efficient backprop. In: Neural networks: Tricks of the trade, pp. 9-48. Springer (2012)</p>
<p>Introduction to machine learning for brain imaging. S Lemm, B Blankertz, T Dickhaus, K R Müller, Neuroimage. 562Lemm, S., Blankertz, B., Dickhaus, T., Müller, K.R.: Introduction to machine learning for brain imaging. Neuroimage 56(2), 387-399 (2011)</p>
<p>Understanding Neural Networks through Representation Erasure. J Li, W Monroe, D Jurafsky, arXiv:1612.08220arXiv preprintLi, J., Monroe, W., Jurafsky, D.: Understanding Neural Networks through Repre- sentation Erasure. arXiv preprint arXiv:1612.08220 (2016)</p>
<p>Machine learning applications in genetics and genomics. M W Libbrecht, W S Noble, Nature Reviews Genetics. 166321Libbrecht, M.W., Noble, W.S.: Machine learning applications in genetics and ge- nomics. Nature Reviews Genetics 16(6), 321 (2015)</p>
<p>Nvidia tesla: A unified graphics and computing architecture. E Lindholm, J Nickolls, S Oberman, J Montrym, IEEE Micro. 282Lindholm, E., Nickolls, J., Oberman, S., Montrym, J.: Nvidia tesla: A unified graphics and computing architecture. IEEE Micro 28(2), 39-55 (2008)</p>
<p>Surpassing human-level face verification performance on LFW with GaussianFace. C Lu, X Tang, 29th AAAI Conference on Artificial Intelligence. Lu, C., Tang, X.: Surpassing human-level face verification performance on LFW with GaussianFace. In: 29th AAAI Conference on Artificial Intelligence. pp. 3811- 3819 (2015)</p>
<p>A unified approach to interpreting model predictions. S M Lundberg, S I Lee, Advances in Neural Information Processing Systems (NIPS). Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions. In: Advances in Neural Information Processing Systems (NIPS). pp. 4765-4774 (2017)</p>
<p>Towards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, International Conference on Learning Representations (ICLR). Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. In: International Conference on Learning Representations (ICLR). (2018)</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level control through deep reinforcement learning. Nature 518(7540), 529-533 (2015)</p>
<p>Gradient-based vs. propagation-based explanations: An axiomatic comparison. G Montavon, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700253265Montavon, G.: Gradient-based vs. propagation-based explanations: An axiomatic comparison. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp. 253265. Springer (2019)</p>
<p>Layer-wise relevance propagation: An overview. G Montavon, A Binder, S Lapuschkin, W Samek, K R Müller, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700Montavon, G., Binder, A., Lapuschkin, S., Samek, W., Müller, K.R.: Layer-wise relevance propagation: An overview. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp. 193-209. Springer (2019)</p>
<p>Explaining nonlinear classification decisions with deep Taylor decomposition. G Montavon, S Lapuschkin, A Binder, W Samek, K R Müller, Pattern Recognition. 65Montavon, G., Lapuschkin, S., Binder, A., Samek, W., Müller, K.R.: Explaining nonlinear classification decisions with deep Taylor decomposition. Pattern Recog- nition 65, 211-222 (2017)</p>
<p>Methods for interpreting and understanding deep neural networks. G Montavon, W Samek, K R Müller, Digital Signal Processing. 73Montavon, G., Samek, W., Müller, K.R.: Methods for interpreting and understand- ing deep neural networks. Digital Signal Processing 73, 1-15 (2018)</p>
<p>Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. M Moravčík, M Schmid, N Burch, V Lisý, D Morrill, N Bard, Science. 3566337Moravčík, M., Schmid, M., Burch, N., Lisý, V., Morrill, D., Bard, N., et al.: Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science 356(6337), 508-513 (2017)</p>
<p>Visualization of neural networks using saliency maps. N Morch, U Kjems, L K Hansen, C Svarer, I Law, B Lautrup, S Strother, K Rehm, International Conference on Neural Networks (ICNN). 4Morch, N., Kjems, U., Hansen, L.K., Svarer, C., Law, I., Lautrup, B., Strother, S., Rehm, K.: Visualization of neural networks using saliency maps. In: International Conference on Neural Networks (ICNN). vol. 4, pp. 2085-2090 (1995)</p>
<p>A Mordvintsev, C Olah, M Tyka, Inceptionism: Going deeper into neural networks. Mordvintsev, A., Olah, C., Tyka, M.: Inceptionism: Going deeper into neural net- works (2015)</p>
<p>Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. A Nguyen, A Dosovitskiy, J Yosinski, T Brox, J Clune, Advances in Neural Information Processing Systems (NIPS). Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., Clune, J.: Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In: Advances in Neural Information Processing Systems (NIPS). pp. 3387-3395 (2016)</p>
<p>Understanding neural networks via feature visualization: A survey. A Nguyen, J Yosinski, J Clune, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer117005576Nguyen, A., Yosinski, J., Clune, J.: Understanding neural networks via feature visualization: A survey. In: Explainable AI: Interpreting, Explaining and Visualiz- ing Deep Learning. Lecture Notes in Computer Science 11700, pp. 5576. Springer (2019)</p>
<p>Comparing Automatic and Human Evaluation of Local Explanations for Text Classification. D Nguyen, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). Nguyen, D.: Comparing Automatic and Human Evaluation of Local Explanations for Text Classification. In: Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies (NAACL- HLT). pp. 1069-1078 (2018)</p>
<p>Analysis of big data in gait biomechanics: Current trends and future directions. A Phinyomark, G Petri, E Ibáñez-Marcelo, S T Osis, R Ferber, Journal of Medical and Biological Engineering. 382Phinyomark, A., Petri, G., Ibáñez-Marcelo, E., Osis, S.T., Ferber, R.: Analysis of big data in gait biomechanics: Current trends and future directions. Journal of Medical and Biological Engineering 38(2), 244-260 (2018)</p>
<p>Accelerating materials property predictions using machine learning. G Pilania, C Wang, X Jiang, S Rajasekaran, R Ramprasad, Scientific Reports. 32810Pilania, G., Wang, C., Jiang, X., Rajasekaran, S., Ramprasad, R.: Accelerating materials property predictions using machine learning. Scientific Reports 3, 2810 (2013)</p>
<p>Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement. N Poerner, B Roth, H Schütze, 56th Annual Meeting of the Association for Computational Linguistics (ACL). Poerner, N., Roth, B., Schütze, H.: Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement. In: 56th Annual Meeting of the Association for Computational Linguistics (ACL). pp. 340-350 (2018)</p>
<p>Interpretable deep learning in drug discovery. K Preuer, G Klambauer, F Rippmann, S Hochreiter, T Unterthiner, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700Preuer, K., Klambauer, G., Rippmann, F., Hochreiter, S., Unterthiner, T.: Inter- pretable deep learning in drug discovery. In: Explainable AI: Interpreting, Explain- ing and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, Springer (2019)</p>
<p>You only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 779-788 (2016)</p>
<p>Enhanced rotational invariant convolutional neural network for supernovae detection. E Reyes, P A Estévez, I Reyes, G Cabrera-Vives, P Huijse, R Carrasco, F Forster, International Joint Conference on Neural Networks (IJCNN). Reyes, E., Estévez, P.A., Reyes, I., Cabrera-Vives, G., Huijse, P., Carrasco, R., Forster, F.: Enhanced rotational invariant convolutional neural network for super- novae detection. In: International Joint Conference on Neural Networks (IJCNN). pp. 1-8 (2018)</p>
<p>Why should i trust you?: Explaining the predictions of any classifier. M T Ribeiro, S Singh, C Guestrin, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Ribeiro, M.T., Singh, S., Guestrin, C.: Why should i trust you?: Explaining the predictions of any classifier. In: ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining. pp. 1135-1144 (2016)</p>
<p>Right for the right reasons: Training differentiable models by constraining their explanations. A S Ross, M C Hughes, F Doshi-Velez, 26th International Joint Conferences on Artificial Intelligence (IJCAI). Ross, A.S., Hughes, M.C., Doshi-Velez, F.: Right for the right reasons: Training differentiable models by constraining their explanations. In: 26th International Joint Conferences on Artificial Intelligence (IJCAI). pp. 2662-2670 (2017)</p>
<p>Evaluating the visualization of what a deep neural network has learned. W Samek, A Binder, G Montavon, S Lapuschkin, K R Müller, IEEE Transactions on Neural Networks and Learning Systems. 2811Samek, W., Binder, A., Montavon, G., Lapuschkin, S., Müller, K.R.: Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning Systems 28(11), 2660-2673 (2017)</p>
<p>Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. W Samek, T Wiegand, K R Müller, ITU Journal: ICT Discoveries -Special Issue 1 -The Impact of Artificial Intelligence (AI) on Communication Networks and Services. 11Samek, W., Wiegand, T., Müller, K.R.: Explainable artificial intelligence: Under- standing, visualizing and interpreting deep learning models. ITU Journal: ICT Discoveries -Special Issue 1 -The Impact of Artificial Intelligence (AI) on Com- munication Networks and Services 1(1), 39-48 (2018)</p>
<p>Image classification with the Fisher vector: Theory and practice. J Sánchez, F Perronnin, T Mensink, J J Verbeek, International Journal of Computer Vision. 1053Sánchez, J., Perronnin, F., Mensink, T., Verbeek, J.J.: Image classification with the Fisher vector: Theory and practice. International Journal of Computer Vision 105(3), 222-245 (2013)</p>
<p>Quantum-chemical insights from deep tensor neural networks. K T Schütt, F Arbabzadah, S Chmiela, K R Müller, A Tkatchenko, Nature Communications. 813890Schütt, K.T., Arbabzadah, F., Chmiela, S., Müller, K.R., Tkatchenko, A.: Quantum-chemical insights from deep tensor neural networks. Nature Commu- nications 8, 13890 (2017)</p>
<p>Gradcam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, IEEE International Conference on Computer Vision (CVPR). Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad- cam: Visual explanations from deep networks via gradient-based localization. In: IEEE International Conference on Computer Vision (CVPR). pp. 618-626 (2017)</p>
<p>A value for n-person games. L S Shapley, Contributions to the Theory of Games. 2Shapley, L.S.: A value for n-person games. Contributions to the Theory of Games 2(28), 307-317 (1953)</p>
<p>A Shrikumar, P Greenside, A Kundaje, arXiv:1704.02685Learning Important Features Through Propagating Activation Differences. arXiv preprintShrikumar, A., Greenside, P., Kundaje, A.: Learning Important Features Through Propagating Activation Differences. arXiv preprint arXiv:1704.02685 (2017)</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, Nature. 5297587Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., et al.: Mastering the game of Go with deep neural networks and tree search. Nature 529(7587), 484-489 (2016)</p>
<p>Mastering the game of Go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 5507676Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of Go without human knowledge. Nature 550(7676), 354-359 (2017)</p>
<p>Deep inside convolutional networks: Visualising image classification models and saliency maps. K Simonyan, A Vedaldi, A Zisserman, In: ICLR Workshop. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps. In: ICLR Workshop. (2014)</p>
<p>D Smilkov, N Thorat, B Kim, F Viégas, M Wattenberg, arXiv:1706.03825Smoothgrad: removing noise by adding noise. arXiv preprintSmilkov, D., Thorat, N., Kim, B., Viégas, F., Wattenberg, M.: Smoothgrad: re- moving noise by adding noise. arXiv preprint arXiv:1706.03825 (2017)</p>
<p>Striving for simplicity: The all convolutional net. J T Springenberg, A Dosovitskiy, T Brox, M Riedmiller, In: ICLR Workshop. Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.: Striving for simplic- ity: The all convolutional net. In: ICLR Workshop. (2015)</p>
<p>Interpretable deep neural networks for single-trial eeg classification. I Sturm, S Lapuschkin, W Samek, K R Müller, Journal of Neuroscience Methods. 274Sturm, I., Lapuschkin, S., Samek, W., Müller, K.R.: Interpretable deep neural networks for single-trial eeg classification. Journal of Neuroscience Methods 274, 141-145 (2016)</p>
<p>Axiomatic attribution for deep networks. M Sundararajan, A Taly, Q Yan, International Conference on Machine Learning (ICML). Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In: International Conference on Machine Learning (ICML). pp. 3319-3328 (2017)</p>
<p>Analyzing neuroimaging data through recurrent deep learning models. A W Thomas, H R Heekeren, K R Müller, W Samek, arXiv:1810.09945arXiv preprintThomas, A.W., Heekeren, H.R., Müller, K.R., Samek, W.: Analyzing neuroimag- ing data through recurrent deep learning models. arXiv preprint arXiv:1810.09945 (2018)</p>
<p>Wavenet: A generative model for raw audio. A Van Den Oord, S Dieleman, H Zen, K Simonyan, O Vinyals, A Graves, N Kalchbrenner, A W Senior, K Kavukcuoglu, SSW. 125Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A.W., Kavukcuoglu, K.: Wavenet: A generative model for raw audio. SSW 125 (2016)</p>
<p>Transparency: Motivations and Challenges. A Weller, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700Weller, A.: Transparency: Motivations and Challenges. In: Explainable AI: Inter- preting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, Springer (2019)</p>
<p>Solving statistical mechanics using variational autoregressive networks. D Wu, L Wang, P Zhang, Physical Review Letters. 122880602Wu, D., Wang, L., Zhang, P.: Solving statistical mechanics using variational au- toregressive networks. Physical Review Letters 122(8), 080602 (2019)</p>
<p>J Yosinski, J Clune, A Nguyen, T Fuchs, H Lipson, arXiv:1506.06579Understanding neural networks through deep visualization. arXiv preprintYosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H.: Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579 (2015)</p>
<p>Visualizing and understanding convolutional networks. M D Zeiler, R Fergus, European Conference Computer Vision (ECCV). Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: European Conference Computer Vision (ECCV). pp. 818-833 (2014)</p>
<p>Top-down neural attention by excitation backprop. J Zhang, Z L Lin, J Brandt, X Shen, S Sclaroff, European Conference on Computer Vision (ECCV). Zhang, J., Lin, Z.L., Brandt, J., Shen, X., Sclaroff, S.: Top-down neural attention by excitation backprop. In: European Conference on Computer Vision (ECCV). pp. 543-559 (2016)</p>
<p>Comparing the interpretability of deep networks via network dissection. B Zhou, D Bau, A Oliva, A Torralba, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer11700243252Zhou, B., Bau, D., Oliva, A., Torralba, A.: Comparing the interpretability of deep networks via network dissection. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science 11700, pp. 243252. Springer (2019)</p>
<p>Visualizing deep neural network decisions: Prediction difference analysis. L M Zintgraf, T S Cohen, T Adel, M Welling, International Conference on Learning Representations (ICLR. Zintgraf, L.M., Cohen, T.S., Adel, T., Welling, M.: Visualizing deep neural network decisions: Prediction difference analysis. In: International Conference on Learning Representations (ICLR). (2017)</p>            </div>
        </div>

    </div>
</body>
</html>
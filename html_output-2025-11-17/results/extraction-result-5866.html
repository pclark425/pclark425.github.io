<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5866 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5866</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5866</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-54972b2e4304d2164a61036ae947df2503c07009</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/54972b2e4304d2164a61036ae947df2503c07009" target="_blank">Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model’s knowledge, and as the examples with new knowledge are eventually learned, they linearly increase the model’s tendency to hallucinate.</p>
                <p><strong>Paper Abstract:</strong> When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model’s knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model’s tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5866.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5866.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SliCK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling-based Categorization of Knowledge (SliCK)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A four-category taxonomy to label (question,answer) pairs by whether a base LLM 'knows' the fact, derived from an empirical P_correct estimate obtained via multiple random few-shot prompts and sampling; categories: HighlyKnown, MaybeKnown, WeaklyKnown, Unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>EntityQuestions (Wikidata triplets converted to QA) closed-book question answering; disjoint train/dev/test splits over relations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Estimate P_correct by running N_ex = 10 random 4-shot in-context prompts (exemplars chosen from same relation). For each prompt: record greedy decoding (T=0) and 16 sampled outputs using temperature T=0.5 (samples drawn from top-40). Categorize (q,a) as HighlyKnown / MaybeKnown / WeaklyKnown / Unknown according to the fraction of correct greedy/sampled outputs (see definitions in paper). Exact Match (EM) is used to determine if an output equals the ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to a P(True)-based approach (Kadavath et al., 2022) which elicits a binary True/False probability for a proposed answer using a direct prompt; also compared different approximations of P_correct by varying N_ex (number of random few-shot exemplar sets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SliCK Unknown examples have extremely low post-finetuning accuracy (<= 3.2% on Unknown subset in Table 2), indicating SliCK successfully identifies examples that remain/are harmful; per-dataset category distributions: e.g., train: HighlyKnown 24%, MaybeKnown 23%, WeaklyKnown 17%, Unknown 36%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>P(True)-based Unknown sets (for the same fraction of test examples) exhibit higher post-finetuning accuracy than SliCK's Unknown set (i.e., P(True) flags fewer of the truly harmful items); using fewer exemplar prompts (N_ex < 10) to approximate P_correct worsens SliCK's selection quality (higher accuracy on selected Unknowns post-finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (SliCK better identifies harmful Unknown examples vs the P(True) thresholding baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Multiple random few-shot exemplar prompts + sampling better captures the model's stochastic tendencies and exemplar-dependence, producing a more reliable estimate of whether the model possesses a fact; greedy-versus-sampled behavior distinguishes stable knowledge (greedy) from fragile/latent knowledge (only seen under sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5866.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>4-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>4-shot in-context prompting with relation-matched exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using 4-shot few-shot exemplars drawn from the same relation as the target question to prompt the base model for P_correct estimation; repeated with N_ex = 10 different random exemplar sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Probe model's parametric knowledge by asking it to answer QA pairs in a few-shot in-context setting to estimate P_correct.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>4-shot exemplars (questions/answers) plus target question; exemplars are selected from same relation to ensure semantic similarity; repeated across N_ex = 10 distinct 4-shot combinations to capture exemplar variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ablation on N_ex (fewer than 10 exemplars sets) showing degraded P_correct estimation quality; also contrasted with greedy-only and sampled decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used as the main protocol to estimate P_correct; reliability improved with N_ex = 10; using fewer exemplar-sets produced higher post-finetuning accuracy on SliCK-identified Unknowns (worse identification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative: N_ex < 10 increases misclassification of Unknown examples (paper shows an upward trend in test accuracy on SliCK-Unknown when using fewer exemplar sets), exact numeric delta not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved estimation fidelity with more exemplar sets (N_ex=10)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Choice of exemplars influences predictions; averaging over multiple random exemplar sets reduces variance and yields a more accurate assessment of whether the model truly knows the fact.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5866.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy vs Temperature decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy decoding (T=0) versus temperature sampling (T=0.5, 16 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding mode used both for SliCK categorization (greedy outcome used to separate HighlyKnown/MaybeKnown) and for detecting latent knowledge via sampling (WeaklyKnown identified when sampled outputs sometimes match).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Determine knowledge strength by comparing deterministic greedy outputs to stochastic sampled outputs for the same few-shot prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Greedy decoding (T=0) provides one deterministic prediction per prompt; temperature sampling (T=0.5) yields 16 samples per prompt; categorizations rely on whether greedy or sampled outputs match ground truth across N_ex prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Operational definitions: HighlyKnown P_correct(T=0)=1, MaybeKnown P_correct(T=0) in (0,1), WeaklyKnown P_correct(T=0)=0 & P_correct(T>0)>0, Unknown P_correct(T>=0)=0. WeaklyKnown test accuracies sit between MaybeKnown and Unknown as expected.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>used to reveal latent vs stable knowledge (sampling reveals knowledge not surfaced by greedy decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Greedy decoding shows the model's dominant (most likely) answer; sampling reveals lower-probability but present associations, allowing a finer-grained knowledge taxonomy which affects which fine-tuning examples are considered 'known' or 'unknown'.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5866.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unknown-ratio fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Varying proportion (%Unknown) of Unknown examples in fine-tuning dataset D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled experiment that constructs fine-tuning datasets with fixed size but different percentages of examples that are Unknown to the base model (per SliCK), to measure how introducing new factual knowledge affects downstream closed-book QA performance and hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tune PaLM 2-S on datasets D with X% Unknown examples (sampled per-relation to preserve relation distribution), evaluate on held-out test/dev (in-distribution) and OOD relations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Supervised fine-tuning on QA pairs (one-line instruction-style prompt shown in paper). Fixed |D| = 6142 examples; vary %Unknown (0% to 100%). Evaluate under two regimes: EARLY_STOP (epoch with best dev accuracy, typically 5–10 epochs) and CONVERGENCE (50 epochs, full training to fit D).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>EARLY_STOP vs CONVERGENCE; also compared D vs D_Known (filtering Unknown examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Higher %Unknown causes systematic performance degradation: e.g., for 50% Unknown, D EARLY_STOP test accuracy 43.0% vs CONVERGENCE 38.8% (Table 3). Overall in-distribution performance drop up to ~14 percentage points between best and worst configurations; OOD drop up to ~6 points (paper I).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Filtering Unknown (D_Known) reduces degradation: for EARLY_STOP D and D_Known nearly identical, while at CONVERGENCE D underperforms D_Known by a margin proportional to %Unknown. Linear model coefficients (in-distribution): beta0=36.9, beta_kn=+7.3, beta_unk=-8.3 (R^2=0.86); OOD: beta0=36.2, beta_kn=+3.2, beta_unk=-3.0 (R^2=0.95).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Quantitative: fitting one Known example contributes ~+7.3 accuracy units (normalized as in paper) while fitting one Unknown contributes ~-8.3 (in the authors' linear regression units); raw test accuracy deltas up to ~14 percentage points in-distribution and ~6 points OOD across %Unknown sweeps.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (increased %Unknown reduces test accuracy; introducing Unknown examples increases hallucination risk)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Fine-tuning on examples that introduce new factual knowledge (Unknown) is learned slowly and, once learned, causes the model to adopt a behavior of generating answers not grounded in its pre-existing knowledge, effectively increasing hallucinations; Unknown examples primarily cause overfitting in later training stages.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>At EARLY_STOP (when model has fit most Known but few Unknown), removing Unknown examples had neutral effect — Unknown examples had little effect early in training; negative effects largely manifest after further training (overfitting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5866.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Filter Unknown (D_Known)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Filtering out Unknown examples from fine-tuning dataset (D_Known)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation that removes SliCK-identified Unknown examples from D, retaining only Known examples for fine-tuning, to test whether filtering reduces hallucination and overfitting without harming performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tune on D_Known (only Known examples) with same |D_Known| = (1 - %Unknown) * |D|; evaluate EARLY_STOP vs CONVERGENCE.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same supervised QA fine-tuning but with Unknown examples removed prior to training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to full D (includes Unknown).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For EARLY_STOP, performance of D and D_Known nearly identical; for CONVERGENCE, D_Known outperforms D, and the gap scales with %Unknown (see Figure 3b). Removing Unknown reduces overfitting and preserves or slightly improves test accuracy at longer training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative/relative: removal of Unknown examples substantially reduces the CONVERGENCE-era performance drop caused by overfitting on Unknowns (gap proportional to %Unknown; authors report up to ~14 point in-distribution drops when Unknown are present and fitted).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (filtering reduces harmful overfitting caused by Unknowns)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Unknown examples are learned late and promote overfitting/hallucination; removing them avoids training the model into the behavior of producing unsupported facts while preserving benefits from Known examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Filtering may remove examples that could teach the model to express uncertainty on unknown test items; thus filtering sacrifices the ability to train abstention unless compensated (see IDK relabeling).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5866.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstention fine-tuning (D_IDK)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning Unknown examples with label 'I don't know' (D_IDK)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace labels of SliCK-identified Unknown training examples with an uncertainty expression 'I don't know' to teach the model to abstain rather than produce (potentially hallucinatory) facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Modify fine-tuning labels for Unknown examples to 'I don't know' and measure how often the fine-tuned model answers and the accuracy on those answered questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same supervised fine-tuning but Unknown examples' target answers replaced by the string 'I don't know' (D_IDK).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to standard fine-tuning on D (Unknown labeled with correct ground truth answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3: For D (50% Unknown) EARLY_STOP accuracy 43.0% (% answered 100.0); CONVERGENCE 38.8% (% answered 100.0). For D_IDK: answered-subset accuracy 61.8% at EARLY_STOP (% answered 58.7) and 61.8% at CONVERGENCE (% answered 55.6). Thus D_IDK increases accuracy on willingly-answered questions and mitigates the CONVERGENCE-era drop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>D_IDK answered-subset accuracy (61.8%) vs D overall answered accuracy (43.0% at EARLY_STOP) — absolute improvement of ~18.8 percentage points on answered items; also D_IDK removes the CONVERGENCE-era drop (61.8% stays constant).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+18.8 pp accuracy on answered subset compared to D EARLY_STOP (for 50% Unknown experimental case); reduces overfitting-related accuracy drop observed when training to convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (teaching abstention increased accuracy on answered items and mitigated overfitting)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Training with 'I don't know' teaches the model to abstain on examples where it lacks knowledge, preventing adoption of the behavior of producing unsupported facts and thereby reducing hallucinations; however it reduces the proportion of questions the model attempts to answer.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5866.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Early-stopping vs Convergence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early-stopping on dev set (EARLY_STOP) vs training to convergence (50 epochs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of stopping criteria showing that early-stopping at the dev-set peak often yields the best test performance and avoids overfitting on Unknown examples which are learned later.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tune and evaluate models across epochs; identify EARLY_STOP epoch (best dev accuracy) and compare to CONVERGENCE (50 epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same fine-tuning inputs; stopping criterion differs (dev-based early stopping vs fixed long training).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>EARLY_STOP vs CONVERGENCE results presented across %Unknown sweeps and single-category datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>EARLY_STOP typically yields highest dev/test performance; example: for 50% Unknown, EARLY_STOP test accuracy 43.0% vs CONVERGENCE 38.8% (Table 3). For D_Known variants, the EARLY_STOP vs CONVERGENCE gap is small, indicating Unknown examples drive late-stage degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to several percentage points (example: ~4.2 pp in the 50% Unknown case) improvement by using EARLY_STOP vs CONVERGENCE; magnitude grows with %Unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (EARLY_STOP reduces overfitting and yields better test accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Unknown examples are fitted slowly and cause later-stage overfitting to behavior that increases hallucinations; early stopping prevents the model from fitting many Unknowns and therefore preserves better utilization of pre-existing knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Early-stopping may be less effective when fine-tuning across many diverse tasks with different optimal stopping points; selecting a single early stop for multi-task mixtures is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5866.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relation-matched exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using few-shot exemplars sampled from the same relation (semantic similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Selecting few-shot exemplars from the same QA relation as the target question to reduce exemplar-distribution mismatch and improve answer-format consistency in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>When building the few-shot prompt, choose exemplars whose question template matches the target question template (same Wikidata relation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>4-shot prompts where each exemplar follows the same question template as the target (e.g., 'Where is [E] located?').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicitly compared to arbitrary exemplar selection (not explicitly tabulated), but used as part of SliCK protocol to stabilize P_correct estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as a single scalar; used to ensure the model outputs are in the correct format and to reduce variance across exemplars. Authors state this approach improved prompt effectiveness for P_correct estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (makes few-shot prompting more reliable for P_correct estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Semantic similarity between exemplars and target question reduces style/format mismatch and the resulting variance in model outputs across exemplar choices.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5866.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5866.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P(True) comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>P(True) method (Kadavath et al., 2022) used for Unknown identification comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-prompt approach that elicits the model's probability that a proposed answer is True vs False; used here as a baseline to compare SliCK's Unknown identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models (mostly) know what they know</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Closed-book QA (EntityQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compute P(True) for each (q,a) by prompting the model to choose True/False for the proposed answer and use thresholding to classify Unknown vs Known.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single direct prompt that asks 'Is the proposed answer True or False?' and measures the model's probability for True; applied to the ground-truth answer a.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared across thresholds T in [0,1] to SliCK's Unknown categorization; plotted fraction labeled Unknown vs post-finetuning accuracy on those examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>P(True)-based Unknown sets (for the same fraction of test examples as SliCK) have substantially higher accuracy after fine-tuning versus SliCK's Unknown, indicating P(True) is less selective for the particularly harmful Unknowns in this setup (see Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative: for fixed % of examples labeled Unknown, P(True)-selected Unknowns produce higher post-finetune accuracy than SliCK-selected Unknowns (i.e., SliCK selects more problematic items); exact numeric deltas across thresholds shown in Fig.5 but not summarized as a single scalar.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>SliCK outperforms P(True) in identifying examples that will cause larger post-finetuning degradation</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>P(True) uses a single prompt and therefore underestimates exemplar and sampling variance; SliCK's multiple exemplar + sampling procedure better captures whether the model can actually produce the ground-truth answer under realistic prompting variability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models (mostly) know what they know <em>(Rating: 2)</em></li>
                <li>R-tuning: Teaching large language models to refuse unknown questions <em>(Rating: 2)</em></li>
                <li>Unfamiliar finetuning examples control how language models hallucinate <em>(Rating: 2)</em></li>
                <li>Learning or self-aligning? rethinking instruction fine-tuning <em>(Rating: 2)</em></li>
                <li>Understanding finetuning for factual knowledge extraction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5866",
    "paper_id": "paper-54972b2e4304d2164a61036ae947df2503c07009",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "SliCK",
            "name_full": "Sampling-based Categorization of Knowledge (SliCK)",
            "brief_description": "A four-category taxonomy to label (question,answer) pairs by whether a base LLM 'knows' the fact, derived from an empirical P_correct estimate obtained via multiple random few-shot prompts and sampling; categories: HighlyKnown, MaybeKnown, WeaklyKnown, Unknown.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "EntityQuestions (Wikidata triplets converted to QA) closed-book question answering; disjoint train/dev/test splits over relations.",
            "problem_format": "Estimate P_correct by running N_ex = 10 random 4-shot in-context prompts (exemplars chosen from same relation). For each prompt: record greedy decoding (T=0) and 16 sampled outputs using temperature T=0.5 (samples drawn from top-40). Categorize (q,a) as HighlyKnown / MaybeKnown / WeaklyKnown / Unknown according to the fraction of correct greedy/sampled outputs (see definitions in paper). Exact Match (EM) is used to determine if an output equals the ground truth.",
            "comparison_format": "Compared to a P(True)-based approach (Kadavath et al., 2022) which elicits a binary True/False probability for a proposed answer using a direct prompt; also compared different approximations of P_correct by varying N_ex (number of random few-shot exemplar sets).",
            "performance": "SliCK Unknown examples have extremely low post-finetuning accuracy (&lt;= 3.2% on Unknown subset in Table 2), indicating SliCK successfully identifies examples that remain/are harmful; per-dataset category distributions: e.g., train: HighlyKnown 24%, MaybeKnown 23%, WeaklyKnown 17%, Unknown 36%.",
            "performance_comparison": "P(True)-based Unknown sets (for the same fraction of test examples) exhibit higher post-finetuning accuracy than SliCK's Unknown set (i.e., P(True) flags fewer of the truly harmful items); using fewer exemplar prompts (N_ex &lt; 10) to approximate P_correct worsens SliCK's selection quality (higher accuracy on selected Unknowns post-finetuning).",
            "format_effect_size": null,
            "format_effect_direction": "improved (SliCK better identifies harmful Unknown examples vs the P(True) thresholding baseline)",
            "explanation_or_hypothesis": "Multiple random few-shot exemplar prompts + sampling better captures the model's stochastic tendencies and exemplar-dependence, producing a more reliable estimate of whether the model possesses a fact; greedy-versus-sampled behavior distinguishes stable knowledge (greedy) from fragile/latent knowledge (only seen under sampling).",
            "counterexample_or_null_result": null,
            "uuid": "e5866.0",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "4-shot in-context prompting",
            "name_full": "4-shot in-context prompting with relation-matched exemplars",
            "brief_description": "Using 4-shot few-shot exemplars drawn from the same relation as the target question to prompt the base model for P_correct estimation; repeated with N_ex = 10 different random exemplar sets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "Probe model's parametric knowledge by asking it to answer QA pairs in a few-shot in-context setting to estimate P_correct.",
            "problem_format": "4-shot exemplars (questions/answers) plus target question; exemplars are selected from same relation to ensure semantic similarity; repeated across N_ex = 10 distinct 4-shot combinations to capture exemplar variability.",
            "comparison_format": "Ablation on N_ex (fewer than 10 exemplars sets) showing degraded P_correct estimation quality; also contrasted with greedy-only and sampled decoding.",
            "performance": "Used as the main protocol to estimate P_correct; reliability improved with N_ex = 10; using fewer exemplar-sets produced higher post-finetuning accuracy on SliCK-identified Unknowns (worse identification).",
            "performance_comparison": null,
            "format_effect_size": "Qualitative: N_ex &lt; 10 increases misclassification of Unknown examples (paper shows an upward trend in test accuracy on SliCK-Unknown when using fewer exemplar sets), exact numeric delta not tabulated.",
            "format_effect_direction": "improved estimation fidelity with more exemplar sets (N_ex=10)",
            "explanation_or_hypothesis": "Choice of exemplars influences predictions; averaging over multiple random exemplar sets reduces variance and yields a more accurate assessment of whether the model truly knows the fact.",
            "counterexample_or_null_result": null,
            "uuid": "e5866.1",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Greedy vs Temperature decoding",
            "name_full": "Greedy decoding (T=0) versus temperature sampling (T=0.5, 16 samples)",
            "brief_description": "Decoding mode used both for SliCK categorization (greedy outcome used to separate HighlyKnown/MaybeKnown) and for detecting latent knowledge via sampling (WeaklyKnown identified when sampled outputs sometimes match).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "Determine knowledge strength by comparing deterministic greedy outputs to stochastic sampled outputs for the same few-shot prompt.",
            "problem_format": "Greedy decoding (T=0) provides one deterministic prediction per prompt; temperature sampling (T=0.5) yields 16 samples per prompt; categorizations rely on whether greedy or sampled outputs match ground truth across N_ex prompts.",
            "comparison_format": null,
            "performance": "Operational definitions: HighlyKnown P_correct(T=0)=1, MaybeKnown P_correct(T=0) in (0,1), WeaklyKnown P_correct(T=0)=0 & P_correct(T&gt;0)&gt;0, Unknown P_correct(T&gt;=0)=0. WeaklyKnown test accuracies sit between MaybeKnown and Unknown as expected.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "used to reveal latent vs stable knowledge (sampling reveals knowledge not surfaced by greedy decoding)",
            "explanation_or_hypothesis": "Greedy decoding shows the model's dominant (most likely) answer; sampling reveals lower-probability but present associations, allowing a finer-grained knowledge taxonomy which affects which fine-tuning examples are considered 'known' or 'unknown'.",
            "counterexample_or_null_result": null,
            "uuid": "e5866.2",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Unknown-ratio fine-tuning",
            "name_full": "Varying proportion (%Unknown) of Unknown examples in fine-tuning dataset D",
            "brief_description": "Controlled experiment that constructs fine-tuning datasets with fixed size but different percentages of examples that are Unknown to the base model (per SliCK), to measure how introducing new factual knowledge affects downstream closed-book QA performance and hallucinations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "Fine-tune PaLM 2-S on datasets D with X% Unknown examples (sampled per-relation to preserve relation distribution), evaluate on held-out test/dev (in-distribution) and OOD relations.",
            "problem_format": "Supervised fine-tuning on QA pairs (one-line instruction-style prompt shown in paper). Fixed |D| = 6142 examples; vary %Unknown (0% to 100%). Evaluate under two regimes: EARLY_STOP (epoch with best dev accuracy, typically 5–10 epochs) and CONVERGENCE (50 epochs, full training to fit D).",
            "comparison_format": "EARLY_STOP vs CONVERGENCE; also compared D vs D_Known (filtering Unknown examples).",
            "performance": "Higher %Unknown causes systematic performance degradation: e.g., for 50% Unknown, D EARLY_STOP test accuracy 43.0% vs CONVERGENCE 38.8% (Table 3). Overall in-distribution performance drop up to ~14 percentage points between best and worst configurations; OOD drop up to ~6 points (paper I).",
            "performance_comparison": "Filtering Unknown (D_Known) reduces degradation: for EARLY_STOP D and D_Known nearly identical, while at CONVERGENCE D underperforms D_Known by a margin proportional to %Unknown. Linear model coefficients (in-distribution): beta0=36.9, beta_kn=+7.3, beta_unk=-8.3 (R^2=0.86); OOD: beta0=36.2, beta_kn=+3.2, beta_unk=-3.0 (R^2=0.95).",
            "format_effect_size": "Quantitative: fitting one Known example contributes ~+7.3 accuracy units (normalized as in paper) while fitting one Unknown contributes ~-8.3 (in the authors' linear regression units); raw test accuracy deltas up to ~14 percentage points in-distribution and ~6 points OOD across %Unknown sweeps.",
            "format_effect_direction": "reduced (increased %Unknown reduces test accuracy; introducing Unknown examples increases hallucination risk)",
            "explanation_or_hypothesis": "Fine-tuning on examples that introduce new factual knowledge (Unknown) is learned slowly and, once learned, causes the model to adopt a behavior of generating answers not grounded in its pre-existing knowledge, effectively increasing hallucinations; Unknown examples primarily cause overfitting in later training stages.",
            "counterexample_or_null_result": "At EARLY_STOP (when model has fit most Known but few Unknown), removing Unknown examples had neutral effect — Unknown examples had little effect early in training; negative effects largely manifest after further training (overfitting).",
            "uuid": "e5866.3",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Filter Unknown (D_Known)",
            "name_full": "Filtering out Unknown examples from fine-tuning dataset (D_Known)",
            "brief_description": "Ablation that removes SliCK-identified Unknown examples from D, retaining only Known examples for fine-tuning, to test whether filtering reduces hallucination and overfitting without harming performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "Fine-tune on D_Known (only Known examples) with same |D_Known| = (1 - %Unknown) * |D|; evaluate EARLY_STOP vs CONVERGENCE.",
            "problem_format": "Same supervised QA fine-tuning but with Unknown examples removed prior to training.",
            "comparison_format": "Compared to full D (includes Unknown).",
            "performance": "For EARLY_STOP, performance of D and D_Known nearly identical; for CONVERGENCE, D_Known outperforms D, and the gap scales with %Unknown (see Figure 3b). Removing Unknown reduces overfitting and preserves or slightly improves test accuracy at longer training.",
            "performance_comparison": null,
            "format_effect_size": "Qualitative/relative: removal of Unknown examples substantially reduces the CONVERGENCE-era performance drop caused by overfitting on Unknowns (gap proportional to %Unknown; authors report up to ~14 point in-distribution drops when Unknown are present and fitted).",
            "format_effect_direction": "improved (filtering reduces harmful overfitting caused by Unknowns)",
            "explanation_or_hypothesis": "Unknown examples are learned late and promote overfitting/hallucination; removing them avoids training the model into the behavior of producing unsupported facts while preserving benefits from Known examples.",
            "counterexample_or_null_result": "Filtering may remove examples that could teach the model to express uncertainty on unknown test items; thus filtering sacrifices the ability to train abstention unless compensated (see IDK relabeling).",
            "uuid": "e5866.4",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Abstention fine-tuning (D_IDK)",
            "name_full": "Fine-tuning Unknown examples with label 'I don't know' (D_IDK)",
            "brief_description": "Replace labels of SliCK-identified Unknown training examples with an uncertainty expression 'I don't know' to teach the model to abstain rather than produce (potentially hallucinatory) facts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "Modify fine-tuning labels for Unknown examples to 'I don't know' and measure how often the fine-tuned model answers and the accuracy on those answered questions.",
            "problem_format": "Same supervised fine-tuning but Unknown examples' target answers replaced by the string 'I don't know' (D_IDK).",
            "comparison_format": "Compared to standard fine-tuning on D (Unknown labeled with correct ground truth answers).",
            "performance": "Table 3: For D (50% Unknown) EARLY_STOP accuracy 43.0% (% answered 100.0); CONVERGENCE 38.8% (% answered 100.0). For D_IDK: answered-subset accuracy 61.8% at EARLY_STOP (% answered 58.7) and 61.8% at CONVERGENCE (% answered 55.6). Thus D_IDK increases accuracy on willingly-answered questions and mitigates the CONVERGENCE-era drop.",
            "performance_comparison": "D_IDK answered-subset accuracy (61.8%) vs D overall answered accuracy (43.0% at EARLY_STOP) — absolute improvement of ~18.8 percentage points on answered items; also D_IDK removes the CONVERGENCE-era drop (61.8% stays constant).",
            "format_effect_size": "+18.8 pp accuracy on answered subset compared to D EARLY_STOP (for 50% Unknown experimental case); reduces overfitting-related accuracy drop observed when training to convergence.",
            "format_effect_direction": "improved (teaching abstention increased accuracy on answered items and mitigated overfitting)",
            "explanation_or_hypothesis": "Training with 'I don't know' teaches the model to abstain on examples where it lacks knowledge, preventing adoption of the behavior of producing unsupported facts and thereby reducing hallucinations; however it reduces the proportion of questions the model attempts to answer.",
            "counterexample_or_null_result": null,
            "uuid": "e5866.5",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Early-stopping vs Convergence",
            "name_full": "Early-stopping on dev set (EARLY_STOP) vs training to convergence (50 epochs)",
            "brief_description": "Comparison of stopping criteria showing that early-stopping at the dev-set peak often yields the best test performance and avoids overfitting on Unknown examples which are learned later.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "Fine-tune and evaluate models across epochs; identify EARLY_STOP epoch (best dev accuracy) and compare to CONVERGENCE (50 epochs).",
            "problem_format": "Same fine-tuning inputs; stopping criterion differs (dev-based early stopping vs fixed long training).",
            "comparison_format": "EARLY_STOP vs CONVERGENCE results presented across %Unknown sweeps and single-category datasets.",
            "performance": "EARLY_STOP typically yields highest dev/test performance; example: for 50% Unknown, EARLY_STOP test accuracy 43.0% vs CONVERGENCE 38.8% (Table 3). For D_Known variants, the EARLY_STOP vs CONVERGENCE gap is small, indicating Unknown examples drive late-stage degradation.",
            "performance_comparison": null,
            "format_effect_size": "Up to several percentage points (example: ~4.2 pp in the 50% Unknown case) improvement by using EARLY_STOP vs CONVERGENCE; magnitude grows with %Unknown.",
            "format_effect_direction": "improved (EARLY_STOP reduces overfitting and yields better test accuracy)",
            "explanation_or_hypothesis": "Unknown examples are fitted slowly and cause later-stage overfitting to behavior that increases hallucinations; early stopping prevents the model from fitting many Unknowns and therefore preserves better utilization of pre-existing knowledge.",
            "counterexample_or_null_result": "Early-stopping may be less effective when fine-tuning across many diverse tasks with different optimal stopping points; selecting a single early stop for multi-task mixtures is non-trivial.",
            "uuid": "e5866.6",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Relation-matched exemplars",
            "name_full": "Using few-shot exemplars sampled from the same relation (semantic similarity)",
            "brief_description": "Selecting few-shot exemplars from the same QA relation as the target question to reduce exemplar-distribution mismatch and improve answer-format consistency in prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "When building the few-shot prompt, choose exemplars whose question template matches the target question template (same Wikidata relation).",
            "problem_format": "4-shot prompts where each exemplar follows the same question template as the target (e.g., 'Where is [E] located?').",
            "comparison_format": "Implicitly compared to arbitrary exemplar selection (not explicitly tabulated), but used as part of SliCK protocol to stabilize P_correct estimation.",
            "performance": "Not reported as a single scalar; used to ensure the model outputs are in the correct format and to reduce variance across exemplars. Authors state this approach improved prompt effectiveness for P_correct estimation.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (makes few-shot prompting more reliable for P_correct estimation)",
            "explanation_or_hypothesis": "Semantic similarity between exemplars and target question reduces style/format mismatch and the resulting variance in model outputs across exemplar choices.",
            "counterexample_or_null_result": null,
            "uuid": "e5866.7",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "P(True) comparison",
            "name_full": "P(True) method (Kadavath et al., 2022) used for Unknown identification comparison",
            "brief_description": "A single-prompt approach that elicits the model's probability that a proposed answer is True vs False; used here as a baseline to compare SliCK's Unknown identification.",
            "citation_title": "Language models (mostly) know what they know",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S",
            "model_size": null,
            "task_name": "Closed-book QA (EntityQuestions)",
            "task_description": "Compute P(True) for each (q,a) by prompting the model to choose True/False for the proposed answer and use thresholding to classify Unknown vs Known.",
            "problem_format": "Single direct prompt that asks 'Is the proposed answer True or False?' and measures the model's probability for True; applied to the ground-truth answer a.",
            "comparison_format": "Compared across thresholds T in [0,1] to SliCK's Unknown categorization; plotted fraction labeled Unknown vs post-finetuning accuracy on those examples.",
            "performance": "P(True)-based Unknown sets (for the same fraction of test examples as SliCK) have substantially higher accuracy after fine-tuning versus SliCK's Unknown, indicating P(True) is less selective for the particularly harmful Unknowns in this setup (see Figure 5).",
            "performance_comparison": null,
            "format_effect_size": "Qualitative: for fixed % of examples labeled Unknown, P(True)-selected Unknowns produce higher post-finetune accuracy than SliCK-selected Unknowns (i.e., SliCK selects more problematic items); exact numeric deltas across thresholds shown in Fig.5 but not summarized as a single scalar.",
            "format_effect_direction": "SliCK outperforms P(True) in identifying examples that will cause larger post-finetuning degradation",
            "explanation_or_hypothesis": "P(True) uses a single prompt and therefore underestimates exemplar and sampling variance; SliCK's multiple exemplar + sampling procedure better captures whether the model can actually produce the ground-truth answer under realistic prompting variability.",
            "counterexample_or_null_result": null,
            "uuid": "e5866.8",
            "source_info": {
                "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 2
        },
        {
            "paper_title": "R-tuning: Teaching large language models to refuse unknown questions",
            "rating": 2
        },
        {
            "paper_title": "Unfamiliar finetuning examples control how language models hallucinate",
            "rating": 2
        },
        {
            "paper_title": "Learning or self-aligning? rethinking instruction fine-tuning",
            "rating": 2
        },
        {
            "paper_title": "Understanding finetuning for factual knowledge extraction",
            "rating": 1
        }
    ],
    "cost": 0.0223295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</h1>
<p>Zorik Gekhman ${ }^{T *}$ Gal Yona ${ }^{G}$ Roee Aharoni ${ }^{G}$ Matan Eyal ${ }^{G}$ Amir Feder ${ }^{G}$<br>Roi Reichart ${ }^{T}$ Jonathan Herzig ${ }^{G}$<br>${ }^{T}$ Technion - Israel Institute of Technology ${ }^{G}$ Google Research<br>zorikgekhman@gmail.com, jherzig@google.com</p>
<h4>Abstract</h4>
<p>When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closedbook QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas finetuning teaches them to use it more efficiently.</p>
<h2>1 Introduction</h2>
<p>Pre-training Large Language Models (LLMs) on textual corpora embeds substantial factual knowledge in their parameters (Petroni et al., 2019; AlKhamissi et al., 2022; Cohen et al., 2023), which is essential for excelling in various downstream applications. These models often require further alignment to desired behaviors, typically achieved through supervised fine-tuning on instructionfollowing tasks (Wei et al., 2022; Mishra et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Train and development accuracies as a function of the fine-tuning duration, when fine-tuning on $50 \%$ Known and $50 \%$ Unknown examples. Unknown examples are fitted substantially slower than Known. The best development performance is obtained when the LLM fits the majority of the Known training examples but only few of the Unknown ones. From this point, fitting Unknown examples reduces the performance.
2022) and preference learning from human feedback (Ouyang et al., 2022; Rafailov et al., 2024).</p>
<p>In the fine-tuning phase, the model is usually trained on outputs created by human annotators or other LLMs. As a result, the model may encounter new factual information, extending beyond the knowledge it acquired during pre-training. This raises the question of how LLMs integrate new facts outside of their pre-existing knowledge. One possibility is that the model simply adapts by learning this new factual information. However, a common conjecture posits that such exposure to new knowledge may encourage the model to hallucinate factually incorrect responses, as the model is essentially trained to generate facts that are not grounded in its pre-existing knowledge (Schulman, 2023; Huang et al., 2023; Gao, 2021; Goldberg,</p>
<p>2023; Gudibande et al., 2023).
In this work, we study how learning new factual knowledge through fine-tuning impacts the model's tendency to hallucinate w.r.t. its pre-existing knowledge, exploring the above conjecture. ${ }^{1}$</p>
<p>To study the impact of new knowledge, we must be able to assess whether a single fine-tuning example is consistent with the model's knowledge. We propose SliCK, a hierarchy of four knowledge categories, derived from a continuous measure that quantifies the agreement between modelgenerated answers and the ground-truth labels. In SliCK, examples are first categorized into Known and Unknown types, where the latter corresponds to examples with facts that are most likely unknown to the model. The Known examples are subsequently split into three categories: HighlyKnown, MaybeKnown, and WeaklyKnown (Figure 2).</p>
<p>Equipped with the above method, we carefully design a controlled study, focused on closed-book question answering (QA), where we vary the proportion of the fine-tuning examples categorized as Unknown, while controlling for other factors.</p>
<p>Our study empirically demonstrates that learning from Unknown fine-tuning examples is linearly correlated with the model's tendency to hallucinate w.r.t. its pre-existing knowledge (§4). Conversely, learning from Known examples is correlated with better utilization of pre-existing knowledge.</p>
<p>Through an analysis of the training dynamics, we discover that the LLM fits Unknown fine-tuning examples substantially slower than Known examples (top plot in Figure 1). This indicates that during fine-tuning, LLMs struggle ${ }^{2}$ to integrate new factual knowledge (present in the Unknown finetuning examples). Instead, they mostly learn to expose their pre-existing knowledge (using the Known fine-tuning examples).</p>
<p>From a practical perspective, mitigating overfitting using early-stopping (vertical dotted line in Figure 1) can minimize the risk of the hallucinations caused by fitting the Unknown examples, since they primarily emerge in later training stages as a form of overfitting (as illustrated by the development performance decline in the bottom plot of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Figure 1). Alternatively, we also show that filteringout the Unknown fine-tuning examples substantially reduces the risk of overfitting, without sacrificing performance.</p>
<p>We further evaluate the impact of fine-tuning examples from each of our three Known knowledge categories on performance (§5). Unexpectedly, we find that a model fine-tuned only on examples from the highest knowledge degree, denoted HighlyKnown, does not yield the best results. Our analysis reveals that incorporating MaybeKnown fine-tuning examples, representing facts with lower degrees of certainty, plays an important part in properly handling such examples in test time. This indicates that the composition of fine-tuning examples significantly influences the extent to which LLMs effectively utilize their pre-existing knowledge.</p>
<p>To summarize, we study the effect of new factual knowledge in the fine-tuning data by designing a controlled setup that isolates this factor. We find that fine-tuning examples that introduce new knowledge are learned slowly, which suggests that LLMs struggle to integrate new knowledge through finetuning and supports the view that LLMs mostly acquire knowledge through pre-training (Zhou et al., 2023; Lin et al., 2023). However, we also find that as the model eventually learns new knowledge through fine-tuning, it becomes more prone to hallucinations w.r.t. its pre-existing knowledge. Collectively, our findings highlight the potential for unintended consequences when introducing new knowledge through fine-tuning, and imply that finetuning may be more useful as a mechanism to enhance the utilization of pre-existing knowledge.</p>
<h2>2 Study Setup</h2>
<p>Given a fine-tuning dataset $D$ and a pre-trained LLM $M$, we denote by $M_{D}$ a model obtained by fine-tuning $M$ on $D$. To study how new knowledge in $D$ affects $M_{D}$ 's performance, we design a controlled setup creating variants of $D$ with varying proportions of examples that are unknown to $M$.</p>
<p>When constructing $D$, our objective is to reflect instruction tuning on diverse knowledge-intensive tasks while maintaining control over the experimental setting. We thus focus on factual knowledge that can be structured as (subject, relation, object) triplets, which are converted into closed-book QA format. In this setup, $D=\left{\left(q_{i}, a_{i}\right)\right}_{i=1}^{N}$, where $q$ is a knowledge-seeking question corresponding to a specific triplet (e.g., "Where is Paris located?")</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Category</th>
<th>Definition</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Known</td>
<td>HighlyKnown</td>
<td>$P_{\text {Correct }}(q, a ; M, T=0)=1$</td>
<td>Greedy decoding always predicts the correct answer.</td>
</tr>
<tr>
<td></td>
<td>MaybeKnown</td>
<td>$P_{\text {Correct }}(q, a ; M, T=0) \in(0,1)$</td>
<td>Greedy decoding sometimes (but not always) predicts the correct answer.</td>
</tr>
<tr>
<td></td>
<td>WeaklyKnown</td>
<td>$P_{\text {Correct }}(q, a ; M, T=0)=0 \wedge$ <br> $P_{\text {Correct }}(q, a ; M, T&gt;0)&gt;0$</td>
<td>Greedy decoding never predicts the correct answer, whereas temperature <br> sampling with $T&gt;0$ sometimes predicts the correct answer.</td>
</tr>
<tr>
<td>Unknown</td>
<td>Unknown</td>
<td>$P_{\text {Correct }}(q, a ; M, T \geq 0)=0$</td>
<td>The model never predicts the correct answer, thus it seem to lack the <br> knowledge of the correct answer.</td>
</tr>
</tbody>
</table>
<p>(a)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Gold Answer</th>
<th style="text-align: center;">Greedy Answers</th>
<th style="text-align: center;">Sampled Answers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">HighlyKnown</td>
<td style="text-align: center;">Who founded Science of Mind?</td>
<td style="text-align: center;">Ernest Holmes</td>
<td style="text-align: center;">Ernest Holmes, Ernest Holmes, $\ldots$</td>
<td style="text-align: center;">$[.,, \ldots]$</td>
</tr>
<tr>
<td style="text-align: center;">MaybeKnown</td>
<td style="text-align: center;">What is the capital of Toledo District?</td>
<td style="text-align: center;">Panta Gonda</td>
<td style="text-align: center;">[Belmopan, ... Panta Gonda, ..]</td>
<td style="text-align: center;">$[., \ldots]$</td>
</tr>
<tr>
<td style="text-align: center;">WeaklyKnown</td>
<td style="text-align: center;">What kind of work does Scott McGrew do?</td>
<td style="text-align: center;">Journalist</td>
<td style="text-align: center;">[Film director, .. Actor, ..]</td>
<td style="text-align: center;">[Musician, .. Journalist, ..]</td>
</tr>
<tr>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">Where is Benedict located?</td>
<td style="text-align: center;">Hubbard County</td>
<td style="text-align: center;">[Louisiana, .. New Mexico, ..]</td>
<td style="text-align: center;">[Washington, .. Texas, ..]</td>
</tr>
</tbody>
</table>
<p>(b)</p>
<p>Figure 2: Formal definitions of the SliCK knowledge categories, based on the $P_{\text {Correct }}$ measure as defined in $\S 3$ (a), accompanied with real examples from the annotated EntityQuestions dataset used in our study (b).
and $a$ is the ground-truth answer (e.g., "France"). To this end, we use EntityQuestions (Sciavolino et al., 2021), where triplets from a diverse set of relations from Wikidata (Vrandečić and Krötzsch, 2014) are converted to QA pairs. These relations encompass a broad spectrum of factual knowledge, including biographical information, geographical data, ownership and authorship details, history and more. We use the original development and test splits, and we sub-sample the train split to create different variants of $D$. We focus on 12 diverse relations and reserve 7 additional relations for an out-of-distribution test set, used (only) in $\S 4.5$.</p>
<p>As $M$, we use the PaLM 2-S base model ${ }^{3}$ (Anil et al., 2023). We focus on exact match (EM) as our evaluation metric. ${ }^{4}$ Full technical details are in §A.</p>
<h2>3 Quantifying Knowledge in LLMs</h2>
<p>To assess the effect of new knowledge in $D$ on the performance of $M_{D}$, we have to annotate each $(q, a)$ pair in $D$ w.r.t. whether $M$ knows that the answer to $q$ is $a .{ }^{5}$ To estimate this, we define a continuous $P_{\text {Correct }}$ measure based on samples from $M$, and use it to divide $(q, a)$ pairs into four knowledge categories. We name this approach SliCK (Sampling-based Categorization of Knowledge).</p>
<p>Defining $\boldsymbol{P}_{\text {Correct. }}$ We adopt the perspective that $M$ knows that the answer to $q$ is $a$ if it generates $a$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>when prompted to answer $q$ (Kadavath et al., 2022; Manakul et al., 2023). Since $M$ is a base model that has not been specifically fine-tuned to follow instructions, we prompt $M$ using in-context learning with few-shot exemplars. Following Rubin et al. (2022), we make sure that the few-shot exemplars have high semantic similarity to $q .{ }^{6}$</p>
<p>In practice, $M$ can predict different answers since (1) the choice of exemplars influences individual predictions and (2) temperature sampling, if used, introduces randomness. To reflect this, we define $P_{\text {Correct }}(q, a ; M, T)$ as an estimate of how likely is $M$ to accurately generate the correct answer $a$ to $q$, when prompted with random few-shot exemplars and using decoding temperature $T$.</p>
<p>For the purposes of our study we approximate the value of $P_{\text {Correct }}$ using $N_{\text {ex }}=10$ different random 4 -shot prompts. ${ }^{7}$ For each 4 -shot prompt, we predict the greedy answer using $T=0$ and 16 sampled answers using $T=0.5 . P_{\text {Correct }}(q, a ; M, T=0)$ is estimated by the fraction of correct greedy answers, and $P_{\text {Correct }}(q, a ; M, T&gt;0)$ by the fraction of correct sampled answers. Full details are in $\S \mathrm{C}$.</p>
<p>Deriving knowledge categories from $\boldsymbol{P}<em _Correct="{Correct" _text="\text">{\text {Correct }}$. We define the Unknown category (bottom row in Figures 2a and 2b) to represent $(q, a)$ pairs for which $M$ never predicts the correct answer to $q$. In our notations this means that $P</em>(q, a ; M, T \geq 0)&gt;0$, i.e. $M$ sometimes predicts the correct answer to $q$, we consider $(q, a)$}}(q, a ; M, T \geq 0)=0$. Alternatively, if $P_{\text {Correct }</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Test performance as a function of the % of Unknown examples in the fine-tuning dataset <em>D</em>. In (<strong>a</strong>), each line corresponds to a different (fixed) number of epochs, except the EARLY_STOP, which corresponds to early-stopping using the development set (see §4). In (<strong>b</strong>) we present the ablation from §4.2. Full lines correspond to fine-tuning on <em>D</em> and are identical to (a). Dotted lines correspond to fine-tuning on the ablated variants <em>D</em>Known, where Unknown examples are filtered-out. For 0% Unknown <em>D</em> = <em>D</em>Known and for 100% Unknown there is no <em>D</em>Known.</p>
<p>as Known. In this choice, we posit that if prompting <em>M</em> to answer <em>q</em> can <em>sometimes</em> result with the correct answer <em>a</em>, then <em>M</em> must have some association with the relevant fact.</p>
<p>Recognizing that knowledge can vary in degrees of certainty and extent, we divide the Known (<em>q, a</em>) pairs into three distinct categories (top three rows in Tables 2a and 2b). Motivated by the principle that <em>M</em> should <em>consistently</em> predict <em>a</em> if (<em>q, a</em>) is Known, we put emphasis on <em>greedy decoding</em> outcomes, represented with <em>P</em>Correct (<em>q, a</em>; <em>M, T</em> = 0). HighlyKnown represents (<em>q, a</em>) pairs for which <em>M always</em> greedily predicts <em>a</em>. If <em>M sometimes</em> (but not always) greedily predicts <em>a</em>, we consider (<em>q, a</em>) as MaybeKnown. Lastly, if <em>M never</em> greedily predicts <em>a</em>, we classify (<em>q, a</em>) as WeaklyKnown.</p>
<p>We apply <strong>SliCK</strong> to annotate each (<em>q, a</em>) pair in our dataset with its knowledge category w.r.t. <em>M</em>.<sup>8</sup> We analyze the quality of our categories in §6.</p>
<h2>4 How Harmful are Unknown Examples?</h2>
<p>In this section we study the effect of new knowledge in the fine-tuning dataset <em>D</em> on performance. To isolate this effect, we vary the proportion of Unknown examples in <em>D</em>, while controlling for other factors. Specifically, we fix |<em>D</em>| and create variants of <em>D</em> with <em>X%</em> of Unknown and (100 − <em>X</em>)% Known examples (full details in §E). We treat the Known categories collectively (see Figure 2a), and provide a per-category analysis in §5. We de-</p>
<p>note early-stopping based on the development set as EARLY_STOP (happens after 5-10 epochs) and 50 fine-tuning epochs as CONVERGENCE, as at this point <em>M</em> always completely fits <em>D</em> (i.e. 100% training accuracy). We measure test performance as a proxy for hallucinations since we are in a closed-book QA setup with disjoint train/test splits, where the model has to use its per-existing knowledge to answer test questions (see §B for further discussion).</p>
<h3>4.1 Higher Unknown Ratio is Proportional to Performance Degradation</h3>
<p>Figure 3a presents the performance as a function of the % of Unknown examples in <em>D</em>, for different fine-tuning durations. Higher %Unknown leads to performance degradation, regardless of the fine-tuning duration, which indicates that Unknown examples are less useful than Known. Performance is also strongly affected by the fine-tuning duration, with EARLY_STOP typically yielding the best performance. Training for more epochs usually reduces performance (with the lowest performance observed for CONVERGENCE), which can be attributed to overfitting <em>D</em>. Interestingly, this effect increases with larger %Unknown (the inter-line spacing from EARLY_STOP exhibits a monotonic increase along the positive x-axis), suggesting that a higher %Unknown increases the risk of overfitting.</p>
<h3>4.2 Unknown Examples: Harmful or Neutral?</h3>
<p>Since |<em>D</em>| is fixed, performance drops for higher %Unknown could stem from simply the lower number of the Known fine-tuning examples. Thus, it is</p>
<p><sup>8</sup> In ENTITYQUESTIONS we have 24% HighlyKnown, 23% MaybeKnown, 17%, WeaklyKnown, and 36% Unknown. Full per-relation statistics are in §D.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The state of the examples in the fine-tuning dataset $D$ after early_stop. For each variant of $D$ (yaxis), we illustrate which portion of the examples in $D$ the model fits (i.e. predicts the correct answer for $q$ ).
still not clear if Unknown examples are harmful or neutral. To address this, we measure the effect of filtering-out all the Unknown examples from $D$. For each $D$ variant, we create a corresponding ablated variant $D_{\text {Known }}$, consisting only from the Known examples in $D$. E.g., if $D$ has $25 \%$ Unknown, we filter them out and are left with the remaining $75 \%$ Known examples and get $\left|D_{\text {Known }}\right|=0.75 \times|D|$.</p>
<p>Figure $3 b$ presents the results. Perhaps surprisingly, for early_stop the results for $D$ are almost identical to $D_{\text {Known }}$, indicating that the Unknown examples had a neutral effect on performance (as their removal had minimal impact). Conversely, the CONVERgence results show that with longer training, Unknown examples are actually very harmful. In this case $D$ under-performs $D_{\text {Known }}$, and the gap between them is proportional to the Unknown ratio.</p>
<p>Interestingly, for $D_{\text {Known }}$, the gap between early_stop and Convergence is very small (dotted lines), while this gap is very large for $D$ (full lines). This indicates that the presence of Unknown examples is what makes the variants with higher Unknown ratios more prone to overfitting.</p>
<h3>4.3 Unknown Examples are Fitted Slower than Known Examples</h3>
<p>We showed that Unknown examples are harmful, but their negative effect is mostly materialized in later training stages, and thus can be empirically avoided using early stopping. To better understand these trends, we analyze the training dynamics by examining which fine-tuning examples in $D$ were fitted by $M$ during various fine-tuning stages. Figure 1 presents the train accuracy of the Known and Unknown subsets of $D$ as a function of the finetuning duration. The development accuracy is presented in a zoomed-in plot at the bottom, as it falls within a narrower range. We include a breakdown</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\beta_{0}$</th>
<th style="text-align: center;">$\beta_{\text {kn }}$</th>
<th style="text-align: center;">$\beta_{\text {unk }}$</th>
<th style="text-align: center;">$R^{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">In-distribution (§4.4)</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">-8.3</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: left;">Out-of-distribution (§4.5)</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">-3.0</td>
<td style="text-align: center;">0.95</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of our linear model for predicting the test accuracy as defined by Equation (1).
of the train accuracy per Known category in §G.
$M$ fits Unknown fine-tuning examples substantially slower than Known. In early_stop (vertical dotted line), $M$ reaches peak performance on the development set, while fitting the majority of the Known examples but only a small fraction of the Unknown. In Figure 4, we show that this behavior is consistent across all our variants of $D$. This can explain why in early_stop the Unknown examples had a neutral effect on performance (§4.2), as at this point $M$ still did not fit most of them. Lastly, since Unknown examples are the ones that are likely to introduce new factual knowledge, their significantly slow fitting rate suggests that LLMs struggle to acquire new factual knowledge through fine-tuning, instead they learn to expose their preexisting knowledge using the Known examples.</p>
<h3>4.4 The Influence of Unknown vs Known on Accuracy: A Linear Model Perspective</h3>
<p>Figure 1 demonstrates that after the development performance peaks at early_stop (vertical dotted line), it deteriorates as $M$ gradually fits more Unknown examples. In this section, we aim to characterize this relationship more accurately by assessing whether a simple linear dependency can tie the impact of fitting Known and Unknown training examples on test accuracy. To this end we use the following linear regression model:</p>
<p>$$
\text { Accuracy }=\beta_{0}+\beta_{\mathrm{kn}} \cdot \frac{N_{\mathrm{kn}}}{|D|}+\beta_{\mathrm{unk}} \cdot \frac{N_{\mathrm{unk}}}{|D|}
$$</p>
<p>where $N_{\mathrm{Kn}}$ and $N_{\mathrm{Unk}}$ are the number of the Known and Unknown examples in $D$ that $M$ fits.</p>
<p>We estimate the coefficients ${ }^{9}$ by collecting (Accuracy, $N_{\mathrm{Kn}}, N_{\mathrm{Unk}}$ ) values after each epoch from models fine-tuned on all $D$ variants. Table 1 presents the results (top row). The high $R^{2}$ indicates a strong linear relationship between test accuracy and the type of training examples that are fitted. Our model entails that fitting Unknown examples hurts performance $\left(\beta_{\text {unk }}&lt;0\right)$, while fitting Known</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">EARLY_STOP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CONVERGENCE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Hkn</td>
<td style="text-align: center;">Mkn</td>
<td style="text-align: center;">Wkn</td>
<td style="text-align: center;">Unk</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Hkn</td>
<td style="text-align: center;">Mkn</td>
<td style="text-align: center;">Wkn</td>
<td style="text-align: center;">Unk</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {HighlyKnown }}$</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {MaybeKnown }}$</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {WeaklyKnown }}$</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {Unknown }}$</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {Natural }}$</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">2.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracies for the single-category variants from $\S 5$, across per-category subsets of the test set. Full is the original test set (all the categories together). Hkn=HighlyKnown, Mkn=MaybeKnown, Wkn=WeaklyKnown, Unk=Unknown. In each column, the best result is in bold, as well as the results for which the difference from the best is not statistically significant with $p&lt;0.05$ (significance test details are in §J).
examples improves it $\left(\beta_{\mathrm{kn}}&gt;0\right)$. The estimated negative impact from Unknown roughly matches the positive impact from Known $\left(\left|\beta_{\text {ukn }}\right| \approx\left|\beta_{\mathrm{kn}}\right|\right)$.</p>
<h3>4.5 Generalization to New Relations</h3>
<p>In the above setup, the $(q, a)$ pairs in the test set correspond to triplets with the same set of 12 relations appearing in $D$. We now investigate whether our observed dynamics has a broader effect on the model's knowledge, and transfers to relations not represented in $D$. To test this, we reserve a subset of the relations for an out-of-distribution (OOD) test set, excluding them from the train and development splits. See §A for details and Tables 5 and 6 for in-distribution vs OOD relations.</p>
<p>Our results on the OOD test set reveal similar key insights: (1) Higher Unknown ratio leads to lower OOD test performance and (2) Unknown examples are harmful for OOD performance, but mostly when $M$ fits them. A linear model of the OOD test accuracy (Equation (1)), shows similar trends: $\beta_{\text {unk }}&lt;0, \beta_{\mathrm{kn}}&gt;0,\left|\beta_{\text {ukn }}\right| \approx\left|\beta_{\mathrm{kn}}\right|$ and $R^{2}=0.95$ (see Table 1). More details are in §I.</p>
<p>Overall, our insights transfer across relations. This essentially shows that fine-tuning on Unknown examples such as "Where is [E1] located?", can encourage hallucinations on seemingly unrelated questions, such as "Who founded [E2]?". This further supports the conclusion that the observed effects likely stem from the model learning the behavior of generating answers that are not grounded in its pre-existing knowledge.</p>
<h2>5 Understanding Knowledge Types: Their Value and Impact</h2>
<p>When addressing our main research question on the effect of Unknown fine-tuning examples, we
treated the Known categories collectively for simplicity (see Figure 2a). We now examine the effect of each category, exploring the following questions: Q1: How training examples from each category impact the test performance? Q2: What is the model's performance across test examples from each category? To address Q1 we created single-category variants of the fine-tuning dataset $D$. A variant of $D$ consisting solely of examples from the category CAT is denoted as $D_{\text {CAT }}$. For reference, we include a variant with the natural categories distribution in EntityQuestions, denoted $D_{\text {Natural }}$. $|D|$ is fixed and identical to our experiments in $\S 4$. To address Q2, we further break down the test set performance by category. Table 2 presents the results.</p>
<p>MaybeKnown Examples are Essential. Since Unknown examples are harmful, one might expect that it would be best to fine-tune on the most exemplary HighlyKnown examples. Surprisingly, $D_{\text {HighlyKnown }}$ does not obtain the best overall results, as it excels on HighlyKnown test examples, yet its performance on the remaining categories is inferior. $D_{\text {MaybeKnown }}$ yields the best overall performance. Compared to $D_{\text {HighlyKnown }}, D_{\text {MaybeKnown }}$ enhances $M_{D}$ 's performance on MaybeKnown ( $60.1 \rightarrow 69.9$ ), without compromising performance on HighlyKnown ( $98.7 \rightarrow 98.4$ ). This suggests that MaybeKnown fine-tuning examples are essential for $M_{D}$ to correctly handle such examples during inference. It also demonstrates that with the right fine-tuning examples, $M_{D}$ becomes more capable of utilizing its pre-existing knowledge.</p>
<p>Limited Knowledge Enhances Overfitting. In $\S 4.2$, we demonstrated that Unknown fine-tuning examples increase the risk of overfitting. We now observe that this also applies to WeaklyKnown, though to a lesser degree. Specifically, at</p>
<p>Convergence, $D_{\text {WeaklyKnown }}$ and $D_{\text {Unknown }}$ experience significant performance drops compared to EARLY_STOP ( $39.2 \rightarrow 35.4$ and $37.5 \rightarrow 25.8$ ). With training to Convergence, they show a modest improvement on WeaklyKnown and Unknown but substantially degrade on HighlyKnown and MaybeKnown. This highlights that the decrease in performance is strongly attributed to an increased rate of hallucinations w.r.t. facts that were already known to $M$ after pre-training.</p>
<p>Interestingly, $D_{\text {Natural }}$ performs on-par with $D_{\text {MaybeKnown }}$ in EARLY_STOP, suggesting that the mere presence of MaybeKnown examples in $D$ suffices for high performance on MaybeKnown, even if $D$ has additional examples from other categories. Yet, $D_{\text {Natural }}$ 's performance degrades significantly ${ }^{10}$ after Convergence, under-performing $D_{\text {MaybeKnown }}$ - indicating that it still suffers from overfitting, most-likely due to the presence of WeaklyKnown and Unknown examples. Taken together these results demonstrate that $D_{\text {MaybeKnown }}$ stands out both in terms of top performance and reduced risk to overfitting.</p>
<h2>6 SliCK Knowledge Categories Analysis</h2>
<p>Assessing a model's knowledge remains an open problem, particularly since evaluating the quality of such methods is challenging due to the lack of ground truth about what the model truly knows. In this work we proposed SliCK (§3): a four-category classification of facts w.r.t. the model's knowledge. We now further analyze and discuss our design choices, hoping that SliCK can serve as a useful taxonomy to guide future research on this subject.</p>
<p>Fine-grained Known Categories We first reflect on whether our choice of splitting Known into more fine-grained categories, based on the greedy decoding outcome, has been proven meaningful. As shown in Table 2, HighlyKnown indeed captures facts with high degree of knowledge, as it consistently exceeds $95 \%$ accuracy post fine-tuning, while MaybeKnown and WeaklyKnown seem to represent weaker knowledge degrees. As intended, the performance on WeaklyKnown is worse that on MaybeKnown but better than on Unknown. Additionally, the exact categories distinction we made was proven useful since it revealed important insights on the importance of the MaybeKnown finetuning examples, as discussed in detail in $\S 5$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: SliCK Unknown categorization vs. classifying examples with $\mathrm{P}($ True $)&lt;T$ as Unknown. The xaxis is the $\%$ of test examples classified as Unknown and the y-axis is the accuracy on these examples post fine-tuning. The yellow line is $\mathrm{P}($ True $)$ for $T \in[0,1]$. Our Unknown category is the blue circle and the blue line corresponds to approximating $P_{\text {Correct }}$ with less than 10 random 4-shot exemplars (see $\S 3$ and $\S \mathrm{C}$ ).</p>
<p>Benchmarking Unknown Test Examples A desired property for $(q, a)$ pairs classified as Unknown that appear in the test set, is that $M$ will incorrectly answer $q$ post fine-tuning (otherwise they are not truly Unknown). ${ }^{11}$ In Table 2 we can see that the accuracy on Unknown is extremely low ( $3.2 \%$ or less), which is a strong indicator that most of the Unknown examples are actually unknown to $M$.</p>
<p>As a case study for comparison, we analyze the $\mathrm{P}($ True $)$ approach by Kadavath et al. (2022): a continuous score that estimates the probability a model assigns to the correctness of a specific answer. $\mathrm{P}($ True $)$ was originally used for self-evaluating model-generated answers, while we use it to assess whether $M$ considers the ground-truth answer as correct. ${ }^{12}$ In Figure 5, we explore classifying examples below a $\mathrm{P}($ True $)$ threshold as Unknown and compare this methodology to SliCK. Our results indicate that, at least in our setting, our approach categorizes Unknown examples for which the model's performance after fine-tuning is significantly worse. Specifically, looking at fixed values on the x-axis shows that if we would label a similar fraction of test examples as Unknown using both methods, the accuracy on the $\mathrm{P}($ True $)$-based Unknown examples would be much higher post fine-tuning. ${ }^{13}$ Lastly,</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>the blue line shows that using samples from multiple few-shot prompts to approximate $P_{\text {Correct }}$ is crucial, as using $N_{\mathrm{ex}}&lt;10$ leads to higher test accuracy on SliCK Unknown examples.</p>
<h2>7 Fine-tuning to Abstain on Unknown Examples</h2>
<p>We showed that fitting Unknown fine-tuning examples negatively affects the test performance (\$4.2 and $\S 4.4$ ). However, this negative effect manifests as a form of overfitting. From practical perspective, we showed that we can mitigate overfitting by either using early-stopping or filtering-out Unknown examples from the fine-tuning dataset.</p>
<p>We now explore an additional approach where we fine-tune the model to abstain from Unknown examples as a potential mitigation. Specifically, we replace the label of the Unknown fine-tuning examples with the expression "I don't know" and test whether this mitigates the observed overfitting.</p>
<p>Table 3 presents the $\%$ of the test questions that were answered (i.e. $M_{D}$ did not respond with "I don't know") and the accuracy on those questions. Consistent with the findings from previous work (Zhang et al., 2023), we observe an improved accuracy on willingly answered test examples (when comparing $D$ vs $D_{\text {IDK }}$ ). When we compare EARLY_STOP vs CONVERgence for $D$ we observe a performance drop $(43.0 \rightarrow 38.8)$ which illustrates the overfitting effect. However, we observe that re-labeling the Unknown examples with uncertainty expression seem to reduce the risk of overfitting. Specifically, the accuracy for $D_{\text {IDK }}$ remains 61.8 for both EARLY_STOP and CONVERGENCE, with a small decrease on the number of willingly answered questions $(58.7 \rightarrow 55.6)$.</p>
<h2>8 Discussion</h2>
<p>Practical Implications. This work highlights the risk in using supervised fine-tuning to update LLMs' knowledge, as we present empirical evidence that acquiring new knowledge through fine-tuning is correlated with hallucinations w.r.t pre-existing knowledge. Additionally, this work raises important questions for future exploration regarding fine-tuning practices. We saw that Unknown examples are fitted slower than the Known ones, thus their negative effect manifests as a form of overfitting, which emphasizes the importance of using early-stopping instead of a fixed number of fine-tuning steps. However, early-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">EARLY_STOP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CONVERGENCE</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">\% Answered</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">\% Answered</td>
</tr>
<tr>
<td style="text-align: center;">$D$</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {IDK }}$</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">55.6</td>
</tr>
</tbody>
</table>
<p>Table 3: Results where the label of the Unknown finetuning examples is replaced with "I don't know". $D$ in this case is the variant with $50 \%$ Known and $50 \%$ Unknown. $D_{\text {IDK }}$ is the variant where all the $50 \%$ Unknown fine-tuning examples were re-labeled with "I don't know". The accuracy is measured on the subset of the test questions that were answered, i.e. $M_{D}$ did not respond with "I don't know".
stopping may be less effective when fine-tuning on numerous tasks with distinct optimal stopping points. An alternative solution can be to avoid adding new knowledge, by aligning the fine-tuning data with the model's knowledge through filteringout Unknown examples. We show initial evidence that this can reduce the risk of overfitting without compromising performance. A possible drawback of filtering, is that Unknown fine-tuning examples can still be useful to teach LLMs to express uncertainty on Unknown test examples (Zhang et al., 2023; Yang et al., 2023). This raises the question: can re-labeling Unknown fine-tuning examples with uncertainty expressions (e.g., "I don't know") reduce their negative effect? Our experiment (§7) suggests that the answer is yes, which indicates that such approaches could be the most promising.</p>
<p>Superficial Alignment Hypothesis. Zhou et al. (2023) hypothesized that the knowledge and capabilities of LLMs are mostly learned during pretraining, while alignment is a simple process where the model learns the style or format for interacting with users. They substantiate this hypothesis by showing that fine-tuning on just 1 k high-quality examples can result with a competitive assistant LLM, named LIMA. As discussed in $\S 4.3$, we show evidence that LLMs struggle to acquire new knowledge present in the Unknown examples and mostly learn to utilize their pre-existing knowledge. We also showed that fine-tuning on HighlyKnown examples led to sub-optimal utilization of preexisting knowledge, despite our task format being simpler than LIMA's and our dataset being six times larger. Taken together, our findings suggest that even though most of the LLM's knowledge is indeed acquired through pre-training, the model learns more than just style or format through finetuning, as the selection of fine-tuning examples</p>
<p>significantly influences the model's capability to utilize its pre-existing knowledge post fine-tuning.</p>
<h2>9 Related Work</h2>
<p>New knowledge and hallucinations. Schulman (2023), Goldberg (2023) and Gudibande et al. (2023) mention the conjecture that fine-tuning on new factual knowledge may encourage hallucinations. Huang et al. (2023) categorized hallucination causes and formally defined this scenario as capability misalignment, also highlighting that limited research addresses capability misalignment due to the challenge of defining the knowledge boundary of LLMs.</p>
<p>Recent work support our findings. For instance, Ghosal et al. (2024) showed that models fine-tuned on well-known facts exhibit enhanced factuality compared to those fine-tuned on unpopular facts, which can be attributed to the model's lesser familiarity with unpopular facts. Another example is Lin et al. (2024), who fine-tuned a model using data generated by either a pre-trained model or a retrieval-augmented variant. They found that the latter resulted in reduced factuality, which can be attributed to the introduction of new factual knowledge in the retrieved texts. Ren et al. (2024) have also investigated the effects of introducing new factual knowledge through fine-tuning in a considerably different methodological setup, focusing on multiple-choice questions, conducting relatively short fine-tuning runs, and testing only $100 \%$ known and $100 \%$ unknown mixtures. Their results align with ours, which further reinforces our conclusions. Lastly, these insights were also integrated into the instruction-tuning phase of Llama 3 models (Dubey et al., 2024), ensuring that the examples are aligned with pre-training knowledge.</p>
<p>Another line of work explores the model's behavior on new knowledge in test time. Kang et al. (2024) showed that when a fine-tuned LLM encounters unknown queries at test time, its responses mimic the responses associated with the unknown examples in the fine-tuning data. Yin et al. (2023) showed that LLMs' performance is not satisfactory when they face new knowledge in their input contexts and Lee et al. (2023) analyzed the impact of unknown in-context learning examples.</p>
<p>Quantifying knowledge in LLMs. SliCK can be seen as a confidence elicitation method for the ground truth label ( $M$ knows $(q, a)$ if it is confident that $a$ is the answer to $q$ ). Existing work derive cali-
brated confidence from LLMs by examining agreement across multiple samples (Kuhn et al., 2023; Manakul et al., 2023; Tian et al., 2023a; Lyu et al., 2024), probing internal representations (Azaria and Mitchell, 2023; Burns et al., 2022), eliciting verbalized probability (Tian et al., 2023b) or direct prompting (Kadavath et al., 2022). Kadavath et al. also trained a separate $\mathrm{P}(\mathrm{IK})$ model to predict if the LLM knows the answer to $q$. The label for $\mathrm{P}(\mathrm{IK})$ was approximated by the fraction of correct sampled answers, which is conceptually aligned with $P_{\text {correct }}(\$ 3)$. A key difference is that we also define the SliCK categories, and provide evidence that we capture meaningful and useful categories.</p>
<h2>10 Conclusion</h2>
<p>We study the impact of integrating new factual knowledge through fine-tuning on the model's tendency to hallucinate. We first propose SliCK, a categorization of facts w.r.t. LLM's knowledge. We then design a controlled study where we isolate the impact of new knowledge and rigorously evaluate its effects. We provide multiple insights on the fine-tuning dynamics, with the following key findings: (1) Acquiring new knowledge via supervised fine-tuning is correlated with hallucinations w.r.t. pre-existing knowledge. (2) LLMs struggle to integrate new knowledge through fine-tuning and mostly learn to use their pre-existing knowledge.</p>
<h2>11 Limitations</h2>
<p>Our experiments were conducted using a single LLM, and thus it is unclear whether results will vary with different LLMs. Having said that, our study is extremely compute-heavy and thus challenging to replicate on multiple LLMs: First, our fine-tuning is compute-heavy as its runs are very long as we wanted to analyze the behavior during different stages of fine-tuning (including the overfitting stages). Second, and most importantly, to facilitate our study we needed to annotate a large scale dataset w.r.t the SliCK categories. To derive reliable conclusions, it was crucial to accurately assess the model's knowledge w.r.t. a single finetuning example. In our case we run 170 inference steps per example, i.e., more than $15 M$ inference steps to categorize our full dataset.</p>
<p>In addition, since we focus on closed-book QA, the practical implications from our study such as filtering-out Unknown fine-tuning examples still require validation in settings involving long-form</p>
<p>text generation. To filter-out examples that introduce new factual knowledge in long-form generation tasks, one would need to make adaptations to $\mathbf{S i i C K}$ and come up with an effective way to compare the sampled answer with the ground-truth to approximate $P_{\text {Correct }}$. We leave this for future work. Long-form generation tasks introduce evaluation challenges, leading to a wide adoption of LLM-based evaluations. Our choice to focus explicitly on closed book QA facilitates more accurate evaluation that enhances the reliability of our findings.</p>
<p>Lastly, we did not test the effect of adding additional fine-tuning examples from diverse tasks into the fine-tuning mixture. While this could more closely approximate a typical instruction finetuning scenario, such dataset extension may introduce new factual knowledge in an uncontrollable way, which will limit our findings.</p>
<h2>12 Acknowledgments</h2>
<p>We thank the reviewers for their valuable comments and suggestions for improving this work. We would also like to thank Ori Ram, Uri Shaham, Alon Jacovi, Mor Ventura, Yochai Blau, Eyal BenDavid, Avi Caciularu, Avinatan Hassidim and the members of Roi Reichart's NLP group for reviewing the paper draft and providing valuable feedback. Special thanks to Uri Shaham for assisting in setting up the fine-tuning pipeline during the early stages of our research.</p>
<h2>References</h2>
<p>Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. 2022. A review on language models as knowledge bases. arXiv preprint arXiv:2204.06031.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734.</p>
<p>Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John Patrick Cunningham. 2024. LoRA learns less and forgets less. Transactions on Machine Learning Research. Featured Certification.</p>
<p>Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827.</p>
<p>I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528.</p>
<p>Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. 2023. Crawling the internal knowledgebase of language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1856-1869, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.</p>
<p>Leo Gao. 2021. Behavior cloning is miscalibrated. AI Alignment Forum.</p>
<p>Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. TrueTeacher: Learning factual consistency evaluation with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2053-2070, Singapore. Association for Computational Linguistics.</p>
<p>Gaurav Rohit Ghosal, Tatsunori Hashimoto, and Aditi Raghunathan. 2024. Understanding finetuning for factual knowledge extraction. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net.</p>
<p>Yoav Goldberg. 2023. Reinforcement learning for language models.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717.</p>
<p>Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024. Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608.</p>
<p>Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3905-3920, Seattle, United States. Association for Computational Linguistics.</p>
<p>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. $q^{2}$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856-7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232.</p>
<p>Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats Leon Richter, Quentin Gregory Anthony, Eugene Belilovsky, Timothée Lesort, and Irina Rish. 2024. Simple and scalable strategies to continually pre-train large language models. Transactions on Machine Learning Research.</p>
<p>Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Lin, Wen-tau Yih, and Srini Iyer. 2024. Instruction-tuned language models are better knowledge learners. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5421-5434, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, and Davood Rafiei. 2023. Evaluating open-domain question answering in the era of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5591-5606. Association for Computational Linguistics.</p>
<p>Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. 2024. Unfamiliar finetuning examples control how language models hallucinate. arXiv preprint arXiv:2403.05612.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In</p>
<p>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664.</p>
<p>Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-visiting NLIBased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177.</p>
<p>Yoonsang Lee, Pranav Atreya, Xi Ye, and Eunsol Choi. 2023. Crafting in-context examples according to lms' parametric knowledge. arXiv preprint arXiv:2311.09579.</p>
<p>Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023. The unlocking spell on base ilms: Rethinking alignment via incontext learning. ArXiv preprint.</p>
<p>Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, and Xilun Chen. 2024. Flame: Factuality-aware alignment for large language models. arXiv preprint arXiv:2405.01525.</p>
<p>Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, and Chris Callison-Burch. 2024. Calibrating large language models with sample consistency. arXiv preprint arXiv:2402.13904.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.</p>
<p>Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. 2023. Massediting memory in a transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076-12100, Singapore. Association for Computational Linguistics.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.</p>
<p>Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Reuse, don't retrain: A recipe for continued pretraining of language models. arXiv preprint arXiv:2407.07263.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Mengjie Ren, Boxi Cao, Hongyu Lin, Cao Liu, Xianpei Han, Ke Zeng, Wan Guanglu, Xunliang Cai, and Le Sun. 2024. Learning or self-aligning? rethinking instruction fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6090-6105, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655-2671, Seattle, United States. Association for Computational Linguistics.</p>
<p>John Schulman. 2023. Reinforcement learning from human feedback: Progress and challenges.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric questions challenge dense retrievers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 6138-6148. Association for Computational Linguistics.</p>
<p>Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. 2023a. Finetuning language models for factuality. arXiv preprint arXiv:2311.08401.</p>
<p>Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023b. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975.</p>
<p>Pranav Narayanan Venkit, Tatiana Chakravorti, Vipul Gupta, Heidi Biggs, Mukund Srinath, Koustava Goswami, Sarah Rajtmajer, and Shomir Wilson. 2024. " confidently nonsensical?": A critical survey on the perspectives and challenges of 'hallucinations' in nlp. arXiv preprint arXiv:2404.07461.</p>
<p>Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM, 57(10):78-85.</p>
<p>Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, and Yue Zhang. 2023. Evaluating open-qa evaluation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for honesty. arXiv preprint arXiv:2312.07000.</p>
<p>Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. ALCUNA: Large language models meet new knowledge. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1397-1414, Singapore. Association for Computational Linguistics.</p>
<p>Gal Yona, Roee Aharoni, and Mor Geva. 2024. Narrowing the knowledge evaluation gap: Open-domain question answering with multi-granularity answers. arXiv preprint arXiv:2401.04695.</p>
<p>Hanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2023. R-tuning: Teaching large language models to refuse unknown questions. arXiv preprint arXiv:2311.09677.</p>
<p>Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen. 2023. Mquake: Assessing knowledge editing in language models via multi-hop questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 15686-15702. Association for Computational Linguistics.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363.</p>
<h2>A Data Preprocessing</h2>
<p>This section expands $\S 2$ with additional details about our data preprocessing steps. The EntiTYQuestions dataset (Sciavolino et al., 2021) consists of train, development and test splits and spans 24 relations. Our train, development and test sets are curated based on the original splits from EntiTYQuestions. However, we use only 12 relations, since we wanted to reserve some relations for out-of-distribution test set. To avoid cherry-picking, the 12 relations used in our train, development and test sets are randomly sampled. The resulting relations are presented in Tables 4 and 5.</p>
<p>We reserved the remaining 12 relations for out-of-distribution test set. However, we found that in those 12 reserved relations, 5 were too similar to some of the relations that we train on (Table 4), thus we suspected that this could lead to a test set that is not truly out-of-distribution. To address that, we filtered out those relations and were left with 7 relations for our-of-distribution. These 7 out-ofdistribution relations are presented in Table 6. The relations that were filtered-out are as follows:</p>
<ul>
<li>P276 was filtered out since it directly overlaps with P131 since for both relations the question in EntityQuestions is of the form "Where is [E] located?". P276 stands for "location" (https://www. wikidata.org/wiki/Property:P276) and P131 stands for "located in the administrative territorial entity" (https://www.wikidata. org/wiki/Property:P131).</li>
<li>P20, for which the question template is "Where did [E] die?", was filtered out since it may require knowledge that relates to P19, for which the question template is "Where was [E] born?". P20 stands for "place of death" (https://www.wikidata.org/wiki/ Property:P20) and P19 stands for "place of birth" (https://www.wikidata.org/wiki/ Property:P19).</li>
<li>P106, for which the question template is "What kind of work does [E] do?", was filtered out since it may require knowledge that relates to P800, for which the question template is "What is [E] famous for?". P106 stands for "occupation" (https://www.wikidata. org/wiki/Property:P106) and P800 stands for "notable work" (https://www.wikidata. org/wiki/Property:P800).</li>
<li>P413, for which the question template is "What position does [E] play?", was filtered out since it may require knowledge that relates to P800, for which the question template is "What is [E] famous for?". P413 stands for "position played on team / speciality" (https://www.wikidata. org/wiki/Property:P413) and P800 stands for "notable work" (https://www.wikidata. org/wiki/Property:P800).</li>
<li>P159, for which the question template is "Where is the headquarters of [E]?", was filtered out since it may require knowledge that relates to P36, for which the question template is "What is the capital of [E]?". P159 stands for "headquarters location" (https://www.wikidata. org/wiki/Property:P159) and P36 stands for "capital" (https://www.wikidata.org/ wiki/Property:P36).</li>
</ul>
<p>Lastly, we perform two additional filtering steps: (1) To simplify the process of categorizing the examples w.r.t. $M$ 's knowledge (§3), we filter-out examples with more than 1 correct answer. ${ }^{14}$ (2) We make sure that no subjects or objects overlap between the train and test sets, ${ }^{15}$ by filtering-out overlapping examples from the train set. ${ }^{16}$</p>
<h2>B Hallucinations in the Context of our Study</h2>
<p>In general, the term "hallucinations" is not welldefined in NLP (Venkit et al., 2024). For clarity, in $\S$ B. 1 we define the type of hallucinations we target in this work. In addition, while our main takeaway is that acquiring new knowledge can lead to hallucinations, our experiments focus on measuring accuracy drops on the test set. Therefore, in $\S$ B. 2 we clarify why worse performance on the test set in our study is attributed to higher rate of hallucinations, as defined in §B.1.</p>
<h2>B. 1 Hallucinations w.r.t. the Pre-existing Knowledge.</h2>
<p>Huang et al. (2023) categorized hallucinations into two main categories: (1) factuality hallucination</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">relation</th>
<th style="text-align: left;">question template</th>
<th style="text-align: right;">HighlyKnown</th>
<th style="text-align: right;">MaybeKnown</th>
<th style="text-align: right;">WeaklyKnown</th>
<th style="text-align: right;">Unknown</th>
<th style="text-align: right;">Total</th>
<th style="text-align: right;">Min</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P131</td>
<td style="text-align: left;">Where is [E] located?</td>
<td style="text-align: right;">553</td>
<td style="text-align: right;">2529</td>
<td style="text-align: right;">1493</td>
<td style="text-align: right;">3071</td>
<td style="text-align: right;">7646</td>
<td style="text-align: right;">553</td>
</tr>
<tr>
<td style="text-align: left;">P136</td>
<td style="text-align: left;">What type of music does [E] play?</td>
<td style="text-align: right;">236</td>
<td style="text-align: right;">3410</td>
<td style="text-align: right;">1892</td>
<td style="text-align: right;">1978</td>
<td style="text-align: right;">7516</td>
<td style="text-align: right;">236</td>
</tr>
<tr>
<td style="text-align: left;">P17</td>
<td style="text-align: left;">Which country is [E] located in?</td>
<td style="text-align: right;">4387</td>
<td style="text-align: right;">2628</td>
<td style="text-align: right;">511</td>
<td style="text-align: right;">364</td>
<td style="text-align: right;">7890</td>
<td style="text-align: right;">364</td>
</tr>
<tr>
<td style="text-align: left;">P19</td>
<td style="text-align: left;">Where was [E] born?</td>
<td style="text-align: right;">369</td>
<td style="text-align: right;">1884</td>
<td style="text-align: right;">1498</td>
<td style="text-align: right;">4170</td>
<td style="text-align: right;">7921</td>
<td style="text-align: right;">369</td>
</tr>
<tr>
<td style="text-align: left;">P26</td>
<td style="text-align: left;">Who is [E] married to?</td>
<td style="text-align: right;">1609</td>
<td style="text-align: right;">1503</td>
<td style="text-align: right;">1087</td>
<td style="text-align: right;">3257</td>
<td style="text-align: right;">7456</td>
<td style="text-align: right;">1087</td>
</tr>
<tr>
<td style="text-align: left;">P264</td>
<td style="text-align: left;">What music label is [E] represented by?</td>
<td style="text-align: right;">206</td>
<td style="text-align: right;">1444</td>
<td style="text-align: right;">1854</td>
<td style="text-align: right;">3820</td>
<td style="text-align: right;">7324</td>
<td style="text-align: right;">206</td>
</tr>
<tr>
<td style="text-align: left;">P36</td>
<td style="text-align: left;">What is the capital of [E]?</td>
<td style="text-align: right;">4160</td>
<td style="text-align: right;">1634</td>
<td style="text-align: right;">449</td>
<td style="text-align: right;">572</td>
<td style="text-align: right;">6815</td>
<td style="text-align: right;">449</td>
</tr>
<tr>
<td style="text-align: left;">P40</td>
<td style="text-align: left;">Who is [E]'s child?</td>
<td style="text-align: right;">692</td>
<td style="text-align: right;">1467</td>
<td style="text-align: right;">1271</td>
<td style="text-align: right;">2680</td>
<td style="text-align: right;">6110</td>
<td style="text-align: right;">692</td>
</tr>
<tr>
<td style="text-align: left;">P495</td>
<td style="text-align: left;">Which country was [E] created in?</td>
<td style="text-align: right;">5459</td>
<td style="text-align: right;">1101</td>
<td style="text-align: right;">408</td>
<td style="text-align: right;">706</td>
<td style="text-align: right;">7674</td>
<td style="text-align: right;">408</td>
</tr>
<tr>
<td style="text-align: left;">P69</td>
<td style="text-align: left;">Where was [E] educated?</td>
<td style="text-align: right;">233</td>
<td style="text-align: right;">1126</td>
<td style="text-align: right;">1712</td>
<td style="text-align: right;">3650</td>
<td style="text-align: right;">6721</td>
<td style="text-align: right;">233</td>
</tr>
<tr>
<td style="text-align: left;">P740</td>
<td style="text-align: left;">Where was [E] founded?</td>
<td style="text-align: right;">1323</td>
<td style="text-align: right;">1618</td>
<td style="text-align: right;">1428</td>
<td style="text-align: right;">2902</td>
<td style="text-align: right;">7271</td>
<td style="text-align: right;">1323</td>
</tr>
<tr>
<td style="text-align: left;">P800</td>
<td style="text-align: left;">What is [E] famous for?</td>
<td style="text-align: right;">301</td>
<td style="text-align: right;">330</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">503</td>
<td style="text-align: right;">1356</td>
<td style="text-align: right;">222</td>
</tr>
<tr>
<td style="text-align: left;">TOTAL</td>
<td style="text-align: left;">-</td>
<td style="text-align: right;">19528</td>
<td style="text-align: right;">20674</td>
<td style="text-align: right;">13825</td>
<td style="text-align: right;">27673</td>
<td style="text-align: right;">81700</td>
<td style="text-align: right;">6142</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistics of the EntityQuestions train split annotated with SliCK categories. We annotate the entire train split but always fine-tune on exactly 6142 examples (see the Min column). Refer to §E for more details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">relation</th>
<th style="text-align: left;">question template</th>
<th style="text-align: right;">HighlyKnown</th>
<th style="text-align: right;">MaybeKnown</th>
<th style="text-align: right;">WeaklyKnown</th>
<th style="text-align: right;">Unknown</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P131</td>
<td style="text-align: left;">Where is [E] located?</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">362</td>
<td style="text-align: right;">158</td>
<td style="text-align: right;">388</td>
<td style="text-align: right;">965</td>
</tr>
<tr>
<td style="text-align: left;">P136</td>
<td style="text-align: left;">What type of music does [E] play?</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">432</td>
<td style="text-align: right;">248</td>
<td style="text-align: right;">281</td>
<td style="text-align: right;">967</td>
</tr>
<tr>
<td style="text-align: left;">P17</td>
<td style="text-align: left;">Which country is [E] located in?</td>
<td style="text-align: right;">448</td>
<td style="text-align: right;">432</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">996</td>
</tr>
<tr>
<td style="text-align: left;">P19</td>
<td style="text-align: left;">Where was [E] born?</td>
<td style="text-align: right;">107</td>
<td style="text-align: right;">148</td>
<td style="text-align: right;">243</td>
<td style="text-align: right;">501</td>
<td style="text-align: right;">999</td>
</tr>
<tr>
<td style="text-align: left;">P26</td>
<td style="text-align: left;">Who is [E] married to?</td>
<td style="text-align: right;">177</td>
<td style="text-align: right;">238</td>
<td style="text-align: right;">158</td>
<td style="text-align: right;">378</td>
<td style="text-align: right;">951</td>
</tr>
<tr>
<td style="text-align: left;">P264</td>
<td style="text-align: left;">What music label is [E] represented by?</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">157</td>
<td style="text-align: right;">268</td>
<td style="text-align: right;">486</td>
<td style="text-align: right;">958</td>
</tr>
<tr>
<td style="text-align: left;">P36</td>
<td style="text-align: left;">What is the capital of [E]?</td>
<td style="text-align: right;">580</td>
<td style="text-align: right;">152</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">86</td>
<td style="text-align: right;">880</td>
</tr>
<tr>
<td style="text-align: left;">P40</td>
<td style="text-align: left;">Who is [E]'s child?</td>
<td style="text-align: right;">99</td>
<td style="text-align: right;">191</td>
<td style="text-align: right;">167</td>
<td style="text-align: right;">344</td>
<td style="text-align: right;">801</td>
</tr>
<tr>
<td style="text-align: left;">P495</td>
<td style="text-align: left;">Which country was [E] created in?</td>
<td style="text-align: right;">699</td>
<td style="text-align: right;">147</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">96</td>
<td style="text-align: right;">993</td>
</tr>
<tr>
<td style="text-align: left;">P69</td>
<td style="text-align: left;">Where was [E] educated?</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">145</td>
<td style="text-align: right;">227</td>
<td style="text-align: right;">441</td>
<td style="text-align: right;">840</td>
</tr>
<tr>
<td style="text-align: left;">P740</td>
<td style="text-align: left;">Where was [E] founded?</td>
<td style="text-align: right;">182</td>
<td style="text-align: right;">245</td>
<td style="text-align: right;">181</td>
<td style="text-align: right;">334</td>
<td style="text-align: right;">942</td>
</tr>
<tr>
<td style="text-align: left;">P800</td>
<td style="text-align: left;">What is [E] famous for?</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">76</td>
<td style="text-align: right;">189</td>
</tr>
<tr>
<td style="text-align: left;">TOTAL</td>
<td style="text-align: left;">-</td>
<td style="text-align: right;">2464</td>
<td style="text-align: right;">2699</td>
<td style="text-align: right;">1856</td>
<td style="text-align: right;">3462</td>
<td style="text-align: right;">10481</td>
</tr>
</tbody>
</table>
<p>Table 5: In-distribution test set statistics.
and (2) faithfulness hallucination. The first case refers to factual inconsistency between the generated content and verifiable real-world facts. Common examples include wrong answers in closedbook QA setting (Chern et al., 2023) or factual mistakes in long-form generations of knowledge intensive passages such as biographies (Manakul et al., 2023; Min et al., 2023). On the other hand, faithfulness hallucination refers to cases where the generated content is factually inconsistent with the context provided by the input. A common example is when the model summarizes a document and the resulting summary is factually inconsistent with the input document (Honovich et al., 2022; Laban et al., 2022; Honovich et al., 2021; Gekhman et al., 2023; Scialom et al., 2021; Kryscinski et al., 2020).</p>
<p>In this work we focus on a subset of factuality hallucinations. Our goal is to study how introducing new factual knowledge through fine-tuning affects the utilization of the model's pre-existing knowledge. To reflect this, we define hallucina-
tions w.r.t the model's pre-existing knowledge as $(q, a)$ pairs that were known to the model after pretraining (as defined by SliCK), while the fine-tuned model fails to answer $q$ correctly post fine-tuning. ${ }^{17}$</p>
<h2>B. 2 Test performance as Proxy for Hallucinations.</h2>
<p>We now detail the relation between the test performance in our setting and hallucinations. In our study, poorer performance of a fine-tuned model $M_{D 1}$, compared to another fine-tuned model $M_{D 2}$ on the test set, can be attributed to a higher rate of hallucinations in $M_{D 1}$, relative to its pre-existing knowledge, due to the following explanation.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">relation</th>
<th style="text-align: left;">question template</th>
<th style="text-align: right;">HighlyKnown</th>
<th style="text-align: right;">MaybeKnown</th>
<th style="text-align: right;">WeaklyKnown</th>
<th style="text-align: right;">Unknown</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P127</td>
<td style="text-align: left;">Who owns [E]?</td>
<td style="text-align: right;">125</td>
<td style="text-align: right;">383</td>
<td style="text-align: right;">168</td>
<td style="text-align: right;">314</td>
<td style="text-align: right;">990</td>
</tr>
<tr>
<td style="text-align: left;">P50</td>
<td style="text-align: left;">Who is the author of [E]?</td>
<td style="text-align: right;">287</td>
<td style="text-align: right;">193</td>
<td style="text-align: right;">115</td>
<td style="text-align: right;">372</td>
<td style="text-align: right;">967</td>
</tr>
<tr>
<td style="text-align: left;">P407</td>
<td style="text-align: left;">Which language was [E] written in?</td>
<td style="text-align: right;">366</td>
<td style="text-align: right;">153</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">623</td>
</tr>
<tr>
<td style="text-align: left;">P176</td>
<td style="text-align: left;">Which company is [E] produced by?</td>
<td style="text-align: right;">289</td>
<td style="text-align: right;">277</td>
<td style="text-align: right;">181</td>
<td style="text-align: right;">225</td>
<td style="text-align: right;">972</td>
</tr>
<tr>
<td style="text-align: left;">P170</td>
<td style="text-align: left;">Who was [E] created by?</td>
<td style="text-align: right;">142</td>
<td style="text-align: right;">284</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">304</td>
<td style="text-align: right;">850</td>
</tr>
<tr>
<td style="text-align: left;">P175</td>
<td style="text-align: left;">Who performed [E]?</td>
<td style="text-align: right;">94</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">103</td>
<td style="text-align: right;">663</td>
<td style="text-align: right;">980</td>
</tr>
<tr>
<td style="text-align: left;">P112</td>
<td style="text-align: left;">Who founded [E]?</td>
<td style="text-align: right;">134</td>
<td style="text-align: right;">116</td>
<td style="text-align: right;">76</td>
<td style="text-align: right;">140</td>
<td style="text-align: right;">466</td>
</tr>
<tr>
<td style="text-align: left;">TOTAL</td>
<td style="text-align: left;">-</td>
<td style="text-align: right;">1437</td>
<td style="text-align: right;">1526</td>
<td style="text-align: right;">822</td>
<td style="text-align: right;">2063</td>
<td style="text-align: right;">5848</td>
</tr>
</tbody>
</table>
<p>Table 6: Out-of-distribution test set statistics.</p>
<p>The test set can be conceptually divided into two types of questions. First, there are questions with answers that are unknown to $M$. Those questions will remain unknown post fine-tuning, as we make sure that the training set is disjoint from the test set (§A). This means that both $M_{D 1}$ and $M_{D 2}$ will fail to answer these questions. Thus, the test performance difference between $M_{D 1}$ and $M_{D 2}$ is mostly attributed to the second type of questions: ones that are known to $M$, i.e. $M$ can answer them correctly since it posses the relevant knowledge. Thus, $M_{D 1}$ and $M_{D 2}$ must rely on their pre-existing knowledge to answer such questions, and a lower performance on such question can be only categorized as an hallucination w.r.t. pre-existing knowledge.</p>
<h2>C $\boldsymbol{P}_{\text {Correct }}$ Approximation</h2>
<p>This section expands $\S 3$ with additional details about our $P_{\text {Correct }}$ approximation. In our study we approximate $P_{\text {Correct }}(q, a ; M, T)$ based on the fraction of correct answers to $q$ sampled from $M$. We begin with randomly sampling $N_{\mathrm{ex}}$ distinct $k$ shot exemplars for each relation in our dataset (§A). Then, to approximate $P_{\text {Correct }}(q, a ; M, T)$, we use $M$ to generate answers to $q$ using each of the $N_{\mathrm{ex}}$ exemplars from the relation corresponding to $q$. We first use temperature sampling with $T=0.5$ to sample $N_{\text {sample }}$ answers for each of the $N_{\text {ex }}$ exemplars. $P_{\text {Correct }}(q, a ; M, T&gt;0)$ is then approximated by the fraction of correct answers from the total of $N_{\mathrm{ex}} \cdot N_{\text {sample }}$ predictions. We also generate the greedy decoding prediction $(T=0)$ for each of the $N_{\text {ex }}$ exemplars. $P_{\text {Correct }}(q, a ; M, T=0)$ is then approximated by the fraction of correct answers from the total of $N_{\text {ex }}$ predictions. ${ }^{18}$</p>
<p>We use $k=4$ in our study, simply since we found it enough for $M$ to output answers in the</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Wrong Answer | Paraphrase | Higher Granularity | Lower Granularity |
| :--: | :--: | :--: | :--: |
| $90 \%$ | $6 \%$ | $2 \%$ | $2 \%$ |</p>
<p>Table 7: Error Analysis of 100 Predictions of the Pretrained Model, for Which Exact Match is False.
correct format. We use $N_{\mathrm{ex}}=10$ and $N_{\text {sample }}=$ 16. The $N_{\text {sample }}=16$ samples using $T=0.5$ are sampled from Top 40.</p>
<p>The $k$ exemplars are sampled from the development split. We sample $N_{\text {ex }}$ different samples since we found that even when the few-shot exemplars are sampled per-relation, their exact choice still affects the prediction. In $\S 6$ and Figure 5 we show evidence that this also improves the quality of our categories.</p>
<p>Below is an example of our 4-shot prompt format, from real example from EntityQuestions with the relation $P 106$ representing occupation. ${ }^{19}$ The question in this case is "What kind of work does Ron Konopka do?" and the ground truth asnwer is "geneticist".</p>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">kind</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">Nicolas</span><span class="w"> </span><span class="n">Roeg</span><span class="w"> </span><span class="k">do</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">film</span><span class="w"> </span><span class="n">director</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">kind</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">Crystal</span><span class="w"> </span><span class="n">Geoffré</span><span class="w"> </span><span class="k">do</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">actor</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">kind</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">Maurice</span><span class="w"> </span><span class="n">Blondel</span><span class="w"> </span><span class="k">do</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">philosopher</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">kind</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">Javier</span><span class="w"> </span><span class="n">de</span><span class="w"> </span><span class="n">Burgos</span><span class="w"> </span><span class="k">do</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">politician</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">kind</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">Ron</span><span class="w"> </span><span class="n">Konopka</span><span class="w"> </span><span class="k">do</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span>
</code></pre></div>

<p>To decide whether a sampled answer is correct, we use the Exact Match (EM) metric to compare it with the ground truth answer. The main advantage in this choice is that when EM is True, we know that the answer is correct for $100 \%$. The main potential risk associated with this choice is that we may wrongly classify answers as incorrect due to paraphrases or answers with different granularity</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>levels (Wang et al., 2023; Kamalloo et al., 2023; Yona et al., 2024)). To address this, we perform an error analysis on 100 predictions for which EM was False. We randomly sample 50 greedy predictions $(T=0)$ and 50 samples with $T=0.5$. The results are in Table 7. This analysis suggest that in $90 \%$ of the cases where EM is False, the predicted answer is indeed incorrect. Which is a reasonable performance for our purpose, especially considering that when EM is True the answer is $100 \%$ correct.</p>
<h2>D Data Annotation</h2>
<p>we first calculate $P_{\text {Correct }}(q, a ; M, T=0)$ and $P_{\text {Correct }}(q, a ; M, T&gt;0)$ for each $(q, a)$ pair in our preprocessed dataset ( $\S 2$ and $\S \mathrm{A}$ ), using our $P_{\text {Correct }}(\cdot)$ approximation ( $\S 3$ and $\S \mathrm{C}$ ). We then use these values to categorize each $(q, a)$ pair into one of our four categories ( $\S 3$ and Figure 2). We provide the full statistics of the categories on the train and test set, as well as the out-of-distribution test set in Tables 4, 5 and 6.</p>
<h2>E Fine-tuning Details</h2>
<p>Fine-tuning Data. In $\S 4$ we examine the effect of new knowledge in the fine-tuning dataset $D$ on the performance of $M_{D}$, by varying the proportion of Unknown examples in $D$. When we create variants of $D$ with exactly $X \%$ of Unknown and $(100-X) \%$ Known examples, we make sure that the relation distribution remains consistent. To achieve that we sample $X \%$ of Unknown from each relation.</p>
<p>In $\S 5$ we create single-category variants of $D$. Since we want to work with a fixed $|D|$ across all variants, we want to make sure that we have $|D|$ examples from each category. To ensure this, we measure the size of the smallest category in each relation (see the "Min" column in Table 4) and define $|D|$ as their sum. In other words, for each relation we calculate the size of the smallest category and sum these values. This leads to $|D|=6142$, as illustrated by the last column in Table 4. More formally, for each relation $r$ in the training split, and for each category CAT from our 4 SliCK categories, we define $\mathrm{CAT}<em _mathrm_r="\mathrm{r">{\mathrm{r}}$ to be the examples from category CAT and relation r. Consequently $\operatorname{size}\left(\mathrm{CAT}</em>}}\right)$ is the number of the examples in $\mathrm{CAT<em _P131="{P131" _text="\text">{\mathrm{r}}$. For example size $(\text { HighlyKnown }</em>=553$ (see})</p>
<p>Table 4). We then define:
<img alt="img-4.jpeg" src="img-4.jpeg" />
where $\mathbf{R}_{\text {Train }}$ are the 12 relations from the training set.</p>
<p>Below is an example of our data format in the train, development and test sets, from real example from EntityQuestions with the relation $P 106$ representing occupation. ${ }^{20}$ The question in this case is "What kind of work does Ron Konopka do?" and the ground truth asnwer is "geneticist".</p>
<p>Answer the following question.
What kind of work does Ron Konopka do?</p>
<p>Fine-tuning Regime. In this work, we focus on full fine-tuning, where all model parameters are updated. An interesting direction for future research is to investigate similar questions within parameterefficient fine-tuning regimes (Han et al., 2024), such as LoRA (Hu et al., 2022). For instance, Biderman et al. (2024) demonstrated that, compared to full fine-tuning, LoRA better preserves the base model's performance on tasks outside the target domain, though at the cost of diminished performance within the target domain. It could be interesting to check if this also holds to hallucinations w.r.t. the models pre-existing knowledge as we define it in this work (§B). Another interesting avenue for future research is to explore how new knowledge is acquired during continual pre-training (Jiang et al., 2024; Parmar et al., 2024; Ibrahim et al., 2024) as one of the key objectives in continual pre-training is to inject new (up to date) knowledge to the model.</p>
<p>Fine-tuning hypeparameters. We fine-tune every model for 50 epochs for all our model variants to completely fit the training set, so we can examine all stages of fine-tuning. We evaluate the models every epoch on the development set. The EARLY_STOP stopping criteria is defined to be the epoch with the maximum accuracy on the development set. We use learning rate of 1e-5, a batch size of 128, and a dropout rate of 0.05 . Our experimental design intentionally utilized a fixed learning rate, which is the standard for supervised fine-tuning, as opposed to the dynamic learning rate strategies typically</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Performance on the test set with a slower learning rate of 1e-4. This plot is equivalent to Figure 3a, and the results are similar, except that the experiment were run with a learning rate of 1e-4 instead of 1e-5.</p>
<p>employed in continual pre-training. We have experimented with both slower and faster fixed learning rates (1e-4 and 1e-6) to ensure the robustness of our conclusions. These experiments consistently supported our findings. For instance, in Figure 6 we present the performance as a function of the % of the Unknown examples in <em>D</em> (i.e. similar plot to Figure 3a) when using a learning rate of 1e-4 instead of 1e-5.</p>
<h1><strong>F The Case for Avoiding Fake Facts</strong></h1>
<p>One limitation of using the Unknown examples in our study is that <strong>SliCK</strong> only approximates the LLM's knowledge. This means that some examples can be incorrectly classified as unknown to <em>M</em>. As we discuss in §6, our results indicate that this happens in at most 3% of the cases, meaning that the vast majority of the examples classified as Unknown are actually unknown to <em>M</em>.</p>
<p>Alternative approach could be to simply use fake facts as unknown fine-tuning examples. We considered this in early stages of the project and were concerned that this would introduce confounding factors into our study, as fake facts may behave differently than real ones; In our setup where the knowledge is represented with <em>(subject, relation, object)</em> triplets, there are 2 main ways to generate fake facts: (1) creating triplets where both the <em>subject</em> and the <em>object</em> are fake (Yin et al., 2023). (2) Creating triplets where the <em>subject</em> is real and the <em>object</em> is fake (Zhu et al., 2020; Meng et al., 2022, 2023; Zhong et al., 2023). Focusing exclusively on (1) will capture only a small subset of the cases of new factual knowledge, as in the majority of the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Training accuracy as a function of fine-tuning duration, evaluated on the variant with 50% Unknown fine-tuning examples. For reference, we also include the accuracy on the development set, accompanied by a zoom-in plot within a narrower range, to provide a more visible and clear view.</p>
<p>cases the subject will be familiar to the pre-trained model. E.g., the model may know that there is a person named <em>"Barack Obama"</em> but not know where he was born. If we consider (2), using real subjects with fake objects may compromise our study as in many cases this will introduce <strong>knowledge updates</strong> and not new knowledge. To illustrate this, let's consider the triplet <em>("Barack Obama", "place of birth", "Honolulu")</em>, and let's assume that we generate the fake triplet <em>("Barack Obama", "place of birth", "London")</em>. Now, since the original (correct) triplet may be known to the model we essentially do not simulate introducing new factual knowledge but <strong>updating existing knowledge</strong>. Considering the above, we decided that using real world facts will make our findings more reliable. We then invested a considerable effort to ensure that the examples that are classified as unknown are truly unknown to the model (as discussed above).</p>
<h1><strong>G Train Accuracy on Different Known Categories</strong></h1>
<p>In §4.3 we analyze the fine-tuning dynamic and present the training accuracy as function of the fine-tuning duration in Figure 1. For simplicity we treated the Known categories collectively. For reference we also include the plot with the full per-category breakdown in Figure 7.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Performance on the out-of-distribution (OOD) test set as a function of the \% of Unknown examples in the fine-tuning dataset $D$. This plot is the OOD version of Figure 3. Everything is similar to Figure 3, except that y-axis is the accuracy on the OOD test set. We note that the development set did not change (not OOD), thus it does not necessarily reflects the optimal stopping point for OOD.</p>
<h2>H Linear Model</h2>
<p>In $\S 4.4$ and $\S 4.5$ we use a linear model (Equation (1)) that predicts that test accuracy and the out-of-distribution test accuracy. We estimate the parameters of this linear model based on results from all our variants of $D$ used in $\S 4$. For all these variants, we measure the test accuracy and the number of Known and Unknown fine-tuning examples that $M$ fits during different fine-tuning stages. This way we collect a dataset with examples of the form (Accuracy, $N_{\mathrm{Kn}}, N_{\text {Unk }}$ ), which we use to fit a linear regression model.</p>
<h2>I Out-of-distribution (OOD) Evaluation</h2>
<p>In $\S 4.5$ we discuss out-of-distribution (OOD) results. In these experiments we simply used our OOD test set consisting of 7 relations unseen during fine-tuning (see $\S \mathrm{A}$ ). When we perform the analysis discussed in $\S 4.1$ and $\S 4.2$, we additionally evaluated the models on the OOD test set. For completeness, we add here Figure 8, which is the out-of-distribution version of Figure 3. Figure 8a presents the OOD test performance as a function of $\%$ of Unknown examples in $D$ for different finetuning duration. The corresponding in-distribution results (Figure 3a) were discussed in $\S 4.1$. Figure 8 b presents the OOD test performance for the ablation where we filter-out Unknown fine-tuning examples. The corresponding in-distribution results (Figure 3b) were discussed in $\S 4.2$. We notice that similar trends, just with a smaller overall magnitude of the performance drop, up to 6 points
drop compared to up to 14 for in-distribution. This smaller drop magnitude is also reflected in smaller values of $\left|\beta_{\text {ukn }}\right|$ and $\left|\beta_{\text {kn }}\right|$ (Table 1).</p>
<h2>J Statistic Significance Tests</h2>
<p>In $\S 5$ we present Table 2. As mentioned in the caption, we perform statistic significance tests for each column. To this end we compare all the values to the maximal value in this column.</p>
<p>For each subset of the test set, we randomly shuffle all the examples in it, split them up into 100 approximately equally sized subsets, and compute accuracy for each of them for all the models of interest. We then apply paired-sample t-test with $p&lt;0.05$ and $p&lt;0.01$.</p>
<p>In Table 2, the best result is in bold, as well as all the results with statistically non-significant difference from the best with $p&lt;0.05$. We additionally include a copy of Table 2 where all the statistical tests outcomes are annotated, see Table 8. We can see that in almost all cases the difference is statistically significant with $p&lt;0.01$, except two cases where it is only with $p&lt;0.05$ ( $D_{\text {Natural }}$ Unk and $D_{\text {MaybeKnown }}$ Mkn).</p>
<p>Since we also discuss "horizontal" comparisons, where we compare EARLY_STOP to CONVERGENCE, we additionally run significance tests (not annotated in Table 2) for All, comparing EARLY_STOP to ConVERGENCE. The difference for $D_{\text {MaybeKnown }}$ was not statistically significant while for all others (including $D_{\text {Natural }}$ ) it was significant with $p&lt;0.01$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">EARLY_STOP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CONVERGENCE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Hkn</td>
<td style="text-align: center;">Mkn</td>
<td style="text-align: center;">Wkn</td>
<td style="text-align: center;">Unk</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Hkn</td>
<td style="text-align: center;">Mkn</td>
<td style="text-align: center;">Wkn</td>
<td style="text-align: center;">Unk</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {HighlyKnown }}$</td>
<td style="text-align: center;">$40.5^{<em> </em>}$</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">$60.1^{<em> </em>}$</td>
<td style="text-align: center;">$9.0^{<em> </em>}$</td>
<td style="text-align: center;">$0.6^{<em> </em>}$</td>
<td style="text-align: center;">$40.0^{<em> </em>}$</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">$58.8^{<em> </em>}$</td>
<td style="text-align: center;">$8.5^{<em> </em>}$</td>
<td style="text-align: center;">$0.7^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {MaybeKnown }}$</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">$12.1^{<em> </em>}$</td>
<td style="text-align: center;">$1.0^{<em> </em>}$</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">$97.5^{*}$</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">$12.9^{<em> </em>}$</td>
<td style="text-align: center;">$1.3^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {WeeklyKnown }}$</td>
<td style="text-align: center;">$39.2^{<em> </em>}$</td>
<td style="text-align: center;">$95.0^{<em> </em>}$</td>
<td style="text-align: center;">$59.2^{<em> </em>}$</td>
<td style="text-align: center;">$8.6^{<em> </em>}$</td>
<td style="text-align: center;">$0.4^{<em> </em>}$</td>
<td style="text-align: center;">$35.4^{<em> </em>}$</td>
<td style="text-align: center;">$73.5^{<em> </em>}$</td>
<td style="text-align: center;">$55.8^{<em> </em>}$</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">$2.2^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {Unknown }}$</td>
<td style="text-align: center;">$37.5^{<em> </em>}$</td>
<td style="text-align: center;">$95.6^{<em> </em>}$</td>
<td style="text-align: center;">$52.9^{<em> </em>}$</td>
<td style="text-align: center;">$6.5^{<em> </em>}$</td>
<td style="text-align: center;">$0.6^{<em> </em>}$</td>
<td style="text-align: center;">$25.8^{<em> </em>}$</td>
<td style="text-align: center;">$55.8^{<em> </em>}$</td>
<td style="text-align: center;">$36.6^{<em> </em>}$</td>
<td style="text-align: center;">$12.2^{<em> </em>}$</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: center;">$D_{\text {Natural }}$</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">$98.0^{*}$</td>
<td style="text-align: center;">$67.6^{<em> </em>}$</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">$41.8^{<em> </em>}$</td>
<td style="text-align: center;">$95.5^{<em> </em>}$</td>
<td style="text-align: center;">$61.7^{<em> </em>}$</td>
<td style="text-align: center;">$14.8^{<em> </em>}$</td>
<td style="text-align: center;">$2.5^{*}$</td>
</tr>
</tbody>
</table>
<p>Table 8: A copy of Table 2 with detailed notation of the statistic significant test results. In each column, statistically significant differences from the best result are indicated using * and ${ }^{<em> </em>}$ for $p&lt;0.05$ and $p&lt;0.01$ respectively.</p>
<h1>K The P(True) Case Study</h1>
<p>In $\S 6$ we used the $\mathrm{P}($ True $)$ metric from Kadavath et al. (2022) as a case study for comparison. In Figure 5 we compare our Unknown category vs classifying as Unknown based on a threshold of $\mathrm{P}($ True). We calculated $\mathrm{P}($ True $)$ for every $(q, a)$ pair in the test set using Kadavath et al. (2022)'s prompt:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Where</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">Paris</span><span class="w"> </span><span class="n">located</span><span class="o">?</span>
<span class="n">Proposed</span><span class="w"> </span><span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="n">France</span>
<span class="n">Is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">answer</span><span class="o">:</span>
<span class="w">    </span><span class="o">(</span><span class="n">A</span><span class="o">)</span><span class="w"> </span><span class="n">True</span>
<span class="w">    </span><span class="o">(</span><span class="n">B</span><span class="o">)</span><span class="w"> </span><span class="n">False</span>
<span class="n">The</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span><span class="o">:</span>
</code></pre></div>

<p>We then treated $(q, a)$ pairs with $\mathrm{P}($ True $)$ below a threshold as Unknown. We experimented with each possible threshold $T$ in $[0,1]$, according to our test set. For each threshold $T$ we then measured (1) how many examples were classified as Unknown out of the test set, (2) what was the accuracy on these examples after fine-tuning. We plot the results in Figure 5, where $\mathrm{P}($ True $)$ is represented with the yellow line and our Unknown is represented with the blue circle. As discussed in $\S \mathrm{C}$, it was approximated using 10 defferent samples of 4 -shot exemplars ( $N_{\text {ex }}=10$ ). We also check smaller values of $N_{\text {ex }}$ and plot the results with the blue line. The accuracy after fine-tuning for all the results is measured after fine-tuning with $D_{\text {Natural }}(\S 5)$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{20}$ https://www.wikidata.org/wiki/Property:P106&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{19}$ https://www.wikidata.org/wiki/Property:P106&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
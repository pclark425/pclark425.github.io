<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2616 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2616</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2616</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-46299fee72ca833337b3882ae1d8316f44b32b3c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/46299fee72ca833337b3882ae1d8316f44b32b3c" target="_blank">Reflexion: an autonomous agent with dynamic memory and self-reflection</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2616.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2616.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that improves LLM-based agents by converting scalar or binary environment feedback into natural-language self-reflections which are stored in episodic memory and used as contextual guidance in subsequent trials instead of updating model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reflexion is a modular language-agent framework with three components: an Actor (LLM) that generates actions and text, an Evaluator that scores trajectories (binary/scalar or LLM-based), and a Self-Reflection LLM that converts feedback + trajectories into concise natural-language experience summaries (sr_t) that are stored in a bounded long-term memory (mem). At each trial the Actor conditions on short-term trajectory history plus mem; after execution the Evaluator produces a reward and the Self-Reflection module produces a verbal lesson which is appended to mem. Reflexion does not fine-tune model weights; learning occurs by augmenting context with distilled natural-language feedback. It supports multiple evaluator types (heuristics, LLM classifiers, unit-test execution) and actor prompting variants (ReAct, Chain-of-Thought).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery / AI-Agent (LLM-based verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Natural language decision-making, multi-hop reasoning (QA), and program synthesis / code generation (interpreted & compiled languages).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Applied to (1) sequential decision-making in text-based interactive environments (ALFWorld) requiring multi-step action planning and recovery from early errors; (2) multi-hop reading-comprehension / reasoning (HotPotQA) requiring retrieval and reasoning over supporting documents; (3) code synthesis and debugging (HumanEval, MBPP, LeetcodeHardGym), where generated unit tests and execution feedback are used to drive iterative improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Varies by domain: ALFWorld: long-horizon discrete action sequences with branching trajectories and possible hallucinations (many possible object locations, long planning horizon). HotPotQA: reasoning over large textual contexts (multi-hop retrieval, large search space of documents). Programming: combinatorial code search spaces, syntactic and semantic correctness constraints; tasks include single-function generation and hard LeetCode problems. Quantitative results reported: 134 ALFWorld tasks (ReAct+Reflexion solved 130/134), HumanEval: pass@1 91.0% (Python), MBPP: pass@1 77.1% (Python), LeetcodeHard (Python): pass@1 15.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses existing benchmark datasets/environments (ALFWorld, HotPotQA, HumanEval, MBPP, LeetcodeHardGym). For programming, Reflexion also generates synthetic unit tests via LLM prompting (up to n=6 tests) and filters by AST validity. No additional labeled training data is required; all learning is done via episodic interaction and generated tests.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Lightweight relative to fine-tuning: no gradient-based model updates required. Main costs are LLM inference (multiple trials per task), test execution (for code), and maintaining prompt context; the paper gives no explicit compute-hour or dollar cost. Memory size for stored reflections is bounded (Ω typically 1-3) to respect LLM context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined episodic tasks with clear success signals in benchmarks (binary success/fail for ALFWorld and exact-match grading for HotPotQA; unit-test pass/fail for code). Problems are discrete (actions, code edits) with stochasticity from LLM sampling; evaluation metrics are explicit (task completion, exact match, pass@1). Domains require domain knowledge (environment affordances, retrieval API, programming language semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion rate (ALFWorld), exact-match accuracy (HotPotQA), pass@1 code-generation accuracy and derived TP/FN/FP/TN statistics (HumanEval/MBPP/LeetCode).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported improvements vs baselines: ALFWorld: ReAct+Reflexion solved 130/134 tasks (~97.0% completion) and showed an absolute +22 percentage-point improvement over strong baselines across 12 iterative trials; HotPotQA: up to +20% absolute improvement reported in paper summary (specific experimental setups report e.g. CoT(GT)+gpt-4 baseline 68% -> Reflexion 80% in one subset); HumanEval (Python) pass@1 = 91.0% (Reflexion) vs 80.1% (GPT-4 SOTA baseline) — +10.9 percentage points; MBPP (Python) Reflexion 77.1% vs GPT-4 80.1% (decrease); HumanEval (Rust) Reflexion 68% vs GPT-4 60% baseline in their constructed Rust subset; Leetcode Hard (Python) Reflexion 15.0% vs GPT-4 7.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Relies on quality of generated feedback/tests: flaky or incorrect generated unit tests can produce false positives (incorrect code passing tests) or false negatives (correct code failing invalid tests). Struggles on tasks requiring very broad, diverse exploration or highly creative escapes from local minima (example: WebShop e-commerce search tasks — Reflexion failed to improve significantly after a few trials). Also limited by LLM self-evaluation capacity and context window length (memory truncated to last 1–3 reflections). No formal convergence guarantees; susceptible to local minima in policy space.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Clear, informative environment feedback (e.g., executable tests in code domain) enables effective self-reflection; tasks with observable discrete signals and recoverable failure modes (ALFWorld, programming with tests) benefit most. Stronger LLM base models exhibit emergent ability to generate actionable self-corrections; bounded, distilled natural-language memory provides interpretable, reusable hints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reflexion consistently improved over baseline LLM prompting strategies across domains, with largest gains in ALFWorld (≈+22pp) and HumanEval Python (+~11pp vs GPT-4). In some settings (MBPP Python) Reflexion underperformed the GPT-4 baseline, attributed to higher false-positive rates in generated test suites for MBPP. Ablations showed both test generation AND the natural-language self-reflection step are necessary for best code results (omitting test generation or self-reflection reduced pass@1). On WebShop (e-commerce search) Reflexion did not outperform ReAct and failed to show improvement, illustrating domain-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human baselines are not directly measured on all tasks in this paper; comparisons are primarily to prior automated baselines (ReAct, CoT) and SOTA LLM baselines (GPT-4). Where available, Reflexion surpasses GPT-4 on HumanEval Python (91.0% vs 80.1%). No explicit human researcher performance numbers provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2616.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that interleaves chain-of-thought style reasoning traces with environment actions to enable LLMs to act in interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ReAct is used as the Actor prompting strategy in Reflexion experiments; it produces explicit intermediate thoughts and actions which are executed in the environment. In this paper ReAct serves as the baseline action-generation policy for ALFWorld and for some HotPotQA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-agent prompting framework / Action-generation policy</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Interactive text-based decision-making (ALFWorld) and retrieval + reasoning tasks (HotPotQA).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate action sequences and intermediate reasoning steps for multi-step tasks requiring environment interactions and retrieval. In ALFWorld, ReAct provides few-shot examples to produce long trajectories of actions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Long-horizon discrete action spaces; credit assignment across long trajectories is challenging (hallucination problem).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates with few-shot in-context examples; uses environment observations during trials; no additional training data required.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference per step; used with GPT-family models (gpt-3.5, gpt-4) in experiments. No fine-tuning required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Episodic, discrete-action tasks with binary completion signals; stochasticity from LLM sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion rate in ALFWorld; improvements over baseline behavior across trials when combined with Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>As baseline (ReAct-only) the paper reports convergence with a hallucination rate of ~22% on ALFWorld and poorer trial-to-trial recovery (unable to reach near-perfect performance); when combined with Reflexion, ReAct+Reflexion solved 130/134 ALFWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Prone to hallucinations (believing it possesses objects when it does not) and inability to backtrack in long trajectories; performance plateaus after a few trials without long-term memory/self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit intermediate thoughts enable better action justification and can be improved by adding distilled self-reflections to provide long-term corrective hints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>ReAct+Reflexion far outperforms ReAct-only on ALFWorld and HotPotQA; ReAct-only converged earlier and had persistent hallucination failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No human baseline reported for ALFWorld comparisons in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2616.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative framework where LLMs refine their own generations using self-evaluation and editing steps, effective for single-generation reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as prior related work: Self-Refine uses LLM self-evaluation and iterative rewriting to improve single-shot text generations by conditioning on self-feedback; unlike Reflexion it is limited to single-generation tasks and does not maintain a persisting episodic memory across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Iterative self-improvement / self-evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Single-generation natural language reasoning and writing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve a single generated output by repeatedly evaluating and refining it under task-specific constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Lower complexity than multi-episode RL-style tasks; limited to single-output optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses the LLM's own generations and prompts; no external training data required.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Multiple LLM inference passes per example for evaluation and rewriting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Single-shot generation with refinement loop; well-defined evaluation criteria per generation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Quality improvements on generation tasks (not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper; cited as effective but limited in scope relative to Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited to single-generation tasks and lacks long-term memory across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Works where self-evaluation can reliably identify local defects in a single output.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper argues Reflexion extends beyond Self-Refine by supporting multi-episode memory and action-oriented tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2616.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaCode</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaCode (Competition-level code generation with AlphaCode)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for competition-level program synthesis that generates many candidate programs and filters them with test cases to achieve high performance on coding competitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Competition-level code generation with alphacode</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaCode</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited in related work for program synthesis approaches that evaluate generated candidates on hidden testcases; AlphaCode generates many candidates then ranks/filters them using execution on tests. Unlike Reflexion, AlphaCode relies on large candidate pools and hidden tests and does not use self-reflective episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated program synthesis / candidate generation & filtering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Competition-level programming problems.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate program candidates and select correct solutions by execution against testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High combinatorial search across program space; requires many candidates and robust ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Standard coding benchmarks and hidden testcases used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Large-scale sampling and execution of many candidates; significant compute.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined test-driven evaluation but requires production of many program variants.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Competition success rates / pass@k; exact metrics described in AlphaCode but not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Relies on hidden tests; may not provide interpretable self-improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Extensive candidate sampling and effective ranking/selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper positions Reflexion as different: Reflexion focuses on iterative single-sample self-improvement via natural-language feedback rather than generating huge candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2616.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeT: Code Generation with Generated Tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that generates unit tests alongside code to score and select candidate implementations without access to hidden testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codet: Code generation with generated tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as related work for generating unit tests to evaluate code generations; CodeT generates tests but (according to the paper) does not implement a self-learning step to iteratively improve code writing via stored self-reflections as Reflexion does.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated test generation for program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Program synthesis / code generation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate internal tests to evaluate candidate code when hidden tests are not available.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Depends on the richness/diversity of generated tests; test quality directly affects selection.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates without hidden testcases; relies on model-generated tests.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference for test & code generation; execution of tests to filter candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Test-driven evaluation with potential for flaky/incorrect tests.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Pass rates on public benchmarks; specifics not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Can produce insufficient or incorrect tests, leading to false positives/negatives in candidate evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality and diversity of generated tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reflexion extends the idea by coupling test-generation with verbal self-reflection and an iterative memory to refine implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2616.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Debugging: Teaching Large Language Models to Self-Debug</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that adds a debugging component to LLM-based code generation that uses execution feedback to iteratively fix implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Debugging</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as related work in program debugging: Self-Debugging uses execution feedback to revise implementations; differs from Reflexion by not maintaining long-term verbal episodic memory linking error identification to improvement across repeated episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated debugging / iterative code repair</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Program synthesis and debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Use execution traces and feedback to edit and fix code until tests pass.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handles syntactic and logic bugs in relatively less-complex programs; effectiveness decreases for harder program synthesis tasks according to related work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses runtime execution outputs and testcases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Multiple LLM inference rounds and program executions per example.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Closed-loop debugging with clear pass/fail signals from tests.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Ability to repair programs and increase test pass rates; exact numbers not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May be less effective on complex program bugs and lacks persisting self-reflective memory to generalize across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Clear execution feedback and targeted debugging hints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper argues Reflexion's explicit natural-language memory bridges error identification and implementation improvement more effectively on harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2616.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeRL: Mastering Code Generation through RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that frames program synthesis as a reinforcement learning problem (actor-critic) to iteratively debug programs given execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Coderl: Mastering code generation through pretrained models and deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeRL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as prior art using reinforcement learning (actor-critic) with execution feedback to debug programs. Unlike Reflexion, CodeRL uses gradient-based RL objectives and model updates rather than purely contextual natural-language memory.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Reinforcement learning for program synthesis / Automated debugging</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Program synthesis and debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Optimize code-generating policies using RL signals derived from execution/test feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional policy optimization over program-space; requires many environment interactions and reward shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses code benchmarks and execution feedback; training is data and compute intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Substantial compute for RL training and actor-critic updates; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Continuous optimization over policy parameters, typically heavy compute and large sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Pass@k metrics and improvement through RL training; specific numbers not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High sample complexity, compute cost, and difficulty in credit assignment; may not be interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Effective reward shaping and access to execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reflexion described as a lightweight alternative that avoids costly fine-tuning by using natural-language feedback instead of parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2616.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised approach to teach LMs to call external tools by inserting API calls into pretraining corpora and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as related work demonstrating LLMs' capacity to learn tool-use; differs from Reflexion in that Toolformer focuses on augmenting LM pretraining/fine-tuning to integrate tools, whereas Reflexion uses episodic natural-language feedback at inference time to guide behavior without weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Tool-use / self-supervised tool integration</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General language tasks requiring tool/APIs usage.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Teach models to insert and use external tool calls for improved task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Requires designing self-supervised objectives and corpora augmentation; not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires large-scale pretraining data and API logs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Fine-tuning/pretraining scale compute not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined integration of deterministic tool calls with text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task performance improvement when using tools; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of tool APIs and relevant data for self-supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reflexion does not require fine-tuning and is complementary to tool-use approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2616.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HuggingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HuggingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system orchestrating multiple specialized models (from HuggingFace) under a language-model controller to solve complex AI tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HuggingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as an approach to orchestrate multiple models/tools controlled by an LLM; contrasted with Reflexion's focus on episodic self-reflection rather than multi-model orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-model orchestration / language-model controller</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General multimodal AI tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Route tasks to specialized models and aggregate results under LLM control.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Complex pipeline orchestration, multiple model interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on constituent models and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Varies depending on invoked models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Modular task decomposition and routing.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion across multimodal tasks; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Diversity and competency of component models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reflexion emphasizes iterative linguistic reinforcement and episodic memory rather than model orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2616.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT: Browser-assisted Question-Answering with Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent that uses a browser to retrieve information and cites sources, trained with human feedback to improve answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Webgpt: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a precedent for LLMs interacting with external environments (the web) and using external retrieval; contrasted with Reflexion's method of learning from self-generated verbal feedback rather than human RLHF at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM agent with web-retrieval and human-feedback training</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Open-domain question answering with web access.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Retrieve, synthesize, and cite web content to answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Large retrieval/search space, relevance ranking challenges, source attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Web-scale retrieval; requires browsing infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Multiple retrieval and generation steps; training with human feedback is costly.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended retrieval + generation with qualitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Answer quality and citation correctness under human evaluation; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality retrieval and human preference data for training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reflexion aims for interpretable verbal reinforcement instead of large-scale RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2616.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebShop (failure case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebShop (e-commerce navigation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A web-based e-commerce task where agents must search and purchase products given customer requests; used in this paper to illustrate a domain where Reflexion struggled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct + Reflexion (on WebShop)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applied Reflexion (with ReAct actor) to WebShop tasks: two-shot ReAct+Reflexion agent was run in 100 environments but failed to show improvement after four trials and did not produce helpful self-reflections, indicating difficulty escaping local minima and generating diverse search strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-agent evaluation (failure case)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>E-commerce product search/navigation (web interaction).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given ambiguous natural-language requests, the agent must locate and purchase the correct product on an e-commerce site, a task that requires diverse, precise search queries and exploration of ambiguous search results.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very large, ambiguous search space; requires diverse exploration strategies and precision in search queries; high combinatorial branching and need for creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Interactive web environment with many possible products and pages; environment observations present actions but require precise queries.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference over web-retrieval and environment actions; experiments terminated early due to lack of improvement (no explicit compute numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, ambiguous retrieval/search with high requirement for diverse exploration; not well-suited to short episodic memory of Reflexion as implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task-specific completion rate on WebShop environments; Reflexion did not improve over ReAct baseline in 100 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No significant improvement; ReAct+Reflexion failed to significantly outperform ReAct on 100 customer shopping requests (no percentage improvement reported).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Insufficient diversity in generated self-reflections; inability to generate creative exploration strategies; early termination after 4 trials due to stagnation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>N/A in this paper — this was presented as a counterexample where Reflexion did not succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Contrasts with success on ALFWorld and HotPotQA; highlights domain dependence of Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2616.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2616.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability large language model used in this paper as the primary baseline model for many experiments and comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the underlying LLM for Actors, Evaluators, and Self-Reflection modules in many experiments; served as a strong baseline (SOTA) for code generation and reasoning tasks against which Reflexion performance was compared.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large language model (LLM) used as agent core</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>All evaluated domains (decision-making, reasoning, programming).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provides language generation for Actor and Self-Reflection and can be used as an Evaluator; its base single-shot performance is compared to Reflexion-augmented runs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-capacity model enabling more reliable self-evaluation and generation of corrective reflections; experiments indicate stronger models show emergent self-correction abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Pretrained model; no task-specific fine-tuning in Reflexion experiments (mostly zero- or few-shot prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Inference cost per call; Reflexion relies on multiple inference calls per trial but avoids fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Used within episodic interactive tasks where success signals are explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Baseline accuracy / pass@1 on benchmarks (e.g., GPT-4 HumanEval pass@1 80.1% reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Baseline reported values: HumanEval (PY) pass@1 80.1 (GPT-4); HumanEval (RS) 60.0; MBPP (PY) 80.1; MBPP (RS) 70.9; Leetcode Hard (PY) 7.5 (GPT-4 baseline). Reflexion sometimes outperforms GPT-4 (e.g., HumanEval PY Reflexion 91.0 vs GPT-4 80.1) and sometimes not (MBPP PY).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>As a baseline single-shot agent, GPT-4 lacks episodic memory unless augmented; may benefit from Reflexion's iterative memory-based prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Model size and capability strongly correlate with emergent self-correction and effective self-reflection generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reflexion augmented GPT-4 to surpass GPT-4 single-shot baselines in several benchmarks, indicating significant gains from verbal reinforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Competition-level code generation with alphacode <em>(Rating: 2)</em></li>
                <li>Codet: Code generation with generated tests <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
                <li>Webgpt: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>Coderl: Mastering code generation through pretrained models and deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2616",
    "paper_id": "paper-46299fee72ca833337b3882ae1d8316f44b32b3c",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "brief_description": "A framework that improves LLM-based agents by converting scalar or binary environment feedback into natural-language self-reflections which are stored in episodic memory and used as contextual guidance in subsequent trials instead of updating model weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Reflexion",
            "system_description": "Reflexion is a modular language-agent framework with three components: an Actor (LLM) that generates actions and text, an Evaluator that scores trajectories (binary/scalar or LLM-based), and a Self-Reflection LLM that converts feedback + trajectories into concise natural-language experience summaries (sr_t) that are stored in a bounded long-term memory (mem). At each trial the Actor conditions on short-term trajectory history plus mem; after execution the Evaluator produces a reward and the Self-Reflection module produces a verbal lesson which is appended to mem. Reflexion does not fine-tune model weights; learning occurs by augmenting context with distilled natural-language feedback. It supports multiple evaluator types (heuristics, LLM classifiers, unit-test execution) and actor prompting variants (ReAct, Chain-of-Thought).",
            "system_type": "Automated Discovery / AI-Agent (LLM-based verbal reinforcement learning)",
            "problem_domain": "Natural language decision-making, multi-hop reasoning (QA), and program synthesis / code generation (interpreted & compiled languages).",
            "problem_description": "Applied to (1) sequential decision-making in text-based interactive environments (ALFWorld) requiring multi-step action planning and recovery from early errors; (2) multi-hop reading-comprehension / reasoning (HotPotQA) requiring retrieval and reasoning over supporting documents; (3) code synthesis and debugging (HumanEval, MBPP, LeetcodeHardGym), where generated unit tests and execution feedback are used to drive iterative improvements.",
            "problem_complexity": "Varies by domain: ALFWorld: long-horizon discrete action sequences with branching trajectories and possible hallucinations (many possible object locations, long planning horizon). HotPotQA: reasoning over large textual contexts (multi-hop retrieval, large search space of documents). Programming: combinatorial code search spaces, syntactic and semantic correctness constraints; tasks include single-function generation and hard LeetCode problems. Quantitative results reported: 134 ALFWorld tasks (ReAct+Reflexion solved 130/134), HumanEval: pass@1 91.0% (Python), MBPP: pass@1 77.1% (Python), LeetcodeHard (Python): pass@1 15.0%.",
            "data_availability": "Uses existing benchmark datasets/environments (ALFWorld, HotPotQA, HumanEval, MBPP, LeetcodeHardGym). For programming, Reflexion also generates synthetic unit tests via LLM prompting (up to n=6 tests) and filters by AST validity. No additional labeled training data is required; all learning is done via episodic interaction and generated tests.",
            "computational_requirements": "Lightweight relative to fine-tuning: no gradient-based model updates required. Main costs are LLM inference (multiple trials per task), test execution (for code), and maintaining prompt context; the paper gives no explicit compute-hour or dollar cost. Memory size for stored reflections is bounded (Ω typically 1-3) to respect LLM context limits.",
            "problem_structure": "Well-defined episodic tasks with clear success signals in benchmarks (binary success/fail for ALFWorld and exact-match grading for HotPotQA; unit-test pass/fail for code). Problems are discrete (actions, code edits) with stochasticity from LLM sampling; evaluation metrics are explicit (task completion, exact match, pass@1). Domains require domain knowledge (environment affordances, retrieval API, programming language semantics).",
            "success_metric": "Task completion rate (ALFWorld), exact-match accuracy (HotPotQA), pass@1 code-generation accuracy and derived TP/FN/FP/TN statistics (HumanEval/MBPP/LeetCode).",
            "success_rate": "Reported improvements vs baselines: ALFWorld: ReAct+Reflexion solved 130/134 tasks (~97.0% completion) and showed an absolute +22 percentage-point improvement over strong baselines across 12 iterative trials; HotPotQA: up to +20% absolute improvement reported in paper summary (specific experimental setups report e.g. CoT(GT)+gpt-4 baseline 68% -&gt; Reflexion 80% in one subset); HumanEval (Python) pass@1 = 91.0% (Reflexion) vs 80.1% (GPT-4 SOTA baseline) — +10.9 percentage points; MBPP (Python) Reflexion 77.1% vs GPT-4 80.1% (decrease); HumanEval (Rust) Reflexion 68% vs GPT-4 60% baseline in their constructed Rust subset; Leetcode Hard (Python) Reflexion 15.0% vs GPT-4 7.5%.",
            "failure_modes": "Relies on quality of generated feedback/tests: flaky or incorrect generated unit tests can produce false positives (incorrect code passing tests) or false negatives (correct code failing invalid tests). Struggles on tasks requiring very broad, diverse exploration or highly creative escapes from local minima (example: WebShop e-commerce search tasks — Reflexion failed to improve significantly after a few trials). Also limited by LLM self-evaluation capacity and context window length (memory truncated to last 1–3 reflections). No formal convergence guarantees; susceptible to local minima in policy space.",
            "success_factors": "Clear, informative environment feedback (e.g., executable tests in code domain) enables effective self-reflection; tasks with observable discrete signals and recoverable failure modes (ALFWorld, programming with tests) benefit most. Stronger LLM base models exhibit emergent ability to generate actionable self-corrections; bounded, distilled natural-language memory provides interpretable, reusable hints.",
            "comparative_results": "Reflexion consistently improved over baseline LLM prompting strategies across domains, with largest gains in ALFWorld (≈+22pp) and HumanEval Python (+~11pp vs GPT-4). In some settings (MBPP Python) Reflexion underperformed the GPT-4 baseline, attributed to higher false-positive rates in generated test suites for MBPP. Ablations showed both test generation AND the natural-language self-reflection step are necessary for best code results (omitting test generation or self-reflection reduced pass@1). On WebShop (e-commerce search) Reflexion did not outperform ReAct and failed to show improvement, illustrating domain-dependence.",
            "human_baseline": "Human baselines are not directly measured on all tasks in this paper; comparisons are primarily to prior automated baselines (ReAct, CoT) and SOTA LLM baselines (GPT-4). Where available, Reflexion surpasses GPT-4 on HumanEval Python (91.0% vs 80.1%). No explicit human researcher performance numbers provided.",
            "uuid": "e2616.0"
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "brief_description": "A prompting framework that interleaves chain-of-thought style reasoning traces with environment actions to enable LLMs to act in interactive environments.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "system_name": "ReAct",
            "system_description": "ReAct is used as the Actor prompting strategy in Reflexion experiments; it produces explicit intermediate thoughts and actions which are executed in the environment. In this paper ReAct serves as the baseline action-generation policy for ALFWorld and for some HotPotQA experiments.",
            "system_type": "LLM-agent prompting framework / Action-generation policy",
            "problem_domain": "Interactive text-based decision-making (ALFWorld) and retrieval + reasoning tasks (HotPotQA).",
            "problem_description": "Generate action sequences and intermediate reasoning steps for multi-step tasks requiring environment interactions and retrieval. In ALFWorld, ReAct provides few-shot examples to produce long trajectories of actions.",
            "problem_complexity": "Long-horizon discrete action spaces; credit assignment across long trajectories is challenging (hallucination problem).",
            "data_availability": "Operates with few-shot in-context examples; uses environment observations during trials; no additional training data required.",
            "computational_requirements": "LLM inference per step; used with GPT-family models (gpt-3.5, gpt-4) in experiments. No fine-tuning required.",
            "problem_structure": "Episodic, discrete-action tasks with binary completion signals; stochasticity from LLM sampling.",
            "success_metric": "Task completion rate in ALFWorld; improvements over baseline behavior across trials when combined with Reflexion.",
            "success_rate": "As baseline (ReAct-only) the paper reports convergence with a hallucination rate of ~22% on ALFWorld and poorer trial-to-trial recovery (unable to reach near-perfect performance); when combined with Reflexion, ReAct+Reflexion solved 130/134 ALFWorld tasks.",
            "failure_modes": "Prone to hallucinations (believing it possesses objects when it does not) and inability to backtrack in long trajectories; performance plateaus after a few trials without long-term memory/self-reflection.",
            "success_factors": "Explicit intermediate thoughts enable better action justification and can be improved by adding distilled self-reflections to provide long-term corrective hints.",
            "comparative_results": "ReAct+Reflexion far outperforms ReAct-only on ALFWorld and HotPotQA; ReAct-only converged earlier and had persistent hallucination failure modes.",
            "human_baseline": "No human baseline reported for ALFWorld comparisons in this paper.",
            "uuid": "e2616.1"
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback",
            "brief_description": "An iterative framework where LLMs refine their own generations using self-evaluation and editing steps, effective for single-generation reasoning tasks.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "system_name": "Self-Refine",
            "system_description": "Referenced as prior related work: Self-Refine uses LLM self-evaluation and iterative rewriting to improve single-shot text generations by conditioning on self-feedback; unlike Reflexion it is limited to single-generation tasks and does not maintain a persisting episodic memory across trials.",
            "system_type": "Iterative self-improvement / self-evaluation framework",
            "problem_domain": "Single-generation natural language reasoning and writing tasks.",
            "problem_description": "Improve a single generated output by repeatedly evaluating and refining it under task-specific constraints.",
            "problem_complexity": "Lower complexity than multi-episode RL-style tasks; limited to single-output optimization.",
            "data_availability": "Uses the LLM's own generations and prompts; no external training data required.",
            "computational_requirements": "Multiple LLM inference passes per example for evaluation and rewriting.",
            "problem_structure": "Single-shot generation with refinement loop; well-defined evaluation criteria per generation.",
            "success_metric": "Quality improvements on generation tasks (not quantified in this paper).",
            "success_rate": "Not quantified in this paper; cited as effective but limited in scope relative to Reflexion.",
            "failure_modes": "Limited to single-generation tasks and lacks long-term memory across episodes.",
            "success_factors": "Works where self-evaluation can reliably identify local defects in a single output.",
            "comparative_results": "Paper argues Reflexion extends beyond Self-Refine by supporting multi-episode memory and action-oriented tasks.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.2"
        },
        {
            "name_short": "AlphaCode",
            "name_full": "AlphaCode (Competition-level code generation with AlphaCode)",
            "brief_description": "A system for competition-level program synthesis that generates many candidate programs and filters them with test cases to achieve high performance on coding competitions.",
            "citation_title": "Competition-level code generation with alphacode",
            "mention_or_use": "mention",
            "system_name": "AlphaCode",
            "system_description": "Cited in related work for program synthesis approaches that evaluate generated candidates on hidden testcases; AlphaCode generates many candidates then ranks/filters them using execution on tests. Unlike Reflexion, AlphaCode relies on large candidate pools and hidden tests and does not use self-reflective episodic memory.",
            "system_type": "Automated program synthesis / candidate generation & filtering",
            "problem_domain": "Competition-level programming problems.",
            "problem_description": "Generate program candidates and select correct solutions by execution against testcases.",
            "problem_complexity": "High combinatorial search across program space; requires many candidates and robust ranking.",
            "data_availability": "Standard coding benchmarks and hidden testcases used for evaluation.",
            "computational_requirements": "Large-scale sampling and execution of many candidates; significant compute.",
            "problem_structure": "Well-defined test-driven evaluation but requires production of many program variants.",
            "success_metric": "Competition success rates / pass@k; exact metrics described in AlphaCode but not reproduced here.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Relies on hidden tests; may not provide interpretable self-improvements.",
            "success_factors": "Extensive candidate sampling and effective ranking/selection.",
            "comparative_results": "Paper positions Reflexion as different: Reflexion focuses on iterative single-sample self-improvement via natural-language feedback rather than generating huge candidate pools.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2616.3"
        },
        {
            "name_short": "CodeT",
            "name_full": "CodeT: Code Generation with Generated Tests",
            "brief_description": "A method that generates unit tests alongside code to score and select candidate implementations without access to hidden testcases.",
            "citation_title": "Codet: Code generation with generated tests",
            "mention_or_use": "mention",
            "system_name": "CodeT",
            "system_description": "Referenced as related work for generating unit tests to evaluate code generations; CodeT generates tests but (according to the paper) does not implement a self-learning step to iteratively improve code writing via stored self-reflections as Reflexion does.",
            "system_type": "Automated test generation for program synthesis",
            "problem_domain": "Program synthesis / code generation benchmarks.",
            "problem_description": "Generate internal tests to evaluate candidate code when hidden tests are not available.",
            "problem_complexity": "Depends on the richness/diversity of generated tests; test quality directly affects selection.",
            "data_availability": "Operates without hidden testcases; relies on model-generated tests.",
            "computational_requirements": "LLM inference for test & code generation; execution of tests to filter candidates.",
            "problem_structure": "Test-driven evaluation with potential for flaky/incorrect tests.",
            "success_metric": "Pass rates on public benchmarks; specifics not reproduced in this paper.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Can produce insufficient or incorrect tests, leading to false positives/negatives in candidate evaluation.",
            "success_factors": "Quality and diversity of generated tests.",
            "comparative_results": "Reflexion extends the idea by coupling test-generation with verbal self-reflection and an iterative memory to refine implementations.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.4"
        },
        {
            "name_short": "Self-Debugging",
            "name_full": "Self-Debugging: Teaching Large Language Models to Self-Debug",
            "brief_description": "A framework that adds a debugging component to LLM-based code generation that uses execution feedback to iteratively fix implementations.",
            "citation_title": "Teaching large language models to self-debug",
            "mention_or_use": "mention",
            "system_name": "Self-Debugging",
            "system_description": "Cited as related work in program debugging: Self-Debugging uses execution feedback to revise implementations; differs from Reflexion by not maintaining long-term verbal episodic memory linking error identification to improvement across repeated episodes.",
            "system_type": "Automated debugging / iterative code repair",
            "problem_domain": "Program synthesis and debugging.",
            "problem_description": "Use execution traces and feedback to edit and fix code until tests pass.",
            "problem_complexity": "Handles syntactic and logic bugs in relatively less-complex programs; effectiveness decreases for harder program synthesis tasks according to related work discussion.",
            "data_availability": "Uses runtime execution outputs and testcases.",
            "computational_requirements": "Multiple LLM inference rounds and program executions per example.",
            "problem_structure": "Closed-loop debugging with clear pass/fail signals from tests.",
            "success_metric": "Ability to repair programs and increase test pass rates; exact numbers not given here.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "May be less effective on complex program bugs and lacks persisting self-reflective memory to generalize across episodes.",
            "success_factors": "Clear execution feedback and targeted debugging hints.",
            "comparative_results": "Paper argues Reflexion's explicit natural-language memory bridges error identification and implementation improvement more effectively on harder tasks.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.5"
        },
        {
            "name_short": "CodeRL",
            "name_full": "CodeRL: Mastering Code Generation through RL",
            "brief_description": "An approach that frames program synthesis as a reinforcement learning problem (actor-critic) to iteratively debug programs given execution feedback.",
            "citation_title": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "CodeRL",
            "system_description": "Referenced as prior art using reinforcement learning (actor-critic) with execution feedback to debug programs. Unlike Reflexion, CodeRL uses gradient-based RL objectives and model updates rather than purely contextual natural-language memory.",
            "system_type": "Reinforcement learning for program synthesis / Automated debugging",
            "problem_domain": "Program synthesis and debugging.",
            "problem_description": "Optimize code-generating policies using RL signals derived from execution/test feedback.",
            "problem_complexity": "High-dimensional policy optimization over program-space; requires many environment interactions and reward shaping.",
            "data_availability": "Uses code benchmarks and execution feedback; training is data and compute intensive.",
            "computational_requirements": "Substantial compute for RL training and actor-critic updates; not quantified in this paper.",
            "problem_structure": "Continuous optimization over policy parameters, typically heavy compute and large sample complexity.",
            "success_metric": "Pass@k metrics and improvement through RL training; specific numbers not provided here.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "High sample complexity, compute cost, and difficulty in credit assignment; may not be interpretable.",
            "success_factors": "Effective reward shaping and access to execution feedback.",
            "comparative_results": "Reflexion described as a lightweight alternative that avoids costly fine-tuning by using natural-language feedback instead of parameter updates.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.6"
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "brief_description": "A self-supervised approach to teach LMs to call external tools by inserting API calls into pretraining corpora and fine-tuning.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "system_name": "Toolformer",
            "system_description": "Cited as related work demonstrating LLMs' capacity to learn tool-use; differs from Reflexion in that Toolformer focuses on augmenting LM pretraining/fine-tuning to integrate tools, whereas Reflexion uses episodic natural-language feedback at inference time to guide behavior without weight updates.",
            "system_type": "Tool-use / self-supervised tool integration",
            "problem_domain": "General language tasks requiring tool/APIs usage.",
            "problem_description": "Teach models to insert and use external tool calls for improved task performance.",
            "problem_complexity": "Requires designing self-supervised objectives and corpora augmentation; not evaluated here.",
            "data_availability": "Requires large-scale pretraining data and API logs.",
            "computational_requirements": "Fine-tuning/pretraining scale compute not detailed here.",
            "problem_structure": "Well-defined integration of deterministic tool calls with text generation.",
            "success_metric": "Task performance improvement when using tools; not quantified in this paper.",
            "success_rate": "Not reported here.",
            "failure_modes": "Not discussed in this paper.",
            "success_factors": "Availability of tool APIs and relevant data for self-supervision.",
            "comparative_results": "Reflexion does not require fine-tuning and is complementary to tool-use approaches.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.7"
        },
        {
            "name_short": "HuggingGPT",
            "name_full": "HuggingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace",
            "brief_description": "A system orchestrating multiple specialized models (from HuggingFace) under a language-model controller to solve complex AI tasks.",
            "citation_title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "mention_or_use": "mention",
            "system_name": "HuggingGPT",
            "system_description": "Mentioned in related work as an approach to orchestrate multiple models/tools controlled by an LLM; contrasted with Reflexion's focus on episodic self-reflection rather than multi-model orchestration.",
            "system_type": "Multi-model orchestration / language-model controller",
            "problem_domain": "General multimodal AI tasks.",
            "problem_description": "Route tasks to specialized models and aggregate results under LLM control.",
            "problem_complexity": "Complex pipeline orchestration, multiple model interfaces.",
            "data_availability": "Depends on constituent models and datasets.",
            "computational_requirements": "Varies depending on invoked models.",
            "problem_structure": "Modular task decomposition and routing.",
            "success_metric": "Task completion across multimodal tasks; not quantified here.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Not discussed in this paper.",
            "success_factors": "Diversity and competency of component models.",
            "comparative_results": "Reflexion emphasizes iterative linguistic reinforcement and episodic memory rather than model orchestration.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.8"
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT: Browser-assisted Question-Answering with Human Feedback",
            "brief_description": "An LLM-based agent that uses a browser to retrieve information and cites sources, trained with human feedback to improve answer quality.",
            "citation_title": "Webgpt: Browser-assisted question-answering with human feedback",
            "mention_or_use": "mention",
            "system_name": "WebGPT",
            "system_description": "Referenced as a precedent for LLMs interacting with external environments (the web) and using external retrieval; contrasted with Reflexion's method of learning from self-generated verbal feedback rather than human RLHF at scale.",
            "system_type": "LLM agent with web-retrieval and human-feedback training",
            "problem_domain": "Open-domain question answering with web access.",
            "problem_description": "Retrieve, synthesize, and cite web content to answer questions.",
            "problem_complexity": "Large retrieval/search space, relevance ranking challenges, source attribution.",
            "data_availability": "Web-scale retrieval; requires browsing infrastructure.",
            "computational_requirements": "Multiple retrieval and generation steps; training with human feedback is costly.",
            "problem_structure": "Open-ended retrieval + generation with qualitative evaluation.",
            "success_metric": "Answer quality and citation correctness under human evaluation; not quantified in this paper.",
            "success_rate": "Not reported here.",
            "failure_modes": "Not discussed in this paper.",
            "success_factors": "High-quality retrieval and human preference data for training.",
            "comparative_results": "Reflexion aims for interpretable verbal reinforcement instead of large-scale RLHF.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.9"
        },
        {
            "name_short": "WebShop (failure case)",
            "name_full": "WebShop (e-commerce navigation benchmark)",
            "brief_description": "A web-based e-commerce task where agents must search and purchase products given customer requests; used in this paper to illustrate a domain where Reflexion struggled.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReAct + Reflexion (on WebShop)",
            "system_description": "Applied Reflexion (with ReAct actor) to WebShop tasks: two-shot ReAct+Reflexion agent was run in 100 environments but failed to show improvement after four trials and did not produce helpful self-reflections, indicating difficulty escaping local minima and generating diverse search strategies.",
            "system_type": "LLM-agent evaluation (failure case)",
            "problem_domain": "E-commerce product search/navigation (web interaction).",
            "problem_description": "Given ambiguous natural-language requests, the agent must locate and purchase the correct product on an e-commerce site, a task that requires diverse, precise search queries and exploration of ambiguous search results.",
            "problem_complexity": "Very large, ambiguous search space; requires diverse exploration strategies and precision in search queries; high combinatorial branching and need for creativity.",
            "data_availability": "Interactive web environment with many possible products and pages; environment observations present actions but require precise queries.",
            "computational_requirements": "LLM inference over web-retrieval and environment actions; experiments terminated early due to lack of improvement (no explicit compute numbers).",
            "problem_structure": "Open-ended, ambiguous retrieval/search with high requirement for diverse exploration; not well-suited to short episodic memory of Reflexion as implemented.",
            "success_metric": "Task-specific completion rate on WebShop environments; Reflexion did not improve over ReAct baseline in 100 trials.",
            "success_rate": "No significant improvement; ReAct+Reflexion failed to significantly outperform ReAct on 100 customer shopping requests (no percentage improvement reported).",
            "failure_modes": "Insufficient diversity in generated self-reflections; inability to generate creative exploration strategies; early termination after 4 trials due to stagnation.",
            "success_factors": "N/A in this paper — this was presented as a counterexample where Reflexion did not succeed.",
            "comparative_results": "Contrasts with success on ALFWorld and HotPotQA; highlights domain dependence of Reflexion.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.10"
        },
        {
            "name_short": "GPT-4 (baseline LLM)",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A high-capability large language model used in this paper as the primary baseline model for many experiments and comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GPT-4",
            "system_description": "Used as the underlying LLM for Actors, Evaluators, and Self-Reflection modules in many experiments; served as a strong baseline (SOTA) for code generation and reasoning tasks against which Reflexion performance was compared.",
            "system_type": "Large language model (LLM) used as agent core",
            "problem_domain": "All evaluated domains (decision-making, reasoning, programming).",
            "problem_description": "Provides language generation for Actor and Self-Reflection and can be used as an Evaluator; its base single-shot performance is compared to Reflexion-augmented runs.",
            "problem_complexity": "High-capacity model enabling more reliable self-evaluation and generation of corrective reflections; experiments indicate stronger models show emergent self-correction abilities.",
            "data_availability": "Pretrained model; no task-specific fine-tuning in Reflexion experiments (mostly zero- or few-shot prompting).",
            "computational_requirements": "Inference cost per call; Reflexion relies on multiple inference calls per trial but avoids fine-tuning.",
            "problem_structure": "Used within episodic interactive tasks where success signals are explicit.",
            "success_metric": "Baseline accuracy / pass@1 on benchmarks (e.g., GPT-4 HumanEval pass@1 80.1% reported in paper).",
            "success_rate": "Baseline reported values: HumanEval (PY) pass@1 80.1 (GPT-4); HumanEval (RS) 60.0; MBPP (PY) 80.1; MBPP (RS) 70.9; Leetcode Hard (PY) 7.5 (GPT-4 baseline). Reflexion sometimes outperforms GPT-4 (e.g., HumanEval PY Reflexion 91.0 vs GPT-4 80.1) and sometimes not (MBPP PY).",
            "failure_modes": "As a baseline single-shot agent, GPT-4 lacks episodic memory unless augmented; may benefit from Reflexion's iterative memory-based prompting.",
            "success_factors": "Model size and capability strongly correlate with emergent self-correction and effective self-reflection generation.",
            "comparative_results": "Reflexion augmented GPT-4 to surpass GPT-4 single-shot baselines in several benchmarks, indicating significant gains from verbal reinforcement.",
            "human_baseline": "Not provided.",
            "uuid": "e2616.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Competition-level code generation with alphacode",
            "rating": 2
        },
        {
            "paper_title": "Codet: Code generation with generated tests",
            "rating": 2
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        },
        {
            "paper_title": "Webgpt: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.020440999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reflexion: Language Agents with Verbal Reinforcement Learning</h1>
<p>Noah Shinn<br>Northeastern University<br>noahshinn024@gmail.com</p>
<p>Edward Berman<br>Northeastern University<br>berman.ed@northeastern.edu</p>
<p>Karthik Narasimhan<br>Princeton University<br>karthikn@princeton.edu</p>
<p>Federico Cassano<br>Northeastern University<br>cassano.f@northeastern.edu<br>Ashwin Gopinath<br>Massachusetts Institute of Technology<br>agopi@mit.edu<br>Shunyu Yao<br>Princeton University<br>shunyuy@princeton.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a $91 \%$ pass@ 1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves $80 \%$. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. We release all code, demos, and datasets at https://github.com/noahshinn024/reflexion.</p>
<h2>1 Introduction</h2>
<p>Recent works such as ReAct [30], SayCan [1], Toolformer [22], HuggingGPT [23], generative agents [19], and WebGPT [17] have demonstrated the feasibility of autonomous decision-making agents that are built on top of a large language model (LLM) core. These methods use LLMs to generate text and 'actions' that can be used in API calls and executed in an environment. Since they rely on massive models with an enormous number of parameters, such approaches have been so far limited to using in-context examples as a way of teaching the agents, since more traditional optimization schemes like reinforcement learning with gradient descent require substantial amounts of compute and time.</p>
<p>In this paper, we propose an alternative approach called Reflexion that uses verbal reinforcement to help agents learn from prior failings. Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as additional context for the LLM agent in the next episode. This self-reflective feedback acts as a 'semantic' gradient signal by providing the agent with a concrete direction to improve upon, helping it learn from prior mistakes to perform better on the task. This is akin to how humans iteratively learn to accomplish complex tasks in a few-shot manner - by reflecting on their previous failures in order to form an improved plan of attack for the next attempt. For example, in figure 1, a Reflexion agent learns to optimize its own behavior to solve decision-making, programming, and reasoning tasks through trial, error, and self-reflection.</p>
<p>Generating useful reflective feedback is challenging since it requires a good understanding of where the model made mistakes (i.e. the credit assignment problem [25]) as well as the ability to generate a summary containing actionable insights for improvement. We explore three ways for doing this - simple binary environment feedback, pre-defined heuristics for common failure cases, and self-evaluation such as binary classification using LLMs (decision-making) or self-written unit tests (programming). In all implementations, the evaluation signal is amplified to natural language experience summaries which can be stored in long-term memory.</p>
<p>Reflexion has several advantages compared to more traditional RL approaches like policy or valuebased learning: 1) it is lightweight and doesn't require finetuning the LLM, 2) it allows for more nuanced forms of feedback (e.g. targeted changes in actions), compared to scalar or vector rewards that are challenging to perform accurate credit assignment with, 3) it allows for a more explicit and interpretable form of episodic memory over prior experiences, and 4) it provides more explicit hints for actions in future episodes. At the same time, it does have the disadvantages of relying on the power of the LLM's self-evaluation capabilities (or heuristics) and not having a formal guarantee for success. However, as LLM capabilities improve, we only expect this paradigm to get better over time.</p>
<p>We perform experiments on (1) decision-making tasks to test sequential action choices over long trajectories, (2) reasoning tasks to test knowledge-intensive, single-step generation improvement, and (3) programming tasks to teach the agent to effectively use external tools such as compilers and interpreters. Across all three types of tasks, we observe Reflexion agents are better decisionmakers, reasoners, and programmers. More concretely, Reflexion agents improve on decision-making AlfWorld [24] tasks over strong baseline approaches by an absolute $22 \%$ in 12 iterative learning steps, and on reasoning questions in HotPotQA [28] by 20\%, and Python programming tasks on HumanEval [6] by as much as $11 \%$.</p>
<p>To summarize, our contributions are the following:</p>
<ul>
<li>We propose Reflexion, a new paradigm for 'verbal' reinforcement that parameterizes a policy as an agent's memory encoding paired with a choice of LLM parameters.</li>
<li>We explore this emergent property of self-reflection in LLMs and empirically show that self-reflection is extremely useful to learn complex tasks over a handful of trials.</li>
<li>We introduce LeetcodeHardGym, a code-generation RL gym environment consisting of 40 challenging Leetcode questions ('hard-level') in 19 programming languages.</li>
<li>We show that Reflexion achieves improvements over strong baselines across several tasks, and achieves state-of-the-art results on various code generation benchmarks.</li>
</ul>
<h1>2 Related work</h1>
<p>Reasoning and decision-making Self-Refine [15] employs an iterative framework for selfrefinement to autonomously improve generation through self-evaluation. These self-evaluation and self-improvement steps are conditioned on given task constraints, such as "How can this generation be written in a more positive way". Self-Refine is effective but is limited to single-generation reasoning tasks. Pryzant et al. [21] performs a similar semantic prompt-writing optimization, but is also limited to single-generation tasks. Paul et al. [20] fine-tune critic models to provide intermediate feedback within trajectories to improve reasoning responses. Xie et al. [27] use stochastic beam search over actions to perform a more efficient decision-making search strategy which allows the agent to use foresight advantage due to its self-evaluation component. Yoran et al. [31] and Nair et al.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Reflexion works on decision-making 4.1, programming 4.3, and reasoning 4.2 tasks.</p>
<table>
<thead>
<tr>
<th>Related work on reasoning and decision-making</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Approach</td>
<td>Self refine</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Self-refine [15]</td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Beam search [27]</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Reflexion (ours)</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Related work on programming</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Approach</td>
<td>Test</td>
<td>Debugging</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Test execution</td>
<td>execution</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AlphaCode [14]</td>
<td>✓</td>
<td>✗</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>✗</td>
</tr>
<tr>
<td>CodeT [5]</td>
<td>✓</td>
<td>✗</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>✗</td>
</tr>
<tr>
<td>Self-debugging [7]</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>✗</td>
</tr>
<tr>
<td>CodeRL [12]</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>✗</td>
</tr>
<tr>
<td>Reflexion (ours)</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>[16] use decider models to reason over several generations. Kim et al. [10] use a retry pattern over a fixed number of steps without an evaluation step. Goodman [9] perform a qualitative evaluation step that proposes optimizations to the previous generation. In this paper, we show that several of these concepts can be enhanced with self-reflection to build a persisting memory of self-reflective experiences which allows an agent to identify its own errors and self-suggest lessons to learn from its mistakes over time.</p>
<p>Programming Several past and recent works employ variations of test-driven development or code debugging practices. AlphaCode [14] evaluates a set of generations on hidden test cases. CodeT [5] uses self-generated unit tests that are used to score generated function implementations. Self-Debugging [7] employs a debugging component that is used to improve existing implementations given feedback from a code execution environment. CodeRL [12] sets the problem in an RL framework using an actor-critic setup to debug programs given feedback from an execution environment. AlphaCode, Self-Debugging and CodeRL are effective in fixing less-complex program bugs, but they rely upon ground truth test cases that invalidate pass@1 eligibility, and do not use self-reflection to bridge the gap between error identification and implementation improvement. CodeT does not access hidden test cases but does not implement a self-learning step to improve code writing.</p>
<h1>3 Reflexion: reinforcement via verbal reflection</h1>
<p>We develop a modular formulation for Reflexion, utilizing three distinct models: an Actor, denoted as $M_{a}$, which generates text and actions; an Evaluator model, represented by $M_{e}$, that scores the outputs produced by $M_{a}$; and a Self-Reflection model, denoted as $M_{s r}$, which generates verbal reinforcement cues to assist the Actor in self-improvement. We provide a detailed description of each of these models and subsequently elucidate their collaborative functioning within the Reflexion framework.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Reinforcement</span><span class="w"> </span><span class="nx">via</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">reflection</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">Actor</span><span class="p">,</span><span class="w"> </span><span class="nx">Evaluator</span><span class="p">,</span><span class="w"> </span><span class="k">Self</span><span class="o">-</span><span class="nx">Reflection</span><span class="p">:</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="nx">a</span><span class="p">},</span><span class="w"> </span><span class="nx">M_</span><span class="p">{</span><span class="nx">e</span><span class="p">},</span><span class="w"> </span><span class="nx">M_</span><span class="p">{</span><span class="nx">s</span><span class="w"> </span><span class="nx">r</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">policy</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="p">=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">M_</span><span class="p">{</span><span class="nx">a</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">mem</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">trajectory</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Evaluate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">tau_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">reflection</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="nx">s</span><span class="w"> </span><span class="nx">r</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">mem</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">sr</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="p">=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">pass</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="p">&lt;</span><span class="err">\</span><span class="nx">max</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">trials</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Generate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">tau_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="nx">a_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Evaluate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">tau_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Generate</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">reflection</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="nx">s</span><span class="w"> </span><span class="nx">r</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Append</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">mem</span>
<span class="w">        </span><span class="nx">Increment</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">    </span><span class="k">return</span>
</code></pre></div>

<p>Figure 2: (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm</p>
<p>Actor The Actor is built upon a large language model (LLM) that is specifically prompted to generate the necessary text and actions conditioned on the state observations. Analogous to traditional policy-based RL setups, we sample an action or generation, $a_{t}$, from the current policy $\pi_{\theta}$ at time $t$, receive an observation from the environment $o_{t}$. We explore various Actor models, including Chain of Thought [26] and ReAct [30]. These diverse generation models allow us to explore different aspects of text and action generation within the Reflexion framework, providing valuable insights into their performance and effectiveness. In addition, we also add a memory component mem that provides additional context to this agent. This adaption was inspired by Brooks et al. [3], who suggest a policy iteration approach using in-context learning. Details on how this is populated are provided below.</p>
<p>Evaluator The Evaluator component of the Reflexion framework plays a crucial role in assessing the quality of the generated outputs produced by the Actor. It takes as input a generated trajectory and computes a reward score that reflects its performance within the given task context. Defining effective value and reward functions that apply to semantic spaces is difficult, so we investigate several variants of the Evaluator model. For reasoning tasks, we explore reward functions based on exact match (EM) grading, ensuring that the generated output aligns closely with the expected solution. In decision-making tasks, we employ pre-defined heuristic functions that are tailored to specific evaluation criteria. Additionally, we experiment with using a different instantiation of an LLM itself as an Evaluator, generating rewards for decision-making and programming tasks. This multi-faceted approach to Evaluator design allows us to examine different strategies for scoring generated outputs, offering insights into their effectiveness and suitability across a range of tasks.</p>
<p>Self-reflection The Self-Reflection model instantiated as an LLM, plays a crucial role in the Reflexion framework by generating verbal self-reflections to provide valuable feedback for future trials. Given a sparse reward signal, such as a binary success status (success/fail), the current trajectory, and its persistent memory mem, the self-reflection model generates nuanced and specific feedback. This feedback, which is more informative than scalar rewards, is then stored in the agent's memory (mem). For instance, in a multi-step decision-making task, when the agent receives a failure signal, it can infer that a specific action $a_{i}$ led to subsequent incorrect actions $a_{i+1}$ and $a_{i+2}$. The agent can then verbally state that it should have taken a different action, $a_{i}^{\prime}$, which would have resulted in $a_{i+1}^{\prime}$ and $a_{i+2}^{\prime}$, and store this experience in its memory. In subsequent trials, the agent can leverage its past experiences to adapt its decision-making approach at time $t$ by choosing action $a_{i}^{\prime}$. This iterative process of trial, error, self-reflection, and persisting memory enables the agent to rapidly improve its decision-making ability in various environments by utilizing informative feedback signals.</p>
<p>Memory Core components of the Reflexion process are the notion of short-term and long-term memory. At inference time, the Actor conditions its decisions on short and long-term memory, similar</p>
<p>to the way that humans remember fine-grain recent details while also recalling distilled important experiences from long-term memory. In the RL setup, the trajectory history serves as the short-term memory while outputs from the Self-Reflection model are stored in long-term memory. These two memory components work together to provide context that is specific but also influenced by lessons learned over several trials, which is a key advantage of Reflexion agents over other LLM action choice works.</p>
<p>The Reflexion process Reflexion is formalized as an iterative optimization process in 1. In the first trial, the Actor produces a trajectory $\tau_{0}$ by interacting with the environment. The Evaluator then produces a score $r_{0}$ which is computed as $r_{t}=M_{e}\left(\tau_{0}\right)$. $r_{t}$ is only a scalar reward for trial $t$ that improves as task-specific performance increases. After the first trial, to amplify $r_{0}$ to a feedback form that can be used for improvement by an LLM, the Self-Reflection model analyzes the set of $\left{\tau_{0}, r_{0}\right}$ to produce a summary $s r_{0}$ which is stored in the memory mem. $s r_{t}$ is a verbal experience feedback for trial $t$. The Actor, Evaluator, and Self-Reflection models work together through trials in a loop until the Evaluator deems $\tau_{t}$ to be correct. As mentioned in 3, the memory component of Reflexion is crucial to its effectiveness. After each trial $t$, $s r_{t}$, is appended mem. In practice, we bound mem by a maximum number of stored experiences, $\Omega$ (usually set to 1-3) to adhere to max context LLM limitations.</p>
<h1>4 Experiments</h1>
<p>We evaluate various natural language RL setups on decision-making, reasoning, and code generation tasks. Specifically, we challenge an agent to perform search-based question answering on HotPotQA [28], multi-step tasks in common household environments in AlfWorld [24], and code writing tasks in competition-like environments with interpreters and compilers in HumanEval [6], MBPP [2], and LeetcodeHard, a new benchmark. Most notably, Reflexion improves performance over strong baselines by 22\% in AlfWorld, 20\% in HotPotQA, and 11\% on HumanEval.</p>
<h3>4.1 Sequential decision making: ALFWorld</h3>
<p>AlfWorld is a suite of text-based environments that challenge an agent to solve multi-step tasks in a variety of interactive environments based on TextWorld [8]. Following Yao et al. [30], we run the agent in 134 AlfWorld environments across six different tasks, including finding hidden objects (e.g., finding a spatula in a drawer), moving objects (e.g., moving a knife to the cutting board), and manipulating objects with other objects (e.g., chilling a tomato in the fridge). We use ReAct [30] as the action generator as Yao et al. [30] has shown success in long trajectory decisionmaking using explicit intermediate thoughts. AlfWorld tasks naturally require a self-evaluation step as the environment can only signal if a task is complete. To achieve fully autonomous behavior, we implement two self-evaluation techniques: natural language classification using an LLM and a hand-written heuristic. The heuristic is simple: if the agent executes the same action and receives the same response for more than 3 cycles, or if the number of actions taken in the current environment exceeds 30 (inefficient planning), we self-reflect. In the baseline runs, if self-reflection is suggested, we skip the self-reflection process, reset the environment, and start a new trial. In the Reflexion runs, the agent uses self-reflection to find its mistake, update its memory, reset the environment, and start a new trial. To avoid very long prompt windows that may exceed the maximum limit, we truncate the agent's memory to the last 3 self-reflections (experiences).
To avoid syntactic errors, we provide two domain-specific few-shot trajectories to the agent. We use the same few-shot trajectory examples as Yao et al. [30] with GPT-3 for the LLM. AlfWorld tasks, ReAct few-shot prompts, and Reflexion examples are included in the appendix.</p>
<p>Results ReAct + Reflexion significantly outperforms ReAct by completing 130 out of 134 tasks using the simple heuristic to detect hallucinations and inefficient planning. Further, ReAct + Reflexion learns to solve additional tasks by learning in 12 consecutive trials. In the ReAct-only approach, we see that performance increase halts between trials 6 and 7.</p>
<p>Analysis A common error in baseline failed AlfWorld trajectories is when an agent thinks that it has possession of an item but does not actually have the item. The agent proceeds to execute several actions in a long trajectory and is not able to backtrack its actions to find the mistake. Reflexion</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) AlfWorld performance across 134 tasks showing cumulative proportions of solved tasks using self-evaluation techniques of (Heuristic) and (GPT) for binary classification. (b) Classification of AlfWorld trajectories by reason of failure.</p>
<p>eliminates almost all of these cases by using self-reflection to distill long, failed trajectories into relevant experiences that can are used as "self-hints" in the future. There are two main cases in which long-term memory helps an agent in AlfWorld: 1) An early mistake in a long trajectory can be easily identified. The agent can suggest a new action choice or even a new long-term plan. 2) There are too many surfaces/containers to check for an item. The agent can exploit its experience memory over several trials to thoroughly search a room. In 3, the learning curve suggests that the learning process occurs over several experiences, meaning that the agent is successfully balancing cases 1 and 2 shown in the immediate spike in the improvement between the first two trials, then a steady increase over the next 11 trials to a near-perfect performance. On the other hand, 3 shows a ReAct-only agent converging at a hallucination rate of 22% with no signs of long-term recovery.</p>
<h3>4.2 Reasoning: HotpotQA</h3>
<p>HotPotQA [28] is a Wikipedia-based dataset with 113k question-and-answer pairs that challenge agents to parse content and reason over several supporting documents. To test improvement in reasoning <em>only</em> ability, we implement Reflexion + Chain-of-Thought (CoT) [26] for step-by-step $Q \rightarrow A$ and $Q$, $C_{gt} \rightarrow A$ implementations, where $Q$ is the question, $C_{gt}$ is the ground truth context from the dataset, and $A$ is the final answer. Since CoT is not a multi-step decision-making technique, we give $C_{gt}$ to the agent so that we can isolate the reasoning behavior over large sections of the provided text. To test holistic question and answering ability, which requires reasoning and action choice, we implement a Reflexion + ReAct [30] agent that can retrieve relevant context using a Wikipedia API and infer answers using step-by-step explicit thinking. For CoT implementations, we use 6-shot prompting; for ReAct, we use 2-shot prompting, and for self-reflection, we use 2-shot prompting. All examples can be found in the appendix.</p>
<p>Robustly evaluating natural language answers is a long-standing problem in NLP. Therefore, between trials, we use exact match answer grading using the environment to give a binary success signal to the agent. After each trial, the self-reflection loop is employed to amplify the binary signal, similar to the decision-making setup 4.1 in AlfWorld with a memory size of 3 experiences.</p>
<p>Results Reflexion outperforms all baseline approaches by significant margins over several learning steps. Furthermore, ReAct-only, CoT-only, and CoT (GT)-only implementations fail to probabilistically improve on any tasks, meaning that no failed tasks from the first trial from any of the baseline approaches were able to be solved in subsequent trials using a temperature of 0.7 In the Reflexion runs, we allowed the agent to gather experience and retry on failed tasks until it produced 3 consecutive failed attempts on the particular task. Naturally, the CoT (GT) achieved higher accuracy scores as it was given access to the ground truth context of the question. Still, the CoT (GT) agent is unable to correctly infer the correct answer for 39% of the questions, but Reflexion helps the agent to correct its mistakes without access to the ground truth answer to improve its accuracy by 14%.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Chain-of-Thought (CoT) and ReAct. Reflexion improves search, information retrieval, and reasoning capabilities on 100 HotPotQA questions. (a) Reflexion ReAct vs Reflexion CoT (b) Reflexion CoT (GT) for reasoning only (c) Reflexion vs episodic memory ablation.</p>
<p>Analysis We perform an ablation experiment to isolate the advantage of the self-reflective step for reasoning using CoT (GT) as the baseline approach 4. Recall that CoT (GT) uses Chain-of-Thought reasoning with provided ground truth context, which tests reasoning ability over long contexts. Next, we add an element of episodic memory (EPM) by including the most recent trajectory. For the Reflexion agent, we implement the standard self-reflection step as a final pass. Intuitively, we test if the agent is iteratively learning more effectively by using verbal explanation using language written in the first person. 4 shows that self-reflection improves learning by an 8% absolute boost over the episodic memory learning advantage. This result supports the argument that refinement-only approaches are not as effective as self-reflection-guided refinement approaches.</p>
<h3>4.3 Programming</h3>
<p>We evaluate the baseline and Reflexion approaches on Python and Rust code writing on MBPP [2], HumanEval [6], and LeetcodeHardGym, our new dataset. MBPP and HumanEval measure function body generation accuracy given natural language descriptions. We use a benchmark language compiler, MultiPL-E [4], to translate subsets of HumanEval and MBPP to the Rust language. MultiPL-E is a collection of small compilers that can be used to translate Python benchmark questions to 18 other languages. We include experiments for Rust code generation to demonstrate that Reflexion implementations for code generation are language-agnostic and can be used for interpreted and compiled languages. Lastly, we introduce a new benchmark, LeetcodeHardGym, which is an interactive programming gym that contains 40 Leetcode hard-rated questions that have been released after October 8, 2022, which is the pre-training cutoff date of GPT-4 [18].</p>
<p>The task of programming presents a unique opportunity to use more grounded self-evaluation practices such as self-generated unit test suites. Thus, our Reflexion-based programming task implementation is eligible for pass@1 accuracy reporting. To generate a test suite, we use Chain-of-Thought prompting [26] to produce diverse, extensive tests with corresponding natural language descriptions. Then, we filter for syntactically valid test statements by attempting to construct a valid abstract syntax tree (AST) for each proposed test. Finally, we sample n tests from the collection of generated unit tests to produce a test suite T, denoted as {t₀, t₁, ..., tₙ}. We set n to a maximum of 6 unit tests. Aside from the unit test suite component, the setup for the learning loop for a Reflexion programming agent is identical to the reasoning and decision-making agents with a max memory limit of 1 experience.</p>
<table>
<thead>
<tr>
<th>Benchmark + Language</th>
<th>Prev SOTA Pass@1</th>
<th>SOTA Pass@1</th>
<th>Reflexion Pass@1</th>
</tr>
</thead>
<tbody>
<tr>
<td>HumanEval (PY)</td>
<td>65.8 (CodeT [5] + GPT-3.5)</td>
<td>80.1 (GPT-4)</td>
<td>91.0</td>
</tr>
<tr>
<td>HumanEval (RS)</td>
<td>–</td>
<td>60.0 (GPT-4)</td>
<td>68.0</td>
</tr>
<tr>
<td>MBPP (PY)</td>
<td>67.7 (CodeT [5] + Codex [6])</td>
<td>80.1 (GPT-4)</td>
<td>77.1</td>
</tr>
<tr>
<td>MBPP (RS)</td>
<td>–</td>
<td>70.9 (GPT-4)</td>
<td>75.4</td>
</tr>
<tr>
<td>Leetcode Hard (PY)</td>
<td>–</td>
<td>7.5 (GPT-4)</td>
<td>15.0</td>
</tr>
</tbody>
</table>
<p>Table 1: Pass@1 accuracy for various model-strategy-language combinations. The base strategy is a single code generation sample. All instruction-based models follow zero-shot code generation.</p>
<table>
<thead>
<tr>
<th>Benchmark + Language</th>
<th>Base</th>
<th>Reflexion</th>
<th>TP</th>
<th>FN</th>
<th>FP</th>
<th>TN</th>
</tr>
</thead>
<tbody>
<tr>
<td>HumanEval (PY)</td>
<td>0.80</td>
<td>$\mathbf{0 . 9 1}$</td>
<td>0.99</td>
<td>0.40</td>
<td>0.01</td>
<td>0.60</td>
</tr>
<tr>
<td>MBPP (PY)</td>
<td>$\mathbf{0 . 8 0}$</td>
<td>0.77</td>
<td>0.84</td>
<td>0.59</td>
<td>0.16</td>
<td>0.41</td>
</tr>
<tr>
<td>HumanEval (RS)</td>
<td>0.60</td>
<td>$\mathbf{0 . 6 8}$</td>
<td>0.87</td>
<td>0.37</td>
<td>0.13</td>
<td>0.63</td>
</tr>
<tr>
<td>MBPP (RS)</td>
<td>0.71</td>
<td>$\mathbf{0 . 7 5}$</td>
<td>0.84</td>
<td>0.51</td>
<td>0.16</td>
<td>0.49</td>
</tr>
</tbody>
</table>
<p>Table 2: Overall accuracy and test generation performance for HumanEval and MBPP. For Rust, HumanEval is the hardest 50 problems from HumanEval Python translated to Rust with MultiPL-E [4]. TP: unit tests pass, solution pass; FN: unit tests fail, solution pass; FP: unit tests pass, solution fail; TN: unit tests fail, solution fail.</p>
<p>Results Reflexion outperforms all baseline accuracies and sets new state-of-the-art standards on all benchmarks for Python and Rust except for MBPP Python 1. We further investigate the inferior performance of Reflexion on MBPP Python.</p>
<p>Analysis We acknowledge that self-reflecting code-generation agents are bound to their ability to write diverse, comprehensive tests. Therefore, in the case in which the model generates a flaky test suite, it is possible that all tests pass on an incorrect solution and lead to a false positive label on a code completion [11]. On the other hand, if the model produces an incorrectly written test suite, it is possible for some of the tests to fail on a correct solution, leading to a self-reflection generation that is conditioned on a false negative code completion. Given the implementation of Reflexion, false negatives are preferred over false positives as the agent may be able to use self-reflection to identify the incorrect test(s) and prompt itself to keep the original code completion intact. On the other hand, if an invalid test suite returns a false positive completion (all internal test cases pass but the implementation is incorrect), the agent will prematurely report an invalid submission. In 2, various conditions are measured to analyze performance beyond pass@1 accuracy. Previously, we displayed the inferior performance of Reflexion to the baseline GPT-4 on MBPP Python. In 2, we observe a notable discrepancy between the false positive labels produced by internal test execution, P (not pass@1 generation correct | tests pass). That is, the probability that a submission will fail given that it passes all unit tests. For HumanEval and MBPP Python, the baseline pass@1 accuracies are relatively similar, $82 \%$ and $80 \%$, respectively. However, the false positive test execution rate for MBPP Python is $16.3 \%$ while the rate for HumanEval Python is a mere $1.4 \%$, leading to $91 \%$ overall accuracy 1 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: left;">Test Generation</th>
<th style="text-align: left;">Self-reflection</th>
<th style="text-align: left;">Pass@1 (Acc)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base model</td>
<td style="text-align: left;">False</td>
<td style="text-align: left;">False</td>
<td style="text-align: left;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">Test generation omission</td>
<td style="text-align: left;">False</td>
<td style="text-align: left;">True</td>
<td style="text-align: left;">0.52</td>
</tr>
<tr>
<td style="text-align: left;">Self-reflection omission</td>
<td style="text-align: left;">True</td>
<td style="text-align: left;">False</td>
<td style="text-align: left;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion</td>
<td style="text-align: left;">True</td>
<td style="text-align: left;">True</td>
<td style="text-align: left;">$\mathbf{0 . 6 8}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Pass@1 accuracy for various compromised approaches on the Reflexion approach using GPT-4 as the base model on HumanEval Rust - 50 hardest problems</p>
<p>Ablation study We test the composite approach of Reflexion for test generation and self-reflection cooperation on a subset of the 50 hardest HumanEval Rust problems. Our Rust compiler environment provides verbose error logs and helpful debugging hints, therefore serving as a good playground for compromised approaches. First, we omit internal test generation and execution steps, which test the agent to self-reflect without guidance from current implementations. 3 shows an inferior $52 \%$ vs $60 \%$ (baseline) accuracy, which suggests that the agent is unable to determine if the current implementation is correct without unit tests. Therefore, the agent must participate in all iterations of the run without the option to return early, performing harmful edits to the implementation.</p>
<p>Next, we test self-reflection contribution by omitting the natural language explanation step following failed unit test suite evaluations. Intuitively, this challenges the agent to combine the tasks of error identification and implementation improvement across all failed unit tests. Interestingly, the compromised agent does not improve performance over the baseline run. We observe that the test generation and code compilation steps are able to catch syntax and logic errors, but the implementation fixes do not reflect these indications. These empirical results suggest that several recent works that</p>
<p>propose blind trial and error debugging techniques without self-reflection are ineffective on harder tasks such as writing complex programs in Rust.</p>
<h1>5 Limitations</h1>
<p>At its core, Reflexion is an optimization technique that uses natural language to do policy optimization. Policy optimization is a powerful approach to improve action choice through experience, but it may still succumb to non-optimal local minima solutions. In this study, we limit long-term memory to a sliding window with maximum capacity, but we encourage future work to extend the memory component of Reflexion with more advanced structures such as vector embedding databases or traditional SQL databases. Specific to code generation, there are many practical limitations to testdriven development in specifying accurate input-output mappings such as non-deterministic generator functions, impure functions that interact with APIs, functions that vary output according to hardware specifications, or functions that invoke parallel or concurrent behavior that may be difficult to predict.</p>
<h2>6 Broader impact</h2>
<p>Large language models are increasingly used to interact with external environments (e.g. the Internet, software, robotics, etc.) and humans. Our work has the potential of reinforcing and empowering these agents toward greater automation and work efficiency, but it also amplifies the risks when these agents were put into misuse. We believe that this direction of research will need more effort in safety and ethical considerations.</p>
<p>On the other hand, reinforcement learning has suffered from its black-box policy and optimization setups in which interpretability and alignment have been challenging. Our proposed "verbal" reinforcement learning might address some of the issues and turn autonomous agents more interpretable and diagnosable. For example, in the case of tool-usage that may be too hard for humans to understand, self-reflections could be monitored to ensure proper intent before using the tool.</p>
<h2>7 Conclusion</h2>
<p>In this work, we present Reflexion, an approach that leverages verbal reinforcement to teach agents to learn from past mistakes. We empirically show that Reflexion agents significantly outperform currently widely-used decision-making approaches by utilizing self-reflection. In future work, Reflexion could be used to employ more advanced techniques that have been thoroughly studied in traditional RL settings, such as value learning in natural language or off-policy exploration techniques.</p>
<h2>8 Reproducibility</h2>
<p>We highly advise others to use isolated execution environments when running autonomous code writing experiments as the generated code is not validated before execution.</p>
<h1>References</h1>
<p>[1] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. (2022). Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691.
[2] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. (2021). Program synthesis with large language models. arXiv preprint arXiv:2108.07732.
[3] Brooks, E., Walls, L., Lewis, R. L., and Singh, S. (2022). In-context policy iteration. arXiv preprint arXiv:2210.03821.
[4] Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., Guha, A., Greenberg, M., and Jangda, A. (2022). Multipl-e: A scalable and extensible approach to benchmarking neural code generation.
[5] Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. (2022). Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397.
[6] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.
[7] Chen, X., Lin, M., Schärli, N., and Zhou, D. (2023). Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.
[8] Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., El Asri, L., Adada, M., et al. (2019). Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 41-75. Springer.
[9] Goodman, N. (2023). Meta-prompt: A simple self-improving language agent. noahgoodman.substack.com.
[10] Kim, G., Baldi, P., and McAleer, S. (2023). Language models can solve computer tasks. arXiv preprint arXiv:2303.17491.
[11] Lam, W., Winter, S., Wei, A., Xie, T., Marinov, D., and Bell, J. (2020). A large-scale longitudinal study of flaky tests. Proc. ACM Program. Lang., 4(OOPSLA).
[12] Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. (2022). Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314-21328.
[13] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. (2023). Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161.
[14] Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. (2022). Competition-level code generation with alphacode. Science, 378(6624):1092-1097.
[15] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. (2023). Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.
[16] Nair, V., Schumacher, E., Tso, G., and Kannan, A. (2023). Dera: Enhancing large language model completions with dialog-enabled resolving agents. arXiv preprint arXiv:2303.17071.
[17] Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. (2021). Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332.
[18] OpenAI (2023). Gpt-4 technical report. ArXiv.</p>
<p>[19] Park, J. S., O’Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.
[20] Paul, D., Ismayilzada, M., Peyrard, M., Borges, B., Bosselut, A., West, R., and Faltings, B. (2023). Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904.
[21] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M. (2023). Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495.
[22] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.
[23] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023). Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.
[24] Shridhar, M., Yuan, X., Côté, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. (2021). ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR).
[25] Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.
[26] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.
[27] Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He, J., and Xie, Q. (2023). Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633.
[28] Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018). HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP).
[29] Yao, S., Chen, H., Yang, J., and Narasimhan, K. (preprint). Webshop: Towards scalable real-world web interaction with grounded language agents. In ArXiv.
[30] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023). ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR).
[31] Yoran, O., Wolfson, T., Bogin, B., Katz, U., Deutch, D., and Berant, J. (2023). Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007.</p>
<h1>A Evaluation with additional models</h1>
<p>We further investigated the applicability of trial-and-error problem-solving with models of various strengths. We found that the ability to specify self-corrections is an emergent quality of stronger, larger models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: left;">Pass@1 accuracy (avg over 8 trials)</th>
<th style="text-align: left;">Pass@1 accuracy (std)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: left;">0.26</td>
<td style="text-align: left;">0.00481</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion</td>
<td style="text-align: left;">0.26</td>
<td style="text-align: left;">0.00305</td>
</tr>
</tbody>
</table>
<p>Table 4: Pass@1 accuracy on HumanEval Python using starchat-beta [13].</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Baseline accuracy</th>
<th style="text-align: left;">Reflexion accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT (GT) + text-davinci-003</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">$\mathbf{0 . 7 7}$</td>
</tr>
<tr>
<td style="text-align: left;">CoT (GT) + gpt-3.5-turbo</td>
<td style="text-align: left;">0.57</td>
<td style="text-align: left;">$\mathbf{0 . 7 1}$</td>
</tr>
<tr>
<td style="text-align: left;">CoT (GT) + gpt-4</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">$\mathbf{0 . 8 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ReAct + text-davinci-003</td>
<td style="text-align: left;">0.30</td>
<td style="text-align: left;">$\mathbf{0 . 5 5}$</td>
</tr>
<tr>
<td style="text-align: left;">ReAct + gpt-3.5-turbo</td>
<td style="text-align: left;">0.26</td>
<td style="text-align: left;">$\mathbf{0 . 3 8}$</td>
</tr>
<tr>
<td style="text-align: left;">ReAct + gpt-4</td>
<td style="text-align: left;">0.39</td>
<td style="text-align: left;">$\mathbf{0 . 5 1}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Pass@1 accuracy on 100 HotPotQA using various models.</p>
<h1>B Decision-making</h1>
<p>Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.
Task: examine the mug with the desklamp.
Trial #1
$&gt;$ think: To solve the task, I need to find and take a mug, then find and use a desklamp.
$&gt;$ think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), desk (1-2), shelf (1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1.
$&gt;$ go to drawer 1
The drawer 1 is closed.
・.
$&gt;$ open drawer 6
You open the drawer 6. The drawer 6 is open. In it, you see a keychain 2.
$&gt;$ go to desk 1
On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.
$&gt;$ think: Now I find a mug (1). Next, I need to take it.
$&gt;$ take mug 1 from desk 1
You pick up the mug 1 from the desk 1.
$&gt;$ think: Now I take a mug (1). Next, I need to find a desklamp. A desklamp is more likely to appear in desk (1-2), sidetable (1-2), shelf (1-6), bed (1), drawer (1-6). I can check one by one, starting with desk 1 .
$&gt;$ go to desk 1
Nothing happens.
$&gt;$ go to desk 2
On the desk 2, you see a alarmclock 1, a bowl 1, a mug 3, a pencil 3, and a pencil 2.
$&gt;$ think: Now I find a desklamp (1). Next, I need to use it.
$&gt;$ use desklamp 1
Nothing happens.
$&gt;$ use desklamp 1
Nothing happens.
Status: Fail
Reflection: In this environment, my plan was to find a mug then find and use a desklamp. However, the task says to examine the mug with the desklamp. I should have looked for the desklamp first, then looked for the mug. I noticed that the desklamp was found on desk 1. In the next trial, I will go to desk 1, find the lamp, then look for the mug and examine it with the desklamp.
Trial #2
$&gt;$ go to desk 1
On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.
$&gt;$ think: To solve the task, I need to find and take a mug, then find and use a desklamp.
$&gt;$ take mug 1 from desk 1
You pick up the mug 1 from the desk 1.
$&gt;$ think: To solve the task, I need to find and take a mug, then find and use a desklamp.
$&gt;$ use desklamp 1
You turn on the desklamp 1.
Status: Success
Figure 5: [Top] An AlfWorld trajectory in which the agent failed due to inefficient planning. In the reflection, the agent recognizes that it should have looked for the desklamp then the mug, not the mug then the desklamp. [Bottom] The agent is able to correct its reasoning trace and execute a sequence of actions in a concise manner.</p>
<h1>B. 1 WebShop Limitation</h1>
<p>In 5, we briefly state that Reflexion struggles to overcome local minima choices that require extremely creative behavior to escape. We observe this shortcoming in an experiment on WebShop [29]. WebShop is a web-based problem-solving benchmark that tests agents to navigate an e-commerce website to locate and purchase products given requests from clients. We test a two-shot ReAct + Reflexion agent in 100 environments. However, after only four trials, we terminate the runs as the agent does not show signs of improvement 6 . Further, the agent does not generate helpful, intuitive self-reflections after failed attempts. We conclude that Reflexion is unable to solve tasks that require a significant amount of diversity and exploration. In AlfWorld, the agent is able to adequately explore new environments because the permissible actions can be seen in the observations. In HotPotQA, the agent faces a similar WebShop search query task but is more successful as the search space for Wikipedia articles is more diverse and requires less precise search queries. A common problem for e-commerce search engines is properly handling ambiguity in natural language search interpretations. Thus, WebShop presents a task that requires very diverse and unique behavior from a Reflexion agent.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Reflexion vs React performance on WebShop across 100 customer shopping requests. ReAct + Reflexion fails to significantly outperform ReAct.</p>
<h2>C Programming</h2>
<p>Programming LLM calls require strict instructions to produce function bodies only, due to the extensive dialogue training of the LLMs. A few programming examples are reported below with instructions highlighted in blue and templates. See the full implementation at https://github. com/noahshinn024/reflexion.</p>
<h2>C. 1 Programming function implementation example (HumanEval Python)</h2>
<p>Sample function signature:</p>
<div class="codehilite"><pre><span></span><code>def minSubArraySum(nums):
    &quot;&quot;&quot;
    Given an array of integers nums, find the minimum sum of
        any
    non-empty sub-array of nums.
    Example
    minSubArraySum([2, 3, 4, 1, 2, 4]) == 1
</code></pre></div>

<div class="codehilite"><pre><span></span><code>minSubArraySum([-1, -2, -3]) == -6
    &quot;&quot;&quot;
</code></pre></div>

<p>Sample function body implementation:</p>
<div class="codehilite"><pre><span></span><code><span class="n">min_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">))</span><span class="err">:</span>
<span class="w">    </span><span class="n">current_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">))</span><span class="err">:</span>
<span class="w">        </span><span class="n">current_sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">nums</span><span class="o">[</span><span class="n">j</span><span class="o">]</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">current_sum</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nl">min_sum</span><span class="p">:</span>
<span class="w">                </span><span class="n">min_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">current_sum</span>
<span class="k">return</span><span class="w"> </span><span class="n">min_sum</span>
</code></pre></div>

<h1>C. 2 Reflexion Actor instruction</h1>
<p>You are a Python writing assistant. You will be given your previous implementation of a function, a series of unit tests results, and your self-reflection on your previous implementation. Apply the necessary changes below by responding only with the improved body of the function. Do not include the signature in your response. The first line of your response should have 4 spaces of indentation so that it fits syntactically with the user provided signature. You will be given a few examples by the user.</p>
<p>Reflexion Actor generations follow the form:
(Instruction)
(Function implementation)
(Unit test feedback)
(Self-reflection)
(Instruction for next function implmentation)
Full examples are shown in https://github.com/noahshinn024/reflexion</p>
<h2>C. 3 Reflexion Self-reflection instruction and example</h2>
<p>You are a Python writing assistant. You will be given your previous implementation of a function, a series of unit tests results, and your self-reflection on your previous implementation. Apply the necessary changes below by responding only with the improved body of the function. Do not include the signature in your response. The first line of your response should have 4 spaces of indentation so that it fits syntactically with the user provided signature. You will be given a few examples by the user. Reflexion Self-Reflection generations follow the form:
(Instruction)
(Function implementation)
(Unit test feedback)</p>
<h2>C. 4 Reflexion programming no Self-Reflection ablation example</h2>
<p>Reflexion no Self-Reflection ablation Actor generations follow the form:
(Instruction)
(Function implementation)
(Unit test feedback)
(Self-reflection)
(Instruction for next function implmentation)</p>
<h2>C. 5 Reflexion programming no test generation ablation example</h2>
<p>Reflexion no test generation ablation Actor generations follow the form:
(Instruction)</p>
<p>(Function implementation)
(Unit test feedback)
(Self-reflection)
(Instruction for next function implementation)</p>
<h1>D Reasoning</h1>
<h2>D. 1 Full example</h2>
<p>Trial #1: Question: Grown-Ups starred the actor who was best known for which role on "'Allo 'Allo!"?
Thought 1: I need to search Grown-Ups and "'Allo 'Allo!", find the actor who starred in Grown-Ups, then find the role he was best known for in "'Allo 'Allo!".
Action 1: Search[Grown-Ups]
Observation 1: Grown-Ups is a 1980 British BBC television film devised and directed by Mike Leigh. It stars Lesley Manville, Philip Davis, Brenda Blethyn, Janine Duvitski, Lindsay Duncan and Sam Kelly. It was edited by Robin Sales and produced by Louis Marks for the BBC, and originally shown on BBC 2 on 28 November 1980.</p>
<p>Thought 2: Grown-Ups starred Lesley Manville, Philip Davis, Brenda Blethyn, Janine Duvitski, Lindsay Duncan and Sam Kelly. I need to search "'Allo 'Allo!" and find which actor was best known for which role.
Action 2: Search["'Allo 'Allo!"]
Observation 2: Could not find ["'Allo 'Allo!"]. Similar: ["'Allo 'Allo!", "List of 'Allo 'Allo! characters", 'Gorden Kaye', "'Allo 'Allo! (series 2)", 'Allô, allô', 'Francesca Gonshaw', "'Allo 'Allo! (series 4)", 'Kim Hartman', "'Allo 'Allo! (series 6)", 'Jeremy Lloyd']
Thought 3: To find the actor who was best known for which role on "'Allo 'Allo!", I can search Gorden Kaye.
Action 3: Search[Gorden Kaye]
Observation 3: Gordon Irving Kaye (7 April 1941 - 23 January 2017), known professionally as Gorden Kaye, was an English actor, best known for playing womanising café owner René Artois in the television comedy series 'Allo 'Allo!.
Thought 4: Gorden Kaye was best known for playing womanising café owner René Artois in the television comedy series "'Allo 'Allo!". So the answer is Rene Artois.
Action 4: Finish[Rene Artois]
Observation 4: Answer is INCORRECT
Reflection: I searched the wrong title for the show, "'Allo 'Allo!", which resulted in no results. I should have searched the show's main character, Gorden Kaye, to find the role he was best known for in the show.</p>
<p>Figure 7: Two HotPotQA trials within the same environment and task. The Reflexion + ReAct agent uses self-reflection to determine a better search method for the next trial.</p>
<p>Trial #2: Question: Grown-Ups starred the actor who was best known for which role on "'Allo 'Allo!"?
Thought 1: I need to search the actor who was best known for a role on "'Allo 'Allo!" and find out what role they were best known for.
Action 1: Search[Grown-Ups]
Observation 1: Grown-Ups is a 1980 British BBC television film devised and directed by Mike Leigh. It stars Lesley Manville, Philip Davis, Brenda Blethyn, Janine Duvitski, Lindsay Duncan and Sam Kelly. It was edited by Robin Sales and produced by Louis Marks for the BBC, and originally shown on BBC 2 on 28 November 1980.</p>
<p>Thought 2: The paragraph does not mention the actor who was best known for a role on "'Allo 'Allo!". I need to search the actor's name instead. Action 2: Search[Sam Kelly]
Observation 2: Roger Michael Kelly (19 December 1943 - 14 June 2014), known by the stage name Sam Kelly, was an English actor who appeared in film, television, radio and theatre. He is best known for his roles as Captain Hans Geering in 'Allo 'Allo!, Warren in Porridge, Sam in On the Up, and Ted Liversidge in Barbara.
Thought 3: Sam Kelly is best known for his role as Captain Hans Geering in "'Allo 'Allo!", so the answer is Captain Hans Geering.
Action 3: Finish[Captain Hans Geering]
Observation 3: Answer is CORRECT</p>
<h1>D. 2 Chain-of-Thought + Reflexion</h1>
<p>Trial #1: Question: What profession does John Lanchester and Alan Dean Foster have in common?
Thought 1: Let's think step by step. John Lanchester is a novelist, journalist, and critic. Alan Dean Foster is a novelist and screenwriter. So the profession John Lanchester and Alan Dean Foster have in common is novelist and screenwriter.
Action 1: Finish[novelist and screenwriter]
Observation 1: Answer is INCORRECT
Trial #2: Question: What profession does John Lanchester and Alan Dean Foster have in common?
Thought 1: Let's think step by step. John Lanchester is a novelist, journalist, and financial journalist. Alan Dean Foster is a novelist and screenwriter. So the profession John Lanchester and Alan Dean Foster have in common is novelist. Action 1: Finish[novelist]
Observation 1: Answer is CORRECT
Reflection: My reasoning for the profession John Lanchester and Alan Dean Foster have in common failed because I incorrectly assumed that they both had the same profession. In the future, when attempting this question, I should focus on better researching the individual backgrounds of both authors to ensure that I am accurately identifying their professions. Additionally, I should consider the possibility that they may have multiple professions in common.</p>
<h2>D. 3 HotPotQA Chain-of-Thought (GT) + Reflexion</h2>
<p>Context: The Battle of White Plains was a battle in the New York and New Jersey campaign of the American Revolutionary War fought on October 28, 1776, near White Plains, New York. Following the retreat ...
Trial #1: Question: What was a series of battles during the Revolutionary War, for control of New York City and the state of New Jersey, fought on October 28, 1776 near White Plains, New York? Thought 1: The question is asking about a series of battles during the Revolutionary War for control of New York City and the state of New Jersey, fought on October 28, 1776 near White Plains, New York. From the context, I know that this was the Battle of White Plains.
Action 1: Finish[Battle of White Plains]
Observation 1: Answer is INCORRECT</p>
<p>Trial #2: Question: What was a series of battles during the Revolutionary War, for control of New York City and the state of New Jersey, fought on October 28, 1776 near White Plains, New York? Thought 1: The question is asking for a series of battles during the Revolutionary War, for control of New York City and the state of New Jersey, fought on October 28, 1776 near White Plains, New York. Based on the context, I know that this series of battles was the New York and New Jersey campaign.
Action 1: Finish[The New York and New Jersey campaign]
Observation 1: Answer is CORRECT
Reflection: Upon reflecting on the incorrect answer I provided, I realize that I may not have provided enough context to accurately answer the question. The question asked for a series of battles, but I only provided the name of one battle. To better answer this question in the future, I will make sure to provide more context, such as the name of the campaign, which was the New York and New Jersey campaign, and the fact that it was a series of battles. Additionally, I will make sure to include the dates of the battles, as well as the locations, to ensure that my answer is comprehensive and accurate.</p>
<h1>D. 4 HotPotQA episodic memory (EPM) ablation prompts</h1>
<h2>D.4.1 (EPM) Chain-of-Thought + Reflexion</h2>
<h2>Trial #1: Question: Which of Jonny Craig and Pete Doherty has been a member of more bands ?</h2>
<p>Thought 1: Let's think step by step. Jonny Craig has been a member of six bands, including Emarosa, Dance Gavin Dance, and Slaves. Pete Doherty has been a member of seven bands, including The Libertines, Babyshambles, and The Puta Madres. So Pete Doherty has been a member of more bands.
Action 1: Finish[Pete Doherty]
Observation 1: Answer is INCORRECT</p>
<p>Trial #2: Question: Which of Jonny Craig and Pete Doherty has been a member of more bands 2
Thought 1: Let's think step by step. I will research the past and current bands of both musicians to ensure that I am accurately comparing their band memberships. After researching, I have found that Jonny Craig has been a member of seven bands, including Emarosa, Dance Gavin Dance, and Slaves. Pete Doherty has been a member of seven bands, including The Libertines, Babyshambles, and The Puta Madres. So Jonny Craig has been a member of more bands. Action 2: Finish[Jonny Craig]
Observation 2: Answer is CORRECT
Reflection: My reasoning for which musician has been a member of more bands failed because I did not take into account that Jonny Craig has been a member of more bands in the past, even though Pete Doherty is currently a member of more bands. In the future, when attempting this question, I should focus on researching the past and current bands of both musicians to ensure that I am accurately comparing their band memberships.</p>
<h2>D.4.2 (EPM) Chain-of-Thought (GT) + Reflexion</h2>
<p>Context: Hari Bahadur Basnet is a Nepalese politician. He is the head of the Foreign Relations Department of the Rastriya Janashakti Party. Basnet holds a M.Sc. in Engineering.
A Master of Science (Latin: "Magister Scientiae" ; abbreviated MS, M.S., MSc, M.Sc., MSci, M.Sci., $\mathrm{ScM}, \mathrm{Sc} . \mathrm{M}$., SciM or Sci.M.) is a master's degree in the field of science...
Trial #1: Question: The head of the Foreign Relations Department of the Rastriya Janashakti Party holds a degree that can be abbreviated MS, M.S., or ScM, in what field?
(Previous trajectory)
Thought 1: The question is asking what field of study the head of the Foreign Relations Department of the Rastriya Janashakti Party holds a degree in. From the context, I know that the degree is a Master of Science degree, which is typically granted for studies in sciences, engineering, and medicine.
Action 1: Finish[Sciences, Engineering, and Medicine]
Observation 1: Answer is INCORRECT</p>
<p>Trial #2: Question: The head of the Foreign Relations Department of the Rastriya Janashakti Party holds a degree that can be abbreviated MS, M.S., or ScM, in what field?
(Previous trajectory)
Thought 1: The question is asking what field of study the head of the Foreign Relations Department of the Rastriya Janashakti Party holds a degree in. From the context, I know that the degree is a Master of Science degree, which is typically granted for studies in sciences, engineering, and medicine. However, the question is asking for the field of study, not the degree. Based on the context, I know that Hari Bahadur Basnet holds a M.Sc. in Engineering.
Action 1: Finish[Engineering]
Observation 1: Answer is CORRECT
Reflection: For the first trial, I misunderstood the question. I thought the question was asking for a category of degrees, rather than a specific degree. I learned from my mistake and in the second trial, I focused more closely on the question to understand that it was asking for a specific field of study.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1018 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1018</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1018</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-268691738</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.17266v1.pdf" target="_blank">Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning</a></p>
                <p><strong>Paper Abstract:</strong> This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture. Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameters. The key factors of two learning strategies and corresponding effects were explored in context-aware and context-unaware scenarios, enabling us to identify the scenarios where the methods demonstrate optimal performance, derive conclusive insights, and contribute to a broader range of learning-based engineering applications.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1018.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1018.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TriFinger-SAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tri-finger robotic manipulation agent trained with Soft Actor-Critic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated tri-finger robotic agent (three 1-DoF fingers, 9-D action space) trained with the Soft Actor-Critic (SAC) algorithm in CausalWorld to perform block pushing/manipulation under varying task and environment parameters using fine-tuning and curriculum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>tri-finger robotic agent (TriFinger-SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated tri-finger manipulation agent using model-free reinforcement learning (Soft Actor-Critic, actor-critic with clipped double-Q and entropy regularization). Actions are 9-D joint position commands; observations use a 56-D structured state vector (task variables, joint positions/velocities, block velocities, remaining time), not raw pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic agent in physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld tri-finger block-pushing tasks (customized interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3D simulated robotic manipulation tasks in the CausalWorld benchmark (PyBullet): pushing a block to a goal using three fingers. Complexity arises from coordinated multi-finger control, continuous 9-D action space, sparse/dense compound rewards, and the ability to intervene on many properties (block size/shape, goal position/orientation, block weight, friction). Experiments include both context-aware scenarios (task-specific variables observed by agent, e.g., block size and goal position randomized) and context-unaware scenarios (environmental variables randomized but not provided to agent, e.g., block weight).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized qualitatively as multi-finger coordination and quantitatively via: 9-dimensional continuous action space; 56-dimensional structured state vector; task difficulty manipulated by ranges of object/task parameters (e.g., block dimensions, goal pose, block mass). Specific parameter ranges used: block size dimensions sampled in ranges expanded to [0.015, 0.095] m; goal radial/angle ranges from (0, -œÄ, 0.0375) to (0.15, œÄ, 0.0375) m/rad; weight varied from 0.015 to 0.5 kg. Evaluation metrics for complexity effects include jump-starts, time-to-threshold, asymptotic performance and cumulative reward.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (multi-finger coordination, high-dimensional continuous control, and wide parameter ranges specified above)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Measured by randomized / intervened parameter ranges per episode: (1) block size/shape ranges (e.g., length/width/height independently sampled in [0.015,0.095] m), (2) goal position/orientation ranges (coordinates and orientation ranges as above), (3) block weight range [0.015,0.5] kg; out-of-distribution test enlarged these ranges by ~67% (task1), ~13% (task2), and ~21% (task3). Variation also described qualitatively as context-aware (observable task variables) vs context-unaware (unobserved environment variables).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (explicit, wide domain randomization across multiple continuous parameters; OOD increases up to ~67% used for testing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Training cumulative reward; evaluation metrics: jump-start (initial reward boost), time-to-threshold (steps to reach performance threshold), asymptotic performance, fractional success (mean and std over 100 episodes, in-distribution and out-of-distribution), and qualitative convergence speed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes ‚Äî the paper explicitly analyzes trade-offs between environment complexity and variation: (1) Context-aware variation (observable task-specific changes like size or goal) benefits from whole-model (policy+value) transfer: whole-model fine-tuning gives jump-starts, faster time-to-convergence and sometimes higher asymptotic performance. (2) Context-unaware variation (unobserved environmental changes like weight) breaks value estimates from source tasks; whole-model transfer can hurt (oscillations, poor asymptotic behavior) while policy-only transfer generalizes better. (3) Curriculum design mediates the complexity √ó variation trade-off: staged curricula (2- or 3-stage) speed early learning but poorly-timed transitions cause performance drops; 2-stage curricula sometimes give fastest convergence while 3-stage curricula can improve generalization (smoother progression). The paper emphasizes timing of sub-task transitions as crucial ‚Äî too early causes insufficient skill acquisition, too late causes overfitting to the simpler stage and worse final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>qualitative: When complexity is high but variation low (e.g., fixed parameters or small randomization), pretraining and whole-model fine-tuning produced jump-starts and faster convergence; curriculum initially speeds learning but may not improve asymptotic performance (no numeric values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>qualitative: Not directly cast as low complexity + high variation; closest is context-unaware high variation (weight randomization) with simpler task structure ‚Äî policy-only transfer and 2-stage curriculum improved generalization and final reward relative to whole-model transfer and baseline, but numeric values not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>qualitative: For fully randomized hard tasks (e.g., random size + random goal + wide weight), the best strategies varied by task: Task 1 (size): 2-stage curriculum was best for convergence and robustness; Task 2 (random goal): fine-tuning (pretrain+finetune) was best; Task 3 (weight, unobserved): 2-stage curriculum gave best convergence and final reward. No absolute numeric performance values were provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>qualitative: In preliminary fixed-stage tasks (no randomness) agents learned quickly (used for pre-training or initial curriculum stages) and provided useful initializations for fine-tuning; quantitative numbers not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Fine-tuning (whole-model transfer, policy-only transfer, across-task pretraining (grasp) then fine-tune), Curriculum learning (manual 2-stage and 3-stage curricula with controlled randomness progression), baseline training from scratch. Pretraining tasks included a grasping pretrain (across-task) and fixed-parameter pushing pretrain (within-task).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated both in-distribution and out-of-distribution (OOD) by expanding parameter ranges (~67%/13%/21% for tasks 1/2/3). Results: except for Task 2 (random goal) where curriculum was not applicable, transfer methods (best candidate per task) achieved equivalent or better generalization than baseline. Notably, cross-task fine-tuning (grasp ‚Üí push) matched within-task fine-tuning performance while reducing pretraining time by a factor of ~10. Policy-only transfer for Task 3 had modest training gains but improved generalization relative to baseline. Exact fractional success means/std were reported in figures but not numerically in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: Curriculum learning accelerated early learning (jump-starts) by simplifying early stages; fine-tuning (pretrained whole-model) reduced time-to-threshold and improved convergence for context-aware tasks; across-task pretraining (grasp‚Üípush) required ~1/10 of the pre-training time of within-task pretraining to reach comparable fine-tune performance. No absolute counts of episodes or interactions are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Whole-model transfer (policy + value) outperforms policy-only transfer in context-aware scenarios because value estimates and policies are tightly coupled; 2) In context-unaware scenarios (unobserved environmental variation), whole-model transfer can hurt learning due to incorrect value generalization ‚Äî policy-only transfer generalizes better; 3) Across-task pretraining (grasp ‚Üí push) can achieve similar final performance as within-task pretraining while drastically reducing pretraining time (~10√ó speedup); 4) Curriculum learning (manual staged progression) speeds early learning but transition timing is critical ‚Äî optimal timing balances sufficient mastery and avoidance of overfitting to simple stages; 2-stage curricula often gave faster convergence while 3-stage curricula sometimes improved generalization; 5) Overall strategy selection should depend on whether task variations are observable (favor whole-model fine-tuning) or unobservable (favor policy-only transfer or curriculum strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causalworld: A robotic manipulation benchmark for causal structure and transfer learning <em>(Rating: 2)</em></li>
                <li>Causal Curiosity: RL agents discovering self-supervised experiments for causal representation learning <em>(Rating: 2)</em></li>
                <li>Causal Counterfactuals for improving the robustness of reinforcement learning <em>(Rating: 2)</em></li>
                <li>Curriculum learning for reinforcement learning domains: A framework and survey <em>(Rating: 2)</em></li>
                <li>Guided curriculum learning for walking over complex terrain <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1018",
    "paper_id": "paper-268691738",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "TriFinger-SAC",
            "name_full": "Tri-finger robotic manipulation agent trained with Soft Actor-Critic",
            "brief_description": "A simulated tri-finger robotic agent (three 1-DoF fingers, 9-D action space) trained with the Soft Actor-Critic (SAC) algorithm in CausalWorld to perform block pushing/manipulation under varying task and environment parameters using fine-tuning and curriculum learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "tri-finger robotic agent (TriFinger-SAC)",
            "agent_description": "A simulated tri-finger manipulation agent using model-free reinforcement learning (Soft Actor-Critic, actor-critic with clipped double-Q and entropy regularization). Actions are 9-D joint position commands; observations use a 56-D structured state vector (task variables, joint positions/velocities, block velocities, remaining time), not raw pixels.",
            "agent_type": "simulated agent (robotic agent in physics simulator)",
            "environment_name": "CausalWorld tri-finger block-pushing tasks (customized interventions)",
            "environment_description": "3D simulated robotic manipulation tasks in the CausalWorld benchmark (PyBullet): pushing a block to a goal using three fingers. Complexity arises from coordinated multi-finger control, continuous 9-D action space, sparse/dense compound rewards, and the ability to intervene on many properties (block size/shape, goal position/orientation, block weight, friction). Experiments include both context-aware scenarios (task-specific variables observed by agent, e.g., block size and goal position randomized) and context-unaware scenarios (environmental variables randomized but not provided to agent, e.g., block weight).",
            "complexity_measure": "Characterized qualitatively as multi-finger coordination and quantitatively via: 9-dimensional continuous action space; 56-dimensional structured state vector; task difficulty manipulated by ranges of object/task parameters (e.g., block dimensions, goal pose, block mass). Specific parameter ranges used: block size dimensions sampled in ranges expanded to [0.015, 0.095] m; goal radial/angle ranges from (0, -œÄ, 0.0375) to (0.15, œÄ, 0.0375) m/rad; weight varied from 0.015 to 0.5 kg. Evaluation metrics for complexity effects include jump-starts, time-to-threshold, asymptotic performance and cumulative reward.",
            "complexity_level": "high (multi-finger coordination, high-dimensional continuous control, and wide parameter ranges specified above)",
            "variation_measure": "Measured by randomized / intervened parameter ranges per episode: (1) block size/shape ranges (e.g., length/width/height independently sampled in [0.015,0.095] m), (2) goal position/orientation ranges (coordinates and orientation ranges as above), (3) block weight range [0.015,0.5] kg; out-of-distribution test enlarged these ranges by ~67% (task1), ~13% (task2), and ~21% (task3). Variation also described qualitatively as context-aware (observable task variables) vs context-unaware (unobserved environment variables).",
            "variation_level": "high (explicit, wide domain randomization across multiple continuous parameters; OOD increases up to ~67% used for testing)",
            "performance_metric": "Training cumulative reward; evaluation metrics: jump-start (initial reward boost), time-to-threshold (steps to reach performance threshold), asymptotic performance, fractional success (mean and std over 100 episodes, in-distribution and out-of-distribution), and qualitative convergence speed.",
            "performance_value": null,
            "complexity_variation_relationship": "Yes ‚Äî the paper explicitly analyzes trade-offs between environment complexity and variation: (1) Context-aware variation (observable task-specific changes like size or goal) benefits from whole-model (policy+value) transfer: whole-model fine-tuning gives jump-starts, faster time-to-convergence and sometimes higher asymptotic performance. (2) Context-unaware variation (unobserved environmental changes like weight) breaks value estimates from source tasks; whole-model transfer can hurt (oscillations, poor asymptotic behavior) while policy-only transfer generalizes better. (3) Curriculum design mediates the complexity √ó variation trade-off: staged curricula (2- or 3-stage) speed early learning but poorly-timed transitions cause performance drops; 2-stage curricula sometimes give fastest convergence while 3-stage curricula can improve generalization (smoother progression). The paper emphasizes timing of sub-task transitions as crucial ‚Äî too early causes insufficient skill acquisition, too late causes overfitting to the simpler stage and worse final performance.",
            "high_complexity_low_variation_performance": "qualitative: When complexity is high but variation low (e.g., fixed parameters or small randomization), pretraining and whole-model fine-tuning produced jump-starts and faster convergence; curriculum initially speeds learning but may not improve asymptotic performance (no numeric values reported).",
            "low_complexity_high_variation_performance": "qualitative: Not directly cast as low complexity + high variation; closest is context-unaware high variation (weight randomization) with simpler task structure ‚Äî policy-only transfer and 2-stage curriculum improved generalization and final reward relative to whole-model transfer and baseline, but numeric values not reported.",
            "high_complexity_high_variation_performance": "qualitative: For fully randomized hard tasks (e.g., random size + random goal + wide weight), the best strategies varied by task: Task 1 (size): 2-stage curriculum was best for convergence and robustness; Task 2 (random goal): fine-tuning (pretrain+finetune) was best; Task 3 (weight, unobserved): 2-stage curriculum gave best convergence and final reward. No absolute numeric performance values were provided in the text.",
            "low_complexity_low_variation_performance": "qualitative: In preliminary fixed-stage tasks (no randomness) agents learned quickly (used for pre-training or initial curriculum stages) and provided useful initializations for fine-tuning; quantitative numbers not reported.",
            "training_strategy": "Fine-tuning (whole-model transfer, policy-only transfer, across-task pretraining (grasp) then fine-tune), Curriculum learning (manual 2-stage and 3-stage curricula with controlled randomness progression), baseline training from scratch. Pretraining tasks included a grasping pretrain (across-task) and fixed-parameter pushing pretrain (within-task).",
            "generalization_tested": true,
            "generalization_results": "Evaluated both in-distribution and out-of-distribution (OOD) by expanding parameter ranges (~67%/13%/21% for tasks 1/2/3). Results: except for Task 2 (random goal) where curriculum was not applicable, transfer methods (best candidate per task) achieved equivalent or better generalization than baseline. Notably, cross-task fine-tuning (grasp ‚Üí push) matched within-task fine-tuning performance while reducing pretraining time by a factor of ~10. Policy-only transfer for Task 3 had modest training gains but improved generalization relative to baseline. Exact fractional success means/std were reported in figures but not numerically in the text.",
            "sample_efficiency": "Qualitative: Curriculum learning accelerated early learning (jump-starts) by simplifying early stages; fine-tuning (pretrained whole-model) reduced time-to-threshold and improved convergence for context-aware tasks; across-task pretraining (grasp‚Üípush) required ~1/10 of the pre-training time of within-task pretraining to reach comparable fine-tune performance. No absolute counts of episodes or interactions are provided in the text.",
            "key_findings": "1) Whole-model transfer (policy + value) outperforms policy-only transfer in context-aware scenarios because value estimates and policies are tightly coupled; 2) In context-unaware scenarios (unobserved environmental variation), whole-model transfer can hurt learning due to incorrect value generalization ‚Äî policy-only transfer generalizes better; 3) Across-task pretraining (grasp ‚Üí push) can achieve similar final performance as within-task pretraining while drastically reducing pretraining time (~10√ó speedup); 4) Curriculum learning (manual staged progression) speeds early learning but transition timing is critical ‚Äî optimal timing balances sufficient mastery and avoidance of overfitting to simple stages; 2-stage curricula often gave faster convergence while 3-stage curricula sometimes improved generalization; 5) Overall strategy selection should depend on whether task variations are observable (favor whole-model fine-tuning) or unobservable (favor policy-only transfer or curriculum strategies).",
            "uuid": "e1018.0",
            "source_info": {
                "paper_title": "Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causalworld: A robotic manipulation benchmark for causal structure and transfer learning",
            "rating": 2,
            "sanitized_title": "causalworld_a_robotic_manipulation_benchmark_for_causal_structure_and_transfer_learning"
        },
        {
            "paper_title": "Causal Curiosity: RL agents discovering self-supervised experiments for causal representation learning",
            "rating": 2,
            "sanitized_title": "causal_curiosity_rl_agents_discovering_selfsupervised_experiments_for_causal_representation_learning"
        },
        {
            "paper_title": "Causal Counterfactuals for improving the robustness of reinforcement learning",
            "rating": 2,
            "sanitized_title": "causal_counterfactuals_for_improving_the_robustness_of_reinforcement_learning"
        },
        {
            "paper_title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "rating": 2,
            "sanitized_title": "curriculum_learning_for_reinforcement_learning_domains_a_framework_and_survey"
        },
        {
            "paper_title": "Guided curriculum learning for walking over complex terrain",
            "rating": 1,
            "sanitized_title": "guided_curriculum_learning_for_walking_over_complex_terrain"
        }
    ],
    "cost": 0.00935625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EXPLORING CAUSALWORLD: ENHANCING ROBOTIC MANIPULATION VIA KNOWLEDGE TRANSFER AND CURRICULUM LEARNING</p>
<p>Xinrui Wang xinruiw@usc.edu 
Yan Jin yjin@usc.edu </p>
<p>Dept. of Aerospace &amp; Mechanical Engineering University of Southern California Los Angeles
USA</p>
<p>Dept. of Aerospace &amp; Mechanical Engineering University of Southern California Los Angeles
USA</p>
<p>EXPLORING CAUSALWORLD: ENHANCING ROBOTIC MANIPULATION VIA KNOWLEDGE TRANSFER AND CURRICULUM LEARNING
79E52CAB76E84F70804DB60125D5D166Artificial intelligencetransfer learningcurriculum learningreinforcement learningrobotic manipulation
This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers.By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation.To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture.Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks.Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated.To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn.The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameters.The key factors of two learning strategies and corresponding effects were explored in context-aware and context-unaware scenarios, enabling us to identify the scenarios where the methods demonstrate optimal performance, derive conclusive insights, and contribute to a broader range of learning-based engineering applications.</p>
<p>INTRODUCTION</p>
<p>Robotic manipulation, a fundamental aspect of robotics research, involves various tasks where robots interact with objects to achieve specific goals, such as grasping, pushing, and stacking them.Although many studies on robotic arm manipulation focus on using grippers [4,8,9], which rely on actuators for basic open and close movements, our research explores the transfer of knowledge in tasks that require complex control and intricate movements.To accomplish this, we have chosen a tri-finger robotic arm manipulation task from CausalWorld [10] to conduct case studies.This setup features a three-fingered arm, with each finger having a joint and an endeffector interacting with blocks.Such a configuration demands synchronized movements for grasping and lifting objects, leading to a wide range of possible finger movements and interactions, thereby significantly increasing the task's complexity.Additionally, CausalWorld allows us to modify the characteristics of the arms and blocks, such as size, shape, weight, friction, etc.This flexibility provides us an opportunity to explore the effects of such interventions on our chosen approach, as well as to assess the generalization and robustness.</p>
<p>Learning-based methods have been leveraged and demonstrated significant advancements in the robotic manipulation field [37,38].Rather than relying solely on bootstrap learning, knowledge transfer techniques enable agents to tackle target tasks with greater efficiency and effectiveness by leveraging previously acquired knowledge.In general, knowledge can be classified into explicit and implicit categories.The explicit knowledge can be conveyed explicitly through structured information sources, such as books, manuals, procedures, or expert systems [7,18,19,25].Implicit knowledge is typically shared through task illustration or execution, which can be transferred between different tasks [20].Our previous study explored implicit knowledge transfer within the ship collision avoidance domain.We integrated transfer learning into the reinforcement learning domain and proposed the transfer reinforcement learning methodology [21,22].By capturing and utilizing both image and dynamic features, the agent demonstrated adaptability to complex scenarios after being pretrained in simpler tasks.To validate the transfer reinforcement learning approach further, we have expanded our investigation from 2D to 3D environments in this study.Moreover, our previous work employed the relatively straightforward Deep Q-Network (DQN) model [40], focusing on the whole model or layer-level transfer.In pursuit of a deeper understanding of knowledge transfer, this study utilized an actor-critic based model [35], distinguishing between the policy and value functions.The model's complex structure provides opportunities to explore knowledge transfer across its different components.</p>
<p>In this study, the fine-tuning approach was employed as a method for knowledge transfer, utilizing the actor-critic model to incorporate various learning strategies.Typically, the whole model is pre-trained in the source task and transferred to the target task.When the source and target tasks have misaligned reward structures or environmental dynamics.By transferring only the policy, there's a reduced risk of importing irrelevant value estimations to the target task learning.Besides the knowledge transfer across the model structure, another variation, knowledge transfer across different tasks, was explored.As humans, we are often able to easily learn a complicated task based on a related but simpler task.For example, we can master pushing blocks to arbitrary goals quickly as an add-on skill if we ¬© 2024 by ASME already know how to grasp a block.Inspired by this observation, we sought to determine whether robot learning could follow a similar logic.</p>
<p>To eliminate the need for additional efforts for pre-training the agent, curriculum learning was employed as another option to facilitate learning in a single iteration [39].Curriculum learning is analogous to educational curricula in human learning, where foundational skills are developed through solving basic tasks before progressing to more advanced ones [31].It is an extension of transfer learning, as the knowledge gained in simpler tasks is transferred to solve more complex tasks.The primary motivation for applying curriculum learning is that, by breaking down the learning process into a series of progressively challenging tasks, the agent can not only speed up the learning process but also lead to more robust and generalizable results.The curriculum design, like what sub-tasks can be involved and how to set the schedule for the subtasks, is one of the main challenges, as it directly affects the learning process and results.</p>
<p>Upon these, we raised the following research questions for our investigation of learning strategies for robotic manipulation:</p>
<ol>
<li>
<p>How do variations in fine-tuning, such as the wholemodel or policy-only transfer and the within-task or across-task transfer, impact learning performance?</p>
</li>
<li>
<p>What are the critical design parameters for curriculum design, and how do these parameters influence the curriculum learning process?</p>
</li>
</ol>
<p>Given the distinct advantages of each learning approach, how can we determine the most suitable choice for different scenarios?</p>
<p>To address these questions, we designed a series of case studies to investigate the learning methods we proposed.The key factors of the learning approaches and the corresponding effect were explored, enabling us to identify the scenarios where the methods demonstrate optimal performance, thereby deriving conclusive insights.Furthermore, our findings are intended to contribute to a broader range of learning-based engineering applications.</p>
<p>The remaining sections of this paper are structured as follows.Section 2 provides the related work to the selected benchmark and learning methodology.Section 3 outlines the methods employed to conduct the case studies, which are detailed in Section 4. The results and discussions are covered in Section 5.The conclusion and future work are presented subsequently.</p>
<p>RELATED WORK</p>
<p>Reinforcement Learning in Robotic Manipulation</p>
<p>Recognized as a highly complex task, robotic manipulation requires a sophisticated integration of techniques from multiple disciplines like control systems, motion planning, and trajectory tracking, empowering robots to effectively interact with and manipulate objects within their surroundings [1].</p>
<p>Reinforcement learning, as an important method in the robotic manipulation domain, enables the robotic to learn the optimal policy from trial and error through interacting with the surrounding environment.Zhao et al. combined the control method with actor-critic and developed an RL-based controller to maintain safety and stability [23].Wang et al. proposed an end-to-end RL framework for robotic manipulation tasks such as grasping and pushing., utilizing the state representation learned from camera images through self-supervised autoencoder [4].Gu et al. demonstrated that an off-policy deep Q-functions-based algorithm can be applied to various complex 3D manipulation tasks in simulation and a complex door-opening skill on real robots efficiently without any prior demonstrations [5].Liu et al. provided a review of deep reinforcement learning applications in robotic manipulation, tackling the sample efficiency and generalization challenges [6].They also highlighted the remaining challenge of developing robust and versatile manipulation skills, pointing to the need for the integration of other machine learning to generate new algorithm solutions.Building upon this foundation, our research is focusing on improving the learning efficiency and robustness of highly complex robotic manipulation tasks.By integrating transfer learning and curriculum learning algorithms, we aim to address the gaps identified by Liu et al. and discuss the inherent knowledge and capabilities in various situations.</p>
<p>CausalWorld</p>
<p>CausalWorld is a causal reinforcement learning benchmark developed on the PyBullet physics engine [11], designed to facilitate research in causal structure and transfer learning within robotic manipulation tasks.This platform has become foundational for a broad spectrum of research aiming at diverse objectives.In the realm of reinforcement learning, CausalWorld plays an important role in its well-designed state and action space.Its experimental environment has been leveraged to create novel reinforcement learning models [13].Moreover, the platform's reward function is intricately designed to address a variety of manipulation tasks, enhancing its utility and flexibility for research.Allshire et al. follow the position-orientation-based success criterion and dense reward function for object reaching of CausalWorld to conduct their reward function design for various manipulation tasks like grasping and fine corrections [14].</p>
<p>For studies in causal reasoning, CausalWorld also served as a crucial benchmark due to its design centered around causal tasks and the extensive space for interventions it offers.Its carefully designed tasks and the capability for random interventions have also inspired the design of other study cases in causal reasoning [12].Meanwhile, to our knowledge, no existing research has utilized CausalWorld with the explicit aim of exploring transfer learning and curriculum learning as one of its primary objectives, particularly in examining the effects of interventions and learning curriculums as we have.The two studies most closely related to our work are Causal Curiosity [15] and Causal Counterfactuals [16].Causal Curiosity introduced a reward mechanism that encouraged agents to learn action sequences to deduce the binary quantized representation of factors in dynamic environments, with interventions applied to manipulate these factors.The learned representation could then ¬© 2024 by ASME be concatenated to the state and transferred to downstream tasks.Inspired by the intervention utilization and causal representation concatenation, Causal Counterfactuals learned the causal representations by modifying CoPhy [17], utilizing these representations to enhance the efficiency of learning in manipulation tasks.The primary aim of these two research is to efficiently uncover causal factors to facilitate policy training, whereas our research focuses on examining how these factors affect knowledge transfer and the learning process.We also employ curriculum learning to gradually familiarize the agent with the environment in a single iteration, eliminating the need for additional efforts to identify causal representations.</p>
<p>Knowledge Transfer</p>
<p>Knowledge transfer aims to leverage knowledge from source domains to facilitate the learning process in target domains, achieving efficient adaptation and quicker mastery of new tasks.In the domain of reinforcement learning, Zhu et al. [23] identified that transferable knowledge can encompass demonstrated trajectories, model dynamics, teacher policies, and value functions.Furthermore, based on our literature review, the knowledge can be transferred across task domains, from simulation to real-world settings [10,14], and from visual representations to policy formulations [13,21,24] within the reinforcement learning framework.</p>
<p>An exemplary method for transferring visual knowledge within the RL-based robotics areas is through contrastive unsupervised representation learning (CURL).This approach leveraged contrastive learning to derive visual knowledge and updated the visual encoder during the policy training process, effectively capturing and utilizing visual cues to inform policy decisions to improve improved generalization and sample efficiency [24].In studies related to CausalWorld, Yoon et al. acquired structured and semantic visual representations from images via object-centric representation (OCR) pre-training, employing the encoded OCR to enhance reinforcement learning (RL) training [13].Their evaluation focused on the efficiency of a single finger reaching a block.However, for more complex tasks, such as pushing and picking up a block with three fingers, the utility of the learned visual information proved limited.Our experiments with both CURL and OCR frameworks to extract visual information revealed that the agent was unable to learn, indicating that the visual information provided was insufficient for mapping to the optimal policy in tasks requiring intrinsic coordination and cooperation.For this study, state input containing the current and goal information for the task was used instead.</p>
<p>Our prior work in transfer reinforcement learning within the ship collision avoidance domain encompassed both feature and policy transfer [21,22].We developed an end-to-end framework that integrates a feature extractor with a policy network.The agent was pre-trained on relatively simple source tasks.Then, the whole or partially acquired knowledge was applied to target tasks of varying similarity and complexity.Despite incorporating rules, risk assessment, and dynamics in the environment, the input was solely visual, relying on 2D images, and the action space was coarsely discretized.This study builds upon our previous work by extending it to 3D scenarios, addressing more complex tasks such as robotic manipulation, and incorporating more realistic control mechanisms and dynamics.</p>
<p>Curriculum Learning</p>
<p>Curriculum learning is a strategic approach in machine learning that structures the learning process to mimic the way humans learn progressively.Several studies indicate that carefully designed curricula could lead to faster learning and the discovery of better policies compared to traditional RL approaches and enable learning in complex environments that were beyond the reach of standard RL methods [26,27].Narvekar et al. have identified key evaluation metrics for assessing the benefits of curriculum learning over noncurriculum RL approaches, including jump starts, time to threshold, and asymptotic performance [28].</p>
<p>Curriculum design is a critical challenge, and human participation plays a vital role in sequencing and evaluating intermediate tasks.The study most closely related to ours is the Guided Curriculum Learning by Tidd et al., which introduced a 3-stage curriculum for training robots to navigate complex terrains.The curriculum progressively increased in complexity while reducing guiding forces across the stages [31].In our study, we increased the complexity but avoided using guidance as we observed the agent acquired skills in a manner distinctly different from our human in this task.Rather than pre-designing the curriculum, human feedback can be integrated into the learning process.Zeng et al. introduced a human-in-the-loop curriculum, developing an interactive platform that allows humans to offer curriculum feedback and adjust task difficulty levels, customizing the reinforcement learning process to achieve desired performance outcomes [32].</p>
<p>Beyond manual design, auxiliary networks are widely used to help generate curriculum automatically.A Teacher-Student Automatic Curriculum Learning framework was proposed by Campbell et al. [29].The student network worked as an RL learner.The teacher network also worked following the Markov Decision Process and adjusted the curriculum based on the learning progress of the student network provided by gradient norms.Racaniere et al. introduced a judge model to predict the feasibility, besides the curriculum prediction model and the RL training model.The goals with different levels of validity and feasibility were sampled [30].However, in our study, where goals are of similar difficulty, and the main challenge lies in the randomness introduced to the variables in each episode, these automatic methods may not be directly applicable.Nevertheless, they provide valuable insights for future research directions.</p>
<p>METHOD</p>
<p>In this section, we describe the methodology employed for our study.Our approach involves training an agent to obtain knowledge about the task using the Soft Actor-Critic (SAC) reinforcement learning algorithm [36].To effectively transfer ¬© 2024 by ASME the acquired knowledge, we employed two distinct strategies: initially establishing a strong foundation by fine-tuning a pretrained model, which is described in 3.2.Or addressing the hard task by constructing a curriculum that introduces complexity in increasing order, allowing the agent to build upon its skills incrementally through curriculum learning, as described in 3.3.</p>
<p>Training the Agent by Reinforcement Learning.</p>
<p>SAC, a well-known model-free RL method, was selected to train the agent for its efficiency and stability in solving complex decision-making problems.SAC is built on the actor-critic architecture, where the 'actor' is a policy network that outputs a probability distribution over actions, facilitating exploration and exploitation by sampling from this distribution.On the other hand, the' critic' is a value network that evaluates these actions by estimating the Q-value.The clipped double-Q trick [33] is implemented, mirroring strategies in the Twin Delayed DDPG (TD3) [34], mitigating the overestimation issue of Q-value predictions commonly seen in deep RL.</p>
<p>Moreover, SAC integrates entropy regulation into the objective function and aims to optimize the cumulative rewards while also maximizing entropy.The objective function () of the policy  is shown in Eqn. 1, where  is a coefficient of the trade-off between the reward (  ,   ) and the entropy ‚Ñã( ‚Ä¢ |  ), governing the randomness of the optimal policy.In contrast to traditional RL algorithms that aim to find a deterministic policy maximizing the expected return, SAC seeks a stochastic policy that balances the expected return and entropy.This results in a 'soft' policy update strategy, making the learning process more stable and less prone to getting stuck in local optima.
ùêΩ(ùúã) = ‚àë ùê∏ (ùë† ùë° , ùëé ùë° )‚àºùúå ùúã [ùëü(ùë† ùë° , ùëé ùë° ) + ùõº‚Ñã( ‚Ä¢ |ùë† ùë° ) ùëá ùë°=0 )(1)
The robot's actions are represented by a 9-dimensional array corresponding to the joint positions of each finger, sampled from a predefined continuous space.As for the state input, the environment provides visual inputs consisting of current and goal images captured from three viewpoints by cameras.Additionally, the structured observation is represented by a 56dimensional array, which includes task-relevant variables such as the remaining time for the task, joint positions, joint velocities, and the linear velocities of blocks, among others, as illustrated in Figure 1.Initially, we attempted to utilize visual inputs, aiming to apply a methodology similar to that of our previous research [21,22].Our goal was to compare the results.However, despite experimenting with various CNN-based SAC models from Stable Baselines3 and other transformer-based visual encoders [13,24], and despite trying various modifications such as altering the number of input images, employing different preprocessing methods, varying the stacking techniques, and switching from images of different views to images from different time steps to create a new frame, all attempts were unsuccessful.This failure can be attributed to the difficulty in distinguishing the actions (i.e., the joint positions of the fingers) through images due to the complexity of the action space where numerous combinations can occur and the fact that the fingers are relatively small compared to the entire image.Consequently, we opted to leverage structured state input instead.</p>
<p>The implementation was carried out using the PyTorchbased Stable Baselines3 library.The hyperparameter followed the original paper [10].</p>
<p>Fine-tuning</p>
<p>The case study task involves pushing a block to a specified goal, with the block's size, weight, or the goal's position being randomly sampled to increase the difficulty of the pushing task.The primary challenge in addressing this complex manipulation task stemmed from the agent's initial inability to push the block toward the goal.Because three fingers haven't mastered foundation skills such as grasping and moving the block with cooperation.Meanwhile, the sparse reward defined by the intersection portion dominated the total reward function.The agent will receive a significant reward only when the block intersects with the goal position.Thus, the experience buffer lacked experience with high reward initially, hindering the training process.</p>
<p>¬© 2024 by ASME To address this issue, one effective strategy involves knowledge transfer, beginning with a pre-trained model as a foundation rather than starting from scratch.This approach, known as fine-tuning in the context of transfer learning, allows the agent to adapt more efficiently to the target task by leveraging prior knowledge.We tried two levels of knowledge transfer.The first one is the whole model transfer.The whole model was pre-trained in a simplified, straightforward pushing task, which took a very short time to achieve.The pre-trained parameter was utilized as the starting point for the target-pushing tasks with randomness embedment with the task variables.As illustrated in Figure 2, the actor network and critic network (including the state value network) were initialized with the pretrained parameter and fine-tuned in the target tasks.The whole model transfer includes both the policy and value functions, which might be tightly coupled with the specifics of the source task environment, including its dynamics and reward structure.If the target task operates under significantly different dynamics or objectives, the value function from the whole model might not generalize well, potentially leading to suboptimal performance.Policy-only transfer, by focusing on the decisionmaking strategy independently of the value estimation, may offer better generalization across tasks with different dynamics.Also, it allows for the application of a learned strategy within a new reward context, offering the flexibility to adapt the policy to optimize for different reward signals without being constrained by the value function's assumptions about rewards.To achieve this flexibility, we employed policy transfer from the source task to the target task.As illustrated in Figure 3, the parameters of the actor (policy) network from the model pre-trained on the source task were transferred to the target network for fine-tuning on the target task while the critic network was randomly initialized.The outcomes of both approaches are discussed in Section 5.Although the fine-tuning method shows promise in enhancing learning efficiency, the necessity for pre-training should also be considered.To address this, we explored curriculum learning as a strategy to train the agent to tackle the challenging task in a single training session through designing an effective curriculum, eliminating the need for pre-training and fine-tuning.</p>
<p>As previously mentioned, in highly complex tasks, it is impossible for the agent to get close to the goal and receive the sparse reward in the early episodes.Our goal was to provide guidance that increased the likelihood of the agent obtaining this reward early, thereby facilitating the accumulation of valuable experience as swiftly as possible.Driven by this motivation, we broke down the original task into simpler sub-tasks of varying complexity to design the curriculum.The task that rapidly enhanced performance was identified through trials and implemented as the initial stage of the curriculum, termed the  .In this phase, all randomness was eliminated, and task variables were fixed to ensure a focused and effective learning environment.After acquiring knowledge in the first stage, randomness within a small range was introduced to the  .This step was designed to help the agent manage variations and achieve generalization.Subsequently, the level of randomness was gradually increased to the desired level in the  , enabling the agent to fully master the original task.Throughout this process, knowledge was learned and transferred stage by stage.</p>
<p>FIGURE 4: Illustration of curriculum design</p>
<p>The overall curriculum design is conceptually shown in Figure 4.Note that it doesn't exactly show the exact curriculum we used.In the experiments, the   may be skipped, the length of each stage may be varied as needed.</p>
<p>CASE STUDY DESIGN ¬© 2024 by ASME</p>
<p>The case study involved pushing a block to a designated goal, introducing randomness in both task-specific and environmental variables.During training, the agent was only provided with task-specific variables as state inputs for the neural networks, while changes in environmental variables, such as weight and friction, remained undisclosed.One might question the exclusion of environmental variables from the observation space.Our experimental findings indicated that incorporating environmental variables could detrimentally affect performance.This can be because, although environmental variables are relevant to the manipulation task, they are not as critical as task-specific variables that directly indicate the goal and current progress.Including environmental variables tended to distract the agent's focus from the primary task.Consequently, we designed tasks that were either context-aware by varying task-specific variables (such as size and goal positions) or context-unaware by altering environmental variables (like weight) to investigate both fine-tuning and curriculum learning methods.The challenge in the context-aware scenario was to adjust the policy to account for observable changes.In contrast, the context-unaware scenario demanded the development of a generalized policy capable of performing effectively across a range of unknown changes.</p>
<p>To ensure the task's difficulty level, we expanded the range of variables beyond the original task settings [10].For  1, the block's length, width, and height were independently sampled from a range of 0.035 to 0.095 meters, potentially resulting in an irregular cuboid. 2 involved pushing the block to a goal position within a cylindrical coordinate system, with the position varying between (0, ‚àí, 0.0375) and (0.15, , 0.0375) meters and the orientation ranging from ‚àí to  radians. 3 required pushing a block with a weight that varied from 0.015 to 0.5 kilograms.After trials and performance comparisons, the curriculum for each task was manually designed and is detailed in Table 1.The fixed value was applied to the   .For the   and   , the ranges provided were used to randomly sample the variables for each episode.The timing of progressing through tasks is crucial.We aimed to investigate whether a sub-task should be fully mastered before advancing to the next level or if it is more effective to introduce the next level as soon as the agent shows some, but not perfect, progress on the current sub-tasks.We explored different timings to determine the most appropriate curriculum structure.Additionally, it's important to note that for the fine-tuning method, the model was pre-trained on the preliminary task to reduce pre-training time and then transferred to the original task (which is the same as the advanced task) for fine-tuning.</p>
<p>RESULTS AND DISCUSSION</p>
<p>Fine-tuning Results and Discussion</p>
<p>In this subsection, we present the results from fine-tuning case studies and discuss the features and implications of the results obtained from within-task and across-task cases.</p>
<p>Fine-tuning within the tasks</p>
<p>Both whole network and policy transfer methods were implemented for context-aware scenarios in  1 (random size) and  2 (random goal), as well as for the contextunaware scenario in  3 (random weight).The model was pre-trained on pushing the block with fixed task parameters and subsequently fine-tuned for pushing blocks in a dynamic environment, incorporating variations in both task and environmental parameters, as described in Section 4. The cumulative reward obtained throughout the learning process is depicted in Figures 5 to 7. Each line in these figures was produced using 10 random seeds and has been smoothed for clarity.</p>
<p>We utilized the jump starts, time to threshold, and asymptotic performance evaluation metric [28] to access the learning process of the fine-tuning method against the baseline (learning from scratch).It was observed that whole network transfer altered the learning process in context-aware scenarios for  1 and  2 by providing jump-starts and reducing the time to convergence, significantly enhancing learning efficiency.Furthermore, the asymptotic performance of the whole model transfer method exceeded that of the baseline in  2, demonstrating that the whole model transfer and finetuning improved the learning process by knowledge reuse and adaptation.However, the policy-only transfer method did not show significant improvement, indicating that the policy network alone lacked sufficient knowledge to boost the learning process.This may also suggest a tight coupling between the policy network and the value network during the update process; thus, transferring only the policy without value evaluation does not accelerate convergence.</p>
<p>In  3 , which is a context-unaware scenario, different from the context-aware scenario, the policy-only transfer demonstrated better performance than the whole model transfer and slightly better than the baseline.The potential reason is that, despite changes in weight in the target task, these were not observed by the agent due to the unawareness setting.The agent initially treated it similarly to the source task, and the pre-learned policy provided some initial insight.However, while the whole network transfer achieved jump starts in the early stage, it experienced significant oscillations and performed much worse compared to the baseline.This may be due to the fact that the difference between the source task and the target task (weight ¬© 2024 by ASME variation) was not observable by the model, leading to inaccurate value estimations based on the observations.Moreover, the pretrained whole network, having converged to the optimal solution in the source task, may have caused the fine-tuning process to become stuck in local optima.</p>
<p>Fine-tuning across the tasks</p>
<p>As humans, we are often able to easily learn a complicated task based on a related but simpler task.For example, we can master pushing blocks to arbitrary goals quickly as an add-on skill if we already know how to grasp a block.Inspired by this observation, we sought to determine whether robot learning could follow a similar logic.If so, this could lead to a more efficient training pipeline.Additionally, pre-training the robot to grasp a block required significantly less time than pre-training it to push a block to a fixed goal in the fine-tuning experiment above.</p>
<p>The pre-training task was designed to have the robot grasp the block from the same initial position as in the pushing task.The desired outcome was for the distance between the three fingers and the block to be zero, with minimal movement of the block, ensuring it does not move to an unintended location before the goal is known.The reward function, defined in Equation 3, uses ‚Äñ  ‚àí  ‚àí1 ‚Äñ to represent the distance between the current and previous block locations, serving to limit block movement.This reward function design closely mirrored that of the pushing task in Eqn. 2, aiming to maintain a similar reward function structure and value estimation distribution.</p>
<p>The results, presented in Figure 8, are labeled as "acrosstask" to indicate that the agent was pre-trained on grasping and then fine-tuned for pushing.These results were compared with the previous fine-tuning outcomes shown in Figure 6, which are labeled as "within-task" transfer in this comparison.We observed that across-task fine-tuning achieved performance very similar to that of within-task fine-tuning but required only onetenth of the pre-training time.From this, we can deduce that, from the knowledge acquired during the fixed-goal-pushing pretraining, only the grasping skill was applicable to the randomgoal-pushing task.The skill of moving with forward momentum could be developed based on the learned grasping skill during the fine-tuning process.This suggests that the entire pushing task can be broken down into simpler pre-training and fine-tuning stages.Furthermore, this finding provides us with future research directions, as other complex tasks like picking, placing, and stacking blocks may also benefit from following this training pipeline.3-stage curriculum learning following the curriculum in section 4 has been applied to  1, as shown in Figure 9.By observing the learning process, we can see that learning efficiency was boosted a lot in the earlier stage compared to the baseline.This improvement is attributed to the agent focusing on simpler sub-tasks, such as pushing the block with a fixed size or minor shape variations, which resulted in more valuable experiences being stored in the experiment buffer.However, performance experienced a drop each time the sub-task was changed, indicating that the timing of sub-task transitions significantly impacted learning outcomes.Furthermore, while 3stage learning enhanced learning effectiveness in the initial episodes, it did not lead to improvements in the time to threshold or asymptotic performance due to two instances of performance decline.</p>
<p>¬© 2024 by ASME To achieve faster convergence, a 2-stage curriculum learning (skipping the   ) strategy was also applied to  1.This approach was chosen because it resulted in only one performance drop during the learning process.The timing of the transition to the next sub-task became even more critical, as depicted in Figure 10.Although the performance drop in the 2-stage curriculum was more pronounced than in the 3stage curriculum-due to a larger increase in complexity-the optimal timing for changing sub-tasks (at 1200 timesteps) significantly reduced the time to converge and enhanced the asymptotic performance.Changing the sub-task at 800 timesteps proved too early, as insufficient knowledge had been acquired and performance had not reached a sufficiently high level.</p>
<p>Conversely, transitioning at 1800 timesteps was too late, as prolonged engagement with a simpler task negatively impacted the agent's ability to achieve as high a final reward when faced with a more complex task compared to the optimal timing.We applied a similar procedure to  2 and  3 to identify the optimal curriculum design and sub-task transition timing and to compare these with the fine-tuning method in order to determine the most suitable overall learning strategy for each task.</p>
<p>Contrary to  1, the results for  2, as shown in Figure 12, indicate that the 3-stage curriculum outperformed the 2-stage curriculum.The gradual increase in complexity led to improved learning performance with respect to goal position randomization.However, none of the curriculum learning candidates showed improvement in terms of total reward.The optimal solution for pushing a block to an arbitrary goal was found to be fine-tuning the pre-trained model.Although the curriculum led to an increase in rewards when reaching the fixed goal in the , this effect disappeared when dealing with random goals in more challenging curriculums.This suggests that changing the goal position significantly alters the task process, requiring the agent to spend more time fine-tuning rather than quickly mastering it within a curriculum with limited learning time.As the result for context-unaware  3 shown in Figure 13, the best solution is 2-stage curriculum learning.As we discussed in 5.1.1,the fine-tuning method didn't improve the learning process well due to the difference between the source task and the target task being unobserved.For curriculum learning, unlike in  1 , introducing the advanced task BEFORE the agent fully mastered the preliminary task yielded the best results in terms of convergence speed and the final ¬© 2024 by ASME reward as shown with the red line.Since the change was unobserved, performance did not suffer the drop typically seen in context-aware scenarios.Introducing an   midway through the process (indicated by the purple line) did not enhance efficiency compared to the early-stage progression of the 2-stage curriculum (indicated by the red line).This is because adding the   at 400 steps in the 3stage curriculum had a similar effect to introducing the advanced task at the same point in the 2-stage curriculum.Furthermore, the performance drop associated with progressing to the   was not alleviated by the inclusion of the   in the 3-stage curriculum.However, did help mitigate the performance drop compared to when the   was introduced directly at 800 steps (indicated by the orange line).</p>
<p>In-distribution and Out-of-distribution Evaluation</p>
<p>The best option from each learning method was evaluated over 100 episodes for both in-distribution and out-of-distribution tests.The out-of-distribution tests extended the randomness range of  1,  2 and  3 by approximately 67%, 13%, and 21%, respectively, representing the maximum applicable value for each task.The mean and standard deviation of the fractional success for each task are presented in Figure 13.</p>
<p>From the results, we can find that except for  2, where the curriculum learning method was not applicable, all the transfer learning methods achieved equivalent generalization capability or surpassed the baseline.It demonstrated that the knowledge can be transferred through pre-training and finetuning or within the curriculum.</p>
<p>The evaluation rankings were highly consistent with the order of rewards, indicating a strong correlation between learning efficiency and generalization.For example,  1, the best solution 2-stage curriculum learning selected in 5.2 has also been proved to be the most robust and generalizable.</p>
<p>There were some inconsistencies between training rewards and testing results, highlighting the impact of transfer learning from various perspectives.In  2, although 3-stage learning did not show a clear efficiency improvement, as depicted in Figure 11, the evaluation results demonstrated enhanced generalization, indicating that the 3-stage curriculum contributed to better generalization.Also, it was surprising to observe that although the best fine-tune candidate (policy-only transfer) of  3 didn't show much improvement regarding the learning process, it exhibited better generalization compared to the baseline and 3-stage curriculum for both in-distribution and outof-distribution tests.In-Distribution and Out-of-Distribution.</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>This study presented a comprehensive examination of the use of knowledge transfer strategies, namely fine-tuning and curriculum learning, in the context of robotic manipulation tasks utilizing a tri-finger robotic arm.Through extensive experiments within the CausalWorld platform, we demonstrated the effectiveness of these strategies in enhancing learning efficiency and robustness in complex manipulation tasks.The key findings of our work are summarized as follows:</p>
<p>‚Ä¢ Whole network transfer vs. policy-only transfer: The whole network transfer demonstrated superiority over policy-only transfer in scenarios requiring context-awareness.This highlights the interdependence between the policy network and the value network throughout the learning process.</p>
<p>‚Ä¢ Cross-task fine-tuning efficiency: Cross-task fine-tuning proved to be highly efficient, achieving comparable training outcomes to within-task fine-tuning while significantly reducing the need for pre-training time.</p>
<p>‚Ä¢ Context-unaware scenario adaptability: In context-unaware scenarios, transferring the complete model from a source task proved detrimental to learning, likely due to unobserved environmental variations.However, policy-only transfer was not only more effective but also exhibited enhanced generalization capabilities.</p>
<p>‚Ä¢ Curriculum learning design parameters: In curriculum learning, the sequencing of learning stages, the relevance of sub-task contexts, and the timing of transitions emerged as pivotal factors for successful task mastery.</p>
<p>‚Ä¢ Sub-task transition timing: Premature transitions to subsequent sub-tasks hindered the agent's knowledge acquisition.Conversely, excessive delays caused an overfitting to simpler tasks, underscoring the importance of well-timed progressions.</p>
<p>‚Ä¢ Stage-based learning efficacy: A 2-stage learning approach accelerated the learning process, allowing the agent to apply basic skills to more complex real tasks promptly.In contrast, a 3-stage curriculum ensured a more gradual and smooth ¬© 2024 by ASME learning experience, ultimately leading to better generalization.</p>
<p>These findings showed the key factors of each learning method and their effects.They also highlight the strategy of selecting the appropriate learning based on various task conditions, providing some insights for designing training procedures for complex manipulation tasks.Building on the insights gained from this study, our future research endeavors will focus on several directions:</p>
<p>‚Ä¢ Extended application of cross-task fine-tuning: Given the promising results observed with cross-task fine-tuning, we plan to explore its application to a broader range of complex robotic manipulation tasks.Investigating the transferability of skills across different task domains could further enhance learning efficiency and generalization.</p>
<p>‚Ä¢ Automatic curriculum generation: While manual curriculum design has proven effective, it requires significant effort and domain knowledge.Future work will explore automated methods for curriculum generation, aiming to dynamically adjust the learning process based on the agent's performance.This could lead to more adaptable and scalable learning strategies for complex engineering problems.</p>
<p>‚Ä¢ Broader exploration of knowledge transfer and curriculum learning: We intend to apply the principles of knowledge transfer and curriculum learning to other engineering scenarios beyond robotic manipulation.By exploring these strategies in diverse applications, we aim to uncover universal guidelines that can facilitate the development of advanced learning-based solutions.</p>
<p>FIGURE 1 :
1
FIGURE 1:Illustration of structured state input[10] A compound reward function from the original paper[10] was utilized, as shown in Eqn. 2.   represented the intersection area between the current block position and the desired goal position, while   represented the union of the two positions.(, ) represents the distance between the block(object) and the goal.(, ) represents the distance from the end-effector of the robotic arm to the block.The first component was designed to guide the agent in pushing the block toward the desired goal through sparse rewards.It became greater than 0 only when the block intersected with the goal, which was hard to achieve at the beginning.Meanwhile, the second component encouraged the agent to move the block towards the goal by reducing (, ) at each time step, complementing the sparse reward with more continuous feedback.Similarly, the third component motivated the agent to move the finger closer to the block for grasping.The weights for each component were determined through reward engineering.</p>
<p>FIGURE 2 :
2
FIGURE 2: Whole model transfer and fine-tuning pipeline</p>
<p>FIGURE 3 :
3
FIGURE 3: Policy transfer and fine-tuning pipeline 3.3 Designing the Learning Curriculum.Although the fine-tuning method shows promise in enhancing learning efficiency, the necessity for pre-training should also be considered.To address this, we explored curriculum learning as a strategy to train the agent to tackle the challenging task in a single training session through designing an effective curriculum, eliminating the need for pre-training and fine-tuning.As previously mentioned, in highly complex tasks, it is impossible for the agent to get close to the goal and receive the sparse reward in the early episodes.Our goal was to provide guidance that increased the likelihood of the agent obtaining this reward early, thereby facilitating the accumulation of valuable experience as swiftly as possible.Driven by this motivation, we broke down the original task into simpler sub-tasks of varying complexity to design the curriculum.The task that rapidly enhanced performance was identified through trials and implemented as the initial stage of the curriculum, termed the  .In this phase, all randomness was eliminated, and task variables were fixed to ensure a focused and effective learning environment.After acquiring knowledge in the first stage, randomness within a small range was introduced to the  .This step was designed to help the agent manage variations and achieve generalization.Subsequently, the level of randomness was gradually increased to the desired level in the  , enabling the agent to fully master the original task.Throughout this process, knowledge was learned and transferred stage by stage.</p>
<p>FIGURE 5 :FIGUR 6 :
56
FIGURE 5:  1 fine-tuning learning process</p>
<p>FIGURE 7 :
7
FIGURE 7:  3 fine-tuning learning process 5.1.2Fine-tuning across the tasksAs humans, we are often able to easily learn a complicated task based on a related but simpler task.For example, we can master pushing blocks to arbitrary goals quickly as an add-on skill if we already know how to grasp a block.Inspired by this observation, we sought to determine whether robot learning could follow a similar logic.If so, this could lead to a more efficient training pipeline.Additionally, pre-training the robot to grasp a block required significantly less time than pre-training it to push a block to a fixed goal in the fine-tuning experiment above.The pre-training task was designed to have the robot grasp the block from the same initial position as in the pushing task.The desired outcome was for the distance between the three fingers and the block to be zero, with minimal movement of the block, ensuring it does not move to an unintended location before the goal is known.The reward function, defined in Equation 3,</p>
<p>= ‚àí100 *   (, ) ‚àí 250 * ‚Äñ  ‚àí  ‚àí1 ‚Äñ ‚àí 750*(  (, ) ‚àí  ‚àí1 (, ))</p>
<p>FIGURE 8 :
8
FIGURE 8:  2 3-stage-curriculum learning process.5.2 Curriculum Learning Results and Discussion.3-stagecurriculum learning following the curriculum in section 4 has been applied to  1, as shown in Figure9.By observing the learning process, we can see that learning efficiency was boosted a lot in the earlier stage compared to the baseline.This improvement is attributed to the agent focusing on simpler sub-tasks, such as pushing the block with a fixed size or minor shape variations, which resulted in more valuable experiences being stored in the experiment buffer.However, performance experienced a drop each time the sub-task was changed, indicating that the timing of sub-task transitions significantly impacted learning outcomes.Furthermore, while 3stage learning enhanced learning effectiveness in the initial episodes, it did not lead to improvements in the time to threshold or asymptotic performance due to two instances of performance decline.</p>
<p>FIGURE 9 :
9
FIGURE 9:  1 3-stage-curriculum learning process.</p>
<p>FIGURE 10 :
10
FIGURE 10:  1 2-stage-curriculum learning process.The top performers from both the 3-stage and 2-stage curriculum learning approaches were compared with the best candidate from the fine-tuning method.The results are presented in Figure11.All learning surpassed the baseline, highlighting the advantages of knowledge transfer.Specifically, the 2-stage curriculum, with the sub-task transition occurring at 1200 timesteps, emerged as the optimal solution for pushing blocks of varied sizes.</p>
<p>FIGURE 11 :
11
FIGURE 11: Comparing the best options of each learning strategy in  1</p>
<p>(a) 2 -
2
stage-curriculum (b) Comparison of the best option learning process.Of each learning strategy</p>
<p>FIGURE 12 :
12
FIGURE 12: Curriculum learning and comparisons in  2</p>
<p>(a) 2 -
2
stage-curriculum (b) Comparison of the best option learning process.ofeach learning strategy</p>
<p>FIGURE 13 :
13
FIGURE 13: Curriculum learning and comparisons in  3</p>
<p>FIGURE 13 :
13
FIGURE 13: Mean and Standard Deviation of Fractional Success:</p>
<p>Table 1 . Curriculum details
1ùíëùíìùíÜùíçùíäùíéùíäùíèùíÇùíìùíöùíäùíèùíïùíÜùíìùíéùíÜùíÖùíäùíÇùíïùíÜùíÇùíÖùíóùíÇùíèùíÑùíÜùíÖùíïùíÇùíîùíåùíïùíÇùíîùíåùíïùíÇùíîùíåùíïùíÇùíîùíå ùüè (size)0.075[0.015,0.095][0.015,0.095]ùíïùíÇùíîùíå ùüê (goal)(0.08, ùúã/2, 0.0375)[(0.04, 0, 0.0375), (0.12, ùúã, 0.0375)][(0, ‚àíùúã, 0.0375), (0.15, ùúã, 0.0375)]ùíïùíÇùíîùíå ùüë (weight)0.15[0.05, 0.25][0.015, 0.5]
ACKNOWLEDGEMENTSWe adapted the CausalWorld code by extending the intervention space, incorporating customized tasks, interventions, curriculums and evaluation pipeline, and modifying the reinforcement learning algorithm to suit our case study.The code and implementations can be provided by the corresponding author upon reasonable request.This paper is based on the work supported in part by the Autonomous Ship Consortium (ASC) with members of BEMAC Corporation, ClassNK, MTI Co. Ltd., Nihon Shipyard Co. (NSY), Tokyo KEIKI Inc., and National Maritime Research Institute of Japan.The authors are grateful for their support and collaboration on this research.
Autonomous multi-robot servicing for spacecraft operation extension. Longsen Gao, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>NLBAC: A Neural Ordinary Differential Equations-based Framework for Stable and Safe Reinforcement Learning. Liqun Zhao, arXiv:2401.131482024arXiv preprint</p>
<p>Stable and Safe Reinforcement Learning via a Barrier-Lyapunov Actor-Critic Approach. L Zhao, K Gatsis, A Papachristodoulou, 10.1109/CDC49753.2023.103837422023 62nd IEEE Conference on Decision and Control (CDC). Singapore, Singapore2023</p>
<p>End-to-end reinforcement learning of robotic manipulation with robust keypoints representation. Tianying Wang, 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE2022</p>
<p>Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. Shixiang Gu, IEEE international conference on robotics and automation. 2017. 2017IEEEICRA</p>
<p>Deep reinforcement learning for the control of robotic manipulation: a focussed mini-review. Rongrong Liu, Robotics. 10222021</p>
<p>Knowledge engineering within a generalized Bayesian framework. Stephen W Barth, Steven W Norton, Machine Intelligence and Pattern Recognition. North-Holland19885</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Conference on Robot Learning. PMLR2023</p>
<p>ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. Ali Shafiee, ACM SIGARCH Computer Architecture News. 442016</p>
<p>Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. Ossama Ahmed, arXiv:2010.042962020arXiv preprint</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. Erwin Coumans, Yunfei Bai, 2016</p>
<p>Generalizing goal-conditioned reinforcement learning with variational causal reasoning. Wenhao Ding, Advances in Neural Information Processing Systems. 352022</p>
<p>An investigation into pre-training object-centric representations for reinforcement learning. Jaesik Yoon, arXiv:2302.044192023arXiv preprint</p>
<p>Transferring dexterous manipulation from gpu simulation to a remote real-world trifinger. Arthur Allshire, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning. Sumedh A Sontakke, International conference on machine learning. PMLR2021</p>
<p>Causal counterfactuals for improving the robustness of reinforcement learning. Tom He, Jasmina Gajcin, Ivana Dusparic, arXiv:2211.055512022arXiv preprint¬© 2024 by ASME</p>
<p>Cophy: Counterfactual learning of physical dynamics. Fabien Baradel, arXiv:1909.120002019arXiv preprint</p>
<p>Long-Range Risk-Aware Path Planning for Autonomous Ships in Complex and Dynamic Environments. Chuanhui Hu, Yan Jin, Journal of Computing and Information Science in Engineering. 23410072023</p>
<p>A new method for knowledge representation in expert system's (XMLKR). Mehdi Bahrami, Siavosh Kaviani, 2008 First International Conference on Emerging Trends in Engineering and Technology. IEEE2008</p>
<p>Multi-modal facial action unit detection with large pre-trained models for the 5th competition on affective behavior analysis in-the-wild. Yufeng Yin, arXiv:2303.105902023arXiv preprint</p>
<p>International Design Engineering Technical Conferences and Computers and Information in Engineering Conference. Xinrui Wang, Yan Jin, 2022American Society of Mechanical Engineers86212Work Process Transfer Reinforcement Learning: Feature Extraction and Finetuning in Ship Collision Avoidance</p>
<p>International Design Engineering Technical Conferences and Computers and Information in Engineering Conference. Xinrui Wang, Yan Jin, 2023American Society of Mechanical Engineers87318Transfer Reinforcement Learning: Feature Transferability in Ship Collision Avoidance</p>
<p>Transfer learning in deep reinforcement learning: A survey. Zhuangdi Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. Michael Laskin, Aravind Srinivas, Pieter Abbeel, International Conference on Machine Learning. PMLR2020</p>
<p>Path Planning for Autonomous Systems Design: A Focus Genetic Algorithm for Complex Environments. Chuanhui Hu, Yan Jin, Journal of Autonomous Vehicles and Systems. 242022</p>
<p>Self-paced deep reinforcement learning. Pascal Klink, Advances in Neural Information Processing Systems. 332020</p>
<p>An optimization framework for task sequencing in curriculum learning. Francesco Foglino, Christiano Coletto Christakou, Matteo Leonetti, Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics. IEEE2019. 2019</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, The Journal of Machine Learning Research. 2112020</p>
<p>Automatic Curriculum Learning with Gradient Reward Signals. Ryan Campbell, Junsang Yoon, arXiv:2312.135652023arXiv preprint</p>
<p>Automated curriculum generation through setter-solver interactions. Sebastien Racaniere, International conference on learning representations. 2019</p>
<p>Guided curriculum learning for walking over complex terrain. Brendan Tidd, Nicolas Hudson, Akansel Cosgun, arXiv:2010.038482020arXiv preprint</p>
<p>Human Decision Makings on Curriculum Reinforcement Learning with Difficulty Adjustment. Yilei Zeng, arXiv:2208.029322022arXiv preprint</p>
<p>Double Q-learning. Hado Hasselt, Advances in neural information processing systems. 201023</p>
<p>Addressing function approximation error in actor-critic methods. Scott Fujimoto, Herke Hoof, David Meger, International conference on machine learning. PMLR2018</p>
<p>Advances in neural information processing systems. Vijay Konda, John Tsitsiklis, 199912Actor-critic algorithms</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, International conference on machine learning. PMLR2018</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. Sergey Levine, The International journal of robotics research. 372018</p>
<p>Sim-to-real robot learning from pixels with progressive nets. Andrei A Rusu, 2017</p>
<p>Curriculum learning. Yoshua Bengio, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Human-level control through deep reinforcement learning. Mnih, Volodymyr, nature. 5182015</p>            </div>
        </div>

    </div>
</body>
</html>
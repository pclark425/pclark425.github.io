<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3302 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3302</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3302</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4ab41d9780f1d1ac34d39fa7e527e73652507fcc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4ab41d9780f1d1ac34d39fa7e527e73652507fcc" target="_blank">GreaseLM: Graph REASoning Enhanced Language Models for Question Answering</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> GreaseLM is a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances in the context to inform the graph representations of knowledge.</p>
                <p><strong>Paper Abstract:</strong> Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3302.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3302.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GreASELM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GreASELM: Graph Reasoning Enhanced Language Models for Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-layer LM+KG architecture that bidirectionally fuses pretrained transformer token representations with graph neural network (GNN) node representations via layerwise modality-interaction bottlenecks (interaction token/node), enabling joint reasoning over situational text and structured knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GreASELM (seeded with RoBERTa-Large / AristoRoBERTa / SapBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture stacks N unimodal LM transformer layers (pretrained LM) followed by M GreASELM layers; each GreASELM layer runs a transformer LM block for tokens, a GNN over a retrieved KG subgraph, and a modality-interaction (MInt) MLP mixing an interaction token and an interaction node to allow bidirectional, multi-layer information exchange. KG subgraphs are retrieved per-example and node embeddings are initialized from BERT-like entity encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~359M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['multi-layer cross-modal fusion (LM <-> KG)', 'GNN message-passing over retrieved KG subgraph', 'modality-interaction bottleneck (interaction token + interaction node)', 'retrieval-augmented knowledge grounding']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multi-layer cross-modal fusion: after each LM and GNN layer the special interaction token (in LM) and interaction node (in GNN) are concatenated and passed through an MLP (MInt) to mix modalities; the updated interaction representations are then used by subsequent LM layers and GNN layers so that token-level representations receive grounded KG information and KG node representations are contextualized by the language. GNN uses graph-attention style message passing; KG retrieval returns a focused subgraph per QA example.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse: GreASELM implements a *diverse set of reasoning styles* by combining transformer-style token reasoning (contextual, distributed representations) with structured GNN multi-hop reasoning over an explicit KG and an explicit modality-mixing operator; this contrasts with single-style or one-way fusion approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA; MedQA-USMLE (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice question answering benchmarks: CommonsenseQA (5-way, general commonsense), OpenBookQA (4-way, elementary science + fact book), MedQA-USMLE (4-way, medical licensing exam style clinical questions). Each requires reasoning beyond surface patterns and often external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (in-house split): GreASELM IHdev 78.5% (±0.5), IHtest 74.2% (±0.4). OpenBookQA test accuracy 84.8%. MedQA-USMLE (with SapBERT seed): +GreASELM 38.5% (SapBERT baseline 37.2%). Reported improvements over fine-tuned LMs: +5.5% (CommonsenseQA), +6.6% (OpenBookQA), +1.3% (MedQA-USMLE) in the paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparisons/ablations: removing modality interaction drops CommonsenseQA dev from 78.5% → 76.5% (approx QA-GNN level). Interaction every-other-layer yields 76.3%. No parameter sharing for MInt: 77.1%. Random node initialization collapses performance to 60.8% (vs 78.5% with BERT-based node init); using TransE yields 77.7%. Connecting the interaction node to all retrieved subgraph nodes (rather than only linked entities) hurts performance (−0.9%). Varying M (GreASELM layers) shows M=5 best; M=7 decreases performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Layerwise bidirectional fusion of LM and KG (via interaction token/node) is more expressive and yields higher QA accuracy than one-way or shallow fusion approaches; GreASELM shows larger gains on questions with textual nuance (negation, hedging) and with multiple explicit constraints (prepositional phrases). The modality interaction bottleneck and good KG node initialization are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Design choices that reduce fusion expressivity hurt performance: skipping modality-interaction layers, random KG node initialization, connecting interaction node to all nodes (overloading the bottleneck), and too many GreASELM layers (M=7) produce worse results. No case reported where adding modality interaction harmed performance compared to LM-only when KG retrieval succeeded, but there is a small domain-dependent improvement on MedQA-USMLE (only +1.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GreaseLM: Graph REASoning Enhanced Language Models for Question Answering', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3302.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3302.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QAGNN: Reasoning with language models and knowledge graphs for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A joint LM+GNN QA approach that initializes a context node in a GNN with a pooled LM representation and then performs GNN message passing over a retrieved KG subgraph; interactions between modalities are performed by seeding the GNN from a single pooled LM vector.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QAGNN: Reasoning with language models and knowledge graphs for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QA-GNN (LM+GNN; pooled LM seeding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM encodes text; the final LM pooled representation seeds a context node in a GNN which then performs multi-hop message passing over the retrieved KG. Interaction is effectively one-shot (LM → GNN) via the pooled vector rather than iterative per-token per-node exchanges.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~360M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['GNN multi-hop message passing', 'single pooled LM-to-GNN seeding (one-way interaction)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Uses a pooled LM vector (single summary of the text context) to initialize a node in the GNN; the GNN performs multi-hop attention/message passing to propagate information through the KG and produce answer scores. Interaction is primarily one-way and compresses textual nuance into one vector.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar/single-style: QA-GNN uses a single dominant interaction style (pooled LM representation seeding GNN) rather than diverse per-token ↔ per-node iterative interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same MCQA benchmarks as above used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (in-house): IHdev 76.5% (±0.2), IHtest 73.4% (±0.9). OpenBookQA test accuracy 82.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared directly to GreASELM: GreASELM achieves higher accuracy (CommonsenseQA IHdev: 78.5% vs 76.5%; IHtest: 74.2% vs 73.4%; OpenBookQA: 84.8% vs 82.8%). The paper attributes gains to GreASELM's multi-layer bidirectional token/node interactions versus QA-GNN's pooled-vector compression.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QA-GNN is a strong LM+KG baseline but compressing the textual context to a single vector limits cross-relationships between language tokens and KG entities; GreASELM's per-layer multi-token ↔ node mixing captures nuanced constraints better.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>QA-GNN and GreASELM perform comparably on questions with no prepositional phrases (simpler items). QA-GNN sometimes over-attends to frequently-mentioned entities in the context because its context node is not reformulated by token-level language representations across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GreaseLM: Graph REASoning Enhanced Language Models for Question Answering', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3302.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3302.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MHGRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MHGRN: Scalable multi-hop relational reasoning for knowledge-aware question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-hop GNN-based method that uses language representations to augment graph reasoning over KG subgraphs for QA, focusing on scalable multi-hop relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scalable multi-hop relational reasoning for knowledge-aware question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MHGRN (LM-augmented GNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Computes node embeddings for KG entities and runs a multi-head GNN for multi-hop relational reasoning; leverages textual signals to augment KG reasoning but uses limited cross-modal interaction compared to GreASELM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['multi-hop GNN message passing', 'text-informed graph reasoning (one-way augmentation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MHGRN uses GNNs to perform multi-hop relational reasoning over KG subgraphs; text is used to inform node relevance/initialization but interaction between modalities is not iteratively mixed across multiple LM layers.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar/single-style: primarily GNN-based multi-hop relational reasoning with text used to initialize/augment graph features (one-way or limited interaction).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA (benchmarks where MHGRN is a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice QA requiring multi-hop and relational knowledge reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (in-house): IHdev 74.5% (±0.1), IHtest 71.1% (±0.8). OpenBookQA test accuracy 80.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>MHGRN outperforms simple shallow baselines but is outperformed by QA-GNN and GreASELM; GreASELM's multi-layer fusion yields higher accuracy than MHGRN on the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GNN multi-hop relational reasoning helps QA, but limited cross-modal fusion constrains the capacity to handle fine-grained textual nuance; stronger gains are realized when LM and KG representations are allowed to iteratively and bi-directionally interact.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MHGRN provides competitive results but does not capture as much improvement on nuanced or multi-constraint questions as GreASELM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GreaseLM: Graph REASoning Enhanced Language Models for Question Answering', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3302.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3302.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KagNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KagNet: Knowledge-aware graph networks for commonsense reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts KG facts into structural features/paths and uses them to augment a language model for commonsense QA (KG facts used to inform text representations via path-aware features).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>KagNet: Knowledge-aware graph networks for commonsense reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KagNet (knowledge-to-text augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extracts paths (multi-hop) in the KG relevant to the question and encodes those paths to augment textual representations; fusion is comparatively shallow (KG information used to augment LM inputs/features rather than deep layerwise mixing).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['path-based KG augmentation', 'knowledge-to-text feature augmentation (shallow fusion)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>KG paths are encoded and used as additional structured features or inputs that augment the LM; reasoning relies on path representation extraction and LM processing rather than iterative token↔node mixing.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar/single-style: uses KG-derived features to augment LM inputs (shallow fusion), not iterative bidirectional fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Commonsense multiple-choice QA requiring background knowledge and reasoning over relations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CommonsenseQA (in-house): IHdev 73.5% (±0.2), IHtest 69.0% (±0.8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>KagNet yields modest gains over LM-only baselines in some settings but is outperformed by multi-hop GNN methods (MHGRN, QA-GNN) and by GreASELM which uses deeper cross-modal fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shallow KG augmentation of LM inputs helps but is less effective than methods that enable richer interactions between token-level language representations and entity-level KG representations.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>KagNet's shallow fusion design limits the ability to refine node and token representations jointly; no ablation in this paper shows KagNet beating deeper fusion approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GreaseLM: Graph REASoning Enhanced Language Models for Question Answering', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3302.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3302.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-only (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned pretrained language models (e.g., RoBERTa-Large, AristoRoBERTa, SapBERT) without KG augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained transformer LMs fine-tuned on each QA dataset without using external KGs; they rely on implicit knowledge encoded during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-Large; AristoRoBERTa; SapBERT (LM-only baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard pretrained transformer encoders fine-tuned end-to-end on downstream QA; AristoRoBERTa uses extra scientific facts text as input for OpenBookQA; SapBERT is a biomedical LM trained with entity-disambiguation objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['implicit knowledge in pretrained weights', 'end-to-end fine-tuning on QA without explicit structured KG reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LMs use distributed contextual representations and learned surface/pattern heuristics; reasoning is implicit and distributed across transformer layers rather than performed via explicit multi-hop or symbolic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style: only transformer-based implicit reasoning; no structured KG fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; OpenBookQA; MedQA-USMLE</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same MCQA benchmarks used as baselines to measure KG-augmented gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>RoBERTa-Large on CommonsenseQA: IHdev 73.1% (±0.5), IHtest 68.7% (±0.6). AristoRoBERTa on OpenBookQA (no KG): 78.4% test. SapBERT-BASE on MedQA-USMLE: 37.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>GreASELM and other LM+KG methods outperform LM-only baselines on these datasets; improvements are larger on datasets/questions requiring external or structured knowledge and nuanced constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuned LMs provide a strong baseline but often lack robustness for distributionally-different or knowledge-intensive QA examples; explicit KG augmentation and deeper cross-modal fusion provide measurable gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some simple questions requiring specific factual recall are solved by LMs alone; however, overall gains from KG-augmented methods are consistent across evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GreaseLM: Graph REASoning Enhanced Language Models for Question Answering', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>QAGNN: Reasoning with language models and knowledge graphs for question answering <em>(Rating: 2)</em></li>
                <li>Scalable multi-hop relational reasoning for knowledge-aware question answering <em>(Rating: 2)</em></li>
                <li>KagNet: Knowledge-aware graph networks for commonsense reasoning <em>(Rating: 2)</em></li>
                <li>Knowledgeable Reader: Enhancing cloze-style reading comprehension with external commonsense knowledge <em>(Rating: 1)</em></li>
                <li>Knowledge-aware path generation and other LM+KG integration methods (e.g., Mihaylov & Frank, 2018; Lin et al., 2019) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3302",
    "paper_id": "paper-4ab41d9780f1d1ac34d39fa7e527e73652507fcc",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GreASELM",
            "name_full": "GreASELM: Graph Reasoning Enhanced Language Models for Question Answering",
            "brief_description": "A multi-layer LM+KG architecture that bidirectionally fuses pretrained transformer token representations with graph neural network (GNN) node representations via layerwise modality-interaction bottlenecks (interaction token/node), enabling joint reasoning over situational text and structured knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GreASELM (seeded with RoBERTa-Large / AristoRoBERTa / SapBERT)",
            "model_description": "Architecture stacks N unimodal LM transformer layers (pretrained LM) followed by M GreASELM layers; each GreASELM layer runs a transformer LM block for tokens, a GNN over a retrieved KG subgraph, and a modality-interaction (MInt) MLP mixing an interaction token and an interaction node to allow bidirectional, multi-layer information exchange. KG subgraphs are retrieved per-example and node embeddings are initialized from BERT-like entity encodings.",
            "model_size": "~359M",
            "reasoning_methods": [
                "multi-layer cross-modal fusion (LM &lt;-&gt; KG)",
                "GNN message-passing over retrieved KG subgraph",
                "modality-interaction bottleneck (interaction token + interaction node)",
                "retrieval-augmented knowledge grounding"
            ],
            "reasoning_methods_description": "Multi-layer cross-modal fusion: after each LM and GNN layer the special interaction token (in LM) and interaction node (in GNN) are concatenated and passed through an MLP (MInt) to mix modalities; the updated interaction representations are then used by subsequent LM layers and GNN layers so that token-level representations receive grounded KG information and KG node representations are contextualized by the language. GNN uses graph-attention style message passing; KG retrieval returns a focused subgraph per QA example.",
            "diversity_of_methods": "Diverse: GreASELM implements a *diverse set of reasoning styles* by combining transformer-style token reasoning (contextual, distributed representations) with structured GNN multi-hop reasoning over an explicit KG and an explicit modality-mixing operator; this contrasts with single-style or one-way fusion approaches.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA; MedQA-USMLE (MCQA)",
            "reasoning_task_description": "Multiple-choice question answering benchmarks: CommonsenseQA (5-way, general commonsense), OpenBookQA (4-way, elementary science + fact book), MedQA-USMLE (4-way, medical licensing exam style clinical questions). Each requires reasoning beyond surface patterns and often external knowledge.",
            "performance_by_method": "CommonsenseQA (in-house split): GreASELM IHdev 78.5% (±0.5), IHtest 74.2% (±0.4). OpenBookQA test accuracy 84.8%. MedQA-USMLE (with SapBERT seed): +GreASELM 38.5% (SapBERT baseline 37.2%). Reported improvements over fine-tuned LMs: +5.5% (CommonsenseQA), +6.6% (OpenBookQA), +1.3% (MedQA-USMLE) in the paper's summary.",
            "comparison_of_methods": "Direct comparisons/ablations: removing modality interaction drops CommonsenseQA dev from 78.5% → 76.5% (approx QA-GNN level). Interaction every-other-layer yields 76.3%. No parameter sharing for MInt: 77.1%. Random node initialization collapses performance to 60.8% (vs 78.5% with BERT-based node init); using TransE yields 77.7%. Connecting the interaction node to all retrieved subgraph nodes (rather than only linked entities) hurts performance (−0.9%). Varying M (GreASELM layers) shows M=5 best; M=7 decreases performance.",
            "key_findings": "Layerwise bidirectional fusion of LM and KG (via interaction token/node) is more expressive and yields higher QA accuracy than one-way or shallow fusion approaches; GreASELM shows larger gains on questions with textual nuance (negation, hedging) and with multiple explicit constraints (prepositional phrases). The modality interaction bottleneck and good KG node initialization are critical.",
            "counter_examples_or_negative_results": "Design choices that reduce fusion expressivity hurt performance: skipping modality-interaction layers, random KG node initialization, connecting interaction node to all nodes (overloading the bottleneck), and too many GreASELM layers (M=7) produce worse results. No case reported where adding modality interaction harmed performance compared to LM-only when KG retrieval succeeded, but there is a small domain-dependent improvement on MedQA-USMLE (only +1.3%).",
            "uuid": "e3302.0",
            "source_info": {
                "paper_title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "QA-GNN",
            "name_full": "QAGNN: Reasoning with language models and knowledge graphs for question answering",
            "brief_description": "A joint LM+GNN QA approach that initializes a context node in a GNN with a pooled LM representation and then performs GNN message passing over a retrieved KG subgraph; interactions between modalities are performed by seeding the GNN from a single pooled LM vector.",
            "citation_title": "QAGNN: Reasoning with language models and knowledge graphs for question answering",
            "mention_or_use": "use",
            "model_name": "QA-GNN (LM+GNN; pooled LM seeding)",
            "model_description": "LM encodes text; the final LM pooled representation seeds a context node in a GNN which then performs multi-hop message passing over the retrieved KG. Interaction is effectively one-shot (LM → GNN) via the pooled vector rather than iterative per-token per-node exchanges.",
            "model_size": "~360M",
            "reasoning_methods": [
                "GNN multi-hop message passing",
                "single pooled LM-to-GNN seeding (one-way interaction)"
            ],
            "reasoning_methods_description": "Uses a pooled LM vector (single summary of the text context) to initialize a node in the GNN; the GNN performs multi-hop attention/message passing to propagate information through the KG and produce answer scores. Interaction is primarily one-way and compresses textual nuance into one vector.",
            "diversity_of_methods": "Similar/single-style: QA-GNN uses a single dominant interaction style (pooled LM representation seeding GNN) rather than diverse per-token ↔ per-node iterative interactions.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA",
            "reasoning_task_description": "Same MCQA benchmarks as above used for comparison.",
            "performance_by_method": "CommonsenseQA (in-house): IHdev 76.5% (±0.2), IHtest 73.4% (±0.9). OpenBookQA test accuracy 82.8%.",
            "comparison_of_methods": "Compared directly to GreASELM: GreASELM achieves higher accuracy (CommonsenseQA IHdev: 78.5% vs 76.5%; IHtest: 74.2% vs 73.4%; OpenBookQA: 84.8% vs 82.8%). The paper attributes gains to GreASELM's multi-layer bidirectional token/node interactions versus QA-GNN's pooled-vector compression.",
            "key_findings": "QA-GNN is a strong LM+KG baseline but compressing the textual context to a single vector limits cross-relationships between language tokens and KG entities; GreASELM's per-layer multi-token ↔ node mixing captures nuanced constraints better.",
            "counter_examples_or_negative_results": "QA-GNN and GreASELM perform comparably on questions with no prepositional phrases (simpler items). QA-GNN sometimes over-attends to frequently-mentioned entities in the context because its context node is not reformulated by token-level language representations across layers.",
            "uuid": "e3302.1",
            "source_info": {
                "paper_title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "MHGRN",
            "name_full": "MHGRN: Scalable multi-hop relational reasoning for knowledge-aware question answering",
            "brief_description": "A multi-hop GNN-based method that uses language representations to augment graph reasoning over KG subgraphs for QA, focusing on scalable multi-hop relational reasoning.",
            "citation_title": "Scalable multi-hop relational reasoning for knowledge-aware question answering",
            "mention_or_use": "use",
            "model_name": "MHGRN (LM-augmented GNN)",
            "model_description": "Computes node embeddings for KG entities and runs a multi-head GNN for multi-hop relational reasoning; leverages textual signals to augment KG reasoning but uses limited cross-modal interaction compared to GreASELM.",
            "model_size": null,
            "reasoning_methods": [
                "multi-hop GNN message passing",
                "text-informed graph reasoning (one-way augmentation)"
            ],
            "reasoning_methods_description": "MHGRN uses GNNs to perform multi-hop relational reasoning over KG subgraphs; text is used to inform node relevance/initialization but interaction between modalities is not iteratively mixed across multiple LM layers.",
            "diversity_of_methods": "Similar/single-style: primarily GNN-based multi-hop relational reasoning with text used to initialize/augment graph features (one-way or limited interaction).",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA (benchmarks where MHGRN is a baseline)",
            "reasoning_task_description": "Multiple-choice QA requiring multi-hop and relational knowledge reasoning.",
            "performance_by_method": "CommonsenseQA (in-house): IHdev 74.5% (±0.1), IHtest 71.1% (±0.8). OpenBookQA test accuracy 80.6%.",
            "comparison_of_methods": "MHGRN outperforms simple shallow baselines but is outperformed by QA-GNN and GreASELM; GreASELM's multi-layer fusion yields higher accuracy than MHGRN on the evaluated benchmarks.",
            "key_findings": "GNN multi-hop relational reasoning helps QA, but limited cross-modal fusion constrains the capacity to handle fine-grained textual nuance; stronger gains are realized when LM and KG representations are allowed to iteratively and bi-directionally interact.",
            "counter_examples_or_negative_results": "MHGRN provides competitive results but does not capture as much improvement on nuanced or multi-constraint questions as GreASELM.",
            "uuid": "e3302.2",
            "source_info": {
                "paper_title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "KagNet",
            "name_full": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
            "brief_description": "A method that converts KG facts into structural features/paths and uses them to augment a language model for commonsense QA (KG facts used to inform text representations via path-aware features).",
            "citation_title": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
            "mention_or_use": "use",
            "model_name": "KagNet (knowledge-to-text augmentation)",
            "model_description": "Extracts paths (multi-hop) in the KG relevant to the question and encodes those paths to augment textual representations; fusion is comparatively shallow (KG information used to augment LM inputs/features rather than deep layerwise mixing).",
            "model_size": null,
            "reasoning_methods": [
                "path-based KG augmentation",
                "knowledge-to-text feature augmentation (shallow fusion)"
            ],
            "reasoning_methods_description": "KG paths are encoded and used as additional structured features or inputs that augment the LM; reasoning relies on path representation extraction and LM processing rather than iterative token↔node mixing.",
            "diversity_of_methods": "Similar/single-style: uses KG-derived features to augment LM inputs (shallow fusion), not iterative bidirectional fusion.",
            "reasoning_task_name": "CommonsenseQA",
            "reasoning_task_description": "Commonsense multiple-choice QA requiring background knowledge and reasoning over relations.",
            "performance_by_method": "CommonsenseQA (in-house): IHdev 73.5% (±0.2), IHtest 69.0% (±0.8).",
            "comparison_of_methods": "KagNet yields modest gains over LM-only baselines in some settings but is outperformed by multi-hop GNN methods (MHGRN, QA-GNN) and by GreASELM which uses deeper cross-modal fusion.",
            "key_findings": "Shallow KG augmentation of LM inputs helps but is less effective than methods that enable richer interactions between token-level language representations and entity-level KG representations.",
            "counter_examples_or_negative_results": "KagNet's shallow fusion design limits the ability to refine node and token representations jointly; no ablation in this paper shows KagNet beating deeper fusion approaches.",
            "uuid": "e3302.3",
            "source_info": {
                "paper_title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "LM-only (fine-tuned)",
            "name_full": "Fine-tuned pretrained language models (e.g., RoBERTa-Large, AristoRoBERTa, SapBERT) without KG augmentation",
            "brief_description": "Pretrained transformer LMs fine-tuned on each QA dataset without using external KGs; they rely on implicit knowledge encoded during pretraining.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-Large; AristoRoBERTa; SapBERT (LM-only baselines)",
            "model_description": "Standard pretrained transformer encoders fine-tuned end-to-end on downstream QA; AristoRoBERTa uses extra scientific facts text as input for OpenBookQA; SapBERT is a biomedical LM trained with entity-disambiguation objectives.",
            "model_size": null,
            "reasoning_methods": [
                "implicit knowledge in pretrained weights",
                "end-to-end fine-tuning on QA without explicit structured KG reasoning"
            ],
            "reasoning_methods_description": "LMs use distributed contextual representations and learned surface/pattern heuristics; reasoning is implicit and distributed across transformer layers rather than performed via explicit multi-hop or symbolic steps.",
            "diversity_of_methods": "Single-style: only transformer-based implicit reasoning; no structured KG fusion.",
            "reasoning_task_name": "CommonsenseQA; OpenBookQA; MedQA-USMLE",
            "reasoning_task_description": "Same MCQA benchmarks used as baselines to measure KG-augmented gains.",
            "performance_by_method": "RoBERTa-Large on CommonsenseQA: IHdev 73.1% (±0.5), IHtest 68.7% (±0.6). AristoRoBERTa on OpenBookQA (no KG): 78.4% test. SapBERT-BASE on MedQA-USMLE: 37.2%.",
            "comparison_of_methods": "GreASELM and other LM+KG methods outperform LM-only baselines on these datasets; improvements are larger on datasets/questions requiring external or structured knowledge and nuanced constraints.",
            "key_findings": "Fine-tuned LMs provide a strong baseline but often lack robustness for distributionally-different or knowledge-intensive QA examples; explicit KG augmentation and deeper cross-modal fusion provide measurable gains.",
            "counter_examples_or_negative_results": "Some simple questions requiring specific factual recall are solved by LMs alone; however, overall gains from KG-augmented methods are consistent across evaluated benchmarks.",
            "uuid": "e3302.4",
            "source_info": {
                "paper_title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "QAGNN: Reasoning with language models and knowledge graphs for question answering",
            "rating": 2
        },
        {
            "paper_title": "Scalable multi-hop relational reasoning for knowledge-aware question answering",
            "rating": 2
        },
        {
            "paper_title": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
            "rating": 2
        },
        {
            "paper_title": "Knowledgeable Reader: Enhancing cloze-style reading comprehension with external commonsense knowledge",
            "rating": 1
        },
        {
            "paper_title": "Knowledge-aware path generation and other LM+KG integration methods (e.g., Mihaylov & Frank, 2018; Lin et al., 2019)",
            "rating": 1
        }
    ],
    "cost": 0.01787825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GreASELM: GRAPH REASONING ENHANCED Language MODELS FOR QUESTION ANSWERING</h1>
<p>Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren<br>Percy Liang, Christopher D. Manning, Jure Leskovec<br>Stanford University<br>{xikunz2, antoineb, myasu, hyren, pliang, manning, jure}@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreASELM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreASELM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models $8 \times$ larger. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Question answering is a challenging task that requires complex reasoning over both explicit constraints described in the textual context of the question, as well as unstated, relevant knowledge about the world (i.e., knowledge about the domain of interest). Recently, large pretrained language models fine-tuned on QA datasets have become the dominant paradigm in NLP for question answering tasks (Khashabi et al., 2020). After pretraining on an extreme-scale collection of general text corpora, these language models learn to implicitly encode broad knowledge about the world, which they are able to leverage when fine-tuned on a domain-specific downstream QA task. However, despite the strong performance of this two-stage learning procedure on common benchmarks, these models struggle when given examples that are distributionally different from examples seen during fine-tuning (McCoy et al., 2019). Their learned behavior often relies on simple (at times spurious) patterns to offer shortcuts to an answer, rather than robust, structured reasoning that effectively fuses the explicit information provided by the context and implicit external knowledge (Marcus, 2018).</p>
<p>On the other hand, massive knowledge graphs (KG), such as Freebase (Bollacker et al., 2008), Wikidata (Vrandečić \&amp; Krötzsch, 2014), ConceptNet (Speer et al., 2017), and Yago (Suchanek et al., 2007) capture such external knowledge explicitly using triplets that capture relationships between entities. Previous research has demonstrated the significant role KGs can play in structured reasoning and query answering (Ren et al., 2020; 2021; Ren \&amp; Leskovec, 2020). However, extending these reasoning advantages to general QA (where questions and answers are expressed in natural language and not easily mapped to strict logical queries) requires finding the right integration of knowledge from the KG with the information and constraints provided by the QA example. Prior</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: GreASELM Architecture. The textual context is appended with a special interaction token and passed through $N$ LM-based unimodal encoding layers. Simultaneously, a local KG of relevant knowledge is extracted and connected to an interaction node. In the later GreASELM layers, the language representation continues to be updated through LM layers and the KG is processed using a GNN, simulating reasoning over its knowledge. In each layer, after each modality's representation is updated, the representations of the interaction token and node are pulled, concatenated, and passed through a modality interaction (MInt) unit to mix their representations. In subsequent layers, the mixed information from the interaction elements mixes with their respective modalities, allowing knowledge from the KG to affect the representations of individual tokens, and context from language to affect fine-grained entity knowledge representations in the GNN.
methods propose various ways to leverage both modalities (i.e., expressive large language models and structured KGs) for improved reasoning (Mihaylov \&amp; Frank, 2018; Lin et al., 2019; Feng et al., 2020). However, these methods typically fuse the two modalities in a shallow and non-interactive manner, encoding both separately and fusing them at the output for a prediction, or using one to augment the input of the other. Consequently, previous methods demonstrate restricted capacity to exchange useful information between the two modalities. It remains an open question how to effectively fuse the KG and LM representations in a truly unified manner, where the two representations can interact in a non-shallow way to simulate structured, situational reasoning.</p>
<p>In this work, we present GreASELM, a new model that enables fusion and exchange of information from both the LM and KG in multiple layers of its architecture (see Figure 1). Our proposed GreASELM consists of an LM that takes as input the natural language context, as well as a graph neural network (GNN) that reasons over the KG. After each layer of the LM and GNN, we design an interactive scheme to bidirectionally transfer the information from each modality to the other through specially initialized interaction representations (i.e., interaction token for the LM; interaction node for the GNN). In such a way, all the tokens in the language context receive information from the KG entities through the interaction token and the KG entities indirectly interact with the tokens through the interaction node. By such a deep integration across all layers, GreASELM enables joint reasoning over both the language context and the KG entities under a unified framework agnostic to the specific language model or graph neural network, so that both modalities can be contextualized by the other.</p>
<p>GREASELM demonstrates significant performance gains across different LM architectures. We perform experiments on several standard QA benchmarks: CommonsenseQA, OpenbookQA and MedQA-USMLE, which require external knowledge across different domains (commonsense reasoning and medical reasoning) and use different KGs (ConceptNet and Disease Database). Across both domains, GreASELM outperforms comparably-sized prior QA models, including strong fine-</p>
<p>tuned LM baselines (by $5.5 \%, 6.6 \%$, and $1.3 \%$, respectively) and state-of-the-art KG+LM models (by $0.9 \%, 1.8 \%$, and $0.5 \%$, respectively) on the three competitive benchmarks. Furthermore, with the deep fusion of both modalities, GreASELM exhibits strong performance over baselines on questions that exhibit textual nuance, such as resolving multiple constraints, negation, and hedges, and which require effective reasoning over both language context and KG.</p>
<h1>2 Related Work</h1>
<p>Integrating KG information has become a popular research area for improving neural QA systems. Some works explore using two-tower models to answer questions, where a graph representation of knowledge and language representation are fused with no interaction between them (Wang et al., 2019). Other works seek to use one modality to ground the other, such as using an encoded representation of a linked KG to augment the textual representation of a QA example (e.g., Knowledgeable Reader, Mihaylov \&amp; Frank, 2018; KagNet, Lin et al., 2019; KT-NET, Yang et al., 2019). Others reverse the flow of information and use a representation of the text (e.g., final layer of LM) to provide an augmentation to a graph reasoning model over an extracted KG for the example (e.g., MHGRN, Feng et al., 2020; Lv et al., 2020). In all of these settings, however, the interaction between both modalities is limited as information between them only flows one way.</p>
<p>More recent approaches explore deeper integrations of both modalities. Certain approaches learn to access implicit knowledge encoded in LMs (Bosselut et al., 2019; Petroni et al., 2019; Hwang et al., 2021) by training on structured KG data, and then use the LM to generate local KGs that can be used for QA (Wang et al., 2020; Bosselut et al., 2021). However, these approaches discard the static KG once they train the LM on its facts, losing important structure that can guide reasoning. More recently, QA-GNN (Yasunaga et al., 2021) proposed to jointly update the LM and GNN representations via message passing. However, they use a single pooled representation of the LM to seed the textual component of this joint structure, limiting the updates that can be made to the textual representation. In contrast to prior works, we propose to make individual token representations in the LM and node representations in the GNN mix for multiple layers, enabling representations of both modalities to reflect particularities of the other (e.g., knowledge grounds language; language nuances specifies which knowledge is important). Simultaneously, we retain the individual structure of both modalities, which we demonstrate improves QA performance substantially (§5).</p>
<p>Additionally, some works explore integrating knowledge graphs with language models in the pretraining stage. However, much like for QA, the modality interaction is typically limited to knowledge feeding language (Zhang et al., 2019; Shen et al., 2020; Yu et al., 2020), rather than designing interactions across multiple layers. Sun et al. (2020)'s work is perhaps most similar, but they do not use the same interaction bottleneck, requiring high-precision entity mention spans for linking, and they limit expressivity through shared modality parameters for the LM and KG.</p>
<h2>3 Proposed Approach: GreASELM</h2>
<p>In this work, we augment large-scale language models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Liu et al., 2021) with graph reasoning modules over KGs. Our method, GreASELM (depicted in Figure 1), consists of two stacked components: (1) a set of unimodal LM layers which learn an initial representation of the input tokens, and (2) a set of upper cross-modal GreASELM layers which learn to jointly represent the language sequence and linked knowledge graph, allowing textual representations formed from the underlying LM layers and a graph representation of the KG to mix with one another. We denote the number of LM layers as $N$, and the number of GreASELM layers as $M$. The total number of layers in our model is $N+M$.</p>
<p>Notation. In the task of multiple choice question answering (MCQA), a generic MCQA-type dataset consists of examples with a context paragraph $c$, a question $q$ and a candidate answer set $\mathcal{A}$, all expressed in text. In this work, we also assume access to an external knowledge graph (KG) $\mathcal{G}$ that provides background knowledge that is relevant to the content of the multiple choice questions.</p>
<p>Given a QA example $(c, q, \mathcal{A})$, and the KG $\mathcal{G}$ as input, our goal is to identify which answer $a \in \mathcal{A}$ is correct. Without loss of generality, when an operation is applied to an arbitrary answer, we refer to that answer as $a$. We denote a sequence of tokens in natural language as $\left{w_{1}, \ldots, w_{T}\right}$, where $T$ is</p>
<p>the total number of tokens, and the representation of a token $w_{t}$ from the $\ell$-th layer of the model as $\boldsymbol{h}<em 1="1">{t}^{(\ell)}$. We denote a set of nodes from the KG as $\left{e</em>$.}, \ldots, e_{J}\right}$, where $J$ is the total number of nodes, and the representation of a node $e_{j}$ in the $\ell$-th layer of the model as $\boldsymbol{e}_{j}^{(\ell)</p>
<h1>3.1 INPUT REPRESENTATION</h1>
<p>We concatenate our context paragraph $c$, question $q$, and candidate answer $a$ with separator tokens to get our model input $[c ; q ; a]$ and tokenize the combined sequence into $\left{w_{1}, \ldots, w_{T}\right}$. Second, we use the input sequence to retrieve a subgraph of the KG $\mathcal{G}$ (denoted $\mathcal{G}<em _sub="{sub" _text="\text">{\text {sub }}$ ), which provides knowledge from the KG that is relevant to this QA example. We denote the set of nodes in $\mathcal{G}</em>\right}$.
KG Retrieval. Given each QA context, we follow the procedure from Yasunaga et al. (2021) to retrieve the subgraph $\mathcal{G}}}$ as $\left{e_{1}, \ldots, e_{J<em _sub="{sub" _text="\text">{\text {sub }}$ from $\mathcal{G}$. We describe this procedure in Appendix B.1. Each node in $\mathcal{G}</em>}}$ is assigned a type based on whether its corresponding entity was linked from the context $c$, question $q$, answer $a$, or as a neighbor to these nodes. In the rest of the paper, we use "KG" to refer to $\mathcal{G<em i="i" n="n" t="t">{\text {sub }}$.
Interaction Bottlenecks. In the cross-modal GreASELM layers, information is fused between both modalities, for which we define a special interaction token $w</em>}$ and a special interaction node $c_{i n t}$ whose representations serve as the bottlenecks through which the two modalities interact (§3.3). We prepend $w_{i n t}$ to the token sequence and connect $e_{i n t}$ to all the linked nodes $\mathcal{V<em _sub="{sub" _text="\text">{\text {linked }}$ in $\mathcal{G}</em>$.}</p>
<h3>3.2 LANGUAGE Pre-ENCODING</h3>
<p>In the unimodal encoding component, given the sequence of tokens $\left{w_{i n t}, w_{1}, \ldots, w_{T}\right}$, we first sum the token, segment, and positional embeddings for each token to compute its $\ell=0$ input representation $\left{\boldsymbol{h}<em 1="1">{i n t}^{(0)}, \boldsymbol{h}</em>\right}$, and then compute an output representation for each layer $\ell$ :}^{(0)}, \ldots, \boldsymbol{h}_{T}^{(0)</p>
<p>$$
\begin{aligned}
\left{\boldsymbol{h}<em 1="1">{i n t}^{(\ell)}, \boldsymbol{h}</em>}^{(\ell)}, \ldots, \boldsymbol{h<em i="i" n="n" t="t">{T}^{(\ell)}\right}= &amp; \text { LM-Layer }\left(\left{\boldsymbol{h}</em>}^{(\ell-1)}, \boldsymbol{h<em T="T">{1}^{(\ell-1)}, \ldots, \boldsymbol{h}</em>\right}\right) \
&amp; \text { for } \ell=1, \ldots, N
\end{aligned}
$$}^{(\ell-1)</p>
<p>where LM-Layer( $\cdot$ ) is a single LM encoder layer, whose parameters are initialized using a pretrained model (§4.1). We refer readers to Vaswani et al. (2017) for technical details of these layers.</p>
<h3>3.3 GreASELM</h3>
<p>GREASELM uses a cross-modal fusion component to inject information from the KG into language representations and information from language into KG representations. The GreASELM layer is designed to separately encode information from both modalities, and fuse their representations using the bottleneck of the special interaction token and node. It is comprised of three components: (1) a transformer LM encoder block which continues to encode the language context, (2) a GNN layer that reasons over KG entities and relations, and (3) a modality interaction layer that takes the unimodal representations of the interaction token and interaction node and exchanges information through them. We discuss these three components below.
Language Representation. In the $\ell$-th GreASELM layer, the input token embeddings $\left{\boldsymbol{h}<em 1="1">{i n t}^{(N+\ell-1)}, \boldsymbol{h}</em>\right}$ are fed into additional transformer LM encoder blocks that continue to encode the textual context based on the LM's pretrained representations:}^{(N+\ell-1)}, \ldots, \boldsymbol{h}_{T}^{(N+\ell-1)</p>
<p>$$
\begin{aligned}
\left{\tilde{\boldsymbol{h}}<em 1="1">{i n t}^{(N+\ell)}, \tilde{\boldsymbol{h}}</em>}^{(N+\ell)}, \ldots, \tilde{\boldsymbol{h}<em i="i" n="n" t="t">{T}^{(N+\ell)}\right}= &amp; \text { LM-Layer }\left(\left{\boldsymbol{h}</em>}^{(N+\ell-1)}, \boldsymbol{h<em T="T">{1}^{(N+\ell-1)}, \ldots, \boldsymbol{h}</em>\right}\right) \
&amp; \text { for } \ell=1, \ldots, M
\end{aligned}
$$}^{(N+\ell-1)</p>
<p>where $\tilde{\boldsymbol{h}}$ corresponds to pre-fused embeddings of the language modality. As we will discuss below, because $\boldsymbol{h}<em _sub="{sub" _text="\text">{i n t}^{N+\ell-1}$ will encode information received from the knowledge graph representation, these late language encoding layers will also allow the token representations to mix with KG knowledge.
Graph Representation. The GreASELM layers also encode a representation of the local KG $\mathcal{G}</em>}}$ linked from the QA example. To represent the graph, we first compute initial node embeddings $\left{\boldsymbol{e<em J="J">{1}^{(0)}, \ldots, \boldsymbol{e}</em>$ is initialized randomly.}^{(0)}\right}$ for the retrieved entities using pretrained KG embeddings for these nodes (§4.1). The initial embedding of the interaction node $\boldsymbol{e}_{i n t}^{0</p>
<p>Then, in each layer of the GNN, the current representation of the node embeddings $\left{\boldsymbol{e}<em 1="1">{i n t}^{(\ell-1)}, \boldsymbol{e}</em>\right}$ is fed into the layer to perform a round of information propagation between nodes in the graph and yield pre-fused node embeddings for each entity:}^{(\ell-1)}, \ldots, \boldsymbol{e}_{J}^{(\ell-1)</p>
<p>$$
\begin{aligned}
\left{\tilde{\boldsymbol{e}}<em 1="1">{i n t}^{(\ell)}, \tilde{\boldsymbol{e}}</em>}^{(\ell)}, \ldots, \tilde{\boldsymbol{e}<em i="i" n="n" t="t">{J}^{(\ell)}\right}= &amp; \operatorname{GNN}\left(\left{\boldsymbol{e}</em>}^{(\ell-1)}, \boldsymbol{e<em J="J">{1}^{(\ell-1)}, \ldots, \boldsymbol{e}</em>\right}\right) \
&amp; \text { for } \ell=1, \ldots, M
\end{aligned}
$$}^{(\ell-1)</p>
<p>where GNN corresponds to a variant of graph attention networks (Veličković et al., 2018) that is a simplification of the method of Yasunaga et al. (2021). The GNN computes node representations $\hat{\boldsymbol{e}}<em j="j">{j}^{(\ell)}$ for each node $e</em>\right}$ via message passing between neighbors on the graph.} \in\left{e_{1}, \ldots, e_{J</p>
<p>$$
\hat{\boldsymbol{e}}<em n="n">{j}^{(\ell)}=f</em>}\left(\sum_{e_{s} \in \mathcal{N<em j="j">{e</em>}} \cup\left{e_{j}\right}} \alpha_{s j} \boldsymbol{m<em j="j">{s j}\right)+\boldsymbol{e}</em>
$$}^{(\ell-1)</p>
<p>where $\mathcal{N}<em j="j">{e</em>}}$ represents the neighborhood of an arbitrary node $e_{j}, \boldsymbol{m<em s="s">{s j}$ denotes the message one of its neighbors $e</em>}$ passes to $e_{j}, \alpha_{s j}$ is an attention weight that scales the message $\boldsymbol{m<em n="n">{s j}$, and $f</em>$ between nodes allow entity information from a node to affect the model's representation of its neighbors, and are computed in the following manner:}$ is a 2-layer MLP. The messages $\boldsymbol{m}_{s j</p>
<p>$$
\boldsymbol{r}<em r="r">{s j}=f</em>}\left(\tilde{\boldsymbol{r}<em s="s">{s j}, \boldsymbol{u}</em>}, \boldsymbol{u<em j="j" s="s">{j}\right) \quad \boldsymbol{m}</em>}=f_{m}\left(\boldsymbol{e<em s="s">{s}^{(\ell-1)}, \boldsymbol{u}</em>\right)
$$}, \boldsymbol{r}_{s j</p>
<p>where $\boldsymbol{u}<em j="j">{s}, \boldsymbol{u}</em>}$ are node type embeddings, $\tilde{\boldsymbol{r}<em s="s">{s j}$ is a relation embedding for the relation connecting $e</em>$ scale the contribution of each neighbor's message by its importance, and are computed as follows:}$ and $e_{j}, f_{r}$ is a 2-layer MLP, and $f_{m}$ is a linear transformation. The attention weights $\alpha_{s j</p>
<p>$$
\begin{aligned}
\boldsymbol{q}<em q="q">{s}=f</em>}\left(\boldsymbol{e<em s="s">{s}^{(\ell-1)}, \boldsymbol{u}</em>}\right) &amp; \text { (7) } \quad \boldsymbol{k<em k="k">{j}=f</em>}\left(\boldsymbol{e<em j="j">{j}^{(\ell-1)}, \boldsymbol{u}</em>}, \boldsymbol{r<em j="j" s="s">{s j}\right) \
\gamma</em>}=\frac{\boldsymbol{q<em j="j">{s}^{\top} \boldsymbol{k}</em>}}{\sqrt{D}} &amp; \alpha_{s j}=\frac{\exp \left(\gamma_{s j}\right)}{\sum_{e_{s} \in \mathcal{N<em j="j">{e</em>
\end{aligned}
$$}} \cup\left{e_{j}\right}} \exp \left(\gamma_{s j}\right)</p>
<p>where $f_{q}$ and $f_{k}$ are linear transformations and $\boldsymbol{u}<em j="j">{s}, \boldsymbol{u}</em>}, \boldsymbol{r<em i="i" n="n" t="t">{s j}$ are defined the same as above.
As discussed in the following paragraph, message passing between the interaction node $e</em>$ to propagate to the other nodes in the graph.
Modality Interaction. Finally, after using a transformer LM layer and a GNN layer to update token embeddings and node embeddings respectively, we use a modality interaction layer (MInt) to let the two modalities fuse information through the bottleneck of the interaction token $w_{i n t}$ and the interaction node $e_{i n t}$. We concatenate the pre-fused embeddings of the interaction token $\tilde{\boldsymbol{h}}}$ and the nodes from the retrieved subgraph will allow information from text that $e_{i n t}$ receives from $w_{i n t<em i="i" n="n" t="t">{i n t}^{(i)}$ and interaction node $\hat{\boldsymbol{e}}</em>}^{(i)}$, pass the joint representation through a mixing operation (MInt), and then split the output post-fused embeddings into $\boldsymbol{h<em i="i" n="n" t="t">{i n t}^{(i)}$ and $\boldsymbol{e}</em>$ :}^{(i)</p>
<p>$$
\left[\boldsymbol{h}<em i="i" n="n" t="t">{i n t}^{(\ell)} ; \boldsymbol{e}</em>}^{(\ell)}\right]=\operatorname{MInt}\left(\left[\tilde{\boldsymbol{h}<em i="i" n="n" t="t">{i n t}^{(\ell)} ; \hat{\boldsymbol{e}}</em>\right]\right)
$$}^{(\ell)</p>
<p>We use a two-layer MLP as our MInt operation, though other fusion operators could be used to mix the representation. All the tokens other than the interaction token $w_{i n t}$ and all the nodes other than the interaction node $e_{i n t}$ are not involved in the modality interaction process: $\boldsymbol{w}^{(\ell)}=\tilde{\boldsymbol{w}}^{(\ell)}$ for $w \in$ $\left{w_{1}, \ldots, w_{T}\right}$ and $\boldsymbol{e}^{(\ell)}=\hat{\boldsymbol{e}}^{(\ell)}$ for $e \in\left{e_{1}, \ldots, e_{J}\right}$. However, they receive information from the interaction representations $\boldsymbol{h}<em i="i" n="n" t="t">{i n t}^{(\ell)}$ and $\boldsymbol{e}</em>$ in the next layers of their respective modal propagation (i.e., Eqs. 2, 3). Consequently, across multiple GreASELM layers, information propagates between both modalities (see Fig. 1 for visual depiction), grounding language representations to KG knowledge, and knowledge representations to contextual constraints.}^{(\ell)</p>
<p>Learning \&amp; Inference. For the MCQA task, given a question $q$ and an answer $a$ from all the candidates $\mathcal{A}$, we compute the probability of $a$ being the correct answer as $p(a \mid q, c) \propto$ $\exp \left(\operatorname{MLP}\left(\boldsymbol{h}<em i="i" n="n" t="t">{i n t}^{(N+M)}, \boldsymbol{e}</em>}^{(M)}, \boldsymbol{g}\right)\right)$, where $\boldsymbol{g}$ denotes attention-based pooling of $\left{\boldsymbol{e<em j="j">{j}^{(M)} \mid e</em>} \in\right.$ $\left.\left{e_{1}, \ldots, e_{J}\right}\right}$ using $\boldsymbol{h<em _in="\in" _mathcal_A="\mathcal{A" a="a">{i n t}^{(N+M)}$ as a query. We optimize the whole model end-to-end using the cross entropy loss. At inference time, we predict the most plausible answer as $\arg \max </em> p(a \mid q, c)$.}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CommonsenseQA</td>
<td style="text-align: center;">A weasel has a thin body and short legs to easier burrow after prey in a what? <br> (A) tree (B) mulberry bush (C) chicken coop (D) viking ship (E) rabbit warren</td>
</tr>
<tr>
<td style="text-align: center;">OpenbookQA</td>
<td style="text-align: center;">Which of these would let the most heat travel through? <br> (A) a new pair of jeans (B) a steel spoon in a cafeteria <br> (C) a cotton candy at a store (D) a calvin klein cotton hat</td>
</tr>
<tr>
<td style="text-align: center;">MedQA-USMLE</td>
<td style="text-align: center;">A 57-year-old man presents to his primary care physician with a 2-month <br> history of right upper and lower extremity weakness. He noticed the weakness <br> when he started falling far more frequently while running errands. Since then, <br> he has had increasing difficulty with walking and lifting objects. His past <br> medical history is significant only for well-controlled hypertension, but he says <br> that some members of his family have had musculoskeletal problems. His right <br> upper extremity shows forearm atrophy and depressed reflexes while his right <br> lower extremity is hypertonic with a positive Babinski sign. Which of the <br> following is most likely associated with the cause of this patients symptoms? <br> (A) HLA-B8 haplotype <br> (B) HLA-DR2 haplotype <br> (C) Mutation in SOD1 <br> (D) Mutation in SMN1</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of the MCQA task for each of the datasets evaluated in this work.</p>
<h1>4 EXPERIMENTAL SETUP</h1>
<p>We evaluate GreASELM on three diverse multiple-choice question answering datasets across two domains: CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018) as commonsense reasoning benchmarks, and MedQA-USMLE (Jin et al., 2021) as a clinical QA task.
CommonsenseQA is a 5-way multiple-choice question answering dataset of 12,102 questions that require background commonsense knowledge beyond surface language understanding. We perform our experiments using the in-house data split of Lin et al. (2019) to compare to baseline methods.
OpenbookQA is a 4-way multiple-choice question answering dataset that tests elementary scientific knowledge. It contains 5,957 questions along with an open book of scientific facts. We use the official data splits from Mihaylov \&amp; Frank (2018).
MedQA-USMLE is a 4-way multiple-choice question answering dataset, which requires biomedical and clinical knowledge. The questions are originally from practice tests for the United States Medical License Exams (USMLE). The dataset contains 12,723 questions. We use the original data splits from Jin et al. (2021).</p>
<h3>4.1 IMPLEMENTATION \&amp; TRAINING DETAILS</h3>
<p>Language Models. We seed GreASELM with RoBERTa-Large (Liu et al., 2019) for our experiments on CommonsenseQA, AristoRoBERTa (Clark et al., 2019) for our experiments on OpenbookQA, and SapBERT (Liu et al., 2021) for our experiments on MedQA-USMLE, demonstrating GreASELM's generality with respect to language model initializations. Hyperparameters for training these models can be found in Appendix Table 7.</p>
<p>Knowledge Graphs. We use ConceptNet (Speer et al., 2017), a general-domain knowledge graph, as our external knowledge source $\mathcal{G}$ for both CommonsenseQA and OpenbookQA. It has 799,273 nodes and 2,487,810 edges in total. For MedQA-USMLE, we use a self-constructed knowledge graph that integrates the Disease Database portion of the Unified Medical Language System (UMLS; Bodenreider, 2004) and DrugBank (Wishart et al., 2018). The knowledge graph contains 9,958 nodes and 44,561 edges. Additional information about node initialization and hyperparameters for preprocessing these KGs can be found in Appendix B.2.</p>
<h3>4.2 BASELINE METHODS</h3>
<p>Fine-tuned LMs. To study the effect of using KGs as external knowledge sources, we compare our method with vanilla fine-tuned LMs, which are knowledge-agnostic. We fine-tune RoBERTa-</p>
<p>Table 2: Performance comparison on CommonsenseQA in-house split (controlled experiments). As the official test is hidden, here we report the in-house Dev (IHdev) and Test (IHtest) accuracy, following the data split of Lin et al. (2019). Experiments are controlled using same seed LM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">IHdev-Acc. (\%)</th>
<th style="text-align: center;">IHtest-Acc. (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa-Large (w/o KG)</td>
<td style="text-align: center;">$73.1( \pm 0.5)$</td>
<td style="text-align: center;">$68.7( \pm 0.6)$</td>
</tr>
<tr>
<td style="text-align: left;">RGCN (Schlichtkrull et al., 2018)</td>
<td style="text-align: center;">$72.7( \pm 0.2)$</td>
<td style="text-align: center;">$68.4( \pm 0.7)$</td>
</tr>
<tr>
<td style="text-align: left;">GconAttn (Wang et al., 2019)</td>
<td style="text-align: center;">$72.6( \pm 0.4)$</td>
<td style="text-align: center;">$68.6( \pm 1.0)$</td>
</tr>
<tr>
<td style="text-align: left;">KagNet (Lin et al., 2019)</td>
<td style="text-align: center;">$73.5( \pm 0.2)$</td>
<td style="text-align: center;">$69.0( \pm 0.8)$</td>
</tr>
<tr>
<td style="text-align: left;">RN (Santoro et al., 2017)</td>
<td style="text-align: center;">$74.6( \pm 0.9)$</td>
<td style="text-align: center;">$69.1( \pm 0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">MHGRN (Feng et al., 2020)</td>
<td style="text-align: center;">$74.5( \pm 0.1)$</td>
<td style="text-align: center;">$71.1( \pm 0.8)$</td>
</tr>
<tr>
<td style="text-align: left;">QA-GNN (Yasunaga et al., 2021)</td>
<td style="text-align: center;">$76.5( \pm 0.2)$</td>
<td style="text-align: center;">$73.4( \pm 0.9)$</td>
</tr>
<tr>
<td style="text-align: left;">GREASELM (Ours)</td>
<td style="text-align: center;">$\mathbf{7 8 . 5}( \pm 0.5)$</td>
<td style="text-align: center;">$\mathbf{7 4 . 2}( \pm 0.4)$</td>
</tr>
</tbody>
</table>
<p>Table 3: Test Accuracy comparison on OpenBookQA. Experiments are controlled using the same seed LM for all LM+KG methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AristoRoBERTa (no KG)</td>
<td style="text-align: center;">78.4</td>
</tr>
<tr>
<td style="text-align: left;">+ RGCN</td>
<td style="text-align: center;">74.6</td>
</tr>
<tr>
<td style="text-align: left;">+ GconAttn</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">+ RN</td>
<td style="text-align: center;">75.4</td>
</tr>
<tr>
<td style="text-align: left;">+ MHGRN</td>
<td style="text-align: center;">80.6</td>
</tr>
<tr>
<td style="text-align: left;">+ QA-GNN</td>
<td style="text-align: center;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">GREASELM (Ours)</td>
<td style="text-align: center;">$\mathbf{8 4 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Test accuracy comparison to public OpenBookQA model implementations. *UnifiedQA (11B params) and T5 (3B) are 30x and 8x larger than our model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Acc.</th>
<th style="text-align: center;"># Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ALBERT (Lan et al., 2020) + KB</td>
<td style="text-align: left;">81.0</td>
<td style="text-align: center;">$\sim 235 \mathrm{M}$</td>
</tr>
<tr>
<td style="text-align: left;">HGN (Yan et al., 2020)</td>
<td style="text-align: left;">81.4</td>
<td style="text-align: center;">$\geq 355 \mathrm{M}$</td>
</tr>
<tr>
<td style="text-align: left;">AMR-SG (Xu et al., 2021)</td>
<td style="text-align: left;">81.6</td>
<td style="text-align: center;">$\sim 361 \mathrm{M}$</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT + KPG (Wang et al., 2020)</td>
<td style="text-align: left;">81.8</td>
<td style="text-align: center;">$\geq 235 \mathrm{M}$</td>
</tr>
<tr>
<td style="text-align: left;">QA-GNN (Yasunaga et al., 2021)</td>
<td style="text-align: left;">82.8</td>
<td style="text-align: center;">$\sim 360 \mathrm{M}$</td>
</tr>
<tr>
<td style="text-align: left;">T5* (Raffel et al., 2020)</td>
<td style="text-align: left;">83.2</td>
<td style="text-align: center;">$\sim 3 \mathrm{~B}$</td>
</tr>
<tr>
<td style="text-align: left;">T5 + KB (Pirtoaca)</td>
<td style="text-align: left;">85.4</td>
<td style="text-align: center;">$\geq 11 \mathrm{~B}$</td>
</tr>
<tr>
<td style="text-align: left;">UnifiedQA* (Khashabi et al., 2020)</td>
<td style="text-align: left;">$\mathbf{8 7 . 2}$</td>
<td style="text-align: center;">$\sim \mathbf{1 1 B}$</td>
</tr>
<tr>
<td style="text-align: left;">GREASELM (Ours)</td>
<td style="text-align: left;">84.8</td>
<td style="text-align: center;">$\sim 359 \mathrm{M}$</td>
</tr>
</tbody>
</table>
<p>Large (Liu et al., 2019) for CommonsenseQA, and AristoRoBERTa ${ }^{2}$ (Clark et al., 2019) for OpenbookQA. For MedQA-USMLE, we use a state-of-the-art biomedical language model, SapBERT (Liu et al., 2021), which is an augmentation of PubmedBERT (Gu et al., 2022) that is trained with entity disambiguation objectives to allow the model to better understand entity knowledge.
LM+KG models. We also evaluate GreASELM's ability to exploit its knowledge graph augmentation by comparing with existing LM+KG methods: (1) Relation Network (RN; Santoro et al., 2017), (2) RGCN (Schlichtkrull et al., 2018), (3) GconAttn (Wang et al., 2019), (4) KagNet (Lin et al., 2019), (5) MHGRN (Feng et al., 2020), and (6) QA-GNN (Yasunaga et al., 2021). QA-GNN is the existing top-performing model under this LM+KG paradigm. The key difference between GREASELM and these baseline methods is that they do not fuse the representations of both modalities across multiple interaction layers, allowing the representation of both modalities to affect the other (§3.3). For fair comparison, we use the same LM to initialize these baselines as for our model.</p>
<h1>5 EXPERIMENTAL RESULTS</h1>
<p>Our results in Tables 2 and 3 demonstrate a consistent improvement on the CommonsenseQA and OpenbookQA datasets. On CommonsenseQA, our model's test performance improves by $5.5 \%$ over fine-tuned LMs and $0.9 \%$ over existing LM+KG models. On OpenbookQA, these improvements are magnified, with $6.4 \%$ over raw LMs, and $2.0 \%$ over the prior best LM+KG system, QA-GNN. The boost over QA-GNN suggests that GREASELM's multi-layer fusion component that passes information between the text and KG representations is more expressive than LM+KG methods which do</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Performance of GreASELM on the CommonsenseQA IH-dev set on complex questions with semantic nuance such as prepositional phrases, negation terms, and hedge terms.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Model</th>
<th style="text-align: right;"># Prepositional Phrases</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Negation <br> Term</th>
<th style="text-align: right;">Hedge <br> Term</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;"></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: right;">$n$</td>
<td style="text-align: right;">210</td>
<td style="text-align: right;">429</td>
<td style="text-align: right;">316</td>
<td style="text-align: right;">171</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">83</td>
<td style="text-align: right;">167</td>
</tr>
<tr>
<td style="text-align: right;">RoBERTa-Large</td>
<td style="text-align: right;">66.7</td>
<td style="text-align: right;">72.3</td>
<td style="text-align: right;">76.3</td>
<td style="text-align: right;">74.3</td>
<td style="text-align: right;">69.5</td>
<td style="text-align: right;">63.8</td>
<td style="text-align: right;">70.7</td>
</tr>
<tr>
<td style="text-align: right;">QA-GNN</td>
<td style="text-align: right;">$\mathbf{7 6 . 7}$</td>
<td style="text-align: right;">76.2</td>
<td style="text-align: right;">79.1</td>
<td style="text-align: right;">74.9</td>
<td style="text-align: right;">81.4</td>
<td style="text-align: right;">66.2</td>
<td style="text-align: right;">76.0</td>
</tr>
<tr>
<td style="text-align: right;">GreASELM (Ours)</td>
<td style="text-align: right;">75.7</td>
<td style="text-align: right;">$\mathbf{7 9 . 3}$</td>
<td style="text-align: right;">$\mathbf{8 0 . 4}$</td>
<td style="text-align: right;">$\mathbf{7 7 . 2}$</td>
<td style="text-align: right;">$\mathbf{8 4 . 7}$</td>
<td style="text-align: right;">$\mathbf{6 9 . 9}$</td>
<td style="text-align: right;">$\mathbf{7 8 . 4}$</td>
</tr>
</tbody>
</table>
<p>not integrate such sustained interaction between both modalities. We also achieve competitive results to other systems on the leaderboard of OpenbookQA (Table 4), posting the third highest score. However, we note that the T5 (Raffel et al., 2020) and UnifiedQA (Khashabi et al., 2020) models are pretrained models with $8 \times$ and $30 \times$ more parameters, respectively, than our model. Among models with comparable parameter counts, GreASELM achieves the highest score. An ablation study on different model components and hyperparameters is reported in Appendix C.1.</p>
<p>Quantitative Analysis. Given these overall performance improvements, we investigated whether GreASELM's improvements were reflected in questions that required more complex reasoning. Because we had no gold structures from these datasets to categorize the reasoning complexity of different questions, we defined three proxies: the number of prepositional phrases in the questions, the presence of negation terms, and the presence of hedging terms. We use the number of prepositional phrases as a proxy for the number of explicit reasoning constraints being set in the questions. For example, the CommonsenseQA question in Table 1, "A weasel has a thin body and short legs to easier burrow after prey in a what?" has three prepositional phrases: to easier burrow, after prey, in a what, which each provide an additional search constraint for the answer (n.b., in certain cases, the prepositional phrases do not provide constraints that are needed for selecting the correct answer). The presence of negation and hedging terms stratifies our evaluation to questions that have explicit negation mentions (e.g., no, never) and terms indicating uncertainty (e.g., sometimes; maybe).
Our results in Table 5 demonstrate that GreASELM generally outperforms RoBERTa-Large and QA-GNN for both questions with negation terms and hedge terms, indicating GreASELM handles contexts with nuanced constraints. Furthermore, we also note that GreASELM performs better than the baselines across all questions with prepositional phrases, our measure for reasoning complexity. QA-GNN and GreASELM perform comparably on questions with no prepositional phrases, but the increasing complexity of questions requires deeper cross-modal fusion between language and knowledge representations. While QA-GNN's end fusion approach of initializing a node in the GNN from the LM's final representation of the context is an effective approach, it compresses the language context to a single vector before allowing interaction with the KG, potentially limiting the cross-relationships between language and knowledge that can be captured (see example in Figure 2). Interestingly, we note that both GreASELM and QA-GNN significantly outperform RoBERTaLarge even when no prepositional phrases are in the question. We hypothesize that some of these questions may require less reasoning, but require specific commonsense knowledge that RoBERTa may not have learned during pretraining (e.g., "What is a person considered a bully known for?").</p>
<p>Qualitative Analysis. In Figure 2, we examine GreASELM's node-to-node attention weights induced by the GNN layers of the model, and analyze whether they reflect more expressive reasoning steps compared to QA-GNN. Figure 2 shows an example from the CommonsenseQA IH-dev set. In this example, GreASELM correctly predicts that the answer is "airplane" while QA-GNN makes an incorrect prediction, "motor vehicle". For both models, we perform Best First Search (BFS) on the retrieved KG subgraph $\mathcal{G}_{\text {sub }}$ to trace high attention weights from the interaction node (purple).
For GreASELM, we observe that the attention by the interaction node increases on the "bug" entity in the intermediate GNN layers, but drops again by the final layer, resembling a suitable intuition surrounding the hedge term "unlikely". Meanwhile, the attention on "windshield" consistently increases across all layers. For QA-GNN, the attention on "bug" increases over multiple layers. As "bug" is mentioned multiple times in the context, it may be well-represented in QA-GNN's context node initialization, which is never reformulated by language representations, unlike in GreASELM.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Qualitative analysis of GreASELM's graph attention weight changes across multiple layers of message passing compared with QA-GNN. GreASELM demonstrates attention change patterns that more closely resemble the expected change in focus on the "bug" entity.</p>
<p>Domain generality Our reported results thus far demonstrate the viability of our method in the general commonsense reasoning domain. In this section, we explore whether GreASELM could be adapted to other domains by evaluating on the MedQA-USMLE dataset. Our results in Table 6 demonstrate that GreASELM outperforms state-of-the-art fine-tuned LMs (e.g., SapBERT; Liu et al., 2021) and a QA-GNN augmentation of SapBERT. Additionally, we note the improved performance over all classical methods and LM methods first reported in Jin et al. (2021). Additional results in Appendix C show that our approach is also agnostic to the language model used with improvements recorded by GreASELM when it is seeded with other LMs, such as PubmedBERT (Gu et al., 2022), and BioBERT (Lee et al., 2020). While these results are promising as they suggest that GreASELM is an effective augmentation of pretrained LMs for different domains and KGs (i.e., the medical domain with the DDB + Drugbank KG), there is still ample room for improvement on this task.</p>
<h2>6 CONCLUSION</h2>
<p>In this paper, we introduce GreASELM, a new model that enables interactive fusion through joint information exchange between knowledge from language models and knowledge graphs. Experimental results demonstrate superior performance compared to prior KG+LM and LM-only baselines across standard datasets from multiple domains (commonsense and medical). Our analysis shows improved capability modeling questions exhibiting textual nuances, such as negation and hedging.</p>
<h1>ACKNOWLEDGMENT</h1>
<p>We thank Rok Sosic, Maria Brbic, Jordan Troutman, Rajas Bansal, and our anonymous reviewers for discussions and for providing feedback on our manuscript. We thank Xiaomeng Jin for help with data preprocessing. We also gratefully acknowledge the support of DARPA under Nos. HR00112190039 (TAMI), N660011924033 (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID), NIH under No. R56LM013365; Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan Chase, Docomo, Hitachi, Intel, JD.com, KDDI, Toshiba, NEC, and UnitedHealth Group. J. L. is a Chan Zuckerberg Biohub investigator. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.</p>
<h2>REFERENCES</h2>
<p>Olivier Bodenreider. The unified medical language system (UMLS): Integrating biomedical terminology. Nucleic acids research, 2004.</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD, 2008.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NeurIPS), 2013.</p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Çelikyilmaz, and Yejin Choi. Comet: Commonsense transformers for automatic knowledge graph construction. In Association for Computational Linguistics (ACL), 2019.</p>
<p>Antoine Bosselut, Ronan Le Bras, and Yejin Choi. Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.</p>
<p>Peter Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, et al. From 'f' to 'a'on the NY Regents science exams: An overview of the Aristo project. arXiv preprint arXiv:1909.01958, 2019.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.</p>
<p>Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-hop relational reasoning for knowledge-aware question answering. In Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Yuxian Gu, Robert Tinn, Hao Cheng, Michael R. Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3:1 - 23, 2022.</p>
<p>Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In AAAI, 2021.</p>
<p>Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 2021.</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system. In Findings of EMNLP, 2020.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36:1234 - 1240, 2020.</p>
<p>Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph networks for commonsense reasoning. In Empirical Methods in Natural Language Processing (EMNLP), 2019.</p>
<p>Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. Self-alignment pretraining for biomedical entity representations. In NAACL, 2021.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, and Songlin Hu. Graph-based reasoning over heterogeneous external knowledge for commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.
G. Marcus. Deep learning: A critical appraisal. ArXiv, abs/1801.00631, 2018.
R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In ACL, 2019.</p>
<p>Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pujara, Xiang Ren, and A. G. Galstyan. Lawyers are dishonest? quantifying representational harms in commonsense knowledge resources. ArXiv, abs/2103.11320, 2021.</p>
<p>Todor Mihaylov and Anette Frank. Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge. In Association for Computational Linguistics (ACL), 2018.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Empirical Methods in Natural Language Processing (EMNLP), 2018.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? In Empirical Methods in Natural Language Processing (EMNLP), 2019.</p>
<p>George Sebastian Pirtoaca. Ai2 leaderboard. URL https://leaderboard.allenai.org/ open_book_qa/submission/brhieieqaupc4cnddfg0.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2020.</p>
<p>Hongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure Leskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs. In International Conference on Machine Learning (ICML), 2021.</p>
<p>Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.</p>
<p>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, 2018.</p>
<p>Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, and Weizhu Chen. Exploiting structured knowledge in text via graph-guided representation learning. In EMNLP, 2020.</p>
<p>Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. Towards controllable biases in language generation. In the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)-Findings, long, 2020.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, 2017.</p>
<p>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: A Core of Semantic Knowledge. In 16th International Conference on the World Wide Web, pp. 697-706, 2007.</p>
<p>Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. Colake: Contextualized language and knowledge embedding. In COLING, 2020.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.</p>
<p>Denny Vrandečić and Markus Krötzsch. Wikidata: A free collaborative knowledgebase. Commun. $A C M, 57(10): 78-85$, September 2014. ISSN 0001-0782. doi: 10.1145/2629489. URL https: //doi.org/10.1145/2629489.</p>
<p>Peifeng Wang, Nanyun Peng, Pedro Szekely, and Xiang Ren. Connecting the dots: A knowledgeable path generator for commonsense question answering. arXiv preprint arXiv:2005.00691, 2020.</p>
<p>Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, et al. Improving natural language inference using external knowledge in the science questions domain. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.</p>
<p>David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, et al. Drugbank 5.0: a major update to the drugbank database for 2018. Nucleic acids research, 2018.</p>
<p>Weiwen Xu, Huihui Zhang, Deng Cai, and Wai Lam. Dynamic semantic graph construction and reasoning for explainable multi-hop science question answering. arXiv preprint arXiv:2105.11776, 2021.</p>
<p>Jun Yan, Mrigank Raman, Aaron Chan, Tianyu Zhang, Ryan Rossi, Handong Zhao, Sungchul Kim, Nedim Lipka, and Xiang Ren. Learning contextualized knowledge structures for commonsense reasoning. arXiv preprint arXiv:2010.12873, 2020.</p>
<p>An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, and Sujian Li. Enhancing pre-trained language representations with rich knowledge for machine reading comprehension. In Association for Computational Linguistics (ACL), 2019.</p>
<p>Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. QAGNN: Reasoning with language models and knowledge graphs for question answering. ArXiv, abs/2104.06378, 2021.</p>
<p>Donghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. Jaket: Joint pre-training of knowledge graph and language understanding. ArXiv, abs/2010.00796, 2020.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language representation with informative entities. In $A C L, 2019$.</p>
<h1>A Ethics Statement</h1>
<p>We outline potential ethical issues with our work below. First, GreASELM is a method to fuse language representations and knowledge graph representations for effective reasoning about textual situations. Consequently, GreASELM could reflect many of the same biases and toxic behaviors exhibited by language models and knowledge graphs that are used to initialize it. For example, prior large-scale language models have been shown to encode biases about race, gender, and other demographic attributes (Sheng et al., 2020). Because GreASELM is seeded with pretrained language models that often learn these patterns, it is possible to reflect them in open-world settings. Second, the ConceptNet knowledge graph (Speer et al., 2017) used in this work has been shown to encode stereotypes (Mehrabi et al., 2021), rather than completely clean commonsense knowledge. If GreASELM were used outside these standard benchmarks in conjunction with ConceptNet as a KG, it might rely on unethical relationships in its knowledge resource to arrive at conclusions. Consequently, while GreASELM could be used for applications outside these standard benchmarks, we would encourage implementers to use the same precautions they would apply to other language models and methods that use noisy knowledge sources.
Another source of ethical concern is the use of the MedQA-USMLE evaluation. While we find clinical reasoning using language models and knowledge graphs to be an interesting testbed for GreASELM and for joint language and reasoning models in general, we do not encourage users to use these models for real world clinical prediction, particularly at these performance levels.</p>
<h2>B EXPERIMENTAL SETUP DETAILS</h2>
<h2>B. 1 Entity Linking</h2>
<p>Given each QA context, we follow the procedure from Yasunaga et al. (2021) to retrieve the subgraph $\mathcal{G}<em _linked="{linked" _text="\text">{\text {sub }}$ from $\mathcal{G}$. First, we perform entity linking to $\mathcal{G}$ to retrieve an initial set of nodes $\mathcal{V}</em>}}$. Second, we add any bridge entities that are in a 2-hop path between any pair of linked entities in $\mathcal{V<em _retrieved="{retrieved" _text="\text">{\text {linked }}$ to get the set of retrieved entities $\mathcal{V}</em>}}$. Then we prune the set of nodes $\mathcal{V<em _sub="{sub" _text="\text">{\text {retrieved }}$ using a relevance score computed for each node. To compute the relevance score, we follow the procedure of Yasunaga et al. (2021) - we concatenate the node name with the context of the QA example, and pass it through a pre-trained LM, using the output score of the node name as the relevance score. We only retain the top 200 scores nodes and prune the remaining ones. Finally, we retrieve all the edges that connect any two nodes in $\mathcal{V}</em>}}$, forming the retrieved subgraph $\mathcal{G<em _sub="{sub" _text="\text">{\text {sub }}$. Each node in $\mathcal{G}</em>$ is assigned a type according to whether its corresponding entity was linked from the context $c$, question $q$, answer $a$, or from a bridge path.}</p>
<h2>B. 2 Graph Initialization</h2>
<p>To compute initial node embeddings (§3.3) for entities retrieved in $\mathcal{G}_{\text {sub }}$ from ConceptNet, we follow the method of MHGRN (Feng et al., 2020). We convert knowledge triples in the KG into sentences using pre-defined templates for each relation. Then, these sentences are fed into a BERT-large LM to compute embeddings for each sentence. Finally, for all sentences containing an entity, we extract all token representations of the entity's mention spans in these sentences, mean pool over these representations and project this mean-pooled representation.
For MedQA-USMLE, node embeddings are initialized similarly using the pooled token output embeddings of the entity name from the SapBERT model (described in $\S 4.2$; Liu et al., 2021). For MedQA, $5 \%$ of examples do not yield a retrieved entity. In these cases, we represent the graph using a dummy node initialized with 0 . In essence, GreaseLM backs off to only using LM representations as the graph propagates no information.</p>
<h2>B. 3 HYPERPARAMETERS</h2>
<p>Table 7: Hyperparameter settings for models and experiments</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Hyperparameter</th>
<th>Dataset</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>CommonsenseQA</td>
<td>OpenbookQA</td>
<td>MedQA-USMLE</td>
</tr>
<tr>
<td>Model architecture</td>
<td>Number of GreASELM layers $M$</td>
<td>5</td>
<td>6</td>
<td>3</td>
</tr>
<tr>
<td></td>
<td>Number of Unimodal LM layers $N$</td>
<td>19</td>
<td>18</td>
<td>9</td>
</tr>
<tr>
<td></td>
<td>Number of attention heads in GNN</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td></td>
<td>Dimension of node embeddings and the messages in GNN</td>
<td>200</td>
<td>200</td>
<td>200</td>
</tr>
<tr>
<td></td>
<td>Dimension of MLP hidden layers (except MInt operator)</td>
<td>200</td>
<td>200</td>
<td>200</td>
</tr>
<tr>
<td></td>
<td>Number of hidden layers of MLPs</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td></td>
<td>Dimension of MInt operator hidden layer</td>
<td>400</td>
<td>200</td>
<td>400</td>
</tr>
<tr>
<td>Regularization</td>
<td>Dropout rate of the embedding layer, GNN layers and fully-connected layers</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
</tr>
<tr>
<td>Optimization</td>
<td>Learning rate of parameters in LM</td>
<td>$1.00 \mathrm{E}-05$</td>
<td>$1.00 \mathrm{E}-05$</td>
<td>$5.00 \mathrm{E}-05$</td>
</tr>
<tr>
<td></td>
<td>Learning rate of parameters not in LM</td>
<td>$1.00 \mathrm{E}-03$</td>
<td>$1.00 \mathrm{E}-03$</td>
<td>$1.00 \mathrm{E}-03$</td>
</tr>
<tr>
<td></td>
<td>Number of epochs in which LM's parameters are kept frozen</td>
<td>4</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>Optimizer</td>
<td>RAdam</td>
<td>RAdam</td>
<td>RAdam</td>
</tr>
<tr>
<td></td>
<td>Learning rate schedule</td>
<td>constant</td>
<td>constant</td>
<td>constant</td>
</tr>
<tr>
<td></td>
<td>Batch size</td>
<td>128</td>
<td>128</td>
<td>128</td>
</tr>
<tr>
<td></td>
<td>Number of epochs</td>
<td>30</td>
<td>70</td>
<td>20</td>
</tr>
<tr>
<td></td>
<td>Max gradient norm (gradient clipping)</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>Data</td>
<td>Max number of nodes</td>
<td>200</td>
<td>200</td>
<td>200</td>
</tr>
<tr>
<td></td>
<td>Max number of tokens</td>
<td>100</td>
<td>100</td>
<td>512</td>
</tr>
</tbody>
</table>
<h1>C ADDITIONAL EXPERIMENTAL RESULTS</h1>
<h2>C. 1 ABLATION STUDIES</h2>
<p>In Table 8, we summarize an ablation study conducted using the CommonsenseQA IHdev set.
Modality interaction. A key component of GreASELM is the connection of the LM to the GNN via the modality interaction module (Eq. 11). If we remove modality interaction, the performance drops significantly, from $78.5 \%$ to $76.5 \%$ (approximately the performance of QA-GNN). Integrating the modality interaction in every other layer instead of consecutive layers also hurts performance. A possible explanation is that skipping layers could impede learning consistent representations across layers for both the LM and the GNN, a property which may be desirable given we initialize the model using a pretrained LM's weights (e.g., RoBERTa). We also find that sharing parameters between modality interaction layers (Eq. 11) outperforms not sharing, possibly because our datasets are not very large (e.g., 10k for CommonsenseQA), and sharing parameters helps prevent overfitting.</p>
<p>Table 8: Ablation study of our model components, using the CommonsenseQA IH-dev set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ablation Type</th>
<th style="text-align: center;">Ablation</th>
<th style="text-align: center;">Dev Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GreASELM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.5</td>
</tr>
<tr>
<td style="text-align: center;">Modality Interaction</td>
<td style="text-align: center;">No interaction</td>
<td style="text-align: center;">76.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Interaction in every other layer</td>
<td style="text-align: center;">76.3</td>
</tr>
<tr>
<td style="text-align: center;">Interaction Layer Parameter Sharing</td>
<td style="text-align: center;">No parameter sharing</td>
<td style="text-align: center;">77.1</td>
</tr>
<tr>
<td style="text-align: center;">Number of GreASELM layers $(M)$</td>
<td style="text-align: center;">$M=4$</td>
<td style="text-align: center;">77.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$M=6$</td>
<td style="text-align: center;">78.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$M=7$</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: center;">Graph Connectivity</td>
<td style="text-align: center;">Interaction node connected to all <br> nodes in $\mathcal{V}<em _linked="{linked" _text="\text">{\text {sub }}$, not only $\mathcal{V}</em>$}</td>
<td style="text-align: center;">77.6</td>
</tr>
<tr>
<td style="text-align: center;">Node Initialization</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">60.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TransE (Bordes et al., 2013)</td>
<td style="text-align: center;">77.7</td>
</tr>
</tbody>
</table>
<p>Number of GreASELM layers. We find that $M=5$ GreASELM layers achieves the highest performance. However, both the results for $M=4$ and $M=6$ are relatively close to the top performance, indicating our method is not overly sensitive to this hyperparameter.</p>
<p>Graph connectivity. The interaction node $e_{\text {int }}$ is a key component of GreASELM that bridges the interaction between the KG and the text. Selecting which nodes in the KG are directly connected to $e_{\text {int }}$ affects the rate at which information from different portions of the KG can reach the text representations. We find that connecting $e_{\text {int }} \mathrm{KG}$ nodes explicitly linked to the input text performs best. Connecting $e_{\text {int }}$ to all nodes in the subgraph (e.g., bridge entities) hurts performance ( $-0.9 \%$ ), possibly because the interaction node is overloaded by having to attend to all nodes in the graph (up to 200). By connecting the interaction node only to linked entities, each linked entity serves as a filter for relevant information that reaches the interaction node.</p>
<p>KG node embedding initialization. Effectively initializing KG node representations is critical. When we initialize nodes randomly instead of using the BERT-based initialization method from Feng et al. (2020), the performance drops significantly ( $78.5 \% \rightarrow 60.8 \%$ ). While using standard KG embeddings (e.g., TransE; Bordes et al., 2013) recovers much of the performance drop ( $77.7 \%$ ), we still find that using BERT-based entity embeddings performs best.</p>
<h1>C. 2 Effect of LM Initialization on GreASELM</h1>
<p>Table 9: Performance on the in-house splits of CommonsenseQA for different LM initializations of our method, GreASELM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">IHdev-Acc.</th>
<th style="text-align: center;">IHtest-Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTA-LARGE</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: left;">+ GreASELM (Ours)</td>
<td style="text-align: center;">$\mathbf{7 8 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTA-BASE</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">59.8</td>
</tr>
<tr>
<td style="text-align: left;">+ GreASELM (Ours)</td>
<td style="text-align: center;">$\mathbf{6 9 . 3}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 10: Initialization on MedQAUSMLE</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Acc. (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SAPBERT-BASE</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: left;">+ GreASELM (Ours)</td>
<td style="text-align: center;">$\mathbf{3 8 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">BioBERT-BASE</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: left;">+ GreASELM (Ours)</td>
<td style="text-align: center;">$\mathbf{3 4 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">PubmedBERT-BASE</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: left;">+ GreASELM (Ours)</td>
<td style="text-align: center;">$\mathbf{3 8 . 7}$</td>
</tr>
</tbody>
</table>
<p>To evaluate whether our method is agnostic to the LM used to seed the GreaseLM layers, we replace the LMs we use in previous experiments (RoBERTa-large for CommonsenseQA and SapBERT for MedQA-USMLE) with RoBERTa-base for CommonsenseQA, and BioBERT and PubmedBERT for MedQA-USMLE. Across multiple LM initializations in two domains, our results demonstrate that GreASELM can provide a consistent improvement for multiple LMs when used as a modality junction between KGs and language.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ OpenbookQA provides an extra corpus of scientific facts in a textual form. AristoRoBERTa is based off RoBERTa-Large, but uses the facts corresponding to each question, prepared by Clark et al. (2019), as an additional input along with the QA context.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-547 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-547</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-547</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-cf9b8da26d9b92e75ba49616ed2a1033f59fce14</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cf9b8da26d9b92e75ba49616ed2a1033f59fce14" target="_blank">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student), which uses the teacher model to encode category texts and image regions of object proposals and trains a student detector, whose region embeddings of detected boxes are aligned with the text and image embedDings inferred by the teacher.</p>
                <p><strong>Paper Abstract:</strong> We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP$_r$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP$_r$. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP$_{50}$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e547.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e547.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A joint image-text model trained contrastively on large-scale image-text pairs that produces normalized image and text embeddings in a shared space; used here as a teacher to provide both text and image embeddings for open-vocabulary detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (ViT-B/32 variant used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive pretraining on (hundreds of) millions of image-text pairs; image encoder (Vision Transformer in the variant used) and a transformer text encoder produce normalized embeddings; used zero-shot for classifying cropped regions and as a teacher for knowledge distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary object detection (classification of cropped proposals / teacher supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify image region proposals by computing image embeddings for cropped regions and comparing them (cosine similarity) to text embeddings computed from category prompts; used as a teacher to provide category-level and region-level supervisory embeddings for a two-stage detector trained on base categories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object detection / open-vocabulary classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (category concepts, attributes, visual similarity); limited contextual/spatial cues from crop context</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large-scale image-text pairs (contrastive learning)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot classification on cropped regions; used as teacher via offline embedding extraction for distillation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Normalized dense vector embeddings for images and text in a shared space; cosine similarity used for matching; prompts ensembled for text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mask AP and breakdowns (AP_r: AP on rare/novel categories, AP_c/AP_f, overall AP); box AP used in some comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CLIP on cropped regions (with objectness ensembling): mask AP_r = 18.9, AP = 17.7 (Table 2). Box AP (CLIP on cropped regions) reported as AP_r = 19.5, AP = 18.6 in Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong at recognizing novel categories and fine-grained visual concepts when applied to reasonably tight crops; text embeddings derived from CLIP encode visual similarity useful for zero-shot category recognition; works well when the crop contains a single salient object.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Confidence scores do not reflect localization quality (high scores may be given to poor/partial boxes); sensitive to aspect-ratio distortion from fixed-size resizing; performance degrades when multiple objects appear in a crop or when crop context dominates; confuses visually similar categories; not trained to assess bounding-box quality.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms supervised-only models on novel-category AP (e.g., supervised base-only had AP_r = 0.0 while CLIP on cropped regions achieves AP_r = 18.9 in mask AP setting), but overall AP trails supervised methods on base/common categories.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not ablated in isolation in this paper beyond comparisons to stronger teacher models (ALIGN, larger CLIP variants) which improve AP substantially (ALIGN on cropped regions: box AP_r = 39.6, AP = 31.4 in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CLIP text embeddings, when paired with cropped image embeddings, encode object-concept and attribute information that generalizes to novel categories, but these embeddings do not reliably encode localization quality or fine-grained spatial information about bounding-box correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e547.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e547.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALIGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALIGN (Scaling up visual and vision-language representation learning with noisy text supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large image-text contrastive model (EfficientNet image encoder + BERT text encoder in variants used) trained on noisy web image-text pairs; used as a stronger teacher to produce image and text embeddings for distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling up visual and vision-language representation learning with noisy text supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALIGN (EfficientNet variants reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive pretraining on large-scale image-text pairs (noisy web data) using EfficientNet image encoders and BERT-like text encoders; provides higher-capacity embeddings that improve distillation outcomes when used as teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary object detection (teacher for distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide higher-quality image and text embeddings for region proposals and category prompts that the student detector distills to enable open-vocabulary detection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object detection / open-vocabulary classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (concepts, attributes) with improved visual discrimination; limited spatial/localization cues</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large-scale image-text pairs</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>offline embedding extraction for teacher supervision; used directly to classify cropped regions (R-CNN style) and to produce image embeddings for ViLD-image distillation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>High-dimensional normalized image and text embeddings in a shared space; cosine similarity matching; teacher embeddings distilled into region embeddings of detector</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mask AP, box AP over novel/base categories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used as teacher, ViLD-ensemble with ALIGN (EfficientNet-b7 student backbone) achieves mask AP_r = 26.3 and overall mask AP = 29.3 (Table 3). ALIGN on cropped regions (Table 6) achieves box AP_r = 39.6 and AP = 31.4.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Stronger teacher embeddings yield substantially better novel-category detection performance when distilled into the student detector; allows student to approach fully supervised performance on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Same qualitative limitations as CLIP regarding localization-aware scoring and multiple-object crops; not a remedy for detectors' localization failures without supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ViLD distilled from ALIGN outperforms ViLD distilled from CLIP and supervised baselines on AP_r in this paper; closely approaches fully-supervised challenge winner on rare categories (within ~3.7 AP_r).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using stronger teacher (ALIGN) increases AP_r substantially compared to CLIP teacher; architectural adjustments (increased FC dims, MBConv blocks) were required to match the teacher embedding geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Higher-capacity image-text teachers (ALIGN) encode richer object-relational visual knowledge that transfers to open-vocabulary detection via distillation, improving novel-category recognition; however, these language-aligned embeddings still do not solve localization-quality estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e547.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e547.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLD-text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViLD-text (text-embedding classifier for region embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the student two-stage detector where the final classifier is replaced with fixed text embeddings (from a teacher text encoder) and a learnable background embedding; trains region embeddings to align with text embeddings via cross-entropy on base categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViLD-text (student Mask R-CNN with classifier replaced by text embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage detector whose classification head uses precomputed text embeddings (ensemble of prompts) as fixed class prototypes; region embeddings are learned (with a projection) and trained with cross-entropy over base categories and a learnable background embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary object detection (text-embedding classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train a detector on annotated base categories so that region embeddings align with text embeddings; at inference, include novel-category text embeddings to perform zero-shot detection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object detection / open-vocabulary classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (category concepts and attribute recognition via text embeddings); weak contextual/spatial encoding via region features</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>teacher text encoder (pretrained CLIP/ALIGN text embeddings) + supervised detection labels on base categories</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised training of region embeddings against fixed text embeddings (cross-entropy), i.e., fine-tuning via knowledge distillation from text encoder</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Text embeddings used as fixed class prototypes; region embeddings projected to the same space and matched via cosine similarity and softmax</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mask AP and breakdowns by rare/common/frequent categories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ViLD-text (ResNet-50, CLIP text embeddings) achieves mask AP_r = 10.1, AP = 24.9 (Table 3). ViLD-text+CLIP (R-CNN style ensemble with CLIP teacher) can yield AP_r = 22.6 (mask) in a slower R-CNN style setup (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Aligning region embeddings with text embeddings trained jointly with images (CLIP) improves generalization over purely text-only word vectors (GloVe) in representing visually-similar concepts; improves base/common AP when supervised on base categories.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limited generalization to novel categories when only base-category text embeddings are used for training (ViLD-text's AP_r < CLIP on cropped regions); competes with visual-distillation objective causing trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ViLD-text (10.1 AP_r) outperforms GloVe-based baseline (3.0 AP_r) but underperforms CLIP-on-crops (18.9 AP_r) for novel-category recognition; supervised-RFS (base+novel) reaches higher AP overall (Table 3 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using CLIP text embeddings provides large gains over GloVe; prompt ensembling yields a modest +0.4 AP_r improvement (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text encoders trained jointly with images (e.g., CLIP) provide class prototypes that encode visual similarity useful for zero-shot detection, but relying solely on text-supervised alignment (ViLD-text) limits novel-category generalization compared to distillation from image embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e547.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e547.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLD-image</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViLD-image (visual embedding distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the student detector that distills precomputed image embeddings from the teacher into region embeddings using an L1 loss, enabling the student to learn from visual concepts present in both base and novel categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViLD-image (student Mask R-CNN with image-embedding distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Student detector trains region embeddings to match ensembled normalized image embeddings (teacher image encoder outputs on offline-cropped proposals) using an L1 loss; offline proposals can include novel categories so distillation transfers visual concepts beyond labeled base categories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary object detection (visual distillation from teacher image embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Precompute image embeddings for offline proposals (1x and 1.5x crops ensembled), and train the detector's region embeddings to match them, enabling open-vocabulary recognition when combined with text embeddings at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object detection / open-vocabulary classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (visual concept-level knowledge, including novel categories) and some contextual/spatial cues from larger crops</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>teacher image encoder embeddings (pretrained CLIP/ALIGN) extracted from cropped regions</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>L1 distillation loss between precomputed teacher image embeddings and learned region embeddings (offline embedding extraction + supervised regression)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Dense normalized image embeddings (vector prototypes) are regressed by region embedding head; these are then compared to text embeddings at inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mask AP (AP_r etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ViLD-image (ResNet-50, CLIP teacher) achieves mask AP_r = 11.2 and overall AP = 11.2 when trained only with image-embedding distillation (Table 3). ViLD-image box AP ~10.3 (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Distillation captures visual concepts present in teacher embeddings, allowing student to gain some recognition ability for novel categories not annotated in detector training; improves transferability (better cross-dataset transfer when combined with text distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Standalone visual-distillation yields lower AP on base/common categories and overall AP compared to text-alignment or combined methods; distilled embeddings may not align perfectly with text prototypes without joint objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ViLD-image (11.2 AP_r) is below CLIP-on-crops (18.9 AP_r) and below combined ViLD variants; combining with ViLD-text boosts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Distillation weight sweep shows L1 loss outperforms L2 for improving AP_r; increasing L1 distillation weight raises AP_r at the cost of AP_c and AP_f (Table 7), indicating a trade-off between novel and base/common performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Distilling teacher image embeddings transfers visual (object-relational) knowledge for novel categories into a detector, but must be balanced with text-based supervision to avoid degrading base-category performance; L1 distillation is effective and stronger teachers yield better transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e547.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e547.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLD (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViLD (Vision and Language knowledge Distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The full method combining ViLD-text (text-embedding classification) and ViLD-image (image-embedding distillation) into a single training objective (cross-entropy to text prototypes + weighted L1 distillation to teacher image embeddings) to enable open-vocabulary detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViLD (two-stage Mask R-CNN student distilled from image-text teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A two-stage detector trained with a joint loss: cross-entropy against fixed text embeddings for base categories (plus learnable background embedding) and an L1 distillation loss aligning region embeddings to teacher image embeddings from cropped proposals; at inference, text embeddings for novel categories are added for open-vocabulary detection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary object detection on LVIS (and transfer to COCO, PASCAL VOC, Objects365)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect and segment objects from a large open vocabulary (LVIS: 1,203 categories) while training only on base-category annotations; generalize to novel categories using text and image embedding supervision from a pretrained image-text model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object detection / open-vocabulary detection / transfer</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (category semantics, attributes, fine-grained distinctions) and limited spatial/contextual cues via region features and ensembled crops; procedural knowledge not represented</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervision from teacher image-text model embeddings (CLIP or ALIGN) plus base-category detection labels</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>joint training/fine-tuning of detector using fixed text prototypes (cross-entropy) and offline teacher image embedding distillation (L1 loss)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Region embeddings in detector are trained to align with teacher image embeddings and text prototypes; matching is via cosine similarity and normalization; attribute/category probabilities computed via softmax over similarities (with temperature).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mask AP and breakdowns (AP_r for novel/rare categories, AP_c/AP_f for common/frequent categories), also box AP for some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ViLD (ResNet-50, CLIP teacher, w=0.5) achieves mask AP_r = 16.1, AP = 22.5 (Table 3). ViLD-ensemble with ALIGN teacher and EfficientNet-b7 backbone achieves mask AP_r = 26.3 and overall mask AP = 29.3 (Table 3). Transfers: ViLD (R50) yields PASCAL VOC AP50 = 72.2, COCO AP = 36.6, Objects365 AP = 11.8 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Combining text- and image-embedding supervision improves novel-category detection beyond either alone; enables attribute and fine-grained recognition (e.g., color attributes, some bird species) and allows direct transfer to other datasets by swapping text prototypes; region proposals trained class-agnostically generalize to novel categories.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Joint objectives can compete, producing trade-offs (higher distillation weight increases AP_r but reduces AP_c/AP_f); detector still relies on visual localization quality and mask head can fail due to low-level appearance cues; model does not encode procedural knowledge for manipulation/planning; CLIP-derived embeddings still do not encode localization-quality assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ViLD outperforms supervised counterpart on rare-category AP (ViLD AP_r 16.1 vs supervised-RFS 12.3 in ResNet-50 mask AP setting) and outperforms previous open-vocabulary detectors on COCO by several AP points (see Table 4); stronger teacher (ALIGN) yields substantial gains (ViLD-ensemble w/ ALIGN AP_r 26.3).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Distillation weight sweep (Table 7) shows L1 loss and increasing weight raise AP_r but reduce AP_c/AP_f; prompt ensembling yields small AP_r gains (+0.4, Table 9); separating heads for text- and image-alignment (ViLD-ensemble) reduces competition and improves AP_c and AP_r.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language models' text embeddings (when trained jointly with images) provide class prototypes that capture object-relational and attribute information useful for zero-shot detection; distilling teacher image embeddings transfers visual concept knowledge (including to novel categories) into region embeddings; however, such language-aligned embeddings do not encode reliable localization-quality (spatial) signals or procedural knowledge — spatial understanding remains tied to detector's visual features and proposal quality, and balancing text vs visual distillation is critical to trade off novel-category vs base-category performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e547.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e547.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLD-ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViLD-ensemble (separate heads + ensembling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensembling variant that trains two separate region-embedding heads (one optimized for text alignment and one for image-embedding distillation) and combines their text-based predictions to reduce conflict between objectives and improve final detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ViLD-ensemble (two-head student detector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses two identical-head architectures: one trained with ViLD-text objective and one with ViLD-image objective; at inference, both heads' similarity scores to text embeddings are ensembled (weighted geometric average) with different weights for base vs novel categories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary object detection (improved ensembling to reconcile conflicting objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mitigate competition between text- and image-alignment objectives by learning separate region embeddings and ensembling their outputs to improve both base and novel category detection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object detection / open-vocabulary detection</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational primarily; reduces interference so both text-aligned and image-distilled knowledge are preserved</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>teacher image-text embeddings + detection labels on base categories</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>joint training of two heads (one with cross-entropy to text prototypes, one with L1 distillation to teacher image embeddings) followed by weighted ensembling at inference</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Separate learned region embeddings projected to text-embedding space; final probabilities are geometric-average ensembles of softmaxed cosine similarities to class text embeddings with category-dependent weights.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mask AP (AP_r, AP_c, AP_f, overall AP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ViLD-ensemble (ResNet-50, w=0.5) achieves mask AP_r = 16.6, overall AP = 25.5 (Table 3). ViLD-ensemble w/ ViT-L/14 teacher (EfficientNet-b7 backbone) yields mask AP_r = 21.7, AP = 29.6; ViLD-ensemble w/ ALIGN yields mask AP_r = 26.3, AP = 29.3.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Separating objectives into distinct heads reduces the competition observed in single-head ViLD and improves both novel and common category performance; ensembling with teacher (R-CNN style) can further boost novel AP though at inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Inference-time ensembling with teacher models is slow (requires feeding crops to teacher); trade-off hyperparameters (ensemble weights) must be tuned; still limited in encoding spatial/localization quality from language alone.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ViLD-ensemble outperforms single-head ViLD and ViLD-text/ViLD-image in combined metrics; ViLD-ensemble w/ ALIGN approaches supervised strong baselines on some metrics (Table 3 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Separating heads and ensembling reduces the negative trade-offs shown in distillation-weight sweeps; using stronger teacher embeddings (ViT-L/14, ALIGN) further improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Architectural separation of text- and image-alignment objectives preserves complementary knowledge from language (text prototypes) and vision (teacher image embeddings), improving open-vocabulary detection performance; however, language encodings still do not substitute for explicit localization/spatial scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e547.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e547.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attribute-Conditional Expansion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Systematic expansion of dataset vocabulary via attribute-conditioned probabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper to expand detection vocabulary by combining category and attribute text embeddings under conditional-independence assumptions, computing Pr(category, attribute | region) = Pr(category | region) * Pr(attribute | region).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Attribute-Conditional Expansion (uses ViLD region embeddings and text encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compute cosine similarities between a region's learned embedding and text embeddings for categories and attributes (prompt-ensembled); softmax with temperature gives Pr(category | region) and Pr(attribute | region); combine multiplicatively to yield joint category-attribute predictions, enabling automated vocabulary expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary detection with attribute augmentation (e.g., color attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a detector's region embedding, compute probabilities over categories and separately over attributes (colors, fine-grained labels) using text encoder prototypes, and combine them to produce joint predictions for category-attribute pairs (p × q vocabulary expansion).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object detection with attribute/description expansion (semantic augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (attributes, fine-grained categories), limited spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>text encoder (prompt-ensembled prototypes) + learned region embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>on-the-fly softmax classification of region embeddings against sets of textual prompts (attributes and categories) and multiplicative combination</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Normalized vector text prototypes for attributes and categories; probabilistic outputs (softmax over cosine similarities) combined under conditional-independence assumption</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative demonstration (figures) and example detection quality; no global AP metrics reported specifically for every attribute-expanded vocabulary in tables (but qualitative results shown in Figure 6)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative success: example in Figure 6 shows correct color attribution for fruits; no global numeric metric reported for attribute-expanded vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables detection with attribute labels (colors) and systematic expansion to p×q vocabulary; works for visually-distinctive attributes and categories.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Conditional-independence assumption (Pr(attribute|category,region)=Pr(attribute|region)) may be violated; results vary when attributes are subtle or visually confounded; limitations inherit detector localization errors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No direct baseline reported; method is an application enabled by ViLD's learned region embeddings and text prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not explicitly ablated; prompt ensembling for text prototypes yields small gains and is used in this method.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text embeddings aligned with region embeddings can be used to predict attributes and fine-grained categories on-the-fly, allowing systematic vocabulary expansion (object-relational knowledge by language prototypes), but this is constrained by visual discriminability and localization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Scaling up visual and vision-language representation learning with noisy text supervision <em>(Rating: 2)</em></li>
                <li>Open-vocabulary object detection using captions <em>(Rating: 2)</em></li>
                <li>Zero-shot object detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-547",
    "paper_id": "paper-cf9b8da26d9b92e75ba49616ed2a1033f59fce14",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A joint image-text model trained contrastively on large-scale image-text pairs that produces normalized image and text embeddings in a shared space; used here as a teacher to provide both text and image embeddings for open-vocabulary detection.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP (ViT-B/32 variant used in experiments)",
            "model_size": null,
            "model_description": "Contrastive pretraining on (hundreds of) millions of image-text pairs; image encoder (Vision Transformer in the variant used) and a transformer text encoder produce normalized embeddings; used zero-shot for classifying cropped regions and as a teacher for knowledge distillation.",
            "task_name": "Open-vocabulary object detection (classification of cropped proposals / teacher supervision)",
            "task_description": "Classify image region proposals by computing image embeddings for cropped regions and comparing them (cosine similarity) to text embeddings computed from category prompts; used as a teacher to provide category-level and region-level supervisory embeddings for a two-stage detector trained on base categories.",
            "task_type": "object detection / open-vocabulary classification",
            "knowledge_type": "object-relational (category concepts, attributes, visual similarity); limited contextual/spatial cues from crop context",
            "knowledge_source": "pre-training on large-scale image-text pairs (contrastive learning)",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot classification on cropped regions; used as teacher via offline embedding extraction for distillation",
            "knowledge_representation": "Normalized dense vector embeddings for images and text in a shared space; cosine similarity used for matching; prompts ensembled for text embeddings",
            "performance_metric": "Mask AP and breakdowns (AP_r: AP on rare/novel categories, AP_c/AP_f, overall AP); box AP used in some comparisons",
            "performance_result": "CLIP on cropped regions (with objectness ensembling): mask AP_r = 18.9, AP = 17.7 (Table 2). Box AP (CLIP on cropped regions) reported as AP_r = 19.5, AP = 18.6 in Table 6.",
            "success_patterns": "Strong at recognizing novel categories and fine-grained visual concepts when applied to reasonably tight crops; text embeddings derived from CLIP encode visual similarity useful for zero-shot category recognition; works well when the crop contains a single salient object.",
            "failure_patterns": "Confidence scores do not reflect localization quality (high scores may be given to poor/partial boxes); sensitive to aspect-ratio distortion from fixed-size resizing; performance degrades when multiple objects appear in a crop or when crop context dominates; confuses visually similar categories; not trained to assess bounding-box quality.",
            "baseline_comparison": "Outperforms supervised-only models on novel-category AP (e.g., supervised base-only had AP_r = 0.0 while CLIP on cropped regions achieves AP_r = 18.9 in mask AP setting), but overall AP trails supervised methods on base/common categories.",
            "ablation_results": "Not ablated in isolation in this paper beyond comparisons to stronger teacher models (ALIGN, larger CLIP variants) which improve AP substantially (ALIGN on cropped regions: box AP_r = 39.6, AP = 31.4 in Table 6).",
            "key_findings": "CLIP text embeddings, when paired with cropped image embeddings, encode object-concept and attribute information that generalizes to novel categories, but these embeddings do not reliably encode localization quality or fine-grained spatial information about bounding-box correctness.",
            "uuid": "e547.0",
            "source_info": {
                "paper_title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ALIGN",
            "name_full": "ALIGN (Scaling up visual and vision-language representation learning with noisy text supervision)",
            "brief_description": "A large image-text contrastive model (EfficientNet image encoder + BERT text encoder in variants used) trained on noisy web image-text pairs; used as a stronger teacher to produce image and text embeddings for distillation.",
            "citation_title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "mention_or_use": "use",
            "model_name": "ALIGN (EfficientNet variants reported)",
            "model_size": null,
            "model_description": "Contrastive pretraining on large-scale image-text pairs (noisy web data) using EfficientNet image encoders and BERT-like text encoders; provides higher-capacity embeddings that improve distillation outcomes when used as teacher.",
            "task_name": "Open-vocabulary object detection (teacher for distillation)",
            "task_description": "Provide higher-quality image and text embeddings for region proposals and category prompts that the student detector distills to enable open-vocabulary detection.",
            "task_type": "object detection / open-vocabulary classification",
            "knowledge_type": "object-relational (concepts, attributes) with improved visual discrimination; limited spatial/localization cues",
            "knowledge_source": "pre-training on large-scale image-text pairs",
            "has_direct_sensory_input": true,
            "elicitation_method": "offline embedding extraction for teacher supervision; used directly to classify cropped regions (R-CNN style) and to produce image embeddings for ViLD-image distillation",
            "knowledge_representation": "High-dimensional normalized image and text embeddings in a shared space; cosine similarity matching; teacher embeddings distilled into region embeddings of detector",
            "performance_metric": "Mask AP, box AP over novel/base categories",
            "performance_result": "When used as teacher, ViLD-ensemble with ALIGN (EfficientNet-b7 student backbone) achieves mask AP_r = 26.3 and overall mask AP = 29.3 (Table 3). ALIGN on cropped regions (Table 6) achieves box AP_r = 39.6 and AP = 31.4.",
            "success_patterns": "Stronger teacher embeddings yield substantially better novel-category detection performance when distilled into the student detector; allows student to approach fully supervised performance on some metrics.",
            "failure_patterns": "Same qualitative limitations as CLIP regarding localization-aware scoring and multiple-object crops; not a remedy for detectors' localization failures without supervision.",
            "baseline_comparison": "ViLD distilled from ALIGN outperforms ViLD distilled from CLIP and supervised baselines on AP_r in this paper; closely approaches fully-supervised challenge winner on rare categories (within ~3.7 AP_r).",
            "ablation_results": "Using stronger teacher (ALIGN) increases AP_r substantially compared to CLIP teacher; architectural adjustments (increased FC dims, MBConv blocks) were required to match the teacher embedding geometry.",
            "key_findings": "Higher-capacity image-text teachers (ALIGN) encode richer object-relational visual knowledge that transfers to open-vocabulary detection via distillation, improving novel-category recognition; however, these language-aligned embeddings still do not solve localization-quality estimation.",
            "uuid": "e547.1",
            "source_info": {
                "paper_title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ViLD-text",
            "name_full": "ViLD-text (text-embedding classifier for region embeddings)",
            "brief_description": "A variant of the student two-stage detector where the final classifier is replaced with fixed text embeddings (from a teacher text encoder) and a learnable background embedding; trains region embeddings to align with text embeddings via cross-entropy on base categories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViLD-text (student Mask R-CNN with classifier replaced by text embeddings)",
            "model_size": null,
            "model_description": "Two-stage detector whose classification head uses precomputed text embeddings (ensemble of prompts) as fixed class prototypes; region embeddings are learned (with a projection) and trained with cross-entropy over base categories and a learnable background embedding.",
            "task_name": "Open-vocabulary object detection (text-embedding classification)",
            "task_description": "Train a detector on annotated base categories so that region embeddings align with text embeddings; at inference, include novel-category text embeddings to perform zero-shot detection.",
            "task_type": "object detection / open-vocabulary classification",
            "knowledge_type": "object-relational (category concepts and attribute recognition via text embeddings); weak contextual/spatial encoding via region features",
            "knowledge_source": "teacher text encoder (pretrained CLIP/ALIGN text embeddings) + supervised detection labels on base categories",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised training of region embeddings against fixed text embeddings (cross-entropy), i.e., fine-tuning via knowledge distillation from text encoder",
            "knowledge_representation": "Text embeddings used as fixed class prototypes; region embeddings projected to the same space and matched via cosine similarity and softmax",
            "performance_metric": "Mask AP and breakdowns by rare/common/frequent categories",
            "performance_result": "ViLD-text (ResNet-50, CLIP text embeddings) achieves mask AP_r = 10.1, AP = 24.9 (Table 3). ViLD-text+CLIP (R-CNN style ensemble with CLIP teacher) can yield AP_r = 22.6 (mask) in a slower R-CNN style setup (Table 3).",
            "success_patterns": "Aligning region embeddings with text embeddings trained jointly with images (CLIP) improves generalization over purely text-only word vectors (GloVe) in representing visually-similar concepts; improves base/common AP when supervised on base categories.",
            "failure_patterns": "Limited generalization to novel categories when only base-category text embeddings are used for training (ViLD-text's AP_r &lt; CLIP on cropped regions); competes with visual-distillation objective causing trade-offs.",
            "baseline_comparison": "ViLD-text (10.1 AP_r) outperforms GloVe-based baseline (3.0 AP_r) but underperforms CLIP-on-crops (18.9 AP_r) for novel-category recognition; supervised-RFS (base+novel) reaches higher AP overall (Table 3 comparisons).",
            "ablation_results": "Using CLIP text embeddings provides large gains over GloVe; prompt ensembling yields a modest +0.4 AP_r improvement (Table 9).",
            "key_findings": "Text encoders trained jointly with images (e.g., CLIP) provide class prototypes that encode visual similarity useful for zero-shot detection, but relying solely on text-supervised alignment (ViLD-text) limits novel-category generalization compared to distillation from image embeddings.",
            "uuid": "e547.2",
            "source_info": {
                "paper_title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ViLD-image",
            "name_full": "ViLD-image (visual embedding distillation)",
            "brief_description": "A variant of the student detector that distills precomputed image embeddings from the teacher into region embeddings using an L1 loss, enabling the student to learn from visual concepts present in both base and novel categories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViLD-image (student Mask R-CNN with image-embedding distillation)",
            "model_size": null,
            "model_description": "Student detector trains region embeddings to match ensembled normalized image embeddings (teacher image encoder outputs on offline-cropped proposals) using an L1 loss; offline proposals can include novel categories so distillation transfers visual concepts beyond labeled base categories.",
            "task_name": "Open-vocabulary object detection (visual distillation from teacher image embeddings)",
            "task_description": "Precompute image embeddings for offline proposals (1x and 1.5x crops ensembled), and train the detector's region embeddings to match them, enabling open-vocabulary recognition when combined with text embeddings at inference.",
            "task_type": "object detection / open-vocabulary classification",
            "knowledge_type": "object-relational (visual concept-level knowledge, including novel categories) and some contextual/spatial cues from larger crops",
            "knowledge_source": "teacher image encoder embeddings (pretrained CLIP/ALIGN) extracted from cropped regions",
            "has_direct_sensory_input": true,
            "elicitation_method": "L1 distillation loss between precomputed teacher image embeddings and learned region embeddings (offline embedding extraction + supervised regression)",
            "knowledge_representation": "Dense normalized image embeddings (vector prototypes) are regressed by region embedding head; these are then compared to text embeddings at inference",
            "performance_metric": "Mask AP (AP_r etc.)",
            "performance_result": "ViLD-image (ResNet-50, CLIP teacher) achieves mask AP_r = 11.2 and overall AP = 11.2 when trained only with image-embedding distillation (Table 3). ViLD-image box AP ~10.3 (Table 8).",
            "success_patterns": "Distillation captures visual concepts present in teacher embeddings, allowing student to gain some recognition ability for novel categories not annotated in detector training; improves transferability (better cross-dataset transfer when combined with text distillation).",
            "failure_patterns": "Standalone visual-distillation yields lower AP on base/common categories and overall AP compared to text-alignment or combined methods; distilled embeddings may not align perfectly with text prototypes without joint objectives.",
            "baseline_comparison": "ViLD-image (11.2 AP_r) is below CLIP-on-crops (18.9 AP_r) and below combined ViLD variants; combining with ViLD-text boosts performance.",
            "ablation_results": "Distillation weight sweep shows L1 loss outperforms L2 for improving AP_r; increasing L1 distillation weight raises AP_r at the cost of AP_c and AP_f (Table 7), indicating a trade-off between novel and base/common performance.",
            "key_findings": "Distilling teacher image embeddings transfers visual (object-relational) knowledge for novel categories into a detector, but must be balanced with text-based supervision to avoid degrading base-category performance; L1 distillation is effective and stronger teachers yield better transfer.",
            "uuid": "e547.3",
            "source_info": {
                "paper_title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ViLD (combined)",
            "name_full": "ViLD (Vision and Language knowledge Distillation)",
            "brief_description": "The full method combining ViLD-text (text-embedding classification) and ViLD-image (image-embedding distillation) into a single training objective (cross-entropy to text prototypes + weighted L1 distillation to teacher image embeddings) to enable open-vocabulary detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViLD (two-stage Mask R-CNN student distilled from image-text teacher)",
            "model_size": null,
            "model_description": "A two-stage detector trained with a joint loss: cross-entropy against fixed text embeddings for base categories (plus learnable background embedding) and an L1 distillation loss aligning region embeddings to teacher image embeddings from cropped proposals; at inference, text embeddings for novel categories are added for open-vocabulary detection.",
            "task_name": "Open-vocabulary object detection on LVIS (and transfer to COCO, PASCAL VOC, Objects365)",
            "task_description": "Detect and segment objects from a large open vocabulary (LVIS: 1,203 categories) while training only on base-category annotations; generalize to novel categories using text and image embedding supervision from a pretrained image-text model.",
            "task_type": "object detection / open-vocabulary detection / transfer",
            "knowledge_type": "object-relational (category semantics, attributes, fine-grained distinctions) and limited spatial/contextual cues via region features and ensembled crops; procedural knowledge not represented",
            "knowledge_source": "supervision from teacher image-text model embeddings (CLIP or ALIGN) plus base-category detection labels",
            "has_direct_sensory_input": true,
            "elicitation_method": "joint training/fine-tuning of detector using fixed text prototypes (cross-entropy) and offline teacher image embedding distillation (L1 loss)",
            "knowledge_representation": "Region embeddings in detector are trained to align with teacher image embeddings and text prototypes; matching is via cosine similarity and normalization; attribute/category probabilities computed via softmax over similarities (with temperature).",
            "performance_metric": "Mask AP and breakdowns (AP_r for novel/rare categories, AP_c/AP_f for common/frequent categories), also box AP for some experiments",
            "performance_result": "ViLD (ResNet-50, CLIP teacher, w=0.5) achieves mask AP_r = 16.1, AP = 22.5 (Table 3). ViLD-ensemble with ALIGN teacher and EfficientNet-b7 backbone achieves mask AP_r = 26.3 and overall mask AP = 29.3 (Table 3). Transfers: ViLD (R50) yields PASCAL VOC AP50 = 72.2, COCO AP = 36.6, Objects365 AP = 11.8 (Table 5).",
            "success_patterns": "Combining text- and image-embedding supervision improves novel-category detection beyond either alone; enables attribute and fine-grained recognition (e.g., color attributes, some bird species) and allows direct transfer to other datasets by swapping text prototypes; region proposals trained class-agnostically generalize to novel categories.",
            "failure_patterns": "Joint objectives can compete, producing trade-offs (higher distillation weight increases AP_r but reduces AP_c/AP_f); detector still relies on visual localization quality and mask head can fail due to low-level appearance cues; model does not encode procedural knowledge for manipulation/planning; CLIP-derived embeddings still do not encode localization-quality assessment.",
            "baseline_comparison": "ViLD outperforms supervised counterpart on rare-category AP (ViLD AP_r 16.1 vs supervised-RFS 12.3 in ResNet-50 mask AP setting) and outperforms previous open-vocabulary detectors on COCO by several AP points (see Table 4); stronger teacher (ALIGN) yields substantial gains (ViLD-ensemble w/ ALIGN AP_r 26.3).",
            "ablation_results": "Distillation weight sweep (Table 7) shows L1 loss and increasing weight raise AP_r but reduce AP_c/AP_f; prompt ensembling yields small AP_r gains (+0.4, Table 9); separating heads for text- and image-alignment (ViLD-ensemble) reduces competition and improves AP_c and AP_r.",
            "key_findings": "Language models' text embeddings (when trained jointly with images) provide class prototypes that capture object-relational and attribute information useful for zero-shot detection; distilling teacher image embeddings transfers visual concept knowledge (including to novel categories) into region embeddings; however, such language-aligned embeddings do not encode reliable localization-quality (spatial) signals or procedural knowledge — spatial understanding remains tied to detector's visual features and proposal quality, and balancing text vs visual distillation is critical to trade off novel-category vs base-category performance.",
            "uuid": "e547.4",
            "source_info": {
                "paper_title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ViLD-ensemble",
            "name_full": "ViLD-ensemble (separate heads + ensembling)",
            "brief_description": "An ensembling variant that trains two separate region-embedding heads (one optimized for text alignment and one for image-embedding distillation) and combines their text-based predictions to reduce conflict between objectives and improve final detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ViLD-ensemble (two-head student detector)",
            "model_size": null,
            "model_description": "Uses two identical-head architectures: one trained with ViLD-text objective and one with ViLD-image objective; at inference, both heads' similarity scores to text embeddings are ensembled (weighted geometric average) with different weights for base vs novel categories.",
            "task_name": "Open-vocabulary object detection (improved ensembling to reconcile conflicting objectives)",
            "task_description": "Mitigate competition between text- and image-alignment objectives by learning separate region embeddings and ensembling their outputs to improve both base and novel category detection.",
            "task_type": "object detection / open-vocabulary detection",
            "knowledge_type": "object-relational primarily; reduces interference so both text-aligned and image-distilled knowledge are preserved",
            "knowledge_source": "teacher image-text embeddings + detection labels on base categories",
            "has_direct_sensory_input": true,
            "elicitation_method": "joint training of two heads (one with cross-entropy to text prototypes, one with L1 distillation to teacher image embeddings) followed by weighted ensembling at inference",
            "knowledge_representation": "Separate learned region embeddings projected to text-embedding space; final probabilities are geometric-average ensembles of softmaxed cosine similarities to class text embeddings with category-dependent weights.",
            "performance_metric": "Mask AP (AP_r, AP_c, AP_f, overall AP)",
            "performance_result": "ViLD-ensemble (ResNet-50, w=0.5) achieves mask AP_r = 16.6, overall AP = 25.5 (Table 3). ViLD-ensemble w/ ViT-L/14 teacher (EfficientNet-b7 backbone) yields mask AP_r = 21.7, AP = 29.6; ViLD-ensemble w/ ALIGN yields mask AP_r = 26.3, AP = 29.3.",
            "success_patterns": "Separating objectives into distinct heads reduces the competition observed in single-head ViLD and improves both novel and common category performance; ensembling with teacher (R-CNN style) can further boost novel AP though at inference cost.",
            "failure_patterns": "Inference-time ensembling with teacher models is slow (requires feeding crops to teacher); trade-off hyperparameters (ensemble weights) must be tuned; still limited in encoding spatial/localization quality from language alone.",
            "baseline_comparison": "ViLD-ensemble outperforms single-head ViLD and ViLD-text/ViLD-image in combined metrics; ViLD-ensemble w/ ALIGN approaches supervised strong baselines on some metrics (Table 3 comparisons).",
            "ablation_results": "Separating heads and ensembling reduces the negative trade-offs shown in distillation-weight sweeps; using stronger teacher embeddings (ViT-L/14, ALIGN) further improves results.",
            "key_findings": "Architectural separation of text- and image-alignment objectives preserves complementary knowledge from language (text prototypes) and vision (teacher image embeddings), improving open-vocabulary detection performance; however, language encodings still do not substitute for explicit localization/spatial scoring.",
            "uuid": "e547.5",
            "source_info": {
                "paper_title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Attribute-Conditional Expansion",
            "name_full": "Systematic expansion of dataset vocabulary via attribute-conditioned probabilities",
            "brief_description": "A method introduced in this paper to expand detection vocabulary by combining category and attribute text embeddings under conditional-independence assumptions, computing Pr(category, attribute | region) = Pr(category | region) * Pr(attribute | region).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Attribute-Conditional Expansion (uses ViLD region embeddings and text encoder)",
            "model_size": null,
            "model_description": "Compute cosine similarities between a region's learned embedding and text embeddings for categories and attributes (prompt-ensembled); softmax with temperature gives Pr(category | region) and Pr(attribute | region); combine multiplicatively to yield joint category-attribute predictions, enabling automated vocabulary expansion.",
            "task_name": "Open-vocabulary detection with attribute augmentation (e.g., color attributes)",
            "task_description": "Given a detector's region embedding, compute probabilities over categories and separately over attributes (colors, fine-grained labels) using text encoder prototypes, and combine them to produce joint predictions for category-attribute pairs (p × q vocabulary expansion).",
            "task_type": "object detection with attribute/description expansion (semantic augmentation)",
            "knowledge_type": "object-relational (attributes, fine-grained categories), limited spatial",
            "knowledge_source": "text encoder (prompt-ensembled prototypes) + learned region embeddings",
            "has_direct_sensory_input": true,
            "elicitation_method": "on-the-fly softmax classification of region embeddings against sets of textual prompts (attributes and categories) and multiplicative combination",
            "knowledge_representation": "Normalized vector text prototypes for attributes and categories; probabilistic outputs (softmax over cosine similarities) combined under conditional-independence assumption",
            "performance_metric": "Qualitative demonstration (figures) and example detection quality; no global AP metrics reported specifically for every attribute-expanded vocabulary in tables (but qualitative results shown in Figure 6)",
            "performance_result": "Qualitative success: example in Figure 6 shows correct color attribution for fruits; no global numeric metric reported for attribute-expanded vocabulary.",
            "success_patterns": "Enables detection with attribute labels (colors) and systematic expansion to p×q vocabulary; works for visually-distinctive attributes and categories.",
            "failure_patterns": "Conditional-independence assumption (Pr(attribute|category,region)=Pr(attribute|region)) may be violated; results vary when attributes are subtle or visually confounded; limitations inherit detector localization errors.",
            "baseline_comparison": "No direct baseline reported; method is an application enabled by ViLD's learned region embeddings and text prototypes.",
            "ablation_results": "Not explicitly ablated; prompt ensembling for text prototypes yields small gains and is used in this method.",
            "key_findings": "Text embeddings aligned with region embeddings can be used to predict attributes and fine-grained categories on-the-fly, allowing systematic vocabulary expansion (object-relational knowledge by language prototypes), but this is constrained by visual discriminability and localization quality.",
            "uuid": "e547.6",
            "source_info": {
                "paper_title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "rating": 2
        },
        {
            "paper_title": "Open-vocabulary object detection using captions",
            "rating": 2
        },
        {
            "paper_title": "Zero-shot object detection",
            "rating": 1
        }
    ],
    "cost": 0.02084175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION</h1>
<p>Xiuye Gu ${ }^{1}$, Tsung-Yi Lin ${ }^{2}$, Weicheng Kuo ${ }^{1}$, Yin Cui ${ }^{1}$<br>${ }^{1}$ Google Research, ${ }^{2}$ Nvidia*<br>{xiuyegu, weicheng, yincui}@google.com tsungyil@nvidia.com</p>
<h4>Abstract</h4>
<p>We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask $\mathrm{AP}<em r="r">{r}$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves $\mathbf{2 6 . 3} \mathrm{AP}</em>$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-theart (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/ tree/master/models/official/detection/projects/vild.}$. The model can directly transfer to other datasets without finetuning, achieving $72.2 \mathrm{AP}_{50</p>
<h2>1 INTRODUCTION</h2>
<p>Consider Fig. 1, can we design object detectors beyond recognizing only base categories (e.g., toy) present in training labels and expand the vocabulary to detect novel categories (e.g., toy elephant)? In this paper, we aim to train an open-vocabulary object detector that detects objects in any novel categories described by text inputs, using only detection annotations in base categories.</p>
<p>Existing object detection algorithms often learn to detect only the categories present in detection datasets. A common approach to increase the detection vocabulary is by collecting images with more labeled categories. The research community has recently collected new object detection datasets with large vocabularies (Gupta et al., 2019; Kuznetsova et al., 2020). LVIS (Gupta et al., 2019) is a milestone of these efforts by building a dataset with 1,203 categories. With such a rich vocabulary, it becomes quite challenging to collect enough training examples for all categories. By Zipf's law, object categories naturally follow a long-tailed distribution. To find sufficient training examples for rare categories, significantly more data is needed (Gupta et al., 2019), which makes it expensive to scale up detection vocabularies.</p>
<p>On the other hand, paired image-text data are abundant on the Internet. Recently, Radford et al. (2021) train a joint vision and language model using 400 million image-text pairs and demonstrate impressive results on directly transferring to over 30 datasets. The pretrained text encoder is the key to the zero-shot transfer ability to arbitrary text categories. Despite the great success on learning image-level representations, learning object-level representations for open-vocabulary detection is still challenging. In this work, we consider borrowing the knowledge from a pretrained openvocabulary classification model to enable open-vocabulary detection.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of our open-vocabulary detector with arbitrary texts. After training on base categories (purple), we can detect novel categories (pink) that are not present in the training data.</p>
<p>We begin with an R-CNN (Girshick et al., 2014) style approach. We turn open-vocabulary detection into two sub-problems: 1) generalized object proposal and 2) open-vocabulary image classification. We train a region proposal model using examples from the base categories. Then we use the pretrained open-vocabulary image classification model to classify cropped object proposals, which can contain both base and novel categories. We benchmark on LVIS (Gupta et al., 2019) by holding out all rare categories as novel categories and treat others as base categories. To our surprise, the performance on the novel categories already surpasses its supervised counterpart. However, this approach is very slow for inference, because it feeds object proposals one-by-one into the classification model.</p>
<p>To address the above issue, we propose ViLD (Vision and Language knowledge Distillation) for training two-stage open-vocabulary detectors. ViLD consists of two components: learning with text embeddings (ViLD-text) and image embeddings (ViLD-image) inferred by an open-vocabulary image classification model, e.g., CLIP. In ViLD-text, we obtain the text embeddings by feeding category names into the pretrained text encoder. Then the inferred text embeddings are used to classify detected regions. Similar approaches have been used in prior detection works (Bansal et al., 2018; Rahman et al., 2018; Zareian et al., 2021). We find text embeddings learned jointly with visual data can better encode the visual similarity between concepts, compared to text embeddings learned from a language corpus, e.g., GloVe (Pennington et al., 2014). Using CLIP text embeddings achieves $10.1 \mathrm{AP}<em r="r">{r}$ (AP of novel categories) on LVIS, significantly outperforming the $3.0 \mathrm{AP}</em>$ of using GloVe. In ViLD-image, we obtain the image embeddings by feeding the object proposals into the pretrained image encoder. Then we train a Mask R-CNN whose region embeddings of detected boxes are aligned with these image embeddings. In contrast to ViLD-text, ViLD-image distills knowledge from both base and novel categories since the proposal network may detect regions containing novel objects, while ViLD-text only learns from base categories. Distillation enables ViLD to be general in choosing teacher and student architectures. ViLD is also energy-efficient as it works with off-the-shelf open-vocabulary image classifiers. We experiment with the CLIP and ALIGN (Jia et al., 2021) teacher models with different architectures (ViT and EfficientNet).</p>
<p>We show that ViLD achieves 16.1 AP for novel categories on LVIS, surpassing the supervised counterpart by 3.8. We further use ALIGN as a stronger teacher model to push the performance to 26.3 novel AP, which is close (only 3.7 worse) to the 2020 LVIS Challenge winner (Tan et al., 2020) that is fully-supervised. We directly transfer ViLD trained on LVIS to other detection datasets without finetuning, and obtain strong performance of $72.2 \mathrm{AP}_{50}$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. We also outperform the previous state-of-the-art open-vocabulary detector on COCO (Zareian et al., 2021) by 4.8 novel AP and 11.4 overall AP.</p>
<h1>2 Related Work</h1>
<p>Increasing vocabulary in visual recognition: Recognizing objects using a large vocabulary is a long-standing research problem in computer vision. One focus is zero-shot recognition, aiming at recognizing categories not present in the training set. Early works (Farhadi et al., 2009; Rohrbach et al., 2011; Jayaraman \&amp; Grauman, 2014) use visual attributes to create a binary codebook representing categories, which is used to transfer learned knowledge to unseen categories. In this direction, researchers have also explored class hierarchy, class similarity, and object parts as discriminative features to aid the knowledge transfer (Rohrbach et al., 2011; Akata et al., 2016; Zhao et al., 2017; Elhoseiny et al., 2017; Ji et al., 2018; Cacheux et al., 2019; Xie et al., 2020). Another focus is learning to align latent image-text embeddings, which allows to classify images using arbitrary texts. Frome et al. (2013) and Norouzi et al. (2014) are pioneering works that learn a visual-semantic embedding space using deep learning. Wang et al. (2018) distills information</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of using ViLD for open-vocabulary object detection. ViLD distills the knowledge from a pretrained open-vocabulary image classification model. First, the category text embeddings and the image embeddings of cropped object proposals are computed, using the text and image encoders in the pretrained classification model. Then, ViLD employs the text embeddings as the region classifier (ViLD-text) and minimizes the distance between the region embedding and the image embedding for each proposal (ViLD-image). During inference, text embeddings of novel categories are used to enable open-vocabulary detection.
from both word embeddings and knowledge graphs. Recent work CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) push the limit by collecting million-scale image-text pairs and then training joint image-text models using contrastive learning. These models can directly transfer to a suite of classification datasets and achieve impressive performances. While these work focus on image-level open-vocabulary recognition, we focus on detecting objects using arbitrary text inputs.</p>
<p>Increasing vocabulary in object detection: It's expensive to scale up the data collection for large vocabulary object detection. Zhao et al. (2020) and Zhou et al. (2021) unify the label space from multiple datasets. Joseph et al. (2021) incrementally learn identified unknown categories. Zero-shot detection (ZSD) offers another direction. Most ZSD methods align region features to pretrained text embeddings in base categories (Bansal et al., 2018; Demirel et al., 2018; Rahman et al., 2019; Hayat et al., 2020; Zheng et al., 2020). However, there is a large performance gap to supervised counterparts. To address this issue, Zareian et al. (2021) pretrain the backbone model using image captions and finetune the pretrained model with detection datasets. In contrast, we use an image-text pretrained model as a teacher model to supervise student object detectors. All previous methods are only evaluated on tens of categories, while we are the first to evaluate on more than 1,000 categories.</p>
<h1>3 Method</h1>
<p>Notations: We divide categories in a detection dataset into the base and novel subsets, and denote them by $C_{B}$ and $C_{N}$. Only annotations in $C_{B}$ are used for training. We use $\mathcal{T}(\cdot)$ to denote the text encoder and $\mathcal{V}(\cdot)$ to denote the image encoder in the pretrained open-vocabulary image classifier.</p>
<h3>3.1 LOCALIZATION FOR NOVEL CATEGORIES</h3>
<p>The first challenge for open-vocabulary detection is to localize novel objects. We modify a standard two-stage object detector, e.g., Mask R-CNN (He et al., 2017), for this purpose. We replace its classspecific localization modules, i.e., the second-stage bounding box regression and mask prediction layers, with class-agnostic modules for general object proposals. For each region of interest, these modules only predict a single bounding box and a single mask for all categories, instead of one prediction per category. The class-agnostic modules can generalize to novel objects.</p>
<h3>3.2 OPEN-VOCABULARY DETECTION WITH CROPPED REGIONS</h3>
<p>Once object candidates are localized, we propose to reuse a pretrained open-vocabulary image classifier to classify each region for detection.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Model architecture and training objectives. (a) The classification head of a vanilla two-stage detector, e.g., Mask R-CNN. (b) ViLD-text replaces the classifier with fixed text embeddings and a learnable background embedding. The projection layer is introduced to adjust the dimension of region embeddings to be compatible with the text embeddings. (c) ViLD-image distills from the precomputed image embeddings of proposals with an $\mathcal{L}_{1}$ loss. (d) ViLD combines ViLD-text and ViLD-image.</p>
<p>Image embeddings: We train a proposal network on base categories $C_{B}$ and extract the region proposals $\tilde{r} \in \widetilde{P}$ offline. We crop and resize the proposals, and feed them into the pretrained image encoder $\mathcal{V}$ to compute image embeddings $\mathcal{V}(\operatorname{crop}(I, \tilde{r}))$, where $I$ is the image.</p>
<p>We ensemble the image embeddings from $1 \times$ and $1.5 \times$ crops, as the $1.5 \times$ crop provides more context cues. The ensembled embedding is then renormalized to unit norm:</p>
<p>$$
\mathcal{V}\left(\operatorname{crop}\left(I, \tilde{r}<em 1="1" _times="\times">{{1 \times, 1.5 \times}}\right)\right)=\frac{\mathbf{v}}{|\mathbf{v}|}, \text { where } \mathbf{v}=\mathcal{V}\left(\operatorname{crop}\left(I, \tilde{r}</em>\right)\right)
$$}\right)\right)+\mathcal{V}\left(\operatorname{crop}\left(I, \tilde{r}_{1.5 \times</p>
<p>Text embeddings: We generate the text embeddings offline by feeding the category texts with prompt templates, e.g., "a photo of {category} in the scene", into the text encoder $\mathcal{T}$. We ensemble multiple prompt templates and the synonyms if provided.</p>
<p>Then, we compute cosine similarities between the image and text embeddings. A softmax activation is applied, followed by a per-class NMS to obtain final detections. The inference is slow since every cropped region is fed into $\mathcal{V}$.</p>
<h1>3.3 ViLD: Vision and Language Knowledge Distillation.</h1>
<p>We propose ViLD to address the slow inference speed of the above method. ViLD learns region embeddings in a two-stage detector to represent each proposal $r$. We denote region embeddings by $\mathcal{R}(\phi(I), r)$, where $\phi(\cdot)$ is a backbone model and $\mathcal{R}(\cdot)$ is a lightweight head that generates region embeddings. Specifically, we take outputs before the classification layer as region embeddings.</p>
<p>Replacing classifier with text embeddings: We first introduce ViLD-text. Our goal is to train the region embeddings such that they can be classified by text embeddings. Fig. 3(b) shows the architecture and training objective. ViLD-text replaces the learnable classifier in Fig. 3(a) with the text embeddings introduced in Sec. 3.2. Only $\mathcal{T}\left(C_{B}\right)$, the text embeddings of $C_{B}$, are used for training. For the proposals that do not match any groundtruth in $C_{B}$, they are assigned to the background category. Since the text "background" does not well represent these unmatched proposals, we allow the background category to learn its own embedding $\mathbf{e}<em B="B">{b g}$. We compute the cosine similarity between each region embedding $\mathcal{R}(\phi(I), r)$ and all category embeddings, including $\mathcal{T}\left(C</em>$. Then we apply softmax activation with a temperature $\tau$ to compute the cross entropy loss. To train the first-stage region proposal network of the two-stage detector, we extract region proposals $r \in P$ online, and train the detector with ViLD-text from scratch. The loss for ViLD-text can be written as:}\right)$ and $\mathbf{e}_{b g</p>
<p>$$
\begin{aligned}
&amp; \mathbf{e}<em r="r">{r}=\mathcal{R}(\phi(I), r) \
&amp; \mathbf{z}(r)=\left[\operatorname{sim}\left(\mathbf{e}</em>}, \mathbf{e<em r="r">{b g}\right), \operatorname{sim}\left(\mathbf{e}</em>}, \mathbf{t<em r="r">{1}\right), \cdots, \operatorname{sim}\left(\mathbf{e}</em>}, \mathbf{t<em B="B">{\left|C</em>\right)\right] \
&amp; \mathcal{L}}\right|<em P="P" _in="\in" r="r">{\text {ViLD-text }}=\frac{1}{N} \sum</em>} \mathcal{L<em r="r">{\mathrm{CE}}\left(\operatorname{softmax}(\mathbf{z}(r) / \tau), y</em>\right)
\end{aligned}
$$</p>
<p>where $\operatorname{sim}(\mathbf{a}, \mathbf{b})=\mathbf{a}^{\top} \mathbf{b} /\left(|\mathbf{a}||\mathbf{b}|\right)$, $\mathbf{t}<em B="B">{i}$ denotes elements in $\mathcal{T}\left(C</em>$ is the cross entropy loss.}\right), y_{r}$ denotes the class label of region $r, N$ is the number of proposals per image $\left(|P|\right)$, and $\mathcal{L}_{C E</p>
<p>During inference, we include novel categories $\left(C_{N}\right)$ and generate $\mathcal{T}\left(C_{B} \cup C_{N}\right)$ (sometimes $\mathcal{T}\left(C_{N}\right)$ only) for open-vocabulary detection (Fig. 2). Our hope is that the model learned from annotations in $C_{B}$ can generalize to novel categories $C_{N}$.</p>
<p>Distilling image embeddings: We then introduce ViLD-image, which aims to distill the knowledge from the teacher image encoder $\mathcal{V}$ into the student detector. Specifically, we align region embeddings $\mathcal{R}(\phi(I), \hat{r})$ to image embeddings $\mathcal{V}(\operatorname{crop}(I, \hat{r}))$ introduced in Sec. 3.2.</p>
<p>To make the training more efficient, we extract $M$ proposals $\hat{r} \in \hat{P}$ offline for each training image, and precompute the $M$ image embeddings. These proposals can contain objects in both $C_{B}$ and $C_{N}$, as the network can generalize. In contrast, ViLD-text can only learn from $C_{B}$. We apply an $\mathcal{L}_{1}$ loss between the region and image embeddings to minimize their distance. The ensembled image embeddings in Sec. 3.2 are used for distillation:</p>
<p>$$
\mathcal{L}<em _hat_r="\hat{r">{\text {ViLD-image }}=\frac{1}{M} \sum</em>} \in \hat{P}}\left|\mathcal{V}\left(\operatorname{crop}\left(I, \hat{r<em 1="1">{{1 \times, 1.5 \times}}\right)\right)-\mathcal{R}(\phi(I), \hat{r})\right|</em>
$$</p>
<p>Fig. 3(c) shows the architecture. Zhu et al. (2019) use a similar approach to make Faster R-CNN features mimic R-CNN features, however, the details and goals are different: They reduce redundant context to improve supervised detection; while ViLD-image is to enable open-vocabulary detection on novel categories.</p>
<p>The total training loss of ViLD is simply a weighted sum of both objectives:</p>
<p>$$
\mathcal{L}<em _ViLD-text="{ViLD-text" _text="\text">{\text {ViLD }}=\mathcal{L}</em>
$$}}+w \cdot \mathcal{L}_{\text {ViLD-image }</p>
<p>where $w$ is a hyperparameter weight for distilling the image embeddings. Fig. 3(d) shows the model architecture and training objectives. ViLD-image distillation only happens in training time. During inference, ViLD-image, ViLD-text and ViLD employ the same set of text embeddings as the detection classifier, and use the same architecture for open-vocabulary detection (Fig. 2).</p>
<h1>3.4 MODEL ENSEMBLING</h1>
<p>In this section, we explore model ensembling for the best detection performance over base and novel categories. First, we combine the predictions of a ViLD-text detector with the open-vocabulary image classification model. The intuition is that ViLD-image learns to approximate the predictions of its teacher model, and therefore, we assume using the teacher model directly may improve performance. We use a trained ViLD-text detector to obtain top $k$ candidate regions and their confidence scores. Let $p_{i, \text { ViLD-text }}$ denote the confidence score of proposal $\hat{r}$ belonging to category $i$. We then feed $\operatorname{crop}(I, \hat{r})$ to the open-vocabulary classification model to obtain the teacher's confidence score $p_{i, \text { cls }}$. Since we know the two models have different performance on base and novel categories, we introduce a weighted geometric average for the ensemble:</p>
<p>$$
p_{i, \text { ensemble }}=\left{\begin{array}{ll}
p_{i, \text { ViLD-text }}^{\lambda} \cdot p_{i, \text { cls }}^{(1-\lambda)}, &amp; \text { if } i \in C_{B} \
p_{i, \text { ViLD-text }}^{(1-\lambda)} \cdot p_{i, \text { cls }}^{\lambda}, &amp; \text { if } i \in C_{N}
\end{array}\right.
$$</p>
<p>$\lambda$ is set to $2 / 3$, which weighs the prediction of ViLD-text more on base categories and vice versa. Note this approach has a similar slow inference speed as the method in Sec. 3.2.</p>
<p>Next, we introduce a different ensembling approach to mitigate the above inference speed issue. Besides, in ViLD, the cross entropy loss of ViLD-text and the $\mathcal{L}<em ViLD-text="ViLD-text" _="{" _text="\text" i_="i,">{1}$ distillation loss of ViLD-image is applied to the same set of region embeddings, which may cause contentions. Here, instead, we learn two sets of embeddings for ViLD-text (Eq. 2) and ViLD-image (Eq. 3) respectively, with two separate heads of identical architectures. Text embeddings are applied to these two regions embeddings to obtain confidence scores $p</em>$. We name this approach ViLD-ensemble.}}$ and $p_{i, \text { ViLD-image }}$, which are then ensembled in the same way as Eq. 5, with $p_{i, \text { ViLD-image }}$ replacing $p_{i, \text { cls }</p>
<h1>4 EXPERIMENTS</h1>
<p>Implementation details: We benchmark on the Mask R-CNN (He et al., 2017) with ResNet (He et al., 2016) FPN (Lin et al., 2017) backbone and use the same settings for all models unless explicitly specified. The models use $1024 \times 1024$ as input image size, large-scale jittering augmentation of range $[0.1,2.0]$, synchronized batch normalization (Ioffe \&amp; Szegedy, 2015; Girshick et al., 2018) of batch size 256, weight decay of $4 \mathrm{e}-5$, and an initial learning rate of 0.32 . We train the model from scratch for 180,000 iterations, and divide the learning rate by 10 at $0.9 \times, 0.95 \times$, and $0.975 \times$ of total iterations. We use the publicly available pretrained CLIP model ${ }^{1}$ as the open-vocabulary classification model, with an input size of $224 \times 224$. The temperature $\tau$ is set to 0.01 , and the maximum number of detections per image is 300 . We refer the readers to Appendix D for more details.</p>
<h3>4.1 BENCHMARK SETTINGS</h3>
<p>We mainly evaluate on LVIS (Gupta et al., 2019) with our new setting. To compare with previous methods, we also use the setting in Zareian et al. (2021), which is adopted in many zero-shot detection works.</p>
<p>LVIS: We benchmark on LVIS v1. LVIS contains a large and diverse set of vocabulary (1,203 categories) that is more suitable for open-vocabulary detection. We take its 866 frequent and common categories as the base categories $C_{B}$, and hold out the 337 rare categories as the novel categories $C_{N} . \mathrm{AP}_{r}$, the AP of rare categories, is the main metric.</p>
<p>COCO: Bansal et al. (2018) divide COCO-2017 (Lin et al., 2014) into 48 base categories and 17 novel categories, removing 15 categories without a synset in the WordNet hierarchy. We follow previous works and do not compute instance masks. We evaluate on the generalized setting.</p>
<h3>4.2 LEARNING GENERALIZABLE OBJECT PROPOSALS</h3>
<p>We first study whether a detector can localize novel categories when only trained on base categories. We evaluate the region proposal networks in Mask R-CNN with a ResNet-50 backbone. Table 1 shows the average recall (AR) (Lin et al., 2014) on novel categories. Training with only base categories performs slightly worse by $\sim 2 \mathrm{AR}$ at 100,300 , and 1000 proposals, compared to using both base and novel categories. This experiment demonstrates that, without seeing novel categories during training, region proposal networks can generalize to novel categories, only suffering a small performance drop. We believe better proposal networks focusing on unseen category generalization should further improve the performance, and leave this for future research.</p>
<p>Table 1: Training with only base categories achieves comparable average recall (AR) for novel categories on LVIS. We compare RPN trained with base only vs. base+novel categories and report the bounding box AR.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Supervision</th>
<th style="text-align: center;">AR $_{r} @ 100$</th>
<th style="text-align: center;">AR $_{r} @ 300$</th>
<th style="text-align: center;">AR $_{r} @ 1000$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">base</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">55.6</td>
</tr>
<tr>
<td style="text-align: left;">base + novel</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">57.0</td>
</tr>
</tbody>
</table>
<h3>4.3 OPEN-VOCABULARY CLASSIFIER ON CROPPED REGIONS</h3>
<p>In Table 2, we evaluate the approach in Sec. 3.2, i.e., using an open-vocabulary classifier to classify cropped region proposals. We use CLIP in this experiment and find it tends to output confidence scores regardless of the localization quality (Appendix B). Given that, we ensemble the CLIP confidence score with a proposal objectness score by geometric mean. Results show it improves both base and novel APs. We compare with supervised baselines trained on base/base+novel categories, as well as Supervised-RFS (Mahajan et al., 2018; Gupta et al., 2019) that uses category frequency for balanced sampling. CLIP on cropped regions already outperforms supervised baselines on $\mathrm{AP}<em c="c">{r}$ by a large margin, without accessing detection annotations in novel categories. However, the performances of $\mathrm{AP}</em>$ are still trailing behind. This experiment shows that a strong openvocabulary classification model can be a powerful teacher model for detecting novel objects, yet there is still much improvement space for inference speed and overall AP.}$ and $\mathrm{AP}_{f</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Using CLIP for open-vocabulary detection achieves high detection performance on novel categories. We apply CLIP to classify cropped region proposals, with or without ensembling objectness scores, and report the mask average precision (AP). The performance on novel categories $\left(\mathrm{AP}_{r}\right)$ is far beyond supervised learning approaches. However, the overall performance is still behind.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">$\mathrm{AP}_{r}$</th>
<th style="text-align: right;">$\mathrm{AP}_{c}$</th>
<th style="text-align: right;">$\mathrm{AP}_{f}$</th>
<th style="text-align: right;">AP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Supervised (base class only)</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">22.6</td>
<td style="text-align: right;">32.4</td>
<td style="text-align: right;">22.5</td>
</tr>
<tr>
<td style="text-align: left;">CLIP on cropped regions w/o objectness</td>
<td style="text-align: right;">13.0</td>
<td style="text-align: right;">10.6</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">9.2</td>
</tr>
<tr>
<td style="text-align: left;">CLIP on cropped regions</td>
<td style="text-align: right;">$\mathbf{1 8 . 9}$</td>
<td style="text-align: right;">18.8</td>
<td style="text-align: right;">16.0</td>
<td style="text-align: right;">17.7</td>
</tr>
<tr>
<td style="text-align: left;">Supervised (base+novel)</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">23.5</td>
<td style="text-align: right;">33.2</td>
<td style="text-align: right;">23.9</td>
</tr>
<tr>
<td style="text-align: left;">Supervised-RFS (base+novel)</td>
<td style="text-align: right;">12.3</td>
<td style="text-align: right;">24.3</td>
<td style="text-align: right;">32.4</td>
<td style="text-align: right;">25.4</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of ViLD and its variants. ViLD outperforms the supervised counterpart on novel categories. Using ALIGN as the teacher model achieves the best performance without bells and whistles. All results are mask AP. We average over 3 runs for R50 experiments. ${ }^{\dagger}$ : methods with R-CNN style; runtime is $630 \times$ of Mask R-CNN style. ${ }^{\ddagger}$ : for reference, fully-supervised learning with additional tricks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Backbone</th>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">$\mathrm{AP}_{r}$</th>
<th style="text-align: right;">$\mathrm{AP}_{c}$</th>
<th style="text-align: right;">$\mathrm{AP}_{f}$</th>
<th style="text-align: right;">AP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ResNet-50+ViT-B/32</td>
<td style="text-align: left;">CLIP on cropped regions ${ }^{\dagger}$</td>
<td style="text-align: right;">18.9</td>
<td style="text-align: right;">18.8</td>
<td style="text-align: right;">16.0</td>
<td style="text-align: right;">17.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ViLD-text+CLIP ${ }^{\dagger}$</td>
<td style="text-align: right;">$\mathbf{2 2 . 6}$</td>
<td style="text-align: right;">24.8</td>
<td style="text-align: right;">29.2</td>
<td style="text-align: right;">26.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Supervised-RFS (base+novel)</td>
<td style="text-align: right;">12.3</td>
<td style="text-align: right;">24.3</td>
<td style="text-align: right;">32.4</td>
<td style="text-align: right;">25.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GloVe baseline</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">20.1</td>
<td style="text-align: right;">30.4</td>
<td style="text-align: right;">21.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ViLD-text</td>
<td style="text-align: right;">10.1</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: right;">32.5</td>
<td style="text-align: right;">24.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ViLD-image</td>
<td style="text-align: right;">11.2</td>
<td style="text-align: right;">11.3</td>
<td style="text-align: right;">11.1</td>
<td style="text-align: right;">11.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ViLD $(w=0.5)$</td>
<td style="text-align: right;">16.1</td>
<td style="text-align: right;">20.0</td>
<td style="text-align: right;">28.3</td>
<td style="text-align: right;">22.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ViLD-ensemble $(w=0.5)$</td>
<td style="text-align: right;">$\mathbf{1 6 . 6}$</td>
<td style="text-align: right;">24.6</td>
<td style="text-align: right;">30.3</td>
<td style="text-align: right;">25.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ViLD-ensemble w/ ViT-L/14 $(w=1.0)$</td>
<td style="text-align: right;">21.7</td>
<td style="text-align: right;">29.1</td>
<td style="text-align: right;">33.6</td>
<td style="text-align: right;">29.6</td>
</tr>
<tr>
<td style="text-align: left;">EfficientNet-b7</td>
<td style="text-align: left;">ViLD-ensemble w/ ALIGN $(w=1.0)$</td>
<td style="text-align: right;">$\mathbf{2 6 . 3}$</td>
<td style="text-align: right;">27.2</td>
<td style="text-align: right;">32.9</td>
<td style="text-align: right;">29.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">2020 Challenge winner (Tan et al., 2020)</td>
<td style="text-align: right;">30.0</td>
<td style="text-align: right;">41.9</td>
<td style="text-align: right;">46.0</td>
<td style="text-align: right;">41.5</td>
</tr>
</tbody>
</table>
<h1>4.4 VISION AND LANGUAGE KNOWLEDGE DISTILLATION</h1>
<p>We evaluate the performance of ViLD and its variants (ViLD-text, ViLD-image, and ViLDensemble), which are significantly faster compared to the method in Sec. 4.3. Finally, we use stronger teacher models to demonstrate our best performance. Table 3 summarizes the results.</p>
<p>Text embeddings as classifiers (ViLD-text): We evaluate ViLD-text using text embeddings generated by CLIP, and compare it with GloVe text embeddings (Pennington et al., 2014) pretrained on a large-scale text-only corpus. Table 3 shows ViLD-text achieves $10.1 \mathrm{AP}<em r="r">{r}$, which is significantly better than $3.0 \mathrm{AP}</em>}$ using GloVe. This demonstrates the importance of using text embeddings that are jointly trained with images. ViLD-text achieves much higher $\mathrm{AP<em f="f">{c}$ and $\mathrm{AP}</em>$ is worse, showing that using only 866 base categories in LVIS does not generalize as well as CLIP to novel categories.}$ compared to CLIP on cropped regions (Sec. 4.3), because ViLD-text uses annotations in $C_{B}$ to align region embeddings with text embeddings. The $\mathrm{AP}_{r</p>
<p>Distilling image embeddings (ViLD-image): We evaluate ViLD-image, which distills from the image embeddings of cropped region proposals, inferred by CLIP's image encoder, with a distillation weight of 1.0. Experiments show that ensembling with objectness scores doesn't help with other ViLD variants, so we only apply it to ViLD-image. Without training with any object category labels, ViLD-image achieves $11.2 \mathrm{AP}_{r}$ and 11.2 overall AP. This demonstrates that visual distillation works for open-vocabulary detection but the performance is not as good as CLIP on cropped regions.</p>
<p>Text+visual embeddings (ViLD): ViLD shows the benefits of combining distillation loss (ViLDimage) with classification loss using text embeddings (ViLD-text). We explore different hyperparameter settings in Appendix Table 7 and observe a consistent trade-off between $\mathrm{AP}<em c_="c," f="f">{r}$ and $\mathrm{AP}</em>}$, which suggests there is a competition between ViLD-text and ViLD-image. In Table 3, we compare ViLD with other methods. Its $\mathrm{AP<em r="r">{r}$ is 6.0 higher than ViLD-text and 4.9 higher than ViLD-image, indicating combining the two learning objectives boosts the performance on novel categories. ViLD outperforms Supervised-RFS by $3.8 \mathrm{AP}</em>$, showing our open-vocabulary detection approach is better than supervised models on rare categories.</p>
<p>Table 4: Performance on COCO dataset compared with existing methods. ViLD outperforms all the other methods in the table trained with various sources by a large margin, on both novel and base categories.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Training source</th>
<th>Novel AP</th>
<th>Base AP</th>
<th>Overall AP</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bilen \&amp; Vedaldi (2016)</td>
<td>image-level labels in $C_{B} \cup C_{N}$</td>
<td>19.7</td>
<td>19.6</td>
<td>19.6</td>
</tr>
<tr>
<td>Ye et al. (2019)</td>
<td></td>
<td>20.3</td>
<td>20.1</td>
<td>20.1</td>
</tr>
<tr>
<td>Bansal et al. (2018)</td>
<td>instance-level labels in $C_{B}$</td>
<td>0.31</td>
<td>29.2</td>
<td>24.9</td>
</tr>
<tr>
<td>Zhu et al. (2020)</td>
<td></td>
<td>3.41</td>
<td>13.8</td>
<td>13.0</td>
</tr>
<tr>
<td>Rahman et al. (2020)</td>
<td></td>
<td>4.12</td>
<td>35.9</td>
<td>27.9</td>
</tr>
<tr>
<td>Zareian et al. (2021)</td>
<td>image captions in $C_{B} \cup C_{N}$ instance-level labels in $C_{B}$</td>
<td>22.8</td>
<td>46.0</td>
<td>39.9</td>
</tr>
<tr>
<td>CLIP on cropped regions</td>
<td>image-text pairs from Internet (may contain $C_{B} \cup C_{N}$ ) instance-level labels in $C_{B}$</td>
<td>26.3</td>
<td>28.3</td>
<td>27.8</td>
</tr>
<tr>
<td>ViLD-text</td>
<td></td>
<td>5.9</td>
<td>61.8</td>
<td>47.2</td>
</tr>
<tr>
<td>ViLD-image</td>
<td></td>
<td>24.1</td>
<td>34.2</td>
<td>31.6</td>
</tr>
<tr>
<td>ViLD $(w=0.5)$</td>
<td></td>
<td>27.6</td>
<td>59.5</td>
<td>51.3</td>
</tr>
</tbody>
</table>
<p>Model ensembling: We study methods discussed in Sec. 3.4 to reconcile the conflict of joint training with ViLD-text and ViLD-image. We use two ensembling approaches: 1) ensembling ViLD-text with CLIP (ViLD-text+CLIP); 2) ensembling ViLD-text and ViLD-image using separate heads (ViLD-ensemble). As shown in Table 3 ViLD-ensemble improves performance over ViLD, mainly on $\mathrm{AP}<em r="r">{c}$ and $\mathrm{AP}</em>}$. This shows ensembling reduces the competition. ViLD-text+CLIP obtains much higher $\mathrm{AP<em c_="c," f="f">{r}$, outperforming ViLD by 6.5 , and maintains good $\mathrm{AP}</em>$. Note that it is slow and impractical for real world applications. This experiment is designed for showing the potential of using open-vocabulary classification models for open-vocabulary detection.</p>
<p>Stronger teacher model: We use CLIP ViT-L/14 and ALIGN (Jia et al., 2021) to explore the performance gain with a stronger teacher model (details in Appendix D). As shown in Table 3, both models achieve superior results compared with R50 ViLD w/ CLIP. The detector distilled from ALIGN is only trailing to the fully-supervised 2020 Challenge winner (Tan et al., 2020) by $3.7 \mathrm{AP}_{r}$, which employs two-stage training, self-training, and multi-scale testing etc. The results demonstrate ViLD scales well with the teacher model, and is a promising open-vocabulary detection approach.</p>
<h1>4.5 PERFORMANCE COMPARISON ON COCO DATASET</h1>
<p>Several related works in zero-shot detection and open-vocabulary detection are evaluated on COCO. To compare with them, we train and evaluate ViLD variants following the benchmark setup in Zareian et al. (2021) and report box AP with an IoU threshold of 0.5 . We use the ResNet-50 backbone, shorten the training schedule to 45,000 iterations, and keep other settings the same as our experiments on LVIS. Table 4 summarizes the results. ViLD outperforms Zareian et al. (2021) by 4.8 Novel AP and 13.5 Base AP. Different from Zareian et al. (2021), we do not have a pretraining phase tailored for detection. Instead, we use an off-the-shelf classification model. The performance of ViLD-text is low because only 48 base categories are available, which makes generalization to novel categories challenging. In contrast, ViLD-image and ViLD, which can distill image features of novel categories, outperform all existing methods (not apple-to-apple comparison though, given different methods use different settings).</p>
<h3>4.6 TRANSFER TO OTHER DATASETS</h3>
<p>Trained ViLD models can be transferred to other detection datasets, by simply switching the classifier to the category text embeddings of the new datasets. For simplicity, we keep the background embedding trained on LVIS. We evaluate the transferability of ViLD on PASCAL VOC (Everingham et al., 2010), COCO (Lin et al., 2014), and Objects365 (Shao et al., 2019). Since the three datasets have much smaller vocabularies, category overlap is unavoidable and images can be shared among datasets, e.g., COCO and LVIS. As shown in Table 5 ViLD achieves better transfer performance than ViLD-text. In PASCAL and COCO, the gap is large. This improvement should be credited to visual distillation, which better aligns region embeddings with the text classifier. We also compare with supervised learning and finetuning the classification layer. Although across datasets, ViLD has 3-6 AP gaps compared to the finetuning method and larger gaps compared to the supervised method, it is the first time we can directly transfer a trained detector to different datasets using language.</p>
<p>Table 5: Generalization ability of ViLD. We evaluate the LVIS-trained model with ResNet-50 backbone on PASCAL VOC 2007 test set, COCO validation set, and Objects365 v1 validation set. Simply replacing the text embeddings, our approaches are able to transfer to various detection datasets. The supervised baselines of COCO and Objects365 are trained from scratch. ${}^{\dagger}$ : the supervised baseline of PASCAL VOC is initialized with an ImageNet-pretrained checkpoint. All results are box APs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">PASCAL VOC ${ }^{\dagger}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">COCO</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Objects365</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{AP}_{50}$</td>
<td style="text-align: center;">$\mathrm{AP}_{75}$</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">$\mathrm{AP}_{50}$</td>
<td style="text-align: center;">$\mathrm{AP}_{75}$</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">$\mathrm{AP}_{50}$</td>
<td style="text-align: center;">$\mathrm{AP}_{75}$</td>
</tr>
<tr>
<td style="text-align: left;">ViLD-text</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">ViLD</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">12.6</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">16.2</td>
</tr>
<tr>
<td style="text-align: left;">Supervised</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">28.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative results on LVIS, COCO, and Objects365. First row: ViLD is able to correctly localize and recognize objects in novel categories. For clarity, we only show the detected novel objects. Second row: The detected objects on base+novel categories. The performance on base categories is not degraded with ViLD. Last two rows: ViLD can directly transfer to COCO and Objects365 without further finetuning.</p>
<h1>4.7 QUALITATIVE RESULTS</h1>
<p>Qualitative examples: In Fig. 4, we visualize ViLD's detection results. It illustrates ViLD is able to detect objects of both novel and base categories, with high-quality mask predictions on novel objects, e.g., it well separates banana slices from the crepes (novel category). We also show qualitative results on COCO and Objects365, and find ViLD generalizes well. We show more qualitative results, e.g., interactive detection and systematic expansion, in Appendix A.</p>
<p>On-the-fly interactive object detection: We tap the potential of ViLD by using arbitrary text to interactively recognize fine-grained categories and attributes. We extract the region embedding and compute its cosine similarity with a small set of on-the-fly arbitrary texts describing attributes and/or fine-grained categories; we apply softmax with temperature $\tau$ on top of the similarities. To our surprise, though never trained on fine-grained dog breeds (Fig. 5), it correctly distinguishes husky from shiba inu. It also works well on identifying object colors (Fig. 1). The results demonstrate knowledge distillation from an open-vocabulary image classification model helps ViLD to gain understanding of concepts not present in the detection training. Of course, ViLD does not work all the time, e.g., it fails to recognize poses of animals.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Fine-grained breeds and colors.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Colors of body parts.</p>
<p>Figure 5: On-the-fly interactive object detection. One application of ViLD is using on-the-fly arbitrary texts to further recognize more details of the detected objects, e.g., fine-grained categories and color attributes.</p>
<p>Systematic expansion of dataset vocabulary: In addition, we propose to systematically expand the dataset vocabulary ( $\mathbf{v}=\left{v_{1}, \ldots, v_{p}\right}$ ) with a set of attributes $\left(\mathbf{a}=\left{a_{1}, \ldots, a_{q}\right}\right)$ as follows:</p>
<p>$$
\begin{aligned}
\operatorname{Pr}\left(v_{i}, a_{j} \mid \mathbf{e}<em i="i">{r}\right) &amp; =\operatorname{Pr}\left(v</em>} \mid \mathbf{e<em j="j">{r}\right) \cdot \operatorname{Pr}\left(a</em>} \mid v_{i}, \mathbf{e<em i="i">{r}\right) \
&amp; =\operatorname{Pr}\left(v</em>} \mid \mathbf{e<em j="j">{r}\right) \cdot \operatorname{Pr}\left(a</em>\right)
\end{aligned}
$$} \mid \mathbf{e}_{r</p>
<p>where $\mathbf{e}<em i="i">{r}$ denotes the region embedding. We assume $v</em>} \Perp a_{j} \mid \mathbf{e<em r="r">{r}$, i.e., given $\mathbf{e}</em>$.}$ the event the object belongs to category $v_{i}$ is conditionally independent to the event it has attribute $a_{j</p>
<p>Let $\tau$ denote the temperature used for softmax and $\mathcal{T}$ denote the text encoder as in Eq. 2. Then</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Pr}\left(v_{i} \mid \mathbf{e}<em i="i">{r}\right)=\operatorname{softmax}</em>}\left(\operatorname{sim}\left(\mathbf{e<em j="j">{r}, \mathcal{T}(\mathbf{v})\right) / \tau\right) \
&amp; \operatorname{Pr}\left(a</em>} \mid \mathbf{e<em j="j">{r}\right)=\operatorname{softmax}</em>)\right) / \tau\right)
\end{aligned}
$$}\left(\operatorname{sim}\left(\mathbf{e}_{r}, \mathcal{T}(\mathbf{a</p>
<p>In this way, we are able to expand $p$ vocabularies into a new set of $p \times q$ vocabularies with attributes. The conditional probability approach is similar to YOLO9000 (Redmon \&amp; Farhadi, 2017). We show a qualitative example of this approach in Fig. 6, where we use a color attribute set as a. Our open-vocabulary detector successfully detects fruits with color attributes.</p>
<p>We further expand the detection vocabulary to fine-grained bird categories by using all 200 species from CUB-200-2011 (Wah et al., 2011). Fig. 7 shows successful and failure examples of our openvocabulary fine-grained detection on CUB-200-2011 images. In general, our model is able to detect visually distinctive species, but fails at other ones.</p>
<h1>5 CONCLUSION</h1>
<p>We present ViLD, an open-vocabulary object detection method by distilling knowledge from openvocabulary image classification models. ViLD is the first open-vocabulary detection method evaluated on the challenging LVIS dataset. It attains 16.1 AP for novel cateogires on LVIS with a ResNet50 backbone, which surpasses its supervised counterpart at the same inference speed. With a stronger teacher model (ALIGN), the performance can be further improved to 26.3 novel AP. We demonstrate that the detector learned from LVIS can be directly transferred to 3 other detection datasets. We hope that the simple design and strong performance make ViLD a scalable alternative approach for detecting long-tailed categories, instead of collecting expensive detection annotations.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Using LVIS vocabulary.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) After expanding vocabulary with color attributes.</p>
<p>Figure 6: Systematic expansion of dataset vocabulary with colors. We add 11 color attributes (red orange, dark orange, light orange, yellow, green, cyan, blue, purple, black, brown, white) to LVIS categories, which expand the vocabulary size by $11 \times$. Above we show an example of detection results. Our open-vocabulary detector is able to assign the correct color to each fruit. A class-agnostic NMS with threshold 0.9 is applied. Each figure shows top 15 predictions.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: Systematic expansion of dataset vocabulary with fine-grained categories. We use the systematic expansion method to detect 200 fine-grained bird species in CUB-200-2011. (a): Our open-vocabulary detector is able to perform fine-grained detection (bottom) using the detector trained on LVIS (top). (b): It fails at recognizing visually non-distinctive species. It incorrectly assigns "Western Gull" to "Horned Puffin" due to visual similarity.</p>
<h1>ETHICS STATEMENT</h1>
<p>Our paper studies open-vocabulary object detection, a sub-field in computer vision. Our method is based on knowledge distillation, a machine learning technique that has been extensively used in computer vision, natural language processing, etc. All of our experiments were conducted on public datasets with pretrained models that are either publicly available or introduced in published papers. The method proposed in our paper is a principled method for open-vocabulary object detection that can be used in a wide range of applications. Therefore, the ethical impact of our work would primarily depends on the specific applications. We foresee positive impacts if our method is applied to object detection problems where the data collection is difficult to scale, such as detecting rare objects for self-driving cars. But the method can also be applied to other sensitive applications that could raise ethical concerns, such as video surveillance systems.</p>
<h2>REPRODUCIBILITY STATEMENT</h2>
<p>We provide detailed descriptions of the proposed method in Sec. 3. Details about experiment settings, hyper-parameters and implementations are presented in Sec. 4, Appendix C and Appendix D. We release our code and pretrained models at https://github.com/tensorflow/tpu/ tree/master/models/official/detection/projects/vild to facilitate the reproducibility of our work.</p>
<h2>REFERENCES</h2>
<p>Zeynep Akata, Mateusz Malinowski, Mario Fritz, and Bernt Schiele. Multi-cue zero-shot learning with strong supervision. In CVPR, 2016.</p>
<p>Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran. Zero-shot object detection. In ECCV, 2018.</p>
<p>Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In CVPR, 2016.
Yannick Le Cacheux, Herve Le Borgne, and Michel Crucianu. Modeling inter and intra-class relations in the triplet loss for zero-shot learning. In ICCV, 2019.</p>
<p>Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis. Zero-shot object detection by hybrid region embedding. In BMVC, 2018.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2020.</p>
<p>Mohamed Elhoseiny, Yizhe Zhu, Han Zhang, and Ahmed Elgammal. Link the head to the "beak": Zero shot learning from noisy text description at part precision. In CVPR, 2017.</p>
<p>Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010.</p>
<p>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. Describing objects by their attributes. In CVPR, 2009.</p>
<p>Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In NeurIPS, 2013.</p>
<p>Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.</p>
<p>Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. Detectron. https://github.com/facebookresearch/detectron, 2018.</p>
<p>Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In CVPR, 2019.</p>
<p>Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection. In $A C C V, 2020$.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.</p>
<p>Dinesh Jayaraman and Kristen Grauman. Zero shot recognition with unreliable attributes. NeurIPS 2014, 2014.</p>
<p>Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei Mark Zhang, et al. Stacked semanticsguided attention model for fine-grained zero-shot learning. In NeurIPS, 2018.</p>
<p>Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. ICML, 2021.</p>
<p>KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open world object detection. In CVPR, 2021.</p>
<p>Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.</p>
<p>Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017.</p>
<p>Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, 2018.</p>
<p>Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of semantic embeddings. ICLR, 2014.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In EMNLP, 2014.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. ICML, 2021.</p>
<p>Shafin Rahman, Salman Khan, and Fatih Porikli. Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts. In ACCV, 2018.</p>
<p>Shafin Rahman, Salman Khan, and Nick Barnes. Transductive learning for zero-shot object detection. In ICCV, 2019.</p>
<p>Shafin Rahman, Salman Khan, and Nick Barnes. Improved visual-semantic alignment for zero-shot object detection. In AAAI, 2020.</p>
<p>Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In CVPR, 2017.</p>
<p>Shaoqing Ren, Kaiming He, Ross B Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.</p>
<p>Marcus Rohrbach, Michael Stark, and Bernt Schiele. Evaluating knowledge transfer and zero-shot learning in a large-scale setting. In CVPR, 2011.</p>
<p>Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In ICCV, 2019.</p>
<p>Jingru Tan, Gang Zhang, Hanming Deng, Changbao Wang, Lewei Lu, quanquan Li, and Jifeng Dai. Technical report: A good box is not a guarantee of a good mask. Joint COCO and LVIS workshop at ECCV 2020: LVIS Challenge Track, 2020.</p>
<p>Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.</p>
<p>Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.</p>
<p>Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In CVPR, 2018.</p>
<p>Guo-Sen Xie, Li Liu, Fan Zhu, Fang Zhao, Zheng Zhang, Yazhou Yao, Jie Qin, and Ling Shao. Region graph embedding network for zero-shot learning. In ECCV, 2020.</p>
<p>Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Danfeng Qin, and Jesse Berent. Cap2det: Learning to amplify weak caption supervision for object detection. In ICCV, 2019.</p>
<p>Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In CVPR, 2021.</p>
<p>Hang Zhao, Xavier Puig, Bolei Zhou, Sanja Fidler, and Antonio Torralba. Open vocabulary scene parsing. In ICCV, 2017.</p>
<p>Xiangyun Zhao, Samuel Schulter, Gaurav Sharma, Yi-Hsuan Tsai, Manmohan Chandraker, and Ying Wu. Object detection with a unified label space from multiple datasets. In ECCV, 2020.</p>
<p>Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, and Li Cui. Background learnable cascade for zero-shot object detection. In ACCV, 2020.</p>
<p>Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Simple multi-dataset detection. arXiv preprint arXiv:2102.13086, 2021.</p>
<p>Pengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama. Don’t even look once: Synthesizing features for zero-shot detection. In CVPR, 2020.</p>
<p>Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In CVPR, 2019.</p>
<h1>APPENDIX</h1>
<h2>A ADDITIONAL QUALITATIVE RESULTS</h2>
<p>Transfer to PASCAL VOC: In Fig. 8, we show qualitative results of transferring an openvocabulary detector trained on LVIS (Gupta et al., 2019) to PASCAL VOC Detection (2007 test set) (Everingham et al., 2010), without finetuning (Sec. 4.6 in the main paper). Results demonstrate that the transferring works well.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: Transfer to PASCAL VOC. ViLD correctly detects objects when transferred to PASCAL VOC, where images usually have lower resolution than LVIS (our training set). In the third picture, our detector is able to find tiny bottles, though it fails to detect the person.</p>
<p>Failure cases: In Fig. 9, we show two failure cases of ViLD. The most common failure cases are the missed detection. A less common mistake is misclassifying the object category.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9: Failure cases on LVIS novel categories. The red bounding boxes indicate the groundtruths of the failed detections. (a) A common failure type where the novel objects are missing, e.g., the elevator car is not detected. (b) A less common failure where (part of) the novel objects are misclassified, e.g., half of the waffle iron is detected as a calculator due to visual similarity.</p>
<p>We show a failure case of mask prediction on PASCAL VOC in Fig. 10. It seems that the mask prediction is sometimes based on low-level appearance rather than semantics.</p>
<h2>B ANALYSIS OF CLIP ON CROPPED REGIONS</h2>
<p>In this section, we analyze some common failure cases of CLIP on cropped regions and discuss possible ways to mitigate these problems.</p>
<p>Visual similarity: This confusion is common for any classifiers and detectors, especially on large vocabularies. In Fig. 11(a), we show two failure examples due to visual similarity. Since we only use a relatively small ViT-B/32 CLIP model, potentially we can improve the performance with a higher-capacity pretrained model. In Table 6, when replacing this CLIP model with an EfficientNet12 ALIGN model, we see an increase on AP.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 10: An example of ViLD on PASCAL VOC showing a mask of poor quality. The class-agnostic mask prediction head occasionally predicts masks based on low-level appearance rather than semantics, and thus fails to obtain a complete instance mask.</p>
<p>Table 6: ALIGN on cropped regions achieves superior $\mathbf{A P}_{r}$, and overall very good performance. It shows a stronger open-vocabulary classification model can improve detection performance by a large margin. We report box APs here.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">$\mathbf{A P}_{r}$</th>
<th style="text-align: center;">$\mathrm{AP}_{c}$</th>
<th style="text-align: center;">$\mathrm{AP}_{f}$</th>
<th style="text-align: center;">AP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CLIP on cropped regions</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">18.6</td>
</tr>
<tr>
<td style="text-align: left;">ALIGN on cropped regions</td>
<td style="text-align: center;">$\mathbf{3 9 . 6}$</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">31.4</td>
</tr>
</tbody>
</table>
<p>Aspect ratio: This issue is introduced by the pre-processing of inputs in CLIP. We use the ViTB/32 CLIP with a fixed input resolution of $224 \times 224$. It resizes the shorter edge of the image to 224 , and then uses a center crop. However, since region proposals can have more extreme aspect ratios than the training images for CLIP, and some proposals are tiny, we directly resize the proposals to that resolution, which might cause some issues. For example, the thin structure in Fig. 11(b) right will be highly distorted with the pre-processing. And the oven and fridge can be confusing with the distorted aspect ratio. There might be some simple remedies for this, e.g., pasting the cropped region with original aspect ratio on a black background. We tried this simple approach with both CLIP and ALIGN. Preliminary results show that it works well on the fully convolutional ALIGN, while doesn't work well on the transformer-based CLIP, probably because CLIP is never trained with black image patches.</p>
<p>Multiple objects in a bounding box: Multiple objects in a region interfere CLIP's classification results, see Fig. 11(c), where a corner of an aquarium dominates the prediction. This is due to CLIP pretraining, which pairs an entire image with its caption. The caption is usually about salient objects in the image. It's hard to mitigate this issue at the open-vocabulary classification model's end. On the other hand, a supervised detector are trained to recognize the object tightly surrounded by the bounding box. So when distilling knowledge from an open-vocabulary image classification model, keeping training a supervised detector on base categories could help, as can be seen from the improvement of ViLD over ViLD-image (Sec. 4.4).</p>
<p>Confidence scores predicted by CLIP do not reflect the localization quality: For example, in Fig. 12(a), CLIP correctly classifies the object, but gives highest scores to partial detection boxes. CLIP is not trained to measure the quality of bounding boxes. Nonetheless, in object detection, it is important for the higher-quality boxes to have higher scores. In Fig. 12(c), we simply re-score by taking the geometric mean of the CLIP confidence score and the objectness score from the proposal model, which yields much better top predictions. In Fig. 12(b), we show top predictions of the Mask R-CNN model. Its top predictions have good bounding boxes, while the predicted categories are wrong. This experiment shows that it's important to have both an open-vocabulary classification model for better recognition, as well as supervision from detection dataset for better localization.</p>
<h1>C ADDITIONAL QUANTITATIVE RESULTS</h1>
<p>Hyperparameter sweep for visual distillation: Table 7 shows the parameter sweep of different distillation weights using $\mathcal{L}<em 2="2">{1}$ and $\mathcal{L}</em>$ loss}$ losses. Compared with no distillation, additionally learning from image embeddings generally yields better performance on novel categories. We find $\mathcal{L}_{1</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 11: Typical errors of CLIP on cropped regions. (a): The prediction and the groundtruth have high visual similarity. (b): Directly resizing the cropped regions changes the aspect ratios, which may cause troubles. (c): CLIP's predictions are sometimes affected by other objects appearing in the region, rather than predicting what the entire bounding box is.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 12: The prediction scores of CLIP do not reflect the quality of bounding box localization. (a): Top predictions of CLIP on cropped region. Boxes of poor qualities receive high scores, though the classification is correct. (b): Top predictions of a vanilla Mask R-CNN model. Box qualities are good while the classification is wrong. (c): We take the geometric mean of CLIP classification score and objectiveness score, and use it to rescore (a). In this way, a high-quality box as well as the correct category rank first.
can better improve the $\mathrm{AP}<em c="c">{r}$ performance with the trade-off against $\mathrm{AP}</em>$. This suggests there is a competition between ViLD-text and ViLD-image.}$ and $\mathrm{AP}_{f</p>
<p>Table 7: Hyperparameter sweep for visual distillation in ViLD. $\mathcal{L}<em 2="2">{1}$ loss is better than $\mathcal{L}</em>}$ loss. For $\mathcal{L<em r="r">{1}$ loss, there is a trend that $\mathrm{AP}</em>}$ increases as the weight increases, while $\mathrm{AP<em r="r">{f, c}$ decrease. For all parameter combinations, ViLD outperforms ViLD-text on $\mathrm{AP}</em>$. We use ResNet-50 backbone and shorter training iterations ( 84,375 iters), and report mask AP in this table.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Distill loss</th>
<th style="text-align: center;">Distill weight $w$</th>
<th style="text-align: center;">$\mathrm{AP}_{r}$</th>
<th style="text-align: center;">$\mathrm{AP}_{c}$</th>
<th style="text-align: center;">$\mathrm{AP}_{f}$</th>
<th style="text-align: center;">AP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No distill</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$\mathbf{1 3 . 7}$</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{L}_{2}$ loss</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">24.4</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{L}_{1}$ loss</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">23.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">21.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">$\mathbf{1 7 . 3}$</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">20.7</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance of ViLD variants. This table shows additional box APs for models in Table 3 and ResNet-152 results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Box</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mask</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{AP}_{r}$</td>
<td style="text-align: center;">$\mathrm{AP}_{c}$</td>
<td style="text-align: center;">$\mathrm{AP}_{f}$</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">$\mathrm{AP}_{r}$</td>
<td style="text-align: center;">$\mathrm{AP}_{c}$</td>
<td style="text-align: center;">$\mathrm{AP}_{f}$</td>
<td style="text-align: center;">AP</td>
</tr>
<tr>
<td style="text-align: center;">ResNet-50</td>
<td style="text-align: center;">CLIP on cropped regions</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">17.7</td>
</tr>
<tr>
<td style="text-align: center;">+ViT-B/32</td>
<td style="text-align: center;">ViLD-text+CLIP</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: center;">ResNet-50</td>
<td style="text-align: center;">Supervised-RFS (base+novel)</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">25.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GloVe baseline</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">21.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD-text</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">24.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD-image</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD ( $w=0.5)$</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">22.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD-ensemble ( $w=0.5)$</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">25.5</td>
</tr>
<tr>
<td style="text-align: center;">ResNet-152</td>
<td style="text-align: center;">Supervised-RFS (base+novel)</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">27.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD-text</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">26.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD-image</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">13.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD ( $w=1.0)$</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">23.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD-ensemble ( $w=2.0)$</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: center;">EfficientNet-b7</td>
<td style="text-align: center;">ViLD-ensemble w/ ViT-L/14 ( $w=1.0)$</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">29.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ViLD-ensemble w/ ALIGN ( $w=1.0)$</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">29.3</td>
</tr>
</tbody>
</table>
<p>Box APs and ResNet-152 backbone: Table 8 shows the corresponding box AP of Table 3 in the main paper. In general, box AP is slightly higher than mask AP. In addition, we include the results of ViLD variants with the ResNet-152 backbone. The deeper backbone improves all metrics. The trend/relative performance is consistent for box and mask APs, as well as for different backbones. ViLD-ensemble achieves the best box and mask $\mathrm{AP}_{r}$.</p>
<p>Ablation study on prompt engineering: We conduct an ablation study on prompt engineering. We compare the text embeddings ensembled over synonyms and 63 prompt templates (listed in Appendix D) with a non-ensembled version: Using the single prompt template "a photo of {article} {category}". Table 9 illustrates that ensembling multiple prompts slightly improves the performance by $0.4 \mathrm{AP}_{r}$.</p>
<p>Table 9: Ablation study on prompt engineering. Results indicate ensembling multiple prompt templates slightly improves $\mathrm{AP}_{r}$. ViLD w/ multiple prompts is the same ViLD model in Table 3, and ViLD w/ single prompt only changes the text embeddings used as the classifier.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">$\mathrm{AP}_{r}$</th>
<th style="text-align: center;">$\mathrm{AP}_{c}$</th>
<th style="text-align: center;">$\mathrm{AP}_{f}$</th>
<th style="text-align: center;">AP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ViLD w/ single prompt</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">22.6</td>
</tr>
<tr>
<td style="text-align: left;">ViLD w/ multiple prompts</td>
<td style="text-align: center;">$\mathbf{1 6 . 1}$</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">22.5</td>
</tr>
</tbody>
</table>
<h1>D MORE IMPLEMENTATION DETAILS</h1>
<p>ViLD-ensemble architecture: In Fig. 13, we show the detailed architecture and learning objectives for ViLD-ensemble, the ensembling technique introduced in Sec. 3.4.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 13: Model architecture and training objectives for ViLD-ensemble. The learning objectives are similar to ViLD. Different from ViLD, we use two separate heads of identical architecture in order to reduce the competition between ViLD-text and ViLD-image objetvies. During inference, the results from the two heads are ensembled as described in Sec. 3.4. Please refer to Fig. 3 for comparison with other ViLD variants.</p>
<p>Model used for qualitative results: For all qualitative results, we use a ViLD model with ResNet152 backbone, whose performance is shown in Table 8.</p>
<p>Details for supervised baselines: For a fair comparison, we train the second stage box/mask prediction heads of Supervised and Supervised-RFS baselines in the class-agnostic manner introduced in Sec. 3.1.</p>
<p>Details for R-CNN style experiments: We provide more details here for the R-CNN style experiments: CLIP on cropped regions in Sec. 4.2 and ViLD-text+CLIP in Sec. 4.3. 1) Generalized object proposal: We use the standard Mask R-CNN R50-FPN model. To report mask AP and compare with other methods, we treat the second-stage refined boxes as proposals and use the corresponding masks.</p>
<p>We apply a class-agnostic NMS with 0.9 threshold, and output a maximum of 1000 proposals. The objectness score is one minus the background score. 2) Open-vocabulary classification on cropped regions: After obtaining CLIP confidence scores for the 1000 proposals, we apply a class-specific NMS with a threshold of 0.6 , and output the top 300 detections as the final results.</p>
<p>Additional details for ViLD variants: Different from the R-CNN style experiments, for all ViLD variants (Sec. 3.3, Sec. 3.4), we use the standard two-stage Mask R-CNN with the class-agnostic localization modules introduced in Sec. 3.1. Both the $M$ offline proposals and $N$ online proposals are obtained from the first-stage RPN (Ren et al., 2015). In general, the R-CNN style methods and ViLD variants share the same concept of class-agnostic object proposals. We use the second-stage outputs in R-CNN style experiments only because we want to obtain the Mask AP, the main metric, to compare with other methods. For ViLD variants, we remove the unnecessary complexities and show that using a simple one-stage RPN works well.</p>
<p>Architecture for open-vocabulary image classification models: Popular open-vocabulary image classification models (Radford et al., 2021; Jia et al., 2021) perform contrastive pre-training on a large number of image-text pairs. Given a batch of paired images and texts, the model learns to maximize the cosine similarity between the embeddings of the corresponding image and text pairs, while minimizing the cosine similarity between other pairs. Specifically, for CLIP (Radford et al., 2021), we use the version where the image encoder adopts the Vision Transformer (Dosovitskiy et al., 2020) architecture and the text encoder is a Transformer (Vaswani et al., 2017). For ALIGN (Jia et al., 2021), its image encoder is an EfficientNet (Tan \&amp; Le, 2019) and its text encoder is a BERT (Devlin et al., 2019).</p>
<p>Details for ViLD with stronger teacher models: In both experiments with CLIP ViT-L/14 and ALIGN, we use EfficientNet-b7 as the backbone and ViLD-ensemble for better performance. We</p>
<p>also crop the RoI features from only FPN level $P_{3}$ in the feature pyramid. The large-scale jittering range is reduced to [0.5, 2.0]. For CLIP ViT-L/14, since its image/text embeddings have 768 dimensions, we increase the FC dimension of the Faster R-CNN heads to 1,024, and the FPN dimension to 512. For ViLD w/ ALIGN, we use the ALIGN model with an EfficientNet-l2 image encoder and a BERT-large text encoder as the teacher model. We modify several places in the Mask RCNN architecture to better distill the knowledge from the teacher. We equip the ViLD-image head in ViLD-ensemble with the MBConvBlocks in EfficientNet. Since the MBConvBlocks are fullyconvolutional, we apply a global average pooling to obtain the image embeddings, following the teacher. The ViLD-text head keeps the same Faster R-CNN head architecture as in Mask R-CNN. Since ALIGN image/text embeddings have 1,376 dimensions ( $2.7 \times$ CLIP embedding dimension), we increase the number of units in the fully connected layers of the ViLD-text head to 2,048, and the FPN dimension to 1,024 .</p>
<p>Text prompts: Since the open-vocabulary classification model is trained on full sentences, we feed the category names into a prompt template first, and use an ensemble of various prompts. Following Radford et al. (2021), we curate a list of 63 prompt templates. We specially include several prompts containing the phrase "in the scene" to better suit object detection, e.g., "There is {article} {category} in the scene".</p>
<p>Our list of prompt templates is shown below:</p>
<div class="codehilite"><pre><span></span><code>&#39;There is {article} {category} in the scene.&#39;
&#39;There is the {category} in the scene.&#39;
&#39;a photo of {article} {category} in the scene.&#39;
&#39;a photo of the {category} in the scene.&#39;
&#39;a photo of one {category} in the scene.&#39;
&#39;itap of {article} {category}.&#39;
&#39;itap of my {category}.&#39;
&#39;itap of the {category}.&#39;
&#39;a photo of {article} {category}.&#39;
&#39;a photo of my {category}.&#39;
&#39;a photo of the {category}.&#39;
&#39;a photo of one {category}.&#39;
&#39;a photo of many {category}.&#39;
&#39;a good photo of {article} {category}.&#39;
&#39;a good photo of the {category}.&#39;
&#39;a bad photo of {article} {category}.&#39;
&#39;a bad photo of the {category}.&#39;
&#39;a photo of a nice {category}.&#39;
&#39;a photo of the nice {category}.&#39;
&#39;a photo of a cool {category}.&#39;
&#39;a photo of the cool {category}.&#39;
&#39;a photo of a weird {category}.&#39;
&#39;a photo of the weird {category}.&#39;
&#39;a photo of a small {category}.&#39;
&#39;a photo of the small {category}.&#39;
&#39;a photo of a large {category}.&#39;
&#39;a photo of the large {category}.&#39;
&#39;a photo of a clean {category}.&#39;
&#39;a photo of the clean {category}.&#39;
&#39;a photo of a dirty {category}.&#39;
&#39;a photo of the dirty {category}.&#39;
&#39;a bright photo of {article} {category}.&#39;
&#39;a bright photo of the {category}.&#39;
&#39;a dark photo of {article} {category}.&#39;
&#39;a dark photo of the {category}.&#39;
&#39;a photo of a hard to see {category}.&#39;
&#39;a photo of the hard to see {category}.&#39;
&#39;a low resolution photo of {article} {category}.&#39;
&#39;a low resolution photo of the {category}.&#39;
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&#39;a cropped photo of {article} {category}.&#39;
&#39;a cropped photo of the {category}.&#39;
&#39;a close-up photo of {article} {category}.&#39;
&#39;a close-up photo of the {category}.&#39;
&#39;a jpeg corrupted photo of {article} {category}.&#39;
&#39;a jpeg corrupted photo of the {category}.&#39;
&#39;a blurry photo of {article} {category}.&#39;
&#39;a blurry photo of the {category}.&#39;
&#39;a pixelated photo of {article} {category}.&#39;
&#39;a pixelated photo of the {category}.&#39;
&#39;a black and white photo of the {category}.&#39;
&#39;a black and white photo of {article} {category}.&#39;
&#39;a plastic {category}.&#39;
&#39;the plastic {category}.&#39;
&#39;a toy {category}.&#39;
&#39;the toy {category}.&#39;
&#39;a plushie {category}.&#39;
&#39;the plushie {category}.&#39;
&#39;a cartoon {category}.&#39;
&#39;the cartoon {category}.&#39;
&#39;an embroidered {category}.&#39;
&#39;the embroidered {category}.&#39;
&#39;a painting of the {category}.&#39;
&#39;a painting of a {category}.&#39;
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/openai/CLIP, ViT-B/32.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
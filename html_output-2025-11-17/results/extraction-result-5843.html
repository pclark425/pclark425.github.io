<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5843 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5843</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5843</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-268857142</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.01268v1.pdf" target="_blank">Mapping the Increasing Use of LLMs in Scientific Papers</a></p>
                <p><strong>Paper Abstract:</strong> Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5843",
    "paper_id": "paper-268857142",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0043485,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mapping the Increasing Use of LLMs in Scientific Papers
1 Apr 2024</p>
<p>Weixin Liang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#120;&#108;&#105;&#97;&#110;&#103;&#64;&#115;&#116;&#97;&#110;&#102;&#111;&#114;&#100;&#46;&#101;&#100;&#117;">&#119;&#120;&#108;&#105;&#97;&#110;&#103;&#64;&#115;&#116;&#97;&#110;&#102;&#111;&#114;&#100;&#46;&#101;&#100;&#117;</a> 
Yaohui Zhang 
Zhengxuan Wu 
Haley Lepp 
Wenlong Ji 
Xuandong Zhao 
Hancheng Cao 
Sheng Liu 
Siyu He 
Zhi Huang 
Diyi Yang 
Christopher Potts 
Christopher D Manning 
James Y Zou <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#97;&#109;&#101;&#115;&#122;&#64;&#115;&#116;&#97;&#110;&#102;&#111;&#114;&#100;&#46;&#101;&#100;&#117;">&#106;&#97;&#109;&#101;&#115;&#122;&#64;&#115;&#116;&#97;&#110;&#102;&#111;&#114;&#100;&#46;&#101;&#100;&#117;</a> </p>
<p>Stanford University</p>
<p>Stanford University
Santa BarbaraUC</p>
<p>Stanford University</p>
<p>Stanford University</p>
<p>Mapping the Increasing Use of LLMs in Scientific Papers
1 Apr 202438F6E8D119F1BB018F03A1DC5F0BFD64arXiv:2404.01268v1[cs.CL]
Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time.Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices.However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs.To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time.Our statistical estimation operates on the corpus level and is more robust than inference on individual instances.Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%).In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%).Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths.Our findings suggests that LLMs are being broadly used in scientific writings.</p>
<p>Introduction</p>
<p>Since the release of ChatGPT in late 2022, anecdotal examples of both published papers (Okunyt ė, 2023;Deguerin, 2024) and peer reviews (Oransky &amp; Marcus, 2024) which appear to be ChatGPT-generated have inspired humor and concern. 1 While certain tells, such as "regenerate response" (Conroy, 2023b;a) and "as an AI language model" (Vincent, 2023), found in published papers indicate modified content, less obvious cases are nearly impossible to detect at the individual level (Else, 2023;Gao et al., 2022).Liang et al. (2024) present a method for detecting the percentage of LLM-modified text in a corpus beyond such obvious Preprint 2021Preprint .1 2021Preprint .4 2021Preprint .7 2021Preprint .10 2022Preprint .1 2022Preprint .4 2022Preprint .7 2022Preprint .10 2023Preprint .1 2023Preprint .4 2023Preprint .7 2023.10 .10Computer Science(arXiv) Electrical Engineering and Systems Science(arXiv) Statistics(arXiv) bioRxiv Physics(arXiv) Nature portfolio Mathematics(arXiv)</p>
<p>Figure 1: Estimated Fraction of LLM-Modified Sentences across Academic Writing Venues over Time.This figure displays the fraction (α) of sentences estimated to have been substantially modified by LLM in abstracts from various academic writing venues.The analysis includes five areas within arXiv (Computer Science, Electrical Engineering and Systems Science, Mathematics, Physics, Statistics), articles from bioRxiv, and a combined dataset from 15 journals within the Nature portfolio.Estimates are based on the distributional GPT quantification framework, which provides population-level estimates rather than individual document analysis.Each point in time is independently estimated, with no temporal smoothing or continuity assumptions applied.Error bars indicate 95% confidence intervals by bootstrap.Further analysis of paper introductions is presented in Figure 7.</p>
<p>cases.Applied to scientific publishing, the importance of this at-scale approach is two-fold: first, rather than looking at LLM-use as a type of rule-breaking on an individual level, we can begin to uncover structural circumstances which might motivate its use.Second, by examining LLM-use in academic publishing at-scale, we can capture epistemic and linguistic shifts, miniscule at the individual level, which become apparent with a birdseye view.</p>
<p>Measuring the extent of LLM-use on scientific publishing has urgent applications.Concerns about accuracy, plagiarism, anonymity, and ownership have prompted some prominent scientific institutions to take a stance on the use of LLM-modified content in academic publications.The International Conference on Machine Learning (ICML) 2023, a major machine learning conference, has prohibited the inclusion of text generated by LLMs like ChatGPT in submitted papers, unless the generated text is used as part of the paper's experimental analysis (ICML, 2023).Similarly, the journal Science has announced an update to their editorial policies, specifying that text, figures, images, or graphics generated by ChatGPT or any other LLM tools cannot be used in published works (Thorp, 2023).Taking steps to measure the extent of LLM-use can offer a first-step in identifying risks to the scientific publishing ecosystem.Furthermore, exploring the circumstances in which LLMuse is high can offer publishers and academic institutions useful insight into author behavior.Sites of high LLM-use can act as indicators for structural challenges faced by scholars.These range from pressures to "publish or perish" which encourage rapid production of papers to concerns about linguistic discrimination that might lead authors to use LLMs as prose editors.</p>
<p>We conduct the first systematic, large-scale analysis to quantify the prevalence of LLMmodified content across multiple academic platforms, extending a recently proposed, stateof-the-art distributional GPT quantification framework (Liang et al., 2024) for estimating the fraction of AI-modified content in a corpus.Throughout this paper, we use the term "LLMmodified" to refer to text content substantially updated by ChatGPT beyond basic spelling and grammatical edits.Modifications we capture in our analysis could include, for example, summaries of existing writing or the generation of prose based on outlines.A key characteristic of this framework is that it operates on the population level, without the need to perform inference on any individual instance.As validated in the prior paper, the framework is orders of magnitude more computationally efficient and thus scalable, produces more accurate estimates, and generalizes better than its counterparts under significant temporal distribution shifts and other realistic distribution shifts.</p>
<p>We apply this framework to the abstracts and introductions (Figures 1 and 7) of academic papers across multiple academic disciplines,including arXiv, bioRxiv, and 15 journals within the Nature portfolio, such as Nature, Nature Biomedical Engineering, Nature Human Behaviour, and Nature Communications.Our study analyzes a total of 950,965 papers published between January 2020 and February 2024, comprising 773,147 papers from arXiv, 161,280 from bioRxiv, and 16,538 from the Nature portfolio journals.The papers from arXiv cover multiple academic fields, including Computer Science, Electrical Engineering and Systems Science, Mathematics, Physics, and Statistics.These datasets allow us to quantify the prevalence of LLM-modified academic writing over time and across a broad range of academic fields.</p>
<p>Our results indicate that the largest and fastest growth was observed in Computer Science papers, with α reaching 17.5% for abstracts and 15.3% for introductions by February 2024.In contrast, Mathematics papers and the Nature portfolio showed the least increase, with α reaching 4.9% and 6.3% for abstracts and 3.5% and 6.4% for introductions, respectively.Moreover, our analysis reveals at an aggregate level that higher levels of LLM-modification Preprint are associated with papers whose first authors post preprints more frequently and papers with shorter lengths.Results also demonstrate a closer relationship between papers with LLM-modifications, which could indicate higher use in more crowded fields of study (as measured by the distance to the nearest neighboring paper in the embedding space), or that generated-text is flattening writing diversity.</p>
<p>Related Work</p>
<p>GPT Detectors Various methods have been proposed for detecting LLM-modified text, including zero-shot approaches that rely on statistical signatures characteristic of machinegenerated content (Lavergne et al., 2008;Badaskar et al., 2008;Beresneva, 2016;Solaiman et al., 2019;Mitchell et al., 2023a;Yang et al., 2023a;Bao et al., 2023;Tulchinskii et al., 2023) and training-based methods that finetune language models for binary classification of human vs. LLM-modified text (Bhagat &amp; Hovy, 2013;Zellers et al., 2019;Bakhtin et al., 2019;Uchendu et al., 2020;Chen et al., 2023;Yu et al., 2023;Li et al., 2023;Liu et al., 2022;Bhattacharjee et al., 2023;Hu et al., 2023a).However, these approaches face challenges such as the need for access to LLM internals, overfitting to training data and language models, vulnerability to adversarial attacks (Wolff, 2020), and bias against non-dominant language varieties (Liang et al., 2023a).The effectiveness and reliability of publicly available LLM-modified text detectors have also been questioned (OpenAI, 2019;Jawahar et al., 2020;Fagni et al., 2021;Ippolito et al., 2019;Mitchell et al., 2023b;Gehrmann et al., 2019;Heikkilä, 2022;Crothers et al., 2022;Solaiman et al., 2019;Kirchner et al., 2023;Kelly, 2023), with the theoretical possibility of accurate instance-level detection being debated (Weber-Wulff et al., 2023;Sadasivan et al., 2023;Chakraborty et al., 2023).In this study, we apply the recently proposed distributional GPT quantification framework (Liang et al., 2024), which estimates the fraction of LLM-modified content in a text corpus at the population level, circumventing the need for classifying individual documents or sentences and improving upon the stability, accuracy, and computational efficiency of existing approaches.A more comprehensive discussion of related work can be found in Appendix G.</p>
<p>Background: the distributional LLM quantification framework</p>
<p>We adapt the distributional LLM quantification framework from Liang et al. (2024) to quantify the use of AI-modified academic writing.The framework consists of the following steps:</p>
<ol>
<li>Problem formulation: Let P and Q be the probability distributions of human-written and LLM-modified documents, respectively.The mixture distribution is given by D α (X) = (1 − α)P (x) + αQ(x), where α is the fraction of AI-modified documents.The goal is to estimate α based on observed documents {X i } N i=1 ∼ D α .2. Parameterization: To make α identifiable, the framework models the distributions of token occurrences in human-written and LLM-modified documents, denoted as P T and Q T , respectively, for a chosen list of tokens T = {t i } M i=1 .The occurrence probabilities of each token in human-written and LLM-modified documents, p t and q t , are used to parameterize P T and Q T :
P T (X) = ∏ t∈T p 1{t∈X} t (1 − p t ) 1{t/ ∈X} , Q T (X) = ∏ t∈T q 1{t∈X} t (1 − q t ) 1{t/ ∈X} .</li>
</ol>
<p>Estimation:</p>
<p>The occurrence probabilities p t and q t are estimated using collections of known human-written and LLM-modified documents, {X P j } n P j=1 and {X
Q j } n Q j=1 , respec- tively: pt = 1 n P n P ∑ j=1 1{t ∈ X P j }, qt = 1 n Q n Q ∑ j=1 1{t ∈ X Q j }.</p>
<p>Inference:</p>
<p>The fraction α is estimated by maximizing the log-likelihood of the observed documents under the mixture distribution Dα,T (X) = (1 − α) PT (X) + α QT (X): Liang et al. (2024) demonstrate that the data points {X i } N i=1 ∼ D α can be constructed either as a document or as a sentence, and both work well.Following their method, we use sentences as the unit of data points for the estimates for the main results.In addition, we extend this framework for our application to academic papers with two key differences:
αMLE T = argmax α∈[0,1] N ∑ i=1 log (1 − α) PT (X i ) + α QT (X i ) .</p>
<p>Generating Realistic LLM-Produced Training Data</p>
<p>We use a two-stage approach to generate LLM-produced text, as simply prompting an LLM with paper titles or keywords would result in unrealistic scientific writing samples containing fabricated results, evidence, and ungrounded or hallucinated claims.</p>
<p>Specifically, given a paragraph from a paper known to not include LLM-modification, we first perform abstractive summarization using an LLM to extract key contents in the form of an outline.We then prompt the LLM to generate a full paragraph based the outline (see Appendix for full prompts).</p>
<p>Our two-stage approach can be considered a counterfactual framework for generating LLM text: given a paragraph written entirely by a human, how would the text read if it conveyed almost the same content but was generated by an LLM?This additional abstractive summarization step can be seen as the control for the content.This approach also simulates how scientists may be using LLMs in the writing process, where the scientists first write the outline themselves and then use LLMs to generate the full paragraph based on the outline.</p>
<p>Using the Full Vocabulary for Estimation</p>
<p>We use the full vocabulary instead of only adjectives, as our validation shows that adjectives, adverbs, and verbs all perform well in our application (Figure 3).Using the full vocabulary minimizes design biases stemming from vocabulary selection.We also find that using the full vocabulary is more sample-efficient in producing stable estimates, as indicated by their smaller confidence intervals by bootstrap.</p>
<p>Implementation and Validations</p>
<p>Data Collection and Sampling</p>
<p>We collect data from three sources: arXiv, bioRxiv, and 15 journals from the Nature portfolio.For each source, we randomly sample up to 2,000 papers per month from January 2020 to February 2024.The procedure for generating the LLM-generated corpus data is described in Section § 3. We focused on the introduction sections for the main texts, as the introduction was the most consistently and commonly occurring section across diverse categories of papers.See Appendix C for comprehensive implementation details.</p>
<p>Data Split, Model Fitting, and Evaluation</p>
<p>For model training, we count word frequencies for scientific papers written before the release of ChatGPT and the LLM-modified corpora described in Section 3. We fit the model with data from 2020, and use data from January 2021 onwards for validation and inference.We fit separate models for abstracts and introductions for each major category.</p>
<p>To evaluate model accuracy and calibration under temporal distribution shift, we use 3,000 papers from January 1, 2022, to November 29, 2022, a time period prior to the release of ChatGPT, as the validation data.We construct validation sets with LLM-modified content proportions (α) ranging from 0% to 25%, in 5% increments, and compared the model's estimated α with the ground truth α (Figure 3).Full vocabulary, adjectives, adverbs, and verbs all performed well in our application, with a prediction error consistently less than 3.5% at the population level across various ground truth α values (Figure 3).In all cases, the estimation error for α is less than 3.5%.The first 7 panels (a-g) are the validation on abstracts for each academic writing venue, while the later 6 panels (h-m) are the validation on introductions.We did not include bioRxiv introductions due to the unavailability of bulk PDF downloads.Error bars indicate 95% confidence intervals by bootstrap.</p>
<p>Main Results and Findings</p>
<p>Temporal Trends in AI-Modified Academic Writing</p>
<p>Setup We apply the model to estimate the fraction of LLM-modified content (α) for each paper category each month, for both abstracts and introductions.Each point in time was independently estimated, with no temporal smoothing or continuity assumptions applied.</p>
<p>Results</p>
<p>Our findings reveal a steady increase in the fraction of AI-modified content (α) in both the abstracts (Figure 1) and the introductions (Figure 7), with the largest and fastest growth observed in Computer Science papers.By February 2024, the estimated α for Computer Science had increased to 17.5% for abstracts and 15.5% for introductions.The second-fastest growth was observed in Electrical Engineering and Systems Science, with the estimated α reaching 14.4% for abstracts and 12.4% for introductions during the same period.In contrast, Mathematics papers and the Nature portfolio showed the least increase.By the end of the studied period, the estimated α for Mathematics had increased to 4.9% for abstracts and 3.9% for introductions, while the estimated α for the Nature portfolio had reached 6.3% for abstracts and 4.3% for introductions.</p>
<p>The November 2022 estimates serve as a pre-ChatGPT reference point for comparison, as ChatGPT was launched on November 30, 2022.The estimated α for Computer Science in November 2022 was 2.3%, while for Electrical Engineering and Systems Science, Mathematics, and the Nature portfolio, the estimates were 2.9%, 2.4%, and 3.1%, respectively.These values are consistent with the false positive rate reported in the earlier section ( § 4.2).</p>
<p>Relationship Between First-Author Preprint Posting Frequency and GPT Usage</p>
<p>We found a notable correlation between the number of preprints posted by the first author on arXiv and the estimated number of LLM-modified sentences in their academic writing.Papers were stratified into two groups based on the number of first-authored arXiv Computer Science preprints by the first author in the year: those with two or fewer (≤ 2) preprints and those with three or more (≥ 3) preprints (Figure 4).We used the 2023 author grouping for the 2024.1-2data, as we don't have the complete 2024 author data yet.</p>
<p>By February 2024, abstracts of papers whose first authors had ≥ 3 preprints in 2023 showed an estimated 19.3% of sentences modified by AI, compared to 15.6% for papers whose first authors had ≤ 2 preprints (Figure 4a).We observe a similar trend in the introduction sections, with first authors posting more preprints having an estimated 16.9% LLM-modified sentences, compared to 13.7% for first authors posting fewer preprints (Figure 4b).Since the first-author preprint posting frequency may be confounded by research field, we conduct an additional robustness check for our findings.We find that the observed trend holds for each of the three arXiv Computer Science sub-categories: cs.CV (Computer Vision and Pattern Recognition), cs.</p>
<p>LG (Machine Learning), and cs.CL (Computation and Language) (Supp Figure 12).</p>
<p>Our results suggest that researchers posting more preprints tend to utilize LLMs more extensively in their writing.One interpretation of this effect could be that the increasingly competitive and fast-paced nature of CS research communities incentivizes taking steps to accelerate the writing process.We do not evaluate whether these preprints were accepted for publication.</p>
<p>Relationship Between Paper Similarity and LLM Usage</p>
<p>We investigate the relationship between a paper's similarity to its closest peer and the estimated LLM usage in the abstract.To measure similarity, we first embed each abstract from the arXiv Computer Science papers using OpenAI's text-embedding-ada-002 model, creating a vector representation for each abstract.We then calculate the distance between each paper's vector and its nearest neighbor within the arXiv Computer Science abstracts.</p>
<p>Based on this similarity measure we divide papers into two groups: those more similar to their closest peer (below median distance) and those less similar (above median distance).</p>
<p>The temporal trends of LLM usage for these two groups are shown in Figure 5.After the release of ChatGPT, papers most similar to their closest peer consistently showed higher LLM usage compared to those least similar.By February 2024, the abstracts of papers more similar to their closest peer had an estimated 22.2% of sentences modified by LLMs, compared to 14.7% for papers less similar to their closest peer.To account for potential confounding effects of research fields, we conducted an additional robustness check by measuring the nearest neighbor distance within each of the three arXiv Computer LG (Machine Learning), and cs.CL (Computation and Language), and found that the observed trend holds for each sub-category (Supp Figure 13).</p>
<p>There are several ways to interpret these findings.First, LLM-use in writing could cause the similarity in writing or content.Community pressures may even motivate scholars to try to sound more similar -to assimilate to the "style" of text generated by an LLM.Alternatively, LLMs may be more commonly used in research areas where papers tend to be more similar to each other.This could be due to the competitive nature of these crowded subfields, which may pressure researchers to write faster and produce similar findings.Future interdisciplinary research should explore these hypotheses.</p>
<p>Relationship Between Paper Length and AI Usage</p>
<p>We also explored the association between paper length and LLM usage in arXiv Computer Science papers.Papers were stratified by their full text word count, including appendices, into two bins: below or above 5,000 words (the rounded median).</p>
<p>Figure 6 shows the temporal trends of LLM usage for these two groups.After the release of ChatGPT, shorter papers consistently showed higher AI usage compared to longer papers.By February 2024, the abstracts of shorter papers had an estimated 17.7% of sentences modified by LLMs, compared to 13.6% for longer papers (Figure 6a).A similar trend was observed in the introduction sections (Figure 6b).To account for potential confounding effects of research fields, we conducted an additional robustness check.The finding holds for both cs.CV (Computer Vision and Pattern Recognition) and cs.</p>
<p>LG (Machine Learning) (Supp Figure 14).However, for cs.CL (Computation and Language), we found no significant difference in LLM usage between shorter and longer papers, possibly due to the limited sample size, as we only parsed a subset of the PDFs and calculated their full length.</p>
<p>As Computer Science conference papers typically have a fixed page limit, longer papers likely have more substantial content in the appendix.The lower LLM usage in these papers may suggest that researchers with more comprehensive work rely less on LLM-assistance in their writing.However, further investigation is needed to determine the relationship between paper length, content comprehensiveness, and the quality of the research.</p>
<p>Discussion</p>
<p>Our findings show a sharp increase in the estimated fraction of LLM-modified content in academic writing beginning about five months after the release of ChatGPT, with the fastest growth observed in Computer Science papers.This trend may be partially explained by Computer Science researchers' familiarity with and access to large language models.Additionally, the fast-paced nature of LLM research and the associated pressure to publish quickly may incentivize the use of LLM writing assistance (Foster et al., 2015).</p>
<p>We expose several other factors associated with higher LLM usage in academic writing.First, authors who post preprints more frequently show a higher fraction of LLM-modified content in their writing.Second, papers in more crowded research areas, where papers tend to be more similar, showed higher LLM-modification compared to those in less crowded areas.Third, shorter papers consistently showed higher LLM-modification compared to longer papers, which may indicate that researchers working under time constraints are Preprint more likely to rely on AI for writing assistance.These results may be an indicator of the competitive nature of certain research areas and the pressure to publish quickly.</p>
<p>If the majority of modification comes from an LLM owned by a private company, there could be risks to the security and independence of scientific practice.We hope our results inspire further studies of widespread LLM-modified text and conversations about how to promote transparent, epistemically diverse, accurate, and independent scientific publishing.</p>
<p>Limitations While our study focused on ChatGPT, which accounts for more than threequarters of worldwide internet traffic in the category (Van Rossum, 2024), we acknowledge that there are other large language models used for assisting academic writing.Furthermore, while Liang et al. (2023a) demonstrate that GPT-detection methods can falsely identify the writing of language learners as LLM-generated, our results showed that consistently low false positives estimates of α in 2022, which contains a significant fraction of texts written by multilingual scholars.We recognize that significant author population changes (MacroPolo, 2024) or other language-use shifts could still impact the accuracy of our estimates.Finally, the associations that we observe between LLM usage and paper characteristics are correlations which could be affected by other factors such as research topics.More causal studies is an important direction for future work.Computer Science(arXiv) Electrical Engineering and Systems Science(arXiv) Statistics(arXiv) Physics(arXiv) Nature portfolio Mathematics(arXiv)</p>
<p>A Estimated Fraction of LLM-Modified Sentences in Introductions</p>
<p>Figure 7: Estimated Fraction of LLM-Modified Sentences in Introductions Across Academic Writing Venues Over Time.We focused on the introduction sections for the main texts, as the introduction was the most consistently and commonly occurring section across different categories of papers.This figure presents the estimated fraction (α) of sentences in introductions which are LLM-modified, across the same venues as Figure 1.We found that the results are consistent with those observed in abstracts (Figure 1).We did not include bioRxiv introductions as there is no bulk download of PDFs available.Error bars indicate 95% confidence intervals by bootstrap.</p>
<p>B LLM prompts used in the study</p>
<p>The aim here is to reverse -engineer the author ' s writing process by taking a piece of text from a paper and compressing it into a more concise form .This process simulates how an author might distill their thoughts and key points into a structured , yet not overly condensed form . Now as a first step , first summarize the goal of the text , e.g., is it introduction , or method , results ? and then given a complete piece of text from a paper , reverse -engineer it into a list of bullet points .</p>
<p>Figure 8: Example prompt for summarizing a paragraph from a human-authored paper into a skeleton: This process simulates how an author might first only write the main ideas and core information into a concise outline.The goal is to capture the essence of the paragraph in a structured and succinct manner, serving as a foundation for the previous prompt.</p>
<p>Following the initial step of reverse -engineering the author ' s writing process by compressing a text segment from a paper , you now enter the second phase .Here , your objective is to expand upon the concise version previously crafted .This stage simulates how an author elaborates on the distilled thoughts and key points , enriching them into a detailed , structured narrative .</p>
<p>Given the concise output from the previous step , your task is to develop it into a fully fleshed -out text .</p>
<p>Figure 9: Example prompt for expanding the skeleton into a full text: The aim here is to simulate the process of using the structured outline as a basis to generate comprehensive and coherent text.This step mirrors the way an author might flesh out the outline into detailed paragraphs, effectively transforming the condensed ideas into a fully articulated section of a paper.The format and depth of the expansion can vary, reflecting the diverse styles and requirements of different academic publications.</p>
<p>Your task is to proofread the provided sentence for grammatical accuracy .Ensure that the corrections introduce minimal distortion to the original content .LG, and cs.CL) are stratified by their full text word count, including appendices, into two bins: below or above 5,000 words (the rounded median).For cs.CL, no significant difference in LLM usage was found between shorter and longer papers, possibly due to the limited sample size, as only a subset of the PDFs were parsed to calculate the full length.Error bars indicate 95% confidence intervals by bootstrap.</p>
<p>F Proofreading Results on arXiv data</p>
<p>G Extended Related Work</p>
<p>Zero-shot LLM detection.A major category of LLM text detection uses statistical signatures that are characteristic of machine-generated text, and the scope is to detect the text within individual documents.Initially, techniques to distinguish AI-modified text from human-written text employed various metrics, such as entropy (Lavergne et al., 2008), the frequency of rare n-grams (Badaskar et al., 2008), perplexity (Beresneva, 2016), and log-probability scores (Solaiman et al., 2019), which are derived from language models.More recently, DetectGPT (Mitchell et al., 2023a) found that AI-modified text is likely to be found in areas with negative log probability curvature.DNA-GPT (Yang et al., 2023a) improves performance by examining the divergence in n-gram patterns.Fast-DetectGPT (Bao et al., 2023) (Liu et al., 2022;Bhattacharjee et al., 2023;Hu et al., 2023a).We refer to recent surveys Yang et al. (2023b); Ghosal et al. (2023) for additional methods and details.However, these publicly available tools for detecting AI-modified content have sparked a debate about their effectiveness and reliability (OpenAI, 2019;Jawahar et al., 2020;Fagni et al., 2021;Ippolito et al., 2019;Mitchell et al., 2023b;Gehrmann et al., 2019;Heikkilä, 2022;Crothers et al., 2022;Solaiman et al., 2019).OpenAI's decision to discontinue its AI-modified text classifier in 2023 due to "low rate of accuracy" further highlighted this discussion (Kirchner et al., 2023;Kelly, 2023).</p>
<p>Training-based detection methods face challenges such as overfitting to training data and language models, making them vulnerable to adversarial attacks (Wolff, 2020) and biased against non-dominant language varieties (Liang et al., 2023a).The theoretical possibility of achieving accurate instance-level detection has also been questioned (Weber-Wulff et al., 2023;Sadasivan et al., 2023;Chakraborty et al., 2023).</p>
<p>LLM watermarking.</p>
<p>Text watermarking introduces a method to detect AI-modified text by embedding an imperceptible signal, known as a watermark, directly into the text.This watermark can be retrieved by a detector that shares the model owner's secret key.Early watermarking techniques included synonym substitution (Chiang et al., 2003;Topkara et al., 2006b) and syntactic restructuring (Atallah et al., 2001;Topkara et al., 2006a).Modern watermarking strategies involve integrating watermarks into the decoding process of language models (Aaronson, 2023;Kirchenbauer et al., 2023;Zhao et al., 2023).Researchers have developed various techniques, such as the Gumbel watermark (Aaronson, 2023), which uses traceable pseudo-random softmax sampling, and the red-green list approach (Kirchenbauer et al., 2023;Zhao et al., 2024a), which splits the vocabulary based on hash values of previous n-grams.Some methods focus on preserving the original token probability distributions (Hu et al., 2023b;Kuditipudi et al., 2023;Wu et al., 2023), while others aim to improve detectability and perplexity (Zhao et al., 2024b) or incorporate multi-bit watermarks (Yoo et al., 2023;Fernandez et al., 2023).However, one major concern with watermarking is that it requires the involvement of the model or service owner, such as OpenAI, to implant the watermark during the text generation process.In contrast, the framework by Liang et al. (2024) operates independently of the model or service owner's intervention, allowing for the monitoring of AI-modified content without requiring their active participation or adoption.</p>
<p>Implications for LLM Pretraining Data Quality The increasing prevalence of AI-modified content in academic papers, particularly on platforms like arXiv, has important implications for the quality of LLM pretraining data.arXiv has become a significant source of training data for LLMs, contributing approximately 2.5% of the data for models like Llama (Touvron et al., 2023), 12% for RedPajama (Elazar et al., 2023), and 8.96% for the Pile (Gao et al., 2020).</p>
<p>Our findings suggest that a growing proportion of this pretraining data may contain LLMmodified content.Preliminary research indicates that the inclusion of LLM-modified content (Veselovsky et al., 2023) in LLM training can lead to several pitfalls, such as the reinforcement of stereotypes and biases against anyone who is not a middle-aged "European/North American man" (Ghosh &amp; Caliskan, 2023;Santurkar et al., 2023), the flattening of variation in language and content (Dell'Acqua et al., 2023), and the potential failure of models to accurately capture the true distribution of the original content, which may result in model collapse (Shumailov et al., 2023).Santurkar et al. (2023) demonstrate that this phenomenon amplifies the effect of LLMs providing content that is unrepresentative of most of the world.As such, our results underscore the importance of robust data curation and filtering strategies even in seemingly unpolluted datasets.</p>
<p>Figure 2 :
2
Figure 2: Word Frequency Shift in arXiv Computer Science abstracts over 14 years (2010-2024).The plot shows the frequency over time for the top 4 words most disproportionately used by LLM compared to humans, as measured by the log odds ratio.The words are: realm, intricate, showcasing, pivotal.These terms maintained a consistently low frequency in arXiv CS abstracts over more than a decade (2010-2022) but experienced a sudden surge in usage starting in 2023.</p>
<p>Preprint</p>
<p>Figure 3 :
3
Figure 3: Fine-grained Validation of Model Performance Under Temporal Distribution Shift.We evaluate the accuracy of our models in estimating the fraction of LLM-modified content (α) under a challenging temporal data split, where the validation data (sampled from 2022-01-01 to 2022-11-29) are temporally separated from the training data (collected up to 2020-12-31) by at least a year.The X-axis indicates the ground truth α, while the Y-axis indicates the model's estimated α.In all cases, the estimation error for α is less than 3.5%.The first 7 panels (a-g) are the validation on abstracts for each academic writing venue, while the later 6 panels (h-m) are the validation on introductions.We did not include bioRxiv introductions due to the unavailability of bulk PDF downloads.Error bars indicate 95% confidence intervals by bootstrap.</p>
<p>Figure 4 :
4
Figure 4: Papers authored by first authors who post preprints more frequently tend to have a higher fraction of LLM-modified content.Papers in arXiv Computer Science are stratified into two groups based on the preprint posting frequency of their first author, as measured by the number of first-authored preprints in the year.Error bars indicate 95% confidence intervals by bootstrap.</p>
<p>Figure 5 :
5
Figure 5: Papers in more crowded research areas tend to have a higher fraction of LLMmodified content.Papers in arXiv Computer Science are divided into two groups based on their abstract's embedding distance to their closest peer: papers more similar to their closest peer (below median distance) and papers less similar to their closest peer (above median distance).Error bars indicate 95% confidence intervals by bootstrap.</p>
<p>Figure 6 :
6
Figure 6: Shorter papers tend to have a higher fraction of LLM-modified content.arXiv Computer Science papers are stratified by their full text word count, including appendices, into two bins: below or above 5,000 words (the rounded median).Error bars indicate 95% confidence intervals by bootstrap.</p>
<p>Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :
1011121314
Figure 10: Example prompt for proofreading.</p>
<p>Robustness of estimations to proofreading.The plot demonstrates a slight increase in the fraction of LLM-modified content after using Large Language Models (LLMs) for "proofreading" across different arXiv main categories.This observation validates our method's robustness to minor LLM-generated text edits, such as those introduced by simple proofreading.The analysis was conducted on 1,000 abstracts from each arXiv main category, randomly sampled from the period between January 1, 2022, and November 29, 2022.Error bars indicate 95% confidence intervals by bootstrap.
Computer Science(arXiv) Electrical Engineering &amp; Systems Science(arXiv) Before Proofread After Proofread Figure 15: Preprint 0% 5% 10% 15% 20% Estimated AlphaMathematics(arXiv)Physics(arXiv)Statistics(arXiv)</p>
<p>Li et al. (2023))020);Uchendu et al., 2020)ng et al., 2023b;vature over raw probability.Tulchinskii et al. (2023)studied the intrinsic dimensionality of generated text to perform the detection.We refer to recent surveys byYang et al. (2023b);Ghosal et al. (2023)for additional details and more related works.However, zero-shot detection requires direct access to LLM internals to enable effective detection.Closed-source commercial LLMs, like GPT-4, necessitate using proxy LLMs, which compromises the robustness of zero-shot detection methods across various scenarios(Sadasivan et al., 2023;Shi et al., 2023;Yang et al., 2023b; Zhang et al., 2023).Another category is training-based detection, which involves training classification models on datasets that consist of both human and AI-modified texts for the binary classification task of detection.Early efforts applied classification algorithms to identify AI text across various domains, such as peer review submissions (Bhagat &amp; Hovy, 2013), media publications(Zellers et al., 2019), and other contexts(Bakhtin et al., 2019;Uchendu et al., 2020).Recently, researchers have finetuned pretrained language model backbones for this binary classification.GPT-Sentinel(Chen et al., 2023)uses the constructed dataset OpenGPTText to train RoBERTa(Liu et al., 2019)and T5(Raffel et al., 2020)classifiers.GPT-Pat(Yu et al., 2023)trains a Siamese neural network to compute the semantic similarity of AI text and human text.Li et al. (2023)build a wild testbed by gathering texts from various human writings and texts generated by different LLMs.Using techniques such as contrastive and adversarial learning can enhance classifier robustness
Training-based LLM detection.
AcknowledgmentsWe thank Daniel A. McFarland, Dan Jurafsky, Zachary Izzo, Xi Victoria Lin, Lingjiao Chen, and Haotian Ye for their helpful comments and discussions.J.Z. is supported by the National Science Foundation (CCF 1763191 and CAREER 1942926), the US National Institutes of Health (P30AG059307 and U01MH098953) and grants from the Silicon Valley Foundation and the Chan-Zuckerberg Initiative.and H.L. is supported by the National Science Foundation (2244804 and 2022435) and the Stanford Institute for Human-Centered Artificial Intelligence (HAI).PreprintC Additional Information on Implementation and ValidationsSupplementary Information about Data We collected data for this study from three publicly accessible sources: official APIs provided by arXiv and bioRxiv, and web pages from the Nature portfolio.For each of the five major arXiv categories (Computer Science, Electrical Engineering and Systems Science, Mathematics, Physics, Statistics), we randomly sampled 2,000 papers per month from January 2020 to February 2024.Similarly, from bioRxiv, we randomly sampled 2,000 papers for each month within the same timeframe.For the Nature portfolio, encompassing 15 Nature journals including Nature, Nature Biomedical Engineering, Nature Human Behaviour, and Nature Communications, we followed the same sampling strategy, selecting 2,000 papers randomly from each month, from January 2020 to February 2024.The procedure for generating the AI corpus data for a given time period is described in aforementioned Section § 3.When there were not enough papers to reach our target of 2,000 per month, we included all available papers.The Nature portfolio encompasses the following 15 Nature journals:Additional Information on Large Language ModelsIn this study, we utilized the gpt-3.5turbo-0125model, which was trained on data up to September 2021, to generate the training data for our analysis.The LLM was employed solely for the purpose of creating the training dataset and was not used in any other aspect of the study.We chose to focus on ChatGPT due to its dominant position in the generative AI market.According to a comprehensive analysis conducted by FlexOS in early 2024, ChatGPT accounts for an overwhelming 76% of global internet traffic in the category, followed by Bing AI at 16%, Bard at 7%, and Claude at 1%(Van Rossum, 2024).This market share underscores ChatGPT's widespread adoption and makes it a highly relevant subject for our investigation.Furthermore, recent studies have also shown that ChatGPT demonstrates substantially better understanding of scientific papers than other LLMs(Liang et al., 2023b;Liu &amp; Shah, 2023).We chose to use GPT-3.5 for generating the training data due to its free availability, which lowers the barrier to entry for users and thereby captures a wider range of potential LLM usage patterns.This accessibility makes our study more representative of the broad phenomenon of LLM-assisted writing.Furthermore, the previous work byLiang et al. (2024)has demonstrated the framework's robustness and generalizability to other LLMs.Their findings suggest that the framework can effectively handle significant content shifts and temporal distribution shifts.Regarding the parameter settings for the LLM, we set the decoding temperature to 1.0 and the maximum decoding length to 2048 tokens during our experiments.The Top P hyperparameter, which controls the cumulative probability threshold for token selection, was set to 1.0.Both the frequency penalty and presence penalty, which can be used to discourage the repetition of previously generated tokens, were set to 0.0.Additionally, we did not configure any specific stop sequences during the decoding process.
Simons Institute Talk on Watermarking of Large Language Models. Scott Aaronson, 2023</p>
<p>Natural Language Watermarking: Design, Analysis, and a Proof-of-Concept Implementation. Mikhail J Atallah, Michael Victor Raskin, Christian F Crogan, Florian Hempelmann, Dina Kerschbaum, Sanket Mohamed, Naik, 2001In Information Hiding</p>
<p>Identifying Real or Fake Articles: Towards better Language Modeling. Sameer Badaskar, Sachin Agarwal, Shilpa Arora, International Joint Conference on Natural Language Processing. 2008</p>
<p>Real or Fake? Learning to Discriminate Machine from Human Generated Text. Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc ' , Aurelio Ranzato, Arthur Szlam, ArXiv, abs/1906.033512019</p>
<p>Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang, ArXiv, abs/2310.051302023</p>
<p>Computer-Generated Text Detection Using Machine Learning: A Systematic Review. Daria Beresneva, International Conference on Applications of Natural Language to Data Bases, 2016. Rahul Bhagat and Eduard H. Hovy. Squibs: What Is a Paraphrase? Computational Linguistics. 201339</p>
<p>ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, Huan Liu, ArXiv, abs/2309.039922023</p>
<p>On the possibilities of ai-generated text detection. Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, Furong Huang, arXiv:2304.047362023arXiv preprint</p>
<p>GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. Yutian Chen, Hao Kang, Vivian Zhai, Liang Li, Rita Singh, Bhiksha Ramakrishnan, ArXiv, abs/2305.079692023</p>
<p>Natural Language Watermarking Using Semantic Substitution for Chinese Text. Yuei-Lin Chiang, Lu-Ping Chang, Wen-Tai Hsieh, Wen-Chih Chen, International Workshop on Digital Watermarking. 2003</p>
<p>How ChatGPT and other AI tools could disrupt scientific publishing. Gemma Conroy, Nature. October 2023a</p>
<p>Scientific sleuths spot dishonest ChatGPT use in papers. Gemma Conroy, Nature. September 2023b</p>
<p>Mack Deguerin. AI-generated nonsense is leaking into scientific journals. Evan Crothers, Nathalie Japkowicz, Herna Viktor, arXiv:2210.07321Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. 2022. March 2024arXiv preprint</p>
<p>Navigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality. Fabrizio Dell'acqua, Edward Mcfowland, Ethan R Mollick, Hila Lifshitz-Assaf, Katherine Kellogg, Saran Rajendran, Lisa Krayer, Franc ¸ois Candelon, Karim R Lakhani, Harvard Business School Technology &amp; Operations Mgt. Unit Working Paper. 2023</p>
<p>What's In My Big Data?. Preprint Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Abstracts written by ChatGPT fool scientists. Holly Else, Nature. Jan 2023</p>
<p>TweepFake: About detecting deepfake tweets. Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, Maurizio Tesconi, Plos one. 165e02514152021</p>
<p>Three Bricks to Consolidate Watermarks for Large Language Models. Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, Teddy Furon, IEEE International Workshop on Information Forensics and Security (WIFS). 2023. 2023</p>
<p>Tradition and innovation in scientists' research strategies. Andrey Jacob G Foster, James A Rzhetsky, Evans, American sociological review. 8052015</p>
<p>Comparing scientific abstracts generated by Chat-GPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers. Catherine A Gao, Frederick M Howard, Nikolay S Markov, Emma C Dyer, Siddhi Ramesh, Yuan Luo, Alexander T Pearson, bioRxiv. 2022</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.000272020arXiv preprint</p>
<p>GLTR: Statistical Detection and Visualization of Generated Text. Sebastian Gehrmann, Hendrik Strobelt, Alexander M Rush, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2019</p>
<p>Towards Possibilities &amp; Impossibilities of AI-generated Text Detection: A Survey. Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, A S Bedi, ArXiv, abs/2310.152642023</p>
<p>Melissa Heikkilä. How to spot AI-generated text. Sourojit Ghosh, Aylin Caliskan, arXiv:2310.19981Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion. 2023. Dec 2022arXiv preprint</p>
<p>RADAR: Robust AI-Text Detection via Adversarial Learning. Xiaobing Hu, Pin-Yu Chen, Tsung-Yi Ho, ArXiv, abs/2307.038382023a259501842</p>
<p>Unbiased Watermark for Large Language Models. Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang, ArXiv, abs/2310.106692023b</p>
<p>ICML. Clarification on large language model policy LLM. 2023</p>
<p>Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks VS Lakshmanan. Automatic detection of machine generated text: A critical survey. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck, arXiv:1911.00650arXiv:2011.013142019. 2020arXiv preprintAutomatic detection of generated text is easiest when humans are fooled</p>
<p>ChatGPT creator pulls AI detection tool due to 'low rate of accuracy'. CNN Business. Samantha Murphy, Kelly , Jul 2023</p>
<p>Recalibrating the scope of scholarly publishing: A modest step in a vast decolonization process. Saurabh Preprint, Jon Khanna, Juan Ball, John Pablo Alperin, Willinsky, 10.1162/qss_a_00228Quantitative Science Studies. 2641-33373412 2022</p>
<p>A watermark for large language models. John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein, International Conference on Machine Learning. 2023</p>
<p>Jan Hendrik Kirchner, Lama Ahmad, Scott Aaronson, Jan Leike, New AI classifier for indicating AI-written text. 2023</p>
<p>Robust Distortion-free Watermarks for Language Models. Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang, ArXiv, abs/2307.155932023</p>
<p>Detecting Fake Content with Relative Entropy Scoring. Thomas Lavergne, Tanguy Urvoy, Franc ¸ois Yvon, Pan. 2008</p>
<p>Deepfake Text Detection in the Wild. Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, Yue Zhang, ArXiv, abs/2305.132422023</p>
<p>GPT detectors are biased against non-native English writers. Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Y Zou, ArXiv, abs/2304.028192023a</p>
<p>Can large language models provide useful feedback on research papers? A large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, arXiv:2310.017832023barXiv preprint</p>
<p>Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A Mcfarland, James Y Zou, arXiv:2403.07183Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews. 2024arXiv preprint</p>
<p>Ryan Liu, Nihar B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023arXiv preprint</p>
<p>CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning. Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Yu Lan, Chao Shen, ArXiv, abs/2212.103412022</p>
<p>RoBERTa: A Robustly Optimized BERT Pretraining Approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, ArXiv, abs/1907.116922019</p>
<p>The Global AI Talent Tracker. Macropolo, 2024</p>
<p>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn, ArXiv, abs/2301.113052023a</p>
<p>DetectGPT: Zero-shot machine-generated text detection using probability curvature. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn, arXiv:2301.113052023barXiv preprint</p>
<p>Google search exposes academics using ChatGPT in research papers. Paulina Okunyt, Ė , Cybernews. November 2023</p>
<p>Preprint Openai, GPT-2: 1.5B release. 2019. 2019-11-05</p>
<p>Papers and peer reviews with evidence of ChatGPT writing . Retraction Watch. Ivan Oransky, Adam Marcus, 2024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Can AI-Generated Text be Reliably Detected?. Aounon Vinu Sankar Sadasivan, S Kumar, Wenxiao Balasubramanian, Soheil Wang, Feizi, ArXiv, abs/2303.111562023</p>
<p>Whose opinions do language models reflect. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, International Conference on Machine Learning. PMLR2023</p>
<p>The curse of recursion: Training on generated data makes models forget. Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, Cho-Jui Hsieh, Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson, arXiv:2305.174932023. 2023arXiv preprintRed Teaming Language Model Detectors with Language Models</p>
<p>Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, arXiv:1908.09203Release strategies and the social impacts of language models. 2019arXiv preprint</p>
<p>Chatgpt is fun, but not an author. H Holden Thorp, 10.1126/science.adg7879Science. 37966302023</p>
<p>Umut Topkara, Mercan Topkara, and Mikhail J. Atallah. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. Mercan Topkara, Giuseppe Riccardi, Z Dilek, Mikhail J Hakkani-T Ür, Atallah, Workshop on Multimedia &amp; Security. 2006a. 2006bElectronic imaging</p>
<p>Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971ArXiv, abs/2306.04723Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, S. Barannikov, Irina Piontkovskaya, Sergey I. Nikolenko, and Evgeny Burnaev2023. 2023arXiv preprintLlama: Open and efficient foundation language models</p>
<p>Authorship Attribution for Neural Text Generation. Adaku Uchendu, Thai Le, Kai Shu, Dongwon Lee, Conference on Empirical Methods in Natural Language Processing. 2020</p>
<p>Dann Van Rossum, The World's Most Used AI Tools. February 2024150</p>
<p>Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production tasks. Veniamin Veselovsky, Manoel Horta Ribeiro, Robert West, arXiv:2306.078992023arXiv preprint</p>
<p>As an AI language model': the phrase that shows how AI is pollulating the web. The Verge. James Preprint, Vincent, Apr 2023</p>
<p>Testing of detection tools for AI-generated text. Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tomáš Folt Ýnek, Jean Guerrero-Dib, Olumide Popoola, Petr Šigut, Lorna Waddington, 10.1007/s40979-023-00146-zInternational Journal for Educational Integrity. 1833-2595191262023</p>
<p>Attacking Text Detectors. Max Wolff, ArXiv, abs/2002.117682020</p>
<p>DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. Yihan Wu, Zhengmian Hu, Hongyang Zhang, Heng Huang ; Xianjun, Wei Yang, Linda Cheng, William Petzold, Wang Yang, Haifeng Chen, ArXiv, abs/2305.173592023. 2023aDiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models</p>
<p>. Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Ruth Petzold, William Yang, Wang , Wei Cheng, ArXiv, abs/2310.15654A Survey on Detection of LLMs-Generated Content. 2023b</p>
<p>Robust Multi-bit Natural Language Watermarking through Invariant Features. Kiyoon Yoo, Wonhyuk Ahn, Jiho Jang, No Jun, Kwak , Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance. Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, Neng H Yu, ArXiv, abs/2305.125192023258833423</p>
<p>Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, ; Yi-Fan, Zhang Zhang, Liang Zhang, Tien-Ping Wang, Rong Tan, Jin, ArXiv, abs/2312.129182019. 2023Defending Against Neural Fake News</p>
<p>Protecting Language Generation Models via Invisible Watermarking. Xuandong Zhao, Yu-Xiang Wang, Lei Li, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Provable Robust Watermarking for AI-Generated Text. Xuandong Zhao, Prabhanjan Vijendra Ananth, Lei Li, Yu-Xiang Wang, International Conference on Learning Representations (ICLR). 2024a</p>
<p>Xuandong Zhao, Lei Li, Yu-Xiang Wang, arXiv:2402.05864Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs. 2024barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6017 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6017</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6017</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-265659184</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.02783v3.pdf" target="_blank">Large Language Models on Graphs: A Comprehensive Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6017.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6017.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model pretrained for scientific domains, including chemistry, with the goal of encoding scientific knowledge and assisting scientific tasks such as molecule understanding and text generation in science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Galactica (LLM for science)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A domain-targeted LLM pretrained on large scientific corpora (including chemical compound data) to encode and generate scientific knowledge; intended to be used as a scientific knowledge source and for downstream tasks such as molecule understanding, captioning, and QA.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Galactica (proprietary architecture described as a large decoder model pretrained on scientific corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Large-scale scientific corpora including text and two million compounds from PubChem (as reported in the survey); inputs are natural language queries or concatenated molecule textual representations (e.g., SMILES) and text context.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Pretraining on large scientific corpora and instruction tuning with domain data; leverages autoregressive text modeling to absorb knowledge from many documents rather than explicit knowledge graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated text in scientific style (answers, summaries, molecule-related text), contextual representations for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Survey reports use of downstream task evaluation (e.g., molecule-related classification/generation tasks), qualitative inspection for scientific knowledge; standard generation metrics and task-specific metrics (not fully enumerated in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey notes Galactica pretrained on large scientific datasets and able to represent molecular information via SMILES; can be adapted with instruction tuning for domain tasks, but also subject to known LLM limitations (hallucination, sensitivity to linearized molecule format).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>PubChem (two million compounds) and broader scientific corpora used for pretraining as mentioned in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Limited by linearized-graph (SMILES) representation issues, hallucination, and the mismatch between string-based molecule encodings and graph structure; not a solved general-purpose scientific theory distillation method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared qualitatively to other molecular LLM approaches (encoder-decoder and GNN+LLM hybrids); survey contrasts LLM-only pretraining vs. joint graph-text alignment approaches (e.g., contrastive GNN+LLM methods).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6017.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mol-instructions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mol-instructions (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale biomolecular instruction dataset created for instruction-tuning LLMs on biomolecular tasks (molecule-text tasks and molecule reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mol-instructions: A large-scale biomolecular instruction dataset for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mol-instructions (instruction dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A dataset of instruction-style examples pairing molecules and textual tasks intended to instruction-tune LLMs to perform molecule-centered tasks and reasoning over biomedical text and molecule descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in survey (designed to be used for instruction-tuning a variety of LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Large corpus of molecule-text instruction examples (survey references the dataset but does not enumerate exact size within the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Instruction tuning: providing LLMs with many molecule-oriented instruction examples so the model learns to map textual instructions and molecular representations to outputs (generation, property prediction, explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Instruction-following outputs: generated text answers, molecule captions, translation between molecule and text, or task-specific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Downstream molecular tasks, standard generation metrics and domain-specific evaluations (validity/novelty for generative tasks), human or expert evaluation for chemical plausibility (survey cites these general evaluation modalities).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey cites Mol-instructions as a resource enabling instruction-tuning for molecule tasks, improving LLM performance on domain tasks when used for tuning; exact quantitative results not provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Mol-instructions itself (as a dataset); used alongside molecular corpora such as PubChem, ChEMBL in related works according to survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality and alignment of instruction examples to chemical correctness; LLMs still vulnerable to hallucination and syntactic issues with SMILES/line notation unless paired with robust representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Positioned as complementary to graph-aware methods (GNN+LLM) and contrastive alignment approaches; enables LLM-only approaches but does not replace graph-structured encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6017.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cross-modal modeling method that projects molecular graph representations and text representations into a shared space using a crossmodal projector (Q-Former-like) and contrastive objectives to align graph and text modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MolCA (crossmodal projector for molecule-text alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Learns a unified multimodal latent space for molecular graphs and their textual descriptions using a query-transformer (Q-Former) to produce compact molecular representations that align with text embeddings via contrastive losses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Survey indicates use of a Q-Former style module together with text encoders; specific LLM backbone not specified in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Paired molecular graphs and textual descriptions (datasets referenced in survey include paired corpora like PubChem descriptions and curated molecule-text pairs); exact sizes depend on referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Contrastive latent-space alignment between GNN-derived graph embeddings and LLM/text embeddings using a crossmodal projection (query tokens) and InfoNCE-style losses; optionally uses classification regularization when labels are available.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Unified embeddings usable for retrieval, property prediction, and zero-shot/few-shot classification; enables text-to-molecule retrieval and molecule-to-text retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Contrastive retrieval metrics, classification/regression metrics when labels present; survey reports use of contrastive retrieval loss and downstream task evaluations in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey reports MolCA achieves improved alignment of modalities and utility for retrieval and downstream classification tasks though exact numbers are in the cited paper rather than the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Survey references molecular-text paired datasets (e.g., PubChem, curated molecule-description corpora); MolCA trained/evaluated on such paired data as per cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Dependent on the availability and quality of molecule-text pairs; risk of over-reliance on one modality; scaling GNN capacity to match LLM representations is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared conceptually to naive contrastive alignment approaches (MoMu, MolFM) and to methods that use simpler readout; claims improved projector design (Q-Former) for crossmodal mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6017.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoMu / MoMu-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoMu (Molecule–Multimodal contrastive pretraining) and MoMu-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrastive pretraining approaches that align molecular graph encodings and natural language descriptions by constructing positive graph-text pairs and applying contrastive losses over multiple augmented views.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MoMu / MoMu-v2 (contrastive graph-text pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Train GNNs and text encoders jointly via contrastive learning using molecule-text pairs and multiple graph augmentations to obtain crossmodal embeddings for retrieval and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Survey does not specify a particular LLM backbone; uses text encoders compatible with contrastive pretraining (e.g., transformer-based encoders).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Paired molecular graphs and textual descriptions; the survey notes methods retrieve two sentences per molecule and apply graph augmentations to create four views for contrastive training (exact dataset sizes not specified in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Contrastive latent-space alignment (InfoNCE-style loss) across graph and text views; uses graph augmentation to create multiple positive views and negative sampling across batch or dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Aligned graph and text embeddings for retrieval, zero-shot/few-shot classification, and downstream supervised tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Contrastive retrieval metrics, downstream classification/regression metrics, and molecular retrieval benchmarks; survey summarizes approach but not specific numeric evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey reports that MoMu variants learn multimodal embeddings that improve molecule-text retrieval and downstream tasks compared to unimodal baselines; exact performance numbers are referenced in original works.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Paired molecule-text corpora (e.g., PubChem descriptions, curated molecular captions) as used in the cited MoMu papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires high-quality molecule-text pairings; negative sampling design and scale of GNN/embedding dimensions affect performance; modality imbalance risk.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Survey compares MoMu-style contrastive alignment with other crossmodal approaches (MolCA, MolFM, GIT-Mol), highlighting differences in projector design and use of augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6017.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIT-Mol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal foundation model that combines graph, image, and text modalities for molecular science, aiming to bridge molecule structures and natural language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GIT-Mol (multimodal molecular foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates GNN-based graph encoders, image encoders, and text transformers into a unified multimodal model through contrastive and alignment objectives to support retrieval, generation, and prediction tasks in chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Survey cites a multimodal foundation approach; specific LLM backbone for text modality not specified in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Multimodal datasets containing molecular graphs, associated images (when available), and textual descriptions; dataset scales and exact sizes are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Contrastive alignment and multimodal pretraining to distill joint representations across graph, image, and text modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multimodal embeddings and generation capabilities (text generation conditioned on molecules, retrieval, and prediction outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Retrieval metrics, generation quality (BLEU/other), molecular property prediction metrics, task-specific evaluations referenced in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey indicates improved downstream multimodal performance when aligning graph, image, and text, enabling richer molecule-language capabilities; survey provides qualitative summary only.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Multimodal molecular corpora combining PubChem-like databases with images and text where available (as reported in the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Complex data requirements (images + text + graphs), scaling multimodal alignments, and ensuring chemical validity in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Positioned against unimodal and bimodal molecule-text models (MoMu, MolCA) with emphasis on benefits of adding image modality for richer representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6017.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-style system that augments LLMs with external chemistry tools (e.g., cheminformatics toolkits) to improve chemical reasoning, reduce hallucinations, and perform tool-based retrieval and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chem-Crow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemCrow (tool-augmented chemical agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Wraps LLMs with calls to external chemistry tools and retrieval systems so the model can propose actions, call tools for validation or computation, and integrate tool outputs into final answers—aimed at more reliable chemical reasoning and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Survey references the concept; underlying LLMs can be general-purpose models (e.g., GPT-family) but survey does not prescribe a specific model.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Natural language chemistry queries, potentially multiple documents or retrieved facts; not primarily a bulk literature distillation pipeline but intended to use tools for trustworthy responses.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a pure distillation pipeline; instead uses tool-in-the-loop execution (external APIs) and retrieval to ground LLM outputs, enabling synthesis of knowledge with tool-validated steps.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Tool-validated answers, stepwise plans (e.g., synthesis planning), and improved factual chemical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Domain-specific correctness checks, chemical validity, human/expert evaluation, and task-oriented benchmarks reported in cited work (survey references conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey cites ChemCrow as an approach to mitigate hallucination and perform more reliable chemistry tasks by combining LLM reasoning with external tools; detailed metrics are in the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>No single dataset; evaluation uses domain-specific tasks and tool outputs (e.g., reaction prediction, synthesis planning datasets) as per the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires robust tool integration, reliable retrieval, and careful orchestration to avoid cascading errors; not a turnkey literature-distillation system for many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to pure LLM inference, tool-augmented agents improve factual correctness but add engineering complexity; contrasted in survey with LLM-only and joint graph-text approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6017.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5 / Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 and Text+Chem T5 (T5-based encoder-decoder models for molecule-text translation and generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-decoder transformer models (T5-based) adapted for molecule-text translation, enabling generation tasks such as molecule captioning and text-conditioned molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolT5 (discussed as MolT5) / Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MolT5 / Text+Chem T5 (encoder-decoder LLMs for molecule-text tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses an encoder-decoder LLM (initialized from T5) trained on both natural language corpora and SMILES corpora to perform translation/generation between molecule representations and text descriptions, supporting property prediction and generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>T5 (encoder-decoder) initialized checkpoints (as reported in the survey for MolT5/Text+Chem T5 adaptations).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Pretraining on C4 natural language corpus and large SMILES corpora (e.g., one million SMILES); downstream inputs include concatenated molecule SMILES and text prompts for generation or prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Pretraining as multilingual-like translation between SMILES and text, fine-tuning on paired molecule-text tasks; framing property prediction as generation where appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated textual descriptions, SMILES (molecule generation), and prediction outputs via language modeling objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Generation metrics (BLEU, validity/novelty/uniqueness for molecules), regression/classification metrics for property prediction; survey notes typical evaluation modalities but not numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey reports MolT5 and Text+Chem T5 enable molecule-to-text and text-to-molecule tasks, leveraging encoder-decoder generation strengths; limitations include sensitivity to linearized representations and need for domain-specific prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>C4 corpus for natural language, large SMILES corpora for molecule modality; paired datasets for fine-tuning referenced in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>SMILES syntactic fragility leading to invalid molecules unless specialized encodings (SELFIES) are used; pretraining mismatch if new linearizations are not included in base corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to encoder-only and decoder-only approaches (SMILES-BERT, MolGPT), encoder-decoder models are more flexible for generation tasks but require careful context design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6017.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReLM: Leveraging language models for enhanced chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that leverages LLMs to augment chemical reaction prediction by using GNNs to propose candidate molecules and then using LLM-based multiple-choice or reasoning templates to select or re-rank candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReLM: Leveraging language models for enhanced chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReLM (LLM-augmented reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses GNNs to suggest top-k molecular candidates and constructs LLM-friendly multiple-choice contexts (or in-context learning prompts) so LLMs can reason or select among candidates, effectively combining graph search with LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Survey cites LLM usage for multiple-choice/in-context reasoning; specific LLM backbone not fixed in survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Top-k candidate sets from GNN retrieval plus textual context; not a bulk literature synthesis pipeline but a hybrid retrieval + LLM reasoning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Uses GNN-suggested candidates to create LLM in-context prompts or multiple-choice contexts, effectively distilling structured graph search into LLM reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Selected reactions/candidates or ranked outputs; textual reasoning traces when chain-of-thought/in-context prompting is used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Reaction prediction accuracy, ranking metrics; survey references ReLM's design without quantitative details.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey indicates ReLM uses hybrid GNN+LLM design to improve candidate selection and leverage LLM reasoning for chemistry tasks; specific performance is in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Chemical reaction prediction datasets used in the cited work (not enumerated in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reliance on candidate generation quality and LLM prompt design; not a substitute for large-scale literature distillation but useful for combining structured search with LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Contrasted with pure GNN or pure LLM approaches; hybrid yields interpretability (candidate lists) and improved reasoning when combined.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6017.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6017.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text2Mol / Prot2Text / GIMLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text2Mol, Prot2Text, GIMLET (graph-text interaction models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative graph-empowered LLM architectures that implement cross-attention or joint position encodings to integrate graph tokens and text tokens within Transformer layers for molecule or protein text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text2Mol: Cross-modal molecule retrieval with natural language queries / Prot2Text: Multimodal protein's function generation with GNNs and transformers / GIMLET: A unified graph-text model for instruction-based molecule zero-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Text2Mol / Prot2Text / GIMLET (graph-empowered LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Designs that modify Transformer attention/positional encoding to jointly encode node tokens and text tokens (e.g., treat graph nodes as tokens with graph-aware positional encodings or cross-attention layers) so LLMs can natively reason about graphs and paired text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Survey references general Transformer/LMM backbones; specific LLM variants depend on the cited work (e.g., encoder-decoder or encoder-only models used in different experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Joint sequences composed of node tokens (graph nodes) and text tokens (graph-level or document text), with graph positional or shortest-path based encodings; dataset sizes vary by task (molecule/protein corpora referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Architectural integration (graph tokens, asymmetric multi-head attention, cross-attention) rather than post-hoc distillation; alignment occurs inside model layers to combine graph and text signals.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Joint graph-text embeddings, generation of text conditioned on graphs (captioning), and improved zero-shot/few-shot prediction on graph-captioned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Retrieval and generation metrics, property prediction/AUC/MAE as applicable, and zero-shot benchmarks for molecule/protein language tasks cited in original works.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey reports that joint-encoding approaches can better capture graph-text interactions and enable zero-shot capabilities in molecular and protein tasks; specifics are in the cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Paired molecule/protein text datasets, PubChem/biomedical corpora, and specialized benchmarks referenced in the cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Architectural complexity, requirement for careful positional encoding design, limited pretraining on such joint sequences (data mismatch), and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to Graph-as-Sequence and contrastive alignment approaches; joint-encoding tends to give stronger joint representations at higher computation and engineering cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter <em>(Rating: 2)</em></li>
                <li>MoMu-v2 <em>(Rating: 2)</em></li>
                <li>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text <em>(Rating: 2)</em></li>
                <li>Chem-Crow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>MolT5 (MolT5 / Text+Chem T5) <em>(Rating: 1)</em></li>
                <li>ReLM: Leveraging language models for enhanced chemical reaction prediction <em>(Rating: 1)</em></li>
                <li>Text2Mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 1)</em></li>
                <li>Prot2Text: Multimodal protein's function generation with GNNs and transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6017",
    "paper_id": "paper-265659184",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "Galactica",
            "name_full": "Galactica",
            "brief_description": "A large language model pretrained for scientific domains, including chemistry, with the goal of encoding scientific knowledge and assisting scientific tasks such as molecule understanding and text generation in science.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "mention",
            "system_name": "Galactica (LLM for science)",
            "system_description": "A domain-targeted LLM pretrained on large scientific corpora (including chemical compound data) to encode and generate scientific knowledge; intended to be used as a scientific knowledge source and for downstream tasks such as molecule understanding, captioning, and QA.",
            "llm_model_used": "Galactica (proprietary architecture described as a large decoder model pretrained on scientific corpora)",
            "input_type_and_size": "Large-scale scientific corpora including text and two million compounds from PubChem (as reported in the survey); inputs are natural language queries or concatenated molecule textual representations (e.g., SMILES) and text context.",
            "distillation_approach": "Pretraining on large scientific corpora and instruction tuning with domain data; leverages autoregressive text modeling to absorb knowledge from many documents rather than explicit knowledge graph construction.",
            "output_type": "Generated text in scientific style (answers, summaries, molecule-related text), contextual representations for downstream tasks.",
            "evaluation_methods": "Survey reports use of downstream task evaluation (e.g., molecule-related classification/generation tasks), qualitative inspection for scientific knowledge; standard generation metrics and task-specific metrics (not fully enumerated in survey).",
            "results": "Survey notes Galactica pretrained on large scientific datasets and able to represent molecular information via SMILES; can be adapted with instruction tuning for domain tasks, but also subject to known LLM limitations (hallucination, sensitivity to linearized molecule format).",
            "datasets_or_benchmarks": "PubChem (two million compounds) and broader scientific corpora used for pretraining as mentioned in the survey.",
            "challenges_or_limitations": "Limited by linearized-graph (SMILES) representation issues, hallucination, and the mismatch between string-based molecule encodings and graph structure; not a solved general-purpose scientific theory distillation method.",
            "comparisons_to_other_methods": "Compared qualitatively to other molecular LLM approaches (encoder-decoder and GNN+LLM hybrids); survey contrasts LLM-only pretraining vs. joint graph-text alignment approaches (e.g., contrastive GNN+LLM methods).",
            "uuid": "e6017.0",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Mol-instructions",
            "name_full": "Mol-instructions (dataset)",
            "brief_description": "A large-scale biomolecular instruction dataset created for instruction-tuning LLMs on biomolecular tasks (molecule-text tasks and molecule reasoning).",
            "citation_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "mention_or_use": "mention",
            "system_name": "Mol-instructions (instruction dataset)",
            "system_description": "A dataset of instruction-style examples pairing molecules and textual tasks intended to instruction-tune LLMs to perform molecule-centered tasks and reasoning over biomedical text and molecule descriptions.",
            "llm_model_used": "Not specified in survey (designed to be used for instruction-tuning a variety of LLMs)",
            "input_type_and_size": "Large corpus of molecule-text instruction examples (survey references the dataset but does not enumerate exact size within the main text).",
            "distillation_approach": "Instruction tuning: providing LLMs with many molecule-oriented instruction examples so the model learns to map textual instructions and molecular representations to outputs (generation, property prediction, explanation).",
            "output_type": "Instruction-following outputs: generated text answers, molecule captions, translation between molecule and text, or task-specific outputs.",
            "evaluation_methods": "Downstream molecular tasks, standard generation metrics and domain-specific evaluations (validity/novelty for generative tasks), human or expert evaluation for chemical plausibility (survey cites these general evaluation modalities).",
            "results": "Survey cites Mol-instructions as a resource enabling instruction-tuning for molecule tasks, improving LLM performance on domain tasks when used for tuning; exact quantitative results not provided in the survey text.",
            "datasets_or_benchmarks": "Mol-instructions itself (as a dataset); used alongside molecular corpora such as PubChem, ChEMBL in related works according to survey.",
            "challenges_or_limitations": "Quality and alignment of instruction examples to chemical correctness; LLMs still vulnerable to hallucination and syntactic issues with SMILES/line notation unless paired with robust representations.",
            "comparisons_to_other_methods": "Positioned as complementary to graph-aware methods (GNN+LLM) and contrastive alignment approaches; enables LLM-only approaches but does not replace graph-structured encoders.",
            "uuid": "e6017.1",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MolCA",
            "name_full": "MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter",
            "brief_description": "A cross-modal modeling method that projects molecular graph representations and text representations into a shared space using a crossmodal projector (Q-Former-like) and contrastive objectives to align graph and text modalities.",
            "citation_title": "MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter",
            "mention_or_use": "mention",
            "system_name": "MolCA (crossmodal projector for molecule-text alignment)",
            "system_description": "Learns a unified multimodal latent space for molecular graphs and their textual descriptions using a query-transformer (Q-Former) to produce compact molecular representations that align with text embeddings via contrastive losses.",
            "llm_model_used": "Survey indicates use of a Q-Former style module together with text encoders; specific LLM backbone not specified in the survey text.",
            "input_type_and_size": "Paired molecular graphs and textual descriptions (datasets referenced in survey include paired corpora like PubChem descriptions and curated molecule-text pairs); exact sizes depend on referenced work.",
            "distillation_approach": "Contrastive latent-space alignment between GNN-derived graph embeddings and LLM/text embeddings using a crossmodal projection (query tokens) and InfoNCE-style losses; optionally uses classification regularization when labels are available.",
            "output_type": "Unified embeddings usable for retrieval, property prediction, and zero-shot/few-shot classification; enables text-to-molecule retrieval and molecule-to-text retrieval.",
            "evaluation_methods": "Contrastive retrieval metrics, classification/regression metrics when labels present; survey reports use of contrastive retrieval loss and downstream task evaluations in referenced work.",
            "results": "Survey reports MolCA achieves improved alignment of modalities and utility for retrieval and downstream classification tasks though exact numbers are in the cited paper rather than the survey.",
            "datasets_or_benchmarks": "Survey references molecular-text paired datasets (e.g., PubChem, curated molecule-description corpora); MolCA trained/evaluated on such paired data as per cited work.",
            "challenges_or_limitations": "Dependent on the availability and quality of molecule-text pairs; risk of over-reliance on one modality; scaling GNN capacity to match LLM representations is challenging.",
            "comparisons_to_other_methods": "Compared conceptually to naive contrastive alignment approaches (MoMu, MolFM) and to methods that use simpler readout; claims improved projector design (Q-Former) for crossmodal mapping.",
            "uuid": "e6017.2",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MoMu / MoMu-v2",
            "name_full": "MoMu (Molecule–Multimodal contrastive pretraining) and MoMu-v2",
            "brief_description": "Contrastive pretraining approaches that align molecular graph encodings and natural language descriptions by constructing positive graph-text pairs and applying contrastive losses over multiple augmented views.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MoMu / MoMu-v2 (contrastive graph-text pretraining)",
            "system_description": "Train GNNs and text encoders jointly via contrastive learning using molecule-text pairs and multiple graph augmentations to obtain crossmodal embeddings for retrieval and downstream tasks.",
            "llm_model_used": "Survey does not specify a particular LLM backbone; uses text encoders compatible with contrastive pretraining (e.g., transformer-based encoders).",
            "input_type_and_size": "Paired molecular graphs and textual descriptions; the survey notes methods retrieve two sentences per molecule and apply graph augmentations to create four views for contrastive training (exact dataset sizes not specified in the survey).",
            "distillation_approach": "Contrastive latent-space alignment (InfoNCE-style loss) across graph and text views; uses graph augmentation to create multiple positive views and negative sampling across batch or dataset.",
            "output_type": "Aligned graph and text embeddings for retrieval, zero-shot/few-shot classification, and downstream supervised tasks.",
            "evaluation_methods": "Contrastive retrieval metrics, downstream classification/regression metrics, and molecular retrieval benchmarks; survey summarizes approach but not specific numeric evaluations.",
            "results": "Survey reports that MoMu variants learn multimodal embeddings that improve molecule-text retrieval and downstream tasks compared to unimodal baselines; exact performance numbers are referenced in original works.",
            "datasets_or_benchmarks": "Paired molecule-text corpora (e.g., PubChem descriptions, curated molecular captions) as used in the cited MoMu papers.",
            "challenges_or_limitations": "Requires high-quality molecule-text pairings; negative sampling design and scale of GNN/embedding dimensions affect performance; modality imbalance risk.",
            "comparisons_to_other_methods": "Survey compares MoMu-style contrastive alignment with other crossmodal approaches (MolCA, MolFM, GIT-Mol), highlighting differences in projector design and use of augmentation.",
            "uuid": "e6017.3",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GIT-Mol",
            "name_full": "GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text",
            "brief_description": "A multimodal foundation model that combines graph, image, and text modalities for molecular science, aiming to bridge molecule structures and natural language descriptions.",
            "citation_title": "GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text",
            "mention_or_use": "mention",
            "system_name": "GIT-Mol (multimodal molecular foundation model)",
            "system_description": "Integrates GNN-based graph encoders, image encoders, and text transformers into a unified multimodal model through contrastive and alignment objectives to support retrieval, generation, and prediction tasks in chemistry.",
            "llm_model_used": "Survey cites a multimodal foundation approach; specific LLM backbone for text modality not specified in survey excerpt.",
            "input_type_and_size": "Multimodal datasets containing molecular graphs, associated images (when available), and textual descriptions; dataset scales and exact sizes are in the cited work.",
            "distillation_approach": "Contrastive alignment and multimodal pretraining to distill joint representations across graph, image, and text modalities.",
            "output_type": "Multimodal embeddings and generation capabilities (text generation conditioned on molecules, retrieval, and prediction outputs).",
            "evaluation_methods": "Retrieval metrics, generation quality (BLEU/other), molecular property prediction metrics, task-specific evaluations referenced in original work.",
            "results": "Survey indicates improved downstream multimodal performance when aligning graph, image, and text, enabling richer molecule-language capabilities; survey provides qualitative summary only.",
            "datasets_or_benchmarks": "Multimodal molecular corpora combining PubChem-like databases with images and text where available (as reported in the cited paper).",
            "challenges_or_limitations": "Complex data requirements (images + text + graphs), scaling multimodal alignments, and ensuring chemical validity in generation.",
            "comparisons_to_other_methods": "Positioned against unimodal and bimodal molecule-text models (MoMu, MolCA) with emphasis on benefits of adding image modality for richer representations.",
            "uuid": "e6017.4",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large-language models with chemistry tools",
            "brief_description": "An agent-style system that augments LLMs with external chemistry tools (e.g., cheminformatics toolkits) to improve chemical reasoning, reduce hallucinations, and perform tool-based retrieval and validation.",
            "citation_title": "Chem-Crow: Augmenting large-language models with chemistry tools",
            "mention_or_use": "mention",
            "system_name": "ChemCrow (tool-augmented chemical agent)",
            "system_description": "Wraps LLMs with calls to external chemistry tools and retrieval systems so the model can propose actions, call tools for validation or computation, and integrate tool outputs into final answers—aimed at more reliable chemical reasoning and planning.",
            "llm_model_used": "Survey references the concept; underlying LLMs can be general-purpose models (e.g., GPT-family) but survey does not prescribe a specific model.",
            "input_type_and_size": "Natural language chemistry queries, potentially multiple documents or retrieved facts; not primarily a bulk literature distillation pipeline but intended to use tools for trustworthy responses.",
            "distillation_approach": "Not a pure distillation pipeline; instead uses tool-in-the-loop execution (external APIs) and retrieval to ground LLM outputs, enabling synthesis of knowledge with tool-validated steps.",
            "output_type": "Tool-validated answers, stepwise plans (e.g., synthesis planning), and improved factual chemical outputs.",
            "evaluation_methods": "Domain-specific correctness checks, chemical validity, human/expert evaluation, and task-oriented benchmarks reported in cited work (survey references conceptually).",
            "results": "Survey cites ChemCrow as an approach to mitigate hallucination and perform more reliable chemistry tasks by combining LLM reasoning with external tools; detailed metrics are in the original paper.",
            "datasets_or_benchmarks": "No single dataset; evaluation uses domain-specific tasks and tool outputs (e.g., reaction prediction, synthesis planning datasets) as per the cited work.",
            "challenges_or_limitations": "Requires robust tool integration, reliable retrieval, and careful orchestration to avoid cascading errors; not a turnkey literature-distillation system for many papers.",
            "comparisons_to_other_methods": "Compared to pure LLM inference, tool-augmented agents improve factual correctness but add engineering complexity; contrasted in survey with LLM-only and joint graph-text approaches.",
            "uuid": "e6017.5",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MolT5 / Text+Chem T5",
            "name_full": "MolT5 and Text+Chem T5 (T5-based encoder-decoder models for molecule-text translation and generation)",
            "brief_description": "Encoder-decoder transformer models (T5-based) adapted for molecule-text translation, enabling generation tasks such as molecule captioning and text-conditioned molecule generation.",
            "citation_title": "MolT5 (discussed as MolT5) / Text+Chem T5",
            "mention_or_use": "mention",
            "system_name": "MolT5 / Text+Chem T5 (encoder-decoder LLMs for molecule-text tasks)",
            "system_description": "Uses an encoder-decoder LLM (initialized from T5) trained on both natural language corpora and SMILES corpora to perform translation/generation between molecule representations and text descriptions, supporting property prediction and generation tasks.",
            "llm_model_used": "T5 (encoder-decoder) initialized checkpoints (as reported in the survey for MolT5/Text+Chem T5 adaptations).",
            "input_type_and_size": "Pretraining on C4 natural language corpus and large SMILES corpora (e.g., one million SMILES); downstream inputs include concatenated molecule SMILES and text prompts for generation or prediction.",
            "distillation_approach": "Pretraining as multilingual-like translation between SMILES and text, fine-tuning on paired molecule-text tasks; framing property prediction as generation where appropriate.",
            "output_type": "Generated textual descriptions, SMILES (molecule generation), and prediction outputs via language modeling objectives.",
            "evaluation_methods": "Generation metrics (BLEU, validity/novelty/uniqueness for molecules), regression/classification metrics for property prediction; survey notes typical evaluation modalities but not numeric results.",
            "results": "Survey reports MolT5 and Text+Chem T5 enable molecule-to-text and text-to-molecule tasks, leveraging encoder-decoder generation strengths; limitations include sensitivity to linearized representations and need for domain-specific prompts.",
            "datasets_or_benchmarks": "C4 corpus for natural language, large SMILES corpora for molecule modality; paired datasets for fine-tuning referenced in the survey.",
            "challenges_or_limitations": "SMILES syntactic fragility leading to invalid molecules unless specialized encodings (SELFIES) are used; pretraining mismatch if new linearizations are not included in base corpora.",
            "comparisons_to_other_methods": "Compared to encoder-only and decoder-only approaches (SMILES-BERT, MolGPT), encoder-decoder models are more flexible for generation tasks but require careful context design.",
            "uuid": "e6017.6",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ReLM",
            "name_full": "ReLM: Leveraging language models for enhanced chemical reaction prediction",
            "brief_description": "A method that leverages LLMs to augment chemical reaction prediction by using GNNs to propose candidate molecules and then using LLM-based multiple-choice or reasoning templates to select or re-rank candidates.",
            "citation_title": "ReLM: Leveraging language models for enhanced chemical reaction prediction",
            "mention_or_use": "mention",
            "system_name": "ReLM (LLM-augmented reaction prediction)",
            "system_description": "Uses GNNs to suggest top-k molecular candidates and constructs LLM-friendly multiple-choice contexts (or in-context learning prompts) so LLMs can reason or select among candidates, effectively combining graph search with LLM reasoning.",
            "llm_model_used": "Survey cites LLM usage for multiple-choice/in-context reasoning; specific LLM backbone not fixed in survey summary.",
            "input_type_and_size": "Top-k candidate sets from GNN retrieval plus textual context; not a bulk literature synthesis pipeline but a hybrid retrieval + LLM reasoning approach.",
            "distillation_approach": "Uses GNN-suggested candidates to create LLM in-context prompts or multiple-choice contexts, effectively distilling structured graph search into LLM reasoning steps.",
            "output_type": "Selected reactions/candidates or ranked outputs; textual reasoning traces when chain-of-thought/in-context prompting is used.",
            "evaluation_methods": "Reaction prediction accuracy, ranking metrics; survey references ReLM's design without quantitative details.",
            "results": "Survey indicates ReLM uses hybrid GNN+LLM design to improve candidate selection and leverage LLM reasoning for chemistry tasks; specific performance is in the referenced work.",
            "datasets_or_benchmarks": "Chemical reaction prediction datasets used in the cited work (not enumerated in survey).",
            "challenges_or_limitations": "Reliance on candidate generation quality and LLM prompt design; not a substitute for large-scale literature distillation but useful for combining structured search with LLM reasoning.",
            "comparisons_to_other_methods": "Contrasted with pure GNN or pure LLM approaches; hybrid yields interpretability (candidate lists) and improved reasoning when combined.",
            "uuid": "e6017.7",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Text2Mol / Prot2Text / GIMLET",
            "name_full": "Text2Mol, Prot2Text, GIMLET (graph-text interaction models)",
            "brief_description": "Representative graph-empowered LLM architectures that implement cross-attention or joint position encodings to integrate graph tokens and text tokens within Transformer layers for molecule or protein text tasks.",
            "citation_title": "Text2Mol: Cross-modal molecule retrieval with natural language queries / Prot2Text: Multimodal protein's function generation with GNNs and transformers / GIMLET: A unified graph-text model for instruction-based molecule zero-shot learning",
            "mention_or_use": "mention",
            "system_name": "Text2Mol / Prot2Text / GIMLET (graph-empowered LLMs)",
            "system_description": "Designs that modify Transformer attention/positional encoding to jointly encode node tokens and text tokens (e.g., treat graph nodes as tokens with graph-aware positional encodings or cross-attention layers) so LLMs can natively reason about graphs and paired text.",
            "llm_model_used": "Survey references general Transformer/LMM backbones; specific LLM variants depend on the cited work (e.g., encoder-decoder or encoder-only models used in different experiments).",
            "input_type_and_size": "Joint sequences composed of node tokens (graph nodes) and text tokens (graph-level or document text), with graph positional or shortest-path based encodings; dataset sizes vary by task (molecule/protein corpora referenced).",
            "distillation_approach": "Architectural integration (graph tokens, asymmetric multi-head attention, cross-attention) rather than post-hoc distillation; alignment occurs inside model layers to combine graph and text signals.",
            "output_type": "Joint graph-text embeddings, generation of text conditioned on graphs (captioning), and improved zero-shot/few-shot prediction on graph-captioned tasks.",
            "evaluation_methods": "Retrieval and generation metrics, property prediction/AUC/MAE as applicable, and zero-shot benchmarks for molecule/protein language tasks cited in original works.",
            "results": "Survey reports that joint-encoding approaches can better capture graph-text interactions and enable zero-shot capabilities in molecular and protein tasks; specifics are in the cited literature.",
            "datasets_or_benchmarks": "Paired molecule/protein text datasets, PubChem/biomedical corpora, and specialized benchmarks referenced in the cited papers.",
            "challenges_or_limitations": "Architectural complexity, requirement for careful positional encoding design, limited pretraining on such joint sequences (data mismatch), and computational cost.",
            "comparisons_to_other_methods": "Compared to Graph-as-Sequence and contrastive alignment approaches; joint-encoding tends to give stronger joint representations at higher computation and engineering cost.",
            "uuid": "e6017.8",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2,
            "sanitized_title": "molinstructions_a_largescale_biomolecular_instruction_dataset_for_large_language_models"
        },
        {
            "paper_title": "MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter",
            "rating": 2,
            "sanitized_title": "molca_molecular_graphlanguage_modeling_with_crossmodal_projector_and_unimodal_adapter"
        },
        {
            "paper_title": "MoMu-v2",
            "rating": 2
        },
        {
            "paper_title": "GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text",
            "rating": 2,
            "sanitized_title": "gitmol_a_multimodal_large_language_model_for_molecular_science_with_graph_image_and_text"
        },
        {
            "paper_title": "Chem-Crow: Augmenting large-language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "MolT5 (MolT5 / Text+Chem T5)",
            "rating": 1,
            "sanitized_title": "molt5_molt5_textchem_t5"
        },
        {
            "paper_title": "ReLM: Leveraging language models for enhanced chemical reaction prediction",
            "rating": 1,
            "sanitized_title": "relm_leveraging_language_models_for_enhanced_chemical_reaction_prediction"
        },
        {
            "paper_title": "Text2Mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 1,
            "sanitized_title": "text2mol_crossmodal_molecule_retrieval_with_natural_language_queries"
        },
        {
            "paper_title": "Prot2Text: Multimodal protein's function generation with GNNs and transformers",
            "rating": 1,
            "sanitized_title": "prot2text_multimodal_proteins_function_generation_with_gnns_and_transformers"
        }
    ],
    "cost": 0.022967249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>27 September 2024</p>
<p>Bowen Jin bowenj4@illinois.edu 
University of Illinois at Urbana-Champaign
61820ChampaignILUSA</p>
<p>Gang Liu 
University of Illinois at Urbana-Champaign
61820ChampaignILUSA</p>
<p>Jin ) Bowen 
University of Illinois at Urbana-Champaign
61820ChampaignILUSA</p>
<p>University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Chi Han chihan3@illinois.edu 
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Heng Ji hengji@illinois.edu 
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Jiawei Han hanj@illinois.edu 
University of Notre Dame
Notre Dame
46556INUSA
27 September 2024C15CD5B6A334EB385E0BB8D58A98792010.1109/TKDE.2024.3469578Received 1 February 2024; revised 16 June 2024; accepted 10 September 2024.</p>
<p>Abstract-Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning).While LLMs are mainly designed to process pure texts, there are many realworld scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions).Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning).In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs.We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs.We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models.Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets.Finally, we conclude with potential future research directions in this fast-growing field.</p>
<p>Index Terms-Graph neural networks, graph representation learning, large language models (LLMs), natural language processing.</p>
<p>I. INTRODUCTION</p>
<p>L ARGE language models (LLMs) (e.g., BERT [23], T5 [29],</p>
<p>LLaMA [118]) which represents a direction of everincreasing models' sizes pre-trained on larger corpora, have demonstrated powerful capabilities in solving natural language processing (NLP) tasks, including question answering [1], text generation [2] and document understanding [3].There are no clear and static thresholds regarding the model sizes.Early LLMs (e.g., BERT [23], RoBERTa [24]) adopt an encoderonly architecture and show capabilities in text representation learning [4] and natural language understanding [3].In recent years, more focus has been given to larger decoder-only architectures [118] or encoder-decoder architectures [29].As the model size scales up, such LLMs have also shown reasoning ability and even more advanced emergent ability [5], exposing a strong potential for Artificial General Intelligence (AGI).</p>
<p>While LLMs are extensively applied to process pure texts, there is an increasing number of applications where the text data are associated with structure information which are represented in the form of graphs.As presented in Fig. 1, in academic networks, papers (with title and description) and authors (with profile text), are interconnected with authorship relationships.Understanding both the author/paper's text information and author-paper structure information on such graphs can contribute to advanced author/paper modeling and accurate recommendations for collaboration; In the scientific domain, molecules are represented as graphs and are often paired with text that describes their basic properties (e.g., mass and weight).Joint modeling of both the molecule structure (graph) and the associated rich knowledge (text) is important for deeper molecule understanding.Since LLMs are mainly proposed for modeling texts that lie in a sequential fashion, those scenarios mentioned above pose new challenges on how to enable LLMs to encode the structure information on graphs.In addition, since LLMs have demonstrated their superb text-based reasoning ability, it is promising to explore whether they have the potential to address fundamental graph reasoning problems on pure graphs.These graph reasoning tasks include inferring connectivity [6], shortest path [7], subgraph matching [8], and logical rule induction [18].</p>
<p>Recently, there has been an increasing interest [9] in extending LLMs for graph-based applications (summarized in Fig. 1).According to the relationship between graph and text presented Fig. 1.According to the relationship between graph and text, we categorize three LLM on graph scenarios.Depending on the role of LLM, we summarize three LLM-on-graph techniques."LLM as Predictor" is where LLMs are responsible for predicting the final answer."LLM as Aligner" will align the inputs-output pairs with those of GNNs."LLM as Encoder" refers to using LLMs to encode and obtain feature vectors. in Fig. 1, the application scenarios can be categorized into pure graphs, text-attributed graphs (nodes/edges are associated with texts), and text-paired graphs.Depending on the role of LLMs and their interaction with graph neural networks (GNNs), the LLM on graphs techniques can be classified into treating LLMs as the final component for prediction (LLM as Predictor), treating LLMs as the feature extractor for GNNs (LLM as Encoder), and align the latent space of LLMs with GNNs (LLM as Aligner).</p>
<p>There are a limited number of existing surveys exploring the intersection between LLMs and graphs.Related to deep learning on graphs, Liu et al. [20] discuss pretrained foundation models on graphs, including their backbone architectures, pretraining methods, and adaptation techniques.Pan et al. [21] review the connection between LLMs and knowledge graphs (KGs) especially on how KGs can enhance LLMs training and inference, and how LLMs can facilitate KG construction and reasoning.Mao et al. [203] and Li et al. [204] review LLM on graphs focusing on techniques rather than applications.In summary, existing surveys either focus more on GNNs rather than LLMs or fail to provide a systematic perspective on their applications in various graph scenarios as in Fig. 1.Our paper provides a comprehensive review of the LLMs on graphs for broader researchers from diverse backgrounds besides the computer science and machine learning community who want to enter this rapidly developing field (Fig. 2).</p>
<p>Our Contributions: The notable contributions of our paper are summarized as follows:</p>
<p>r Categorization of Graph Scenarios: We systematically summarize the graph scenarios where language models can be adopted into: pure graphs, text-attributed graphs, and text-paired graphs.</p>
<p>r Systematic Review of Techniques: We provide the most comprehensive overview of language models on graph techniques.For different graph scenarios, we summarize the representative models, provide detailed illustrations of each of them, and make necessary comparisons.</p>
<p>r Abundant Resources: We collect abundant resources on language models on graphs, including benchmark datasets, open-source codebases, and practical applications.</p>
<p>r Future Directions: We delve into the foundational prin- ciples of language models on graphs and propose six prospective avenues for future exploration.Organization of Survey: The rest of this survey is organized as follows.Section II-B introduces the background of LLMs and GNNs, lists commonly used notations, and defines related concepts.Section III categorizes graph scenarios where LLMs can be adopted and summarizes LLMs on graph techniques.Sections IV, V, and VI provides a detailed illustration of LLM methodologies for different graph scenarios.Section VII delivers available datasets, open-source codebases, and a collection of applications across various domains.Section VIII introduces some potential future directions.Section IX summarizes the paper.</p>
<p>II. DEFINITIONS &amp; BACKGROUND</p>
<p>A. Definitions</p>
<p>We provide definitions of various types of graphs and introduce the notations (as shown in Table I) in this section.</p>
<p>Definition 1 (Graph): A graph can be defined as G = (V, E).Here V signifies the set of nodes, while E denotes the set of edges.A specific node can be represented by v i ∈ V, and an edge directed from node v j to v i can be expressed as e ij = (v i , v j ) ∈ E. The set of nodes adjacent to a particular node v is articulated as
N (v) = {u ∈ V|(v, u) ∈ E}.
Definition 2 (Graph with node-level textual information): This type of graph can be denoted as G = (V, E, D), where V, E and D are node set, edge set, and text set, respectively.Each v i ∈ V is associated with some textual information d v i ∈ D. For instance, in an academic citation network, one can interpret v ∈ V as the scholarly articles, e ∈ E as the citation links between them, and d ∈ D as the textual content of these articles.A graph with node-level textual information is also called a text-attributed graph [31], a text-rich graph [61], or a textual graph [71].</p>
<p>Definition 3 (Graph with edge-level textual information): This type of graph can be denoted as G = (V, E, D).Each e ij ∈ E is associated with some textual information d e ij ∈ D. For example, in a social network, one can interpret v ∈ V as the users, e ∈ E as the interaction between the users, and d ∈ D as the textual content of the messages sent between the users.Such a graph is also called a textual-edge network [73].</p>
<p>Definition 4 (Graph with graph-level textual information): This type of graph can be denoted as the pair (G, d G ), where G = (V, E).V and E are node set and edge set.d G is the text set paired to the graph G.For instance, in a molecular graph G, v ∈ V denotes an atom, e ∈ E represents the strong attractive forces or chemical bonds that hold molecules together, and d G represents the textual description of the molecule.We note that texts may also be associated with subgraph-level concepts and then paired with the entire graph.Such a graph is also called a text-paired graph.</p>
<p>B. Background</p>
<p>(Large) Language Models: Language Models (LMs), or language modeling, is an area in the field of natural language processing (NLP) on understanding and generation from text distributions.In recent years, large language models (LLMs) have demonstrated impressive capabilities in tasks such as machine translation, text summarization, reasoning, and question answering [26], [42], [111], [112], [113], [114], [194], [208].</p>
<p>Language models have evolved significantly over time.BERT [23] marks significant progress in language modeling and representation.BERT models the conditional probability of a word given its bidirectional context, also named masked language modeling (MLM) objective :
E S∼D s i ∈S log p(s i |s 1 , . . . , s i−1 , s i+1 , . . . , s N S ) , (1)
where S is a sentence sampled from the corpus D, s i is the i-th word in the sentence, and N S is the length of the sentence.On the other hand, the objective of causal language modeling or text generation is defined as:
E S∼D s i ∈S log p(s i |s 1 , . . . , s i−1 ) . (2)
Following BERT, other masked language models are proposed, such as RoBERTa [24], ALBERT [115], and ELECTRA [116], with similar architectures and objectives of text representation.</p>
<p>Efforts have been made to combine language models with other modalities such as vision [95], [120] and biochemical structures [46], [121], [122].In this paper, we will discuss its combination with graphs.The lifecycle of an LLM usually involves some or all the following steps: pretraining, finetuning, and prompting.In pretraining, LLMs are usually trained on a larger corpus with multiple language modeling objectives [23], [26], [28], which aims to endow LLMs with strong language understanding and completion capability.If domain-specific abilities are expected, LLMs are then finetuned with a smaller amount of domainspecific data [36], [37], [38], [39], [42], [43].Human preference optimization methods are sometimes applied after this stage to align outputs better with users' intentions or social values [205], [206], [207].Finally, various prompting or prompt engineering techniques can be deployed to boost downstream task performance [47], [48], [49].A more comprehensive description can be found in Appendix A, available online</p>
<p>We would like to point out that the word "large" in LLM is not associated with a clear and static threshold to divide language models."Large" actually refers to a direction in which language models are inevitably evolving, and larger foundational models tend to possess significantly more representation and generalization power.Hence, we define LLMs to encompass both medium-scale PLMs, such as BERT, and large-scale LMs, like GPT-4, as suggested by [21].</p>
<p>Graph Neural Networks &amp; Graph Transformers:</p>
<p>In real-world scenarios, not all the data are sequential like text, many data lies in a more complex non-euclidean structure, i.e., graphs.GNN is proposed as a deep-learning architecture for graph data.Primary GNNs including GCN [83], GraphSAGE [84] and, GAT [85] are designed for solving node-level tasks.They mainly adopt a propagation-aggregation paradigm to obtain node representations:
h (l) v = AGG (l) h (l−1) v , PROP (l) {h (l−1) u | u ∈ N (v)} .
When propagation is global (u ∈ V), the Graph Transformer [140], [141] with attention-weighted node importance during sum aggregation can be defined.Let W Q , W K , W V be the query, key, and value matrices, respectively, and k exp denote the similarity between two nodes.Then, we have:
Attn(h (l−1) v ) = u∈V k exp (h (l−1) v , h (l−1) u ) w∈V k exp (h (l−1) v , h (l−1) w ) h (l−1) u W V ,
where k exp (h
(l−1) v , h (l) w ) = exp( h (l−1) v W Q h (l−1) w W K √ d K
).To solve graph-level tasks, GNN models like GIN [188] or Graph Transformers obtain graph representations using a READOUT function:
h G = READOUT({h v i | v i ∈ G}).
The READOUT functions include mean pooling, max pooling, and so on.Subsequent work on GNN tackles the issues of oversmoothing [138], over-squashing [139], interpretability [144], and bias [142].While message-passing-based GNNs excel in structure encoding, researchers aim to enhance their expressiveness with Graph Transformers.These models leverage global multi-head attention mechanisms and integrate graph inductive biases through positional encoding, structural encoding, combining message-passing with attention layers, or improving attention efficiency on large graphs.Graph Transformers have been proven to be a state-of-the-art solution for many pure graph problems.</p>
<p>Language Models Versus Graph Transformers: Modern language models and graph Transformers both use Transformers [92] as the base model architecture.This makes the two concepts hard to distinguish, especially when the language models are adopted on graph applications.In this paper, "Transformers" typically refers to Transformer language models for simplicity.Here, we provide three points to help distinguish them: 1) Tokens (word token versus node token): Transformers take a token sequence as inputs.For language models, the tokens are word tokens; while for graph Transformers, the tokens are node tokens.In those cases where tokens include both word tokens and node tokens if the backbone Transformers is pretrained on text corpus (e.g., BERT [23] and LLaMA [118]), we will call it a "language model".2) Positional Encoding (sequence versus graph): language models typically adopt the absolute or relative positional encoding considering the position of the word token in the sequence, while graph Transformers adopt shortest path distance [140], random walk distance, the eigenvalues of the graph Laplacian [141] to consider the distance of nodes in the graph.3) Goal (text versus graph): The language models are originally proposed for text encoding and generation; while graph Transformers are proposed for node encoding or graph encoding.In those cases where texts are served as nodes/edges on the graph if the backbone Transformers is pretrained on text corpus, we will call it a "language model".</p>
<p>III. CATEGORIZATION AND FRAMEWORK</p>
<p>In this section, we first introduce our categorization of graph scenarios where language models can be adopted.Then we discuss the categorization of LLM on graph techniques.Finally, Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>we summarize the training &amp; inference framework for language models on graphs.</p>
<p>A. Categorization of Graph Scenarios With LLMs</p>
<p>Pure Graphs without Textual Information are graphs with no text information or no semantically rich text information.Examples include traffic graphs and power transmission graphs.Those graphs often serve as context to test the graph reasoning ability of large language models (solve graph theory problems) or serve as knowledge sources to enhance the large language models (alleviate hallucination).</p>
<p>Text-Attributed Graphs refer to graphs where nodes or edges are associated with semantically rich text information.They are also called text-rich networks [31], textual graphs [71] or textual-edge networks [73].Examples include academic networks, e-commerce networks, social networks, and legal case networks.On these graphs, researchers are interested in learning representations for nodes or edges with both textual and structure information [71] [73].</p>
<p>Text-Paired Graphs have textual descriptions defined for the entire graph structure.For example, graphs like molecules may be paired with captions or textual features.While the graph structure significantly contributes to molecular properties, text descriptions can complement our understanding of molecules.The graph scenarios can be found in Fig. 1.</p>
<p>B. Categorization of LLMs on Graph Techniques</p>
<p>According to the roles of LLMs and what are the final components for solving graph-related problems, we classify LLM on graph techniques into three main categories:</p>
<p>LLM as Predictor: This category of methods serves LLM as the final component to output representations or predictions.It can be enhanced with GNNs and can be classified depending on how the graph information is injected into LLM: 1) Graph as Sequence: This type of method makes no changes to the LLM architecture, but makes it be aware of graph structure by taking a "graph token sequence" as input.The "graph token sequence" can be natural language descriptions for a graph or hidden representations outputted by graph encoders.2) Graph-Empowered LLM: This type of method modifies the architecture of the LLM base model (i.e., Transformers) and enables it to conduct joint text and graph encoding inside their architecture.</p>
<p>3) Graph-Aware LLM Finetuning: This type of method makes no changes to the input of the LLMs or LLM architectures, but only fine-tunes the LLMs with supervision from the graph.</p>
<p>LLM as Encoder: This method is mostly utilized for graphs where nodes or edges are associated with text information (solving node-level or edge-level tasks).GNNs are the final components and we adopt LLM as the initial text encoder.To be specific, LLMs are first utilized to encode the text associated with the nodes/edges.The outputted feature vectors by LLMs then serve as input embeddings for GNNs for graph structure encoding.The output embeddings from the GNNs are adopted as final node/edge representations for downstream tasks.However, these methods suffer from convergence issues, sparse data issues, and inefficient issues, where we summarize solutions from optimization, data augmentation, and knowledge distillation perspectives.</p>
<p>LLM as Aligner: This category of methods adopts LLMs as text-encoding components and aligns them with GNNs which serve as graph structure encoding components.LLMs and GNNs are adopted together as the final components for task solving.To be specific, the alignment between LLMs and GNNs can be categorized into 1) Prediction Alignment where the generated pseudo labels from one modality are utilized for training on the other modality in an iterative learning fashion and 2) Latent Space Alignment where contrastive learning is adopted to align text embeddings generated by LLMs and graph embeddings generated by GNNs.</p>
<p>In the following sections, we will follow our categorization in Section III and discuss detailed methodologies for each graph scenario.</p>
<p>IV. PURE GRAPHS</p>
<p>The study of pure graphs in graph theory is essential for understanding the introduction of LLMs into graph-related reasoning problems.Pure graphs are a universal representation format used to address a wide range of algorithmic problems in computer science.Many graph-based concepts, such as shortest paths, specific sub-graphs, and flow networks, are strongly connected to real-world applications [132], [133], [134], [192].Therefore, reasoning based on pure graphs is crucial for providing theoretical solutions and insights for real-world applications.</p>
<p>Nevertheless, many reasoning tasks require a computation capacity beyond traditional GNNs.GNNs are typically designed to carry out a bounded number of operations given a graph size.In contrast, graph reasoning problems can require up to indefinite complexity depending on the task's nature.On the other hand, LLMs demonstrate excellent emergent reasoning ability [47], [111], [112] recently.This is partially due to their autoregressive mechanism, which enables computing indefinite sequences of intermediate steps with careful prompting or training [47], [48].</p>
<p>The following subsections discuss the attempts to incorporate LLMs into pure graph reasoning problems.We will also discuss the corresponding challenges, limitations, and findings.Table 4 in the Appendix, available online lists a categorization of these efforts.Usually, input graphs are serialized as part of the input sequence, either by verbalizing the graph structure [123], [124], [125], [127], [128], [129], [130], [131] or by encoding the graph structure into implicit feature sequences [41].The studied reasoning problems range from simpler ones like connectivity, shortest paths, and cycle detection to harder ones like maximum flow and Hamiltonian pathfinding (an NP-complete problem).A comprehensive list of the studied problems is listed in Appendix Table 5, available online.Note that we only list representative problems here.This table does not include more domain-specific problems, such as the spatial-temporal reasoning problems in [127].We first briefly describe the approaches to formatting the graph inputs to be fed to LLMs.</p>
<p>Plainly Verbalizing Graphs: Verbalizing the graph structure in natural language is the most straightforward way of representing graphs.Representative approaches include describing the edge Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.and adjacency lists, widely studied in [123], [124], [127], [130].For example, for a triangle graph with three nodes, the edge list can be written as "[(0, 1), (1,2), (2,0)]", which means node 0 is connected to node 1, node 1 is connected to node 2, node 2 is connected to node 0. It can also be written in natural language such as "There is an edge between node 0 and node 1, an edge between node 1 and node 2, and an edge between node 2 and node 0." On the other hand, we can describe the adjacency list from the nodes' perspective.For example, for the same triangle graph, the adjacency list can be written as "Node 0 is connected to node 1 and node 2. Node 1 is connected to node 0 and node 2. Node 2 is connected to node 0 and node 1."</p>
<p>Paraphrasing Graphs: The verbalized graphs can be lengthy, unstructured, and complicated to read, even for humans, so they might not be the best input format for LLMs to infer the answers.To this end, researchers also attempt to paraphrase the graph structure into more natural or concise sentences.[125] find that by prompting LLMs to generate a format explanation of the raw graph inputs for itself (Format-Explanation) or to pretend to play a role in a natural task (Role Prompting), the performance on some problems can be improved but not systematically.[130] explores the effect of grounding the pure graph in a real-world scenario, such as social networks, friendship graphs, or co-authorship graphs.In such graphs, nodes are described as people, and edges are relationships between people.</p>
<p>Encoding Graphs Into Implicit Feature Sequences: Finally, researchers also attempt to encode the graph structure into implicit feature sequences as part of the input sequence [41].Unlike the previous verbalizing approaches, this usually involves training a graph encoder to encode the graph structure into a sequence of features and fine-tuning the LLMs to adapt to the new input format.</p>
<p>A. Direct Answering</p>
<p>Although graph-based reasoning problems usually involve complex computation, researchers still attempt to let language models directly generate answers from the serialized input graphs as a starting point, partially because of the simplicity of the approach and partially in awe of other emergent abilities of LLMs.Although various attempts have been made to optimize how graphs are presented in the input sequence discussed in the sections above, bounded by the finite sequence length and computational operations, this approach has a fundamental limitation to solving complex reasoning problems such as NP-complete ones.Unsurprisingly, most studies find that LLMs possess preliminary graph understanding ability, but the performance is less satisfactory on more complex problems or larger graphs [41], [123], [124], [125], [127], [130] where reasoning is necessary.</p>
<p>On plainly verbalized graphs, one can prompt LLMs to answer questions either in zero-shot or few-shot (in-context learning) settings.The former asks questions directly given the graph structure, while the latter asks questions about the graph structure after providing a few examples of questions and answers.[123], [124], [125] do confirm that LLMs can answer easier questions such as connectivity, neighbor identification, and graph size counting but fail to answer more complex questions such as cycle detection and Hamiltonian pathfinding.Their results also reveal that providing more examples in the few-shot setting increases the performance, especially on easier problems, although it is still not satisfactory.Results on paraphrased graphs indicate that encoding in real-world scenarios can improve performance on some problems, but it still cannot be done consistently.By encoding graphs into features, [41] demonstrates drastic performance improvement on problems including substructure counting, maximum triplet sum, shortest path, and bipartite matching.This indicates that fine-tuning LLMs has great fitting power on a specific task distribution.</p>
<p>B. Heuristic Reasoning</p>
<p>Direct mapping to the output leverages the LLMs' powerful representation power to "guess" the answers.Still, it does not fully utilize the LLMs' impressive emergent reasoning ability, which is essential for solving complex reasoning problems.To this end, attempts have been made to let LLMs perform heuristic reasoning on graphs.This approach encourages LLMs to perform a series of intermediate reasoning steps that might heuristically lead to the correct answer, which resembles a path-finding reasoning schema [202].</p>
<p>Reasoning Step by</p>
<p>Step: Encouraged by the success of chainof-thought (CoT) reasoning [47], [112], researchers also attempt to let LLMs perform reasoning step by step on graphs.Chainof-thought encourages LLMs to roll out a sequence of reasoning steps to solve a problem, similar to how humans solve problems.Zero-shot CoT is a similar approach that does not require any examples.These techniques are studied in [41], [123], [124], [125], [127], [130], [131].Results indicate that CoT-style reasoning can improve the performance on simpler problems, such as cycle detection and shortest path detection.Still, the improvement is inconsistent or diminishes on more complex problems, such as Hamiltonian path finding and topological sorting.</p>
<p>Retrieving Subgraphs as Evidence: Many graph reasoning problems, such as node degree counting and neighborhood detection, only involve reasoning on a subgraph of the whole graph.Such properties allow researchers to let LLMs retrieve the subgraphs as evidence and perform reasoning on the subgraphs.Build-a-Graph prompting [123] encourages LLMs to reconstruct the relevant graph structures and then perform reasoning on them.This method demonstrates promising results on problems except for Hamiltonian pathfinding, a notoriously tricky problem requiring reasoning on the whole graph.Another approach, Context-Summarization [125], encourages LLMs to summarize the key nodes, edges, or sub-graphs and perform reasoning.</p>
<p>Searching on Graphs: This kind of reasoning is related to the search algorithms on graphs, such as breadth-first search (BFS) and depth-first search (DFS) Although not universally applicable, BFS and DFS are the most intuitive and effective ways to solve some graph reasoning problems.Numerous explorations have been made to simulate searching-based reasoning, especially on knowledge-graph question answering.This approach enjoys the advantage of providing interpretable evidence besides the answer.Reasoning-on-Graphs (RoG) [128] is a representative approach that prompts LLMs to generate several relation paths as plans, which are then retrieved from the knowledge graph (KG) and used as evidence to answer the questions.Another approach is to iteratively retrieve and reason on the subgraphs from KG [129], [131], simulating a dynamic searching process.At each step, the LLMs retrieve neighbors of the current nodes and then decide to answer the question or continue the next search step.These methods address the scalability challenge when knowledge from multiple graphs is available.</p>
<p>C. Algorithmic Reasoning</p>
<p>The previous two approaches are heuristic, which means that the reasoning process accords with human intuition but is not guaranteed to lead to the correct answer.In contrast, these problems are usually solved by algorithms in computer science.Therefore, researchers also attempt to let LLMs perform algorithmic reasoning on graphs.[123] proposed "Algorithmic Prompting", which prompts the LLMs to recall the algorithms that are relevant to the questions and then perform reasoning step by step according to the algorithms.Their results, however, do not show consistent improvement over the heuristic reasoning approach.A more direct approach, Graph-ToolFormer [126], lets LLMs generate API calls as explicit reasoning steps.These API calls are then executed externally to acquire answers on an external graph.This approach is suitable for converting real-world tasks into pure graph reasoning problems, and it has demonstrated efficacy in various applications such as knowledge graphs, social networks, and recommendation systems.</p>
<p>D. Discussion</p>
<p>Despite the extensive research, there has not been a consensus about the best practice in graph representation in LLMs.The eventual solution to this problem should reach a perfect balance between computation efficiency and information completeness, probably drawing inspiration from long-context LLM researches [209], [210].The above reasoning methods are not mutually exclusive, and future efforts can be made to combine them to achieve better performance.For example, efficiency in algorithmic searching can be improved by prompting language models for better heuristics.</p>
<p>V. TEXT-ATTRIBUTED GRAPHS</p>
<p>Text-attributed graphs exist ubiquitously in the real world, e.g., academic networks, and legal case networks.Learning on such networks requires the model to encode both the textual information associated with the nodes/edges and the structure information lying inside the input graph.Depending on the role of LLM, existing works can be categorized into three types: LLM as Predictor, LLM as Encoder, and LLM as Aligner.We summarize all surveyed methods in Appendix Table 6, available online.</p>
<p>A. LLM as Predictor</p>
<p>These methods serve the language model as the main model architecture to capture both the text information and graph structure information.They can be categorized into three types: Graph as Sequence methods, Graph-Empowered LLMs, and Graph-Aware LLM finetuning methods, depending on how structure information in graphs is injected into language models (input versus architecture versus loss).In the Graph as Sequence methods, graphs are converted into sequences that can be understood by language models together with texts from the inputs.In the Graph-Empowered LLMs methods, people modify the architecture of Transformers (which is the base architecture for LLMs) to enable it to encode text and graph structure simultaneously.In the Graph-Aware LLM finetuning methods, LLM is fine-tuned with graph structure supervision and can generate graph-contextualized representations.</p>
<p>1) Graph as Sequence: In these methods, the graph information is mainly encoded into the LLM from the "input" side.The ego-graphs associated with nodes/edges are serialized into a sequence H G v which can be fed into the LLM together with the texts d v :
H G v = Graph2Seq(G v ),(3)h v = LLM([H G v , d v ]).(4)
Depending on the choice of Graph2Seq(•) function, the methods can be further categorized into rule-based methods and GNNbased methods.The illustration of the categories can be found in Fig. 3. Rule-Based: Linearizing Graphs into Text Sequence with Rules: These methods design rules to describe the structure with natural language and adopt a text prompt template as Graph2Seq(•).For example, given an ego-graph G v i of the paper node v i connecting to author nodes v j and v k and venue nodes v t and v s ,
H G v i = Graph2Seq(G v i ) = "
The centor paper node is v i .Its author neighbor nodes are v j and v k and its venue neighbor nodes are v t and v s ".This is the most straightforward and easiest way (without introducing extra model parameters) to encode graph structures into language models.Along this line, InstructGLM [45] designs templates to describe local egograph structure (maximum 3-hop connection) for each node and conduct instruction tuning for node classification and link prediction.GraphText [64] further proposes a syntax tree-based method to transfer structure into text sequence.Researchers [81] also study when and why the linearized structure information on graphs can improve the performance of LLM on node classification and find that the structure information is beneficial when the textual information associated with the node is scarce (in this case, the structure information can provide auxiliary information gain).</p>
<p>GNN-Based: Encoding Graphs into Special Tokens with GNNs: Different from rule-based methods which use natural language prompts to linearize graphs into sequences, GNN-based methods adopt graph encoder models (i.e., GNN) to encode the ego-graph associated with nodes into special token representations which are concatenated with the pure text information into Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.the language model:
H G v = Graph2Seq(G v ) = GraphEnc(G v ).(5)
The strength of these methods is they can capture hidden representations of useful structure information with a strong graph encoder, while the challenge is how to fill the gap between graph modality and text modality.GNP [40] adopts a similar philosophy from LLaVA [90], where they utilize GNN to generate graph tokens and then project the graph tokens into the text token space with learnable projection matrices.The projected graph tokens are concatenated with text tokens and fed into the language model.GraphGPT [44] further proposes to train a text-grounded GNN for the projection with a text encoder and contrastive learning.DGTL [75] introduces disentangled graph learning, serves graph representations as positional encoding, and adds them to the text sequence.METERN [74] adds learnable relation embeddings to node textual sequences for text-based multiplex representation learning on graphs [91].</p>
<p>2) Graph-Empowered LLMs: In these methods, researchers design advanced LLM architecture (i.e., Graph-Empowered LLMs) which can conduct joint text and graph encoding inside their model architecture.Transformers [92] serve as the base model for nowadays pretrained LMs [23] and LLMs [35].However, they are designed for natural language (sequence) encoding and do not take non-sequential structure information into consideration.To this end, Graph-Empowered LLMs are proposed.They have a shared philosophy of introducing virtual structure tokens H G v inside each Transformer layer:
H (l) d v = H (l) G v , H (l) d v (6)
where H G v can be learnable embeddings or output from graph encoders.Then the original multi-head attention (MHA) in Transformers is modified into an asymmetric MHA to take the structure tokens into consideration:
MHA asy H (l) d v , H (l) d v = U u=1 head u H (l) d v , H (l) d v ,
where head u H (l)
d v , H (l) d v = softmax ⎛ ⎝ Q (l) u K (l) u d/U ⎞ ⎠ • V (l) u , Q (l) u = H (l) d v W (l) Q,u , K (l) u = H (l) d v W (l) K,u , V (l) u = H (l) d v W (l) V,u . (7)
With the asymmetric MHA mechanism, the node encoding process of the (l + 1)-th layer will be:
H (l) d v = Normalize H (l) d v + MHA asy H (l) d v , H (l) d v , H (l+1) d v = Normalize H (l) d v + MLP H (l) d v . (8)
Along this line of work, GreaseLM [66] proposes to have a language encoding component and a graph encoding component in each layer.These two components interact through a modality-fusion layer (MInt layer), where a special structure token is added to the text Transformer input, and a special node is added to the graph encoding layer.DRAGON [80] further proposes strategies to pretrain GreaseLM with unsupervised signals.GraphFormers [71] are designed for node representation learning on homogeneous text-attributed networks where the current layer [CLS] token hidden states of neighboring documents are aggregated and added as a new token on the current layer center node text encoding.Patton [31] proposes to pretrain GraphFormers with two novel strategies: networkcontextualized masked language modeling and masked node prediction.Heterformer [72] introduces virtual neighbor tokens for text-rich neighbors and textless neighbors which are concatenated with the original text tokens and fed into each Transformer layer.Edgeformers [73] are proposed for representation learning on textual-edge networks where edges are associated with rich textual information.When conducting edge encoding, virtual node tokens will be concatenated onto the original edge text tokens for joint encoding.</p>
<p>3) Graph-Aware LLM Finetuning: In these methods, the graph information is mainly injected into the LLM by "finetuning on graphs".Researchers assume that the structure of graphs can provide hints on what documents are "semantically similar" to what other documents.For example, papers citing each other in an academic graph can be of similar topics.These methods adopt vanilla language models that take text as input (e.g., BERT [23] and SciBERT [25]) as the base model and finetune them with structure signals on the graph [50].After that, the LLMs will learn node/edge representations that capture the graph homophily from the text perspective.This is the simplest way to utilize LLMs on graphs.However, during encoding, the model itself can only consider text.</p>
<p>Most methods adopt the two-tower encoding and training pipeline, where the representation of each node is obtained separately and the model is optimized as follows:
h v i = LLM θ (d v i ), min θ f (h v i , {h v + i }, {h v − i }). (9)
Here v + i represents the positive nodes to v i , v − i represents the negative nodes to v i and f (•) denotes the pairwise training objective.Different methods have different strategies for v + i and v − i with different training objectives f (•).SPECTER [50] constructs the positive text/node pairs with the citation relation, explores random negatives and structure hard negatives, and fine-tunes SciBERT [25] with the triplet loss.SciNCL [51] extends SPECTER by introducing more advanced positive and negative sampling methods based on embeddings trained on graphs.Touchup-G [53] proposes the measurement of feature homophily on graphs and brings up a binary cross-entropy fine-tuning objective.TwHIN-BERT [55] mines positive node pairs with off-the-shelf heterogeneous information network embeddings and trains the model with a contrastive social loss.MI-CoL [58] discovers semantically positive node pairs with metapath [89] and adopts the InfoNCE objective.E2EG [59] utilizes a similar philosophy from GIANT [57] and adds a neighbor prediction objective apart from the downstream task objective.WalkLM [60] conducts random walks for structure linearization before fine-tuning the language model.A summarization of the two-tower graph-centric LLM fine-tuning objectives can be found in Appendix Table 7, available online.</p>
<p>There are other methods using the one-tower pipeline, where node pairs are concatenated and encoded together:
h v i ,v j = LLM θ (d v i , d v j ), min θ f (h v i ,v j ).(10)
LinkBERT [30] proposes a document relation prediction objective (an extension of next sentence prediction in BERT [23]) which aims to classify the relation of two node text pairs from contiguous, random, and linked.MICoL [58] explores predicting the node pairs' binary meta-path or meta-graph indicated relation with the one-tower language model.4) Discussion: Although the community is making good progress, there are still some open questions to be solved.</p>
<p>Graph as Code Sequence: Existing graphs as sequence methods are mainly rule-based or GNN-based.The former relies on natural language to describe the graphs which is not natural for structure data, while the latter has a GNN component that needs to be trained.A more promising way is to obtain a structure-aware sequence for graphs that can support zero-shot inference.A potential solution is to adopt codes (that can capture structures, e.g., graph XML or JSON) to describe the graphs and utilize code LLMs [22].</p>
<p>Advanced Graph-Empowered LLM Techniques: Graphempowered LLM is a promising direction to achieve foundational models for graphs.However, existing works are far from enough: 1) Task.Existing methods are mainly designed for representation learning (with encoder-only LLMs) which are hard to adopt for generation tasks.A potential solution is to design Graph-Empowered LLMs with decoder-only or encoderdecoder LLMs as the base architecture.2) Pretraining.Pretraining is important to enable LLMs with contextualized data understanding capability, which can be generalized to other tasks.However, existing works mainly focus on pretraining LLMs on homogeneous text-attributed networks.Future studies are needed to explore LLM pretraining in more diverse real-world scenarios including heterogeneous text-attributed networks [72], dynamic text-attributed networks [127], and textual-edge networks [73].</p>
<p>B. LLM as Encoder</p>
<p>LLMs extract textual features to serve as initial node feature vectors for GNNs, which then generate node/edge representations and make predictions.These methods typically adopt an LLM-GNN cascaded architecture to obtain the final representation h v i for node v i :
x v i = LLM(d v i ) h v i = GNN(X v , G). (11)
Here x v i is the feature vector that captures the textual information d v i associated with v i .The final representation h v i will contain both textual information and structure information of v i and can be used for downstream tasks.In the following sections, we will discuss the optimization, augmentation, and distillation of such models.The figures for these techniques can be found in Fig. 4.</p>
<p>1) Optimization: One-</p>
<p>Step Training refers to training the LLM and GNN together in the cascaded architecture for the downstream tasks.TextGNN [76] explores GCN [83], Graph-SAGE [84], GAT [85] as the base GNN architecture, adds skip connection between LLM output and GNN output, and optimizes the whole architecture for sponsored search task.Ads-GNN [77] further extends TextGNN by proposing edge-level information aggregation.GNN-LM [65] adds GNN layers to enable the vanilla language model to reference similar contexts in the corpus for language modeling.Joint training LLMs and GNNs in a cascaded pipeline is convenient but may suffer from efficiency [67] (only support sampling a few one-hop neighbors regarding memory complexity) and local minimal [34] (LLM underfits the data) issues.</p>
<p>Two-</p>
<p>Step Training means first adapting LLMs to the graph, and then finetuning the whole LLM-GNN cascaded pipeline.GIANT [57] proposes to conduct neighborhood prediction with the use of XR-Transformers [78] and results in an LLM that can output better feature vectors than bag-of-words and vanilla BERT [23] embedding for node classification.LM-GNN [67] introduces graph-aware pre-fine-tuning to warm up the LLM on the given graph before fine-tuning the whole LLM-GNN pipeline and demonstrating significant performance gain.SimTeG [34] finds that the simple framework of first training the LLMs on the downstream task and then fixing the LLMs and training the GNNs can result in outstanding performance.They further find that using the efficient fine-tuning method, e.g., LoRA [39] to tune the LLM can alleviate overfitting issues.GaLM [79] explores ways to pretrain the LLM-GNN cascaded Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.architecture.The two-step strategy can effectively alleviate the insufficient training of the LLM which contributes to higher text representation quality but is more computationally expensive and time-consuming than the one-step training strategy.</p>
<p>2) Data Augmentation: With its demonstrated zero-shot capability [42], LLMs can be used for data augmentation to generate additional text data for the LLM-GNN cascaded architecture.The philosophy of using LLM to generate pseudo data is widely explored in NLP [82], [88].LLM-GNN [63] proposes to conduct zero-shot node classification on text-attributed networks by labeling a few nodes and using the pseudo labels to fine-tune GNNs.TAPE [69] presents a method that uses LLM to generate prediction text and explanation text, which serve as augmented text data compared with the original text data.A following medium-scale language model is adopted to encode the texts and output features for augmented texts and original text respectively before feeding into GNNs.ENG [70] brings forward the idea of generating labeled nodes for each category, adding edges between labeled nodes and other nodes, and conducting semi-supervised GNN learning for node classification.</p>
<p>3) Knowledge Distillation: LLM-GNN cascaded pipeline is capable of capturing both text information and structure information.However, the pipeline suffers from time complexity issues during inference, since GNNs need to conduct neighbor sampling and LLMs need to encode the text associated with both the center node and its neighbors.A straightforward solution is to serve the LLM-GNN cascade pipeline as the teacher model and distill it into an LLM as the student model.In this case, during inference, the model (which is a pure LLM) only needs to encode the text on the center node and avoid time-consuming neighbor sampling.AdsGNN [77] proposes an L2-loss to force the outputs of the student model to preserve topology after the teacher model is trained.GraD [68] introduces three strategies including the distillation objective and task objective to optimize the teacher model and distill its capability to the student model.</p>
<p>4) Discussion: Given that GNNs are demonstrated as powerful models in encoding graphs, "LLMs as encoders" seems to be the most straightforward way to utilize LLMs on graphs.However, there are still open questions.</p>
<p>Limited Task: Go Beyond Representation Learning: Current "LLMs as encoders" methods or LLM-GNN cascaded architectures are mainly focusing on representation learning, given the single embedding propagation-aggregation mechanism of GNNs, which prevents it from being adopted to generation tasks (e.g., node/text generation).A potential solution to this challenge can be to conduct GNN encoding for LLM-generated token-level representations and to design proper decoders that can perform generation based on the LLM-GNN cascaded model outputs.</p>
<p>Low Efficiency: Advanced Knowledge Distillation: The LLM-GNN cascaded pipeline suffers from time complexity issues since the model needs to conduct neighbor sampling and then embedding encoding for each neighboring node.Although there are methods that explore distilling the learned LLM-GNN model into an LLM model for fast inference, they are far from enough given that the inference of LLM itself is time-consuming.A potential solution is to distill the model into a much smaller LM or even an MLP.Similar methods [86] have been proven effective in GNN to MLP distillation and are worth exploring for the LLM-GNN cascaded pipeline as well.</p>
<p>C. LLM as Aligner</p>
<p>These methods contain an LLM component for text encoding and a GNN component for structure encoding.These two components are served equally and trained iteratively or parallelly.LLMs and GNNs can mutually enhance each other since the LLMs can provide textual signals to GNNs, while the GNNs can deliver structure information to LLMs.According to how the LLM and the GNN interact, these methods can be further categorized into: LLM-GNN Prediction Alignment and LLM-GNN Latent Space Alignment.The illustration of these two categories of methods can be found in Fig. 5. with the structure data on a graph iteratively.LLM will generate labels for nodes from the text perspective and serve them as pseudo-labels for GNN training, while GNN will generate labels for nodes from the structure perspective and serve them as pseudo-labels for LLM training.By this design, these two modality encoders can learn from each other and contribute to a final joint text and graph encoding.In this direction, LTRN [56] proposes a novel GNN architecture with personalized PageRank [93] and attention mechanism for structure encoding while adopting BERT [23] as the language model.The pseudo labels generated by LLM and GNN are merged for the next iteration of training.GLEM [61] formulates the iterative training process into a pseudo-likelihood variational framework, where the E-step is to optimize LLM and the M-step is to train the GNN.</p>
<p>1) LLM-GNN Prediction</p>
<p>2) LLM-GNN Latent Space Alignment: It denotes connecting text encoding (LLM) and structure encoding (GNN) with cross-modality contrastive learning:
h d v i = LLM(d v i ), h v i = GNN(G v ),(12)l(h d v i , h v i ) = Sim(h d v i , h v i ) j =i Sim(h d v i , h v j ) , (13)L = v i ∈G 1 2|G| (l(h d v i , h v i ) + l(h v i , h d v i ))(14)
A similar philosophy is widely used in vision-language joint modality learning [95].Along this line of approaches, Con-Grat [52] adopts GAT [85] as the graph encoder and tries MPNet [33] as the language model encoder.They have expanded the original InfoNCE loss by incorporating graph-specific elements.These elements pertain to the most likely second, third, and subsequent choices regarding the nodes from which a text originates and the texts that a node generates.In addition to the node-level multi-modality contrastive objective, GRENADE [54] proposes KL-divergence-based neighbor-level knowledge alignment: minimize the neighborhood similarity distribution calculated between LLM and GNN.G2P2 [62] further extends node-text contrastive learning by adding textsummary interaction and node-summary interaction.Then, they introduce using label texts in the text modality for zero-shot classification, and using soft prompts for few-show classification.THLM [32] proposes to pretrain the language model by contrastive learning with a heterogeneous GNN on heterogeneous text-attributed networks.The pretrained LLM can be fine-tuned on downstream tasks.</p>
<p>3) Discussion: Most existing methods adopt homogeneous text-graph alignment, assuming that the semantic relation between the two modalities, namely text and graph, is singular.However, this is not usually the case in the real world, given: 1) The existence of multimodal attributes: Other modalities, e.g., images can appear together with text and graph.In this case, it is worth researching how to align the multimodal attributes in a graph scenario.2) Heterogeneous semantic relations: the semantic relationships between data units (text/image/graph) can be multiplex.Different relations have different distributions and a single semantic alignment will fail to capture the comprehensively [74].</p>
<p>VI. TEXT-PAIRED GRAPHS</p>
<p>Graphs are prevalent data objects in scientific disciplines such as cheminformatics [182], [193], [199], material informatics [180], bioinformatics [200], and computer vision [146].Within these diverse fields, graphs frequently come paired with critical graph-level text information.For instance, molecular graphs in cheminformatics are annotated with text properties such as toxicity, water solubility, and permeability properties [180], [182].Research on such graphs (scientific discovery) could be accelerated by the text information and the adoption of LLMs.In this section, we review the application of LLMs on graph-captioned graphs with a focus on molecular graphs.According to the technique categorization in Section III-B, we begin by investigating methods that utilize LLMs as Predictor.Then, we discuss methods that align GNNs with LLMs.We summarize all surveyed methods in Appendix Table 8 and Figure 6, available online.</p>
<p>A. LLM as Predictor</p>
<p>In this subsection, we review how to conduct "LLM as Predictor" for graph-level tasks.Existing methods can be categorized into Graph as Sequence (treat graph data as sequence input) and Graph-Empowered LLMs (design model architecture to encode graphs).</p>
<p>1) Graph as Sequence: For text-paired graphs, we have three steps to utilize existing LLM for graph inputs.Step 1: Linearize graphs into sequence with rule-based methods.Step 2: Tokenize the linearized sequence.Step 3: Train/Finetune different LLMs (e.g., Encoder-only, Encoder-Decoder, Decoder-only) for specific tasks.We will discuss each step as follows.</p>
<p>Step 1: Rule-based Graph Linearization.Rule-based linearization converts molecular graphs into text sequences that can be processed by LLMs.To achieve this, researchers develop specifications based on human expertise in the form of line notations [147].For example, the Simplified Molecular-Input Line-Entry System (SMILES) [147] records the symbols of nodes encountered during a depth-first traversal of a molecular graph.The International Chemical Identifier (InChI) [148] encodes molecular structures into unique string texts with more hierarchical information.Canonicalization algorithms produce unique SMILES for each molecule, often referred to as canonical SMILES.However, there are more than one SMILES corresponding to a single molecule and SMILES sometimes represent invalid molecules; LLMs learned from these linearized sequences can easily generate invalid molecules (e.g., incorrect ring closure symbols and unmatched parentheses) due to syntactical errors.To this end, DeepSMILES [149] is proposed.It can alleviate this issue in most cases but does not guarantee 100% robustness.The linearized string could still violate basic physical constraints.To fully address this problem, SELFIES [150] is introduced which consistently yields valid molecular graphs.</p>
<p>Step 2: Tokenization.These approaches for linearized sequences are typically language-independent.They operate at both character level [166], [177] and substring level [161], [168], [172], [173], [174], [175], based on SentencePiece or BPE [154].</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Additionally, RT [163] proposes a tokenization approach that facilitates handling regression tasks within LM Transformers.</p>
<p>Step 3: Encoding the Linearized Graph with LLMs.Encoder-only LLMs: Earlier LLMs like SciBERT [25] and BioBERT [179] are trained on scientific literature to understand natural language descriptions related to molecules but are not capable of comprehending molecular graph structures.To this end, SMILES-BERT [178] and MFBERT [175] are proposed for molecular graph classification with linearized SMILES strings.Since scientific natural language descriptions contain human expertise which can serve as a supplement for molecular graph structures, recent advances emphasize joint understanding of them [158], [174]: The linearized graph sequence is concatenated with the raw natural language data and then input into the LLMs.Specifically, KV-PLM [174] is built based on BERT [23] to understand the molecular structure in a biomedical context.CatBERTa [158], as developed from RoBERTa [24], specializes in the prediction of catalyst properties for molecular graphs.</p>
<p>Encoder-Decoder LLMs: Encoder-only LLMs may lack the capability for generation tasks.In this section, we discuss LLMs with encoder-decoder architectures.For example, Chemformer [155] uses a similar architecture as BART [28].The representation from the encoder can be used for property prediction tasks, and the whole encoder-decoder architecture can be optimized for molecule generation.Others focus on molecule captioning (which involves generating textual descriptions from a molecule) and text-based molecular generation (where a molecular graph structure is generated from a natural description).Specifically, MolT5 [122] is developed based on the T5 [29], suitable for these two tasks.It formulates molecule-text translation as a multilingual problem and initializes the model using the T5 checkpoint.The model was pre-trained on two monolingual corpora: the Colossal Clean Crawled Corpus (C4) [29] for the natural language modality and one million SMILES [155] for the molecule modality.Text+Chem T5 [170] extends the input and output domains to include both SMILES and texts, unlocking LLMs for more generation functions such as text or reaction generation.ChatMol [165] exploits the interactive capabilities of LLMs and proposes designing molecule structures through multi-turn dialogs with T5.</p>
<p>Decoder-only LLMs: Decoder-only architectures have been adopted for recent LLMs due to their advanced generation ability.MolGPT [176] and MolXPT [168] are GPT-style models used for molecule classification and generation.Specifically, MolGPT [176] focuses on conditional molecule generation tasks using scaffolds, while MolXPT [168] formulates the classification task as a question-answering problem with yes or no responses.RT [163] adopts XLNet [27] and focuses on molecular regression tasks.It frames the regression as a conditional sequence modeling problem.Galactica [177] is a set of LLMs with a maximum of 120 billion parameters, which is pretrained on two million compounds from PubChem [182].Therefore, Galactica could understand molecular graph structures through SMILES.With instruction tuning data and domain knowledge, researchers also adapt general-domain LLMs such as LLaMA to recognize molecular graph structures and solve molecule tasks [159].Recent studies also explore the in-context learning capabilities of LLMs on graphs.LLM-ICL [167] assesses the performance of LLMs across eight tasks in the molecular domain, ranging from property classification to molecule-text translation.MolReGPT [164] proposes a method to retrieve molecules with similar structures and descriptions to improve in-context learning.LLM4Mol [162] utilizes the summarization capability of LLMs as a feature extractor and combines it with a smaller, tunable LLM for specific prediction tasks.</p>
<p>2) Graph-Empowered LLMs: Different from the methods that adopt the original LLM architecture (i.e., Transformers) and input the graphs as sequences to LLMs, graph-empowered LLMs attempt to design LLM architectures that can conduct joint encoding of text and graph structures.Some works modify the positional encoding of Transformers.For instance, GIM-LET [46] treats nodes in a graph as tokens.It uses one Transformer to manage both the graph structure and text sequence
[v 1 , v 2 , . . . , v |V| , s |V|+1 , . . . , s |V|+|d G | ],
where v ∈ V is a node and s ∈ d G is a token in the text associated with G.This sequence cannot reflect graph structure.Therefore, a new position encoding (PE) is used to jointly encode graph structures and text sequences.It defines the relative distance between tokens i and j as follows:
PE(i, j) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ i − j if i, j ∈ d G , GSD(i, j) + Mean e k ∈SP(i,j) x e k if i, j ∈ V, −∞ if i ∈ V, j ∈ d G , 0 if i ∈ d G , j ∈ V. (15)
GSD is the graph shortest distance between two nodes, and Mean k∈SP(i,j) represents the mean pooling of the edge features x e k along the shortest path SP(i, j) between nodes i and j.</p>
<p>GIMLET [46] adapts bi-directional attention for node tokens and enables texts to selectively attend to nodes.These designs render the Transformer's submodule, which handles the graph part, equivalent to a Graph Transformer [140].Cross-attention is also used to interact representations between graphs and texts.Given the graph hidden state h G , its node-level hidden state H v and text hidden state H d G , Text2Mol [121] implemented interaction between representations in the hidden layers of encoders, while Prot2Text [160] implemented this interaction within the layers of between encoder and decoder
H d G = softmax( W Q H d G •(W K H v ) T √ d k ) • W V H v , where W Q , W K , W V
are trainable parameters that transform the query modality (e.g., sequences) and the key/value modality (e.g., graphs) into the attention space.Furthermore, Prot2Text [160] utilizes two trainable parameter matrices W 1 and W 2 to integrate the graph representation into the sequence representation
H d G = (H d G + 1 |d G | h G W 1 )W 2 .
3) Discussion: LLM Inputs with Sequence Prior: The first challenge is that the progress in advanced linearization methods has not progressed in tandem with the development of LLMs.Emerging around 2020, linearization methods for molecular graphs like SELFIES offer significant grammatical advantages, yet advanced LMs and LLMs from graph machine learning and language model communities might not fully utilize these, as these encoded results are not part of pretraining corpora prior to their proposal.Consequently, recent studies [167] indicate that LLMs, such as GPT-3.5/4,may be less adept at using SELFIES compared to SMILES.Therefore, the performance of LM-only and LLM-only methods may be limited by the expressiveness of older linearization methods, as there is no way to optimize these hard-coded rules during the learning pipeline of LLMs.However, the second challenge remains as the inductive bias of graphs may be broken by linearization.Rule-based linearization methods introduce inductive biases for sequence modeling, thereby breaking the permutation invariance assumption inherent in molecular graphs.It may reduce task difficulty by introducing sequence order to reduce the search space.However, it does not mean model generalization.Specifically, there could be multiple string-based representations for a single graph from single or different approaches.Numerous studies [151], [152], [153] have shown that training on different string-based views of the same molecule can improve the sequential model's performance, as these data augmentation approaches manage to retain the permutation-invariance nature of graphs.These advantages are also achievable with a permutation-invariant GNN, potentially simplifying the model by reducing the need for complex, string-based data augmentation design.</p>
<p>LLM Inputs With Graph Prior: Rule-based linearization may be considered less expressive and generalizable compared to the direct graph representation with rich node features, edge features, and the adjacency matrix [186].Various atomic features include atomic number, chirality, degree, formal charge, number of hydrogen atoms, number of radical electrons, hybridization state, aromaticity, and presence in a ring.Bond features encompass the bond's type (e.g., single, double, or triple), the bond's stereochemistry (e.g., E/Z or cis/trans), and whether the bond is conjugated [187].Each feature provides specific information about atomic properties and structure, crucial for molecular modeling and cheminformatics.One may directly vectorize the molecular graph structure into binary vectors [185] and then apply parameterized Multilayer Perceptrons (MLPs) on the top of these vectors to get the graph representation.These vectorization approaches are based on human-defined rules and vary, such as MACCS, ECFP, and CDK fingerprints [185].These rules take inputs of a molecule and output a vector consisting of 0/1 bits.Each bit denotes a specific type of substructure related to functional groups that could be used for various property predictions.Fingerprints consider atoms and structures, but they cannot automatically learn from the graph structure.GNNs could serve as automatic feature extractors to replace or enhance fingerprints.Some specific methods are explored in Section VI-A2, while the other graph prior such as the eigenvectors of a graph Laplacian and the random walk prior could also be used [141].</p>
<p>LLM Outputs for Prediction: LMs like KV-PLM [174], SMILES-BERT [178], MFBERT [175], and Chemformer [155] use a prediction head on the output vector of the last layer.These models are finetuned with standard classification and regression losses but may not fully utilize all the parameters and advantages of the complete architecture.In contrast, models like RT [163], MolXPT [168], and Text+Chem T5 [170] frame prediction as a text generation task.These models are trained with either masked language modeling or autoregressive targets, which requires a meticulous design of the context words in the text [163].Specifically, domain knowledge instructions may be necessary to activate the in-context learning ability of LLMs, thereby making them domain experts [167].For example, a possible template could be divided into four parts: {General Description}{Task-Specific Description}{Question-Answer Examples}{Test Question}.</p>
<p>LLM Outputs for Reasoning: Since string representations of molecular graphs usually carry new and in-depth domain knowledge, which is beyond the knowledge of LLMs, recent work [145], [156], [164] also attempts to utilize the reasoning ability of LLMs, instead of using them as a knowledge source for predicting the property of molecular graphs.ReLM [156] utilizes GNNs to suggest top-k candidates, which were then used to construct multiple-choice answers for in-context learning.ChemCrow [145] designs the LLMs as the chemical agent to implement various chemical tools.It avoided direct inference in an expertise-intensive domain.</p>
<p>B. LLM as Aligner</p>
<p>1) Latent Space Alignment: One may directly align the latent spaces of the GNN and LLM through contrastive learning and predictive regularization.Typically, a graph representation from a GNN can be read out by summarizing all node-level representations, and a sequence representation can be obtained from the [CLS] token.We first use two projection heads, which are usually MLPs, to map the separate representation vectors from the GNN and LLM into a unified space as h G and h d G , and then align them within this space.Specifically, MoMu [173] and MoMu-v2 [172] retrieve two sentences from the corpus for each molecular graph.During training, graph data augmentation was applied to molecular graphs, creating two augmented views.Consequently, there are four pairs of G and d G .For each pair, the contrastive loss for space alignment is as MoMu = − log
exp(cos(h G ,h d G )/τ ) dG =d G exp(cos(h G ,h dG )/τ )
where τ is the temperature hyper-parameter and dG denotes the sequence not paired to the graph G. MoleculeSTM [171] also applies contrastive learning to minimize the representation distance between a molecular graph G and its corresponding texts d G , while maximizing the distance between the molecule and unrelated descriptions.MoleculeSTM [171] randomly samples negative graphs or texts to construct negative pairs of (G, d) and ( G, d).Similarly, MolFM [161] and GIT-Mol [157] implement contrastive loss with mutual information and negative sampling.These two methods also use cross-entropy to regularize the unified space with the assumption that randomly permuted graph and text inputs are predictable if they originate from the same molecule.However, the aforementioned methods cannot leverage task labels.Given a classification label y, CLAMP [169] learns to map active molecules (y = 1) so that they align with the corresponding assay description for each molecular graph G:
CLAMP = y log(σ(τ −1 h T G h d G )) + (1 − y) log(1 −
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>TABLE II DATA COLLECTION
IN SECTION V FOR TEXT-ATTRIBUTED GRAPHS σ(τ −1 h T G h d G ))
. CLAMP [169] requires labels to encourage that active molecules and their corresponding text descriptions are clustered together in the latent space.To advance the alignment between two modalities, MolCA [166] trains the Query Transformer (Q-Former) [189] for moleculetext projecting and contrastive alignment.Q-former initializes N q learnable query tokens {q k } N q k=1 .These query tokens are updated with self-attention and interact with the output of GNNs through cross-attention to obtain the k-th queried molecular representation vector (h G ) k := Q-Former(q k ).The query tokens share the same self-attention modules with the texts, but use different MLPs, allowing the Q-Former to be used for obtaining the representation of text sequence h d G := Q-Former([CLS]).Then we have MolCA = − g2t − t2g , where g2t = log
exp(max k cos((h G ) k ,h d G )/τ ) dG =d G exp(max k cos((h G ) k ,h dG )/τ ) , and t2g = log exp(max k cos(h d G ,(h G ) k )/τ ) G =G exp(max k cos(h d G ,(hG ) k )/τ ) .
2) Discussion: Larger-Scale GNNs: GNNs integrate atomic and graph structural features for molecular representation learning [144].Specifically, Text2Mol [121] utilizes the GCN [83] as its graph encoder and extracts unique identifiers for node features based on Morgan fingerprints [185].MoMu [173], MoMu-v2 [172], MolFM [161], GIT-Mol [157], and MolCA [166] prefer GIN [188] as the backbone, as GIN has been proven to be as expressive and powerful as the Weisfeiler-Lehman graph isomorphism test.As described in Section II-B, there has been notable progress in making GNNs deeper, more generalizable, and more powerful since the proposal of the GCN [83] in 2016 and the GIN [188] in 2018.However, most reviewed works [157], [161], [166], [172], [173] are developed using the GIN [188] as a proof of concept for their approaches.These pretrained GINs feature five layers and 300 hidden dimensions.The scale of GNNs may be a bottleneck in learning semantic meaningful representation and there is a risk of over-reliance on one modality, neglecting the other.Therefore, for future large-scale GNN designs comparable to LLMs, scaling up the dimension size and adding deeper layers, may be considered.</p>
<p>Besides, Transformer encoders [141] may also improve the expressive power of deep GNNs.</p>
<p>Generation Decoder with GNNs: GNNs are often not used as decoders for graph generation.The prevalent decoders are mostly text-based, generating linearized graph structures such as SMILES.These methods may be sensitive to the sequence order in the linearized graph.Generative diffusion models [201] on graphs could be utilized in future work to design generators with GNNs.</p>
<p>VII. RESOURCES AND APPLICATIONS</p>
<p>A. Datasets, Splitting and Evaluation</p>
<p>We summarize the datasets for three scenarios (namely pure graphs, text-attributed graphs, and text-paired graphs) and show them in Tables V, II, and III respectively.</p>
<p>1) Pure Graphs: In Table 5, we summarize the pure graph reasoning problems discussed in Section IV.Many problems are shared or revisited in different datasets due to their commonality.NLGraph [123], LLMtoGraph [124] and GUC [125] study a set of standard graph reasoning problems, including connectivity, shortest path, and graph diameter.GraphQA [130] benchmarks a similar set of problems but additionally describes the graphs in real-world scenarios to study the effect of graph grounding.LLM4DyG [127] focuses on reasoning tasks on temporally evolving graphs.Accuracy is the most common evaluation metric as they are primarily formulated as graph question-answering tasks.</p>
<p>2) Text-Attributed Graphs: We summarize the famous datasets for evaluating models on text-attributed graphs in Table II.The datasets are mostly from the academic, e-commerce, book, social media, and Wikipedia domains.The popular tasks to evaluate models on those datasets include node classification, link prediction, edge classification, regression, and recommendation.The evaluation metrics for node/edge classification include Accuracy, Macro-F1, and Micro-F1.For link prediction and recommendation evaluation, Mean 3) Text-Paired Graphs: Table III shows text-paired graph datasets (including text-available and graph-only datasets).For Data Splitting, options include random splitting, source-based splitting, activity cliffs and scaffolds [195], and data balancing [142].Graph classification usually adopts AUC [187] as the metrics, while regression uses MAE, RMSE, and R2 [144].For text generation evaluation, people tend to use the Bilingual Evaluation Understudy (BLEU) score; while for molecule generation evaluation, heuristic evaluation methods (based on factors including validity, novelty, and uniqueness) are adopted.However, it is worth noted that BLEU score is efficient but less accurate, while heuristic evaluation methods are problematic subject to unintended modes, such as the superfluous addition of carbon atoms in [196].</p>
<p>B. Open-Source Implementations</p>
<p>HuggingFace: HF Transformers1 is the most popular Python library for Transformers-based language models.Besides, it also provides two additional packages: Datasets 2 for easily accessing and sharing datasets and Evaluate3 for easily evaluating machine learning models and datasets.</p>
<p>Fairseq: Fairseq 4 is another open-source Python library for Transformers-based language models.PyTorch Geometric: PyG5 is an open-source Python library for graph machine learning.It packages more than 60 types of GNN, aggregation, and pooling layers.</p>
<p>Deep Graph Library: DGL 6 is another open-source Python library for graph machine learning.</p>
<p>RDKit: RDKit7 is one of the most popular open-source cheminformatics software programs that facilitates various operations and visualizations for molecular graphs.It offers many useful APIs, such as the linearization implementation for molecular graphs, to convert them into easily stored SMILES and to convert these SMILES back into graphs.</p>
<p>C. Practical Applications</p>
<p>1) Scientific Discovery: Virtual Screening: It aims to search a library of unlabeled molecules to identify useful structures for a given task.Machine learning models could automatically screen out trivial candidates to accelerate this process.However, training accurate models is not easy since labeled molecules are limited in size and imbalanced in distribution [142].There are many efforts to improve GNNs against data sparsity [142], [144], [191].However, it is difficult for a model to generalize and understand in-depth domain knowledge that it has never been trained on.Texts could be complementary knowledge sources.Discovering task-related content from massive scientific papers and using them as instructions has great potential to design accurate GNNs in virtual screening [46].</p>
<p>Molecular Generation: Molecular generation and optimization is one fundamental goal for drug and material discovery.Scientific hypotheses of molecules [198], can be represented in the joint space of GNNs and LLMs.Then, one may search in the latent space for a better hypothesis that aligns with the text description (human requirements) and adheres to structural constraints like chemical validity.Chemical space has been found to contain more than 10 60 molecules [197], which is beyond the capacity of exploration in wet lab experiments.Generating constrained candidates within relevant subspaces is a challenge [201] and promising, especially when incorporating textual conditions.</p>
<p>Synthesis Planning: Synthesis designs start from available molecules and involve planning a sequence of steps that can finally produce a desired chemical compound through a series of reactions [198].This procedure includes a sequence of reactant molecules and reaction conditions.Both graphs and texts play important roles in this process.For example, graphs may represent the fundamental structure of molecules, while texts may describe the reaction conditions, additives, and solvents.LLMs can assist in the planning by suggesting possible synthesis paths directly or by serving as agents to operate on existing planning tools [145].</p>
<p>2) Computational Social Science: In computational social science, researchers are interested in modeling the behavior of people/users and discovering new knowledge that can be utilized to forecast the future.The behaviors of users and interactions between users can be modeled as graphs, where the nodes are associated with rich text information (e.g., user profile, messages, emails).We will show two example scenarios below.</p>
<p>E-commerce: In E-commerce platforms, there are many interactions (e.g., purchase, view) between users and products.For example, users can view or purchase products.In addition, the users, products, and their interactions are associated with rich text information.For instance, products have titles/descriptions and users can leave a review of products.In this case, we can construct a graph [101] where nodes are users and products, while edges are their interactions.Both nodes and edges are associated with text.It is important to utilize both the text information and the graph structure information (user behavior) to model users and items and solve complex downstream tasks (e.g., item recommendation [105], bundle recommendation [106], and product understanding [107]).</p>
<p>Social Media: In social media platforms, there are many users and they interact with each other through messages, emails, and so on.In this case, we can build a graph where nodes are users and edges are the interaction between users.There will be text associated with nodes (e.g., user profile) and edges (e.g., messages).Interesting research questions will be how to do joint text and graph structure modeling to deeply understand the users for friend recommendation [108], user analysis [109], community detection [110], and personalized response generation [96], [97].</p>
<p>3) Specific Domains: In many specific domains, text data are interconnected and lie in the format of graphs.The structure information on the graphs can be utilized to better understand the text unit and contribute to advanced problem-solving.</p>
<p>Academic Domain: In the academic domain, graphs [12] are constructed with papers as nodes and their relations (e.g., citation, authorship, etc) as edges.The representation learned for papers on such graphs can be utilized for paper recommendation [102], paper classification [103], and author identification [104].</p>
<p>Legal Domain: In the legal domain, opinions given by the judges always contain references to opinions given for previous cases.In such scenarios, people can construct a graph [98] based on the citation relations between opinions.The representations learned on such a graph with both text and structure information can be utilized for clause classification [99] and opinion recommendation [100].</p>
<p>Education Domain: In the education domain, we can construct a graph with coursework as nodes and their relations as edges.The model learned on such a graph can be utilized for knowledge tracing [135] and student performance prediction [136].</p>
<p>VIII. FUTURE DIRECTIONS</p>
<p>Better Benchmark Datasets: Most pure graph benchmarks evaluate LLMs' reasoning ability on homogeneous graphs but do not include evaluations on heterogeneous or spatial-temporal graphs.For text-attributed graphs, as summarized in Table II, most benchmark datasets are from academic domains and ecommerce domains.However, in the real world, text-attributed graphs are ubiquitous across multiple domains (e.g., legal and health).More diverse datasets are needed to comprehensively evaluate LLMs on real-world scenarios.For text-paired graphs, as summarized in Table III, there is a lack of comprehensive datasets covering various machine learning tasks in chemistry.Although a massive number of scientific papers are available, preprocessing them into a ready-to-use format and pairing them with specific molecular graph data points of interest remains a cumbersome and challenging task.Besides, we could investigate graph-text pairs in 3D space, where each molecule may be associated with atomic coordinates [137].</p>
<p>Broader Task Space with LLMs: More comprehensive studies on the performance of LLMs for graph tasks hold promise for the future.While LLMs as encoder approaches have been explored for text-attributed graphs, their application to text-captioned molecular graphs remains underexplored.Promising directions include using LLMs for data augmentation and knowledge distillation to design domain-specific GNNs for various text-paired graph tasks.Furthermore, although graph generation has been approached in text-paired graphs, it remains an open problem for text-attributed graphs (i.e., how to conduct joint text and graph structure generation)</p>
<p>Efficienct LLMs on Graphs: While LLMs have shown a strong capability to learn on graphs, they suffer from inefficiency in graph linearization and model optimization.On one hand, as discussed in Sections V-A1 and VI-A1, many methods rely on transferring graphs into sequences that can be inputted into LLMs.However, the length of the transferred sequence will increase significantly as the size of the graph increases.This poses challenges since LLMs always have a maximum sequence input length and a long input sequence will lead to higher time and memory complexity.On the other hand, optimizing LLMs itself is computationally expensive.Although some general efficient tuning methods such as LoRA are proposed, there is a lack of discussion on graph-aware LLM efficient tuning methods.</p>
<p>Generalizable and Robust LLMs on Graphs: Another interesting direction is to explore the generalizability and robustness of LLMs on graphs.Generalizability refers to having the ability to transfer the knowledge learned from one domain graph to another; while robustness denotes having consistent prediction regarding obfuscations and attacks.Although LLMs have demonstrated their strong generalizability in processing text, they still suffer from robustness and hallucination issues, which are to be solved for graph data modeling as well.</p>
<p>Multi-Modal Foundation Models: One open question is, "Should we use one foundation model to unify different modalities, and how?"The modalities can include texts, graphs, and even images.For instance, molecules can be represented as graphs, described as texts, and photographed as images; products can be treated as nodes in a graph, associated with a title/description, and combined with an image.Designing a model that can conduct joint encoding for all modalities will be useful but challenging.Furthermore, there has always been tension between building a unified foundational model and customizing model architectures for different domains.It is thus intriguing to ask whether a unified architecture will suit different data types, or if tailoring model designs according to domains will be necessary.Correctly answering this question can save economic and intellectual resources from unnecessary attempts and also shed light on deeper understanding of graph-related tasks.</p>
<p>LLMs as Dynamic Agents on Graphs: Although LLMs have shown their advanced capability in generating text, one-pass generation of LLMs suffers from hallucination and misinformation issues due to the lack of accurate parametric knowledge.Simply augmenting retrieved knowledge in context is also bottlenecked by the capacity of the retriever.In many real-world scenarios, graphs such as academic networks, and Wikipedia are dynamically looked up by humans for knowledge-guided reasoning.Simulating such a role of dynamic agents can help LLMs more accurately retrieve relevant information via multihop reasoning, thereby correcting their answers and alleviating hallucinations.</p>
<p>IX. CONCLUSION</p>
<p>In this paper, we provide a comprehensive review of large language models on graphs.We first categorize graph scenarios where LMs can be adopted and summarize the large language models on graph techniques.We then provide a thorough review, analysis, and comparison of methods within each scenario.Furthermore, we summarize available datasets, open-source codebases, and multiple applications.Finally, we suggest future directions for large language models on graphs.</p>
<p>Fig. 2 .
2
Fig. 2. A taxonomy of LLM on graph scenarios and techniques with representative examples.</p>
<p>Fig. 3 .
3
Fig. 3.The illustration of various LLM as Predictor methods, including (a) Rule-based Graph As Sequence, (b) GNN-based Graph As Sequence, (c) Graph-Empowered LLMs.</p>
<p>Fig. 4 .
4
Fig. 4. The illustration of various techniques related to LLM as Encoder, including (a) One-step Training, (b) Two-step Training, (c) Data Augmentation, and (d) Knowledge Distillation.</p>
<p>Fig. 5 .
5
Fig. 5.The illustration of LLM as Aligner methods, including (a) LLM-GNN Prediction Alignment and (b) LLM-GNN Latent Space Alignment.</p>
<p>Alignment: This refers to training the LLM with the text data on a graph and training the GNN Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Large Language Models on Graphs: A Comprehensive Survey Bowen Jin , Gang Liu , Graduate Student Member, IEEE, Chi Han , Meng Jiang , Heng Ji , Member, IEEE, and Jiawei Han , Fellow, IEEE</p>
<p>(Survey Paper)</p>
<p>TABLE III DATA
III
COLLECTION IN SECTION VI FOR TEXT-CAPTIONED GRAPHSReciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Hit Ratio (Hit) usually serve as metrics.While evaluating model performance on regression tasks, people tend to adopt mean absolute errors (MAE) or root mean square error (RMSE).</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
https://huggingface.co/docs/transformers/index
https://huggingface.co/docs/datasets/index
https://huggingface.co/docs/evaluate/index
https://github.com/facebookresearch/fairseq
https://pytorch-geometric.readthedocs.io/en/latest/index.html
https://www.dgl.ai/
https://www.rdkit.org/docs/ Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
ACKNOWLEDGMENTAny opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government.; date of current version 13 November 2024.This work was supported in part by US DARPA INCAS under Program HR0011-21-C0165, in part by BRIES under Program HR0011-24-3-0325, in part by National Science Foundation under Grant IIS-19-56151, in part by the Molecule Maker LabInstitute: An AI Research Institutes program supported by NSF under Award 2019897, in part by the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award 2118329, in part by U.S. DARPA ITM under Program FA8650-23-C-7316, in part by Agriculture and Food Research Initiative (AFRI) under Grant 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, in part by NSF under Award 2142827, Award 2146761, and Award 2234058, and in part by ONR under Grant N00014-22-1-2507.Recommended for acceptance by K. Zheng.Bowen Jin received the BS degree from Tsinghua University in 2021.He is currently working toward the PhD degree in computer science with the University of Illinois at Urbana-Champaign, advised by Prof. Jiawei Han.His research focuses on large language models, information networks, and data/text mining, with their applications in information retrieval and knowledge discovery.He has published first-authored papers in SIGIR, ICLR, ACL, and KDD.He receives the Apple PhD Fellowship in 2024.Gang Liu (Graduate
End-to-end open-domain question answering with bertserini. W Yang, Proc. Conf. North Amer. Conf. North Amer2019</p>
<p>Text summarization with pretrained encoders. Y Liu, M Lapata, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2019</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2018</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERT-networks. N Reimers, I Gurevych, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2019</p>
<p>Emergent abilities of large language models. J Wei, Trans. Mach. Learn. Res. 2022</p>
<p>Algorithmic Aspects of Graph Connectivity. H Nagamochi, T Ibaraki, 2018Cambridge Univ. PressCambridge, U.K.</p>
<p>Computing the shortest path: A search meets graph theory. A V Goldberg, C Harrelson, Proc. 16th Annu. ACM-SIAM Symp. Discrete Algorithms. 16th Annu. ACM-SIAM Symp. Discrete Algorithms2005</p>
<p>Efficient subgraph matching on billion node graphs. Z Sun, H Wang, H Wang, B Shao, J Li, arXiv:1205.66912012</p>
<p>Exploring the potential of large language models (LLMS) in learning on graphs. Z Chen, arXiv:2307.033932023</p>
<p>Automating the construction of internet portals with machine learning. A K Mccallum, K Nigam, J Rennie, K Seymore, Inf. Retrieval. 32000</p>
<p>CiteSeer: An automatic citation indexing system. C L Giles, K D Bollacker, S Lawrence, Proc. 3rd ACM Conf. Digit. Libraries. 3rd ACM Conf. Digit. Libraries1998</p>
<p>Microsoft academic graph: When experts are not enough. K Wang, Z Shen, C Huang, C H Wu, Y Dong, A Kanakia, Quantitative Sci. Stud. 112020</p>
<p>The effect of metadata on scientific literature tagging: A cross-field cross-model study. Y Zhang, B Jin, Q Zhu, Y Meng, J Han, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2023</p>
<p>Item recommendation on monotonic behavior chains. M Wan, J Mcauley, RecSys. 2018</p>
<p>Justifying recommendations using distantlylabeled reviews and fine-grained aspects. J Ni, J Li, J Mcauley, Proc. Conf. Empir. Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess. 9th Int. Joint Conf. Natural Lang. ess2019</p>
<p>Collective classification in network data. P Sen, G Namata, M Bilgic, L Getoor, B Galligher, AI Mag. 2932008</p>
<p>KEPLER: A unified model for knowledge embedding and pre-trained language representation. X Wang, Trans. Assoc. Comput. Linguistics. 92021</p>
<p>Neural-answering logical queries on knowledge graphs. L Liu, B Du, H Ji, C Zhai, H Tong, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2021</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, G Long, C Zhang, S P Yu, IEEE Trans. Neural Netw. Learn. Syst. 321Jan. 2021</p>
<p>Towards graph foundation models: A survey and beyond. J Liu, arXiv:2310.118292023</p>
<p>Unifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, arXiv:2306.083022023</p>
<p>Codet5 : Open code large language models for code understanding and generation. Y Wang, H Le, A D Gotmare, N D Bui, J Li, S C Hoi, arXiv:2305.079222023</p>
<p>BERT: Pre-Training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, Proc. Conf. North Amer. Conf. North Amer2019</p>
<p>RoBERTa: A robustly optimized bert pretraining approach. Y Liu, arXiv:1907.116922019</p>
<p>SciBERT: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, arXiv:1903.106762019</p>
<p>Language models are few-shot learners. T Brown, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2020</p>
<p>XLNet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2019</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2020</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, 140:1-140:67J. Mach. Learn. Res. 212020</p>
<p>LinkBERT: Pretraining language models with document links. M Yasunaga, J Leskovec, P Liang, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2022</p>
<p>Patton: Language model pretraining on text-rich networks. B Jin, Proc. Conf. Assoc. Comput. Linguistics, 2023. Conf. Assoc. Comput. Linguistics, 2023</p>
<p>Pretraining language models with text-attributed heterogeneous graphs. T Zou, L Yu, Y Huang, L Sun, B Du, arXiv:2310.125802023</p>
<p>MPNet: Masked and permuted pre-training for language understanding. K Song, X Tan, T Qin, J Lu, Proc. Int. Conf. Neural Inf. Int. Conf. Neural Inf2020</p>
<p>SimTeG: A frustratingly simple approach improves textual graph learning. K Duan, arXiv:2308.025652023</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, Learn. Individual Differences. 1032023. 102274</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2021</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Proc. Conf. Assoc. Comput. Linguistics, 2021. Conf. Assoc. Comput. Linguistics, 2021</p>
<p>Parameter-efficient transfer learning for NLP. N Houlsby, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2019</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Graph neural prompting with large language models. Y Tian, arXiv:2309.154272023</p>
<p>GraphLLM: Boosting graph reasoning ability of large language model. Z Chai, arXiv:2310.058452023</p>
<p>Finetuned language models are zero-shot learners. J Wei, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>GraphGPT: Graph instruction tuning for large language models. J Tang, arXiv:2310.130232023</p>
<p>Natural language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, arXiv:2308.071342023</p>
<p>GIMLET: A unified graph-text model for instructionbased molecule zero-shot learning. H Zhao, arXiv:2306.130892023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, arXiv:2305.106012023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, L Gianinazzi, J Gajda, arXiv:2308.096872023</p>
<p>Specter: Document-level representation learning using citation-informed transformers. A Cohan, S Feldman, I Beltagy, D Downey, D S Weld, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2020</p>
<p>Neighborhood contrastive learning for scientific document representations with citation embeddings. M Ostendorff, N Rethmeier, I Augenstein, B Gipp, G Rehm, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2022</p>
<p>ConGraT: Self-supervised contrastive pretraining for joint graph and text embeddings. W Brannon, arXiv:2305.143212023</p>
<p>TouchUp-G: Improving feature representation through graph-centric finetuning. J Zhu, X Song, V N Ioannidis, D Koutra, C Faloutsos, arXiv:2309.138852023</p>
<p>GRENADE: Graph-centric language model for self-supervised representation learning on text-attributed graphs. Y Li, K Ding, K Lee, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>TwHIN-BERT: A socially-enriched pre-trained language model for multilingual tweet representations at twitter. X Zhang, Y Malkov, O Florez, S Park, B Mcwilliams, J Han, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2023</p>
<p>Minimallysupervised structure-rich text categorization via learning on text-rich networks. X Zhang, C Zhang, X L Dong, J Shang, J Han, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2021</p>
<p>Node feature extraction by self-supervised multi-scale neighborhood prediction. E Chien, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Metadata-induced contrastive learning for zero-shot multi-label text classification. Y Zhang, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2022</p>
<p>E2EG: End-to-end node classification using graph topology and text-based node attributes. T A Dinh, J D Boef, J Cornelisse, P Groth, arXiv:2208.046092022</p>
<p>WalkLM: A uniform language model fine-tuning framework for attributed graph embedding. Y Tan, Z Zhou, H Lv, W Liu, C Yang, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>Learning on large-scale text-attributed graphs via variational inference. J Zhao, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2023</p>
<p>Augmenting low-resource text classification with graph-grounded pre-training and prompting. Z Wen, Y Fang, Proc. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval2023</p>
<p>Label-free node classification on graphs with large language models (LLMS). Z Chen, arXiv:2310.046682023</p>
<p>GraphText: Graph reasoning in text space. J Zhao, arXiv:2310.010892023</p>
<p>GNN-LM: Language modeling based on global contexts via GNN. Y Meng, S Zong, X Li, X Sun, T Zhang, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>GreaseLM: Graph reasoning enhanced language models for question answering. X Zhang, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Efficient and effective training of language and graph neural network models. V N Ioannidis, Proc. AAAI Conf. AAAI Conf2023</p>
<p>Train your own GNN teacher: Graph-aware distillation on textual graphs. C Mavromatis, Proc. null2023</p>
<p>Explanations as features: LLM-based features for text-attributed graphs. X He, X Bresson, T Laurent, B Hooi, arXiv:2305.195232023</p>
<p>Empower text-attributed graphs learning with large language models (LLMs). J Yu, Y Ren, C Gong, J Tan, X Li, X Zhang, arXiv:2310.098722023</p>
<p>GraphFormers: GNN-nested transformers for representation learning on textual graph. J Yang, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2021</p>
<p>Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. B Jin, Y Zhang, Q Zhu, J Han, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2023</p>
<p>Edgeformers: Graph-empowered transformers for representation learning on textual-edge networks. B Jin, Y Zhang, Y Meng, J Han, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2023</p>
<p>Learning multiplex embeddings on text-rich networks with one text encoder. B Jin, W Zhang, Y Zhang, Y Meng, H Zhao, J Han, arXiv:2310.066842023</p>
<p>Disentangled representation learning with large language models for text-attributed graphs. Y Qin, X Wang, Z Zhang, W Zhu, arXiv:2310.181522023</p>
<p>TextGNN: Improving text encoder via graph neural network in sponsored search. J Zhu, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2021</p>
<p>Adsgnn: Behavior-graph augmented relevance modeling in sponsored search. C Li, Proc. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval, 2021. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval, 2021</p>
<p>Fast multiresolution transformer fine-tuning for extreme multi-label text classification. J Zhang, W C Chang, H F Yu, I Dhillon, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2021</p>
<p>Graph-aware language model pre-training on a large graph corpus can help multiple graph applications. H Xie, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2023</p>
<p>Deep bidirectional language-knowledge graph pretraining. M Yasunaga, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Can LLMs effectively leverage graph structural information: When and why. J Huang, X Zhang, Q Mei, J Ma, arXiv:2309.165952023</p>
<p>Adversarial robustness for large language NER models using disentanglement and word attributions. X Jin, B Vinzamuri, S Venkatapathy, H Ji, P Natarajan, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2017</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2017</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2018</p>
<p>Graph-less neural networks: Teaching old MLPs new tricks via distillation. S Zhang, Y Liu, Y Sun, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Towards deeper graph neural networks. M Liu, H Gao, S Ji, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2020</p>
<p>Generating training data with language models: Towards zero-shot language understanding. Y Meng, J Huang, Y Zhang, J Han, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>PathSim: Meta path-based top-k similarity search in heterogeneous information networks. Y Sun, J Han, X Yan, P S Yu, T Wu, Proc. VLDB Endowment. VLDB Endowment20114</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>Unsupervised attributed multiplex network embedding. C Park, D Kim, J Han, H Yu, Proc. AAAI Conf. AAAI Conf2020</p>
<p>Attention is all you need. A Vaswani, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2017</p>
<p>Topic-sensitive pagerank. T H Haveliwala, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2002</p>
<p>Representation learning with contrastive predictive coding. A V D Oord, Y Li, O Vinyals, arXiv:1807.037482018</p>
<p>Learning transferable visual models from natural language supervision. A Radford, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2021</p>
<p>Decoding the silent majority: Inducing belief augmented social graph with large language model for response forecasting. C Sun, arXiv:2310.132972023</p>
<p>Measuring the effect of influential messages on varying personas. C Sun, J Li, H P Chan, C Zhai, H Ji, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2023</p>
<p>Legal networks: The promises and challenges of legal network analysis. R Whalen, 2016ArtMichigan State Law Rev</p>
<p>Situation entity types: Automatic classification of clause-level aspect. A Friedrich, A Palmer, M Pinkal, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2016</p>
<p>LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. N Guha, arXiv:2308.114622023</p>
<p>Personalized entity resolution with dynamic heterogeneous knowledge graph representations. Y Lin, arXiv:2104.026672021</p>
<p>Scientific paper recommendation: A survey. X Bai, M Wang, I Lee, Z Yang, X Kong, F Xia, IEEE Access. 72019</p>
<p>Research paper classification using supervised machine learning techniques. S Chowdhury, M P Schoen, Proc. Int. Eng. Technol. Comput. 2020</p>
<p>Author identification on the large scale. D Madigan, A Genkin, D D Lewis, S Argamon, D Fradkin, L Ye, Proc. Meeting. MeetingClassification Soc. North Amer2005</p>
<p>LightGCN: Simplifying and powering graph convolution network for recommendation. X He, K Deng, X Wang, Y Li, Y Zhang, M Wang, Proc. 43rd Int. 43rd Int2020</p>
<p>Bundle recommendation with graph convolutional networks. J Chang, C Gao, X He, D Jin, Y Li, Proc. Int. Int2020</p>
<p>Open-world learning and application to product classification. H Xu, B Liu, L Shu, P Yu, Proc. Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2019</p>
<p>Friend recommendation based on multi-social graph convolutional network. L Chen, Y Xie, Z Zheng, H Zheng, J Xie, IEEE Access. 82020</p>
<p>Unsupervised clickstream clustering for user behavior analysis. G Wang, X Zhang, S Tang, H Zheng, Proc. CHI Conf. CHI Conf2016</p>
<p>Overlapping community detection with graph neural networks. O Shchur, S Günnemann, arXiv:1909.122012019</p>
<p>Large language models are zero-shotreasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Proc. null202235</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Language models are unsupervised multitask learners. A Radford, OpenAI Blog. 182019</p>
<p>ALBERT: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2019</p>
<p>ELECTRA: Pretraining text encoders as discriminators rather than generators. K Clark, M T Luong, Q V Le, C D Manning, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2019</p>
<p>Sparks of artificial general intelligence: Early experiments with GPT-4. S Bubeck, arXiv:2303.127122023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, arXiv:2307.092882023</p>
<p>A Q Jiang, arXiv:2310.06825Mistral 7B. 2023</p>
<p>Flamingo: A visual language model for fewshot learning. J B Alayrac, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Text2Mol: Cross-modal molecule retrieval with natural language queries. C Edwards, C Zhai, H Ji, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2022</p>
<p>Translation between molecules and natural language. C Edwards, T Lai, K Ros, G Honke, H Ji, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2022</p>
<p>Can language models solve graph problems in natural language?. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, arXiv:2305.100372023</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. C Liu, B Wu, arXiv:2308.112242023. 2023</p>
<p>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, arXiv:2305.150662023</p>
<p>Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by ChatGPT. J Zhang, arXiv:2304.111162023</p>
<p>LLM4DyG: Can large language models solve problems on dynamic graphs?. Z Zhang, arXiv:2310.171102023</p>
<p>Reasoning on graphs: Faithful and interpretable large language model reasoning. L Luo, Y F Li, G Haffari, S Pan, arXiv:2310.010612023</p>
<p>StructGPT: A general framework for large language model to reason over structured data. J Jiang, K Zhou, Z Dong, K Ye, W X Zhao, J R Wen, arXiv:2305.096452023</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, arXiv:2310.045602023</p>
<p>Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. J Sun, arXiv:2307.076972023</p>
<p>Developing algorithms and software for geometric path planning problems. D Z Chen, 10.1145/242224.242246ACM Comput. Surv. 28181996</p>
<p>Airline scheduling with max flow algorithm. A Iqbal, M Hossain, A Ebna, Proc. Int. Joint Conf. Int. Joint Conf2018</p>
<p>Scheduling the covering delivery problem in last mile delivery. L Jiang, X Zang, I I Alghoul, X Fang, J Dong, C Liang, Expert Syst. Appl. 1872022Art. no. 115894</p>
<p>Graph-based knowledge tracing: Modeling student proficiency using graph neural network. H Nakagawa, Y Iwasawa, Y Matsuo, Proc. IEEE/WIC/ACM Int. Conf. Web Intell. IEEE/WIC/ACM Int. Conf. Web Intell2019</p>
<p>Peer-inspired student performance prediction in interactive online question pools with graph neural network. H Li, H Wei, Y Wang, Y Song, H Qu, Proc. 29th ACM Int. Conf. Inf. Knowl. Manage. 29th ACM Int. Conf. Inf. Knowl. Manage2020</p>
<p>Artificial intelligence for science in quantum, atomistic, and continuum systems. X Zhang, arXiv:2307.084232023</p>
<p>A survey on oversmoothing in graph neural networks. T K Rusch, M M Bronstein, S Mishra, arXiv:2303.109932023</p>
<p>Understanding over-squashing and bottlenecks on graphs via curvature. J Topping, F D Giovanni, B P Chamberlain, X Dong, M M Bronstein, arXiv:2111.145222021</p>
<p>Do transformers really perform badly for graph representation?. C Ying, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2021</p>
<p>Recipe for a general, powerful, scalable graph transformer. L Rampášek, M Galkin, V P Dwivedi, A T Luu, G Wolf, D Beaini, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Semi-supervised graph imbalanced regression. G Liu, T Zhao, E Inae, T Luo, M Jiang, arXiv:2305.120872023</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Q Wu, W Zhao, Z Li, D P Wipf, J Yan, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Graph rationalization with environment-based augmentations. G Liu, T Zhao, J Xu, T Luo, M Jiang, ACM Trans. Knowl. Discov. Data. 1842022</p>
<p>Chem-Crow: Augmenting large-language models with chemistry tools. A M Bran, S Cox, A D White, P Schwaller, arXiv:2304.053762023</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>IAM graph database repository for graph based pattern recognition and machine learning. K Riesen, H Bunke, Proc. Struct., Syntactic, Statist. Pattern Recognit. Joint IAPR Workshop. Struct., Syntactic, Statist. Pattern Recognit. Joint IAPR Workshop2008</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 2811988</p>
<p>InChI-the worldwide chemical structure identifier standard. S Heller, A Mcnaught, S Stein, D Tchekhovskoi, I Pletnev, J. Cheminformatics. 512013</p>
<p>DeepSMILES: An adaptation of SMILES for use in machine-learning of chemical structures. N O'boyle, A Dalke, 2018</p>
<p>Selfreferencing embedded strings (SELFIES): A 100% robust molecular string representation. M Krenn, F Häse, A Nigam, P Friederich, A Aspuru-Guzik, Mach. Learn.: Sci. Technol. 142020</p>
<p>SMILES enumeration as data augmentation for neural network modeling of molecules. E J Bjerrum, arXiv:1703.070762017</p>
<p>Randomized SMILES strings improve the quality of molecular generative models. J Arús-Pous, J. Cheminformatics. 1112019</p>
<p>Augmentation is what you need!. I V Tetko, P Karpov, E Bruno, T B Kimber, G Godin, Proc. Int. Conf. Artif. Neural Netw. Int. Conf. Artif. Neural NetwSpringer International Publishing2019</p>
<p>SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. T Kudo, J Richardson, Proc. Conf. Empir. Methods Natural Lang. Conf. Empir. Methods Natural Lang2018</p>
<p>Chemformer: A pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Mach. Learn. Sci. Technol. 312022Art. no. 015022</p>
<p>ReLM: Leveraging language models for enhanced chemical reaction prediction. Y Shi, A Zhang, E Zhang, Z Liu, X Wang, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, Z Ren, arXiv:2308.069112023</p>
<p>Catalyst property prediction with CatBERTa: Unveiling feature exploration strategies through large language models. J Ock, C Guntuboina, A B Farimani, arXiv:2309.005632023</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Y Fang, arXiv:2306.080182023</p>
<p>Prot2Text: Multimodal protein's function generation with GNNs and transformers. H Abdine, M Chatzianastasis, C Bouyioukos, M Vazirgiannis, arXiv:2307.143672023</p>
<p>MolFM: A multimodal molecular foundation model. Y Luo, K Yang, M Hong, X Liu, Z Nie, arXiv:2307.094842023</p>
<p>Can large language models empower molecular property prediction?. C Qian, H Tang, Z Yang, H Liang, Y Liu, arXiv:2307.074432023</p>
<p>Regression transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, Nature Mach. Intell. 542023</p>
<p>Empowering molecule discovery for molecule-caption translation with large language models: A ChatGPT perspective. J Li, arXiv:2306.066152023</p>
<p>Interactive molecular discovery with natural language. Z Zeng, B Yin, S Wang, J Liu, C Yang, H Yao, arXiv:2306.119762023</p>
<p>MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter. Z Liu, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. T Guo, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>MolXPT: Wrapping molecules with text for generative pre-training. Z Liu, W Zhang, Y Xia, L Wu, S Xie, T Qin, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2023</p>
<p>Enhancing activity prediction models in drug discovery with the ability to understand human language. P Seidl, A Vall, S Hochreiter, G Klambauer, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2023</p>
<p>Unifying molecular and textual representations via multitask language modelling. D Christofidellis, G Giannone, J Born, O Winther, T Laino, M Manica, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2023</p>
<p>A multi-modal molecule structure-text model for textbased retrieval and editing. S Liu, Nature Mach. Intell. 52023</p>
<p>Extracting molecular properties from natural language with multimodal contrastive learning. R Lacombe, A Gaut, J He, D Lüdeke, K Pistunova, Proc. Int. Conf. Mach. Learn. Workshop Comput. Biol. 2023</p>
<p>A molecular multimodal foundation model associating molecule graphs with natural language. B Su, arXiv:2209.054812022</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Z Zeng, Y Yao, Z Liu, M Sun, Nature Commun. 138622022</p>
<p>Functional output regression for machine learning in materials science. M Iwayama, S Wu, C Liu, R Yoshida, J. Chem. Inf. Model. 62202022</p>
<p>MolGPT: Molecular generation using a transformer-decoder model. V Bagal, R Aggarwal, P K Vinod, U D Priyakumar, J. Chem. Inf. Model. 6292021</p>
<p>Galactica: A large language model for science. R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, arXiv:2211.090852022</p>
<p>Smiles-BERT: Large scale unsupervised pre-training for molecular property prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, Proc. ACM Int. Conf. Bioinf. Comput. Biol. Health Inform. 2019</p>
<p>BioBERT: A pre-trained biomedical language representation model for biomedical text mining. J Lee, Bioinformatics. 3642020</p>
<p>PI1M: A benchmark database for polymer informatics. R Ma, T Luo, J. Chem. Inf. Model. 602020</p>
<p>ChEBI in 2016: Improved services and an expanding collection of metabolites. J Hastings, Nucleic acids Res. 442016</p>
<p>PubChem 2019 update: Improved access to chemical data. S Kim, Nucleic Acids Res. 47D12019</p>
<p>ChEMBL: A large-scale bioactivity database for drug discovery. A Gaulton, Nucleic Acids Res. 402012</p>
<p>The ChEMBL database in 2023: A drug discovery platform spanning multiple bioactivity data types and time periods. B Zdrazil, Nucleic Acids Res. 532024</p>
<p>Molecular fingerprint-derived similarity measures for toxicological read-across: Recommendations for optimal use. C L Mellor, Regulatory Toxicol. Pharmacol. 2019</p>
<p>SELFIES and the future of molecular string representations. M Krenn, Patterns. 3102022. 100588</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2020</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2019</p>
<p>BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.12597</p>
<p>Moflow: An invertible flow model for generating molecular graphs. C Zang, F Wang, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2020</p>
<p>Datacentric learning from unlabeled graphs with diffusion model. G Liu, E Inae, T Zhao, J Xu, T Luo, M Jiang, arXiv:2303.101082023</p>
<p>Knowledge graph prompting for multi-document question answering. Y Wang, N Lipka, R A Rossi, A Siu, R Zhang, T Derr, Proc. Conf. Assoc. Advance. Conf. Assoc. Advance2024</p>
<p>GraSeq: Graph and sequence fusion learning for molecular property prediction. Z Guo, W Yu, C Zhang, M Jiang, N V Chawla, Proc. Int. Conf. Inf. Knowl. Manage. Int. Conf. Inf. Knowl. Manage2020</p>
<p>Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts. W Yu, C Zhu, L Qin, Z Zhang, T Zhao, M Jiang, Proc. Conf. Assoc. Comput. Linguistics Findings. Conf. Assoc. Comput. Linguistics Findings2022</p>
<p>A systematic study of key elements underlying molecular property prediction. J Deng, Z Yang, H Wang, I Ojima, D Samaras, F Wang, Nature Commun. 1412023</p>
<p>On failure modes in molecule generation and optimization. P Renz, D Van Rompaey, J K Wegner, S Hochreiter, G Klambauer, Drug Discov. Today, Technol. 322019</p>
<p>The chemical space project. J L Reymond, Accounts Chem. Res. 4832015</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 62079722023</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>Chemical-reaction-aware molecule representation learning. H Wang, arXiv:2109.098882021</p>
<p>KEBLM: Knowledge-enhanced biomedical language models. T M Lai, C Zhai, H Ji, J. Biomed. Informat. 1432023. 104392</p>
<p>Inverse molecular design with multi-conditional diffusion guidance. G Liu, J Xu, T Luo, M Jiang, arXiv:2401.138582024</p>
<p>The future is not one-dimensional: Complex event schema induction by graph modeling for event prediction. M Li, arXiv:2104.063442021</p>
<p>Advancing graph representation learning with large language models: A comprehensive survey of techniques. Q Mao, Z Liu, C Liu, Z Li, J Sun, arXiv:2402.059522024</p>
<p>A survey of graph meets large language model: Progress and future directions. Y Li, arXiv:2311.123992023</p>
<p>Policy shaping: Integrating human feedback with reinforcement learning. S Griffith, K Subramanian, J Scholz, C L Isbell, A L Thomaz, Proc. Int. Conf. Neural Inf. Int. Conf. Neural Inf2013</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2024</p>
<p>A survey of large language models. W X Zhao, arXiv:2303.182232023</p>
<p>LM-infinite: Simple on-the-fly length generalization for large language models. C Han, Q Wang, W Xiong, Y Chen, H Ji, S Wang, arXiv:2308.161372023</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, M Ahmed, Y Lu, S Pan, W Bo, Y Liu, Neurocomputing. 5682024. 127063</p>            </div>
        </div>

    </div>
</body>
</html>
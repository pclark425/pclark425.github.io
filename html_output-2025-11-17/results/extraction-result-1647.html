<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1647 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1647</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1647</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-233241224</p>
                <p><strong>Paper Title:</strong> Auto-Tuned Sim-to-Real Transfer</p>
                <p><strong>Paper Abstract:</strong> Policies trained in simulation often fail when transferred to the real world due to the ‘reality gap’ where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-to-real transfer, demonstrating significant improvement over naive domain randomization. Project videos at https://yuqingd.github.io/autotuned-sim2real/.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1647.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1647.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rope Peg-in-Hole</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rope Peg-in-Hole task (real-world experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotic manipulation task where a 7-DOF UFactory xArm 7 must move a peg (or rope-attached peg) toward and into a hole; used to evaluate sim-to-real transfer of policies learned in simulation and auto-tuned via the SPM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>UFactory xArm 7 (Rope Peg-in-Hole)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF industrial manipulator (xArm 7) equipped with RGB camera observation (Intel RealSense) used to execute a peg-in-hole style insertion/motion task; perception is RGB-only from an over-the-shoulder viewpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>simulator (custom; unspecified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A task-specific simulator used by the authors that models scene dynamics and RGB observations and allows explicit parametrization of visual and dynamics system parameters (ξ_sim) to produce rendered trajectories for RL and SPM training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics/dynamics with randomized visual rendering (not claimed to be high-fidelity physics or photorealistic rendering)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Parameterized dynamics (e.g., contact-related parameters such as friction and damping), actuator/robot dynamics abstractions, and RGB visual rendering (appearance, lighting, camera viewpoint).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Contact and complex interaction effects are simplified/parametric rather than fully high-fidelity; no depth sensing used (RGB only); only a small set of dynamics parameters randomized and intentionally chosen to have non-interacting effects; the exact physics engine is not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical setup using a UFactory xArm 7 robot and an Intel RealSense RGB camera placed at an over-the-shoulder angle; real rollouts collected without instrumentation (no state trackers or AR tags).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Manipulation: moving a peg toward/in a hole (peg-in-hole style); evaluated whether a policy trained in simulation transfers to physically executing the motion in reality.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (Soft Actor-Critic with contrastive image representations) trained in simulation; pretraining with domain randomization, then iterative auto-tuning of simulator parameters using the Search Param Model (SPM) with occasional real rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate over 10 rollouts (authors also use a redefined 'near-success' metric for this task: robot moves object over the hole even if it does not fully insert), plus qualitative trajectory comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>40% success (authors report 40% for their method; domain-randomization baseline 0%; note: rope task success included 'near-success' redefinition due to difficulty assessing insertion from RGB only).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Visual and dynamics parameters randomized during pretraining; initialization used a misparametrized range (authors mention ranges like 3/4x or 4/3x of pseudo-real values); examples of parameters include contact friction, damping, visual appearance/lighting, and other system parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Misparametrized simulation mean and limited initial randomization coverage, unmodeled or hard-to-identify contact dynamics (friction/damping), limited sensor modality (RGB-only; no depth), camera viewpoint differences, and difficulty inferring some dynamics from image sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Iterative auto-tuning of simulator parameter mean using the SPM fed with unlabeled real RGB trajectory sequences; pretraining SPM and policy in simulation with domain randomization; alternating training where SPM is trained with a wider parameter range (r_sp) while policy trains on a narrower range (r_policy); occasional real rollouts used to update simulator mean and continue policy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate modeling of dynamics/contact-related parameters (e.g., friction, damping) is important for successful transfer; initial randomization must not be too far from reality (extremely misparametrized initializations make finding correct parameters harder); some parameters are difficult to infer from images alone.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>After pretraining in misparametrized simulation, the policy is fine-tuned jointly with SPM updates: occasional real-world rollouts are collected, SPM predicts parameter adjustments, simulator mean (ξ_mean) is incrementally updated, and policy training continues in the updated simulator; authors state this required substantially fewer simulation policy steps than the baseline (their method used <20% of the simulation policy steps of the baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using the SPM to auto-tune simulation parameters from unlabeled real RGB sequences substantially improved sim-to-real transfer for the rope peg-in-hole task compared to naive domain randomization (40% vs 0% reported success when including near-success). Success is limited by RGB-only perception and difficulty identifying certain contact dynamics; initial simulator parameter proximity to reality matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Tuned Sim-to-Real Transfer', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1647.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1647.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cabinet Slide</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cabinet Slide task (real-world experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotic manipulation task where the xArm must grasp/press the cabinet handle and slide open a cabinet lid; used to evaluate sim-to-real transfer effectiveness of the auto-tuning approach versus a domain-randomized baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>UFactory xArm 7 (Cabinet Slide)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF UFactory xArm 7 robot executing a cabinet-handle/slide task, observed through an over-the-shoulder Intel RealSense RGB camera; evaluated for real-world success after simulation-trained policy transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>simulator (custom; unspecified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator parameterized by visual and dynamics system parameters (ξ_sim) and capable of producing simulated RGB observation trajectories and parameterized dynamics for training RL policies and the SPM.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate dynamics modeling with parameterized visual rendering; not presented as high-fidelity contact physics or photorealistic rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Dynamics parameters affecting interactions (e.g., friction, damping and other contact-related parameters), actuator/robot dynamics abstractions, and RGB visual rendering (appearance, camera viewpoint).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simplified contact interactions and limited parameter set (authors intentionally picked a small set of largely non-interacting parameters); lack of depth data; potential omission of complicated real-world couplings that affect sliding/overshoot.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical experiment with UFactory xArm 7 and Intel RealSense RGB camera (over-the-shoulder); real rollouts collected without instrumentation like AR tags or state trackers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Manipulation: approach and slide open the cabinet lid by interacting with the handle; evaluated whether simulation-trained policies transfer to consistent opening behavior in reality.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (SAC with contrastive visual representations) trained in simulation with domain randomization pretraining and then iteratively fine-tuned while the simulator is auto-tuned via SPM using real RGB rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate over 10 rollouts (task completion: consistently sliding the cabinet lid open); qualitative trajectory comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>90% success for the authors' method vs 0% for the naive domain-randomization baseline (baseline policies tended to move toward the handle but consistently overshot).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Visual and dynamics parameters randomized in pretraining; initial misparametrized ranges used (authors mention ranges like 3/2x or 2/3x for this task); parameters included dynamics (friction, damping) and visual variations.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Misparametrized simulator mean, inaccuracies in contact dynamics and friction modeling leading to overshoot behaviors, limited perceptual information (RGB-only), and differences between simulated and real interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Auto-tuning simulator mean using SPM predictions from unlabeled real RGB sequences, alternating training that keeps the SPM trained on a wider parameter range while the policy trains on a narrower range, and iterative real-rollout-guided updates; these allowed the policy to correct for the overshoot behavior and achieve high real-world success.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate dynamics/contact parameterization (friction, damping) is critical for this sliding manipulation; with reasonable parameter coverage and auto-tuning, policies trained in simulation can transfer reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>After simulation pretraining, occasional real rollouts were executed and SPM outputs were used to adjust the simulator mean; policy training continued in the updated simulator until consistent real-world success was achieved. The authors note their method required substantially fewer simulation policy steps than the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Auto-tuning simulator parameters with the SPM using unlabeled real RGB sequences produced a policy that transferred robustly to the real cabinet-slide task (90% success) while the domain-randomized baseline failed (0%); accurate contact/dynamics parameterization and iterative real-guided simulator updates enabled this high transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Tuned Sim-to-Real Transfer', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1647.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1647.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search Parameter Model (SPM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned binary classifier that, given a sequence of observations and actions plus a candidate set of simulator parameters, predicts for each parameter whether the true (real-world) value is higher, lower, or approximately equal — enabling iterative search-style auto-tuning of simulator parameters from unlabeled real RGB sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Search Parameter Model (SPM) — simulator auto-tuner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A convolutional-encoder + per-parameter MLP architecture trained via logistic regression on simulated trajectory windows that outputs per-parameter binary predictions (higher/lower) relative to a candidate parameter value; used to iteratively update simulator parameter means based on real trajectory observations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>simulator (custom; unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same simulator used for policy training: provides parameterized dynamics and RGB rendering to generate labeled simulated trajectories (labels are derived by comparing sampled ξ_sim to candidate ξ_pred).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate/parametric simulation fidelity that models a controllable subset of dynamics and visual parameters rather than complete high-fidelity physics or photorealistic rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Parameterized dynamics and visual factors that can be varied for training (e.g., friction, damping, mass-like parameters, lighting/appearance), and the ability to render RGB observations for training the SPM and policy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not require modeling of every real-world coupling; SPM assumes parameters can be varied independently and that trajectories contain identifiable signals for each parameter — it struggles when many interacting parameters or extreme misparametrizations exist.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>SPM operates on unlabeled sequences of RGB images and actions collected from the real robot (authors used UFactory xArm 7 with RealSense RGB camera).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>SPM itself is not a skill but a method to enable sim-to-real transfer of downstream manipulation policies by adjusting the simulator to better match reality using unlabeled image/action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised binary classification (logistic regression objective) on simulated sequences: pretrain SPM on trajectories from randomized simulation using a random policy, then continue training jointly while the policy learns; encodes images with conv encoder, stacks frames, concatenates actions and candidate ξ_pred, uses sinusoidal encoding for parameter values.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Measured indirectly by improvement in downstream policy real-world success rates (e.g., enabling jumps from 0% to 40% or 90% in their tasks) and by percentage error in estimated simulation-parameter mean (percent error curves reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Enabled improved downstream real-world success (e.g., authors' method achieved 40% and 90% on two tasks where the DR baseline had 0%); SPM also reduced percent error of simulator mean in many environments during experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>SPM pretraining uses a wide distribution of simulator parameters (Ξ) sampled uniformly around a mean (ξ_mean); authors use a larger range for SPM data (r_sp) and a smaller range for policy (r_policy) during joint training; candidate ξ_pred sampled from [0, 2 * ξ_mean].</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>SPM is designed to close gaps caused by misparametrized simulator dynamics/visuals but is hindered when the initial randomization is too far from reality, when dynamics parameters produce subtle signals not visible in image sequences, or when many interacting parameters exist.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Pretraining SPM on a wide simulated parameter distribution; using sequence windows (stacked frames) to capture temporal cues; sinusoidal encoding for continuous parameter inputs; using separate buffers for SPM and policy with r_sp > r_policy; iterative collection of real trajectories and incremental updates to ξ_mean guided by SPM confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>SPM requires that the simulator parameter set includes the true-world parameters within a searchable region (or be close enough to converge); contact/dynamics parameters that strongly affect trajectories are most critical to model; some parameters are intrinsically hard to infer from pixels and reduce SPM efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>After pretraining SPM on simulated random-policy data, the method collects occasional unlabeled real trajectory sequences; SPM predicts per-parameter higher/lower signals and the simulator mean is updated incrementally (confidence-weighted); policy training and SPM training alternate so that the agent can acquire behaviors that expose dynamics informative to the SPM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The classification-based SPM approach enables automatic, image-only system identification that moves simulator parameter means closer to reality and improves downstream sim-to-real transfer — outperforming naive domain randomization and direct parameter regression baselines in the tasks tested; however, SPM performance depends on (1) having a parameterizable simulator whose parameterization includes the real dynamics, (2) sufficient pretraining diversity, and (3) temporal behaviors that expose parameter effects in image sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Tuned Sim-to-Real Transfer', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Preparing for the unknown: Learning a universal policy with online system identification <em>(Rating: 1)</em></li>
                <li>Fast model identification via physics engines for data-efficient policy search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1647",
    "paper_id": "paper-233241224",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Rope Peg-in-Hole",
            "name_full": "Rope Peg-in-Hole task (real-world experiment)",
            "brief_description": "A robotic manipulation task where a 7-DOF UFactory xArm 7 must move a peg (or rope-attached peg) toward and into a hole; used to evaluate sim-to-real transfer of policies learned in simulation and auto-tuned via the SPM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "UFactory xArm 7 (Rope Peg-in-Hole)",
            "agent_system_description": "A 7-DOF industrial manipulator (xArm 7) equipped with RGB camera observation (Intel RealSense) used to execute a peg-in-hole style insertion/motion task; perception is RGB-only from an over-the-shoulder viewpoint.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "simulator (custom; unspecified in paper)",
            "virtual_environment_description": "A task-specific simulator used by the authors that models scene dynamics and RGB observations and allows explicit parametrization of visual and dynamics system parameters (ξ_sim) to produce rendered trajectories for RL and SPM training.",
            "simulation_fidelity_level": "approximate physics/dynamics with randomized visual rendering (not claimed to be high-fidelity physics or photorealistic rendering)",
            "fidelity_aspects_modeled": "Parameterized dynamics (e.g., contact-related parameters such as friction and damping), actuator/robot dynamics abstractions, and RGB visual rendering (appearance, lighting, camera viewpoint).",
            "fidelity_aspects_simplified": "Contact and complex interaction effects are simplified/parametric rather than fully high-fidelity; no depth sensing used (RGB only); only a small set of dynamics parameters randomized and intentionally chosen to have non-interacting effects; the exact physics engine is not specified.",
            "real_environment_description": "Physical setup using a UFactory xArm 7 robot and an Intel RealSense RGB camera placed at an over-the-shoulder angle; real rollouts collected without instrumentation (no state trackers or AR tags).",
            "task_or_skill_transferred": "Manipulation: moving a peg toward/in a hole (peg-in-hole style); evaluated whether a policy trained in simulation transfers to physically executing the motion in reality.",
            "training_method": "Reinforcement learning (Soft Actor-Critic with contrastive image representations) trained in simulation; pretraining with domain randomization, then iterative auto-tuning of simulator parameters using the Search Param Model (SPM) with occasional real rollouts.",
            "transfer_success_metric": "Success rate over 10 rollouts (authors also use a redefined 'near-success' metric for this task: robot moves object over the hole even if it does not fully insert), plus qualitative trajectory comparison.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "40% success (authors report 40% for their method; domain-randomization baseline 0%; note: rope task success included 'near-success' redefinition due to difficulty assessing insertion from RGB only).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Visual and dynamics parameters randomized during pretraining; initialization used a misparametrized range (authors mention ranges like 3/4x or 4/3x of pseudo-real values); examples of parameters include contact friction, damping, visual appearance/lighting, and other system parameters.",
            "sim_to_real_gap_factors": "Misparametrized simulation mean and limited initial randomization coverage, unmodeled or hard-to-identify contact dynamics (friction/damping), limited sensor modality (RGB-only; no depth), camera viewpoint differences, and difficulty inferring some dynamics from image sequences.",
            "transfer_enabling_conditions": "Iterative auto-tuning of simulator parameter mean using the SPM fed with unlabeled real RGB trajectory sequences; pretraining SPM and policy in simulation with domain randomization; alternating training where SPM is trained with a wider parameter range (r_sp) while policy trains on a narrower range (r_policy); occasional real rollouts used to update simulator mean and continue policy fine-tuning.",
            "fidelity_requirements_identified": "Accurate modeling of dynamics/contact-related parameters (e.g., friction, damping) is important for successful transfer; initial randomization must not be too far from reality (extremely misparametrized initializations make finding correct parameters harder); some parameters are difficult to infer from images alone.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "After pretraining in misparametrized simulation, the policy is fine-tuned jointly with SPM updates: occasional real-world rollouts are collected, SPM predicts parameter adjustments, simulator mean (ξ_mean) is incrementally updated, and policy training continues in the updated simulator; authors state this required substantially fewer simulation policy steps than the baseline (their method used &lt;20% of the simulation policy steps of the baseline).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Using the SPM to auto-tune simulation parameters from unlabeled real RGB sequences substantially improved sim-to-real transfer for the rope peg-in-hole task compared to naive domain randomization (40% vs 0% reported success when including near-success). Success is limited by RGB-only perception and difficulty identifying certain contact dynamics; initial simulator parameter proximity to reality matters.",
            "uuid": "e1647.0",
            "source_info": {
                "paper_title": "Auto-Tuned Sim-to-Real Transfer",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Cabinet Slide",
            "name_full": "Cabinet Slide task (real-world experiment)",
            "brief_description": "A robotic manipulation task where the xArm must grasp/press the cabinet handle and slide open a cabinet lid; used to evaluate sim-to-real transfer effectiveness of the auto-tuning approach versus a domain-randomized baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "UFactory xArm 7 (Cabinet Slide)",
            "agent_system_description": "A 7-DOF UFactory xArm 7 robot executing a cabinet-handle/slide task, observed through an over-the-shoulder Intel RealSense RGB camera; evaluated for real-world success after simulation-trained policy transfer.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "simulator (custom; unspecified in paper)",
            "virtual_environment_description": "Simulator parameterized by visual and dynamics system parameters (ξ_sim) and capable of producing simulated RGB observation trajectories and parameterized dynamics for training RL policies and the SPM.",
            "simulation_fidelity_level": "approximate dynamics modeling with parameterized visual rendering; not presented as high-fidelity contact physics or photorealistic rendering.",
            "fidelity_aspects_modeled": "Dynamics parameters affecting interactions (e.g., friction, damping and other contact-related parameters), actuator/robot dynamics abstractions, and RGB visual rendering (appearance, camera viewpoint).",
            "fidelity_aspects_simplified": "Simplified contact interactions and limited parameter set (authors intentionally picked a small set of largely non-interacting parameters); lack of depth data; potential omission of complicated real-world couplings that affect sliding/overshoot.",
            "real_environment_description": "Physical experiment with UFactory xArm 7 and Intel RealSense RGB camera (over-the-shoulder); real rollouts collected without instrumentation like AR tags or state trackers.",
            "task_or_skill_transferred": "Manipulation: approach and slide open the cabinet lid by interacting with the handle; evaluated whether simulation-trained policies transfer to consistent opening behavior in reality.",
            "training_method": "Reinforcement learning (SAC with contrastive visual representations) trained in simulation with domain randomization pretraining and then iteratively fine-tuned while the simulator is auto-tuned via SPM using real RGB rollouts.",
            "transfer_success_metric": "Success rate over 10 rollouts (task completion: consistently sliding the cabinet lid open); qualitative trajectory comparison.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "90% success for the authors' method vs 0% for the naive domain-randomization baseline (baseline policies tended to move toward the handle but consistently overshot).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Visual and dynamics parameters randomized in pretraining; initial misparametrized ranges used (authors mention ranges like 3/2x or 2/3x for this task); parameters included dynamics (friction, damping) and visual variations.",
            "sim_to_real_gap_factors": "Misparametrized simulator mean, inaccuracies in contact dynamics and friction modeling leading to overshoot behaviors, limited perceptual information (RGB-only), and differences between simulated and real interactions.",
            "transfer_enabling_conditions": "Auto-tuning simulator mean using SPM predictions from unlabeled real RGB sequences, alternating training that keeps the SPM trained on a wider parameter range while the policy trains on a narrower range, and iterative real-rollout-guided updates; these allowed the policy to correct for the overshoot behavior and achieve high real-world success.",
            "fidelity_requirements_identified": "Accurate dynamics/contact parameterization (friction, damping) is critical for this sliding manipulation; with reasonable parameter coverage and auto-tuning, policies trained in simulation can transfer reliably.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "After simulation pretraining, occasional real rollouts were executed and SPM outputs were used to adjust the simulator mean; policy training continued in the updated simulator until consistent real-world success was achieved. The authors note their method required substantially fewer simulation policy steps than the baseline.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Auto-tuning simulator parameters with the SPM using unlabeled real RGB sequences produced a policy that transferred robustly to the real cabinet-slide task (90% success) while the domain-randomized baseline failed (0%); accurate contact/dynamics parameterization and iterative real-guided simulator updates enabled this high transfer performance.",
            "uuid": "e1647.1",
            "source_info": {
                "paper_title": "Auto-Tuned Sim-to-Real Transfer",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "SPM",
            "name_full": "Search Parameter Model (SPM)",
            "brief_description": "A learned binary classifier that, given a sequence of observations and actions plus a candidate set of simulator parameters, predicts for each parameter whether the true (real-world) value is higher, lower, or approximately equal — enabling iterative search-style auto-tuning of simulator parameters from unlabeled real RGB sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Search Parameter Model (SPM) — simulator auto-tuner",
            "agent_system_description": "A convolutional-encoder + per-parameter MLP architecture trained via logistic regression on simulated trajectory windows that outputs per-parameter binary predictions (higher/lower) relative to a candidate parameter value; used to iteratively update simulator parameter means based on real trajectory observations.",
            "domain": "general robotics manipulation / sim-to-real transfer",
            "virtual_environment_name": "simulator (custom; unspecified)",
            "virtual_environment_description": "Same simulator used for policy training: provides parameterized dynamics and RGB rendering to generate labeled simulated trajectories (labels are derived by comparing sampled ξ_sim to candidate ξ_pred).",
            "simulation_fidelity_level": "Approximate/parametric simulation fidelity that models a controllable subset of dynamics and visual parameters rather than complete high-fidelity physics or photorealistic rendering.",
            "fidelity_aspects_modeled": "Parameterized dynamics and visual factors that can be varied for training (e.g., friction, damping, mass-like parameters, lighting/appearance), and the ability to render RGB observations for training the SPM and policy.",
            "fidelity_aspects_simplified": "Does not require modeling of every real-world coupling; SPM assumes parameters can be varied independently and that trajectories contain identifiable signals for each parameter — it struggles when many interacting parameters or extreme misparametrizations exist.",
            "real_environment_description": "SPM operates on unlabeled sequences of RGB images and actions collected from the real robot (authors used UFactory xArm 7 with RealSense RGB camera).",
            "task_or_skill_transferred": "SPM itself is not a skill but a method to enable sim-to-real transfer of downstream manipulation policies by adjusting the simulator to better match reality using unlabeled image/action sequences.",
            "training_method": "Supervised binary classification (logistic regression objective) on simulated sequences: pretrain SPM on trajectories from randomized simulation using a random policy, then continue training jointly while the policy learns; encodes images with conv encoder, stacks frames, concatenates actions and candidate ξ_pred, uses sinusoidal encoding for parameter values.",
            "transfer_success_metric": "Measured indirectly by improvement in downstream policy real-world success rates (e.g., enabling jumps from 0% to 40% or 90% in their tasks) and by percentage error in estimated simulation-parameter mean (percent error curves reported in paper).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Enabled improved downstream real-world success (e.g., authors' method achieved 40% and 90% on two tasks where the DR baseline had 0%); SPM also reduced percent error of simulator mean in many environments during experiments.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "SPM pretraining uses a wide distribution of simulator parameters (Ξ) sampled uniformly around a mean (ξ_mean); authors use a larger range for SPM data (r_sp) and a smaller range for policy (r_policy) during joint training; candidate ξ_pred sampled from [0, 2 * ξ_mean].",
            "sim_to_real_gap_factors": "SPM is designed to close gaps caused by misparametrized simulator dynamics/visuals but is hindered when the initial randomization is too far from reality, when dynamics parameters produce subtle signals not visible in image sequences, or when many interacting parameters exist.",
            "transfer_enabling_conditions": "Pretraining SPM on a wide simulated parameter distribution; using sequence windows (stacked frames) to capture temporal cues; sinusoidal encoding for continuous parameter inputs; using separate buffers for SPM and policy with r_sp &gt; r_policy; iterative collection of real trajectories and incremental updates to ξ_mean guided by SPM confidence.",
            "fidelity_requirements_identified": "SPM requires that the simulator parameter set includes the true-world parameters within a searchable region (or be close enough to converge); contact/dynamics parameters that strongly affect trajectories are most critical to model; some parameters are intrinsically hard to infer from pixels and reduce SPM efficacy.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "After pretraining SPM on simulated random-policy data, the method collects occasional unlabeled real trajectory sequences; SPM predicts per-parameter higher/lower signals and the simulator mean is updated incrementally (confidence-weighted); policy training and SPM training alternate so that the agent can acquire behaviors that expose dynamics informative to the SPM.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "The classification-based SPM approach enables automatic, image-only system identification that moves simulator parameter means closer to reality and improves downstream sim-to-real transfer — outperforming naive domain randomization and direct parameter regression baselines in the tasks tested; however, SPM performance depends on (1) having a parameterizable simulator whose parameterization includes the real dynamics, (2) sufficient pretraining diversity, and (3) temporal behaviors that expose parameter effects in image sequences.",
            "uuid": "e1647.2",
            "source_info": {
                "paper_title": "Auto-Tuned Sim-to-Real Transfer",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Preparing for the unknown: Learning a universal policy with online system identification",
            "rating": 1,
            "sanitized_title": "preparing_for_the_unknown_learning_a_universal_policy_with_online_system_identification"
        },
        {
            "paper_title": "Fast model identification via physics engines for data-efficient policy search",
            "rating": 1,
            "sanitized_title": "fast_model_identification_via_physics_engines_for_dataefficient_policy_search"
        }
    ],
    "cost": 0.01603725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Auto-Tuned Sim-to-Real Transfer</p>
<p>Yuqing Du 
UC Berkeley</p>
<p>Olivia Watkins 
UC Berkeley</p>
<p>Trevor Darrell 
UC Berkeley</p>
<p>Pieter Abbeel 
UC Berkeley</p>
<p>Deepak Pathak 
Carnegie Mellon University</p>
<p>Auto-Tuned Sim-to-Real Transfer</p>
<p>Policies trained in simulation often fail when transferred to the real world due to the 'reality gap' where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-toreal transfer, demonstrating significant improvement over naive domain randomization. Project videos at https://yuqingd. github.io/autotuned-sim2real/ and code available at https://github.com/yuqingd/sim2real2sim_rad.</p>
<p>I. INTRODUCTION</p>
<p>Recently, there has been encouraging progress in deep learning techniques for learning complex control tasks ranging from locomotion [1], [2] to manipulation [3]. Many of these approaches, especially deep reinforcement learning, rely on learning in simulation before transferring to real world. However, this transfer is often difficult due to the 'reality gap' [4], which arises from the difficulty of accurately simulating the dynamics and visuals of the real world. One way to circumvent the gap is to learn directly from data collected by real robots [5], but this process is often time-consuming, potentially unsafe, and challenging when a reward function for the real world task is unknown.</p>
<p>On the other hand, training with simulators provides vast amounts of varied data quickly while being safer and less costly. Yet transferring a policy learned in simulation to the real world often fails due to small simulation discrepancies which errors that compound over time in the real world. Current techniques that improve transfer success rates can be categorized into three kinds: system identification, domain adaptation, and domain randomization. These methods have certain tradeoffs: (1) Traditional system identification [6] involves defining and tuning parameters that describe the real world system accurately, but often takes a lot of time *Equal contribution. Correspondence to yuqing du@berkeley.edu and oliviawatkins@berkeley.edu. Fig. 1: Policy is trained on domain-randomized data in simulation and transferred to the real-world. Instead of relying on manual engineering to tune the simulation, we automatically learn to adjust the system parameters of the simulator to generate trajectories that most closely match reality by only using raw observation images from the real world without any instrumentation or any knowledge of state and reward. and leads to systems that have to be re-tuned over time. (2) Domain adaptation methods involve learning a discriminator or other type of predictor which incentivizes a model to learn features invariant to the shift between training (simulation) and test (reality) distributions [7], however it often requires vast amounts of real world image data. In addition, there are some aspects of the domain shift the agent should not be invariant to (e.g. a robot may need to adjust its policy if the real world has much higher friction than the simulation). (3) Domain randomization methods involve training the model across a variety of simulated environments with randomized visuals [8] or dynamics [9] with the hypothesis that given enough variability in the simulations, the real world can be viewed as another 'randomized' instance, thus allowing the model to generalize to the real world.</p>
<p>This paper belongs to the broad category of methods that employ domain randomization. While this technique has become widely used in robotics, it requires task-specific expert knowledge to correctly determine which parameters to randomize and to what extent they should vary in order to learn policies that are robust to transfer without being too conservative. Domain randomization can also potentially make the task too difficult if too much randomization impedes the model's ability to learn a successful policy [10]. Recent works [11], [12] attempt to close the gap between simulation and real-world via trajectory matching but require instrumentation of the real-world via AR tags to obtain true state, which is often time-consuming and unrealistic.</p>
<p>Thus we ask the question: can we auto-tune the system parameters of simulation to match that of reality by just using raw observations (e.g. images) of the real world, thus bypassing the need for instrumenting the real world? We propose a task-agnostic reinforcement learning (RL) method that aims to reduce the amount of task-specific engineering required for domain randomization of both visual and dynamics parameters. Our method uses a small amount of real world video data and learns to slowly shift the simulation system parameters to become more similar to the real world. As directly predicting these parameters from observations is challenging, our key insight is to instead predict whether the current parameters are higher, lower, or close to the real world values in a given sequence of observations. Thus, we are able to leverage both the benefits of simulation: an abundance of varied data and ease of using a shaped reward function, as well as the benefits of the real world: providing a ground truth for improving our simulation. By moving the simulation data distribution closer to the real world data distribution, the simulation should become more representative of reality, leading to a policy more likely to succeed in the real world.</p>
<p>Our main contributions are:</p>
<p>• Proposing an automatic system identification procedure with the key insight of reformulating the problem of tuning a simulation as a search problem. • Designing a Search Param Model (SPM) that updates the system parameters using raw pixel observations of the real world. • Demonstrating that our proposed method outperforms domain randomization on a range of robotic control tasks in both sim-to-sim and sim-to-real transfer.</p>
<p>II. RELATED WORK a) Sim2Real via Domain Randomization: Domain randomization [8], the practice of training a model on several variants of a simulated environment, often enables better transfer to the real world. Prior works have randomized visuals, [13], system dynamics [9], and the placement of objects [14]. Typically, a human must use heuristics, domain knowledge, and trial-and-error to hand-engineer the distribution of simulation parameters used for training. Domain randomization alone is often insufficient for tasks which require precise motions or dexterous manipulation. [15] address this through automatic domain randomization, where they begin with a single simulation variant and then automatically increase parameter ranges as the agent learns. The wide final distribution of parameters trained on may not reflect the real-world distribution of environments. In contrast, our work incorporates real data in order to learn a distribution of simulation parameters which matches the real world. [16] learns mobile robot policies in simulation which condition on an encoded version of the simulation parameters of the environment. In the real world, they search for a set of simulation parameters which enable the policy to achieve high reward in the real world. While we also aim to find the "simulation parameters" of the real world, we do this without requiring any real-world reward supervision. [17] similarly learns a policy conditioned on a set of system parameters, but they obtain system parameters for the test environment by learning a model which predicts system parameters from observations. However, this work only demonstrates simto-sim transfer on state-based observations, a much easier problem than the image-based sim-to-real tasks we tackle. b) Sim2Real with Real Data: While training only in the real world circumvents the reality gap, work in this area is often impractical and time consuming. The recent SimOpt work [11] is most closely related to our work. Here, partial observations of the real world are leveraged to better inform simulation randomization such that the performance of policies in simulation more closely matches their performance in the real world, while avoiding reward instrumentation in the real world. However, SimOpt relies on continuous object tracking in the real world in order to make trajectory comparisons between simulation and the real world, whereas our system solely operates through pixel-based observations. [18] similarly uses realworld observations to perform system identification, but it requires real-world state and reward estimation. [19] tackles the problem of learning robotic manipulation from pixels through sequence-based self-supervision. They execute the learned policy on the real robot to gather sequences of unlabeled real world images which are used with sequences of simulated images to perform domain adaptation. While our approach also uses unlabeled real world images for simto-real transfer, we focus on improving our simulation to better match reality.</p>
<p>III. BACKGROUND AND PRELIMINARIES</p>
<p>Consider a real-world RL problem defined by a POMDP M = (S, O, A, R, γ, ξ real ), where S is the unobserved state space, O is the agent's observation space, A is the agent's action space, R : S × A → R is the reward function, γ is the discount factor, and ξ real defines the system parameters of the real world. These parameters affect both the transition dynamics P (s t+1 |s t , a t ) and the mapping from states to observations P (o t |s t ). As the reward function depends on the unobserved state s, in practice it is challenging to fully instrument the real world in order to give this reward to the agent. Instead, we train the agent in a simulation where S and R are easily accessible. The dynamics and visuals of the simulator are defined by simulator system parameters, ξ sim .</p>
<p>Domain randomization samples the simulator parameters from a distribution of N system parameters, ξ sim ∼ Ξ ⊂ R N . In our work, Ξ is a uniform distribution with mean ξ mean and range proportional to the mean, scaled by a fixed hyperparameter. Each time the simulator resets, the simulator is reparametrized with new sampled parameters ξ sim . The goal of domain randomization is to train the policy in simulation to maximize E ξsim∼Ξ [R(τ ξsim )], where τ ξ is a trajectory collected in a simulator parametrized with ξ. This policy can be trained with any reinforcement learning algorithm and in this work we use SAC [20]. By training on an appropriate distribution of simulation parameters, the policy will hopefully generalize to the real world. Overall System: Using any off-the-shelf RL algorithm, we use simulated data to train both our policy and a Search Parameter Model (SPM) q φ which predicts whether a candidate set of system parameters ξ is higher or lower than those which produced an observed trajectory. We iteratively update our simulation by running our policy in the real world and using our SPM to predict which direction to update our simulator to make it closer to the real world.</p>
<p>IV. OUR APPROACH: AUTO-TUNING SIM2REAL</p>
<p>A challenge of domain randomization is if Ξ does not cover ξ real , then it is unlikely that a policy trained on environments randomized by Ξ will transfer well to the real world. Therefore, it is common practice to choose a wide enough distribution However, this introduces an additional challenge of training a single universal policy that optimizes expected reward across all ξ ∼ Ξ, which can lead to an overly conservative policy and may be computationally expensive. Thus in order to achieve good performance with domain randomization, we must define Ξ using parameters which are close enough to ξ real that we can train on a narrow distribution of parameters.</p>
<p>Standard practice is to use expert knowledge and spend time manually engineering the environment such that parameters ξ mean reasonably approximate ξ real . Prior work [11] has proposed selecting ξ real by comparing trajectories from differently parametrized simulations and from the real world, choosing the simulation parameters which produce trajectories most similar to the real world. However, measuring trajectories requires obtaining state-space information in the real world. This is often impractical in practice, so we propose an approach that only uses raw pixel observations in the real world to find ξ mean and auto-tune our simulator.</p>
<p>A. Reformulating Auto-tuning as a Search Problem</p>
<p>To overcome the challenges of domain randomization and avoid determining the full state in the real world, we propose an approach to automatically find ξ mean ≈ ξ real using a function f : (o 1:T , a 1:T ) → ξ that maps a sequence of observations and actions to their corresponding system parameters. In fact, this is the exact problem studied in system identification. Learning such a function is challenging for two reasons. Firstly, performing system identification for complex dynamical systems from high-dimensional inputs, such as images, remains challenging. Consider the example of a robot opening a cabinet. It is challenging to identify the exact values of the damping on the cabinet hinge, the friction of the handle against the robot's arm, and so forth, solely from observations. Secondly, if the true parameters are outside of the training distribution of simulator parameters, predicting the parameters directly will likely not work. We propose to circumvent these issues by reformulating the autotuning problem as a search procedure. Rather than predicting ξ real exactly, it is much easier to train a model to detect which half-space ξ real resides in.</p>
<p>We propose a Search Param Model (SPM) which is a binary classifier q φ : (o 1:T , a 1:T , ξ pred ) → (0, 1) N which predicts the probability that the true set of system parameters which generated the trajectory o 1:T is higher, lower, or about equal to the given ξ pred for each of the N system parameters. We use this binary classifier to iteratively 'autotune' ξ mean to be closer to ξ real . To do this, we first train the SPM on simulated trajectories (o 1:T , a 1:T ) generated using the current simulation system parameters ξ sim . We then randomly sample candidate system parameters ξ pred and compute labels L = ξ sim &gt; ξ pred ∈ (0, 1) N , then train using logistic regression.</p>
<p>We train the SPM in two stages: first pretraining with the initial Ξ using simulated samples from a random policy, then in an iterative fashion jointly with policy training, described in Section IV-B. After pretraining, we can use the SPM to update ξ mean to be closer to ξ real by occasionally collecting a real world trajectory (o real 1:T , a real 1:T ). Using the SPM, we compute q φ (o real 1:T , a real 1:T , ξ mean ), which predicts whether our current parameters are too high or too low. We update each parameter in ξ mean based on the confidence of the prediction, either increasing or decreasing the parameter value incrementally for confident predictions or maintaining the same parameter for less confident predictions.</p>
<p>B. Jointly Learning to Act and Auto-tune Simulation</p>
<p>Our proposed algorithm interleaves the process of learning a policy in simulation with continuously updating the simulation using the SPM. It is often not sufficient to just pretrain the SPM, as Ξ may change significantly from the initial distribution as auto-tuning continues. Therefore we continue to train and update the SPM as we slowly shift Ξ. An alternating training process is necessary as observing the effect of certain parameters (eg. contact friction) requires learning a reasonable policy to interact with the environment first. We also want to prevent overfitting the policy to a poorly parametrized simulation if the SPM makes incorrect predictions. The alternating training algorithm is detailed in Algorithm 1 and our system is illustrated in Figure 2.</p>
<p>This joint training procedure requires carefully generating the simulation data. In order to gain intuition about the effects of different system parameters on the environment, the SPM needs to be trained on a wide distribution of parameters. On the other hand, the agent's policy often struggles to learn when the distribution of simulation parameters is too large. To address this, we use separate buffers to train the SPM and our policy, D sp and D policy respectively. The system parameters used to generate trajectories for each dataset are sampled from uniform distributions centered around the same distribution mean ξ mean , but with different range hyperparameters r sp and r policy for the SPM and policy buffers respectively, with r sp &gt; r policy .</p>
<p>D sp also stores full sequences of (o t , a t ) rather than individual transitions, as certain parameters can only be gleaned from seeing a sequence of observations.</p>
<p>Algorithm 1: Auto-Tuned Sim2Real</p>
<p>Initialize buffers D sp , D policy , agent, and SPM q φ ; Initialize simulator with Ξ 0 , parametrized by ξ mean ; Fill D sp with simulated trajectories from a random policy and pretrain q φ ; for step k=1:K do //Policy Training Phase At each reset, sample sim params ξ sim t ∼ Ξ k policy ; for training step t=1:policy itrs do Collect simulated trajectories and store into D policy ; Train policy π θ with data from D policy ; end //Sim Param Training Phase At each reset, sample sim params ξ sim t ∼ Ξ k sp ; for training step t=1:sim param itrs do Collect simulated trajectories in D sp ; Train SPM q φ with ({(o sim t , a sim t )} T t=1 , ξ sim t ) ∼ D sp ; end //Update Simulation from Real Collect samples by running π θ in the real world; Update simulator distribution using the SPM:
Ξ k+1 ← Ξ k + αq φ ({(o real t , a real t )} T t=1 , ξ k mean ); end
V. IMPLEMENTATION DETAILS a) Policy Learning: As our method of updating system parameters is independent from the agent policy training, the SPM can be combined with any image-based RL algorithm. In this work we use contrastive learning combined with SAC [21] as the image-based RL algorithm. We use the architecture and hyperparameters in Appendix E of the followup paper [22] with the following deviations: we use batch size 128, the crop augmentation, and episode lengths of 200 policy steps for DM control tasks and 60 for the robotics tasks. We use 1 million steps for the DM control environments and 500k steps for the robotics tasks.</p>
<p>b) SPM Learning: The observations o 1:T are encoded with a convolutional encoder which has the same architecture as the policy encoder from [21]. We stack 10 frames at a time, but since our trajectories are longer than 10 frames, we use multiple windows and average their predictions. We concatenate the encoded features with the agent's actions and ξ pred , which is uniformly sampled from [0, 2 × ξ mean ].</p>
<p>As each system parameter can represent very different parameters, we use sinusoidal encoding [23] to encode the real values to the same higher dimensional space. Any state information accessible in both simulation and reality can also be concatenated if desired. This is fed through separate MLPs (2 layers, 400 hidden units each) for each system parameter. We train the model using logistic regression, optimizing with Adam with learning rate = 1e −3 and β = 0.9.</p>
<p>VI. EXPERIMENTS</p>
<p>We explore two questions: a) Can our method update our simulator to the correct system parameters? b) Can our method improve real world return over sim-to-real transfer with naive domain randomization?</p>
<p>A. Sim-to-Sim Transfer</p>
<p>As proof of concept, we test our method on a suite of 6 sim-to-sim transfer tasks: 4 tasks from the Deepmind Control Suite [24] (Cheetah Run, Walker Walk, Finger Spin, and Ball in Cup Catch) and two robotic arm tasks (Rope Peg-in-Hole and Cabinet Slide).</p>
<p>For each environment we randomize both visual and dynamics parameters as listed below: For all experiments, we select one set of system parameters as the pseudo-real world and initialize our training distribution such that each parameter was either 2x or 0.5x the value of the pseudo-real world, corresponding to a poorly parametrized simulation. For our method found the best result with r sp = 1 and chose r policy = .1. We similarly tuned the domain randomization baseline with a randomization range r dr ∈ {.5, 1} and found the best result with r dr = .5. For context, we also train an oracle directly in the pseudo-real world. Figure 3 presents performance for each task when evaluated in the 'pseudo-real' world. In all environments, our method matches or exceeds the two baselines: domain randomization and a variant of our method that directly regresses on the simulation parameter values. This suggests that naively initializing domain randomization can make transferring to the real task challenging, and that learning to adjust system parameters by directly regressing Fig. 3: Performance on the pseudo-real environment over the course of training. We report reward for the DMC tasks and success on the robotic manipulation tasks. Different colors represent different experimental conditions: Oracle trains directly on the pseudo-real environment; DR Baseline uses domain randomization; Regression Baseline uses a model to directly predict the system parameters and update it using our method; Our method begins with the same inaccurate mean randomization initialization as the baselines and updates it using our classification method during training. All runs are averaged over 3 seeds. We see that our auto-tuning method consistently matches or outperforms the baseline.</p>
<p>on the parameter values is also unlikely to succeed. On the other hand, for many environments our classification method successfully adjusts our simulation parameters to center closely over the correct value.</p>
<p>We found a strong correlation between how well our model identified the correct simulation parameters and our method's performance improvement over the baseline. Figure 4 shows a few representative simulation parameter error curves which illustrate that environments in which our agent outperforms the baseline are typically the environments where simulation parameters earned accurately. The agent's performance is most sensitive to the dynamics parameters, which can prove problematic as some dynamics parameters are difficult to learn from sequences of observations and actions.</p>
<p>B. Sim-to-Real Transfer</p>
<p>We evaluate our method on sim-to-real transfer on the same two robotic arm tasks (Rope Peg-in-Hole and Cabinet Slide) which we used for sim-to-sim transfer. We implement a real world setup as shown in Figures 5 and 6. We use the UFactory xArm 7 and capture RGB observations using a Realsense camera placed at an over-the-shoulder angle.</p>
<p>As with our sim-to-sim experiments, we initialize the training distribution with a more challenging 'misparametrized' distribution with 3 4 x or 4 3 x of the parameters from our simulated pseudo-real world for the Rope Peg-in-Hole task and with 3 2 x or 2 3 x for the Cabinet Slide task. For sample efficiency, we pretrain both the policy and the SPM using domain randomization in the initial misparametrized Fig. 4: Percent error in our system parameter mean ξ mean over the course of training. The x-axis displays environment steps (×1000), and the y-axis shows percent error (100 * |ξ mean − ξ real |/ξ real ). The top row is for Walker, where we outperform the baseline, and the bottom row is for Ballin-Cup, where we inconsistently outperform the baseline. We observe that learning system parameters accurately is generally important for good policy performance.</p>
<p>training environments, then fine tune the policy and simulators jointly using our method with real robot rollouts until the task is successfully completed in the real world. We compare against a domain randomization baseline policy trained to convergence in the initial misparametrized training environment, which also required 5x more additional policy steps than our method. To evaluate the approaches, we look at ten rollouts from each method. Success rates are: challenging to learn a successful policy (e.g. trials may look successful from the camera angle but were not truly successful). As such, future work can incorporate a depth sensor. Due to this challenge our method was only successful occasionally, but qualitatively comparing trajectories in Figure 5 we see that the baseline does not move the peg towards the hole, whereas our method consistently moves towards the hole and occasionally succeeds at insertion. In the table, we redefine "success" to include near-success, where the robot moves the object over the hole, whether or not it actually enters. For the cabinet task, as shown in Figure 6, our method learns to consistently succeed, whereas the baseline moves towards the box but consistently overshoots and fails.</p>
<p>VII. CONCLUSION AND DISCUSSION</p>
<p>We proposed an approach to automatically bring the distribution of system parameters in simulation close to that in the real-world through an iterative search process. In our current work, we only randomize a small set of dynamics parameters, and we intentionally chose parameters with non-interacting effects on the dynamics of the scene. This is sufficient for some tasks, as we showed by transferring policies to the real world, but may not be enough for tasks which require more dexterous manipulation. Future work can apply our method to more complex tasks where many parameters must be learned simultaneously.</p>
<p>The main limitation of our method is that it is difficult to learn the true simulation parameters when the initial randomization is too far from the true values. While our model sometimes converges to the correct parameters even when the true value is outside the initial distribution range, learning is more challenging in this setting. Choosing an appropriate initial randomization range may require some Fig. 7: Ablations: In envs like Cheetah where the system parameters are learned easily our method is robust to different hyperparameters, but in envs such as Ball in Cup, which are sensitive to hard-to-learn dynamics, our method is less stable. Results are averaged over 3 seeds with std error bars.</p>
<p>tuning, but this process should be simpler than individually tuning each parameter as required for domain randomization.</p>
<p>Our method is most helpful for tasks like object manipulation which depend on particular visual and dynamics parameters which are challenging to measure directly. Our approach is unlikely to be helpful in environments which are very sensitive to particular hard-to-model system parameters, such as the trajectory of a leaf blowing through the wind. We also do not recommend our method for tasks which are not sensitive to system dynamics, such as navigation. While our method could still provide realistic visual randomization, domain randomization is often sufficient in these cases. While domain randomization is a strong baseline, we have shown successful transfers to simulation and reality in cases where naive domain randomization performs poorly. Our method demonstrates that using just unsupervised real-world videos we are able to adjust the randomization mean to be closer to the real system parameters, ultimately leading to improved transfer success in the real-world.</p>
<p>Fig. 2 :
2Fig. 2: Overall System: Using any off-the-shelf RL algorithm, we use simulated data to train both our policy and a Search Parameter Model (SPM) q φ which predicts whether a candidate set of system parameters ξ is higher or lower than those which produced an observed trajectory. We iteratively update our simulation by running our policy in the real world and using our SPM to predict which direction to update our simulator to make it closer to the real world.</p>
<p>Fig. 5: Rope Peg-in-Hole task. The policy transferred from the DR baseline generally does not move towards the hole, whereas our method, trained on &lt; 20% of the simulation policy steps used in the baseline, leads to a policy that consistently moves towards the hole and occasionally succeeds.Fig. 6: Cabinet Slide task. Transferring the DR baseline leads to a policy that moves towards the handle but overshoots to its left, whereas our method, trained on &lt; 20% of the simulation policy steps used in the baseline, leads to a policy that moves to the right of the handle and consistently succeeds at sliding open the cabinet lid.Environment 
Ours -Success 
DR -Success 
Rope Peg-in-Hole 
40% 
0% 
Cabinet Slide 
90% 
0% </p>
<p>For the rope task, only using RGB information made it </p>
<p>(a) Training Env </p>
<p>(b) Successful Traj -Ours 
(c) Avg Traj -DR 
(d) Avg Traj -Ours </p>
<p>(a) Training Env 
(b) Test Env 
(c) Avg Traj -DR 
(d) Avg Traj -Ours </p>
<p>ACKNOWLEDGEMENTSWe thank Albert Zhan &amp; Philip Zhao for providing the xArm experimental environment and code-base. The work is supported by Berkeley Deep Drive. YD is supported by Microsoft, BAIR Fellowship and OW by DARPA XAI and NSF Fellowship. DP is supported by Google FRA.
Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo- hez, and V. Vanhoucke, "Sim-to-real: Learning agile locomotion for quadruped robots," 2018. 1</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science Robotics. 20201J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, "Learning quadrupedal locomotion over challenging terrain," Science Robotics, 2020. 1</p>
<p>Learning dexterous in-hand manipulation. M Openai, B Andrychowicz, M Baker, R Chociej, B Jozefowicz, J Mcgrew, A Pachocki, M Petron, G Plappert, A Powell, J Ray, S Schneider, J Sidor, P Tobin, L Welinder, W Weng, Zaremba, OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba, "Learning dexterous in-hand manipulation," 2018. 1</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jacobi, P Husbands, I Harvey, Proceedings of the Third European Conference on Advances in Artificial Life. the Third European Conference on Advances in Artificial LifeLondon, UK, UKSpringer-VerlagN. Jacobi, P. Husbands, and I. Harvey, "Noise and the reality gap: The use of simulation in evolutionary robotics," in Proceedings of the Third European Conference on Advances in Artificial Life. London, UK, UK: Springer-Verlag, 1995, pp. 704-720. 1</p>
<p>Learning handeye coordination for robotic grasping with deep learning and largescale data collection. S Levine, P Pastor, A Krizhevsky, D Quillen, abs/1603.02199CoRR. 1S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, "Learning hand- eye coordination for robotic grasping with deep learning and large- scale data collection," CoRR, vol. abs/1603.02199, 2016. 1</p>
<p>System Identification: Theory for the User. L Ljung, Prentice-Hall, IncUpper Saddle River, NJ, USAL. Ljung, System Identification: Theory for the User. Upper Saddle River, NJ, USA: Prentice-Hall, Inc., 1986. 1</p>
<p>Domain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavi- olette, M. Marchand, and V. Lempitsky, "Domain-adversarial training of neural networks," 2015. 1</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE1J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in 2017 IEEE/RSJ International Con- ference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 23-30. 1, 2</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim-to- real transfer of robotic control with dynamics randomization," in 2018 IEEE international conference on robotics and automation (ICRA).</p>
<p>. IEEE. 12IEEE, 2018, pp. 1-8. 1, 2</p>
<p>Sim-to-real reinforcement learning for deformable object manipulation. J Matas, S James, A J Davison, abs/1806.07851CoRR. 1J. Matas, S. James, and A. J. Davison, "Sim-to-real reinforce- ment learning for deformable object manipulation," CoRR, vol. abs/1806.07851, 2018. 1</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N D Ratliff, D Fox, abs/1810.05687CoRR. 13Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. D. Ratliff, and D. Fox, "Closing the sim-to-real loop: Adapting simulation randomization with real world experience," CoRR, vol. abs/1810.05687, 2018. 1, 2, 3</p>
<p>Sim2real2sim: Bridging the gap between simulation and real-world in flexible object manipulation. P Chang, T Padir, arXiv:2002.02538arXiv preprintP. Chang and T. Padir, "Sim2real2sim: Bridging the gap between simulation and real-world in flexible object manipulation," arXiv preprint arXiv:2002.02538, 2020. 1</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, arXiv:1710.06542arXiv preprintL. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel, "Asymmetric actor critic for image-based robot learning," arXiv preprint arXiv:1710.06542, 2017. 2</p>
<p>(cad)$ˆ2$rl: Real single-image flight without a single real image. F Sadeghi, S Levine, abs/1611.04201CoRR. 2F. Sadeghi and S. Levine, "(cad)$ˆ2$rl: Real single-image flight without a single real image," CoRR, vol. abs/1611.04201, 2016. 2</p>
<p>Solving rubik's cube with a robot hand. I Openai, M Akkaya, M Andrychowicz, M Chociej, B Litwin, A Mcgrew, A Petron, M Paino, G Plappert, R Powell, J Ribas, N Schneider, J Tezak, P Tworek, L Welinder, Q Weng, W Yuan, L Zaremba, Zhang, OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang, "Solving rubik's cube with a robot hand," 2019. 2</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T.-W E Lee, J Tan, S Levine, Robotics: Science and Systems. 07X. B. Peng, E. Coumans, T. Zhang, T.-W. E. Lee, J. Tan, and S. Levine, "Learning agile robotic locomotion skills by imitating animals," in Robotics: Science and Systems, 07 2020. 2</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, arXiv:1702.02453arXiv preprintW. Yu, J. Tan, C. K. Liu, and G. Turk, "Preparing for the unknown: Learning a universal policy with online system identification," arXiv preprint arXiv:1702.02453, 2017. 2</p>
<p>Fast model identification via physics engines for data-efficient policy search. S Zhu, A Kimmel, K E Bekris, A Boularias, arXiv:1710.08893arXiv preprintS. Zhu, A. Kimmel, K. E. Bekris, and A. Boularias, "Fast model identification via physics engines for data-efficient policy search," arXiv preprint arXiv:1710.08893, 2017. 2</p>
<p>Self-supervised sim-to-real adaptation for visual robotic manipulation. R Jeong, Y Aytar, D Khosid, Y Zhou, J Kay, T Lampe, K Bousmalis, F Nori, R. Jeong, Y. Aytar, D. Khosid, Y. Zhou, J. Kay, T. Lampe, K. Bous- malis, and F. Nori, "Self-supervised sim-to-real adaptation for visual robotic manipulation," 2019. 2</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor," 2018. 2</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. M Laskin, A Srinivas, P Abbeel, arXiv:2004.04136.4Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningVienna, AustriaM. Laskin, A. Srinivas, and P. Abbeel, "Curl: Contrastive unsuper- vised representations for reinforcement learning," Proceedings of the 37th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020, arXiv:2004.04136. 4</p>
<p>. M Laskin, K Lee, A Stooke, L Pinto, P Abbeel, A Srinivas, arXiv:2004.14990arXiv preprintReinforcement learning with augmented dataM. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srini- vas, "Reinforcement learning with augmented data," arXiv preprint arXiv:2004.14990, 2020. 4</p>
<p>B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, arXiv:2003.08934Nerf: Representing scenes as neural radiance fields for view synthesis. arXiv preprintB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor- thi, and R. Ng, "Nerf: Representing scenes as neural radiance fields for view synthesis," arXiv preprint arXiv:2003.08934, 2020. 4</p>
<p>DeepMind control suite. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D De Las, D Casas, A Budden, J Abdolmaleki, A Merel, T Lefrancq, M Lillicrap, Riedmiller, DeepMind, Tech. Rep. Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. de Las Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, T. Lillicrap, and M. Riedmiller, "DeepMind control suite," DeepMind, Tech. Rep., Jan. 2018. 4</p>            </div>
        </div>

    </div>
</body>
</html>
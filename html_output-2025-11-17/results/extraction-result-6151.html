<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6151 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6151</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6151</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-9ca11e50e0bed509ceeb839d84485b6ca67cdc57</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ca11e50e0bed509ceeb839d84485b6ca67cdc57" target="_blank">A Closer Look into Using Large Language Models for Automatic Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6151.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6151.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM evaluation (Chiang and Lee, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure for using instruction-following LLMs as automatic evaluation metrics by providing task descriptions, attribute definitions/criteria, the sample, and a prompt to elicit ratings (typically averaged across multiple samples from the LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models be an alternative to human evaluations?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt an LLM with human-written task description, attribute definitions/criteria, and the sample; sample multiple generations (N=20) and average numeric ratings as the metric.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Attribute-specific Likert scales (e.g., coherence, consistency, fluency, relevance for summarization; naturalness, coherence, engagingness, groundedness for dialogue). Correlate LLM ratings with human ratings using Pearson's r (dataset-level) and Kendall's τ (document-level).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613) used in experiments; original method referenced Chiang & Lee 2023</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation (summarization, dialogue); applicable as an evaluation framework for generated hypotheses/theories in principle but not applied to scientific theories here.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable — method is for evaluating generated text quality rather than proposing specific scientific theories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as a baseline concept; paper follows this pipeline and shows that prompting details (output format, asking for explanations) materially affect correlations with human ratings. Sampling N=20 and averaging is the adopted protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval (summarization) and Topical-Chat (dialogue) used to meta-evaluate the metric.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Correlation between LLM-produced ratings and human averaged ratings is measured; Pearson's r and Kendall's τ quantify alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Original method depends on prompt design and output constraints; sensitivity to output format and LLM used (e.g., ChatGPT vs GPT-4).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6151.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Eval (Liu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based automatic evaluation pipeline that augments human-written instructions with auto-generated Chain-of-Thought evaluation steps and prompts the LLM to output a numeric score only (score-only).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpteval: Nlg evaluation using gpt-4 with better human alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Concatenate task description and criteria, ask the LLM to produce ordered evaluation steps (auto-CoT), then prompt the LLM with human-written instructions plus auto-CoT and require a numeric score only; sample multiple outputs and average.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same dataset-specific Likert scales as the datasets' human evaluations (e.g., 1-5 for SummEval attributes; 1-3 or 0-1 for Topical-Chat attributes as appropriate). Alignment measured by Pearson's r and Kendall's τ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Originally evaluated with GPT-4 in Liu et al.; in this paper ChatGPT (gpt-3.5-turbo-0613) is used to reproduce/ablate G-Eval choices.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation (summarization, dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable — G-Eval is an evaluation method for generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>G-Eval (auto-CoT + score-only) can yield strong correlation with human ratings when using GPT-4 per Liu et al.; with ChatGPT results are weaker and sensitive to prompt/formats and decoding temps. The present paper uses G-Eval as a baseline and shows other prompting strategies can outperform it when using ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Reported correlations vs. human ratings (Pearson r, Kendall τ); some GPT-4 numbers from Liu et al. are cited but direct comparison is qualified due to differences in prompts and access.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Auto-CoT does not consistently improve alignment; forcing numeric-only output (score-only) is suboptimal; results are sensitive to sampling temperature and prompt variations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6151.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto Chain-of-Thought (auto-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure where the LLM is prompted to generate ordered step-by-step evaluation steps (an 'evaluation plan') from the provided task description and criteria, which are then included in the prompt for rating.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpteval: Nlg evaluation using gpt-4 with better human alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt LLM to produce evaluation steps (prefix 'Evaluation steps:'), append those steps to the human-written instructions, then ask LLM to rate the sample (often score-only).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Aimed to clarify and structure evaluation criteria for downstream rating; relies on dataset-specific attribute definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used with GPT-4 in original G-Eval; ablated with ChatGPT in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation (summarization, dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>auto-CoT sometimes increases Pearson r for some attributes (e.g., coherence, consistency, relevance in SummEval with ChatGPT) but can hurt other attributes (e.g., fluency) and is not consistently beneficial across datasets; auto-CoT can paraphrase existing instructions and thus offer limited gains.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval, Topical-Chat (ablation studies here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Differences in correlation vs. human ratings are often small (<0.02) and not always statistically significant; some attribute-specific significant changes observed (William's test p<0.05 for some).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not reliably helpful; can degrade performance for some attributes and LLMs; its generated steps may simply paraphrase criteria without adding value.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6151.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Score-only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Score-only prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt design that restricts the LLM output to a single numeric rating (e.g., using the template '{{placeholder}} (score only):').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpteval: Nlg evaluation using gpt-4 with better human alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Constrain the LLM to output only a numeric score for the evaluated attribute; sample multiple times and average.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Numeric Likert scales per attribute as defined by the dataset (e.g., 1–5 or 1–3 scales).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT in experiments here; originally used in G-Eval (with GPT-4 evaluations reported).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation (summarization, dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Score-only is generally outperformed by allowing free text or by asking the LLM to explain/analyze its rating; restricting output to numeric-only is found to be suboptimal with ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Lower Pearson r / Kendall τ compared to free-text or explain/analyze prompts in most attributes tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less robust to decoding temperature and to prompt/persona variations; yields weaker alignment with human ratings for ChatGPT.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6151.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Free-text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Free-text prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Allowing the LLM to respond freely (e.g., 'How {attribute} is the sample?'), without restricting the output format to numeric-only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Ask the LLM an open question about the attribute, permitting explanations; extract numeric rating if present or parse response.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Dataset-specific Likert scales; responses typically contain numeric rating plus optional explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation (summarization, dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Free-text prompts yield higher Pearson r and Kendall τ than score-only for almost all attributes tested; even when the model often emits only a number, allowing free output improves alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Stronger correlation with human ratings compared to score-only prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires post-processing to extract numeric scores; behavior depends on prompt wording.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6151.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rate-explain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rate-explain prompting (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy introduced in this paper that asks the LLM to output the numeric rating first and then provide an explanation/rationale for the rating.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt: require 'Rating:' followed by numeric score and then 'Rationale:' with an explanation; sample multiple generations and average ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same dataset-specific Likert scales; explanation required but rating is generated before explanation in autoregressive decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation (summarization, dialogue); method suggested as a robust best-practice when using LLMs as evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Asking ChatGPT to 'rate-explain' consistently improves Pearson r and Kendall τ versus score-only and often outperforms/equals free-text; improvements are statistically significant in many cases and sometimes match or exceed prior GPT-4 results on some attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Stronger alignment with human ratings than score-only; improvement hypothesized because requiring explanations biases the model to pick ratings it can justify and that align with human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although the numeric rating precedes the rationale (so the rating does not causally depend on the generated rationale), prompting for explanations influences the rating distribution; method was validated with ChatGPT only.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6151.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analyze-rate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analyze-rate prompting (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting variant where the LLM is asked to analyze the sample according to evaluation criteria first and then output a numeric rating (zero-shot CoT style).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt: instruct the model to start with 'Analysis:' providing stepwise reasoning about the sample and then output 'Rating:' with a numeric score; sample multiple times and average.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Dataset-specific Likert scales; the model explicitly analyzes according to the provided evaluation steps/criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation (summarization, dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Analyze-rate achieves the strongest correlations in many settings (often outperforming score-only and sometimes rate-explain); results are robust across sampling temperatures and prompt-persona variations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Improves alignment with human ratings; in some attributes achieved correlations comparable to or better than prior GPT-4 results reported by Liu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires LLMs that can follow analysis instructions; validation limited to ChatGPT and two datasets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6151.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Correlation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pearson's r and Kendall's τ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical metrics used to measure alignment between LLM-generated ratings and human ratings: Pearson's r (linear correlation, dataset-level) and Kendall's τ (rank correlation, document-level).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute Pearson r across all N*M evaluated items (dataset-level) and Kendall's τ per-source-document (document-level) averaging across documents; use scipy.stats implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Alignment quantified by Pearson r (preferred for significance testing) and Kendall's τ (to assess rank discrimination among outputs for the same source).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Metric applied to outputs from ChatGPT and GPT-4 (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Evaluation of generated text; metric choice is general and could be applied to evaluating generated scientific hypotheses if numeric ratings exist.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported Pearson r and Kendall τ values for many ablations (see paper tables); significance between Pearson r compared with baselines tested using William's test.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Directly measures correlation to human averaged ratings; used as primary evidence for which prompting choices align better with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pearson r and Kendall τ capture different aspects (linear vs rank); results depend on how per-item aggregation is done (dataset-level vs document-level).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6151.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SummEval & Topical-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two meta-evaluation datasets used as benchmarks: SummEval (summarization with human ratings for coherence/consistency/fluency/relevance) and Topical-Chat (knowledge-grounded dialogue with human ratings for naturalness/coherence/engagingness/groundedness).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use human-annotated ratings from these datasets as ground truth; evaluate LLM-generated ratings by computing correlations with those human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>SummEval uses 1–5 Likert scales per attribute; Topical-Chat uses 1–3 scales (or 0–1 for groundedness) per attribute depending on attribute definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Benchmarks used to evaluate ChatGPT ratings in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Summarization and dialogue evaluation (text generation).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to demonstrate that prompting for explanations (rate-explain/analyze-rate) increases alignment with human ratings vs. score-only and sometimes surpasses earlier GPT-4 results on certain attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval (Fabbri et al., 2021) and Topical-Chat (with human annotations from Mehri & Eskenazi, 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human annotated ratings in these datasets are the ground truth for correlation calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Findings are validated only on these two datasets; authors note limited task diversity and do not claim full generalization to other tasks or to scientific-theory evaluation specifically.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6151.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely accessible instruction-following LLM used as the primary evaluator in experiments; chosen for accessibility and cost compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as the evaluation model to produce ratings under different prompting regimes (score-only, free-text, rate-explain, analyze-rate) and sampling settings (N=20, varied temperatures).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Produces numeric ratings on dataset-defined Likert scales; correlations of these ratings with human ratings measured by Pearson r and Kendall τ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Text-generation evaluation; tested on summarization and dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>As ChatGPT was prompted to explain/analyze, correlations with human ratings improved substantially compared to score-only and were robust to temperature and persona prompts; in several cases ChatGPT with explain prompts matched or exceeded prior GPT-4 correlations reported for some attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Correlations quantified vs. human ratings; explain/analyze prompting narrowed the gap to (and sometimes surpassed) GPT-4 reported results on specific attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Results limited to ChatGPT; open LLMs (e.g., falcon-40b-instruct) were tested preliminarily and found inadequate for the evaluation role.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6151.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6151.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical testing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>William's test for comparing dependent correlations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical test used to determine whether the difference between two dependent Pearson correlation coefficients (e.g., with human ratings) is significant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Testing for significance of increased correlation with human judgment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Apply William's test to compare Pearson's r obtained under different prompting/ablation conditions to determine statistical significance of differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Uses p-values (paper reports p<0.05 for several attribute-level comparisons where auto-CoT or other changes increased r).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to correlations derived from ChatGPT (and referenced GPT-4 results).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Statistical evaluation of metric alignment to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to show that some differences (e.g., auto-CoT effects on certain attributes; improvements from asking for explanations) are statistically significant.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummEval and Topical-Chat correlations used in tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct statistical test on alignment measures between LLM and human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Significance depends on correlation calculation method (dataset-level vs document-level) and experimental reproducibility.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpteval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Summeval: Re-evaluating summarization evaluation <em>(Rating: 2)</em></li>
                <li>Topical-chat: Towards knowledge-grounded open-domain conversations <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6151",
    "paper_id": "paper-9ca11e50e0bed509ceeb839d84485b6ca67cdc57",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "LLM evaluation",
            "name_full": "LLM evaluation (Chiang and Lee, 2023)",
            "brief_description": "A procedure for using instruction-following LLMs as automatic evaluation metrics by providing task descriptions, attribute definitions/criteria, the sample, and a prompt to elicit ratings (typically averaged across multiple samples from the LLM).",
            "citation_title": "Can large language models be an alternative to human evaluations?",
            "mention_or_use": "use",
            "evaluation_method": "Prompt an LLM with human-written task description, attribute definitions/criteria, and the sample; sample multiple generations (N=20) and average numeric ratings as the metric.",
            "evaluation_criteria": "Attribute-specific Likert scales (e.g., coherence, consistency, fluency, relevance for summarization; naturalness, coherence, engagingness, groundedness for dialogue). Correlate LLM ratings with human ratings using Pearson's r (dataset-level) and Kendall's τ (document-level).",
            "llm_model_name": "ChatGPT (gpt-3.5-turbo-0613) used in experiments; original method referenced Chiang & Lee 2023",
            "theory_domain": "Text-generation evaluation (summarization, dialogue); applicable as an evaluation framework for generated hypotheses/theories in principle but not applied to scientific theories here.",
            "theory_description": "Not applicable — method is for evaluating generated text quality rather than proposing specific scientific theories.",
            "evaluation_results": "Used as a baseline concept; paper follows this pipeline and shows that prompting details (output format, asking for explanations) materially affect correlations with human ratings. Sampling N=20 and averaging is the adopted protocol.",
            "benchmarks_or_datasets": "SummEval (summarization) and Topical-Chat (dialogue) used to meta-evaluate the metric.",
            "comparison_to_human": "Correlation between LLM-produced ratings and human averaged ratings is measured; Pearson's r and Kendall's τ quantify alignment.",
            "limitations_or_challenges": "Original method depends on prompt design and output constraints; sensitivity to output format and LLM used (e.g., ChatGPT vs GPT-4).",
            "uuid": "e6151.0"
        },
        {
            "name_short": "G-Eval",
            "name_full": "G-Eval (Liu et al., 2023)",
            "brief_description": "An LLM-based automatic evaluation pipeline that augments human-written instructions with auto-generated Chain-of-Thought evaluation steps and prompts the LLM to output a numeric score only (score-only).",
            "citation_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "mention_or_use": "use",
            "evaluation_method": "Concatenate task description and criteria, ask the LLM to produce ordered evaluation steps (auto-CoT), then prompt the LLM with human-written instructions plus auto-CoT and require a numeric score only; sample multiple outputs and average.",
            "evaluation_criteria": "Same dataset-specific Likert scales as the datasets' human evaluations (e.g., 1-5 for SummEval attributes; 1-3 or 0-1 for Topical-Chat attributes as appropriate). Alignment measured by Pearson's r and Kendall's τ.",
            "llm_model_name": "Originally evaluated with GPT-4 in Liu et al.; in this paper ChatGPT (gpt-3.5-turbo-0613) is used to reproduce/ablate G-Eval choices.",
            "theory_domain": "Text-generation evaluation (summarization, dialogue).",
            "theory_description": "Not applicable — G-Eval is an evaluation method for generated text.",
            "evaluation_results": "G-Eval (auto-CoT + score-only) can yield strong correlation with human ratings when using GPT-4 per Liu et al.; with ChatGPT results are weaker and sensitive to prompt/formats and decoding temps. The present paper uses G-Eval as a baseline and shows other prompting strategies can outperform it when using ChatGPT.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat used in comparisons.",
            "comparison_to_human": "Reported correlations vs. human ratings (Pearson r, Kendall τ); some GPT-4 numbers from Liu et al. are cited but direct comparison is qualified due to differences in prompts and access.",
            "limitations_or_challenges": "Auto-CoT does not consistently improve alignment; forcing numeric-only output (score-only) is suboptimal; results are sensitive to sampling temperature and prompt variations.",
            "uuid": "e6151.1"
        },
        {
            "name_short": "auto-CoT",
            "name_full": "Auto Chain-of-Thought (auto-CoT)",
            "brief_description": "A procedure where the LLM is prompted to generate ordered step-by-step evaluation steps (an 'evaluation plan') from the provided task description and criteria, which are then included in the prompt for rating.",
            "citation_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "mention_or_use": "use",
            "evaluation_method": "Prompt LLM to produce evaluation steps (prefix 'Evaluation steps:'), append those steps to the human-written instructions, then ask LLM to rate the sample (often score-only).",
            "evaluation_criteria": "Aimed to clarify and structure evaluation criteria for downstream rating; relies on dataset-specific attribute definitions.",
            "llm_model_name": "Used with GPT-4 in original G-Eval; ablated with ChatGPT in this paper.",
            "theory_domain": "Text-generation evaluation (summarization, dialogue).",
            "theory_description": "Not applicable.",
            "evaluation_results": "auto-CoT sometimes increases Pearson r for some attributes (e.g., coherence, consistency, relevance in SummEval with ChatGPT) but can hurt other attributes (e.g., fluency) and is not consistently beneficial across datasets; auto-CoT can paraphrase existing instructions and thus offer limited gains.",
            "benchmarks_or_datasets": "SummEval, Topical-Chat (ablation studies here).",
            "comparison_to_human": "Differences in correlation vs. human ratings are often small (&lt;0.02) and not always statistically significant; some attribute-specific significant changes observed (William's test p&lt;0.05 for some).",
            "limitations_or_challenges": "Not reliably helpful; can degrade performance for some attributes and LLMs; its generated steps may simply paraphrase criteria without adding value.",
            "uuid": "e6151.2"
        },
        {
            "name_short": "Score-only",
            "name_full": "Score-only prompting",
            "brief_description": "A prompt design that restricts the LLM output to a single numeric rating (e.g., using the template '{{placeholder}} (score only):').",
            "citation_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "mention_or_use": "use",
            "evaluation_method": "Constrain the LLM to output only a numeric score for the evaluated attribute; sample multiple times and average.",
            "evaluation_criteria": "Numeric Likert scales per attribute as defined by the dataset (e.g., 1–5 or 1–3 scales).",
            "llm_model_name": "ChatGPT in experiments here; originally used in G-Eval (with GPT-4 evaluations reported).",
            "theory_domain": "Text-generation evaluation (summarization, dialogue).",
            "theory_description": "Not applicable.",
            "evaluation_results": "Score-only is generally outperformed by allowing free text or by asking the LLM to explain/analyze its rating; restricting output to numeric-only is found to be suboptimal with ChatGPT.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat.",
            "comparison_to_human": "Lower Pearson r / Kendall τ compared to free-text or explain/analyze prompts in most attributes tested.",
            "limitations_or_challenges": "Less robust to decoding temperature and to prompt/persona variations; yields weaker alignment with human ratings for ChatGPT.",
            "uuid": "e6151.3"
        },
        {
            "name_short": "Free-text",
            "name_full": "Free-text prompting",
            "brief_description": "Allowing the LLM to respond freely (e.g., 'How {attribute} is the sample?'), without restricting the output format to numeric-only.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Ask the LLM an open question about the attribute, permitting explanations; extract numeric rating if present or parse response.",
            "evaluation_criteria": "Dataset-specific Likert scales; responses typically contain numeric rating plus optional explanation.",
            "llm_model_name": "ChatGPT used in experiments.",
            "theory_domain": "Text-generation evaluation (summarization, dialogue).",
            "theory_description": "Not applicable.",
            "evaluation_results": "Free-text prompts yield higher Pearson r and Kendall τ than score-only for almost all attributes tested; even when the model often emits only a number, allowing free output improves alignment.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat.",
            "comparison_to_human": "Stronger correlation with human ratings compared to score-only prompting.",
            "limitations_or_challenges": "Requires post-processing to extract numeric scores; behavior depends on prompt wording.",
            "uuid": "e6151.4"
        },
        {
            "name_short": "Rate-explain",
            "name_full": "Rate-explain prompting (this paper)",
            "brief_description": "A prompting strategy introduced in this paper that asks the LLM to output the numeric rating first and then provide an explanation/rationale for the rating.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt: require 'Rating:' followed by numeric score and then 'Rationale:' with an explanation; sample multiple generations and average ratings.",
            "evaluation_criteria": "Same dataset-specific Likert scales; explanation required but rating is generated before explanation in autoregressive decoding.",
            "llm_model_name": "ChatGPT (gpt-3.5-turbo-0613) in experiments.",
            "theory_domain": "Text-generation evaluation (summarization, dialogue); method suggested as a robust best-practice when using LLMs as evaluators.",
            "theory_description": "Not applicable.",
            "evaluation_results": "Asking ChatGPT to 'rate-explain' consistently improves Pearson r and Kendall τ versus score-only and often outperforms/equals free-text; improvements are statistically significant in many cases and sometimes match or exceed prior GPT-4 results on some attributes.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat.",
            "comparison_to_human": "Stronger alignment with human ratings than score-only; improvement hypothesized because requiring explanations biases the model to pick ratings it can justify and that align with human judgment.",
            "limitations_or_challenges": "Although the numeric rating precedes the rationale (so the rating does not causally depend on the generated rationale), prompting for explanations influences the rating distribution; method was validated with ChatGPT only.",
            "uuid": "e6151.5"
        },
        {
            "name_short": "Analyze-rate",
            "name_full": "Analyze-rate prompting (this paper)",
            "brief_description": "A prompting variant where the LLM is asked to analyze the sample according to evaluation criteria first and then output a numeric rating (zero-shot CoT style).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt: instruct the model to start with 'Analysis:' providing stepwise reasoning about the sample and then output 'Rating:' with a numeric score; sample multiple times and average.",
            "evaluation_criteria": "Dataset-specific Likert scales; the model explicitly analyzes according to the provided evaluation steps/criteria.",
            "llm_model_name": "ChatGPT (gpt-3.5-turbo-0613) in experiments.",
            "theory_domain": "Text-generation evaluation (summarization, dialogue).",
            "theory_description": "Not applicable.",
            "evaluation_results": "Analyze-rate achieves the strongest correlations in many settings (often outperforming score-only and sometimes rate-explain); results are robust across sampling temperatures and prompt-persona variations.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat.",
            "comparison_to_human": "Improves alignment with human ratings; in some attributes achieved correlations comparable to or better than prior GPT-4 results reported by Liu et al.",
            "limitations_or_challenges": "Requires LLMs that can follow analysis instructions; validation limited to ChatGPT and two datasets.",
            "uuid": "e6151.6"
        },
        {
            "name_short": "Correlation metrics",
            "name_full": "Pearson's r and Kendall's τ",
            "brief_description": "Statistical metrics used to measure alignment between LLM-generated ratings and human ratings: Pearson's r (linear correlation, dataset-level) and Kendall's τ (rank correlation, document-level).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute Pearson r across all N*M evaluated items (dataset-level) and Kendall's τ per-source-document (document-level) averaging across documents; use scipy.stats implementations.",
            "evaluation_criteria": "Alignment quantified by Pearson r (preferred for significance testing) and Kendall's τ (to assess rank discrimination among outputs for the same source).",
            "llm_model_name": "Metric applied to outputs from ChatGPT and GPT-4 (as reported).",
            "theory_domain": "Evaluation of generated text; metric choice is general and could be applied to evaluating generated scientific hypotheses if numeric ratings exist.",
            "theory_description": "Not applicable.",
            "evaluation_results": "Reported Pearson r and Kendall τ values for many ablations (see paper tables); significance between Pearson r compared with baselines tested using William's test.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat results reported.",
            "comparison_to_human": "Directly measures correlation to human averaged ratings; used as primary evidence for which prompting choices align better with humans.",
            "limitations_or_challenges": "Pearson r and Kendall τ capture different aspects (linear vs rank); results depend on how per-item aggregation is done (dataset-level vs document-level).",
            "uuid": "e6151.7"
        },
        {
            "name_short": "Datasets",
            "name_full": "SummEval & Topical-Chat",
            "brief_description": "Two meta-evaluation datasets used as benchmarks: SummEval (summarization with human ratings for coherence/consistency/fluency/relevance) and Topical-Chat (knowledge-grounded dialogue with human ratings for naturalness/coherence/engagingness/groundedness).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Use human-annotated ratings from these datasets as ground truth; evaluate LLM-generated ratings by computing correlations with those human ratings.",
            "evaluation_criteria": "SummEval uses 1–5 Likert scales per attribute; Topical-Chat uses 1–3 scales (or 0–1 for groundedness) per attribute depending on attribute definitions.",
            "llm_model_name": "Benchmarks used to evaluate ChatGPT ratings in this paper.",
            "theory_domain": "Summarization and dialogue evaluation (text generation).",
            "theory_description": "Not applicable.",
            "evaluation_results": "Used to demonstrate that prompting for explanations (rate-explain/analyze-rate) increases alignment with human ratings vs. score-only and sometimes surpasses earlier GPT-4 results on certain attributes.",
            "benchmarks_or_datasets": "SummEval (Fabbri et al., 2021) and Topical-Chat (with human annotations from Mehri & Eskenazi, 2020).",
            "comparison_to_human": "Human annotated ratings in these datasets are the ground truth for correlation calculations.",
            "limitations_or_challenges": "Findings are validated only on these two datasets; authors note limited task diversity and do not claim full generalization to other tasks or to scientific-theory evaluation specifically.",
            "uuid": "e6151.8"
        },
        {
            "name_short": "ChatGPT (gpt-3.5)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0613)",
            "brief_description": "A widely accessible instruction-following LLM used as the primary evaluator in experiments; chosen for accessibility and cost compared to GPT-4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Used as the evaluation model to produce ratings under different prompting regimes (score-only, free-text, rate-explain, analyze-rate) and sampling settings (N=20, varied temperatures).",
            "evaluation_criteria": "Produces numeric ratings on dataset-defined Likert scales; correlations of these ratings with human ratings measured by Pearson r and Kendall τ.",
            "llm_model_name": "gpt-3.5-turbo-0613 (ChatGPT)",
            "theory_domain": "Text-generation evaluation; tested on summarization and dialogue.",
            "theory_description": "Not applicable.",
            "evaluation_results": "As ChatGPT was prompted to explain/analyze, correlations with human ratings improved substantially compared to score-only and were robust to temperature and persona prompts; in several cases ChatGPT with explain prompts matched or exceeded prior GPT-4 correlations reported for some attributes.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat.",
            "comparison_to_human": "Correlations quantified vs. human ratings; explain/analyze prompting narrowed the gap to (and sometimes surpassed) GPT-4 reported results on specific attributes.",
            "limitations_or_challenges": "Results limited to ChatGPT; open LLMs (e.g., falcon-40b-instruct) were tested preliminarily and found inadequate for the evaluation role.",
            "uuid": "e6151.9"
        },
        {
            "name_short": "Statistical testing",
            "name_full": "William's test for comparing dependent correlations",
            "brief_description": "A statistical test used to determine whether the difference between two dependent Pearson correlation coefficients (e.g., with human ratings) is significant.",
            "citation_title": "Testing for significance of increased correlation with human judgment",
            "mention_or_use": "use",
            "evaluation_method": "Apply William's test to compare Pearson's r obtained under different prompting/ablation conditions to determine statistical significance of differences.",
            "evaluation_criteria": "Uses p-values (paper reports p&lt;0.05 for several attribute-level comparisons where auto-CoT or other changes increased r).",
            "llm_model_name": "Applied to correlations derived from ChatGPT (and referenced GPT-4 results).",
            "theory_domain": "Statistical evaluation of metric alignment to human judgments.",
            "theory_description": "Not applicable.",
            "evaluation_results": "Used to show that some differences (e.g., auto-CoT effects on certain attributes; improvements from asking for explanations) are statistically significant.",
            "benchmarks_or_datasets": "SummEval and Topical-Chat correlations used in tests.",
            "comparison_to_human": "Direct statistical test on alignment measures between LLM and human ratings.",
            "limitations_or_challenges": "Significance depends on correlation calculation method (dataset-level vs document-level) and experimental reproducibility.",
            "uuid": "e6151.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2
        },
        {
            "paper_title": "Summeval: Re-evaluating summarization evaluation",
            "rating": 2
        },
        {
            "paper_title": "Topical-chat: Towards knowledge-grounded open-domain conversations",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 1
        }
    ],
    "cost": 0.016387,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Closer Look into Automatic Evaluation Using Large Language Models</h1>
<p>Cheng-Han Chiang<br>National Taiwan University, Taiwan<br>dcml0714@gmail.com</p>
<p>Hung-yi Lee<br>National Taiwan University, Taiwan<br>hungyilee@ntu.edu.tw</p>
<h4>Abstract</h4>
<p>Using large language models (LLMs) to evaluate text quality has recently gained popularity. Some prior works explore the idea of using LLMs for evaluation, while they differ in some details of the evaluation process. In this paper, we analyze LLM evaluation (Chiang and Lee, 2023) ${ }^{1}$ and G-Eval (Liu et al., 2023), and we discuss how those details in the evaluation process change how well the ratings given by LLMs correlate with human ratings. We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings. We also show that forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) trained with task instructions and human feedback can follow natural language instructions to complete a task (Askell et al., 2021; Sanh et al., 2022; Wei et al., 2022a; Ouyang et al., 2022). Recently, the instructionfollowing ability of LLMs makes them promising candidates for automatic evaluation (Chiang and Lee, 2023; Liu et al., 2023; Wang et al., 2023; Huang et al., 2023). By simply instructing the LLMs on how to rate and giving the LLMs the sample to be rated, the LLM can follow the instructions and provide a rating of the sample.</p>
<p>Chiang and Lee (2023) propose LLM evaluation and Liu et al. (2023) propose G-Eval; both of which use LLMs to evaluate samples by giving the LLM instructions, and they both show that some LLMs can yield evaluation results that are aligned to the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>evaluation results of humans. Still, LLM evaluation and G-Eval differ in some specific design choices in the evaluation procedure. Since Chiang and Lee (2023) and Liu et al. (2023) use distinct tasks, it is hard to know how the differences between LLM evaluation and G-Eval affect the evaluation results. This makes practitioners in the future hard to determine how to conduct an automatic evaluation using LLMs.</p>
<p>Given that LLM evaluation and G-Eval have already received significant attention shortly after publication, these methods will likely revolutionize the evaluation in NLP. Therefore, conducting a detailed analysis of these approaches is essential and timely. This paper aims to identify the crucial components in LLM evaluation and G-Eval that contribute to stronger correlations with human ratings. Based on our analysis, we provide guidelines on how to use LLMs for automatic evaluations. We have the following findings:</p>
<ul>
<li>Auto-CoT (proposed by G-Eval) does not always improve the correlation between LLM and human ratings.</li>
<li>Making the LLMs output only a single numeric rating is suboptimal.</li>
<li>Asking the LLMs to rationalize their own ratings significantly improves the correlation between the LLMs' ratings and human ratings.</li>
<li>On two datasets, we improve the best correlation that ChatGPT's rating can achieve, and some correlations even exceed prior SoTA correlations obtained using the ratings of GPT-4 in Liu et al. (2023).</li>
</ul>
<h2>2 Experiment Setup</h2>
<p>Our paper studies what components in LLM evaluation and G-Eval make the ratings generated by LLM correlate with human ratings better, and we aim to improve the correlation.</p>
<h3>2.1 LLM as an Automatic Evaluation Metric</h3>
<p>Both LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et al., 2023) propose to ask LLMs to rate a sample regarding some attributes of the sample (e.g., fluency, grammaticality) using a $k$ point Likert scale. They give the LLMs (1) descriptions of the rating task, (2) the definition and rating criteria of the attribute to be rated, (3) the sample to be rated, and (4) a sentence that prompts the LLM to give the rating ${ }^{2}$. The LLM outputs a sequence containing the rating. Unless specified, we follow prior works to sample $N=20$ sequences from the LLM and average those ratings as the final rating. While the two methods share the core concept, they differ in two details.</p>
<p>Difference 1: Auto Chain-of-Thought The task descriptions and rating criteria in LLM evaluation and G-Eval are all human-written. However, Liu et al. (2023) argue that some evaluated attributes require more than simple definition and evaluation criteria, so they use LLMs to determine the evaluation steps. Specifically, they concatenate the task description, definition, and criteria of the attributes and append a line "Evaluation steps:" to prompt the LLM. The LLM then generates an ordered list containing the step-by-step evaluation steps. They dub this process auto chain-of-thought (CoT). G-Eval uses human-written task instructions and auto-CoT-generated evaluation steps to prompt the LLM to rate the sample.</p>
<p>Difference 2: Prompts for Output At the end of the input to LLMs, G-Eval uses the prompt "{{placeholder}} (score only):" to restrict the LLM to output only the numeric rating; the placeholder will be replaced by the evaluated attributes. In contrast, LLM evaluation uses the following question to ask the LLM to assign the rating: "How {{placeholder}} is the sample? (on a scale of 1-k, with 1 being the lowest)". The LLM's output form is not restricted.</p>
<h3>2.2 Meta-Evaluating an Evaluation Metric</h3>
<p>Given a sample, an evaluation metric assigns it a rating. To evaluate an evaluation metric, we need a dataset containing human ratings for samples in the dataset. We calculate the correlation coefficient between the ratings obtained by the evaluation metric and the human ratings. A higher correlation</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>indicates the evaluation metric better aligns with human ratings. We adopt Pearson $r$ and Kendall's $\tau$ as they are widely used in meta-evaluations (Graham et al., 2015; Bojar et al., 2017; Zhang* et al., 2020). In our paper, all the correlation refers to the correlation coefficient between the ratings of LLM and human ratings. Details on the calculation of correlation coefficients are in Appendix C.</p>
<p>We use SummEval (Fabbri et al., 2021) and Topical-Chat (Gopalakrishnan et al., 2019; Mehri and Eskenazi, 2020) as the meta-evaluation datasets, following Liu et al. (2023). SummEval is a meta-evaluation dataset for summarization derived from the CNN/DailyMail dataset (Hermann et al., 2015). Each summary in SummEval is rated by humans based on the coherence, consistency, fluency of the summary, and relevance between the summary and the source document. Topical-Chat is a dataset that evaluates the quality of a response given the dialogue history and a piece of knowledge relating to the dialogue. We follow Zhong et al. (2022) to evaluate the naturalness, coherence, engagingness, and groundedness (whether the response is grounded on the provided knowledge) of the response. The dataset details are in Appendix E.</p>
<h3>2.3 Large Language Models</h3>
<p>An LLM used as an evaluation metric should be affordable and accessible to whoever wants to use it. Based on this principle, we use ChatGPT (gpt3.5-turbo-0613) (OpenAI, 2022) for evaluation since it has lower cost and improved performance compared with other GPT-3.5 models. ChatGPT is also used in LLM evaluation and G-Eval. While Liu et al. (2023) further use GPT-4 (OpenAI, 2023) in their experiments, we cannot use GPT-4 in our experiments since most people, including us, have limited or no access to GPT-4, making it utterly unsuitable as an evaluation metric.</p>
<p>In our preliminary experiments, we also try to use the best open LLM (at the time of writing this manuscript) on Open LLM leaderboard, the falcon-40b-instruct model (Almazrouei et al., 2023), but we find it cannot follow the instructions and rate the samples very well. Hence, we exclude open LLMs in our paper.</p>
<h2>3 Better Usage of LLM for Evaluation</h2>
<h3>3.1 Is Auto CoT Always Useful?</h3>
<p>Liu et al. (2023) shows that adding the evaluation steps generated by auto CoT improves the correla-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sec.</th>
<th style="text-align: center;">Ablations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Output</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 ${ }^{\dagger}$</td>
<td style="text-align: center;">$?^{\ddagger}$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.409</td>
</tr>
<tr>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.327</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.248</td>
</tr>
<tr>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.248</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Free Text</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.228</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.348</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.305</td>
</tr>
</tbody>
</table>
<p>Table 1: The Pearson's $r$ and Kendall's $\tau$ correlation coefficient between LLMs' ratings and human ratings for SummEval. All the results in this table, except the first row, are from ChatGPT. We consider auto CoT + score only using ChatGPT proposed in G-Eval as the baseline of this paper. We boldface the Pearson's $r$ statistically significantly higher than the baseline (except GPT-4). $\dagger$ : results from Liu et al. (2023). Some numbers are different because we re-calculate the correlations based on the GPT-4 responses Liu et al. (2023) released. $\ddagger$ : The results of GPT-4 cannot serve as a reasonable comparison since we find something odd in the prompts Liu et al. (2023) use, which we elaborate in Appendix A.
tion on SummEval when using GPT-4 for evaluation. By scrutinizing their results, we find that the correlations when using auto CoT and not using it often differ by less than 0.02 . This raises two questions: (1) Is this difference statistically significant? (2) Does auto CoT yield higher correlations for different LLMs and datasets? To answer these questions, we use ChatGPT to rate the samples in SummEval and Topical-Chat using two sets of prompts, one with the evaluation steps generated using auto CoT and one without those evaluation steps. In this experiment, we follow G-Eval and restrict ChatGPT to output only a numeric score. Following Graham and Baldwin (2014), we use William's test for significance to see if the Pearson's $r$ of using and not using auto CoT is statistically significantly different. We try to follow the prompts used in G-Eval when possible; still, we have to construct some prompts since Liu et al. (2023) only release part of the prompts and some of which are problematic. We list all the prompts and how they are obtained in Appendix F.</p>
<p>The experiment results for SummEval are shown in the block in blue in Table 1. We also list the best results of G-Eval using GPT-4 from Liu et al. (2023) in the first row of Table 1 only for reference. Comparing our results with GPT-4 is unfair since we use ChatGPT, which is weaker than GPT4. A more reasonable baseline for our paper is the "auto CoT + score only" using ChatGPT on the second row, which is the method proposed by G-Eval and shows the highest correlation that ChatGPT can achieve in Liu et al. (2023). The numbers here differ from results in Liu et al. (2023) because
we carefully reproduce their results ourselves.
Back to Table 1, we can see that auto CoT leads to higher correlations for coherence, consistency, and relevance. By William's test, these higher correlations reach statistical significance with $p$-values less than 0.05 . However, using auto CoT results in a lower Pearson's $r$ for fluency, and this inferiority in Pearson's $r$ is also statistically significant.</p>
<p>The results for Topical-Chat are illustrated in Table 2. For Topical-Chat, the Pearson's $r$ of using and not using auto CoT are very close for all four attributes except groundedness, with differences less than 0.025 , and these differences are not statistically significant. For groundedness, auto CoT even drastically decreases the correlation. In summary, using auto CoT does not yield consistent and meaningful improvements compared with not using CoT. This should not be surprising since the evaluation steps generated with auto CoT often merely paraphrases the evaluation criterion and instructions given to the LLM.</p>
<h3>3.2 Prompt for Outputs</h3>
<p>In this section, we explore if the difference in how ChatGPT is prompted to output makes it's ratings better aligned with human ratings. We use two sets of prompts that share the same task descriptions and evaluation criteria but differ in how they prompt the LLM to generate the output. One uses "score only", as in G-Eval. The other replaces the "score only" with "How {{placeholder}} is the sample? (on a scale of 1-k, with 1 being the lowest)", as in LLM evaluation. We call the latter prompts free text since they do not</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sec.</th>
<th style="text-align: center;">Ablations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Naturalness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Engagingness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Groundedness</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Output</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.566</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.582</td>
</tr>
<tr>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.582</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Free Text</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.666</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.693</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.693</td>
</tr>
</tbody>
</table>
<p>Table 2: The Pearson's $r$ and Kendall's $\tau$ correlation coefficient between LLMs' ratings and human ratings for Topical-Chat. All the results in this table, except the first row, are from ChatGPT. We boldface the Pearson's $r$ statistically significantly higher than auto CoT + score only. We underline the Pearson's $r$ comparable auto CoT + score only.
restrict the output form.
The results for SummEval are shown in the yellow blocks in Table 1, and the results for TopicalChat are shown in Table 2. We find that allowing ChatGPT to respond to the question freely yields Pearson's $r$ and Kendall's $\tau$ much higher than restricting the model to output a single numeric score for almost all attributes of both datasets. The higher Pearson's $r$ of free text compared with score only is statistically significant. The only exception is the relevance of SummEval, where free text yields slightly lower correlations.</p>
<p>Initially, we thought ChatGPT aligns better with human ratings in free text because it can generate natural language explanations to justify their rating, making the ratings more correlated with human ratings. However, we observe that the responses of ChatGPT when prompted with free text mostly contain a single numeric rating, which is the same behavior when it is instructed by score only. This means that what the model is allowed to generate is more important than what it really generates.</p>
<p>The above observations make us curious if the correlations can be higher if ChatGPT is instructed to justify its ratings. Inspired by chain-of-thought in Wei et al. (2022b) and Kojima et al. (2022) (not the auto CoT in G-Eval), we ask ChatGPT to provide their reasoning and rationales on the ratings. Instead of asking ChatGPT to output only a score, we construct two types of prompts that ask ChatGPT to rationalize its decision. The first type of prompt, called analyze-rate, asks ChatGPT to analyze the samples regarding the evaluated criteria first and give the rating. The second type of prompt, called rate-explain, asks ChatGPT to provide the numeric ratings first and explain why it gives such a rating. analyze-rate is more like the zero-shot
chain-of-thought (Kojima et al., 2022). Refer to Appendix F.1.1 for the exact prompts we use.</p>
<p>The results of asking ChatGPT to explain/analyze how they rate the sample are shown in the last two rows in Table 1 and Appendix Table 2. We find that for all attributes of both datasets, rate-explain and anlyze-rate both lead to correlations stronger than or at least comparable to the correlation of asking ChatGPT to output only a numeric rating (score only). By asking ChatGPT to explain/analyze, we improve the best correlations that can be achieved by ChatGPT in Liu et al. (2023) (the Auto-CoT + score only). Moreover, when asked to explain/analyze when rating, ChatGPT's correlation can be better than or comparable to the state-of-the-art correlation coefficients obtained from GPT-4 in Liu et al. (2023) for coherence of SummEval and three attributes of Topical-Chat. We hypothesize that some attributes (e.g., coherence for SummEval) are harder for ChatGPT to rate, so the correlations for these attributes show a larger improvement when ChatGPT explains how it rates the sample.</p>
<p>In rate-explain, the output of ChatGPT contains a numeric rating followed by some explanations. As an auto-regressive language model, ChatGPT cannot depend on the explanation when generating the rating due to causal attention. If we stop the generation after ChatGPT generates the ratings, the output of rate-explain will only contain the ratings, just like the output forms in score only. Although the ratings in rate-explain do not depend on ChatGPT's rationales for the ratings, the ratings still correlate better with human ratings, compared with the ratings in score only. We think this is because when ChatGPT knows it needs to explain the ratings, it tends to generate ratings that are easier for it to explain, and a rating that is more</p>
<p>aligned to humans' rating is easier for ChatGPT to explain.</p>
<h3>3.3 Empirical Guidelines</h3>
<p>Based on the analysis and results in this section, we provide the following guideline: Always ask ChatGPT to explain/analyze when rating. We do not see rate-explain to be significantly better (or worse) than analyze-rate, so it is hard to determine which one to use. A valid method is sampling some ratings using rate-explain and sampling some ratings using analyze-rate and averaging the ratings from the two prompts as the final rating. Using auto CoT is optional since it does not always lead to higher correlations with human ratings. We also find that using auto CoT does not always improve the correlations when ChatGPT is asked to explain; this result is shown in Appendix Table 3.</p>
<h3>3.4 Robustness of the Guidelines</h3>
<p>LLMs are notorious for their performance fluctuation due to the input prompts, and the sequence generated by LLMs can be different when changing the hyperparameters used in decoding. To verify the validity of our empirical guidelines, we conduct the following two sets of experiments: (1) we vary the temperature used in sampling the output from ChatGPT, and (2) we vary the prompt given to ChatGPT.</p>
<h3>3.4.1 Varying the Temperature</h3>
<p>We check if our guideline holds if we change the temperature $T$ during generation. We compare Pearson's $r$ when using the method proposed in G-Eval (Auto-CoT + score only) with rate-explain and analyze-rate under different temperatures used when generating the output from ChatGPT. We follow Chiang and Lee (2023) and use two temperatures: 0.7 and 0.3 .</p>
<p>The results are shown in Appendix Table 5 and summarized as follows: First, when fixing the sampling temperature, we find that rate-explain and analyze-rate always achieve a higher correlation compared with G-Eval. This supports our guideline that "asking the LLM to explain/analyze outperforms the method proposed in G-Eval." Next, we observe that the correlation of G-Eval when $T=0.3$ is much lower than that of $T=1.0$. This shows that G-Eval is not robust to sampling temperature. Contrarily, we find that the correlations obtained by rate-explain and analyze-rate do not significantly change for different sampling
temperatures for almost all cases. This shows that rate-explain and analyze-rate are more robust than G-Eval with respect to the sampling temperature.</p>
<h3>3.4.2 Changing the Prompts</h3>
<p>We check if our guideline holds if we change the prompt given to ChatGPT. In this experiment, we changed the prompts to ChatGPT by appending some instructions before the descriptions of the rating task. We tried with two prompts: (1) the HHH prompts and (2) the human annotator prompts. The HHH prompt is designed by Bai et al. (2022) to align the output of LLMs to be more harmless, honest, and helpful. The human annotator prompt is inspired by Chiang and Lee (2023), who use a similar prompt to make the LLM behave as a human annotator. These two prompts will be inserted before the prompt we originally used in our paper. We use these two prompts to inject persona into the LLM. This is inspired by Zeng et al. (2023), which shows that the output of GPT3 can be different when prompted with a different persona. The prompts are detailed in Appendix F.3.</p>
<p>The results are shown in Table 6 and summarized as follows: rate-explain and analyze-rate consistently outperform the G-eval when using the human annotator prompts and the HHH prompts. This indicates that our guidelines are robust toward different prompts. We also find that the correlations of G-Eval significantly drop when adding the human-annotator prompts or HHH prompts. On the other hand, the correlation for rate-explain and analyze-rate do not significantly decrease when adding the human-annotator prompt and the HHH prompt. This shows that asking the LLM to explain is more robust to the variation of the prompts.</p>
<h2>4 Conclusion</h2>
<p>We study how to better use ChatGPT as an automatic evaluation tool by scrutinizing LLM evaluation and G-Eval. We provide concrete guidelines and show that by using those guidelines, the correlations of several evaluated attributes given by ChatGPT, a publicly usable model, can be higher than or comparable to the ratings given by GPT-4, a highly restricted and pricey model. We also show that the evaluation results based on our guidelines improve the best correlation that ChatGPT's rating can achieve. We believe our results and guidelines help future researchers better use LLMs for evaluation.</p>
<h2>Limitations</h2>
<p>There are three main limitations of this paper.</p>
<ol>
<li>We only use ChatGPT to conduct the experiments in this paper. We explain why we chose ChatGPT in Section 2.3. We believe that using ChatGPT is already enough since we show that the correlations obtained by using ChatGPT are already comparable to or better than the previous SoTA results obtained by GPT-4.</li>
<li>We only conduct analysis using two tasks, while we know that NLP has more diverse tasks. We do not guarantee that our observations can generalize to all the other datasets. We recommend the users verify the effectiveness of using LLM to evaluate the tasks of interest.</li>
<li>We cannot fairly compare our results with Liu et al. (2023), the previous SoTA results, due to multiple reasons. We explain those reasons in Appendix A.</li>
</ol>
<h2>Ethics Statement</h2>
<p>Our paper follows the ACL Code of Ethics. We do not see a particular harmful outcome of our paper. The code and datasets for reproducing our experiments can be found at https://github.com/d223302/ A-closer-Look-To-LLM-Evaluation/.</p>
<h2>Acknowledgements</h2>
<p>We want to thank the reviews for providing detailed feedback and actionable suggestions, which helped us strengthen our paper. We also want to thank the senior committee members for monitoring the reviewing process. Cheng-Han Chiang is supported by a Ph.D. scholarship program by Delta Electronics.</p>
<h2>References</h2>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A
general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback.</p>
<p>Ondřej Bojar, Yvette Graham, and Amir Kamran. 2017. Results of the WMT17 metrics shared task. In Proceedings of the Second Conference on Machine Translation, pages 489-513, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15607-15631, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anushree Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tür. 2019. Topical-chat: Towards knowledge-grounded open-domain conversations.</p>
<p>Yvette Graham and Timothy Baldwin. 2014. Testing for significance of increased correlation with human judgment. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172-176, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Yvette Graham, Timothy Baldwin, and Nitika Mathur. 2015. Accurate evaluation of segment-level machine translation metrics. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1183-1191, Denver, Colorado. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.</p>
<p>Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.</p>
<p>Matouš Macháček and Ondřej Bojar. 2014. Results of the WMT14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293-301, Baltimore, Maryland, USA. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020. Usr: An unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681-707.</p>
<p>OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. Accessed on January 10, 2023.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2023. Socratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International Conference on Learning Representations.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 20232038, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<h2>A Why We Cannot Fairly Compare with the Results in Liu et al. (2023)</h2>
<p>As a work highly related to G-Eval, we would really like to compare our results with G-Eval. However, we encounter difficulties when comparing our results with those in Liu et al. (2023) for the following reasons.</p>
<ul>
<li>G-Eval proposes to use GPT-4 as the evaluation tool, while it is currently a highly restricted model, and we only have limited access to it.</li>
<li>G-Eval only releases the prompts for SummEval. We need to construct the prompts for Topical-Chat based on the human evaluation instructions released by Mehri and Eskenazi (2020). It is possible that the prompts we use for Topical-Chat are different from the prompts used in Liu et al. (2023), making their results incomparable to ours.</li>
<li>The prompts of fluency in SummEval released by Liu et al. (2023) in here is problematic so we need to construct new prompts for fluency. Refer to Appendix F. 1 for detailed explanations. This makes us unable to directly compare our results with the results in Liu et al. (2023).</li>
<li>We cannot reproduce the numbers on the paper of G-Eval even when using their official implementation and the GPT-4 responses they release. This means that the only thing we</li>
</ul>
<p>do is calculate the correlation coefficient using the data and code released on the official GitHub of G-Eval, but the numbers are quite different from the results in G-Eval's paper. Moreover, the results of fluency they provide is the result not using auto CoT, but the results of the other three attributes for SummEval use auto CoT. That is why we use a question mark for the auto CoT field in Table 1.</p>
<ul>
<li>The Table 2 in Liu et al. (2023) seems to be wrong. The caption (Spearman's $\rho$ and Kendall's $\tau$ ) does not match the headers ( $r$ and $\rho$ ). This makes us hard to compare their results with ours reliably.</li>
</ul>
<h2>B Supplementary Results for Topical-Chat</h2>
<p>Table 2 is the supplementary results for TopicalChat that we referred to in the main content. We plan to move Table 2 to the main content using the additional one page in the camera-ready version if the paper is accepted. See how Pearson's $r$ and Kendall's $\tau$ are calculated in Appendix C.</p>
<h2>B. 1 Is Auto CoT Useful When ChatGPT Is Asked to Explain?</h2>
<p>In Table 3, we show the results when we add the evaluation steps generated by auto CoT when we ask ChatGPT when prompting with (rate-explain). We find that on groundedness, using auto CoT is worse. However, for the other three attributes, using auto CoT is better. This again shows that auto CoT is not particularly useful.</p>
<h2>C Calculation of Correlation Coefficient</h2>
<p>In this paper, we calculate Pearson's $r$ and Kendall's $\tau$ between human ratings and ChatGPT's ratings. Whether to use Spearman's rank correlation or Pearson's (linear) correlation to evaluate the alignment between human ratings and an automatic evaluation metric is long-standing, but there has been an increasing trend towards Pearson's correlation since 2014 (Macháček and Bojar, 2014; Graham and Baldwin, 2014; Zhang* et al., 2020). We use the pearsonr and kendalltau in scipy. stats for calculating the correlation coefficients. For each attribute of each sample, the rating of ChatGPT is obtained by 20 samples; we set the decoding temperature to 1 and the top- $p$ in nucleus sampling to 1, following G-Eval (Liu et al., 2023).</p>
<p>Consider a dataset with $N$ source documents, and each source document has $M$ corresponding target documents. We also have the human ratings for $N \cdot M$ target documents on a specific attribute. While each attribute of each target document is rated by more than one human rater, we average those ratings when calculating the correlation coefficient. So the $N \cdot M$ ratings are the average ratings from different raters. In the case of SummEval, we have $N=100$ source documents and $M=16$ summaries generated by 16 summarization models. There are two different methods for calculating correlation coefficients.</p>
<h2>C.0.1 Method 1: Dataset-Level Correlation Coefficient</h2>
<p>In this method, we first obtain the ratings on $N \cdot M$ target documents from ChatGPT. We then calculate the correlation coefficient between the $N \cdot M$ ChatGPT's ratings and the $N \cdot M$ average human ratings. In this case, the correlation coefficient is calculated among two $N \cdot M$ vectors, meaning that the correlation coefficient is calculated across the entire dataset.</p>
<h2>C.0.2 Method 2: Document-Level Correlation Coefficient</h2>
<p>In this method, for each source document, we obtain the ratings of its $M$ target documents using ChatGPT. Next, we calculate the correlation coefficient between these $M$ ChatGPT ratings and the corresponding $M$ human ratings. After iterating the above process over all the $N$ source documents, we obtain the $N$ correlation coefficients. We average the $N$ correlation coefficients as the final correlation coefficient. In this case, the correlation coefficient is calculated at the document-level and averaged over the whole dataset.</p>
<h2>C. 1 How We Calculate the Correlation Coefficient</h2>
<p>In Table 1 and 2 in this paper, we use Method 1 (Subsection C.0.1) to calculate Pearson's correlation, following the recommendation in Graham et al. (2015). Calculating the correlation coefficient on the dataset level is also used in LLM evaluation (Chiang and Lee, 2023). Calculating a single correlation coefficient on the dataset level allows us to use William's test to test whether two Pearson's $r$ are significantly different.</p>
<p>For Kendall's $\tau$ in Table 1 and 2, we follow most prior works (Zhong et al., 2022; Liu et al., 2023) to</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sec.</th>
<th style="text-align: center;">Ablations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Naturalness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Engagingness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Groundedness</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Output</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.566</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">rate-explain</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.664</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">rate-explain</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.693</td>
</tr>
</tbody>
</table>
<p>Table 3: The Pearson's $r$ and Kendall's $\tau$ correlation coefficient between LLMs' ratings and human ratings for Topical-Chat. All the results in this table, except the first row, are from ChatGPT. We boldface the Pearson's $r$ statistically significantly higher than auto CoT + score only. We underline the Pearson's $r$ comparable auto CoT + score only.
calculate Kendall's $\tau$ using Method 2 (documentlevel, Section C.0.2) to understand if ChatGPT can differentiate the quality difference between different system outputs for the same source document.</p>
<p>In fact, we find that Pearson's $r$ calculated by Method 1 and Method 2 are highly correlated. In Table 4, we show the result of Topical-Chat while we use Method 2 to calculate Pearson's $r$; Kendall's $\tau$ is still calculated by Method 2. Comparing the results of Pearson's $r$ in Table 2 and Table 4, one can easily see that when a method have significantly higher Pearson's $r$ in Table 2, it will also have significantly higher Pearson's $r$. We present the $r$ calculated by Method 1 because it makes more sense when calculating statistical significance when the correlation coefficient is calculated at the dataset-level (Graham et al., 2015).</p>
<h2>D Results of Changing the Temperature and Prompts</h2>
<p>We show the results of varying the temperature used to sample the ChatGPT output in Table 5. In the experiments in this section, we only sample $N=5$ samples from the ChatGPT since we find that G-eval and our proposed guidelines are quite robust to the number of samples when $N \geq 5$.</p>
<h2>E Datasets</h2>
<h2>E. 1 SummEval</h2>
<p>SummEval (Fabbri et al., 2021) is a dataset for the meta-evaluation of summarization. It contains 100 source documents, each with 16 summaries obtained from different summarization models. Each of the 1600 summaries is rated by three workers recruited on Amazon Mturk and two experts in summarization. Each summary in SummEval is rated by humans based on the coherence, consistency, fluency of the summary, and relevance between the summary and the source document. Each attribute is rated based on a 5-point Likert scale.</p>
<p>We download the source documents, summaries, and human ratings from the GitHub repository of G-Eval (https://github.com/nlpyang/geval/ tree/8f54105/data). SummEval was released under MIT License, and our usage for research does not violate the dataset's initial intention.</p>
<h2>E. 2 Topical-Chat</h2>
<p>Topical-Chat (Gopalakrishnan et al., 2019) is a knowledge-grounded open-domain dialogue dataset. The dataset consists of a dialogue context (history), an interesting fact related to the topic of the conversation, and a response. Mehri and Eskenazi (2020) releases high-quality human annotations on the quality of responses. They construct the dataset as follows: they first sample 60 dialogues context from Topical-Chat, and for each dialogue context and corresponding fun fact, they use a transformer model to generate four responses using four decoding methods. Each dialogue content has two additional responses: the human response and the ground truth response. Thus, there are a total of 360 dialogue-response pairs. Those pairs are evaluated based on six attributes, and we follow Zhong et al. (2022) and Liu et al. (2023) to only use four attributes: naturalness, coherence, engagingness, and groundedness (whether the response is grounded on the provided knowledge). We obtain the human ratings of Topical-Chat from the Github repository of UniEval (Zhong et al., 2022): https://github.com/maszhongming/UniEval/ blob/main/reproduce/data/dialogue/ topical_chat.json.</p>
<h2>F Prompts</h2>
<p>We list the prompts we use in this section. In the main content of the paper and in the following parts, we use different highlight colors to represent different parts of the prompt. A prompt is composed</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sec.</th>
<th style="text-align: center;">Ablations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Naturalness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Engagingness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Groundedness</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Output</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 ${ }^{\ddagger}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.566</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.582</td>
</tr>
<tr>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.582</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Free Text</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.666</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.663</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.693</td>
</tr>
</tbody>
</table>
<p>Table 4: The Pearson's $r$ and Kendall's $\tau$ correlation coefficient between LLMs' ratings and human ratings for Topical-Chat. Note that in this table, both Pearson's $r$ and Kendall's $\tau$ are calculated by Method 2 in Appendix C.0.2. All the results in this table, except the first row, are from ChatGPT. The results of GPT-4 are from Liu et al. (2023) but should not be compared with our results since the prompts they use may be different from the prompt we use. Still, we can see that for naturalness, engagingness, and groundedness, the results of rate-explain and analyze-rate is better or comparable to GPT-4.
of four parts: (1) the descriptions of the rating task, (2) the definition and rating criteria of the attribute to be rated, (3) the sample to be rated, and (4) a sentence used to prompt the LLM to give the rating.</p>
<p>The prompts for different attributes of the same dataset share the same descriptions of the rating task. Different attributes use different definition and rating criteria. In G-Eval, the prompts also compose of the evaluation steps generated by auto CoT.</p>
<h3>F. 1 Prompts for SummEval</h3>
<p>The descriptions of the rating task, the definition and rating criteria, the evaluation steps for coherence, consistency, and relevance in SummEval is from the prompts released by G-Eval in their GitHub repository (https://github.com/nlpyang/geval/tree/ 8f54105/prompts/summeval). While G-Eval also releases the prompt they use for fluency, we find something highly problematic in the prompt they use. The prompt for fluency asks the LLM to rate fluency on a scale of 1 to 3 (https://github.com/nlpyang/geval/blob/ 8f54105061e00377fbb909153892d5bfb5b3623a/ prompts/summeval/flu_detailed.txt), while the original rating scale in SummEval is $\mathbf{1}$ to $\mathbf{5}$. We also find that the original rating criteria used in G-Eval for fluency differ largely from the rating criteria of fluency used for human evaluation in SummEval. Through our experiment, we find that the misalignment of evaluation criteria and evaluation scale significantly decreases Pearson's $r$ with human ratings when using analyze-rate to
prompt ChatGPT to output. This is likely because ChatGPT tends to stick to the rating criteria when prompted with analyze-rate, and when using the rating criteria different from the criteria that are used to instruct the human raters, the scores generated by ChatGPT deviates more from the human ratings. This highlights the importance of using the same instructions to the LLM as those instructions used in the human evaluation, as emphasized in Chiang and Lee (2023).</p>
<p>First, we show an example prompt for coherence. This prompt corresponds to the score only + auto CoT in Table 1.</p>
<h2>Coherence</h2>
<p>You will be given one summary written for a news article.
Your task is to rate the summary on one metric.
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.
Evaluation Criteria:
Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of struc ture and coherence whereby "the summary should be well-structured and well-orga nized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic."
Evaluation Steps:</p>
<ol>
<li>Read the news article carefully and</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Auto-CoT</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.263</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 7}$</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 9}$</td>
</tr>
</tbody>
</table>
<p>(a) Temperature $T=0.3$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Auto-CoT</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.334</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 5}$</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 9 2}$</td>
</tr>
</tbody>
</table>
<p>(b) Temperature $T=0.7$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Auto-CoT</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.403</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 7 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 9}$</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 7 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 4}$</td>
</tr>
</tbody>
</table>
<p>(c) Temperature $T=1.0$ (The result in Table 1 )</p>
<p>Table 5: Comparing G-Eval (Auto-CoT + score only) with rate-explain and analyze-rate at different temperatures. We boldface Pearson's r statistically significantly higher than the baseline (the first row in each subtable).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Auto-CoT</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.345</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">textbf0.526</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 5}$</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 9}$</td>
<td style="text-align: center;">0.416</td>
</tr>
</tbody>
</table>
<p>(a) Results when prompted with the human evaluator prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Auto-CoT</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Relevance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Score only</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.301</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Rate-explain</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 5}$</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">$\mathbf{0 . 4 7 8}$</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Analyze-rate</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 7 5}$</td>
<td style="text-align: center;">0.406</td>
</tr>
</tbody>
</table>
<p>(b) Results when prompted with the HHH prompts.</p>
<p>Table 6: Comparing G-Eval (Auto-CoT + score only) with rate-explain and analyze-rate when using different prompts. We boldface Pearson's r statistically significantly higher than the baseline (the first row in each subtable).</p>
<p>identify the main topic and key points.
2. Read the summary and compare it to the news article. Check if the summary cov ers the main topic and key points of the news article, and if it presents them in a clear and logical order.
3. Assign a score for coherence on a scale of 1 to 5 , where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.
Example:
Source Text: {{Document}}
Summary: {{Summary}}
Evaluation Form (scores ONLY):</p>
<ul>
<li>Coherence:</li>
</ul>
<h2>F.1.1 Different Output Prompts</h2>
<p>For different output prompts, which is the ablation in Section 3.2 and the last block in Table 1 and 2, we only change the yellow parts (the last part) in the example prompt above. There are four output prompts used in Section 3.2: score only, free text, rate-explain, and analyze-rate. The prompts for free text is attribute-dependent, and we list them in the Their corresponding output prompts are listed as follows:</p>
<h2>Score only</h2>
<p>Evaluation Form (scores ONLY):</p>
<ul>
<li>{Attribute}:</li>
</ul>
<h2>Rate-explain</h2>
<p>Evaluation Form (Answer by starting with "Rating:" and then give the explanation of the rating on the next line by "Ratio nale:"):</p>
<ul>
<li>{Attribute}:</li>
</ul>
<h2>Analyze-rate</h2>
<p>Evaluation Form (Answer by starting with "Analysis:" to analyze the given example regarding the evaluation criteria as con cise as possible, and then give the nu meric rating on the next line by "Rat ing:):</p>
<ul>
<li>{Attribute}:</li>
</ul>
<h2>F.1.2 Attribute-Dependent Prompts</h2>
<p>The definition and rating criteria of the attribute to be rated, the evaluation steps generated by auto CoT , and output prompt for text-free are attributedependent, and we list them as follows. We use different colors to denote different parts in the prompt.</p>
<p>Note that the following prompts are not the complete prompts used as the model input; they need to be used with the descriptions of the rating task and the sample to be rated.</p>
<h2>Coherence</h2>
<h2>Evaluation Criteria:</h2>
<p>Coherence (1-5) - the collective quality of all sentences. We align this dimen sion with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic."</p>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the news article carefully and identify the main topic and key points.</li>
<li>Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.</li>
<li>Assign a score for coherence on a scale of 1 to 5 , where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.</li>
</ol>
<h2>Question:</h2>
<p>How coherent is the summary? That is, how well do the sentences in the summary fit together? (On a scale of 1-5, with 1 being the lowest)</p>
<h2>Consistency</h2>
<p>Evaluation Criteria:
Consistency (1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are en tailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts.</p>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the news article carefully and identify the main facts and details it presents.</li>
<li>Read the summary and compare it to the</li>
</ol>
<p>article. Check if the summary contains any factual errors that are not supported by the article.
3. Assign a score for consistency based on the Evaluation Criteria.</p>
<h2>Question:</h2>
<p>How consistent is the summary with the source document in terms of the factual alignment? (On a scale of 1-5, with 1 being the lowest)</p>
<h2>Fluency</h2>
<p>Evaluation Criteria:
Fluency (1-5): This rating measures the quality of individual sentences, are they well-written and grammatically cor rect. Consider the quality of individual sentences.</p>
<h2>Evaluation steps:</h2>
<ol>
<li>Read the given summary.</li>
<li>Evaluate the fluency of the summary on a scale of 1-5 based on the criteria provided.</li>
<li>Provide the rating.</li>
</ol>
<h2>Question:</h2>
<p>Based on the evaluation criteria, how fluent is the summary? (On a scale of $1-5$, with 1 being the lowest)</p>
<h2>Relevance</h2>
<p>Evaluation Criteria:
Relevance (1-5) - selection of important content from the source. The summary should include only important informa tion from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information.</p>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the summary and the source document carefully.</li>
<li>Compare the summary to the source document and identify the main points of the article.</li>
<li>Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.</li>
<li>Assign a relevance score from 1 to 5 .</li>
</ol>
<p>Question:
On a scale of 1-5, with 1 being the lowest, is the summary relevant to the source document and does the summary only contain the important information of the source document?</p>
<h2>F. 2 Prompts for Topical-Chat</h2>
<p>First, we show an example prompt for naturalness. This prompt corresponds to the score only + auto CoT in Table 2.</p>
<h2>Naturalness</h2>
<p>You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well.
Your task is to rate the responses on one metric.
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Crieteria:</h2>
<p>Naturalness (1-3) Is the response natu rally written??</p>
<ul>
<li>A score of 1 (bad) means that the response is unnatural.</li>
<li>A score of 2 (ok) means the response is strange, but not entirely unnatural. - A score of 3 (good) means that the response is natural.</li>
</ul>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the conversation between the two individuals.</li>
<li>Read the potential response for the next turn in the conversation.</li>
<li>Evaluate the response based on its naturalness, using the provided criteria.</li>
<li>Assign a rating score of 1,2 , or 3 based on the evaluation.</li>
</ol>
<p>Example:
Conversation History:
{ {Document}}
Corresponding Fact:</p>
<div class="codehilite"><pre><span></span><code><span class="cp">{{</span><span class="nv">Fact</span><span class="cp">}}</span>
<span class="x">Response:</span>
<span class="cp">{{</span><span class="nv">Response</span><span class="cp">}}</span>
</code></pre></div>

<p>Evaluation Form (scores ONLY):</p>
<ul>
<li>Naturalness:</li>
</ul>
<h3>1.2.1 Different Output Prompts</h3>
<p>For Topical-Chat, we also conduct ablations on different output prompts. Those different output prompts for score only, rate-explain, analyze-rate are the same as those listed in Section F.1.1. We do not list them here to save some space. The exact prompts we use can be found in the supplementary data of this paper.</p>
<h2>F.2.2 Attribute-Dependent Prompts</h2>
<p>The definition and rating criteria of the attribute to be rated, the evaluation steps generated by auto CoT , and output prompt for text-free are attributedependent, and we list them as follows. Again, the following prompts are not the complete prompts used as the model input; they need to be used with the descriptions of the rating task and the sample to be rated.</p>
<h2>Naturalness</h2>
<p>Evaluation Crieteria:
Naturalness (1-3) Is the response natu rally written??</p>
<ul>
<li>A score of 1 (bad) means that the response is unnatural.</li>
<li>A score of 2 (ok) means the response is strange, but not entirely unnatural.</li>
<li>A score of 3 (good) means that the response is natural.</li>
</ul>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the conversation between the two individuals.</li>
<li>Read the potential response for the next turn in the conversation.</li>
<li>Evaluate the response based on its naturalness, using the provided criteria.</li>
<li>Assign a rating score of 1,2 , or 3 based on the evaluation.</li>
</ol>
<h2>Question:</h2>
<p>How natural is the reponse? (On a scale of $1-3$, with 1 being the lowest)</p>
<h2>Coherence</h2>
<p>Evaluation Crieteria:</p>
<p>Coherence (1-3) Does the response serve as a valid continuation of the conversa tion history?</p>
<ul>
<li>A score of 1 (no) means that the response drastically changes topic or ignores the conversation history.</li>
<li>A score of 2 (somewhat) means the response refers to the conversation history in a limited capacity (e.g., in a generic way) and shifts the conversation topic.</li>
<li>A score of 3 (yes) means the response is on topic and strongly acknowledges the conversation history.</li>
</ul>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the conversation history.</li>
<li>Read the potential response.</li>
<li>Evaluate the coherence of the response based on the conversation history.</li>
<li>Assign a score of 1,2 , or 3 for coherence.</li>
</ol>
<h2>Question:</h2>
<p>Does the response serve as a valid con tinuation of the conversation history? (On a scale of 1-3, with 1 meaning the response is invalid and 3 meaning the response is coherent)</p>
<h2>Engagingness</h2>
<p>Evaluation Crieteria:
Engagingness (1-3) Is the response dul 1/interesting?</p>
<ul>
<li>A score of 1 (dull) means that the response is generic and dull.</li>
<li>A score of 2 (somewhat interesting) means the response is somewhat inter esting and could engage you in the conversation (e.g., an opinion, thought) - A score of 3 (interesting) means the response is very interesting or presents an interesting fact</li>
</ul>
<h2>Evaluation Steps:</h2>
<ol>
<li>Read the conversation, the correspond ing fact and the response carefully.</li>
<li>Rate the response on a scale of $1-3$ for engagingness, according to the criteria above.</li>
</ol>
<p>Question:</p>
<p>Is the response interesting and engaging? (On a scale of 1-3, with 1 meaning dull and 3 meaning interesting)</p>
<h1>Groundedness</h1>
<p>Evaluation Crieteria:
Groundedness (0-1) given the fact that this response is conditioned on, deter mine whether this response uses that fact.</p>
<ul>
<li>A score of 0 (no) means the response does not mention or refer to the fact at all</li>
<li>A score of 1 (yes) means the response uses the fact well</li>
</ul>
<p>Evaluation Steps:</p>
<ol>
<li>Read the conversation between the two individuals.</li>
<li>Identify the fact that is provided for the potential response.</li>
<li>Read the potential response.</li>
<li>Determine if the potential response uses or mentions the fact.</li>
<li>Assign a score of 0 or 1 for grounded ness based on whether the response uses the fact.</li>
</ol>
<p>Question:
Given the fact that this response is conditioned on, does the response use the fact? (On a scale of $0-1$, with 0 meaning no and 1 meaning yes)</p>
<h2>F. 3 Prompts for Section 3.4.2</h2>
<p>HHH prompts You are an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant is happy to help with almost anything, and will do its best to understand exactly what is needed.</p>
<p>Human annotator prompts Assume that you are a professional and careful human evaluator. You are recruited and paid to conduct the following task. You need to strictly follow the task instruction and ensure that you are doing the job with high-quality.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ In our paper, we use different highlight colors to represent different parts of the prompt, as shown in the above text. Additionally, we use cyan to represent the parts generated by auto Chain-of-Thought&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
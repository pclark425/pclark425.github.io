<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1901 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1901</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1901</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280000411</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.19498v1.pdf" target="_blank">T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Building a general robotic manipulation system capable of performing a wide variety of tasks in real-world settings is a challenging task. Vision-Language Models (VLMs) have demonstrated remarkable potential in robotic manipulation tasks, primarily due to the extensive world knowledge they gain from large-scale datasets. In this process, Spatial Representations (such as points representing object positions or vectors representing object orientations) act as a bridge between VLMs and real-world scene, effectively grounding the reasoning abilities of VLMs and applying them to specific task scenarios. However, existing VLM-based robotic approaches often adopt a fixed spatial representation extraction scheme for various tasks, resulting in insufficient representational capability or excessive extraction time. In this work, we introduce T-Rex, a Task-Adaptive Framework for Spatial Representation Extraction, which dynamically selects the most appropriate spatial representation extraction scheme for each entity based on specific task requirements. Our key insight is that task complexity determines the types and granularity of spatial representations, and Stronger representational capabilities are typically associated with Higher overall system operation costs. Through comprehensive experiments in real-world robotic environments, we show that our approach delivers significant advantages in spatial understanding, efficiency, and stability without additional training.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1901.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1901.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T-Rex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T-Rex: Task-Adaptive Spatial Representation Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-adaptive framework that uses a Vision-Language Model (VLM) to select and invoke heterogeneous, multi-granularity spatial representation extractors per object/stage, and generates constraint-based robot action sequences via a solver or generated Python policies, operating zero-shot without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T-Rex (system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A modular robotics system combining (1) a Chain of Grounding (CoG) prompting pipeline for a VLM to decompose instructions, infer operational hints and spatial constraints, and select extraction tools; (2) a Spatial Representation Extraction Toolkit integrating multiple vision modules (keypoints, vectors, 6D pose, segmentation, local crop); and (3) a low-level action-sequence generator that solves continuous constraints or runs VLM-generated Python policies. The system uses a VLM (GPT-4.1 in experiments) for high-level reasoning and code generation; perception modules are heterogeneous (off-the-shelf VFMs and VLM-invoked code).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language and dialogue-finetuned large multimodal language model used as VLM for reasoning; toolkit modules include vision foundation models pre-trained on standard vision tasks (detection, segmentation, 6D-pose methods).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Paper states VLM pretraining is primarily on text and 2D image internet data and dialogue tasks; vision foundation modules were trained on standard image datasets and specialized 6D-pose datasets. No new finetuning was performed for T-Rex.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world open-vocabulary robotic manipulation (desktop manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>15 real-world manipulation tasks (e.g., open drawer, open bottle, sweep trash, pack shoes, pour water, stack blocks, fold coat, hammer button, close laptop lid, multiple 'setup table' variants) executed on a UR5e arm with 1-DOF gripper using a single overhead Intel RealSense D435i RGB-D camera. Action space: continuous SE(3) end-effector trajectories + discrete gripper commands; tasks vary from single-stage to multi-stage with object pose/orientation and state constraints; experiments are real-world (not simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper discusses alignment qualitatively: VLM pretraining (text+images) provides commonsense and affordance priors useful for mapping language to spatial constraints; explicit Chain of Grounding is used to increase semantic alignment by grounding instructions into operational hints and representation/tool selection. No quantitative overlap analysis between pretraining corpus and task objects/actions is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Overall success rate: 60.7% ±2.1% across the 15 real-world tasks (10 trials per task, randomized poses); mean task execution time: 45.5 s. Per-task breakdown is provided in Table 1 (e.g., Open Drawer 6/10, Open Bottle 8/10, Sweep Trash 9/10, etc.). These results use GPT-4.1 as the VLM and the adaptive toolkit; no additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable — T-Rex is evaluated zero-shot without further training. Baseline comparisons to systems with different perception/representation choices are given (see 'comparison_to_vision_only'). No explicit ablation where the VLM is removed and replaced by a non-language pretrained planner is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper does not report sample-efficiency curves or numbers for learning; T-Rex is explicitly zero-shot (no extra training). The paper emphasizes 'no additional training' as an advantage but gives no quantitative sample-efficiency comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention- or saliency-style analysis of what the VLM attends to is reported. The paper instead uses an explicit multi-stage CoG prompting scheme and local-crop mechanism to focus extractors when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No analysis of embedding spaces or feature clustering is reported for the VLM or vision extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — procedural evidence: CoG converts natural-language instructions into representation-agnostic operational hints, natural-language spatial constraints, tool selections, and executable Python constraint functions; these constraints consume spatial representations (points, vectors, 6D poses) and are solved to produce end-effector trajectories. Ablations show CoG reduces planning errors (VLM hallucinations) and improves success rates. The system demonstrates language→representation grounding by mapping verb and spatial phrases to concrete constraints (e.g., 'upright and facing the camera' → 6D pose/orientation constraint).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in feature-level detail. The paper describes a coarse-to-fine, multi-granularity extraction strategy (coarse whole-image extraction, then localized fine-grained extraction via segmentation and local crop) but gives no representational-level hierarchical feature attribution analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper notes transfer depends on: (1) VLM reasoning capability (stronger VLMs improve constraint generation), (2) availability and quality of vision extractors (e.g., 6D pose tools may require object priors/meshes), (3) similarity between toolkit capabilities and task demands (domain similarity), and (4) tracking robustness (occlusions and motion degrade tracking). It reports that adaptive tool selection helps transfer across task complexity by trading off representational power vs extraction cost.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No explicit split or quantitative comparison between novel vs familiar objects. The paper flags a limitation: model-based 6D pose extractors require object priors (meshes), which limits handling previously unseen objects; therefore behavior on truly novel objects is expected to be worse but not quantitatively reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot: the system is deployed without further task-specific training; the paper emphasizes 'robust zero-shot generalization' across open-vocabulary tasks and reports the 60.7% overall success rate under zero-shot operation.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise or component freezing/ablation analysis of the VLM internals is presented; ablations concern the presence/absence of CoG and the Toolkit (fixed extractor vs toolkit).</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit quantitative negative transfer examples from language pretraining are reported. The paper does report failure modes tied to insufficient tracking, unsuitable extraction tools (when toolkit removed), and VLM hallucinations (mitigated by CoG).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Direct experimental comparison: T-Rex outperforms ReKep (a keypoint-based constraint solver baseline) and VoxPoser (a VLM+LLM composable 3D value map method). Aggregate success rates: VoxPoser 30.0%, ReKep 36.4%, T-Rex 60.7% (Table 1 totals). The paper highlights that tasks requiring 6D pose (orientation) show larger gains relative to keypoint-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No training-time temporal dynamics are reported. The paper discusses runtime dynamics (execution time per task) and that per-tool historical average execution times are tracked and used in tool-selection trade-offs, but not representation-learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality or intrinsic-dimension analyses of representations are provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1901.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1901.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Grounding (CoG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A staged prompting/inference protocol that guides a VLM through decomposition: (1) generate representation-agnostic operational hints, (2) infer natural-language spatial constraints grounded to scene objects, (3) query toolkit registry to select extraction tools balancing success probability vs extraction cost, and (4) emit executable Python constraint functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain of Grounding (CoG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A structured multi-stage prompt/programming pattern applied to a VLM to reduce hallucination and improve stable grounding of natural-language instructions into spatial constraints and tool selections; CoG is not a learned model but a deterministic decomposition and prompting pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not applicable (CoG is a prompting / inference strategy rather than a pretrained model).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Instruction grounding for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used within T-Rex to process free-form instructions and RGB-D scene observations, decompose tasks into stages, produce operational hints and constraint code, and choose per-object spatial extraction tools to feed the low-level solver; applied to the same 15 real-world desk manipulation tasks described for T-Rex.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>CoG explicitly attempts to increase semantic alignment by (a) producing representation-agnostic operational hints, (b) grounding constraints in scene objects, and (c) prompting the VLM to choose the simplest extractor that satisfies task requirements (explicit trade-off with historical execution time). The paper reports this reduces VLM hallucinations and improves the validity of constraint code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>The paper reports that including CoG increases overall success rate (ablation): full system success 60.7% vs 'w/o CoG, w/o Toolkit (VPV)' 51.4% ±1.4% (Table 2 and ablation results indicate CoG yields an uplift; exact isolated CoG-only delta depends on toolkit configuration).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable — CoG operates by prompting a VLM; there is no alternative non-language-pretrained analogue evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported. CoG is not trained and thus sample efficiency is not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable — CoG is a prompting pipeline; no attention diagnostics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — ablation studies show CoG lowers planning (VLM) errors to near-negligible levels and improves the validity of generated constraint code and tool selections (human-evaluated validity metric in Appendix A.3). CoG explicitly maps verbs and spatial language to constraint functions consumed by the low-level planner.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>CoG implements an explicit hierarchical decomposition (hints → constraints → tool selection → code), but no representational feature analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>CoG effectiveness depends on the underlying VLM capacity (stronger VLMs yield better constraint/code validity) and on structured toolkit documentation (the registry) being available to the VLM. The paper notes CoG mitigates but does not eliminate VLM failures when VLM reasoning is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not directly evaluated within CoG-specific analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CoG is designed for zero-shot grounding via prompting; the paper demonstrates zero-shot usage without finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CoG is a language-side method and thus compared indirectly: CoG + toolkit (language-guided selection of vision modules) outperforms fixed vision-only extractor pipelines in overall system performance (ablation shows removing toolkit harms success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1901.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1901.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.1 (vision-language-capable VLM used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multimodal large Vision-Language Model used as the primary VLM for CoG inference and code/constraint generation in T-Rex; employed for instruction decomposition, constraint generation, tool selection, and vision-code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal Vision-Language Model / large multimodal language model used for high-level reasoning, prompt-based code generation and grounding. Processes natural language instructions and scene observations via CoG prompting; used to generate constraint functions and to synthesize vision-extraction code for some extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Described in paper as pretrained primarily on text and 2D image data and finetuned on dialogue tasks (vision-language pretraining and language/dialogue finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Paper states VLM pretraining was primarily on large-scale image-text internet data and dialogue corpora; no task-specific embodied datasets were used for T-Rex. The paper does not provide a detailed dataset list.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Instruction grounding and constraint/code generation for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used within T-Rex to process free-form instructions and RGB-D scene observations to output per-stage hints, spatial constraints, tool selections, and executable Python constraint functions that drive a solver or generated policies for the same real-world desktop manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper qualitatively describes reasonable overlap: the VLM's web-scale pretraining yields commonsense and affordance knowledge that supports mapping language to spatial constraints; however, the paper emphasizes a gap between web-pretraining and embodied 3D tasks that CoG mitigates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 3 (exploratory VLM choice) reports for GPT-4.1: 'Inference Validity Rate' 90.0% and 'Total Success Rate' 60.7% (these numbers reflect T-Rex using GPT-4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported. The paper compares different VLM models (o3, GPT-4o mini, gpt-4-vision-preview) but does not include a non-language-pretrained baseline; success rates with lower-tier VLMs are provided (e.g., gpt-4-vision-preview total success 55.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported — VLM used only for inference; no finetuning or learning curves are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention or vision-language attention visualizations are provided for GPT-4.1.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect evidence: high 'inference validity rate' of constraint code and the system-level success rate when using GPT-4.1 indicate effective mapping from language to spatial constraints and tool selections; CoG plus GPT-4.1 reduced VLM planning errors relative to naive prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper reports that stronger VLMs (e.g., GPT-4.1) improve constraint generation validity and downstream task success; weaker/lower-tier VLMs yield worse validity and success. No systematic domain-shift transfer metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed specific to GPT-4.1; general limitations discussed regarding 6D pose modules needing object priors for unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Demonstrated zero-shot: GPT-4.1 is used without additional task-specific finetuning; system still achieves 60.7% success.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise probing or ablations of GPT-4.1 internals are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper does not report instances where GPT-4.1 pretraining harmed performance, but notes that VLM hallucinations can occur without CoG prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Paper compares system performance across different VLM choices (Table 3) and compares T-Rex (VLM-guided) to non-VLM baselines (ReKep, VoxPoser) at the system level; no direct controlled comparison of GPT-4.1 representations vs vision-only pretrained models is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1901.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1901.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReKep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReKep: Relational Keypoint Constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that formulates manipulation tasks as Python constraint functions over predefined 3D semantic keypoints and solves them via hierarchical optimization; used as a keypoint-only baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReKep</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A constraint-based manipulation framework that uses semantic 3D keypoints as the primary spatial representation and formulates task objectives as cost functions over those keypoints; typically relies on vision modules to extract keypoints and on hierarchical solvers for action sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not described in this paper; ReKep is used as an existing method/baseline (no details of its pretraining provided in T-Rex paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (benchmark baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as a baseline on the same set of 15 real-world desktop manipulation tasks; ReKep in the paper is described as a keypoint-only pipeline, which struggles on tasks requiring 6D pose/orientation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper beyond the empirical observation that a keypoint-only representation can be insufficient to capture orientation/pose semantics required by some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not applicable — ReKep is presented as a comparator. In experiments, ReKep achieved total success 36.4% (Table 1 totals) and mean task execution time ~53.3 s.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>ReKep grounds language into constraints over semantic keypoints in prior work; in this paper it is used as a baseline and is noted to underperform on tasks that require full 6D pose or fine-grained orientation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper observes ReKep (keypoint-only) is less effective on tasks demanding 6D pose/orientation—i.e., transfer across tasks with stronger representational needs is limited when using only keypoints.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>ReKep functions as a zero-shot pipeline in these experiments (no extra training reported), but specific zero-shot metrics vs T-Rex are not decomposed.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported beyond underperformance compared to T-Rex on pose-sensitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>ReKep is effectively a vision-driven keypoint constraint method; T-Rex (VLM-guided, multi-representation) outperforms ReKep by ~24 percentage points overall (36.4% → 60.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1901.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1901.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VoxPoser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VoxPoser</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that uses VLMs/LLMs to infer affordances and composes 3D value maps for trajectory synthesis; used as a baseline for comparison in the T-Rex experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voxposer: Composable 3d value maps for robotic manipulation with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VoxPoser</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method combining VLM/LLM reasoning to map language instructions into composable 3D value maps that serve as objective functions for planners to synthesize zero-shot trajectories. Treated as a baseline in T-Rex comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper (VoxPoser is cited as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as a baseline on the same real-world desktop manipulation tasks. VoxPoser produced weaker overall success rates compared to T-Rex in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in depth here; paper uses VoxPoser as a comparative point to show the benefits of task-adaptive spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>In these experiments VoxPoser achieved total success ~30.0% (Table 1 totals) across the 15 tasks; per-task breakdown is in Table 1 (e.g., Open Drawer 4/10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>VoxPoser grounds language into 3D value maps in its original formulation; within this paper it is used as a baseline and no deeper grounding analyses are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed here beyond empirical baseline performance showing lower success than T-Rex.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>VoxPoser operates in a zero-shot planning style in prior work; in these comparisons it is evaluated under zero-shot conditions as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>VoxPoser represents a different VLM+planner approach; T-Rex's adaptive representation selection and CoG produce substantially higher overall success (60.7% vs ~30%).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Voxposer: Composable 3d value maps for robotic manipulation with language models <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Physically grounded vision-language models for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Language-grounded orientation bridges spatial reasoning and object manipulation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1901",
    "paper_id": "paper-280000411",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "T-Rex",
            "name_full": "T-Rex: Task-Adaptive Spatial Representation Extraction",
            "brief_description": "A task-adaptive framework that uses a Vision-Language Model (VLM) to select and invoke heterogeneous, multi-granularity spatial representation extractors per object/stage, and generates constraint-based robot action sequences via a solver or generated Python policies, operating zero-shot without additional training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T-Rex (system)",
            "model_description": "A modular robotics system combining (1) a Chain of Grounding (CoG) prompting pipeline for a VLM to decompose instructions, infer operational hints and spatial constraints, and select extraction tools; (2) a Spatial Representation Extraction Toolkit integrating multiple vision modules (keypoints, vectors, 6D pose, segmentation, local crop); and (3) a low-level action-sequence generator that solves continuous constraints or runs VLM-generated Python policies. The system uses a VLM (GPT-4.1 in experiments) for high-level reasoning and code generation; perception modules are heterogeneous (off-the-shelf VFMs and VLM-invoked code).",
            "pretraining_type": "vision-language and dialogue-finetuned large multimodal language model used as VLM for reasoning; toolkit modules include vision foundation models pre-trained on standard vision tasks (detection, segmentation, 6D-pose methods).",
            "pretraining_data_description": "Paper states VLM pretraining is primarily on text and 2D image internet data and dialogue tasks; vision foundation modules were trained on standard image datasets and specialized 6D-pose datasets. No new finetuning was performed for T-Rex.",
            "target_task_name": "Real-world open-vocabulary robotic manipulation (desktop manipulation)",
            "target_task_description": "15 real-world manipulation tasks (e.g., open drawer, open bottle, sweep trash, pack shoes, pour water, stack blocks, fold coat, hammer button, close laptop lid, multiple 'setup table' variants) executed on a UR5e arm with 1-DOF gripper using a single overhead Intel RealSense D435i RGB-D camera. Action space: continuous SE(3) end-effector trajectories + discrete gripper commands; tasks vary from single-stage to multi-stage with object pose/orientation and state constraints; experiments are real-world (not simulation).",
            "semantic_alignment": "Paper discusses alignment qualitatively: VLM pretraining (text+images) provides commonsense and affordance priors useful for mapping language to spatial constraints; explicit Chain of Grounding is used to increase semantic alignment by grounding instructions into operational hints and representation/tool selection. No quantitative overlap analysis between pretraining corpus and task objects/actions is reported.",
            "performance_with_language_pretraining": "Overall success rate: 60.7% ±2.1% across the 15 real-world tasks (10 trials per task, randomized poses); mean task execution time: 45.5 s. Per-task breakdown is provided in Table 1 (e.g., Open Drawer 6/10, Open Bottle 8/10, Sweep Trash 9/10, etc.). These results use GPT-4.1 as the VLM and the adaptive toolkit; no additional training.",
            "performance_without_language_pretraining": "Not applicable — T-Rex is evaluated zero-shot without further training. Baseline comparisons to systems with different perception/representation choices are given (see 'comparison_to_vision_only'). No explicit ablation where the VLM is removed and replaced by a non-language pretrained planner is reported.",
            "sample_efficiency_comparison": "The paper does not report sample-efficiency curves or numbers for learning; T-Rex is explicitly zero-shot (no extra training). The paper emphasizes 'no additional training' as an advantage but gives no quantitative sample-efficiency comparison.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention- or saliency-style analysis of what the VLM attends to is reported. The paper instead uses an explicit multi-stage CoG prompting scheme and local-crop mechanism to focus extractors when needed.",
            "embedding_space_analysis": "No analysis of embedding spaces or feature clustering is reported for the VLM or vision extractors.",
            "action_grounding_evidence": "Yes — procedural evidence: CoG converts natural-language instructions into representation-agnostic operational hints, natural-language spatial constraints, tool selections, and executable Python constraint functions; these constraints consume spatial representations (points, vectors, 6D poses) and are solved to produce end-effector trajectories. Ablations show CoG reduces planning errors (VLM hallucinations) and improves success rates. The system demonstrates language→representation grounding by mapping verb and spatial phrases to concrete constraints (e.g., 'upright and facing the camera' → 6D pose/orientation constraint).",
            "hierarchical_features_evidence": "Not analyzed in feature-level detail. The paper describes a coarse-to-fine, multi-granularity extraction strategy (coarse whole-image extraction, then localized fine-grained extraction via segmentation and local crop) but gives no representational-level hierarchical feature attribution analysis.",
            "transfer_conditions": "Paper notes transfer depends on: (1) VLM reasoning capability (stronger VLMs improve constraint generation), (2) availability and quality of vision extractors (e.g., 6D pose tools may require object priors/meshes), (3) similarity between toolkit capabilities and task demands (domain similarity), and (4) tracking robustness (occlusions and motion degrade tracking). It reports that adaptive tool selection helps transfer across task complexity by trading off representational power vs extraction cost.",
            "novel_vs_familiar_objects": "No explicit split or quantitative comparison between novel vs familiar objects. The paper flags a limitation: model-based 6D pose extractors require object priors (meshes), which limits handling previously unseen objects; therefore behavior on truly novel objects is expected to be worse but not quantitatively reported.",
            "zero_shot_or_few_shot": "Zero-shot: the system is deployed without further task-specific training; the paper emphasizes 'robust zero-shot generalization' across open-vocabulary tasks and reports the 60.7% overall success rate under zero-shot operation.",
            "layer_analysis": "No layerwise or component freezing/ablation analysis of the VLM internals is presented; ablations concern the presence/absence of CoG and the Toolkit (fixed extractor vs toolkit).",
            "negative_transfer_evidence": "No explicit quantitative negative transfer examples from language pretraining are reported. The paper does report failure modes tied to insufficient tracking, unsuitable extraction tools (when toolkit removed), and VLM hallucinations (mitigated by CoG).",
            "comparison_to_vision_only": "Direct experimental comparison: T-Rex outperforms ReKep (a keypoint-based constraint solver baseline) and VoxPoser (a VLM+LLM composable 3D value map method). Aggregate success rates: VoxPoser 30.0%, ReKep 36.4%, T-Rex 60.7% (Table 1 totals). The paper highlights that tasks requiring 6D pose (orientation) show larger gains relative to keypoint-only baselines.",
            "temporal_dynamics": "No training-time temporal dynamics are reported. The paper discusses runtime dynamics (execution time per task) and that per-tool historical average execution times are tracked and used in tool-selection trade-offs, but not representation-learning dynamics.",
            "dimensionality_analysis": "No dimensionality or intrinsic-dimension analyses of representations are provided.",
            "uuid": "e1901.0"
        },
        {
            "name_short": "CoG",
            "name_full": "Chain of Grounding (CoG)",
            "brief_description": "A staged prompting/inference protocol that guides a VLM through decomposition: (1) generate representation-agnostic operational hints, (2) infer natural-language spatial constraints grounded to scene objects, (3) query toolkit registry to select extraction tools balancing success probability vs extraction cost, and (4) emit executable Python constraint functions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chain of Grounding (CoG)",
            "model_description": "A structured multi-stage prompt/programming pattern applied to a VLM to reduce hallucination and improve stable grounding of natural-language instructions into spatial constraints and tool selections; CoG is not a learned model but a deterministic decomposition and prompting pipeline.",
            "pretraining_type": "Not applicable (CoG is a prompting / inference strategy rather than a pretrained model).",
            "pretraining_data_description": "Not applicable.",
            "target_task_name": "Instruction grounding for robotic manipulation",
            "target_task_description": "Used within T-Rex to process free-form instructions and RGB-D scene observations, decompose tasks into stages, produce operational hints and constraint code, and choose per-object spatial extraction tools to feed the low-level solver; applied to the same 15 real-world desk manipulation tasks described for T-Rex.",
            "semantic_alignment": "CoG explicitly attempts to increase semantic alignment by (a) producing representation-agnostic operational hints, (b) grounding constraints in scene objects, and (c) prompting the VLM to choose the simplest extractor that satisfies task requirements (explicit trade-off with historical execution time). The paper reports this reduces VLM hallucinations and improves the validity of constraint code.",
            "performance_with_language_pretraining": "The paper reports that including CoG increases overall success rate (ablation): full system success 60.7% vs 'w/o CoG, w/o Toolkit (VPV)' 51.4% ±1.4% (Table 2 and ablation results indicate CoG yields an uplift; exact isolated CoG-only delta depends on toolkit configuration).",
            "performance_without_language_pretraining": "Not applicable — CoG operates by prompting a VLM; there is no alternative non-language-pretrained analogue evaluated.",
            "sample_efficiency_comparison": "Not reported. CoG is not trained and thus sample efficiency is not applicable.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not applicable — CoG is a prompting pipeline; no attention diagnostics reported.",
            "embedding_space_analysis": "Not applicable.",
            "action_grounding_evidence": "Yes — ablation studies show CoG lowers planning (VLM) errors to near-negligible levels and improves the validity of generated constraint code and tool selections (human-evaluated validity metric in Appendix A.3). CoG explicitly maps verbs and spatial language to constraint functions consumed by the low-level planner.",
            "hierarchical_features_evidence": "CoG implements an explicit hierarchical decomposition (hints → constraints → tool selection → code), but no representational feature analysis is provided.",
            "transfer_conditions": "CoG effectiveness depends on the underlying VLM capacity (stronger VLMs yield better constraint/code validity) and on structured toolkit documentation (the registry) being available to the VLM. The paper notes CoG mitigates but does not eliminate VLM failures when VLM reasoning is weak.",
            "novel_vs_familiar_objects": "Not directly evaluated within CoG-specific analyses.",
            "zero_shot_or_few_shot": "CoG is designed for zero-shot grounding via prompting; the paper demonstrates zero-shot usage without finetuning.",
            "layer_analysis": "Not applicable.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "CoG is a language-side method and thus compared indirectly: CoG + toolkit (language-guided selection of vision modules) outperforms fixed vision-only extractor pipelines in overall system performance (ablation shows removing toolkit harms success rate).",
            "temporal_dynamics": "Not applicable.",
            "dimensionality_analysis": "Not applicable.",
            "uuid": "e1901.1"
        },
        {
            "name_short": "GPT-4.1",
            "name_full": "GPT-4.1 (vision-language-capable VLM used in experiments)",
            "brief_description": "The multimodal large Vision-Language Model used as the primary VLM for CoG inference and code/constraint generation in T-Rex; employed for instruction decomposition, constraint generation, tool selection, and vision-code generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4.1",
            "model_description": "A multimodal Vision-Language Model / large multimodal language model used for high-level reasoning, prompt-based code generation and grounding. Processes natural language instructions and scene observations via CoG prompting; used to generate constraint functions and to synthesize vision-extraction code for some extractors.",
            "pretraining_type": "Described in paper as pretrained primarily on text and 2D image data and finetuned on dialogue tasks (vision-language pretraining and language/dialogue finetuning).",
            "pretraining_data_description": "Paper states VLM pretraining was primarily on large-scale image-text internet data and dialogue corpora; no task-specific embodied datasets were used for T-Rex. The paper does not provide a detailed dataset list.",
            "target_task_name": "Instruction grounding and constraint/code generation for robotic manipulation",
            "target_task_description": "Used within T-Rex to process free-form instructions and RGB-D scene observations to output per-stage hints, spatial constraints, tool selections, and executable Python constraint functions that drive a solver or generated policies for the same real-world desktop manipulation tasks.",
            "semantic_alignment": "Paper qualitatively describes reasonable overlap: the VLM's web-scale pretraining yields commonsense and affordance knowledge that supports mapping language to spatial constraints; however, the paper emphasizes a gap between web-pretraining and embodied 3D tasks that CoG mitigates.",
            "performance_with_language_pretraining": "Table 3 (exploratory VLM choice) reports for GPT-4.1: 'Inference Validity Rate' 90.0% and 'Total Success Rate' 60.7% (these numbers reflect T-Rex using GPT-4.1).",
            "performance_without_language_pretraining": "Not reported. The paper compares different VLM models (o3, GPT-4o mini, gpt-4-vision-preview) but does not include a non-language-pretrained baseline; success rates with lower-tier VLMs are provided (e.g., gpt-4-vision-preview total success 55.0%).",
            "sample_efficiency_comparison": "Not reported — VLM used only for inference; no finetuning or learning curves are presented.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No internal attention or vision-language attention visualizations are provided for GPT-4.1.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Indirect evidence: high 'inference validity rate' of constraint code and the system-level success rate when using GPT-4.1 indicate effective mapping from language to spatial constraints and tool selections; CoG plus GPT-4.1 reduced VLM planning errors relative to naive prompting.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Paper reports that stronger VLMs (e.g., GPT-4.1) improve constraint generation validity and downstream task success; weaker/lower-tier VLMs yield worse validity and success. No systematic domain-shift transfer metrics reported.",
            "novel_vs_familiar_objects": "Not analyzed specific to GPT-4.1; general limitations discussed regarding 6D pose modules needing object priors for unseen objects.",
            "zero_shot_or_few_shot": "Demonstrated zero-shot: GPT-4.1 is used without additional task-specific finetuning; system still achieves 60.7% success.",
            "layer_analysis": "No layerwise probing or ablations of GPT-4.1 internals are reported.",
            "negative_transfer_evidence": "Paper does not report instances where GPT-4.1 pretraining harmed performance, but notes that VLM hallucinations can occur without CoG prompting.",
            "comparison_to_vision_only": "Paper compares system performance across different VLM choices (Table 3) and compares T-Rex (VLM-guided) to non-VLM baselines (ReKep, VoxPoser) at the system level; no direct controlled comparison of GPT-4.1 representations vs vision-only pretrained models is provided.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1901.2"
        },
        {
            "name_short": "ReKep",
            "name_full": "ReKep: Relational Keypoint Constraints",
            "brief_description": "A baseline method that formulates manipulation tasks as Python constraint functions over predefined 3D semantic keypoints and solves them via hierarchical optimization; used as a keypoint-only baseline in experiments.",
            "citation_title": "Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation",
            "mention_or_use": "use",
            "model_name": "ReKep",
            "model_description": "A constraint-based manipulation framework that uses semantic 3D keypoints as the primary spatial representation and formulates task objectives as cost functions over those keypoints; typically relies on vision modules to extract keypoints and on hierarchical solvers for action sequence generation.",
            "pretraining_type": "Not described in this paper; ReKep is used as an existing method/baseline (no details of its pretraining provided in T-Rex paper).",
            "pretraining_data_description": "Not provided here.",
            "target_task_name": "Robotic manipulation (benchmark baseline)",
            "target_task_description": "Used as a baseline on the same set of 15 real-world desktop manipulation tasks; ReKep in the paper is described as a keypoint-only pipeline, which struggles on tasks requiring 6D pose/orientation constraints.",
            "semantic_alignment": "Not analyzed in this paper beyond the empirical observation that a keypoint-only representation can be insufficient to capture orientation/pose semantics required by some tasks.",
            "performance_with_language_pretraining": "Not applicable — ReKep is presented as a comparator. In experiments, ReKep achieved total success 36.4% (Table 1 totals) and mean task execution time ~53.3 s.",
            "performance_without_language_pretraining": "Not applicable here.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "ReKep grounds language into constraints over semantic keypoints in prior work; in this paper it is used as a baseline and is noted to underperform on tasks that require full 6D pose or fine-grained orientation constraints.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Paper observes ReKep (keypoint-only) is less effective on tasks demanding 6D pose/orientation—i.e., transfer across tasks with stronger representational needs is limited when using only keypoints.",
            "novel_vs_familiar_objects": "Not analyzed here.",
            "zero_shot_or_few_shot": "ReKep functions as a zero-shot pipeline in these experiments (no extra training reported), but specific zero-shot metrics vs T-Rex are not decomposed.",
            "layer_analysis": "Not applicable.",
            "negative_transfer_evidence": "Not reported beyond underperformance compared to T-Rex on pose-sensitive tasks.",
            "comparison_to_vision_only": "ReKep is effectively a vision-driven keypoint constraint method; T-Rex (VLM-guided, multi-representation) outperforms ReKep by ~24 percentage points overall (36.4% → 60.7%).",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1901.3"
        },
        {
            "name_short": "VoxPoser",
            "name_full": "VoxPoser",
            "brief_description": "A baseline method that uses VLMs/LLMs to infer affordances and composes 3D value maps for trajectory synthesis; used as a baseline for comparison in the T-Rex experiments.",
            "citation_title": "Voxposer: Composable 3d value maps for robotic manipulation with language models",
            "mention_or_use": "use",
            "model_name": "VoxPoser",
            "model_description": "A method combining VLM/LLM reasoning to map language instructions into composable 3D value maps that serve as objective functions for planners to synthesize zero-shot trajectories. Treated as a baseline in T-Rex comparisons.",
            "pretraining_type": "Not specified in this paper (VoxPoser is cited as prior work).",
            "pretraining_data_description": "Not provided here.",
            "target_task_name": "Robotic manipulation (baseline)",
            "target_task_description": "Used as a baseline on the same real-world desktop manipulation tasks. VoxPoser produced weaker overall success rates compared to T-Rex in the reported experiments.",
            "semantic_alignment": "Not analyzed in depth here; paper uses VoxPoser as a comparative point to show the benefits of task-adaptive spatial representations.",
            "performance_with_language_pretraining": "In these experiments VoxPoser achieved total success ~30.0% (Table 1 totals) across the 15 tasks; per-task breakdown is in Table 1 (e.g., Open Drawer 4/10).",
            "performance_without_language_pretraining": "Not applicable in this paper's experiments.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "VoxPoser grounds language into 3D value maps in its original formulation; within this paper it is used as a baseline and no deeper grounding analyses are provided.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Not analyzed here beyond empirical baseline performance showing lower success than T-Rex.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "VoxPoser operates in a zero-shot planning style in prior work; in these comparisons it is evaluated under zero-shot conditions as a baseline.",
            "layer_analysis": "Not applicable.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "VoxPoser represents a different VLM+planner approach; T-Rex's adaptive representation selection and CoG produce substantially higher overall success (60.7% vs ~30%).",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1901.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Voxposer: Composable 3d value maps for robotic manipulation with language models",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "Physically grounded vision-language models for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Language-grounded orientation bridges spatial reasoning and object manipulation",
            "rating": 2
        }
    ],
    "cost": 0.020905749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models
24 Jun 2025</p>
<p>Yiteng Chen 
School of Software Engineering
South China University of Technology</p>
<p>Wenbo Li 
School of Software Engineering
South China University of Technology</p>
<p>Shiyi Wang 
School of Future Technology
School of Intelligent Engineering
South China University of Technology
3 Shien-Ming Wu</p>
<p>South China University of Technology</p>
<p>Huiping Zhuang hpzhuang@scut.edu.cn 
Qingyao Wu 
School of Software Engineering
South China University of Technology</p>
<p>T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models
24 Jun 2025DC59CF12D35586C30119A0CB96A2B44CarXiv:2506.19498v1[cs.RO]Spatial Representation Extraction Toolkit Spatial Representation Extraction Toolkit Many Other Tools
Building a general robotic manipulation system capable of performing a wide variety of tasks in real-world settings is a challenging task.Vision-Language Models (VLMs) have demonstrated remarkable potential in robotic manipulation tasks, primarily due to the extensive world knowledge they gain from large-scale datasets.In this process, Spatial Representations (such as points representing object positions or vectors representing object orientations) act as a bridge between VLMs and real-world scene, effectively grounding the reasoning abilities of VLMs and applying them to specific task scenarios.However, existing VLM-based robotic approaches often adopt a fixed spatial representation extraction scheme for various tasks, resulting in insufficient representational capability or excessive extraction time.In this work, we introduce T-Rex, a Task-Adaptive Framework for Spatial Representation Extraction, which dynamically selects the most appropriate spatial representation extraction scheme for each entity based on specific task requirements.Our key insight is that task complexity determines the types and granularity of spatial representations, and Stronger representational capabilities are typically associated with Higher overall system operation costs.Specifically, we design Chain of Grounding (CoG) to guide VLMs in progressively grounding instructions, incrementally inferring all spatial constraints codes at each stage, as well as the necessary spatial representations and their optimal extraction scheme.We develop an extensible Spatial Representation Extraction Toolkit that dynamically invokes Large Vision Models according to the CoG inference results, and calls fine-grained extraction methods only when deemed necessary by VLMs.Subsequently, low level action sequence generator generates the robot action sequence based on the constraints and the tracked spatial representations.Through comprehensive experiments in real-world robotic environments, we show that our approach delivers significant advantages in spatial understanding, efficiency, and stability without additional training.</p>
<p>Introduction</p>
<p>Developing a general robotic manipulation system capable of performing tasks in complex, variable, and unstructured real-world environments has long been a challenging task.Vision-Language Models (VLMs) [1] are increasingly applied to robotic manipulation tasks, largely because of their extensive world knowledge gained from pretraining on vast datasets.However, due to VLM's pretraining being</p>
<p>Task-Adaptive</p>
<p>Setup the table by stack the red block over the green block， insert the tools into the pen holder handle-end up, and position the plush toys upright side by side facing the same direction.In table organization task it extracts point for blocks, vector for tools and the pen holder, and 6D pose for plush toys, while defining positional constraints on blocks, orientation constraints on pen and pen holder, and pose constraints on plush toys.</p>
<p>Low-level</p>
<p>primarily based on text data and 2D image data from the Internet, and trained on user dialogue tasks, there is a capability between its reasoning abilities and the complex embodied tasks in the 3D real world.This gap makes it difficult for VLMs to fully meet the complex demands of real-world tasks, posing challenges for their application in real world robotic manipulation.</p>
<p>Recent research [2,3,4,5] has introduced promising methods to bridge this gap.These approaches employ spatial representations (e.g., points or vectors) to represent scene entities and utilize VLMs reasoning to define constraints between these representations.Robotic tasks are executed through interactions between these constraints and dynamically tracked spatial representations.The main advantages of these methods include eliminating the need for expensive, high-quality robotic interaction data and providing strong generalization capabilities across different scenarios and robots.</p>
<p>However, existing approaches still face notable challenges.Their spatial representation extraction schemes are typically Fixed or Task-Independent, lacking the flexibility to dynamically adapt to the requirements of different tasks or different stages within a single task.Consequently, this can result in Insufficient representational ability or Prolonged extraction times.For instance, a simple task like "Grasp and Place Plush Cat" may only require extracting the object's centroid as a keypoint.In contrast, "Placing Plush Cat Upright and Facing the Camera" necessitates precisely extracting the object's 6D pose.Furthermore, if the task additionally demands orienting the cat's tail toward a specific direction, an even finer-grained spatial representation, such as a vector indicating tail orientation, becomes necessary.These examples clearly show that the Complexity of a task determines the Types and Granularity of spatial representations required, and that Stronger representational ability typically entails Higher extraction costs.Therefore, methods relying on Fixed extraction schemes (e.g., DINO+SAM+clustering for keypoint extraction) struggle to accommodate varying task needs effectively, impacting task performance adversely.We define the problem as: How to provide Sufficient and Efficient spatial representation extraction support for VLM-guided robotic manipulation methods.</p>
<p>We propose T-Rex, a Task-Adaptive Spatial Representation Extraction approach, dynamically selects the optimal spatial representation extraction scheme for each entity in real-time based on the specific task and scene, effectively balancing representation capability and extraction efficiency.Additionally, to manage the complexity introduced by T-Rex, we introduce the Chain of Grounding (CoG) method, guiding the VLM through a robust, step-by-step grounding of instructions.</p>
<p>Specifically, we first construct an Extensible Spatial Representation Extraction Toolkit Framework integrating diverse Large Vision Models for spatial representation extraction.Through CoG, we explicitly define the VLM's instruction grounding process.When the VLM determines that a task stage requires finer-grained spatial representations, it triggers local image extraction for the relevant region and subsequently extracts the Fine-Grained spatial representations.After obtaining the Heterogeneous and Multi-Granular spatial representations, the VLM will construct constraints based on the acquired Heterogeneous and Multi-Granular spatial representations.Then, Low Level Action Sequence Generator generate the robot action sequence based on the constraints and the tracked spatial representations.</p>
<p>Our method offers several key: (1)</p>
<p>Related Works</p>
<p>Vision-Language Models for Robotics.Vision-Language Models (VLMs) are increasingly applied in robotic manipulation, attributable to their ability to acquire rich environmental understanding and high-level commonsense reasoning from large-scale pretraining data.Many existing studies focus on employing pretrained VLMs for task planning and high-level inference [6,7,8,9,10], while other approaches have begun to leverage inference capabilities of VLMs to guide more fine-grained robot actions beyond abstract planning [11,12,13,14,15].Additionally, some works explore the use of VLMs for reward function design in reinforcement learning [16].Methods most relevant to our work typically first extract spatial representations of the scene (e.g., keypoints or vectors), then have the VLM generate constraints based on these representations, and finally use a solver to compute the robot action sequence [2,4,5].</p>
<p>Grounding Language Instructions.Grounding Language Instructions has been extensively studied in robotic manipulation, aiming to map natural language instructions into forms that can relatively directly guide the robot in performing tasks.In earlier research, classical tools such as lexical analysis, formal logic, and graphical models were widely used to interpret and map language instructions [17,18,19,20].In recent years, end-to-end learning methods have gained significant application in robotic manipulation [21,22,23,24,25,26,27,28,29,30,30,31,32,33,34,35,36,37,38,39,40].These methods implicitly ground language instructions by learning the mapping between language and actions from data with language annotations, although they face challenges such as lack of interpretability and high data requirements.In research on using Vision-Language Models (VLMs) for robotic manipulation, some methods attempt to leverage the powerful reasoning capabilities of VLMs to directly map natural language instructions to robot action sequences [41].Other methods try to map language instructions to task planning representations such as text [42] or PDDL [43] .Additionally, some works explore mapping language instructions to executable code [44,45,46].The line of work most similar to ours [2,4,5] maps language instructions to constraints, which then interact with the scene to guide robotic manipulation.</p>
<p>Vision Foundation Models and Spatial Representation for Robotics.Research on applying Vision Foundation Models(VFMs) to computer vision tasks is a extensive and active field.Robotic manipulation involves processing complex, real-world scene information; therefore, leveraging the visual comprehension capacity of VFMs for scene analysis presents a direct and effective approach.The application of VFMs to robotic manipulation has also become a thriving area of research.We refer readers to recent surveys for an overview of the latest advances in VFMs and their applications in robotics [47,48].Existing work typically relies on VFMs to extract spatial representations from scenes--using the visual comprehension capacity learned from large-scale image datasets to guide robotic manipulation--because spatial representations offer a more immediate and effective approach of directing robotic actions compared to raw image inputs.Object detection [49,50,51,52]and segmentation models [53,54,55] can extract task-relevant bounding boxes or segmentation masks as spatial representations and have been widely adopted as 2D perception modules in robotic manipulation.Keypoints-valued for their flexibility, strong generalization, and ease of extraction-are among the most common spatial representations in robotic manipulation [56,57,2,15,58].Another</p>
<p>Setup the table by placing all small toys into the box, place the pens and tools upright with their tips facing up in the pen holder, and position the plush toys upright side by side facing the same way.</p>
<p>Toolkit Registry</p>
<p>Hints</p>
<p>Constraints</p>
<p>Constraint Codes</p>
<p>Figure 2: Overview framework.Given natural language instruction and scene observation, VLM guides system to adaptively invoke the optimal extractor from Spatial Representation Extraction Toolkit for each task-relevant object to derive spatial representations.Low-Level Action Sequence Generator then generates robot's action sequence based on these representations and VLM-inferred constraints to complete the task.</p>
<p>common representation is the 6D pose: foundation models estimate an object's full 6D pose via model-based [59,60,61] or model-free [62,63,64,65] approaches, offering enhanced spatial representation capability to capture the object's current pose.Several works have explored the utility of 6D poses as spatial representations for robotic manipulation tasks [66].Semantic vectors have been explored to encode task-relevant object orientations(e.g., camera-facing direction) [11].More recently, advances in 3D visual grounding have introduced 3D bounding boxes as a promising spatial representation [67,68,69,70], enabling precise localization of objects' positions and extents in three-dimensional space.</p>
<p>In Appendix A.8, we discuss the Connections and Distinctions between our approach and existing work in these fields.</p>
<p>Method</p>
<p>An overview of our approach is presented in Fig. 2. We begin by providing a concise formalization of the robotic manipulation problem and our framework (Sec.3.1), then give a detailed description of our Task-Adaptive, Heterogeneous Multi-Granularity representation extraction method (Sec.3.2), and finally introduce the Chain of Grounding (CoG) module for guiding the VLM in robust reasoning over robotic manipulation tasks (Sec.3.3).</p>
<p>Problem Formulation</p>
<p>Given a free-form natural-language instruction ℓ (e.g., "pour tea into the cup") and an initial scene observation X 0 (e.g., an RGB-D image), our goal is to produce a sequence of end-effector trajectories {τ i } N i=1 corresponding to subtasks {ℓ i } N i=1 into which ℓ is decomposed.Each trajectory τ i is a sequence of waypoints
τ i = (p i,t , g i,t ) Ti t=1
, where p i,t ∈ SE(3) is the desired 6-DoF end-effector pose at time t, and g i,t ∈ {open, close, hold} is the gripper command.Concatenating all τ i yields the full manipulation plan that transforms the initial scene X 0 to satisfy the instruction ℓ.</p>
<p>Task-Adaptive Extraction of Heterogeneous Spatial Representations</p>
<p>The Spatial Representation Extraction Toolkit includes a Registry containing information on each tool, maintained collaboratively by the system and users.The Registry is provided to the VLM at the first stage of the pipeline (see Sec. 3.3 for details) as one of the inputs to CoG.We discuss the selection of tools in the Toolkit and show a sample registry in Appendix A.5.1.</p>
<p>Formally, we represent the Spatial Representation Extraction Toolkit as a set of tools R = {T i } N i=1 , where each tool is defined in toolkit registry by
T i = (I i , o i , f i , s i , h i ),
with I i denoting the required inputs (e.g., RGB image, depth image, object list), o i the spatial representation output (e.g., 6D pose, keypoints), f i the output format (e.g., a 4 × 4 SE(3) matrix), s i a brief implementation summary, and h i the historical average execution time maintained and updated after each invocation.Given a natural-language instruction I and a scene observation X, tasks are decomposed into a set of stages S = {1, . . ., S}, each stage s having a set of task-relevant objects O s .For each s ∈ S and o ∈ O s , the VLM selects
t * s,o = arg max t∈R P succ (t | I, X, s, o) − λ h t ,
where P succ (t | I, X, s, o) estimates the success probability of extracting the required spatial representation, λ trades off success likelihood against extraction cost h t , and t * s,o denotes the optimal tool selected for stage s and task-relevant object o under this criterion.To guide the VLM in effectively implementing this tool selection process, we detail the Chain of Grounding (CoG) in Sec.3.3.</p>
<p>After selecting the most appropriate spatial representation extraction tool for each task-relevant object at each stage, we use the chosen tool to extract the spatial representation in that stage:
r s,o = t *
s,o (I, X), where r s,o is the extracted spatial representation (e.g., keypoints or 6D pose), and the task scene observation X = {c rgb , c depth }, with c rgb the RGB image from the camera and c depth the corresponding depth map.</p>
<p>In the first stage of the framework, the VLM uses CoG to generate a series of Spatial Constraints represented as Python functions; each constraint function takes the Spatial Representations extracted by the tools selected from the toolkit as input and returns a Scalar Cost that measures how well the constraint is satisfied-when a constraint is met, its cost approaches or equals zero.The details of Constraints based on Heterogeneous Representations are presented in Appendix A.2.</p>
<p>In the third stage of the framework, we enter Low-Level Robot Action-Sequence Generation.For common constraints built on typical spatial representations (e.g., points, vectors, or 6D poses)-whose  cost outputs are usually continuous-we use a solver to directly compute the robot's action sequence.The solver takes as input all of this stage's constraint functions, the tracked spatial representations, and other fixed constraints (such as collision avoidance and IK based reachability constraints).This component follows many of the practices introduced in [2], we refer readers to [2] for further details on constraint generation and solve.</p>
<p>In task stages where constraints rely on unconventional representations, we employ an alternative approach to generate the robot's action sequence.Following practices from [44], we leverage the VLM to assemble a complete Python policy script in which these constraint functions participate(e.g., within control flow logic) to shape the robot's behavior and ensure correct execution.Further details on representations (such as state machines or topologies) are provided in Appendix A.5.1.The toolkit's key function lies in enabling Task-Adaptive Efficiency-Accuracy Trade-Offs; at the same time, its open Registry supports user customization-simply by specifying a few parameters in a configuration file, new representation-extraction tools can be seamlessly integrated.</p>
<p>Task-Adaptive Extraction of Multi-Granularity Spatial Representations</p>
<p>In certain manipulation tasks (e.g., requiring all toy animals on the table to be laid flat, and in the case of a robotic dog, splay its legs to expose its belly) Fine-Grained spatial representations extracted from the raw camera view alone often proved insufficient.When a task demands precise manipulation of a local region of an object (e.g., the legs of the robotic dog), existing extractors we have tried frequently fail to reliably and accurately capture the necessary Fine-Grained spatial representations(e.g., the orientation of the robotic dog's legs), as evidenced by our ablation studies in Sec.4.2.</p>
<p>We propose a Multi-Granularity Spatial Representation Extraction strategy, a lightweight, attention-inspired local zoom strategy, as illustrated in Fig. 4. By default, we extract coarse-grained spatial representations over the complete RGB frame without any additional preprocessing (see Sec. 3.2.1 for details).If, during the first-stage VLM invocation in the pipeline, the VLM judges that a given spatial representation at stage s requires finer detail (e.g., the leg of the robotic dog), a preprocessing step is invoked prior to spatial representations extraction.First, a target segmentation module (e.g., SAM) isolates the object to localize the relevant object region; this mask is then expanded by a fixed or adaptive padding and used to crop a local sub-image.Next, the adaptive extraction method described in Sec.3.2.1 is applied to this sub-image.Finally, the resulting Fine-Grained representations are Merged Back into the main pipeline to guide subsequent robot action sequencing.</p>
<p>By employing this strategy, we explicitly restrict downstream extraction modules to the correct target regions, thereby yielding higher-quality fine-grained representations.Moreover, this approach is Task-Adaptive, it activates only when necessary.</p>
<p>Chain of Grounding (CoG)</p>
<p>The Task-Adaptive Heterogeneous and Multi-Granularity spatial representation extraction methods discussed in Sec.3.2, while enhancing the system's spatial understanding and preserving operational efficiency, impose an additional reasoning burden on the VLM, forcing it to perform task decomposition, constraint code generation, and representation extraction tool selection all within a single call.As shown by our experiments (see Sec. 4.2), without any explicit guidance the VLM frequently produces incomplete or incorrect constraints or chooses clearly unsuitable extraction tools, leading to task failures or unacceptably long processing times.To address this, we introduce and apply the Chain of Grounding (CoG) method to the VLM at the front of the framework's execution pipeline, explicitly guiding the VLM's reasoning to improve stability.</p>
<p>Our key insight is that grounding a natural-language instruction and scene observation in our framework naturally decomposes into four sequential, dependent phases: operational hint inference, constraint inference, tool selection, and constraint code generation.Accordingly, we designed the Chain of Grounding (CoG) to explicitly navigate the VLM through these stages.First, CoG decomposes the task into multiple stages and generates a diverse set of concise, representation-agnostic operational hints.These hints are brief reminders of critical considerations during task execution, aimed at ensuring the successful completion of the task.Next, for each tip, CoG infers the Spatial Constraints required at that stage in natural-language form, grounded in the scene's objects and their spatial representations.It then queries the Toolkit Registry to select the optimal extraction tool for every Task-Relevant object in each stage, balancing representational capacity against extraction efficiency.Finally, CoG converts the natural-language constraints into executable Python functions, each consuming the corresponding spatial representations and returning a Scalar Cost that quantifies constraint satisfaction.Collectively, CoG outputs all per-stage Python constraint functions and the chosen spatial representation extraction tools for every task-relevant object.</p>
<p>Briefly, CoG is defined by the mapping
{({T s,o } o∈Os , F s )} S s=1 = G (I, X)
, which grounds a natural-language instruction I and scene observation X into per-stage tool selections T s,o for each task-relevant object and a series of executable constraint functions F s in every stage.We only provide a high-level formalization of the Chain of Grounding (CoG) in main text, detailed formalization of the decomposition, constraint generation, tool selection, and code generation are provided in Appendix A.4.</p>
<p>Experiments</p>
<p>In this section, we seek to answer the following research questions: (1) How effective and efficient is our system on open-vocabulary manipulation tasks across diverse real-world scenarios?(Sec.4.1);</p>
<p>(2) How does each components contribute to the overall system performance?(Sec.To validate T-Rex, we constructed a real-world desktop platform: an Intel RealSense D435i RGB-D camera was calibrated and mounted at an optimal overhead position for scene perception, and a UR5e 6-DoF arm with a 1-DoF gripper executed the manipulation tasks.Further hardware and experimental setup details are provided in Appendix A.1.For the Vision-Language Model, we employ GPT-4.1 for inference.In Appendix A.3, we analyze the impact of different VLM choices on the system's overall performance.setup 1 duan</p>
<p>Real-World and Open-Vocabulary Manipulation</p>
<p>We designed and selected 15 real-world, open-vocabulary manipulation tasks ranging from simple placement operations to challenging scenarios involving complex spatial constraints.AppendixA.1 contains extended details on task definitions, baselines, etc.For each task, we conducted 10 independent trials with the poses of task-relevant objects randomized in every trial, and randomized both the location and orientation for every object relevant to the task.The evaluation metrics include task execution success rate and task execution time.We compared our method against VoxPoser and ReKep as baselines.</p>
<p>Tab. 1 presents the detailed quantitative results.Our method demonstrates robust zero-shot generalization and comprehensive spatial understanding, achieving significant performance gains over the baseline approaches.We observe that, compared to methods employing a fixed pipeline to extract a single spatial representation, our Task-Adaptive paradigm-which selects the most appropriate representation extraction scheme for each object at each stage-is far more flexible.This design endows our system with both excellent spatial understanding (e.g., correctly orienting plush toys upright in the same direction as shown in Fig. 5) and comparatively favorable runtime efficiency, consistent  with the analysis in Sec.3.2.Furthermore, our approach yields a higher average success rate on multi-stage tasks.This improvement can be attributed to the explicit guidance provided by CoG, which effectively mitigates VLM hallucinations, fully leverages the VLM's reasoning capabilities, and robustly grounds open-vocabulary instructions into comprehensive and correct constraints as well as spatial representations for every object at each stage.The critical importance of CoG within our framework will be examined in detail in the ablation study of Sec.4.2.</p>
<p>Although our experiments clearly demonstrate the advantages of our approach, we must acknowledge several limitations.In Sec. 5, we discuss the key limitations of our method.</p>
<p>Ablation Study</p>
<p>To quantitatively evaluate how our two core modules, Chain of Grounding (CoG) and the Spatial Representation Extraction Toolkit, each contribute to overall system performance, we conducted a set of ablation studies.These ablations reuse the tasks from Sec. 4.1 and evaluate success rate and average completion time as metrics.When the Toolkit is removed, we employ a single fixed extractor; when CoG is removed, the VLM is invoked with only a simple prompt to produce the required outputs.</p>
<p>System Error Breakdown</p>
<p>Due to the framework's interpretability and modular design, we can precisely identify and analyze the sources of system failures, as shown in Fig. 6.With the inclusion of CoG, errors from the planning module (VLM) are effectively negligible, consistent with our experimental evidence and validating the robustness of the instruction-grounding process.While the Spatial Representation Extraction Toolkit is a substantial component, yet it maintains a low error rate relative to its complexity.We must acknowledge that this is due to the stability of the sample tools we selected.</p>
<p>Across all modules, Spatial Representation Tracking introduces the most significant errors.Most existing extraction tools lack continuous tracking capabilities and are not optimized for rapid motion or intermittent occlusions, making consistent representation challenging under these conditions.This gap indicates that robust spatial tracking slightly exceeds the capabilities of current tools.We discuss this in Appendix A.5.4</p>
<p>Furthermore, we observe that employing a more powerful VLM markedly improves performance in the constraint generation stage-this finding aligns with our experimental results and highlights the high inference demands of this stage (see Appendix A.3). Finally, while the low level action sequence generator and other modules can also cause occasional failures, their impact on overall system stability is relatively minor compared to the modules discussed above.</p>
<p>Conclusion</p>
<p>In this work, we present T-Rex, a Task-Adaptive Spatial Representation Extraction Framework that dynamically selects the optimal extraction scheme for each entity, thereby supporting the subsequent generation of spatial constraints and the solving of robotic action sequences.T-Rex achieves both powerful spatial representation capability and efficient extraction, which in turn endows the overall system with comprehensive spatial understanding and high execution efficiency.The Chain of Grounding (CoG) method is applied to guide the VLM through progressive instruction grounding, ensuring stability throughout the reasoning process.Based on an extensible Toolkit Framework, T-Rex flexibly integrates various Large Vision Models and exhibits excellent system scalability.Notably, all without any additional training.Extensive experiments in both simulated and real-world robotic environments demonstrate that our approach offers significant advantages in spatial understanding, efficiency, and system stability.While advantageous, T-Rex also has limitations.In Appendix A.7, we discuss the limitations of T-Rex and outline several directions for future work.As shown in Fig. 7, we deploy a UR5e 6-DoF arm on our desktop platform, outfitted with a steppermotor-driven 1-DoF gripper for object grasping.Robot control from the host PC is implemented via URScript in conjunction with the RTDE interface, supporting a theoretical maximum communication rate of 500 Hz.Because our method requires continuous and smooth end-effector rotation, we use a quaternion (quat) representation rather than traditional Euler angles (rx − ry − rz) to avoid the gimbal-lock singularities and abrupt discontinuities inherent to Euler formulations.Scene perception relies solely on the depth and color streams of a single Intel RealSense D435i RGB-D camera, demonstrating the low sensor-hardware requirements of our approach in practical deployment.Our experiments are conducted on an Intel(R) Core(TM) i7-14700KF CPU and an NVIDIA RTX A6000 GPU.</p>
<p>A Technical Appendices and Supplementary Material</p>
<p>A.1.2 Tasks</p>
<p>In our task design, some tasks are adopted from related works [2][3] [14] while others are newly created to showcase the strengths of our method.Overall, the tasks are drawn from everyday life and are organized with a clear progression from simple to challenging.Fig. 8 provides snapshots of the execution process for some tasks.</p>
<p>Open drawer: The experimental setup consists of a small drawer placed on a table.The drawer's position and orientation are randomized to some extent but kept within the robotic arm's workspace.</p>
<p>The task is divided into two stages: grasp the drawer handle and pull it outward.This requires correctly identifying and gripping the drawer handle, then pulling it in the proper direction.The success criterion is that the drawer is pulled out by at least one-third of its depth.</p>
<p>Open Bottle: The experimental setup includes a glass bottle with a cap standing upright on the table; its position are randomly varied within the robotic arm's workspace.The task is divided into two stages: grasp the cap and lift it straight up to remove it.This requires correctly identifying the cap's three-dimensional position and applying a vertical pulling motion.The success criterion is that the cap is fully removed without any liquid spilling from the bottle.</p>
<p>Sweep Trash: The experimental setup includes crumpled paper randomly scattered within the robot's workspace, plus a broom and a dustpan.The task is divided into two stages: position the broom along the line between the trash and the dustpan, then sweep the paper into the dustpan.This requires accurately locating both the paper and the dustpan, and using the broom to push the paper in the correct direction until it enters the dustpan.The success criterion is that all paper pieces end up inside the dustpan.</p>
<p>Pack Shoes: The experimental setup includes one pair of slippers and an open cardboard box randomly placed within the robot's workspace.The task consists of four stages: grasp each slipper in turn and place it into the box.This requires detecting each slipper's orientation and placing it into the corresponding spot in the box, which is just large enough to hold both slippers.The success criterion is that both slippers are fully inside the box, do not overlap, and have their uppers facing upward.</p>
<p>Pour Water: The experimental setup includes two identical-model plastic cups of different colors randomly placed within the robot's workspace (Cup A contains water; Cup B is empty).The task requires correctly identifying and aligning Cup A's opening toward Cup B and controlling the tilt to pour.The success criterion is that water transfers into Cup B without any residual drops on the table or on Cup A.</p>
<p>Recycle Can: The experimental setup includes an empty beverage can and a two-compartment bin labeled "Recyclable" and "Non-Recyclable", both placed at random within the robot's workspace.The task is divided into two stages: grasp the can and place it into the "Recyclable" compartment.This requires correctly classifying the can as recyclable and locating the bin's compartments.The success criterion is that the can rests inside the "Recyclable" slot without tipping over the bin.</p>
<p>Stack Block: The experimental setup includes 3-6 identical cubes (edge length 5 cm) randomly scattered within the robot's workspace.The task is to pick up each cube from the table in sequence and stack them vertically at a predefined placement point (e.g., on a red reference block).This requires accurately detecting the positions of both the cubes and the stacking location, and planning precise grasp-and-place trajectories to form a stable column.</p>
<p>Fold Coat: The experimental setup includes a coat laid flat on the table.The task requires the robot to repeatedly grasp one end of the coat, pull it toward the opposite end, and lay it down to incrementally fold the coat.The success criterion is that the folded coat occupies less than half of its original tabletop area.</p>
<p>Hammer the Button: The experimental setup includes a hammer and a red push-button randomly placed within the robot's workspace.The task is divided into three stages: grasp the hammer's handle, orient the hammer head toward the button, and strike the button with the hammer head.This requires precise recognition and control of the hammer head's orientation and its relative position to the button.The success criterion is that the hammer head contacts the button in a downward orientation.</p>
<p>Close the Lid of the Laptop: The experimental setup includes a laptop with its screen fully opened placed on the table; its position and orientation are randomized within the robot's workspace.The task is divided into two stages: move the end effector to the upper edge of the screen's back and then smoothly press down along the hinge until the lid is fully closed.This requires correctly identifying the screen's edge and the hinge direction.The success criterion is that the lid sits flush against the keyboard with no gaps.The task requires identifying each toy's front face and bottom, then placing them upright in the box so they all face the same way.The success criterion is that all plush toys stand securely in the box with their fronts aligned.</p>
<p>Setup table -Toys</p>
<p>Setup Table -Mixed:</p>
<p>The experimental setup includes a mix of plastic toys, tools, and plush toys randomly scattered on the table, plus two open-top boxes and a pen holder, all within the robot's workspace.The robot must complete the three "Setup table" subtasks above in sequence.</p>
<p>A.1.3 Baseline Methods</p>
<p>VoxPoser VoxPoser [14] uses Vision-Language Models (VLMs) and Large Language models (LLMs) for reasoning.It infers affordances from natural-language instructions and then maps them into composable 3D value maps, which serve as objective functions for the planner to enable zero-shot trajectory synthesis.</p>
<p>ReKep ReKep [2] (Relational Keypoint Constraints) formulates a manipulation task as a set of Python functions that assign numerical costs to predefined 3D semantic keypoints in the scene, and then uses hierarchical optimization to solve for continuous end-effector actions in SE( 3 We present two two constraint examples for basic tasks to help readers understand the constraint component of our method.Following best practices from related work [2], each constraint is implemented as a Python function that takes as input the spatial representations extracted by the selected tools (e.g., points or 6D poses) and outputs a scalar cost to guide the low-level action sequence generator.The first task-"grasp the red block and place it on the green block"-can be completed using only point-based constraints.The second task-"place the plush cat next to the plush bear while maintaining the same orientation"-requires 6Dpose-based constraints.By analogy, a more complex task may require multiple types of constraints to work together at the same time.Despite employing CoG to steer the VLM's Chain-of-Grounding reasoning as described in Sec.</p>
<p>3.3, we must acknowledge that our system places a heavy inference burden on the VLM, pushing its capabilities to the limit of current state-of-the-art models.To investigate how model choice affects overall system performance, we designed an exploratory experiment using all tasks from the comparative study in Sec.4.1.Evaluation metrics include: (1) the human-evaluated validity of the constraint code and tool selections produced by a single complete CoG inference; and (2) the overall task success rate.</p>
<p>Experimental results are presented in Tab. 3. We observe that lower-tier models achieve overall acceptable performance, demonstrating our method's robustness to model variation, yet more advanced, newer models deliver clearly superior results across both evaluation metrics.This finding conveys two insights.First, our approach will continue to benefit as VLM reasoning capabilities improve in the future.Second, there is a promising research direction in designing system architectures that more effectively structure and guide the VLM's reasoning process so as to lessen dependence on its peak performance.For these reasons, we ultimately selected GPT-4.1 as the VLM for our method: it delivers nearly the best reasoning performance while costing only about twenty percent of o3.</p>
<p>A.4 Details of CoG(Chain of Grounding)</p>
<p>We design Chain of Grounding (CoG) to explicitly guide the VLM's reasoning : starting from the natural-language instruction I and scene observation X, CoG decomposes the task, generates spatial constraints, selects optimal extraction tools, and generate executable constraint functions.First, CoG performs multi-stage decomposition:</p>
<p>{hints s,j } Js j=1 S s=1 = D(I, X) where S is the number of stages produced by the decomposition, and for each stage s, {hints s,j } Js j=1 is the set of J s concise, representation-agnostic natural-language operational hints generated to guide spatial reasoning at stage s.Next, for each stage s it generates a set of natural-language spatial constraints:
C s = {{(c s,j,k , O s,j,k )} Ks,j k=1 } Js j=1 = C({hints s,j } Js j=1 )
where C s is the set of all spatial constraints for stage s, each constraint c s,j,k is a natural-language specification derived from hints s,j , and O s,j,k denotes the set of task-relevant objects involved in constraint c s,j,k ; C is the operator mapping hints to grounded constraint-object pairs.Then, for each object o in stage s, CoG selects the optimal extraction tool from the registry R:
T s,o = S(o, C s , R), o ∈ O s
where O s = j,k O s,j,k is the set of all objects involved in any constraint at stage s, for each o, we prompt the VLM selects the simplest tool T s,o ∈ R whose extracted spatial representation fully satisfies the task requirements for object o.Finally, CoG emits Python-encoded constraint functions for every constraint c ∈ C s :
F s = {E(c, {T s,o } o∈Os ) | c ∈ C s }
where F s is the set of Python functions implementing each constraint c ∈ C s for stage s.For each c ∈ C s , E generates a Python function that takes as input the spatial representations produced by the selected tools {T s,o } o∈Os and returns a scalar cost quantifying how well constraint c is satisfied.In sum, we formalize the entire Chain of Grounding process as
{({T s,o } o∈Os , F s )} S s=1 = G (I, X),
which grounds the natural-language instruction I and scene observation X into per-stage tool selections T s,o for each task-relevant object and into a series of executable constraint functions F s at each stage.14 " input ": [" rgb_image " , " depth_image " , " object_list "] ,</p>
<p>15 " im ple me nta tio n_ sum ma ry ": " Use Grounding DINO to detect bounding boxes , then compute each box ' s center point .",</p>
<p>16</p>
<p>" a v g _ h i s t o r i c a l _ e x e c _ t i m e _ s ":</p>
<p>" tool_name ": " S em a n t i c K e y p o in t E x t r a c t o r " ,</p>
<p>20</p>
<p>" output ": " object_keypoints : semantically informed keypoints on each object in object_list , in the camera coordinate frame " , 29 " input ": [" rgb_image " , " depth_image " , " object_list "] ,</p>
<p>30</p>
<p>" im ple me nta tio n_ sum ma ry ": " Combine Grounding DINO , DINOv2 , SAM , and K -means clustering to extract semantically informed keypoints .", " a v g _ h i s t o r i c a l _ e x e c _ t i m e _ s ": 3.2 area filtering, solidity detection, edge and corner detection, specular highlight isolation, etc.-and to automatically generate locally runnable code that composes multiple basic vision operators (e.g., via OpenCV) to perform the extraction, ultimately outputting the desired spatial representation.We find that the generated code is predominantly based on OpenCV routines.This reasoning-pluscode-generation strategy not only achieves keypoint and vector extraction quality comparable to or exceeding specialized vision models, but also fully leverages the VLM's affordance reasoning priors.</p>
<p>In terms of unconventional representations, we introduce two classes of high-level spatial abstractions beyond traditional constraints: first, object topology graphs that capture stacking, dependency, or occlusion relationships among multiple items in the scene; and second, state-machine representations of task-relevant objects, which reflect each object's state and its transitions across different task steps.For example, in drawer-manipulation tasks we prompt the VLM to generate the drawer's open/closed state machine to support subsequent, state-dependent decisions; whereas in fragile-stack-handling tasks we extract a topological ordering of the objects to guide careful handling.</p>
<p>Open</p>
<p>Close push pull 6D pose, defined as an object's position and orientation in three-dimensional space, is a critical spatial representation for robotic manipulation tasks and is indispensable in a wide range of applications.</p>
<p>Although numerous 6D pose estimation methods have emerged in recent years, challenges remain in achieving real-time performance, broad generality, and ease of deployment.</p>
<p>Typical inputs to a 6D pose estimation tool are RGB-D data and, optionally, a CAD model of the object; the output is usually an estimate of the object's pose in SE (3).Methods can be broadly categorized by their dependence on known object models: (1) model-based approaches [60,59,61], which require a pre-provided mesh model (e.g., PLY or STL format); and (2) model-free approaches [62,63,64], which may still rely on other forms of prior knowledge-such as an object point cloud-rather than a full mesh.</p>
<p>In our toolkit, we include SAM6D [60] as a representative model-based extractor, which requires a 3D model of each target object.To generate these models, we first reconstruct a textured 3D mesh (.glb) from a single RGB image using SPAR3D [73], then convert the mesh into the PLY format accepted by SAM6D via lightweight web tools such as ImageToStl.This pipeline provides the high-quality object models needed for accurate 6D pose estimation.</p>
<p>While single-image 3D reconstruction (e.g., with SRAR3D) is a promising research direction, it currently suffers from quality issues such as incomplete backside reconstruction.Exploring more robust real-time reconstruction tools represents a viable avenue for future work.We also acknowledge that the requirement for object priors (e.g., full mesh models or point clouds) limits the system's ability to handle previously unseen objects.As more reliable, real-time, prior-free 6D pose estimation methods emerge, our modular T-Rex framework will seamlessly integrate them to further enhance performance.Some modules from other works can also be seamlessly integrated into our system as spatialrepresentation extractors [4,14,3,15].These modules provide additional perception capabilities to enhance overall performance.</p>
<p>Although the computer-vision community has produced a wealth of techniques for various vision tasks, they often face significant challenges when directly applied to robotic manipulation.On one hand, robots demand vision modules that are real-time, robust, and lightweight, whereas many vision algorithms rely on complex priors or multi-stage pipelines that struggle to meet such requirements.On the other hand, building and maintaining these priors (e.g., object 3D meshes) requires substantial engineering effort.We believe that adapting state-of-the-art computer-vision advances for robotic manipulation is a highly valuable research direction, and we encourage readers to explore more efficient and scalable solutions in this space.</p>
<p>Beyond visual information, tactile feedback can directly sense contact forces [74,75], which is critical for fine-grained grasping tasks.Such sensory data itself constitutes a form of representation that can be fed in parallel with visual representation into the T-Rex framework, allowing the construction of constraints and generation of robot action sequences to provide the system with richer perceptual modalities.</p>
<p>A.5.4 Details of Representation Tracking</p>
<p>For some fundamental computer vision functions, such as object detection [51,50,49], current methods already support robust video-streamed detection and tracking with good temporal consistency, making them well suited for robotic manipulation tasks.</p>
<p>However, for more advanced spatial representations (e.g., vectors or 6D poses), existing vision techniques are not yet as mature.Robotic manipulation often requires approximate real-time, videostream-level tracking, ensuring that the interval between successive extractions is below the task's acceptable tracking latency and that results remain consistent across frames.</p>
<p>Some works [76,77,78,79] investigate using a separate tracking module to follow the outputs of a preceding extraction stage.Though promising, such approaches can become bottlenecks in practical robotic systems in terms of inference latency, computational overhead, memory footprint, and overall system complexity.</p>
<p>To approximate continuous tracking with our existing tools, we provide a high-frequency invocation interface in our framework.This allows re-extracting spatial representations on key frames or at fixed intervals-e.g., repeatedly calling 'CenterPointExtractor' or 'SemanticKeypointExtractor' during execution.This strategy works well for simple tasks with low tracking demands (e.g., object classification or coarse position correction), but for complex tasks requiring fine-grained, high-frequency tracking, the average execution time per tool call invariably exceeds the ideal frame interval.Consequently, tracking precision and real-time performance suffer, and spatial representation tracking can become a system bottleneck-consistent with observations in related work.</p>
<p>A.6 Details of Low-Level Action Sequence Generation</p>
<p>Low-level action sequence generation is the final stage of our pipeline.Its primary role is to convert the spatial representations and constraint functions produced by CoG and the representation extraction modules into concrete action sequences that the robot can execute.This module takes, as input, the Python constraint functions corresponding to each stage's natural-language constraints and the associated object spatial representations, and outputs trajectories in SE(3) along with grasp/release commands, which together drive the robot to complete the manipulation task.</p>
<p>In robotic manipulation tasks, representations serve as the crucial bridge between real-world scenes and the reasoning capabilities of Vision-Language Models (VLMs), acting as the primary handle by which VLMs perform grounded inference.Although a comprehensive taxonomy of representations in robotic manipulation has yet to emerge, we broadly categorize scene representations into two types: (1) conventional representations: those with group structure, such as keypoints, directional vectors, and 6D poses (SE(3)); and (2) non-conventional representations: discrete attributes that lack group structure, such as state representations (e.g., whether a drawer is open or closed) and topological orderings (e.g., the stacking order of multiple objects)."Non-conventional" here simply denotes representations that have received relatively little attention in the field so far, not that they are unimportant; for more complex tasks, these representations can be critical.They both offer additional ways to parse scene information and point toward promising directions for future research.Further discussion of non-conventional representations can be found in the appendix.</p>
<p>Our low-level action sequence generation module operates as follows: first, if all constraints in this stage are based on conventional geometric representations (e.g., points, vectors, or 6D poses) and these constraints can be efficiently solved via continuous optimization, the system simply invokes the solver, which uses the full set of constraint functions and tracked representations to produce the action sequence.Only when a stage includes constraints built on non-conventional representations, such as those requiring object state judgments or relying on topological relations, does the code generation mechanism trigger: the system first generates a complete, executable script for that stage, integrating the previously defined constraint functions and low-level control interfaces into a coherent execution flow.Within that script, sub-parts corresponding to constraints based on conventional representations defer to the solver to compute the action sequence.This design offers several advantages: (1) adaptability: most scenarios are handled purely by the solver, with no extra code to write or maintain; and (2) layered structure: the solver focuses on continuous motion refinement, while code generation handles decision-making and logical control flow.</p>
<p>We acknowledge that this small portion of our pipeline-the solver-based action sequence generationdraws significant design inspiration from the ReKep [2], and thus we do not claim it as our contribution.We gratefully recognize ReKep's outstanding impact on the field of robotic manipulation.For further details on the solver-based action sequence generation component, we encourage readers to consult ReKep's publications.</p>
<p>We further acknowledge that this constraint-adaptive low-level strategy generation is not necessarily the ultimate solution.In principle, one could devise a more powerful low-level planner that uniformly handles constraints derived from all representation types as inputs.However, since this lies beyond our core contribution (task-adaptive representations), and our current approach already performs well enough to demonstrate our key insights, we do not explore it here-though we recognize it as a valuable avenue for future improvement.We encourage readers to investigate such unified low-level planning methods, which we believe hold great promise.</p>
<p>A.7 Extended Discussion of Limitations and Future Works</p>
<p>While advantageous, T-Rex also has limitations.First, its performance is to some extent constrained by the current state of research on Large Vision Models; as an illustration, it remains challenging to maintain precise spatial representation tracking under conditions of intermittent occlusion.Second, our approach considers only end-effector trajectories, whereas whole-arm planning is clearly a better design choice, as it can more effectively handle collision avoidance and related issues.Third, as a demonstration, our Toolkit currently includes only a limited number of selectively chosen representation extraction tools for spatial representation extraction.Under these conditions, directly providing the Toolkit Registry to the VLM for tool selection is feasible.However, as the Toolkit is scalable and the number of available tools increases significantly, the CoG and context learning-based tool selection approach may face challenges in terms of inference efficiency and accuracy.</p>
<p>The rapid development of Large Vision Models and Vision-Language Models opens several exciting avenues for future work.For example, by collecting and constructing large-scale datasets and finetuning existing VLMs, it may be possible to develop Embodied Reasoning Models with enhanced capabilities in spatial reasoning and affordance inference.Although data collection and training processes are resource-intensive, they represent the most direct path toward endowing VLMs with embodied capabilities.In addition, as advancements in computer vision research continue, it is anticipated that increasingly powerful Large Vision Models will emerge, enabling the extraction of a broader range and higher quality spatial representations.</p>
<p>A.8 Connections and Distinctions between T-Rex and existing works</p>
<p>Vision-Language Models for Robotics.The Key Difference between our method and existing approaches is that existing approaches rely on Fixed Types and Granularities of spatial representations, making it difficult to simultaneously meet diverse task requirements and maintain computational efficiency.Our approach introduces Task-Adaptive spatial representation Types and Granularities, and achieves a favorable trade-off between spatial understanding and overall system efficiency.</p>
<p>Grounding Language Instructions.Since VLMs are typically trained on dialogue tasks, there is a gap in their reasoning abilities when applied to robotic tasks.Without additional guidance, their reasoning in embodied tasks sometimes misses or deviates from the correct path.This issue becomes particularly evident in tasks that require long-chain reasoning, especially after our method introduces adaptive spatial representations extends the reasoning chain of VLMs.To address this, we propose CoG (Chain of Grounding) to guide VLMs step by step in mapping instructions, ultimately obtaining comprehensive and correct spatial constraints for task planning at each stage, and selecting the most appropriate spatial representations for each entity at each stage to complete the task.</p>
<p>Vision Foundation Models and Spatial Representation for Robotics.Compared to existing methods that rely on a fixed extraction pipeline to produce a single type of spatial representation, our approach adaptively selects the most appropriate representation types and extraction methods based on specific task requirements, effectively balancing representational power and runtime efficiency.</p>
<p>Figure 1 :
1
Figure1: T-Rex adaptively invokes the optimal extractor from the Spatial Representation Extraction Toolkit to obtain spatial representations for each task-relevant object.In table organization task it extracts point for blocks, vector for tools and the pen holder, and 6D pose for plush toys, while defining positional constraints on blocks, orientation constraints on pen and pen holder, and pose constraints on plush toys.</p>
<p>Figure 3 :
3
Figure 3: Spatial Representation Extraction Toolkit.System adaptively invokes the optimal extractor for each task to extract the required representation.</p>
<ol>
<li>2
2
Task-Adaptive Spatial Representation Extraction Our Task-Adaptive Spatial Rpresentation Extraction Framework is composed of two core modules: the task-adaptive extraction of Heterogeneous spatial representations (Sec.3.2.1)and the task-adaptive extraction of Multi-Granularity spatial representations (Sec.3.2.2).</li>
</ol>
<p>Lay all Toy Animals Flat, and Splay the Robotic Dog's Legs to Expose its Belly.</p>
<p>Figure 4 :
4
Figure 4: Multi-Granularity spatial representation extraction.First crop a local subgraph, then extract representations from this subgraph.</p>
<p>4.2); (3) To what extent do individual system components contribute to failures, and what are the primary sources and failure modes?(Sec.4.3).</p>
<p>Figure 5 :
5
Figure 5: Because T-Rex Toolkit includes 6D pose estimation module, it outperforms the keypointonly baseline ReKep on tasks that demand object pose estimation.</p>
<p>Figure 7 :
7
Figure 7: Single-Arm Platform.</p>
<p>Figure 8 : 2 " 8 " 2 " 7 " 8 offset 19 "Listing 2 :
828278192
Figure 8: Execution Snapshots for Sample Tasks</p>
<p>A. 5 1 { 2 " 5 " 6 "
51256
Details of ToolKit A.5.1 Samples of the Toolkit we provide output ": " visual_center_points : coordinates of each object ' s visual center in the camera frame " ,</p>
<p>21 "
21</p>
<p>Figure 9 :
9
Figure 9: An example of a State Machine.Modeling the drawer's states to support the generation of state-dependent constraints.</p>
<p>Figure 10 :
10
Figure 10: Example experimental props and the 3D meshes we reconstructed</p>
<p>Comprehensive and Efficient spatial representation extraction: Due to the Task-Adaptive spatial representation extraction scheme, our approach simultaneously achieves powerful spatial understanding and high operational efficiency; (2) Extensibility: New spatial representation extraction tools can be easily integrated into the system; (3) Stability: The system can operate robustly and stably, including the adaptive extraction module described in (1); (4) Plug-and-play: It can be rapidly deployed without requiring Any training.
The primary contributions of our work include: (1) Proposing a Task-Adaptive spatial representationextraction method covering adaptive extraction of Heterogeneous and Multi-Granularity spatialrepresentations; (2) Designing an extensible and open Spatial Representation Extraction Toolkitframework with practical examples provided; (3) Proposing CoG to explicitly guide VLM in step-by-step instruction grounding.</p>
<p>Table 1 :
1
Quantitative results in real-world experiments.T-Rex achieves high success rates on everyday manipulation tasks, demonstrating superior spatial understanding and overall execution efficiency, and significantly surpassing the baseline VoxPoser and ReKep.
TaskVoxPoserReKep(Auto)T-Rex (Ours)Success Time(s) Success Time(s) Success Time(s)Open Drawer4/1017.52/1018.16/1014.3Open Bottle7/1013.37/1019.88/1013.5Sweep Trash9/1011.48/1011.89/1012.9Pack Shoes0/1037.42/1044.65/1038.8Pour Water0/1025.73/1027.57/1024.1Recycle Can6/1019.37/1021.99/1014.2Fold Coat0/1040.94/1047.74/1038.8Stack Block6/1090.76/10101.86/1074.5Hammer the Button0/1027.11/1027.44/1029.4Close Lid of Laptop4/1019.82/1023.17/1021.6Setup table: Toys Place5/1068.45/1070.47/1054.6Setup table: Tools Insert0/1052.93/1057.67/1056.3Setup table: Plush Toys Place1/1031.51/1033.34/1026.9Setup table: Mixed0/10254.90/10240.92/10217.5Total30%50.836.4%53.360.7%45.5</p>
<p>Table 2 :
2
Ablation study results.SP denotes the simplest point extractor (Grounding DINO + center point); VPV denotes the VLM-based point and vector extractor.Results are reported over 3 runs different seeds.CoG, w/o Toolkit (SP) 27.9% ±1.5% 30.0 ±0.9 w/o CoG, w/o Toolkit (VPV) 51.4% ±1.4% 44.3 ±4.4The ablation results are summarized in Tab. 2. We first observe that integrating CoG yields a clear improvement in success rate with insignificant additional latency.Although the improvement is modest, CoG consistently delivers a stable uplift, as a simple yet effective prompt-engineering mechanism, validates our design and understanding of the instruction grounding process (see Sec. 3.3).Conversely, when the Toolkit is removed and a fixed spatial representation extractor is used, the system must trade off success rate against overall execution efficiency, which is consistent with our analysis in Sec.3.2.
13.2%9.9%8.3%Configuration Ours w/o CoG w/o Toolkit (SP) w/o Toolkit (VPV) w/o Others Success(%) Time (s) 60.7% ±2.1% 45.5 ±1.3 52.1% ±2.4% 41.4 ±2.1 30.7% ±3.7% 33.6 ±3.6 55.0% ±2.9% 47.9 ±3.2 Action Sequence Generator VLM as Planner VLM as Constraint Code Generator Spatial Representation Extractor Spatial Representation Tracker 7.1% 19.2% 42.3% Figure 6: System error breakdown.zi too small</p>
<p>Setup Table -Tools Insert: Put All the Tools in the Pen Container with Handles Facing Up.The experimental setup includes multiple hand tools randomly scattered on the table and a pen holder beside them, all within the robot's workspace.The task requires recognizing each tool's handle end and placing the tool into the holder so that every handle faces upward.The success criterion is that all tools rest in the holder, handles oriented up, without tipping it over.Setup Table -Plush Toys Place:Place All Plush Toys into the Box Upright and Facing the Same Direction.The experimental setup includes plush toys randomly scattered on the table and an opentop storage box, all within the robot's workspace.</p>
<p>Place: Quickly Place All Toys in Box.The experimental setup includes various toys randomly scattered on the table and an open-top box, all within the robot's workspace.The success criterion is that all toys are transferred into the box within 60 seconds.</p>
<p>Table 3 :
3
Exploratory study of VLM choice effects on system performance
ModelInference Validity Rate (%) Total Success Rate (%)o391.461.4GPT-4.190.060.7GPT-4o mini85.757.1gpt-4-vision-preview83.855.0A.3 Comparison of VLMs
" a v g _ h i s t o r i c a l _ e x e c _ t i m e _ s ": 2.3
green = np .array ([ points[1][ " x " ] , points[1][ " y " ] , points[1][ " z " ]]) 44 " input ": [" rgb_image " , " depth_image " , " object_list "] ,} , { 49 " tool_name ": " SE3PoseEstimator " , 50 " output ": "6 d_poses of object " ,51" output_format ": { " type ": " float " , " shape ": [4 , 4] } ,52" input ": [" rgb_image " , " depth_image " , " predefined_object_id "] ,53" im ple me nta tio n_ sum ma ry ": " Apply SAM6D to estimate the SE (3) 6 D pose for a specified object .",54" a v g _ h i s t o r i c a l _ e x e c _ t i m e _ s ": 5.2 " output ": " a sub_image around the object " ,59" output_format ": { " type ": " RGB_image " , " shape ": [" H " , " W " , 3] } " start_point ": { " x ": " float " , " y ": " float " , " z ": " float " } ,85" end_point ": { " x ": " float " , " y ": " float " , " z ": " float " }  In this appendix, we briefly introduce the various tools in our Toolkit for spatial representation extraction: CenterPointExtractor computes and outputs each object's center point coordinates in the camera frame, based on bounding boxes detected by Grounding DINO[52]; SemanticKeypointExtractor combines Grounding DINO, DINOv2, SAM, and K-means clustering to extract semantically meaningful keypoints on each object in the provided list[52,71,55], returning their 3D coordinates in the camera frame; UniformSceneKeypoints uses a SAM-based SoM[72]framework to uniformly sample keypoints over the surfaces of most objects in the scene, outputting their 3D locations; SE3PoseEstimator applies SAM6D[60]to estimate a specified object's full 6D pose, outputting the corresponding SE(3) transformation matrix (4×4); LocalSubImageExtractor first uses Grounding DINO to detect an object's bounding box, expands it by a fixed padding, and crops that sub-region from the original image for downstream tools; VLMTaskPointExtractor prompts the VLM to generate code invoking multiple basic vision tools, and-given an image and an instruction-outputs a list of task-relevant 3D points; VLMTaskVectorExtractor similarly leverages the VLM to produce extraction code, outputting ordered point-pair vectors that represent object directions or motion paths; VLMTaskTopoSorter invokes the VLM to directly infer and return a topology-based ordering of task-relevant objects, useful for staged or occluded scenarios; and VLMTaskStateMachineExtractor has the VLM infer a state machine for each task-relevant object (e.g., open vs. closed drawer), supporting operations that depend on object state.We observe an intriguing direction: leveraging Vision-Language Models (VLMs) as fundamental spatial representation extractors.This idea stems from our recognition that certain basic spatial representations-such as keypoints and vectors-are critical for robotic manipulation, yet existing open-source tools often fall short in one or more aspects (e.g., extraction speed, instruction understanding, ease of deployment).To address this, we explore using VLMs with their rich world knowledge and reasoning capabilities to extract these representations.Our approach is to prompt a VLM, given a natural-language instruction, to infer all low-level vision conditions necessary for extracting a given representation-such as hue segmentation, saturation and brightness thresholds, shape and A.9 Further formal discussion Our framework adopts the classical pipeline of representation extraction, constraint generation and low-level solving.First, under the guidance of the Chain of Grounding, the VLM infers the spatial representation extraction tools seletions {T * s,o } and the set of executable Python constraint functions {F s } for each stage s, formalized asNext, using the selected tools {T * s,o }, the Spatial Representation Extraction Toolkit R extracts per-stage, per-object representations {r s,o } viaFinally, the low-level action sequence generator Π consumes the constraint functions {F s } and the extracted representations {r s,o } to produce the robot action sequence:In this way, the resulting trajectory set {τ i } transforms the scene X into one that satisfies the instruction ℓ.The system's core components such as the Spatial Representation Extraction Toolkit and the Chain of Grounding are presented comprehensively in Sec.3.2 and 3.3.Other components of the system are detailed in the appendix such as the Low-Level Action Sequence Generator(see A.6) and Spatial Representation Tracking (see A.5.4).A.10 Broader ImpactsT-Rex can be used in manufacturing to improve the accuracy and flexibility of automated production and reduce repetitive labor.T-Rex enhances robot's spatial reasoning capabilities, but it could be repurposed for negative applications such as advanced autonomous weapon development or pose safety risks if VLM inferences are flawed.To mitigate these hazards, we recommend incorporating human-in-the-loop verification for high-risk tasks and establishing deployment monitoring and feedback mechanisms to ensure responsible use.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. 2024arXiv preprint</p>
<p>Copa: General robotic manipulation through spatial constraints of parts with foundation models. Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, Hao Dong, arXiv:2501.038412025arXiv preprint</p>
<p>Weiliang Tang, Jia-Hui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li, Chi-Wing Fu, Mingyu Ding, arXiv:2501.09783Geomanip: Geometric constraints as general interfaces for robot manipulation. 2025arXiv preprint</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, arXiv:2311.178422023arXiv preprint</p>
<p>Zhutian Yang, Caelan Garrett, Dieter Fox, Tomás Lozano-Pérez, Leslie Pack, Kaelbling , arXiv:2410.02193Guiding long-horizon task and motion planning with vision language models. 2024arXiv preprint</p>
<p>Nishanth Kumar, Fabio Ramos, Dieter Fox, Caelan Reed Garrett, arXiv:2411.08253Open-world task and motion planning via vision-language model inferred constraints. 2024arXiv preprint</p>
<p>Physically grounded vision-language models for robotic manipulation. Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Vlm see, robot do: Human demo video to robot action plan via vision language model. Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, Chen Feng, arXiv:2410.087922024arXiv preprint</p>
<p>Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, arXiv:2502.13143Language-grounded orientation bridges spatial reasoning and object manipulation. 2025arXiv preprint</p>
<p>Vlmpc: Vision-language model predictive control for robotic manipulation. Wentao Zhao, Jiaming Chen, Ziyu Meng, Donghui Mao, Ran Song, Wei Zhang, arXiv:2407.098292024arXiv preprint</p>
<p>Kuda: Keypoints to unify dynamics learning and visual prompting for open-vocabulary robotic manipulation. Zixian Liu, Mingtong Zhang, Yunzhu Li, arXiv:2503.105462025arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Moka: Open-world robotic manipulation through mark-based visual prompting. Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine, arXiv:2403.031742024arXiv preprint</p>
<p>A real-to-sim-to-real approach to robotic manipulation with vlm-generated iterative keypoint rewards. Shivansh Patel, Xinchen Yin, Wenlong Huang, Shubham Garg, Hooshang Nayyeri, Li Fei-Fei, Svetlana Lazebnik, Yunzhu Li, arXiv:2502.086432025arXiv preprint</p>
<p>Learning to interpret natural language commands through human-robot dialog. Jesse Thomason, Shiqi Zhang, Raymond J Mooney, Peter Stone, IJCAI. 201515</p>
<p>Grounding verbs of motion in natural language commands to robots. Thomas Kollar, Stefanie Tellex, Deb Roy, Nicholas Roy, Experimental robotics: The 12th international symposium on experimental robotics. Springer2014</p>
<p>Toward understanding natural language directions. Thomas Kollar, Stefanie Tellex, Deb Roy, Nicholas Roy, 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE2010. 2010</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller, Nicholas Roy, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201125</p>
<p>Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, arXiv:2409.125142024arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu, arXiv:2410.078642024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-languageaction models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, arXiv:2311.019772023arXiv preprint</p>
<p>Aloha 2: An enhanced low-cost hardware for bimanual teleoperation. Jorge Aldaco, Travis Armstrong, Robert Baruch, Jeff Bingham, Sanky Chan, Kenneth Draper, Debidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer Goodrich, arXiv:2405.022922024arXiv preprint</p>
<p>Zipeng Fu, Tony Z Zhao, Chelsea Finn, arXiv:2401.02117Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. 2024arXiv preprint</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, arXiv:2411.196502024arXiv preprint</p>
<p>Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe Xu, arXiv:2403.03954Generalizable visuomotor policy learning via simple 3d representations. 2024arXiv preprint3d diffusion policy</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, arXiv:2410.061582024arXiv preprint</p>
<p>Roboflamingo-plus: Fusion of depth and rgb perception with vision-language models for enhanced robotic manipulation. Sheng Wang, arXiv:2503.195102025arXiv preprint</p>
<p>Vision-language foundation models as effective robot imitators. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, arXiv:2311.013782023arXiv preprint</p>
<p>Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, arXiv:2503.106312025arXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Finn, arXiv:2503.220202025arXiv preprint</p>
<p>Pointvla: Injecting the 3d world into vision-language-action models. Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu, arXiv:2503.075112025arXiv preprint</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, arXiv:2503.147342025arXiv preprint</p>
<p>Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, arXiv:2502.095602025arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Chatgpt for robotics: Design principles and model abilities. H Sai, Rogerio Vemprala, Arthur Bonatti, Ashish Bucker, Kapoor, 2024Ieee Access</p>
<p>Toward general-purpose robots via foundation models: A survey and meta-analysis. Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Hao-Shu Fang, arXiv:2312.087822023arXiv preprint</p>
<p>Foundation models defining a new era in vision: a survey and outlook. Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Fahad Shahbaz Khan, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2025</p>
<p>Yolov10: Real-time end-to-end object detection. Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, Advances in Neural Information Processing Systems. 202437</p>
<p>Yolov12: Attention-centric real-time object detectors. Yunjie Tian, Qixiang Ye, David Doermann, arXiv:2502.125242025arXiv preprint</p>
<p>Yoloe: Real-time seeing anything. Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding, arXiv:2503.074652025arXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, European Conference on Computer Vision. Springer2024</p>
<p>Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, arXiv:2408.00714Segment anything in images and videos. 20242arXiv preprint</p>
<p>. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, Jinqiao Wang, arXiv:2306.121562023Fast segment anything. arXiv preprint</p>
<p>Segment everything everywhere all at once. Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, Yong Jae Lee, Advances in neural information processing systems. 19769-19782, 202336</p>
<p>kpam: Keypoint affordances for category-level robotic manipulation. Lucas Manuelli, Wei Gao, Peter Florence, Russ Tedrake, The International Symposium of Robotics Research. Springer2019</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Pivot: Iterative visual prompting elicits actionable knowledge for vlms. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, arXiv:2402.078722024arXiv preprint</p>
<p>Foundationpose: Unified 6d pose estimation and tracking of novel objects. Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Sam-6d: Segment anything model meets zero-shot 6d object pose estimation. Jiehong Lin, Lihua Liu, Dekun Lu, Kui Jia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Omni6dpose: A benchmark and model for universal 6d object pose estimation and tracking. Jiyao Zhang, Weiyao Huang, Bo Peng, Mingdong Wu, Fei Hu, Zijian Chen, Bo Zhao, Hao Dong, European Conference on Computer Vision. Springer2024</p>
<p>Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images. Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, Wenping Wang, European Conference on Computer Vision. Springer2022</p>
<p>Onepose: One-shot object pose estimation without cad models. Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, Xiaowei Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Onepose++: Keypoint-free one-shot object pose estimation without cad models. Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, Xiaowei Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Gs-pose: Generalizable segmentation-based 6d object pose estimation with 3d gaussian splatting. Dingding Cai, Janne Heikkilä, Esa Rahtu, arXiv:2403.106832024arXiv preprint</p>
<p>You only demonstrate once: Category-level manipulation from single visual demonstration. Wenzhao Bowen Wen, Kostas Lian, Stefan Bekris, Schaal, arXiv:2201.127162022arXiv preprint</p>
<p>Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding. Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang, arXiv:2412.043832024arXiv preprint</p>
<p>Vlmgrounder: A vlm agent for zero-shot 3d visual grounding. Runsen Xu, Zhiwei Huang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin, arXiv:2410.138602024arXiv preprint</p>
<p>Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu, arXiv:2502.10392Textguided sparse voxel pruning for efficient 3d visual grounding. 20253arXiv preprint</p>
<p>Scanreason: Empowering 3d visual grounding with reasoning capabilities. Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, Xihui Liu, European Conference on Computer Vision. Springer2024</p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao, arXiv:2310.114412023arXiv preprint</p>
<p>Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M Rehg, Varun Jampani, arXiv:2501.04689Spar3d: Stable point-aware reconstruction of 3d objects from single images. 2025arXiv preprint</p>
<p>9dtact: A compact vision-based tactile sensor for accurate 3d shape reconstruction and generalizable 6d force estimation. Changyi Lin, Han Zhang, Jikai Xu, Lei Wu, Huazhe Xu, IEEE Robotics and Automation Letters. 922023</p>
<p>Tac3d: A novel vision-based tactile sensor for measuring forces distribution and estimating friction coefficient distribution. Lunwei Zhang, Yue Wang, Yao Jiang, arXiv:2202.062112022arXiv preprint</p>
<p>Pointodyssey: A large-scale synthetic dataset for long-term point tracking. Yang Zheng, Adam W Harley, Bokui Shen, Gordon Wetzstein, Leonidas J Guibas, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision19855-19865, 2023</p>
<p>Cotracker: It is better to track together. Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, Christian Rupprecht, European Conference on Computer Vision. Springer2024</p>
<p>Robotap: Tracking arbitrary points for few-shot visual imitation. Mel Vecerik, Carl Doersch, Yi Yang, Todor Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, Jon Scholz, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Spatialtracker: Tracking any 2d pixels in 3d space. Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>            </div>
        </div>

    </div>
</body>
</html>
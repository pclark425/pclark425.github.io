<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4347 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4347</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4347</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-279070437</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.24302v2.pdf" target="_blank">ScienceMeter: Tracking Scientific Knowledge Updates in Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly used to support scientific research, but their knowledge of scientific advancements can quickly become outdated. We introduce ScienceMeter, a new framework for evaluating scientific knowledge update methods over scientific knowledge spanning the past, present, and future. ScienceMeter defines three metrics: knowledge preservation, the extent to which models'understanding of previously learned papers are preserved; knowledge acquisition, how well scientific claims from newly introduced papers are acquired; and knowledge projection, the ability of the updated model to anticipate or generalize to related scientific claims that may emerge in the future. Using ScienceMeter, we examine the scientific knowledge of LLMs on claim judgment and generation tasks across a curated dataset of 15,444 scientific papers and 30,888 scientific claims from ten domains including medicine, biology, materials science, and computer science. We evaluate five representative knowledge update approaches including training- and inference-time methods. With extensive experiments, we find that the best-performing knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of future knowledge. Inference-based methods work for larger models, whereas smaller models require training to achieve comparable performance. Cross-domain analysis reveals that performance on these objectives is correlated. Even when applying on specialized scientific LLMs, existing knowledge update methods fail to achieve these objectives collectively, underscoring that developing robust scientific knowledge update mechanisms is both crucial and challenging.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4347.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4347.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SyntheticClaimGen-GPT4O</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic Claim Generation using GPT-4O</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that prompts GPT-4O to extract atomic SUPPORT and REFUTE scientific claims from paper abstracts (constrained to ~15 words) to create synthetic datasets for evaluation; claims were expert-validated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Synthetic Claim Generation (GPT-4O)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each paper the authors prompt GPT-4O with templates asking for a concise atomic claim uniquely supported by the paper (SUPPORT) and a relevant but non-supported claim (REFUTE). Claim length is constrained (~15 words). Generated claims are then validated: two domain experts per domain judged support/coverage and a separate author-annotated subset was collected. These synthetic claims are used in downstream claim-judgment and claim-generation evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4O</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (10 domains: medicine, biology, materials science, computer science, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>15,444 papers (claims generated for each paper)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not specific to numeric laws — atomic scientific claims (qualitative/verifiable findings); used to evaluate claim-level knowledge rather than extracting numeric equations</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>short natural-language atomic claims (approx. 15 words); SUPPORT / REFUTE labeled claim tuples</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Expert evaluation by PhD students and a randomized author-annotation study; human adjudication of SUPPORT/REFUTE labels</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Expert validation: ≥80% of synthetic claims strictly adhered to rules; >95% broadly met expectations (paper reports these validation fractions)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared synthetic-claim derived evaluation to author-annotated claims (284 claims): no statistically significant difference in downstream model performance</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Claims are not numeric or symbolic laws; may omit multi-source convergent advances; synthetic generation quality imperfect (room for improvement) and depends on LLM prompting; may not capture quantitative relationships</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4347.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INFER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inference-time knowledge update (INFER / prompt-context augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time update strategy that injects new paper text (e.g., abstracts) into the prompt context at test time so the LLM can answer or generate claims using that contextualized information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>INFER (Inference-time contextual update)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>At inference the method augments the model prompt with the text of P_new (e.g., the paper abstract) when evaluating claim-judgment or claim-generation tasks, so the model conditions on the new evidence without changing model weights. This is analogous to retrieval-augmented or context-augmentation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Tested with LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (10 domains evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>P_test_new sets drawn from overall dataset; dataset covers 15,444 papers (new subset unspecified in this entry)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not specifically focused on extracting numeric laws; used to incorporate new scientific claims (textual findings) for claim verification/generation</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>natural-language judgments or generated claims conditioned on provided paper text</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated via claim judgment and generation tasks using factual accuracy + model confidence; metrics: Knowledge Preservation/Acquisition/Projection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: inference-based methods effective for large models; averaged best-performing methods: Knowledge Preservation 85.9%, Knowledge Acquisition 71.7%, Knowledge Projection 37.7%; for OLMo2-32B INFER preserved 99.1% in one setting (table values in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against training-based updates (CNT PRETRAIN, INST TUNE, PRE INST TUNE); inference outperformed training for large model (OLMo2-32B: inference ~10.9% higher Preservation than training), but performed worse for smaller model (LLaMA-8B: inference ~10.7% worse than training)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Prompt/context noise can distract smaller models; context-window limits; does not change model's parametric knowledge so projection (generalization to future claims) remains limited; efficacy depends on model scale</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4347.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INST_TUNE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Instruction-tuning (INST TUNE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-time update method where models are instruction-tuned on QA pairs plus new documents to internalize new scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Standard Instruction-tuning (INST TUNE)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The model is first autoregressively trained on new documents (P_train_new and P_test_new) and then fine-tuned on QA training pairs by minimizing answer-prediction loss given a question. In the paper LoRA adapters are used for parameter-efficient tuning (1 epoch autoregressive, 4 epochs SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct (LoRA adapters applied)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (10 domains in dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>P_new subsets from 15,444-paper corpus (exact per-experiment counts given in paper splits)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not explicitly extracting numeric laws; used to internalize natural-language scientific claims (support/refute) for downstream judgment/generation</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>model parameters updated to produce natural-language claims/judgments; no direct symbolic/numeric extraction format specified</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Claim judgment and generation tasks using factual accuracy combined with confidence estimation; metrics: Knowledge Preservation/Acquisition/Projection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper tables; averaged across tasks INST TUNE contributed to preservation/acquisition but did not achieve all three objectives simultaneously. Example: INST TUNE preservation ~86.3% (one reported cell) — see paper tables for per-domain numbers</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against CNT PRETRAIN, PRE INST TUNE, INFER, and INST TUNE + INFER; training methods outperform inference on smaller models while vice versa on larger models</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Training can be costly; training-only updates can cause distortion (confident incorrect outputs) and/or loss of prior knowledge; risks of catastrophic forgetting and distortion in claim generation (distortion > loss observed)</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4347.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNT_PRETRAIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual Pre-training (CNT PRETRAIN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive continued pre-training approach that introduces P_new via next-token prediction to update model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Continual Pre-training (CNT PRETRAIN)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The model is further autoregressively trained on new-document tokens (P_test_new) to minimize standard next-token prediction loss, thereby updating parametric knowledge without explicit instruction-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct (experiments use LoRA adapters for efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>P_new subsets from 15,444 total papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not targeted at numeric laws; aimed at integrating textual scientific claims into parametric model knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>updated model parameters enable natural-language generation/judgment</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Same claim judgment/generation evaluation with knowledge preservation/acquisition/projection metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in tables (varies by domain and model); e.g., LLaMA3.1-8B CNT PRETRAIN preservation 85.0 in one table cell — consult paper tables for detailed numbers</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to INST TUNE and INFER; training methods performed differently across scales (smaller models often benefit more from training updates)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Can cause catastrophic forgetting or distortion; computationally costly relative to inference-time updates; limited projection ability</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4347.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INST+INFER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-tuning plus Inference-time context (INST_TUNE + INFER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid update strategy combining instruction-tuning on new data with inference-time context augmentation for test-time grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>INST TUNE + INFER (Training + Inference)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The model is instruction-tuned on training QA and new documents, and at inference the prompts are augmented with the P_new paper text; intended to combine benefits of parametric learning and contextual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct (LoRA adapters used for training)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>P_new subsets drawn from 15,444 corpus</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>used for claim-level knowledge (not specifically quantitative laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>natural-language claims/judgments produced by model after training+context</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Claim judgment/generation tasks with confidence-aware accuracy metrics; Knowledge Preservation/Acquisition/Projection reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper finds combining inference with additional training did not improve over inference alone for large models; averaged metrics: best methods preserve 85.9%, acquire 71.7%, project 37.7% (see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to INFER and INST TUNE alone; diminishing returns observed when combining for larger models</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Adds complexity and compute; limited additional benefit beyond inference for sufficiently large models; still fails to jointly optimize preservation, acquisition, and projection</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4347.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscoveryWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryWorld (automated scientific discovery agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A virtual environment and benchmark for developing and evaluating automated scientific discovery agents that can explore, hypothesize, and test in simulated scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DiscoveryWorld agents</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as related work: DiscoveryWorld provides a virtual environment to train and evaluate agents (which may use LLMs) to carry out automated scientific discovery tasks such as hypothesis generation and experiment planning; the paper is referenced as an example of LLMs used for discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified in this paper (citation points to the DiscoveryWorld work for details)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / simulated scientific environments</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>intended for discovery of scientific hypotheses, relationships and principles in simulated domains (could include quantitative laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>agent-generated hypotheses and experiment descriptions; not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed in this paper (see cited DiscoveryWorld work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper cites discovery frameworks as promising but does not detail their limitations; general issues include simulation-to-reality gaps and evaluation of discovered laws</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4347.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArXivDigestables</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArXivdigestables: Synthesizing scientific literature into tables using language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses LLMs to synthesize scientific literature into structured tables to facilitate literature overview and extraction of relationships and comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArXivdigestables: Synthesizing scientific literature into tables using language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ArXivdigestables (table synthesis via LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as related work: the approach uses language models to extract and organize information from papers into tabular structured outputs, enabling easier inspection of extracted results and relationships across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified in this paper (see cited ArXivdigestables work)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>structured tabular summaries that can surface numerical results, comparisons, and patterns (potentially statistical relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>structured data tables (tabular entries summarizing paper findings)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed here; refer to original ArXivdigestables paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Potential hallucination in table entries, difficulty ensuring fidelity to sources; not elaborated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4347.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit referenced for supporting literature review tasks using LLMs to organize, summarize, and synthesize research findings across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LitLLM toolkit</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as related work: Litllm provides tooling to assist scientific literature review with LLMs, including retrieval, summarization, and structured extraction components to support synthesis of findings across many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified in this paper (see cited Litllm work)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific literature broadly</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>tooling for distilling claims and findings (may support numerical result aggregation and pattern discovery depending on pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>summaries, structured outputs for literature review; specifics in cited work</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>General limitations of literature-synthesis toolkits: fidelity, hallucination, and need for validation; not detailed here</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4347.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HypothesisProposer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as zero-shot hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach showing that LLMs can propose hypotheses or research ideas in a zero-shot fashion, useful for discovery and idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero shot hypothesis proposers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero-shot hypothesis proposing with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited work demonstrates LLMs' ability to propose hypotheses or research directions without fine-tuning; paper references this as relevant to LLMs' role in hypothesis generation and potential discovery of relations or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified here (see the cited work for specific LLMs used)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>hypothesis-level proposals (may include conjectures about relationships or patterns but not necessarily formal quantitative laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>natural-language hypotheses/ideas</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed in this paper; validation in cited work likely involves human or downstream evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Hypotheses need rigorous validation; proposals can be speculative and require experimental follow-up</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4347.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4347.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chime</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chime: LLM-assisted hierarchical organization of scientific studies for literature review support</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses LLMs to hierarchically organize and summarize scientific studies to assist literature review and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chime: Llm-assisted hierarchical organization of scientific studies for literature review support.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Chime (LLM-assisted literature organization)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as a literature-support system that leverages LLMs to organize scientific studies in a hierarchical manner, facilitating discovery of patterns and relationships across bodies of literature.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified in this paper (see cited CHIME work)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>literature review across scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>enables synthesis of study-level results and could surface quantitative comparisons/statistical patterns depending on pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>hierarchical organizational artifacts and summaries; possibly structured comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Potential for noisy aggregation; fidelity to original sources must be validated</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceMeter: Tracking Scientific Knowledge Updates in Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Synthesizing scientific literature with retrieval-augmented lms. <em>(Rating: 2)</em></li>
                <li>ArXivdigestables: Synthesizing scientific literature into tables using language models. <em>(Rating: 2)</em></li>
                <li>Litllm: A toolkit for scientific literature review. <em>(Rating: 2)</em></li>
                <li>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers. <em>(Rating: 2)</em></li>
                <li>Chime: Llm-assisted hierarchical organization of scientific studies for literature review support. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4347",
    "paper_id": "paper-279070437",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "SyntheticClaimGen-GPT4O",
            "name_full": "Synthetic Claim Generation using GPT-4O",
            "brief_description": "A pipeline that prompts GPT-4O to extract atomic SUPPORT and REFUTE scientific claims from paper abstracts (constrained to ~15 words) to create synthetic datasets for evaluation; claims were expert-validated.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Synthetic Claim Generation (GPT-4O)",
            "method_description": "For each paper the authors prompt GPT-4O with templates asking for a concise atomic claim uniquely supported by the paper (SUPPORT) and a relevant but non-supported claim (REFUTE). Claim length is constrained (~15 words). Generated claims are then validated: two domain experts per domain judged support/coverage and a separate author-annotated subset was collected. These synthetic claims are used in downstream claim-judgment and claim-generation evaluations.",
            "llm_model_used": "GPT-4O",
            "scientific_domain": "multi-domain (10 domains: medicine, biology, materials science, computer science, etc.)",
            "number_of_papers": "15,444 papers (claims generated for each paper)",
            "type_of_quantitative_law": "not specific to numeric laws — atomic scientific claims (qualitative/verifiable findings); used to evaluate claim-level knowledge rather than extracting numeric equations",
            "extraction_output_format": "short natural-language atomic claims (approx. 15 words); SUPPORT / REFUTE labeled claim tuples",
            "validation_method": "Expert evaluation by PhD students and a randomized author-annotation study; human adjudication of SUPPORT/REFUTE labels",
            "performance_metrics": "Expert validation: ≥80% of synthetic claims strictly adhered to rules; &gt;95% broadly met expectations (paper reports these validation fractions)",
            "baseline_comparison": "Compared synthetic-claim derived evaluation to author-annotated claims (284 claims): no statistically significant difference in downstream model performance",
            "challenges_limitations": "Claims are not numeric or symbolic laws; may omit multi-source convergent advances; synthetic generation quality imperfect (room for improvement) and depends on LLM prompting; may not capture quantitative relationships",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4347.0",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "INFER",
            "name_full": "Inference-time knowledge update (INFER / prompt-context augmentation)",
            "brief_description": "An inference-time update strategy that injects new paper text (e.g., abstracts) into the prompt context at test time so the LLM can answer or generate claims using that contextualized information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "INFER (Inference-time contextual update)",
            "method_description": "At inference the method augments the model prompt with the text of P_new (e.g., the paper abstract) when evaluating claim-judgment or claim-generation tasks, so the model conditions on the new evidence without changing model weights. This is analogous to retrieval-augmented or context-augmentation strategies.",
            "llm_model_used": "Tested with LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct in experiments",
            "scientific_domain": "multi-domain (10 domains evaluated)",
            "number_of_papers": "P_test_new sets drawn from overall dataset; dataset covers 15,444 papers (new subset unspecified in this entry)",
            "type_of_quantitative_law": "not specifically focused on extracting numeric laws; used to incorporate new scientific claims (textual findings) for claim verification/generation",
            "extraction_output_format": "natural-language judgments or generated claims conditioned on provided paper text",
            "validation_method": "Evaluated via claim judgment and generation tasks using factual accuracy + model confidence; metrics: Knowledge Preservation/Acquisition/Projection",
            "performance_metrics": "Reported: inference-based methods effective for large models; averaged best-performing methods: Knowledge Preservation 85.9%, Knowledge Acquisition 71.7%, Knowledge Projection 37.7%; for OLMo2-32B INFER preserved 99.1% in one setting (table values in paper)",
            "baseline_comparison": "Compared against training-based updates (CNT PRETRAIN, INST TUNE, PRE INST TUNE); inference outperformed training for large model (OLMo2-32B: inference ~10.9% higher Preservation than training), but performed worse for smaller model (LLaMA-8B: inference ~10.7% worse than training)",
            "challenges_limitations": "Prompt/context noise can distract smaller models; context-window limits; does not change model's parametric knowledge so projection (generalization to future claims) remains limited; efficacy depends on model scale",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4347.1",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "INST_TUNE",
            "name_full": "Standard Instruction-tuning (INST TUNE)",
            "brief_description": "A training-time update method where models are instruction-tuned on QA pairs plus new documents to internalize new scientific claims.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Standard Instruction-tuning (INST TUNE)",
            "method_description": "The model is first autoregressively trained on new documents (P_train_new and P_test_new) and then fine-tuned on QA training pairs by minimizing answer-prediction loss given a question. In the paper LoRA adapters are used for parameter-efficient tuning (1 epoch autoregressive, 4 epochs SFT).",
            "llm_model_used": "LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct (LoRA adapters applied)",
            "scientific_domain": "multi-domain (10 domains in dataset)",
            "number_of_papers": "P_new subsets from 15,444-paper corpus (exact per-experiment counts given in paper splits)",
            "type_of_quantitative_law": "not explicitly extracting numeric laws; used to internalize natural-language scientific claims (support/refute) for downstream judgment/generation",
            "extraction_output_format": "model parameters updated to produce natural-language claims/judgments; no direct symbolic/numeric extraction format specified",
            "validation_method": "Claim judgment and generation tasks using factual accuracy combined with confidence estimation; metrics: Knowledge Preservation/Acquisition/Projection",
            "performance_metrics": "Reported in paper tables; averaged across tasks INST TUNE contributed to preservation/acquisition but did not achieve all three objectives simultaneously. Example: INST TUNE preservation ~86.3% (one reported cell) — see paper tables for per-domain numbers",
            "baseline_comparison": "Compared against CNT PRETRAIN, PRE INST TUNE, INFER, and INST TUNE + INFER; training methods outperform inference on smaller models while vice versa on larger models",
            "challenges_limitations": "Training can be costly; training-only updates can cause distortion (confident incorrect outputs) and/or loss of prior knowledge; risks of catastrophic forgetting and distortion in claim generation (distortion &gt; loss observed)",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4347.2",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CNT_PRETRAIN",
            "name_full": "Continual Pre-training (CNT PRETRAIN)",
            "brief_description": "An autoregressive continued pre-training approach that introduces P_new via next-token prediction to update model weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Continual Pre-training (CNT PRETRAIN)",
            "method_description": "The model is further autoregressively trained on new-document tokens (P_test_new) to minimize standard next-token prediction loss, thereby updating parametric knowledge without explicit instruction-tuning.",
            "llm_model_used": "LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct (experiments use LoRA adapters for efficiency)",
            "scientific_domain": "multi-domain",
            "number_of_papers": "P_new subsets from 15,444 total papers",
            "type_of_quantitative_law": "not targeted at numeric laws; aimed at integrating textual scientific claims into parametric model knowledge",
            "extraction_output_format": "updated model parameters enable natural-language generation/judgment",
            "validation_method": "Same claim judgment/generation evaluation with knowledge preservation/acquisition/projection metrics",
            "performance_metrics": "Reported in tables (varies by domain and model); e.g., LLaMA3.1-8B CNT PRETRAIN preservation 85.0 in one table cell — consult paper tables for detailed numbers",
            "baseline_comparison": "Compared to INST TUNE and INFER; training methods performed differently across scales (smaller models often benefit more from training updates)",
            "challenges_limitations": "Can cause catastrophic forgetting or distortion; computationally costly relative to inference-time updates; limited projection ability",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4347.3",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "INST+INFER",
            "name_full": "Instruction-tuning plus Inference-time context (INST_TUNE + INFER)",
            "brief_description": "A hybrid update strategy combining instruction-tuning on new data with inference-time context augmentation for test-time grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "INST TUNE + INFER (Training + Inference)",
            "method_description": "The model is instruction-tuned on training QA and new documents, and at inference the prompts are augmented with the P_new paper text; intended to combine benefits of parametric learning and contextual grounding.",
            "llm_model_used": "LLaMA3.1-8B-Instruct and OLMo2-32B-Instruct (LoRA adapters used for training)",
            "scientific_domain": "multi-domain",
            "number_of_papers": "P_new subsets drawn from 15,444 corpus",
            "type_of_quantitative_law": "used for claim-level knowledge (not specifically quantitative laws)",
            "extraction_output_format": "natural-language claims/judgments produced by model after training+context",
            "validation_method": "Claim judgment/generation tasks with confidence-aware accuracy metrics; Knowledge Preservation/Acquisition/Projection reported",
            "performance_metrics": "Paper finds combining inference with additional training did not improve over inference alone for large models; averaged metrics: best methods preserve 85.9%, acquire 71.7%, project 37.7% (see paper)",
            "baseline_comparison": "Compared to INFER and INST TUNE alone; diminishing returns observed when combining for larger models",
            "challenges_limitations": "Adds complexity and compute; limited additional benefit beyond inference for sufficiently large models; still fails to jointly optimize preservation, acquisition, and projection",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4347.4",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DiscoveryWorld",
            "name_full": "DiscoveryWorld (automated scientific discovery agents)",
            "brief_description": "A virtual environment and benchmark for developing and evaluating automated scientific discovery agents that can explore, hypothesize, and test in simulated scientific domains.",
            "citation_title": "Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents.",
            "mention_or_use": "mention",
            "method_name": "DiscoveryWorld agents",
            "method_description": "Cited as related work: DiscoveryWorld provides a virtual environment to train and evaluate agents (which may use LLMs) to carry out automated scientific discovery tasks such as hypothesis generation and experiment planning; the paper is referenced as an example of LLMs used for discovery.",
            "llm_model_used": "unspecified in this paper (citation points to the DiscoveryWorld work for details)",
            "scientific_domain": "general / simulated scientific environments",
            "number_of_papers": null,
            "type_of_quantitative_law": "intended for discovery of scientific hypotheses, relationships and principles in simulated domains (could include quantitative laws)",
            "extraction_output_format": "agent-generated hypotheses and experiment descriptions; not specified here",
            "validation_method": "Not detailed in this paper (see cited DiscoveryWorld work)",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Paper cites discovery frameworks as promising but does not detail their limitations; general issues include simulation-to-reality gaps and evaluation of discovered laws",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4347.5",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ArXivDigestables",
            "name_full": "ArXivdigestables: Synthesizing scientific literature into tables using language models",
            "brief_description": "A system that uses LLMs to synthesize scientific literature into structured tables to facilitate literature overview and extraction of relationships and comparisons.",
            "citation_title": "ArXivdigestables: Synthesizing scientific literature into tables using language models.",
            "mention_or_use": "mention",
            "method_name": "ArXivdigestables (table synthesis via LLMs)",
            "method_description": "Referenced as related work: the approach uses language models to extract and organize information from papers into tabular structured outputs, enabling easier inspection of extracted results and relationships across papers.",
            "llm_model_used": "unspecified in this paper (see cited ArXivdigestables work)",
            "scientific_domain": "general scientific literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "structured tabular summaries that can surface numerical results, comparisons, and patterns (potentially statistical relationships)",
            "extraction_output_format": "structured data tables (tabular entries summarizing paper findings)",
            "validation_method": "Not detailed here; refer to original ArXivdigestables paper",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Potential hallucination in table entries, difficulty ensuring fidelity to sources; not elaborated in this paper",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4347.6",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "Litllm: A toolkit for scientific literature review",
            "brief_description": "A toolkit referenced for supporting literature review tasks using LLMs to organize, summarize, and synthesize research findings across papers.",
            "citation_title": "Litllm: A toolkit for scientific literature review.",
            "mention_or_use": "mention",
            "method_name": "LitLLM toolkit",
            "method_description": "Cited as related work: Litllm provides tooling to assist scientific literature review with LLMs, including retrieval, summarization, and structured extraction components to support synthesis of findings across many papers.",
            "llm_model_used": "unspecified in this paper (see cited Litllm work)",
            "scientific_domain": "scientific literature broadly",
            "number_of_papers": null,
            "type_of_quantitative_law": "tooling for distilling claims and findings (may support numerical result aggregation and pattern discovery depending on pipeline)",
            "extraction_output_format": "summaries, structured outputs for literature review; specifics in cited work",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "General limitations of literature-synthesis toolkits: fidelity, hallucination, and need for validation; not detailed here",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4347.7",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "HypothesisProposer",
            "name_full": "Large language models as zero-shot hypothesis proposers",
            "brief_description": "An approach showing that LLMs can propose hypotheses or research ideas in a zero-shot fashion, useful for discovery and idea generation.",
            "citation_title": "Large language models are zero shot hypothesis proposers.",
            "mention_or_use": "mention",
            "method_name": "Zero-shot hypothesis proposing with LLMs",
            "method_description": "Cited work demonstrates LLMs' ability to propose hypotheses or research directions without fine-tuning; paper references this as relevant to LLMs' role in hypothesis generation and potential discovery of relations or principles.",
            "llm_model_used": "unspecified here (see the cited work for specific LLMs used)",
            "scientific_domain": "general scientific research",
            "number_of_papers": null,
            "type_of_quantitative_law": "hypothesis-level proposals (may include conjectures about relationships or patterns but not necessarily formal quantitative laws)",
            "extraction_output_format": "natural-language hypotheses/ideas",
            "validation_method": "Not detailed in this paper; validation in cited work likely involves human or downstream evaluation",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Hypotheses need rigorous validation; proposals can be speculative and require experimental follow-up",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4347.8",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Chime",
            "name_full": "Chime: LLM-assisted hierarchical organization of scientific studies for literature review support",
            "brief_description": "A system that uses LLMs to hierarchically organize and summarize scientific studies to assist literature review and synthesis.",
            "citation_title": "Chime: Llm-assisted hierarchical organization of scientific studies for literature review support.",
            "mention_or_use": "mention",
            "method_name": "Chime (LLM-assisted literature organization)",
            "method_description": "Cited as a literature-support system that leverages LLMs to organize scientific studies in a hierarchical manner, facilitating discovery of patterns and relationships across bodies of literature.",
            "llm_model_used": "unspecified in this paper (see cited CHIME work)",
            "scientific_domain": "literature review across scientific domains",
            "number_of_papers": null,
            "type_of_quantitative_law": "enables synthesis of study-level results and could surface quantitative comparisons/statistical patterns depending on pipeline",
            "extraction_output_format": "hierarchical organizational artifacts and summaries; possibly structured comparisons",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Potential for noisy aggregation; fidelity to original sources must be validated",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4347.9",
            "source_info": {
                "paper_title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Synthesizing scientific literature with retrieval-augmented lms.",
            "rating": 2,
            "sanitized_title": "synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "ArXivdigestables: Synthesizing scientific literature into tables using language models.",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        },
        {
            "paper_title": "Litllm: A toolkit for scientific literature review.",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents.",
            "rating": 2,
            "sanitized_title": "discoveryworld_a_virtual_environment_for_developing_and_evaluating_automated_scientific_discovery_agents"
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "Chime: Llm-assisted hierarchical organization of scientific studies for literature review support.",
            "rating": 2,
            "sanitized_title": "chime_llmassisted_hierarchical_organization_of_scientific_studies_for_literature_review_support"
        }
    ],
    "cost": 0.01793775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCIENCEMETER: Tracking Scientific Knowledge Updates in Language Models
28 Jun 2025</p>
<p>Yike Wang yikewang@cs.washington.edu 
Allen Institute for Artificial Intelligence
University of Washington</p>
<p>Shangbin Feng 
Allen Institute for Artificial Intelligence
University of Washington</p>
<p>Yulia Tsvetkov 
Allen Institute for Artificial Intelligence
University of Washington</p>
<p>Hannaneh Hajishirzi 
Allen Institute for Artificial Intelligence
University of Washington</p>
<p>SCIENCEMETER: Tracking Scientific Knowledge Updates in Language Models
28 Jun 20250772D006223818856876E83646CF9AF4arXiv:2505.24302v2[cs.CL]
Large Language Models (LLMs) are increasingly used to support scientific research, but their knowledge of scientific advancements can quickly become outdated.We introduce SCIENCEMETER, a new framework for evaluating scientific knowledge update methods over scientific knowledge spanning the past, present, and future.SCIENCEMETER defines three metrics: knowledge preservation, the extent to which models' understanding of previously learned papers are preserved; knowledge acquisition, how well scientific claims from newly introduced papers are acquired; and knowledge projection, the ability of the updated model to anticipate or generalize to related scientific claims that may emerge in the future.Using SCIENCEMETER, we examine the scientific knowledge of LLMs on claim judgment and generation tasks across a curated dataset of 15,444 scientific papers and 30,888 scientific claims from ten domains including medicine, biology, materials science, and computer science.We evaluate five representative knowledge update approaches including training-and inference-time methods.With extensive experiments, we find that the best-performing knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of future knowledge.Inference-based methods work for larger models, whereas smaller models require training to achieve comparable performance.Cross-domain analysis reveals that performance on these objectives is correlated.Even when applying on specialized scientific LLMs, existing knowledge update methods fail to achieve these objectives collectively, underscoring that developing robust scientific knowledge update mechanisms is both crucial and challenging.Code and Data github.com/yikee/ScienceMeterPreprint.Under review.New Knowledge Scientific Claim: Microglia produce OSM, LIF, activin A, CSF-1, IL-34, GDF-15, FGF-2, and IGF-2 with neurotrophic activity.[Features of Microglia] LLM Scientific Knowledge Update Prior Knowledge Scientific Claim: Microglia are mononuclear phagocytes of mesodermal origin that migrate to the CNS during the early stages of embryonic development.[Basics of Microglia] Future Knowledge Scientific Claim: Microglia play a dual role in neurodegeneration and neuroprotection in Alzheimer's disease.[Roles of Microglia] Scientific Advancement preserve acquire project Paper: <Microglia morphophysiological diversity and its implications for the central nervous system> Paper: <Diversity of Microglia-Derived Molecules with Neurotrophic Properties That Support Neurons in the Central Nervous System and Other Tissues></p>
<p>Introduction</p>
<p>LLMs are being widely used to aid scientific research [30,34,17,44,22], with the potential to enable even greater future discoveries [2,47].However, due to the rapid pace of scientific advancements [26] and the static nature of pre-trained LLMs [7], their scientific knowledge quickly becomes stale.We posit that effective scientific knowledge updates in LLMs must do more than simply adding new information, but preserve existing knowledge, incorporate new findings, and enable generalization to reason about future or yet-undiscovered knowledge.Although generic update strategies have been explored, e.g., via continual pre-training [14], instruction-tuning [59], or retrieval-augmented generation [46], it is not clear whether these methods sufficiently support these goals.</p>
<p>To fill this gap, we propose SCIENCEMETER-a new framework for evaluating how LLMs update and reason over scientific knowledge.As shown in Figure 1, our approach centers on tracking scientific knowledge updates as trajectories along three axes: preservation of prior knowledge (the parametric knowledge already encoded in the LLM), acquisition of new knowledge introduced through knowledge update methods, and projection of future knowledge not yet available to the model Figure 1: We propose an evaluation framework, SCIENCEMETER, along with novel metrics to quantify the reliability and usefulness of scientific knowledge updates in LLMs: preservation of existing scientific claims and their linkage to existing literature, acquisition of new scientific claims from emerging research, and projection of future scientific claims.For example, when we update an LLM with a new paper introducing the effective identification of features of Microglia, our framework evaluates the acquisition of this new knowledge, as well as the preservation of existing knowledge about the fundamentals of Microglia learned from previous literature, and the ability of LLMs effectively use its parametric knowledge to extrapolate future knowledge on Microglia, such as potential roles of Microglia in the Alzheimer's disease.</p>
<p>but can be inferred.Past discoveries serve as the foundation for future advancements and remain valuable for researchers seeking historical context, validation, or reinterpretation of previous findings, while the latter evaluates the utility of knowledge updates in enabling models to internalize new knowledge, moving beyond mere factual memorization to understand the underlying principles and patterns that govern such knowledge.This capability can facilitate advanced reasoning, hypothesis generation [44], and the formulation of novel ideas [47]-key future usages of AI for science.</p>
<p>Inspired by SciFact [54], SCIENCEMETER operationalizes scientific knowledge as atomic scientific claims, i.e., atomic verifiable statements expressing a finding about one aspect of a scientific entity or process, which can be verified against a single source.While prior work in general domains often represents knowledge as factoid information or structured entity tuples [53,60], we argue that scientific claims are more appropriate and meaningful units of knowledge in scientific contexts, as they better capture the core insights and implications of research beyond isolated numerical values.</p>
<p>In SCIENCEMETER, we curate a large-scale, multi-domain dataset encompassing 15,444 research papers and 30,888 scientific claims across 10 rapidly evolving scientific fields, including medicine, biology, materials science, and computer science.As LLMs become increasingly integrated into these domains, it is essential to evaluate whether existing knowledge update methods can support their progress.Related scientific literature is grouped chronologically to represent prior, new, and future knowledge based on publication dates.To evaluate scientific knowledge, we focus on two tasks: claim judgment, and claim generation.To better reflect the rigor of the scientific domain, our evaluation methodology emphasizes both factual accuracy and model's confidence.Specifically, we categorize the model's knowledge of a claim as correct (factually accurate and confident), incorrect (factually inaccurate and confident), or unknown (not confident) and quantify the percentage of two types of errors in preservation, acquisition, and projection, respectively.</p>
<p>We evaluate LLMs' scientific knowledge updates using five methods spanning training, inference, or both.Experimental results across standard and frontier models highlight that the best-performing knowledge update method achieve on average only 85.9% on knowledge preservation, 71.7% on knowledge acquisition, and 37.7% on knowledge projection.While inference-time update methods tend to be effective for large models, smaller models require training-based approaches to achieve comparable performance.Cross-domain analysis reveals that performance on these objectives is correlated, with knowledge preservation and projection heavily influenced by domain volatility, while the availability of domain knowledge during pretraining has limited impact.Moreover, even applying on specialized, domain-adapted scientific LLMs struggle to balance these objectives, underscoring persistent challenges in updating scientific knowledge in LLMs.</p>
<p>The SCIENCEMETER Evaluation Framework</p>
<p>To systematically evaluate scientific knowledge updates in LLMs, SCIENCEMETER integrates three core components: (1) a carefully curated dataset consisting of 15,444 scientific papers and 30,888 scientific claims ( §2.1); (2) evaluation of model's scientific knowledge through claim judgment and generation tasks, assessed by both factual accuracy and model confidence ( §2.2); and (3) novel metrics for evaluating knowledge update methods that aggregate claims from past/present/future data sets ( §2. 3).An overview of SCIENCEMETER is illustrated in Figure 2. LLMs are increasingly integrated into scientific research across these domains, so it is crucial to assess whether LLMs can continuously contribute to these domains through knowledge updates.</p>
<p>Dataset Construction</p>
<p>For each domain, we retrieve 1,000 journal or conference papers (excluding review or survey papers) published at least three months before the knowledge cutoff date of the given model using the Semantic Scholar API [4].This three-month window accounts for potential discrepancies between a paper's online availability and its official publication date, ensuring a more accurate representation of the knowledge.To obtain more recent knowledge on the subjects relevant to each paper, we perform an additional query to the Semantic Scholar API, retrieving papers that cite the original paper and were published at least three months after the knowledge cutoff date.We also set a recent cutoff date, beyond which papers serve as a proxy for future knowledge.The specific cutoff dates used for all models examined in this study are detailed in Appendix C. In total, we constructed</p>
<p>Task Evaluation</p>
<p>To better reflect the rigor of the scientific domain, our evaluation methodology emphasizes both factual accuracy and the model's confidence.We present our choices of measurement methods in Section 3.2.By combining factual accuracy with model confidence, we categorize the model's knowledge of a claim as correct (factually accurate and confident), incorrect (factually inaccurate and confident), or unknown (not confident).We argue that when confidence is low, even a factually accurate answer is not reliable as it may result from hallucination or random chance.This categorization enables a detailed analysis of the impact on prior, new, and future scientific knowledge following knowledge updates, as discussed in the next section.</p>
<p>Evaluation of Knowledge Update Methods</p>
<p>Given the set of papers and associated claims, along with the model's knowledge about each claim (correct, incorrect, or unknown), we can systematically evaluate how a given knowledge update method impact the model's prior, new, and future scientific knowledge.Specifically, we define three core metrics, Knowledge Preservation, Knowledge Acquisition, and Knowledge Projection, as well as two associated error categories, distortion and loss, as detailed below.</p>
<p>Let P prior , P new , and P future be sets of prior, new, and future scientific documents in a particular scientific domain.P new is introduced using the given scientific knowledge update method f .Let g represent either the claim judgment or generation task presented in Section 2.2.Then, given a pre-trained language model LM and a knowledge update method f , we define:</p>
<p>• Knowledge Preservation as the percentage of scientific claims associated with P prior that remain correct.The proportion of previously correct claims that become incorrect is referred to as a distortion in preservation, while the proportion that becomes unknown is considered as loss.• Knowledge Acquisition as the proportion of scientific claims associated with P new that the model LM correctly acquires through f , i.e., changing from unknown to correct.Similarly, the proportion of unknown claims that become incorrect is referred to as a distortion in acquisition, while the proportion that stays as unknown is referred to as loss.• Knowledge Projection as the percentage of scientific claims associated with P future that the model LM successfully projects after update f , i.e., claims changing from unknown to correct.Because some incorrect projections may become correct over time, the true magnitude of Knowledge Projection is likely higher than our current estimate.We define the proportion of unknown claims that remain unknown as loss.</p>
<p>We provide the detailed formulas for each metric in Appendix E. The sum of Knowledge Preservation, distortion, and loss equals one, and the same holds for Acquisition.Among the error types, distortion is considered more severe than loss in both Preservation and Acquisition scenarios.This is because generating a factually inaccurate response with high confidence (e.g., stating "this claim is SUPPORT for sure" to a REFUTE claim) is more problematic than producing a lowconfidence response, regardless of its factual accuracy (e.g., "maybe this claim is SUPPORT/REFUTE").</p>
<p>An optimal scientific knowledge update method should aim to collectively maximize Knowledge Preservation, Knowledge Acquisition, and Knowledge Projection.</p>
<p>Experiment Settings and Results</p>
<p>In this section, we evaluate five knowledge update methods, covering training, inference, or a combination of both.Experiments on both standard and frontier models, along with three confidence measurement approaches, demonstrate the challenge of developing a such reliable scientific knowledge update method capable of meeting all three objectives.</p>
<p>Models</p>
<p>We aim to evaluate the performance of various knowledge update methods on a widely used, reasonably sized model and a frontier large model.As a representative of commonly used mid-sized models, we select LLaMA3.1-8B-Instruct[13], and OLMo2-32B-Instruct [39] serves a representative of frontier models, given computational constraints and the limited openness of commercial models.Notably, OLMo2-32B-Instruct has demonstrated frontier performance while requiring only one-third of the compute of other open-weight models and outperforming GPT-4o mini [40].</p>
<p>Factual Accuracy and Model Confidence Measurement Methods</p>
<p>Factual Accuracy For the Claim Judgment task, we map the model's predictions ŷ(c, t) and ŷ(c) to the set {SUPPORT, REFUTE} using manually identified answer patterns, and compare them against the ground-truth labels y(c, t) and y(c), respectively.For the Claim Generation task, we assess the factual accuracy of the generated claim ĉ by inviting GPT-4O to determine whether y(ĉ, t) = SUPPORT, based on the abstract of the corresponding paper p.</p>
<p>Model Confidence Given the absence of a validation set, we estimate confidence levels using three rule-based measurement methods and finalize the decision through majority voting.</p>
<p>• More Information Following existing prompt-based solutions [10,12], we append a prompt asking whether more information is needed to answer a given question: "Do you need more information to answer this question?(Yes or No)".Indicating the need for more information suggests a lack of confidence.</p>
<p>• Consistency We paraphrase the question three times, sample responses for each version, and classify the model as confident if all responses converged on the same final binary answer.</p>
<p>• Linguistic Confidence Given only the model's response, we prompt GPT-4O [40] with the following question: "Do you think the model is confident about its answer?(Yes or No)", aiming to capture implicit linguistic markers of confidence, such as assertive phrasing, authoritative tone, and decisive language in the response.We also conduct a human evaluation of linguistic confidence.The confidence classification consistency between three human evaluators and GPT-4O is 75.9%,thereby validating the effectiveness of GPT-4O as a judge in this task.All three methods are used as confidence measurement for the judgment responses, while More Information is used for generation responses.</p>
<p>Knowledge Update Methods</p>
<p>We experiment with five knowledge update methods that update new scientific knowledge (i.e., P new ) at either the training stage, the inference stage, or both.P new is split into training and test sets, and only the test set will be evaluated.Following previous work on scientific domains [54,38], we use  [24] introduces a new method that exposes LLMs to QA pairs before continued pre-training on documents.Specifically, the model is instruction-tuned on training QA along with P train new prior to autoregressively trained on P test new .</p>
<p>Inference (INFER).The success of in-context learning [8] highlights the potential for introducing new knowledge at inference time, offering a more cost-efficient approach.Many existing knowledge augmentation methods, including retrieval-augmented generation [46], search engines [43], and multi-LLM collaborations [10,11], leverage this strategy to provide additional information.In our setting, we add corresponding paper p new in P test new to the prompt text and g(LM f (P test new ) , p) = g(LM |p new , p).Training + Inference (INST TUNE + INFER).Following Tang et al. [53], we also explore whether combining training and inference-time methods can yield improved performance.Specifically, we integrate standard instruction-tuning with the inference-time approach.</p>
<p>Results</p>
<p>No knowledge update method can simultaneously achieve all three objectives.As shown in Table 2, the best-performing knowledge update methods, averaged across tasks and models, preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% (or more) of future knowledge.However, we fail to find a method that can achieve all three objectives collectively.Overall, standard instruction-tuning and inference methods remain as the strongest methods across five.Enabling models to project future knowledge presents a new challenge for knowledge update.As LLMs become increasingly integrated into scientific workflows, especially tasks such as hypothesis and idea generation, it becomes critical to develop update methods that not only integrate new claims but also enable models to anticipate and reason about future scientific advancements.Inference-based methods work for larger models, whereas smaller models require training to achieve comparable performance.For instance, in the claim judgment task, OLMo2-32B achieves inference-time performance that is 10.9% higher than training-based methods in Knowledge Preservation on average, whereas the inference-time method on LLaMA-8B performs 10.7% worse than training-based methods.This discrepancy arises in part from the larger models' capacity to effectively filter out irrelevant or noisy contextual information during inference.With their greater representational power and more robust internal attention mechanisms, larger models are less sensitive to distractions in the prompt or context [8,6,37], allowing them to incorporate new knowledge with minimal degradation of prior understanding.From a computational perspective, inferencebased update is significantly more cost-effective than training [16,45,28], especially when updates are frequent.Notably, combining inference with additional training does not lead to performance improvements over inference alone, suggesting diminishing returns from training once a model has sufficient capacity to leverage inference-based strategies effectively.For smaller models, however, training remains a necessary component to compensate for their limited ability to generalize and disambiguate new knowledge in context.</p>
<p>In the Claim Generation task, distortion is significantly greater than loss.Specifically, in the more challenging Claim Generation task, the amount of distortion is, on average, three times higher than loss in both Preservation and Acquisition.As we discussed in Section 2.3, distortion is considered more severe than loss in both scenarios.This observation suggests a significant challenge for knowledge update methods, as they may introduce errors even when they should remain cautious.To address this issue, future developments in knowledge update methods could incorporate an abstention mechanism, avoiding updating models' representations if they lack confidence or certainty about the new content.Such a mechanism would allow models to opt out of updates when faced with ambiguous or uncertain knowledge, helping to preserve accuracy and reduce the propagation of errors.</p>
<p>Analysis</p>
<p>In this section, we perform additional analyses across different scientific domains.The results reveal that performance on the three objectives is correlated; Preservation and Projection exhibit strong dependence on domain volatility, whereas the availability of domain knowledge in the pretraining corpus demonstrates only marginal influence.We also evaluate scientific LLMs in the worst-performing domain (i.e., Materials Science) and find that even applying on domain-adapted models struggle to achieve all three objectives, underscoring persistent challenges in updating scientific knowledge.</p>
<p>Cross-domain Analysis</p>
<p>In this section, we further break down performance by scientific domain and analyze potential factors that may influence the preservation, acquisition, and projection of scientific knowledge.As shown in  Figure 3, performance varies significantly across domains.While over 90% of scientific knowledge in Political Science is preserved, only 72% of Materials Science knowledge can be retained.Similarly, while 48.6% of scientific knowledge in Education can be projected, this drops to just 16.8% in Materials Science.We also notice that these three capabilities appear to be correlated.Performance in certain domains tends to be consistently poor across all three tasks, for example, Materials Science and Environmental Science, whereas domains such as Political Science and Education exhibit relatively strong performance across all three objectives.</p>
<p>We hypothesize that domain performance may be influenced by two key factors:</p>
<p>First, the nature of the domain, specifically the stability or volatility of knowledge within that domain.</p>
<p>In some domains, such as Political Science, knowledge is more stable, with long-established theories and principles that evolve slowly over time.In contrast, other domains, such as Materials Science, may experience more volatility, with frequent breakthroughs or shifting paradigms that rapidly change the state of knowledge.Knowledge preservation and projection may be more challenging in domains with higher volatility compared to those with greater stability.To test this hypothesis, we randomly retrieve 1,000 conference or journal papers published between October 2022 and September 2023 in each domain, and calculate the average citation count for these papers (Appendix G), under the assumption that higher average citation counts reflect higher knowledge volatility.</p>
<p>Second, the availability of domain knowledge in the pretraining corpus.We posit that if domain knowledge appears frequently or widely in the pretraining corpus, knowledge acquisition might be easier.To assess this possibility, we collect the 100 tokens that appear least frequently in the abstracts of these 1,000 papers in each domain, as they tend to be specialized tokens unique to each domain.We then use Infini-gram [31] to count the occurrence of these tokens in the pretraining corpus Dolma-v1.7 [48], and use the average occurrence of domain-specific tokens as a proxy for the availability of domain knowledge in the pretraining data.A complete list of specialized tokens and average occurrences across domains is provided in Appendix G.</p>
<p>As shown in Figure 4, our analysis reveals a strong relationship between the ability to preserve and project scientific knowledge and the dynamics of the domain.Specifically, the Pearson correlation coefficient [42] between average Citation Count and Knowledge Preservation is -0.709, while its correlation with Knowledge Projection is -0.736, both indicating a significant relationship.Highly dynamic domains with frequent updates may lead to more knowledge conflicts [58], making preservation and projection particularly challenging.In contrast, the correlation between pretraining availability and the robustness of scientific knowledge updates is relatively weak, indicating that pretraining alone may have a limited impact on how well models adapt to evolving scientific information.</p>
<p>Scientific LLMs</p>
<p>Rather than relying solely on off-the-shelf LLMs, researchers also use scientific LLMs [62], LLMs specifically trained or adapted for science.In this work, we also experiment with HoneyBee [49], a llama-based model fine-tuned for Materials Science domain using high-quality, relevant textual data from the open literature.As we find that the performance of scientific knowledge updates in Materials Science is significantly lower than other domains (Section 4.1), we wonder if applying on a specialized scientific LLM could help.As shown in Table 3, scientific LLMs show no significant improvement compared to off-the-shelf LLMs of similar sizes, highlighting the unique challenges involved in updating scientific knowledge in terms of preservation, acquisition, and projection.</p>
<p>Related Work</p>
<p>LLMs for Scientific Advancements Recent research has demonstrated the significant potential of LLMs in driving scientific advancements across various domains, revolutionizing the way researchers approach complex problems and innovate in their respective fields [30,34,17,44,22,2,47,50].</p>
<p>Researchers utilize off-the-shelf LLMs [3], domain-specific scientific LLMs [62], or LLMs augmented with external resources [5] to assist in scientific research.Studies show that current LLMs can be useful across various stages of the research cycle [34], including literature review [17,1], hypothesis proposing [44], idea generation [47], and experiment planning and implementation [22,19].However, to the best of our knowledge, we are the first to explore whether LLMs can effectively stay up to date with evolving scientific fields while remaining reliable and useful.Specifically, we examine whether LLMs can continuously contribute to the advancement of these fields.</p>
<p>Evaluation of Knowledge Updates in LLMs Our evaluation of scientific knowledge updates differs from existing work on evaluation of knowledge updates in LLMs [29,51,55] in three key aspects.First, most prior work primarily regards the effective incorporation of new information as the only objective [20,41,24,61,53,60,63,21], with few studies also considering the preservation of old knowledge [24,61].However, they rely on generic benchmarks such as Natural Questions [27] and CommonsenseQA [52], which evaluate the retention of general world knowledge rather than the preservation of knowledge related to the newly updated information.And we further introduce a new evaluation dimension, evaluating the utility of knowledge updates for reasoning, hypothesis generation [44], and the creation of novel ideas [47], which are the key future applications of AI in science.Second, existing approaches heavily rely on Wikipedia as a data source and assess knowledge at the factoid level (e.g., names, locations) [20,41,24,61,53,60,63,21], whereas we extend evaluation to natural language representations, which better capture the core insights and implications of research beyond isolated numerical values as well as the complexity of real-world knowledge.Third, prior work on knowledge alignment primarily focuses on temporal alignment [63,60,21], aiming to align knowledge to specific timestamps, such as associating a president with a particular year, our goal, in scientific domains, is to align scientific claims with scientific literature.</p>
<p>Furthermore, we distinguish our evaluation of knowledge preservation from Catastrophic Forgetting (CF) in Continual Learning (CL) [25,20,9], as our setting involves multiple training stages and methods.Another relevant line of work is knowledge editing [36,57,35,62,33,56,23,32,15], which aims to replace incorrect existing knowledge, whereas our goal is to integrate new knowledge without altering the model's understanding of previously learned scientific literature.</p>
<p>Conclusion</p>
<p>In this work, we investigate scientific knowledge updates of LLMs and propose that an effective and reliable update method should be able to preserve existing scientific knowledge, acquire new scientific knowledge, and project future scientific knowledge, which are crucial for the continual use of LLMs in evolving scientific fields.To this end, we introduce an evaluation framework SCIENCEMETER with rich datasets of scientific papers across domains, new tasks and evaluation of scientific knowledge, and new metrics for evaluating knowledge update methods.With comprehensive experiments on frontier general-purpose and science-focused LLMs, we find that achieving these objectives remains an open research challenge, underscoring the need for further exploration in this direction.</p>
<p>A Limitations</p>
<p>Real Scientific Advancement is Far More Complex In this work, we model scientific advancement as a linear timeline spanning existing, new, and future developments.However, genuine scientific progress is considerably more complex in reality.New advancements often emerge from the convergence of multiple research trajectories across diverse domains.Future work should aim to capture this multidimensional nature of scientific progress.</p>
<p>Beyond Scientific Claims While this work focuses on scientific claims as the fundamental unit of analysis for evaluating scientific knowledge and scientific knowledge updates, we recognize that scientific knowledge operates at multiple meaningful levels of granularity.Other critical dimensions worthy of investigation include the paper-level and researcher-level, which could be potential directions for future research.</p>
<p>Contradictory Claims When evaluating future scientific knowledge using claim classification tasks, we acknowledge that, theoretically, there is a chance that some claims may contradict past findings.However, empirical evidence suggests such occurrences are rare.Moreover, our claims are sufficiently detailed and comprehensive, making it unlikely that identical or highly similar claims have appeared in prior literature.</p>
<p>Disentangling Knowledge from Instruction-Following/Reasoning Capabilities Separating the "knowledge" of LLMs from their instruction-following and reasoning abilities is challenging, particularly if we define "knowledge" as the content they generate.Prior work has attempted to assess LLMs' knowledge using cloze-style tasks [20] at inference time, which rely more on raw knowledge and less on instruction-following ability.However, such formats are limited to evaluating factoid knowledge.</p>
<p>In this work, we define scientific knowledge as scientific claims and propose claim judgment and generation tasks to evaluate it.While these tasks are effective for assessment and analysis, we acknowledge that model performance on them still depends, to some extent, on instruction-following and reasoning capabilities.</p>
<p>B Ethics Statement</p>
<p>We envision certain potential ethical risks of SCIENCEMETER.For example, when evaluating "future" scientific claims, the framework risks creating ethical dilemmas regarding premature validation of unproven hypotheses.This becomes particularly problematic when assessing claims in sensitive domains (e.g., climate science or medical research) where premature endorsement could influence policy or clinical decisions.</p>
<p>However, SCIENCEMETER also provides significant ethical benefits by introducing systematic transparency to scientific knowledge assessment.SCIENCEMETER can help surface meritorious but underrecognized research directions, and these features may ultimately promote more equitable and evidence-based scientific progress when implemented with appropriate ethical safeguards.</p>
<p>C Dataset Details C.1 Date Cutoffs</p>
<p>Table 4 presents the specific date cutoffs used to construct the dataset for all models in this study.A three-month buffer accounts for potential discrepancies between a paper's online availability and its official publication date, allowing a more accurate representation of respective knowledge.</p>
<p>C.2 Synthetic Claims</p>
<p>To generate synthetic claims for each paper, we prompt GPT-4O with the instructions detailed in Table 5.We explored various alternative methods for synthetic claim generation, such as retrieving a relevant paper and using its SUPPORT claim as the REFUTE claim for the given paper.However, the method we ultimately adopted, despite its simplicity, yielded the best results.Additionally, we control the granularity of claims by constraining their length to approximately 15 words, ensuring that they are neither overly simplistic nor excessively verbose (e.g., the entire abstract).</p>
<p>Model</p>
<p>User Prompt</p>
<p>Please identify and extract the main scientific claim that is uniquely supported by the given paper.A scientific claim is a atomic verifiable statements expressing a finding about one aspect of a scientific entity or process, which can be verified against a single source.</p>
<p>Prompt: REFUTE Claim Generation</p>
<p>System Prompt</p>
<p>You are an expert scientific research assistant.</p>
<p>User Prompt</p>
<p>Please identify and extract a scientific claim that is relevant but not supported by the given paper.A scientific claim is a atomic verifiable statements expressing a finding about one aspect of a scientific entity or process, which can be verified against a single source.</p>
<p>Table 5: Prompt templates for synthetic claim generation.</p>
<p>We further conduct an expert evaluation of our synthetic claims.We invite two PhD students in Computer Science and two PhD students in Biology.Each student is assigned 30 papers within their respective domain of expertise.For each paper, we provide the title, abstract, and two synthetic claims, and they are instructed to classify each claim into one of the following categories:</p>
<p>• Uniquely Supported -The claim can only be verified by the given paper.</p>
<p>• Broadly Supported -The claim is supported by the given paper but is likely validated by other sources as well.• Not Supported -The claim is not supported by the given paper.</p>
<p>The results are presented in Table 6, showing that at least 80% of claims strictly adhere to the rule, while more than 95% broadly meet our expectations.While the results demonstrate the effectiveness of our synthetic claims, there is still room for improvement, so we collect author-annotated claims in Section C.3.</p>
<p>Computer Science Biology</p>
<p>C.3 Author-annotated Claims</p>
<p>We argue that the original authors of research papers possess the most appropriate expertise for claim annotation.Under budget constraints, we conducted a randomized survey of 50 computer science researchers, requesting annotations of claims from their own publications.This process yielded 284 scientific claims (142 SUPPORT and 142 REFUTE claims) derived from 142 papers spanning various publication dates.While these papers do not necessarily share citation relationships, we consider them conceptually related as they all belong to the AI subfield of Computer Science.</p>
<p>Our evaluation using LLaMA-8B on this author-annotated dataset (Table 7) reveals no statistically significant performance difference compared to synthetic claims.This finding empirically validates the effectiveness of our synthetic claim generation methodology, suggesting that the synthetic claims maintain comparable quality to human expert annotations while offering scalability advantages.</p>
<p>D Claim Judgment and Generation Tasks</p>
<p>We present the prompts used for the claim judgment and generation tasks in Table 8</p>
<p>E Metrics</p>
<p>Table 10 provides the detailed mathematical definitions of knowledge preservation, knowledge acquisition, and knowledge projection, as well as distortion and loss.</p>
<p>Prompt: Claim Generation Task -Prior and New Knowledge</p>
<p>System Prompt</p>
<p>You are an AI research assistant designed to provide accurate, evidence-based responses.</p>
<p>User Prompt</p>
<p>State the main scientific claim made in the paper {title}.A scientific claim is a atomic verifiable statements expressing a finding about one aspect of a scientific entity or process, which can be verified against a single source.</p>
<p>Prompt: Claim Generation Task -Future Knowledge</p>
<p>System Prompt</p>
<p>You are an AI research assistant designed to provide accurate, evidence-based responses.</p>
<p>User Prompt</p>
<p>State a scientific claim about {subject}.A scientific claim is a atomic verifiable statements expressing a finding about one aspect of a scientific entity or process, which can be verified against a single source.</p>
<p>F Experiment Details</p>
<p>We employ a learning rate of 2 × 10 −4 for model optimization, and experiments are performed on a cluster with 4 A100 GPUs each with 40 GB memory.</p>
<p>G Cross-domain Analysis</p>
<p>This section provides details on cross-domain analysis discussed in Section 4.1.</p>
<p>G.1 Citation Count</p>
<p>See Table 11 for the average citation count across ten scientific domains.</p>
<p>Domain</p>
<p>G.2 Domain-specific Tokens</p>
<p>We randomly retrieve 1,000 conference or journal papers published between October 2022 and September 2023 for each of the ten domains.From these abstracts, we extract the 100 least frequently occurring tokens using the LLAMA3.1 tokenizer.Stop words, punctuation, and numbers are removed from the list.The full list of tokens is provided in Table 12 and Table 13.</p>
<p>G.3 Occurrence in Pretraining Corpus</p>
<p>Refer to Table 14 for the average occurrence of domain-specific tokens in the pretraining data.</p>
<p>Figure 2 :
2
Figure 2: An overview of SCIENCEMETER: (1) We curate chronologically organized datasets of scientific papers and claims across 10 rapidly evolving domains; (2) define claim judgment and generation tasks to evaluate scientific knowledge, incorporating both factual accuracy and model confidence; and (3) introduce metrics for evaluating scientific knowledge updates.</p>
<p>Figure 3 :
3
Figure 3: Performance of Standard Instruction-tuning on LLAMA3.1-8B in the claim judgment task.Performance are color-coded per category: Preservation, Acquisition, Projection.The performance in Materials Science and Environmental Science is poor across all three objectives, whereas Political Science and Education show relatively strong performance in all three.</p>
<p>Figure 4 :
4
Figure 4: The correlation between Preservation, Acquisition, Projection and average citation count, as well as pretraining occurrence.The strength of the correlation is reflected in how closely the data points cluster around the best-fit line.</p>
<p>i</p>
<p>I(g(LM, p i prior ) = correct, g(LM, p i new ) = unknown)distortion in Preservation = i I(g(LM f (P test new ) , p i prior ) = incorrect | g(LM, p i prior ) = correct, g(LM, p i new ) = unknown)) i I(g(LM, p i prior ) = correct, g(LM, p i new ) = unknown)) loss in Preservation = i I(g(LM f (P test new ) , p i prior ) = unknown | g(LM, p i prior ) = correct, g(LM, p i new ) = unknown)) i I(g(LM, p i prior ) = correct, g(LM, p i new ) = unknown)) Knowledge Acquisition = i I(g(LM f (P test new ) , p i new ) = correct | g(LM, p i new ) = unknown) i I(g(LM, p i new ) = unknown) distortion in Acquisition = i I(g(LM f (P test new ) , p i new ) = incorrect | g(LM, p i new ) = unknown) i I(g(LM, p i new ) = unknown) loss in Acquisition = i I(g(LM f (P test new ) , p i new ) = unknown | g(LM, p i new ) = unknown) i I(g(LM, p i new ) = unknown) Knowledge Projection = i I(g(LM f (P test new ) , p i future ) = correct | g(LM, p i future ) = unknown, g(LM, p i new ) = unknown)) i I(g(LM, p i future ) = unknown, g(LM, p i new ) = unknown))loss in Projection = i I(g(LM f (P test new ) , p i future ) = unknown | g(LM, p i future ) = unknown, g(LM, p i new ) = unknown))i I(g(LM, p i future ) = unknown, g(LM, p i new ) = unknown))</p>
<p>very recent cite publication date Evaluation of Knowledge Update Methods ( §2.3)
Dataset Construction ( §2.1)Evaluation of Scientific Knowledge ( §2.2)Paper CollectionClaim JudgmentorClaim Generationloss8%PriorNewFutureModel Responsedistortion 6%training cutoff date Synthetic Claim Generation SUPPORT claimuniquely supportingModel ConfidenceFactual Accuracy Accurate Inaccurate High Low A B C D A CorrectPreservation 86% Correct Prior knowledge Acquisition loss 10% distortion 18% 72%18% loss 44%Projection 38%paperBIncorrectREFUTE claimrelevant but non-supportingC DUnknownNew knowledge UnknownFuture knowledge Unknown</p>
<p>Claim GenerationThe generation task poses a greater challenge.To evaluate a model's knowledge of p prior or p new , the model is given the title t of a prior or new scientific paper p and instructed to generate a supporting claim ĉ(t) such that y(ĉ, t) = SUPPORT.To evaluate a model's knowledge of p future , the model is given the subject s of a "future" scientific paper p (with title t) and tasked with generating a supporting claim ĉ(s) such that y(ĉ, t) = SUPPORT.
claim generation approach. To further validate our methodology, we collect a set of author-annotatedclaims and conduct additional experiments. The results consistently support the efficacy of ourapproach, indicating that synthetic claims achieve comparable quality to those annotated by humanexperts while offering significant scalability benefits. Further details are provided in Appendix C.2.2 Evaluation of Scientific KnowledgeNow we want to evaluate a model's scientific knowledge on the papers collected in Section 2.1,specifically the scientific claims made in each paper. We propose two tasks, judgment and generation,and categorize the model's knowledge of a claim as correct, incorrect, or unknown, based on both theresponse's factual accuracy and the model's confidence.Task Formulation• Claim Judgment To evaluate a model's knowledge of p prior or p new , we frame the task as a claimverification problem: given the title t of a prior or new scientific paper and its associated claimc, the ground-truth label is y(c, t) ∈ {SUPPORT, REFUTE}. The model is instructed to predict alabel ŷ(c, t) for each (c, t) pair. To evaluate a model's knowledge of p future , we adapt the task intoa classification setting: given a claim c associated with a "future" scientific paper, the groundtruth label y(c) ∈ {SUPPORT, REFUTE} indicates whether its associated "future" paper supportsor refutes the claim c. The model is asked to predict a label ŷ(c) based solely on its internalizedknowledge or extrapolative reasoning, without access to any specific paper.•5,148 triplets of (p prior , p new , p future ), each representing a prior, new, and future version of scientificknowledge on the same topic.Synthetic Claim Generation and Expert Validation We synthetically generate one SUPPORT(uniquely supporting) scientific claim and one REFUTE (relevant but non-supporting) scientific claimfor each paper, resulting in a total of 15,444 (p, c SUPPORT ) and 15,444 (p, c REFUTE ) tuples. Expertevaluation confirms that at least 80% of the generated claims strictly comply with the specified rule,while over 95% broadly align with our expectations, demonstrating the effectiveness of our synthetic</p>
<p>Table 2 :
2
Dist Loss Acqu Dist Loss Proj Loss Pres Dist Loss Acqu Dist Loss Proj Loss LLAMA3.1-8B-INSTRUCTCNT PRETRAIN 85.0 5.5 9.5 37.3 29.9 32.8 34.5 48.3 53.3 30.0 16.7 53.1 42.0 5.0 11.8 70.6 INST TUNE 86.3 4.1 9.6 38.9 28.3 32.8 24.1 41.3 72.2 17.8 10.0 56.1 38.2 5.7 29.4 64.7 INST TUNE + INFER 96.1 0.9 2.9 46.6 6.8 46.7 33.3 18.7 41.7 57.1 1.3 80.5 8.7 10.8 30.4 26.4 Performance of knowledge update methods in the domain of Computer Science.Best results in bold and second best in underline.Performance are color-coded per category: Preservation, Acquisition, Projection.Higher values of preservation, acquisition, and projection are better, while lower values of distortion and loss are preferred.All methods fail to meet objectives collectively.abstracts of papers in P new instead of full papers, as they typically contain sufficient information and are easier to fit within the model's context window.Training.Through training, we update the model parameters by minimizing loss defined by different training objectives.Only LoRA adapters [18] are trained for all training baselines, with the training duration set to 1 epoch for autoregressive training and 4 epochs for SFT.Continual Pre-training (CNT PRETRAIN).P test new is introduced through autoregressive training [14], minimizing the standard next-token prediction loss: − 1 |d| t log p θ (d t |d &lt;t ).Standard Instruction-tuning (INST TUNE).The model is first trained autoregressively on both P train new and P test new , and then fine-tuned [59] on training QA by minimizing the answer prediction loss given the question: − 1 |a| t log p θ (a t |q, a &lt;t ).Pre-instruction-tuning (PRE INST TUNE).Jiang et al.
Claim Judgment TaskClaim Generation TaskMethod Pres PRE INST TUNE 59.0 38.3 2.7 64.2 26.8 9.0 44.9 48.2 63.3 23.3 13.3 56.1 37.4 6.5 11.8 64.7INFER68.6 17.8 13.6 43.2 50.8 6.0 48.3 13.7 14.4 62.2 23.3 84.4 8.4 7.3 23.5 5.9INST TUNE + INFER 69.9 19.2 10.9 41.8 43.3 15.0 44.9 6.8 12.2 58.9 28.9 76.0 11.5 12.6 17.6 17.6OLMO2-32B-INSTRUCTCNT PRETRAIN89.4 0.0 10.6 18.7 40.7 40.7 16.6 63.8 82.5 17.5 0.0 68.3 31.7 0.0 13.1 71.5INST TUNE89.5 0.9 9.6 20.3 35.6 44.2 13.8 68.5 85.8 14.2 0.0 67.7 32.3 0.0 18.9 71.3PRE INST TUNE89.4 0.9 9.7 17.0 39.9 43.2 17.6 65.7 84.2 15.8 0.0 68.3 31.7 0.0 18.6 63.9INFER99.1 0.9 0.0 57.7 3.3 39.0 35.3 15.6 42.9 55.8 1.3 79.3 9.9 10.8 37.6 13.7</p>
<p>Table 3 :
3
Performance of Standard Instruction-tuning on off-the-shelf and scientific LLMs in the claim judgment task within the domain of Materials Science.Best results in bold and second best in underline.HoneyBee is a materials science model fine-tuned on LLaMa-7B.
ModelPreservation Distortion Loss Acquisition Distortion Loss Projection LossLLAMA3.1-8B-INSTRUCT72.011.017.015.513.471.716.875.8OLMO2-7B60.316.922.811.936.052.115.676.0HONEYBEE-7B62.60.037.418.242.839.015.169.0</p>
<p>Table 4 :
4
Date cutoffs used to distinguish prior, new, and future knowledge when constructing the dataset for different models.
CutoffPrior KnowledgeNew KnowledgeFuture KnowledgeLLAMA3.1-8B Dec 2023 2022.10.1 -2023.9.30 2024.3.1 -2024.11.30 2024.12.1 -2025.2.1OLMO2-7BDec 2023 2022.10.1 -2023.9.30 2024.3.1 -2024.11.30 2024.12.1 -2025.2.1OLMO2-32BDec 2023 2022.10.1 -2023.9.30 2024.3.1 -2024.11.30 2024.12.1 -2025.2.1HONEYBEEOct 2023 2022.8.1 -2023.7.31 2024.1.1 -2024.11.30 2024.12.1 -2025.3.1Prompt: SUPPORT Claim GenerationSystem PromptYou are an expert scientific research assistant.</p>
<p>Table 6 :
6
Expert evaluation results on synthetic claims, averaged across two experts per domain.
SUPPORTREFUTESUPPORTREFUTEUniquely Supported83.3%0.0%80.0%0.0%Broadly Supported13.3%16.7%15.0%11.7%Not Supported3.3%83.3%5.0%88.3%</p>
<p>Table 7 :
7
We evaluate Standard Instruction-tuning on LLaMA-8B using our claim judgment task with synthetic and author-annotated claims in Computer Science respectively.The results demonstrate no statistically significant difference between model performance on synthetic versus author-annotated claims, which validates the effectiveness of our synthetic claim generation approach.
ModelPreservation Distortion Loss Acquisition Distortion Loss Projection LossSYNTHETIC CLAIMS86.34.19.638.928.332.824.141.3AUTHOR-ANNOTATED CLAIMS89.33.77.033.326.540.220.943.0</p>
<p>Table 8 :
8
and Table9.Prompt templates for Claim Judgment Task.
Prompt: Claim Judgment Task -Claim Verification (Prior and New Knowledge)System PromptYou are an AI research assistant designed to provide accurate, evidence-based responses.User Promptclaim: {claim}Can every detail in the given claim be substantiated by the paper {title}?Prompt: Claim Judgment Task -Claim Classification (Future Knowledge)System PromptYou are an AI research assistant designed to provide accurate, evidence-based responses.User Promptclaim: {claim}Is the claim correct?</p>
<p>Table 9 :
9
Prompt templates for Claim Generation Task.
Knowledge Preservation =
i I(g(LM f (P test new ) , p i prior ) = correct | g(LM, p i prior ) = correct, g(LM, p i new ) = unknown))</p>
<p>Table 10 :
10
Detailed formulations of evaluation metrics introduced in Section 2.3.Note that p i prior and p i future are considered only if p i new is unknown to the model before knowledge updates, as otherwise no new scientific knowledge is introduced.</p>
<p>Table 11 :
11
The average citation count of 1,000 conference or journal papers published between October 2022 and September 2023 across different domains.
Citation CountComputer Science5.957Medicine4.575Biology5.799Materials Science7.192Psychology3.702Business2.447Political Science1.832Environmental Science5.973Agricultural and Food Sciences4.939Education2.002
AcknowledgmentsThis research was developed with funding from the Defense Advanced Research Projects Agency's (DARPA) SciFy program (Agreement No. HR00112520300), and NSF IIS-2044660.The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U.S. Government.This work was also supported in part by Azure credits from Microsoft Accelerate Foundation Models Research.13) in the Dolma-v1.7[48]pretraining corpus.
Litllm: A toolkit for scientific literature review. Shubham Agarwal, H Issam, Laurent Laradji, Christopher Charlin, Pal, arXiv:2402.017882024arXiv preprint</p>
<p>The transformative impact of large language models on medical writing and publishing: current applications, challenges and future directions. The Korean journal of physiology &amp; pharmacology: official journal of the. Sangzin Ahn, 2024Korean Physiological Society and the Korean Society of Pharmacology</p>
<p>arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023arXiv preprint</p>
<p>Construction of the literature graph in semantic scholar. Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, arXiv:1805.022622018arXiv preprint</p>
<p>Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , arXiv:2411.14199Synthesizing scientific literature with retrieval-augmented lms. 2024arXiv preprint</p>
<p>A general language assistant as a laboratory for alignment. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, arXiv:2112.008612021arXiv preprint</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Simone Clemente, Zied Ben Houidi, Alexis Huet, Dario Rossi, Giulio Franzese, Pietro Michiardi, arXiv:2502.04390praise of stubbornness: The case for cognitive-dissonance-aware knowledge updates in llms. 2025arXiv preprint</p>
<p>Knowledge card: Filling llms' knowledge gaps with plug-in specialized language models. Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov, arXiv:2305.099552023arXiv preprint</p>
<p>When one llm drools. Shangbin Feng, multi-llm collaboration rulesWenxuan Ding, multi-llm collaboration rulesAlisa Liu, multi-llm collaboration rulesZifeng Wang, multi-llm collaboration rulesWeijia Shi, multi-llm collaboration rulesYike Wang, multi-llm collaboration rulesZejiang Shen, multi-llm collaboration rulesXiaochuang Han, multi-llm collaboration rulesHunter Lang, multi-llm collaboration rulesChen-Yu Lee, multi-llm collaboration rulesTomas Pfister, multi-llm collaboration rulesYejin Choi, multi-llm collaboration rulesYulia Tsvetkov, multi-llm collaboration rulesarXiv:2403.178522024arXiv preprint</p>
<p>Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration. Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov, arXiv:2402.003672024arXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, arXiv:2004.109642020arXiv preprint</p>
<p>Knowledge updating? no more model editing! just selective contextual reasoning. Guoxiu He, Xin Song, Aixin Sun, arXiv:2503.052122025arXiv preprint</p>
<p>Parameter-efficient transfer learning for nlp. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International conference on machine learning. PMLR2019</p>
<p>Chime: Llm-assisted hierarchical organization of scientific studies for literature review support. Chao-Chun, Erin Hsu, Jenna Bransom, Bailey Sparks, Chenhao Kuehl, David Tan, Lucy Lu Wadden, Aakanksha Wang, Naik, arXiv:2407.161482024arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.096852021. 2021arXiv preprint</p>
<p>Benchmarking large language models as ai research agents. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo, arXiv:2110.03215Towards continual knowledge learning of language models. 2021arXiv preprint</p>
<p>Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Minjoon Seo, arXiv:2204.142112022arXiv preprint</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, Advances in Neural Information Processing Systems. 202537</p>
<p>Learning to edit: Aligning llms with knowledge editing. Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, arXiv:2402.119052024arXiv preprint</p>
<p>Instruction-tuned language models are better knowledge learners. Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Victoria Xi, Wen-Tau Lin, Srinivasan Yih, Iyer, arXiv:2402.128472024arXiv preprint</p>
<p>Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Proceedings of the national academy of sciences. the national academy of sciences2017114</p>
<p>Thomas S Kuhn, The Structure of Scientific Revolutions. University of Chicago Press1962</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Transactions of the Association for Computational Linguistics. 72019</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Aochong Oliver, Li , Tanya Goyal, arXiv:2504.12523Memorization vs. reasoning: Updating llms with new knowledge. 2025arXiv preprint</p>
<p>Mapping the increasing use of llms in scientific papers. Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, arXiv:2404.012682024arXiv preprint</p>
<p>Infinigram: Scaling unbounded n-gram language models to a trillion tokens. Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh Hajishirzi, arXiv:2401.173772024arXiv preprint</p>
<p>Tianci Liu, Zihan Dong, Linjun Zhang, Haoyu Wang, Jing Gao, arXiv:2502.00602Mitigating heterogeneous token overfitting in llm knowledge editing. 2025arXiv preprint</p>
<p>Leo Zeyu, Shrey Liu, Xi Pandit, Eunsol Ye, Greg Choi, Durrett, arXiv:2407.06249Codeupdatearena: Benchmarking knowledge editing on api updates. 2024arXiv preprint</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025arXiv preprint</p>
<p>Locating and editing factual associations in gpt. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in neural information processing systems. 202235</p>
<p>Massediting memory in a transformer. Kevin Meng, Sen Arnab, Alex Sharma, Yonatan Andonian, David Belinkov, Bau, arXiv:2210.072292022arXiv preprint</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>Arxivdigestables: Synthesizing scientific literature into tables using language models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Joseph Chee Daniel S Weld, Kyle Chang, Lo, arXiv:2410.223602024arXiv preprint</p>
<p>. Team Olmo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, arXiv:2501.006562024arXiv preprintet al. 2 olmo 2 furious</p>
<p>arXiv:2410.21276GPT-4o system card. 2024OpenAIarXiv preprint</p>
<p>Fine-tuning or retrieval? comparing knowledge injection in llms. Oded Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha, arXiv:2312.059342023arXiv preprint</p>
<p>Note on regression and inheritance in the case of two parents. Karl Pearson, 10.1098/rspl.1895.0041Proceedings of the Royal Society of London. 581895</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, ArXiv, abs/2210.033502022</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Learning multiple visual domains with residual adapters. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, 201730Advances in neural information processing systems</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrieval-augmented black-box language models. 2023arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo, Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. 2024arXiv preprint</p>
<p>Yu Song, Santiago Miret, Huan Zhang, Bang Liu, Honeybee, arXiv:2310.08511Progressive instruction finetuning of large language models for materials science. 2023arXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler, arXiv:2504.09522How new data permeates llm knowledge and how to dilute it. 2025arXiv preprint</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2018arXiv preprint</p>
<p>Wei Tang, Yixin Cao, Yang Deng, Jiahao Ying, Bo Wang, Yizhe Yang, Yuyue Zhao, Qi Zhang, Xuanjing Huang, Yugang Jiang, arXiv:2412.13582Evaluating llms on evolving knowledge. 2024arXiv preprint</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, arXiv:2004.149742020arXiv preprint</p>
<p>Changyue Wang, Weihang Su, Qingyao Hu Yiran, Yueyue Ai, Cheng Wu, Yiqun Luo, Min Liu, Shaoping Zhang, Ma, arXiv:2407.14192Lekube: A legal knowledge update benchmark. 2024arXiv preprint</p>
<p>Wise: Rethinking the knowledge memory for lifelong model editing of large language models. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Advances in Neural Information Processing Systems. 202437</p>
<p>Knowledge editing for large language models: A survey. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li, ACM Computing Surveys. 5732024</p>
<p>Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov, arXiv:2310.00935Resolving knowledge conflicts in large language models. 2023arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>History matters: Temporal knowledge editing in large language model. Xunjian Yin, Jin Jiang, Liming Yang, Xiaojun Wan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, Helen Meng, arXiv:2406.06326Self-tuning: Instructing llms to effectively acquire new knowledge through self-teaching. 2024arXiv preprint</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, arXiv:2406.108332024arXiv preprint</p>
<p>Domain Specialized Tokens Computer Science [Background, chunks, mined, keywords, -res, ourced, Million, logistic, LR, encoder, coder, isolate, solved, imperfect, realized, abrupt, transmitted, connects, ended, tan, imoto, waveform, coefficients, -current, Self, -driving, navigation, drivers, orientation, camera, installed, videos, combinations, Next, Track, substantially, Thanks, mil, king, Trad, itionally, EB, -from, -more, including, dairy, deviations, sequent, lact, Est, herit, splitting, Rem, Mi, Together, encompass, setup, concluding, seaborn, matplotlib, Num, Py, Log, Literary, properly, client, -server. Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, arXiv:2402.16797Temporal alignment of pretrained language models. Client, Server, -Agent2024arXiv preprintSet the clock. Rapid, oring, planner, inverse, kin, ematic, fourth, Transfer, HTTP, send, Wireless, Control, missions, envi, nets, -agent, Path, -aware, preservation, extends, intermediate</p>
<p>Violence, DV, item, DV, observation, attainment, jun, foregoing, divorce, offspring, focused, uns, aturated, UF, palm, Animals, aily, Spatial, lost, -Jan, uary, Identification, Matrix, Laser, Ion, Time, Flight, rometer, MAL, Possible, encountering, opportun, Admission, inertia, RAP, charge, iny, -fe, alan, mem, brane, reversed, PAR, carbohydrate, -chain, aur, -en, rich, chicken, iated, recipients, misconception, abandonment, sustaining, optim, -ag, Enhanced, -k, Da, property, aiding, TJ, Adopt, -condition, polarization. Medicine [logic, Domestic, MohTr, optic, interf, amil, arth, ref, dup, sister, ismus, disclosed, fe</p>
<p>:p, Nav, exponentially, impose, UG, specialised, Laur, Material, -response, iaux, -comp, -death, Cock, ayne, olated, -rate, visceral, Unexpected, dc, Sp, pm, Nos, Statement, Kid, okes, emergency, monitored, urine, elo, album, -sk, ewed, Highly, inherently, paths, quasi, Americans, idi, opathic, ATIC, III, restrictive, Character, omorphic, stature. Di, Twenty, Animals, Tj, Adopt, Br, Nav, Mic, Biology. c,Qu, QS, attracts, basics, realization, solving, oriented, priorities, productive, oking, subt, timely, priority, ials, uns. height, Binding, slow, cognition, BF, doubled, unexpectedly, Well, come, Council, Horizon</p>
<p>Regular, convinced, rows, satin, stitch, ext, skipping, stitches, spent, row, Regression, duce, choose, passes, -cons, istent, VP, -mult, VP, supplemented, strengthened, deflect, meticulous, seems, Hamilton, energetic, warp, shr, mesh, widening, inferior, Spacer, twisting, earlier, oogeneous, alter, retained, vil, abundance, ol, clin, partition, Applying, minute, iles, Tit, -visible, follow, -first, law, Ev, Add, itive, Manufacturing, yx, ylon, chopped, Fil, Fabric, manners, inevitable, Increase, Rotary, sty, ABS, isot, chips, dispens, conforms, Cross, VR, incomplete] Psychology [supporters, Turkish, Federation, league, season, mekte, aca, reside, deploy, February, undergone, referral, .Pre, vious, Using, partner, .Actor, .Al, though, careers, disabling, hab, ilitation, vo, rehabilit, ROT, Reality, pencil, Compar, ventional, uo, -exec, expanding. Even, Rh, Materials Science. wind, -al, gorithm, izable, Simon, scientists, .L, Rub, Brush, insky, Ya, onom, contempor, Kor, la, Login</p>
<p>. F Spi, . N Lap, Sav, Today, attracted, squares, phys, immense, scan, ancers, dancers, Oper, recognizing] Table 12: Specialized tokens by domains</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3297 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3297</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3297</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-e854977a9c4c2effc42f2e24064726fb6307b6f5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e854977a9c4c2effc42f2e24064726fb6307b6f5" target="_blank">Learning with Latent Language</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure and shows that, in all settings, models with a linguistic parameterization outperform those without.</p>
                <p><strong>Paper Abstract:</strong> The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter’s loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3297.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3297.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L^3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning with Latent Language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learning method that uses natural-language strings as a discrete latent parameter space: an interpretation model f maps a candidate description w and input x to outputs, while a proposal model q suggests candidate descriptions; concept learning is performed by searching (sampling) in the space of natural language descriptions and selecting the description that minimizes interpreter loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>L^3 (latent-language parameterization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses RNN-based encoders/decoders: an interpretation model f(x; η, w) (e.g., rnn-encode(w)^T rep(x) with sigmoid for classification, or rnn-decode for transduction) and a proposal model q(w | examples) implemented as an RNN decoder conditioned on pooled example representations; shared visual/state encoders (VGG features + FC for images, or learned state encoder for RL) provide rep(x). Concept learning samples candidate natural language descriptions from q and selects the one with lowest loss under f; for RL, selected description parameterizes a policy (bilinear scoring of actions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['latent natural-language hypothesis search', 'proposal-based sampling of descriptions', 'discrete hypothesis selection via interpreter loss', 'language-conditioned policy parameterization (for RL)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Latent-language hypothesis search: represent candidate hypotheses as natural-language strings w; interpret them with f to score inputs; use a learned proposal model q to sample plausible candidate descriptions given examples (classification/transduction) or from a prior (RL). Select the best-scoring sampled description under f and use it for prediction; optionally fine-tune the induced predictor/policy. For RL, f(w, state) is implemented as bilinear between encoded w and state to produce action probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses a diverse (discrete) set of candidate reasoning hypotheses expressed in natural language and explicitly explores that space by sampling multiple distinct descriptions from q; this contrasts with baselines that use a single real-valued embedding or pooled representation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Few-shot image classification; string transduction (programming by demonstration); treasure-hunting policy search (RL)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Few-shot classification: given four positive exemplars of a visual concept, decide whether a fifth image matches; String transduction: given 5 I/O string examples of an unknown transformation, apply to a sixth; RL treasure hunt: discover rule mapping landmarks+offsets to treasure positions, then navigate and DIG to obtain reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Classification (ShapeWorld): L^3 Val(old)=70%, Val(new)=72%, Val=71%, Test=70% (Table 1). Oracle description L^3 (given ground-truth description) Val=79% Test=78%. String editing (programming by demonstration): L^3 Val=80%, Test=76% (Table 2). Inference/sample-ablation (string editing): using 1 sample from q gives ~66% for natural language; using 100 samples gives ~80% (Table 3). Regular-expression oracle evaluation can reach ~88-90% (Table 3). RL (treasure hunt): qualitatively, L^3 discovers high-scoring policies faster than multitask and scratch baselines (learning curves in Figure 8); max reward for task is 3; L^3 reaches substantially higher reward during concept-learning and after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared against Multitask, Meta, and Meta+Joint baselines across tasks. In classification (Table 1) L^3 outperforms Multitask (57% val overall) and Meta (62%) and Meta+Joint (66%) on Val; L^3=71% overall. In string editing (Table 2) L^3 (80% val, 76% test) outperforms Multitask (54% val, 50% test), Meta (66% val, 62% test), and Meta+Joint (63% val, 59% test). Table 3 shows that sampling more candidate language descriptions (e.g., 100 vs 1) substantially improves L^3 performance (e.g., natural language 1-sample ~66% -> 100-samples ~80%). Regular-expression annotations with an oracle evaluator do best (up to ~90%), but inferred natural language with sampling recovers most of that performance. In RL, L^3 finds high-reward descriptions and policies faster than multitask embeddings and learning from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing hypotheses as natural language (latent linguistic parameterization) improves few-shot generalization across visual, string-transformation, and RL tasks compared to real-valued task embeddings and pooled meta-representations; sampling multiple diverse natural-language hypotheses from a learned proposal model q is crucial (more samples → better performance); inferred natural-language hypotheses sometimes generalize better than human-provided natural-language annotations; when exact formal execution engines (regular-expression oracle) exist they can outperform learned interpreters, but L^3 recovers a large fraction of that performance via sampling and learned interpreters.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Ground-truth regular expressions with an exact execution evaluator outperform L^3 (~88–90% vs ~80%); Meta+Joint sometimes converges faster but attains lower final performance than Meta or L^3 on some tasks; Multitask embeddings perform poorly on novel concepts (e.g., classification Val(new)=49% for Multitask). Providing ground-truth natural-language annotations to the model does not always beat inferred descriptions (humans sometimes write poor/ambiguous descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning with Latent Language', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3297.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3297.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-learning pooled representation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-learning baseline that collapses concept learning and prediction into a single learned mapping: predictions are produced by similarity between a pooled representation of support examples and the query representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta (pooled average prototype-like model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>f(x) = σ([1/n Σ_j rep(x_j)]^T rep(x)); representation networks are the same as other models (VGG features + FC for images; RNN encoders for strings). Trained in a meta-learning style over tasks so the mapping from pooled supports to predictions is learned end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['single pooled-representation similarity reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Compute an average (pooled) representation of support examples and score queries by inner product with the query representation; this implements a single learned similarity-style reasoning method (prototype-like).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses a single, uniform reasoning style (pooled-prototype similarity), not a diverse set of hypothesized explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same set of few-shot tasks as L^3 (classification, string editing)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Few-shot classification and string transduction tasks where concept must be inferred from few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Classification (Table 1): Meta Val(old)=63% Val(new)=62% Val=62% Test=64%. String editing (Table 2): Meta Val=66% Test=62%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Meta performs substantially better than Multitask on novel-concept generalization (e.g., classification Val(new)=62% vs Multitask 49%), but is outperformed by L^3 which leverages language as hypothesis space (L^3 Val=71% overall).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A single pooled representation (prototype-like) gives decent few-shot performance, but lacks the compositional hypothesis-expressivity offered by natural-language latent representations; performs worse than L^3 especially on tasks requiring compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Meta+Joint (which includes auxiliary language prediction) can converge faster during training but does not necessarily achieve higher final performance; Meta falls short of L^3 when language-parameterized hypothesis search is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning with Latent Language', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3297.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3297.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multitask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multitask per-task embedding baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multitask learning baseline that learns task-specific real-valued parameters θ_i during language-learning and concept-learning; predictions use a linear scoring θ_i^T rep(x).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multitask (task embedding classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interpretation replaced by σ(θ_i^T rep(x)) where θ_i are trainable task-specific parameters optimized during pretraining and concept-learning; same rep(x) encoders as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['task-specific parameterized classifier reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each task is assigned a vector θ_i that encodes its behavior; predictions are a linear function of rep(x) parameterized by θ_i. Learning occurs by fitting θ_i to examples.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses per-task continuous parameters (many task-specific vectors) but does not explicitly explore a discrete space of compositional hypotheses; reasoning style is similar across tasks (real-valued parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Few-shot classification; string editing</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same few-shot tasks as other baselines; Multitask uses learned per-task embeddings instead of language hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Classification (Table 1): Multitask Val(old)=64% Val(new)=49% Val=57% Test=59%. String editing (Table 2): Multitask Val=54% Test=50%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Multitask performs worse than Meta and L^3 in generalization to novel concepts; notably poor on Val(new) classification (49%) relative to L^3 (72%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Per-task continuous embeddings can learn tasks seen during pretraining but generalize poorly to new concepts; do not provide the strong compositional inductive bias present in a language-parameterized hypothesis space.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Performs particularly poorly on new/novel concepts compared to L^3 and Meta; adding language as an auxiliary signal (Meta+Joint) helps convergence but not necessarily final quality over Meta or L^3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning with Latent Language', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3297.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3297.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta+Joint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-learning with auxiliary language prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-learning model that predicts outputs via pooled representations like Meta but is jointly trained to also predict natural-language descriptions (an auxiliary objective) during pretraining; description prediction is discarded during concept-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta+Joint</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as Meta for prediction (pooled rep similarity), but the pretraining loss includes an additional term to predict q (the proposal/description), i.e., an auxiliary language-prediction objective intended to inject linguistic bias into representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['pooled representation similarity with auxiliary language supervision']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Reasoning is via a pooled support representation as in Meta; training is augmented by a joint objective to predict natural language descriptions (but at concept-learning time the model does not search over language hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses a similar single reasoning style (pooled similarity) but attempted to bias representations via auxiliary language supervision rather than by using language as a hypothesis space; thus it does not perform discrete hypothesis search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Few-shot classification; string editing</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same few-shot tasks; Meta+Joint uses language only as an auxiliary pretraining signal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Classification (Table 1): Meta+Joint Val(old)=63% Val(new)=69% Val=66% Test=64%. String editing (Table 2): Meta+Joint Val=63% Test=59%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Meta+Joint sometimes improves validation accuracy on previously-seen concepts (e.g., classification Val(new)=69%) relative to Meta, but overall is still outperformed by L^3 where language is the explicit hypothesis space. It converges faster in some settings but attains lower final performance on structured-output tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Auxiliary language prediction can speed convergence and inject some linguistic bias, but using language as the explicit hypothesis space (L^3) yields stronger improvements in final generalization and structured-task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although Meta+Joint converges faster, its final accuracy is lower than Meta on some tasks (e.g., classification test) and noticeably lower than L^3 on string-editing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning with Latent Language', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3297.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3297.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proposal model q (sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proposal distribution over natural-language descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned RNN decoder q(w | examples) trained during language-learning to approximate the posterior distribution over natural-language descriptions given task examples; used at concept-learning time to sample candidate hypotheses to be scored by the interpreter f.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Proposal model q (RNN decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An RNN decoder conditioned on the average pooled representation of support examples (for classification/transduction) or an unconditional prior (for RL) that emits candidate natural-language descriptions; trained by maximizing log-likelihood of human/generative descriptions during language-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['sampling-based generation of diverse natural-language hypotheses']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>q produces multiple distinct candidate descriptions w which are then interpreted by f; exploring more samples increases the diversity of reasoning hypotheses and improves chance of finding a high-scoring (correct-generalizing) hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Explicitly encourages diversity by sampling multiple distinct candidate descriptions; empirical results show more samples (e.g., 100 vs 1) substantially increases performance on transduction tasks (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>String transduction (primary), few-shot classification, RL (uses unconditional prior)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generates candidate natural-language descriptions for use in discrete hypothesis search over possible explanations of support examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>String editing (Table 3): Natural-language annotations with 1 sample yield ~66% exact-match; with 100 samples performance increases to ~80%; Regular expressions: 1-sample ~60% -> 100-samples ~76%; oracle evaluation of REs yields ~88-90%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Sampling many candidates from q is a simple but powerful way to diversify reasoning strategies; the paper reports substantial gains from increasing the number of samples (e.g., natural language 1-sample 66% -> 100-samples 80%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Diversity in sampled natural-language hypotheses is a major factor in L^3's success: more candidate descriptions → higher chance of finding a correct generalizing explanation, particularly for structured-output tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Sampling helps but cannot fully replace an exact formal evaluator: with a perfect RE evaluator, synthesis-based approaches still outperform the learned interpreter (oracle RE ~90%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning with Latent Language', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3297.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3297.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regular-expression oracle (synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ground-truth regular-expression annotations + exact execution evaluator (oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthesis-style upper bound: when target transformations are available as ground-truth regular expressions and an exact execution engine is used to evaluate and rank candidate REs, performance on string-transduction tasks is higher than learned interpreters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regular-expression oracle / RE execution engine</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses ground-truth formal regular-expression descriptions for rules and an exact evaluator to compute outputs; in experiments this setup is used as an oracle upper bound to compare learned natural-language inference against classical synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['formal-program synthesis and exact execution']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Search in a formal discrete space (regular expressions) combined with an exact execution engine that deterministically applies candidate programs to inputs to check correctness; provides an oracle evaluation of how well formal supervision can perform on transduction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Not directly about diverse reasoning styles: the method searches formal program space but benefits from exact semantics and execution, producing highly-accurate specific solutions when the formal language expresses the rule.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>String transduction (programming by demonstration)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks generated from sampled regular transducers applied to words; ground-truth REs are available and can be used to evaluate synthesis approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>String editing (Table 3): with RE annotations and an oracle evaluator, performance reaches ~88% (annotation oracle) and ~90% (evaluation oracle) exact-match; better than learned L^3 (≈80% with sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RE oracle provides an upper bound; L^3 gets close (~80%) but cannot completely match the oracle's ~90% without access to exact execution; however, L^3 has the advantage of not requiring hand-engineered formal primitives and performs better than RE-annotated training when the model must infer language descriptions (natural-language inference sometimes outperforms training with REs if REs are used naively or with insufficient inference).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When a domain-specific, exact execution engine and ground-truth formal descriptions are available, synthesis approaches outperform learned natural-language interpreters; nevertheless, L^3 recovers a large fraction of the oracle performance by sampling and learning an execution model from data.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>RE oracle outperforms L^3 (≈90% vs ≈80%); but REs require domain-specific formalism and an exact executor which may not be available in many tasks (e.g., vision, RL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning with Latent Language', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RobustFill: Neural program learning under noisy I/O <em>(Rating: 2)</em></li>
                <li>RL^2: Fast reinforcement learning via slow reinforcement learning <em>(Rating: 1)</em></li>
                <li>Model-agnostic meta-learning for fast adaptation of deep networks <em>(Rating: 1)</em></li>
                <li>Reinforcement learning for mapping instructions to actions <em>(Rating: 1)</em></li>
                <li>Representation learning for grounded spatial reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3297",
    "paper_id": "paper-e854977a9c4c2effc42f2e24064726fb6307b6f5",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "L^3",
            "name_full": "Learning with Latent Language",
            "brief_description": "A learning method that uses natural-language strings as a discrete latent parameter space: an interpretation model f maps a candidate description w and input x to outputs, while a proposal model q suggests candidate descriptions; concept learning is performed by searching (sampling) in the space of natural language descriptions and selecting the description that minimizes interpreter loss.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "L^3 (latent-language parameterization)",
            "model_description": "Uses RNN-based encoders/decoders: an interpretation model f(x; η, w) (e.g., rnn-encode(w)^T rep(x) with sigmoid for classification, or rnn-decode for transduction) and a proposal model q(w | examples) implemented as an RNN decoder conditioned on pooled example representations; shared visual/state encoders (VGG features + FC for images, or learned state encoder for RL) provide rep(x). Concept learning samples candidate natural language descriptions from q and selects the one with lowest loss under f; for RL, selected description parameterizes a policy (bilinear scoring of actions).",
            "model_size": null,
            "reasoning_methods": [
                "latent natural-language hypothesis search",
                "proposal-based sampling of descriptions",
                "discrete hypothesis selection via interpreter loss",
                "language-conditioned policy parameterization (for RL)"
            ],
            "reasoning_methods_description": "Latent-language hypothesis search: represent candidate hypotheses as natural-language strings w; interpret them with f to score inputs; use a learned proposal model q to sample plausible candidate descriptions given examples (classification/transduction) or from a prior (RL). Select the best-scoring sampled description under f and use it for prediction; optionally fine-tune the induced predictor/policy. For RL, f(w, state) is implemented as bilinear between encoded w and state to produce action probabilities.",
            "diversity_of_methods": "Uses a diverse (discrete) set of candidate reasoning hypotheses expressed in natural language and explicitly explores that space by sampling multiple distinct descriptions from q; this contrasts with baselines that use a single real-valued embedding or pooled representation.",
            "reasoning_task_name": "Few-shot image classification; string transduction (programming by demonstration); treasure-hunting policy search (RL)",
            "reasoning_task_description": "Few-shot classification: given four positive exemplars of a visual concept, decide whether a fifth image matches; String transduction: given 5 I/O string examples of an unknown transformation, apply to a sixth; RL treasure hunt: discover rule mapping landmarks+offsets to treasure positions, then navigate and DIG to obtain reward.",
            "performance_by_method": "Classification (ShapeWorld): L^3 Val(old)=70%, Val(new)=72%, Val=71%, Test=70% (Table 1). Oracle description L^3 (given ground-truth description) Val=79% Test=78%. String editing (programming by demonstration): L^3 Val=80%, Test=76% (Table 2). Inference/sample-ablation (string editing): using 1 sample from q gives ~66% for natural language; using 100 samples gives ~80% (Table 3). Regular-expression oracle evaluation can reach ~88-90% (Table 3). RL (treasure hunt): qualitatively, L^3 discovers high-scoring policies faster than multitask and scratch baselines (learning curves in Figure 8); max reward for task is 3; L^3 reaches substantially higher reward during concept-learning and after fine-tuning.",
            "comparison_of_methods": "Compared against Multitask, Meta, and Meta+Joint baselines across tasks. In classification (Table 1) L^3 outperforms Multitask (57% val overall) and Meta (62%) and Meta+Joint (66%) on Val; L^3=71% overall. In string editing (Table 2) L^3 (80% val, 76% test) outperforms Multitask (54% val, 50% test), Meta (66% val, 62% test), and Meta+Joint (63% val, 59% test). Table 3 shows that sampling more candidate language descriptions (e.g., 100 vs 1) substantially improves L^3 performance (e.g., natural language 1-sample ~66% -&gt; 100-samples ~80%). Regular-expression annotations with an oracle evaluator do best (up to ~90%), but inferred natural language with sampling recovers most of that performance. In RL, L^3 finds high-reward descriptions and policies faster than multitask embeddings and learning from scratch.",
            "key_findings": "Representing hypotheses as natural language (latent linguistic parameterization) improves few-shot generalization across visual, string-transformation, and RL tasks compared to real-valued task embeddings and pooled meta-representations; sampling multiple diverse natural-language hypotheses from a learned proposal model q is crucial (more samples → better performance); inferred natural-language hypotheses sometimes generalize better than human-provided natural-language annotations; when exact formal execution engines (regular-expression oracle) exist they can outperform learned interpreters, but L^3 recovers a large fraction of that performance via sampling and learned interpreters.",
            "counter_examples_or_negative_results": "Ground-truth regular expressions with an exact execution evaluator outperform L^3 (~88–90% vs ~80%); Meta+Joint sometimes converges faster but attains lower final performance than Meta or L^3 on some tasks; Multitask embeddings perform poorly on novel concepts (e.g., classification Val(new)=49% for Multitask). Providing ground-truth natural-language annotations to the model does not always beat inferred descriptions (humans sometimes write poor/ambiguous descriptions).",
            "uuid": "e3297.0",
            "source_info": {
                "paper_title": "Learning with Latent Language",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Meta",
            "name_full": "Meta-learning pooled representation baseline",
            "brief_description": "A meta-learning baseline that collapses concept learning and prediction into a single learned mapping: predictions are produced by similarity between a pooled representation of support examples and the query representation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta (pooled average prototype-like model)",
            "model_description": "f(x) = σ([1/n Σ_j rep(x_j)]^T rep(x)); representation networks are the same as other models (VGG features + FC for images; RNN encoders for strings). Trained in a meta-learning style over tasks so the mapping from pooled supports to predictions is learned end-to-end.",
            "model_size": null,
            "reasoning_methods": [
                "single pooled-representation similarity reasoning"
            ],
            "reasoning_methods_description": "Compute an average (pooled) representation of support examples and score queries by inner product with the query representation; this implements a single learned similarity-style reasoning method (prototype-like).",
            "diversity_of_methods": "Uses a single, uniform reasoning style (pooled-prototype similarity), not a diverse set of hypothesized explanations.",
            "reasoning_task_name": "Same set of few-shot tasks as L^3 (classification, string editing)",
            "reasoning_task_description": "Few-shot classification and string transduction tasks where concept must be inferred from few examples.",
            "performance_by_method": "Classification (Table 1): Meta Val(old)=63% Val(new)=62% Val=62% Test=64%. String editing (Table 2): Meta Val=66% Test=62%.",
            "comparison_of_methods": "Meta performs substantially better than Multitask on novel-concept generalization (e.g., classification Val(new)=62% vs Multitask 49%), but is outperformed by L^3 which leverages language as hypothesis space (L^3 Val=71% overall).",
            "key_findings": "A single pooled representation (prototype-like) gives decent few-shot performance, but lacks the compositional hypothesis-expressivity offered by natural-language latent representations; performs worse than L^3 especially on tasks requiring compositional generalization.",
            "counter_examples_or_negative_results": "Meta+Joint (which includes auxiliary language prediction) can converge faster during training but does not necessarily achieve higher final performance; Meta falls short of L^3 when language-parameterized hypothesis search is possible.",
            "uuid": "e3297.1",
            "source_info": {
                "paper_title": "Learning with Latent Language",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Multitask",
            "name_full": "Multitask per-task embedding baseline",
            "brief_description": "A multitask learning baseline that learns task-specific real-valued parameters θ_i during language-learning and concept-learning; predictions use a linear scoring θ_i^T rep(x).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multitask (task embedding classifier)",
            "model_description": "Interpretation replaced by σ(θ_i^T rep(x)) where θ_i are trainable task-specific parameters optimized during pretraining and concept-learning; same rep(x) encoders as other models.",
            "model_size": null,
            "reasoning_methods": [
                "task-specific parameterized classifier reasoning"
            ],
            "reasoning_methods_description": "Each task is assigned a vector θ_i that encodes its behavior; predictions are a linear function of rep(x) parameterized by θ_i. Learning occurs by fitting θ_i to examples.",
            "diversity_of_methods": "Uses per-task continuous parameters (many task-specific vectors) but does not explicitly explore a discrete space of compositional hypotheses; reasoning style is similar across tasks (real-valued parameters).",
            "reasoning_task_name": "Few-shot classification; string editing",
            "reasoning_task_description": "Same few-shot tasks as other baselines; Multitask uses learned per-task embeddings instead of language hypotheses.",
            "performance_by_method": "Classification (Table 1): Multitask Val(old)=64% Val(new)=49% Val=57% Test=59%. String editing (Table 2): Multitask Val=54% Test=50%.",
            "comparison_of_methods": "Multitask performs worse than Meta and L^3 in generalization to novel concepts; notably poor on Val(new) classification (49%) relative to L^3 (72%).",
            "key_findings": "Per-task continuous embeddings can learn tasks seen during pretraining but generalize poorly to new concepts; do not provide the strong compositional inductive bias present in a language-parameterized hypothesis space.",
            "counter_examples_or_negative_results": "Performs particularly poorly on new/novel concepts compared to L^3 and Meta; adding language as an auxiliary signal (Meta+Joint) helps convergence but not necessarily final quality over Meta or L^3.",
            "uuid": "e3297.2",
            "source_info": {
                "paper_title": "Learning with Latent Language",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Meta+Joint",
            "name_full": "Meta-learning with auxiliary language prediction",
            "brief_description": "A meta-learning model that predicts outputs via pooled representations like Meta but is jointly trained to also predict natural-language descriptions (an auxiliary objective) during pretraining; description prediction is discarded during concept-learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta+Joint",
            "model_description": "Same architecture as Meta for prediction (pooled rep similarity), but the pretraining loss includes an additional term to predict q (the proposal/description), i.e., an auxiliary language-prediction objective intended to inject linguistic bias into representations.",
            "model_size": null,
            "reasoning_methods": [
                "pooled representation similarity with auxiliary language supervision"
            ],
            "reasoning_methods_description": "Reasoning is via a pooled support representation as in Meta; training is augmented by a joint objective to predict natural language descriptions (but at concept-learning time the model does not search over language hypotheses).",
            "diversity_of_methods": "Uses a similar single reasoning style (pooled similarity) but attempted to bias representations via auxiliary language supervision rather than by using language as a hypothesis space; thus it does not perform discrete hypothesis search.",
            "reasoning_task_name": "Few-shot classification; string editing",
            "reasoning_task_description": "Same few-shot tasks; Meta+Joint uses language only as an auxiliary pretraining signal.",
            "performance_by_method": "Classification (Table 1): Meta+Joint Val(old)=63% Val(new)=69% Val=66% Test=64%. String editing (Table 2): Meta+Joint Val=63% Test=59%.",
            "comparison_of_methods": "Meta+Joint sometimes improves validation accuracy on previously-seen concepts (e.g., classification Val(new)=69%) relative to Meta, but overall is still outperformed by L^3 where language is the explicit hypothesis space. It converges faster in some settings but attains lower final performance on structured-output tasks.",
            "key_findings": "Auxiliary language prediction can speed convergence and inject some linguistic bias, but using language as the explicit hypothesis space (L^3) yields stronger improvements in final generalization and structured-task performance.",
            "counter_examples_or_negative_results": "Although Meta+Joint converges faster, its final accuracy is lower than Meta on some tasks (e.g., classification test) and noticeably lower than L^3 on string-editing tasks.",
            "uuid": "e3297.3",
            "source_info": {
                "paper_title": "Learning with Latent Language",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Proposal model q (sampling)",
            "name_full": "Proposal distribution over natural-language descriptions",
            "brief_description": "A learned RNN decoder q(w | examples) trained during language-learning to approximate the posterior distribution over natural-language descriptions given task examples; used at concept-learning time to sample candidate hypotheses to be scored by the interpreter f.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Proposal model q (RNN decoder)",
            "model_description": "An RNN decoder conditioned on the average pooled representation of support examples (for classification/transduction) or an unconditional prior (for RL) that emits candidate natural-language descriptions; trained by maximizing log-likelihood of human/generative descriptions during language-learning.",
            "model_size": null,
            "reasoning_methods": [
                "sampling-based generation of diverse natural-language hypotheses"
            ],
            "reasoning_methods_description": "q produces multiple distinct candidate descriptions w which are then interpreted by f; exploring more samples increases the diversity of reasoning hypotheses and improves chance of finding a high-scoring (correct-generalizing) hypothesis.",
            "diversity_of_methods": "Explicitly encourages diversity by sampling multiple distinct candidate descriptions; empirical results show more samples (e.g., 100 vs 1) substantially increases performance on transduction tasks (Table 3).",
            "reasoning_task_name": "String transduction (primary), few-shot classification, RL (uses unconditional prior)",
            "reasoning_task_description": "Generates candidate natural-language descriptions for use in discrete hypothesis search over possible explanations of support examples.",
            "performance_by_method": "String editing (Table 3): Natural-language annotations with 1 sample yield ~66% exact-match; with 100 samples performance increases to ~80%; Regular expressions: 1-sample ~60% -&gt; 100-samples ~76%; oracle evaluation of REs yields ~88-90%.",
            "comparison_of_methods": "Sampling many candidates from q is a simple but powerful way to diversify reasoning strategies; the paper reports substantial gains from increasing the number of samples (e.g., natural language 1-sample 66% -&gt; 100-samples 80%).",
            "key_findings": "Diversity in sampled natural-language hypotheses is a major factor in L^3's success: more candidate descriptions → higher chance of finding a correct generalizing explanation, particularly for structured-output tasks.",
            "counter_examples_or_negative_results": "Sampling helps but cannot fully replace an exact formal evaluator: with a perfect RE evaluator, synthesis-based approaches still outperform the learned interpreter (oracle RE ~90%).",
            "uuid": "e3297.4",
            "source_info": {
                "paper_title": "Learning with Latent Language",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Regular-expression oracle (synthesis)",
            "name_full": "Ground-truth regular-expression annotations + exact execution evaluator (oracle)",
            "brief_description": "A synthesis-style upper bound: when target transformations are available as ground-truth regular expressions and an exact execution engine is used to evaluate and rank candidate REs, performance on string-transduction tasks is higher than learned interpreters.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Regular-expression oracle / RE execution engine",
            "model_description": "Uses ground-truth formal regular-expression descriptions for rules and an exact evaluator to compute outputs; in experiments this setup is used as an oracle upper bound to compare learned natural-language inference against classical synthesis.",
            "model_size": null,
            "reasoning_methods": [
                "formal-program synthesis and exact execution"
            ],
            "reasoning_methods_description": "Search in a formal discrete space (regular expressions) combined with an exact execution engine that deterministically applies candidate programs to inputs to check correctness; provides an oracle evaluation of how well formal supervision can perform on transduction tasks.",
            "diversity_of_methods": "Not directly about diverse reasoning styles: the method searches formal program space but benefits from exact semantics and execution, producing highly-accurate specific solutions when the formal language expresses the rule.",
            "reasoning_task_name": "String transduction (programming by demonstration)",
            "reasoning_task_description": "Tasks generated from sampled regular transducers applied to words; ground-truth REs are available and can be used to evaluate synthesis approaches.",
            "performance_by_method": "String editing (Table 3): with RE annotations and an oracle evaluator, performance reaches ~88% (annotation oracle) and ~90% (evaluation oracle) exact-match; better than learned L^3 (≈80% with sampling).",
            "comparison_of_methods": "RE oracle provides an upper bound; L^3 gets close (~80%) but cannot completely match the oracle's ~90% without access to exact execution; however, L^3 has the advantage of not requiring hand-engineered formal primitives and performs better than RE-annotated training when the model must infer language descriptions (natural-language inference sometimes outperforms training with REs if REs are used naively or with insufficient inference).",
            "key_findings": "When a domain-specific, exact execution engine and ground-truth formal descriptions are available, synthesis approaches outperform learned natural-language interpreters; nevertheless, L^3 recovers a large fraction of the oracle performance by sampling and learning an execution model from data.",
            "counter_examples_or_negative_results": "RE oracle outperforms L^3 (≈90% vs ≈80%); but REs require domain-specific formalism and an exact executor which may not be available in many tasks (e.g., vision, RL).",
            "uuid": "e3297.5",
            "source_info": {
                "paper_title": "Learning with Latent Language",
                "publication_date_yy_mm": "2017-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RobustFill: Neural program learning under noisy I/O",
            "rating": 2
        },
        {
            "paper_title": "RL^2: Fast reinforcement learning via slow reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "rating": 1
        },
        {
            "paper_title": "Reinforcement learning for mapping instructions to actions",
            "rating": 1
        },
        {
            "paper_title": "Representation learning for grounded spatial reasoning",
            "rating": 1
        }
    ],
    "cost": 0.01543525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning with Latent Language</h1>
<p>Jacob Andreas Dan Klein Sergey Levine<br>Computer Science Division<br>University of California, Berkeley<br>{jda, klein,svlevine}@eecs.berkeley.edu</p>
<h4>Abstract</h4>
<p>The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter's loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The structure of natural language reflects the structure of the world. For example, the fact that it is easy for humans to communicate the concept left of the circle but comparatively difficult to communicate mean saturation of the first five pixels in the third column reveals something about the abstractions we find useful for interpreting and navigating our environment (Gopnik and Meltzoff, 1987). In machine learning, efficient automatic discovery of reusable abstract structure remains a major challenge. This paper investigates whether</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of our approach on a binary image classification task. We assume access to a pretrained language interpretation model that outputs the probability that an image matches a given description. To learn a new visual concept, we search in the space of natural language descriptions to maximize the interpretation model's score (top). The chosen description can be used with the interpretation model to classify new images (bottom). Figures best viewed in color.
background knowledge from language can provide a useful scaffold for acquiring it. We specifically propose to use language as a latent parameter space for few-shot learning problems of all kinds, including classification, transduction and policy search. We aim to show that this linguistic parameterization produces models that are both more accurate and more interpretable than direct approaches to few-shot learning.</p>
<p>Like many recent frameworks for multitaskand meta-learning, our approach consists of three phases: a pretraining phase, a concept-learning phase, and an evaluation phase. Here, the product of pretraining is a language interpretation model that maps from descriptions to predictors (e.g. image classifiers or reinforcement learners). Our thesis is that language learning is a powerful, generalpurpose kind of pretraining, even for tasks that do not directly involve language.</p>
<p>New concepts are learned by searching directly in the space of natural language strings to mini-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Formulation of the learning problem. Ultimately, we care about our model's ability to learn a concept from a small number of training examples (b) and successfully generalize it to held-out data (c). In this paper, concept learning is supported by a language learning phase (a) that makes use of natural language annotations on other learning problems. These annotations are not provided for the real target task in (b-c).
mize the loss incurred by the language interpretation model (Figure 1). Especially on tasks that require the learner to model high-level compositional structure shared by training examples, natural language hypotheses serve a threefold purpose: they make it easier to discover these compositional concepts, harder to overfit to few examples, and easier for humans to understand inferred patterns.</p>
<p>Our approach can be implemented using a standard kit of neural components, and is simple and general. In a variety of settings, we find that the structure imposed by a natural-language parameterization is helpful for efficient learning and exploration. The approach outperforms both multitask- and meta-learning approaches that map directly from training examples to outputs by way of a real-valued parameterization, as well as approaches that make use of natural language annotations as an additional supervisory signal rather than an explicit latent parameter. The natural language concept descriptions inferred by our approach often agree with human annotations when they are correct, and provide an interpretable debugging signal when incorrect. In short, by equipping models with the ability to "think out loud" when learning, they become both more comprehensible and more accurate.</p>
<h2>2 Background</h2>
<p>Suppose we wish to solve an image classification problem like the one shown in Figure 2b-c, mapping from images $x$ to binary labels $y$. One straightforward way to do this is to solve a learn-
ing problem of the following form:</p>
<p>$$
\underset{\eta \in \mathbf{H}}{\arg \min } \sum_{(x, y)} L(f(x ; \eta), y)
$$</p>
<p>where $L$ is a loss function and $f$ is a richlyparameterized class of models (e.g. convolutional networks) indexed by $\eta$ (e.g. weight matrices) that map from images to labels. Given a new image $x^{\prime}$, $f\left(x^{\prime} ; \eta\right)$ can be used to predict its label.</p>
<p>In the present work, we are particularly interested in few-shot learning problems where the number of $(x, y)$ pairs is small-on the order of five or ten examples. Under these conditions, directly solving Equation 1 is a risky propositionany model class powerful enough to capture the true relation between inputs and outputs is also likely to overfit. For few-shot learning to be successful, extra structure must be supplied to the learner. Existing approaches obtain this structure by either carefully structuring the hypothesis space or providing the learner with alternative training data. The approach we present in this paper combines elements of both, so we begin with a review of existing work.
(Inductive) program synthesis approaches (e.g. Gulwani, 2011) reduce the effective size of the hypothesis class $\mathbf{H}$ by moving the optimization problem out of the continuous space of weight vectors and into a discrete space of formal program descriptors (e.g. regular expressions or Prolog queries). Domain-specific structure like version space algebras (Lau et al., 2003) or type systems (Kitzelmann and Schmid, 2006) can be brought to bear on the search problem, and the bias inherent in the syntax of the formal language provides a strong prior. But while program synthesis techniques are powerful, they are also limited in their application: a human designer must hand-engineer the computational primitives necessary to compactly describe every learnable hypothesis. While reasonable for some applications (like string editing), this is challenging or impossible for others (like computer vision).</p>
<p>An alternative class of multitask learning approaches (Caruana, 1998) import the relevant structure from other learning problems rather than defining it manually (Figure 2a, top). Since we may not know a priori what set of learning problems we ultimately wish to evaluate on, it is useful to think of learning as taking places in three phases:</p>
<ol>
<li>a pretraining (or "meta-training") phase that makes use of various different datasets $i$ with examples $\left{\left(x_{1}^{(\ell)}, y_{1}^{(\ell)}\right), \ldots,\left(x_{n}^{(\ell)}, y_{n}^{(\ell)}\right)\right}$ (Figure 2a)</li>
<li>a concept-learning phase in which the pretrained model is adapted to fit data $\left{\left(x_{1}^{(c)}, y_{1}^{(c)}\right), \ldots,\left(x_{n}^{(c)}, y_{n}^{(c)}\right)\right}$ for a specific new task (Figure 2b)</li>
<li>an evaluation phase in which the learned concept is applied to a new input $x^{(e)}$ to predict $y^{(e)}$ (Figure 2c)</li>
</ol>
<p>In these approaches, learning operates over two collections of parameters: shared parameters $\eta$ and task-specific parameters $\theta$. In pretraining, multitask approaches find:
$\underset{\eta \in \mathbb{R}^{n}, \theta^{(\ell)} \in \mathbb{R}^{b}}{\arg \min } \sum_{i, j} L\left(f\left(x_{j}^{(\ell)} ; \eta, \theta^{(\ell)}\right), y_{j}^{(\ell)}\right)$.
At concept learning time, they solve for:</p>
<p>$$
\underset{\theta^{(c)} \in \mathbb{R}^{b}}{\arg \min } \sum_{j} L\left(f\left(x_{j}^{(c)} ; \eta, \theta^{(c)}\right), y_{j}^{(c)}\right)
$$</p>
<p>on the new dataset, then make predictions for new inputs using $f\left(x^{(e)} ; \eta, \theta^{(c)}\right)$.</p>
<p>Closely related meta-learning approaches (e.g. Schmidhuber, 1987; Santoro et al., 2016; Vinyals et al., 2016) make use of the same data, but collapse the inner optimization over $\theta^{(c)}$ and prediction of $y^{(e)}$ into a single learned model.</p>
<h2>3 Learning with Language</h2>
<p>In this work, we are interested in developing a learning method that enjoys the benefits of both approaches. In particular, we seek an intermediate language of task representations that, like in program synthesis, is both expressive and compact, but like in multitask approaches is learnable directly from training data without domain engineering. We propose to use natural language as this intermediate representation. We call our approach learning with latent language ( $\mathrm{L}^{3}$ ).</p>
<p>Natural language shares many structural advantages with the formal languages used in synthesis approaches: it is discrete, has a rich set of compositional operators, and comes equipped with a natural description length prior. But it also has a considerably more flexible semantics. And crucially, plentiful annotated data exists for learning this semantics: we cannot hand-write a computer
program to recognize a small dog, but we can learn how to do it from image captions. More basically, the set of primitive operators available in language provides a strong prior about the kinds of abstractions that are useful for natural learning problems.</p>
<p>Concretely, we replace the pretraining phase above with a language-learning phase. We assume that at language-learning time we have access to natural-language descriptions $w^{(\ell i)}$ (Figure 2a, bottom). We use these $w$ as parameters, in place of the task-specific parameters $\theta$-that is, we learn a language interpretation model $f(x ; \eta, w)$ that uses shared parameters $\eta$ to turn a description $w$ into a function from inputs to outputs. For the example in Figure 2, $f$ might be an image rating model (Socher et al., 2014) that outputs a scalar judgment $y$ of how well an image $x$ matches a caption $w$.</p>
<p>Because these natural language parameters are observed at language-learning time, we need only learn the real-valued shared parameters $\eta$ used for their interpretation (e.g. the weights of a neural network that implements the image rating model):</p>
<p>$$
\underset{\eta \in \mathbb{R}^{n}}{\arg \min } \sum_{i, j} L\left(f\left(x_{j}^{(\ell)} ; \eta, w^{(\ell)}\right), y_{j}^{(\ell)}\right)
$$</p>
<p>At concept-learning time, conversely, we solve only the part of the optimization problem over natural language strings:</p>
<p>$$
\underset{w^{(c)} \in \Sigma^{*}}{\arg \min } \sum_{j} L\left(f\left(x_{j}^{(c)} ; \eta, w^{(c)}\right), y_{j}^{(c)}\right)
$$</p>
<p>This last step presents something of a challenge. When solving the corresponding optimization problem, synthesis techniques can exploit the algebraic structure of the formal language, while end-to-end learning approaches take advantage of differentiability. Here we can't do either-the language of strings is discrete, and any structure in the interpretation function is wrapped up inside the black box of $f$. Inspired by related techniques aimed at making synthesis more efficient (Devlin et al., 2017), we use learning to help us develop an effective optimization procedure for natural language parameters.</p>
<p>In particular, we simply use the languagelearning datasets, consisting of pairs $\left(x_{j}^{(\ell)}, y_{j}^{(\ell)}\right)$ and descriptions $w_{i}$, to fit a reverse proposal model, estimating:
$\arg \max <em i="i">{\lambda} \sum</em> ; \lambda$ )} \log q\left(w_{i} \mid x_{1}^{(\ell)}, y_{1}^{(\ell)}\right), \ldots, x_{n}^{(\ell)}, y_{n}^{(\ell)</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The few-shot image classification task. Learners are shown four positive examples of a visual concept (left) and must determine whether a fifth image matches the pattern (right). Natural language annotations are provided during language learning but must be inferred for concept learning.
where $q$ provides a (suitably normalized) approximation to the distribution of descriptions given task data. In the running example, this proposal distribution is essentially an image captioning model (Donahue et al., 2015). By sampling from $q$, we expect to obtain candidate descriptions that are likely to obtain small loss. But our ultimate inference criterion is still the true model $f$ : at evaluation time we perform the minimization in Equation 5 by drawing a fixed number of samples, selecting the hypothesis $w^{(e)}$ that obtains the lowest loss, and using $f\left(x^{(e)} ; \eta, w^{(c)}\right)$ to make predictions.</p>
<p>What we have described so far is a generic procedure for equipping collections of related learning problems with a natural language hypothesis space. In Sections 4 and 5, we describe how this procedure can be turned into a concrete algorithm for supervised classification and sequence prediction. In Section 6, we describe how to extend these techniques to reinforcement learning.</p>
<h2>4 Few-shot Classification</h2>
<p>We begin by investigating whether natural language can be used to support high-dimensional few-shot classification. Our focus is on visual reasoning tasks like the one shown in Figure 3. In these problems, the learner is presented with four images, all positive examples of some visual concept like a blue shape near a yellow triangle, and must decide whether a fifth, held-out image matches the same concept. These kinds of reasoning problems have been well-studied in visual question answering settings (Johnson et al., 2017; Suhr et al., 2017). Our version of the problem, where the input and output feature no text data, but an explanation must be inferred, is similar to
the visual reasoning problems proposed by Raven (1936) and Bongard (1968).</p>
<p>To apply the recipe in Section 2, we need to specify an implementation of the interpretation model $f$ and the proposal model $q$. We begin by computing representations of input images $x$. We start with a pre-trained 16-layer VGGNet (Simonyan and Zisserman, 2014). Because spatial information is important for these tasks, we extract a feature representation from the final convolutional layer of the network. This initial featurization is passed through two fully-connected layers to form a final image representation, as follows:
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>We define interpretation and proposal models: ${ }^{2}$</p>
<p>$$
\begin{aligned}
f(x ; w) &amp; =\sigma\left(\operatorname{rnn}-\text { encode }(w)^{\top} \operatorname{rep}(x)\right) \
q\left(w \mid\left{x_{j}\right}\right) &amp; =\operatorname{rnn}-\operatorname{decode}\left(w \mid \frac{1}{n} \sum_{j} \operatorname{rep}\left(x_{j}\right)\right)
\end{aligned}
$$</p>
<p>The interpretation model $f$ outputs the probability that $x$ is assigned a positive class label, and is trained to maximize log-likelihood. Because only positive examples are provided in each language learning set, the proposal model $q$ can be defined in terms of inputs alone. Details regarding training hyperparameters, RNN implementations, etc. may be found in Appendix A.</p>
<p>Our evaluation aims to answer two questions. First, does the addition of language to the learning process provide any benefit over ordinary multitask or meta-learning? Second, is it specifically better to use language as a hypothesis space for concept learning rather than just an additional signal for pretraining? We use several baselines to answer these questions:</p>
<ol>
<li>Multitask: a multitask baseline in which the definition of $f$ above is replaced by $\sigma\left(\theta_{i}^{\top} \operatorname{rep}(x)\right)$ for task-specific parameters $\theta_{i}$ that are optimized during both pretraining and concept-learning.</li>
<li>
<p>Meta: a meta-learning baseline in which $f$ is defined by $\sigma\left(\left[\frac{1}{n} \sum_{j} \operatorname{rep}\left(x_{j}\right)\right]^{\top} \operatorname{rep}(x)\right) .{ }^{3}$
<sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
</li>
<li>
<p>Meta+Joint: as in Meta, but the pretraining objective includes an additional term for predicting $q$ (discarded for concept learning).</p>
</li>
</ol>
<p>We report results on a dataset derived from the ShapeWorld corpus of Kuhnle and Copestake (2017). In this dataset the held-out image matches the target concept $50 \%$ of the time. In the validation and test folds, half of learning problems feature a concept that also appears in the language learning set (but with different exemplar images), while the other half feature both new images and a new concept. Images contain two or three distractor shapes unrelated to the objects that define the target concept. Captions in this dataset were generated from DMRS representations using an HPS grammar (Copestake et al., 2016). (Our remaining experiments use human annotators.) The dataset contains a total of 9000 pretraining tasks and 1000 of each validation and test tasks. More dataset statistics are provided in Appendix B.</p>
<p>Results are shown in Table 1. It can be seen that $\mathrm{L}^{3}$ provides consistent improvements over the baselines, and that these improvements are present both when identifying new instances of previously-learned concepts and when discovering new ones. Some example model predictions are shown in Figure 4. The model often succeeds in making correct predictions, even though its inferred descriptions rarely match the ground truth. Sometimes this is because of inherent ambiguity in the description language (Figure 4a), and sometimes because the model is able to rule out candidates on the basis of partial captions alone (Figure 4 b , where it is sufficient to recognize that the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Val (old)</th>
<th style="text-align: center;">Val (new)</th>
<th style="text-align: center;">Val</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Multitask</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: left;">Meta</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Meta+Joint</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{~L}^{3}$ (ours)</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">$\mathbf{7 1}$</td>
<td style="text-align: center;">$\mathbf{7 0}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{~L}^{3}$ (oracle)</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">78</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation on image classification. Val (old) and Val (new) denote subsets of the validation set that contain respectively previously-used and novel visual concepts. $\mathrm{L}^{3}$ consistently outperforms alternative learning methods based on multitask learning, metalearning, and meta-learning jointly trained to predict descriptions (Meta+Joint). The last row shows results when the model is given a ground-truth concept description rather than having to infer it from examples.
target concept involves a circle). More examples are provided in Appendix C.</p>
<h2>5 Programming by Demonstration</h2>
<p>Next we explore whether the same technique can be applied to tasks that involve more than binary similarity judgments. We focus on structured prediction: specifically a family of string processing tasks. In these tasks, the model is presented with examples of five strings transformed according to some rule; it must then apply an appropriate transformation to a sixth (Figure 5). Learning proceeds as in the previous section, with:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{rep}(x, y)=\operatorname{rnn}-\operatorname{encode}([x, y]) \
&amp; f(y \mid x ; w)= \
&amp; \quad \operatorname{rnn}-\operatorname{decode}(y \mid[\operatorname{rnn}-\operatorname{encode}(x), \operatorname{rnn}-\operatorname{encode}(w)]) \
&amp; q\left(w \mid\left{\left(x_{j}, y_{j}\right)\right}\right)= \
&amp; \quad \operatorname{rnn}-\operatorname{decode}\left(w \left\lvert\, \frac{1}{n} \sum_{j} \operatorname{rep}\left(x_{j}, y_{j}\right)\right)\right.
\end{aligned}
$$</p>
<p>Baselines are analogous to those for classification.
While string editing tasks of the kind shown in Figure 5 are popular in both the programming by demonstration literature (Singh and Gulwani, 2012) and the semantic parsing literature (Kushman and Barzilay, 2013), we are unaware of any datasets that support both learning paradigms at the same time. We have thus created a new dataset of string editing tasks by (1) sampling random regular transducers, (2) applying these transducers to collections of dictionary words, and (3) showing the collected examples to Mechanical Turk users
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Example predictions for image classification. The model achieves high accuracy even though predicted descriptions rarely match the ground truth. High-level structure like the presence of certain shapes or spatial relations is consistently recovered.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Val</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Identity</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: left;">Multitask</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Meta</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: left;">Meta+Joint</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{L}^{3}$</td>
<td style="text-align: center;">$\mathbf{8 0}$</td>
<td style="text-align: center;">$\mathbf{7 6}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for string editing. The reported number is the percentage of cases in which the predicted string exactly matches the reference. $\mathrm{L}^{3}$ is the best performing model; using language data for joint training rather than as a hypothesis space provides little benefit.
and asking them to provide a natural language explanation with their best guess about the underlying rule. The dataset thus features both multiexample learning problems, as well as structured and unstructured annotations for each target concept. There are 3000 tasks for language learning and 500 tasks for each of validation and testing (Appendix B). Annotations are included in the code release for this paper.</p>
<p>Results are shown in Table 2. In these experiments, all models that use descriptions have been trained on the natural language supplied by human annotators. While we did find that the Meta+Joint model converges considerably faster than all the others, its final performance is somewhat lower than the baseline Meta model. As before, $\mathrm{L}^{3}$ outperforms alternative approaches for learning directly from examples with or without descriptions.</p>
<p>Because all of the transduction rules in this dataset were generated from known formal descriptors, these tasks provide an opportunity to perform additional analysis comparing natural language to more structured forms of annotation (since we have access to ground-truth regular expressions) and more conventional synthesis-based methods (since we have access to a ground-truth regular expression execution engine). We additionally investigate the effect of the number of</p>
<table>
<thead>
<tr>
<th style="text-align: center;">warding</th>
<th style="text-align: center;">$\rightarrow$</th>
<th style="text-align: center;">warying</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">curved</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">curved</td>
</tr>
<tr>
<td style="text-align: center;">uranium</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">uranium</td>
</tr>
<tr>
<td style="text-align: center;">pedaled</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">peyaled</td>
</tr>
<tr>
<td style="text-align: center;">drum</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">drum</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Example string editing task. Learners are presented with five examples of strings transformed according to some rule (left), and must apply an appropriate transformation to a sixth string (right). Languagelearning annotations (center) may take the form of either natural language or regular expressions.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Example predictions for string editing.
samples drawn from the proposal model. These results are shown in Table 3.</p>
<p>A few interesting facts stand out. Under the ordinary evaluation condition (with no groundtruth annotations provided), language-learning with natural language data is actually better than language-learning with regular expressions. This might be because the extra diversity helps the model determine the relevant axes of variation and avoid overfitting to individual strings. Allowing the model to do its own inference is also better than providing ground-truth natural language descriptions, suggesting that it is actually better at generalizing from the relevant concepts than our human annotators (who occasionally write things like I have no idea for the inferred rule). Unsurprisingly, with ground truth REs (which unlike the human data are always correct) we can do better than any of the models that require inference. Coupling our inference procedure with an oracle RE evaluator, we essentially recover the synthesisbased approach of Devlin et al. (2017). Our findings are consistent with theirs: when an exact execution engine is available, there is no reason not to use it. But we can get almost $90 \%$ of the way there</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Annotations</th>
<th style="text-align: center;">Samples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Oracle</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">Ann.</td>
<td style="text-align: center;">Eval.</td>
</tr>
<tr>
<td style="text-align: center;">None (Meta)</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Natural language</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Regular expressions</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">90</td>
</tr>
</tbody>
</table>
<p>Table 3: Inference and representation experiments for string editing. Italicized numbers correspond to entries in Table 2. Allowing the model to use multiple samples rather than the 1-best decoder output substantially improves performance. The full model does better with inferred natural language descriptions than either regular expressions or ground-truth natural language.</p>
<p>with an execution model learned from scratch. Examples of model behavior are shown in Figure 6; more may be found in Appendix D.</p>
<h2>6 Policy Search</h2>
<p>The previous two sections examined supervised settings where the learning signal comes from few examples but is readily accessible. In this section, we move to a set of reinforcement learning problems, where the learning signal is instead sparse and time-consuming to obtain. We evaluate on a collection of 2-D treasure hunting tasks. These tasks require the agent to discover a rule that determines the location of buried treasure in a large collection of environments of the kind shown in Figure 7. To recover the treasure, the agent must navigate (while avoiding water) to its goal location, then perform a DIG action. At this point the episode ends; if the treasure is located in the agent's current position, it receives a reward, otherwise it does not. In every task, the treasure has consistently been buried at a fixed position relative to some landmark (in Figure 7 a heart). Both the offset and the identity of the target landmark are unknown to the agent, and the location of the landmark varies across maps. Indeed, there is nothing about the agent's observations or action space to suggest that landmarks and offsets are even the relevant axes of variation across tasks: only the language reveals this structure.</p>
<p>The interaction between language and learning in these tasks is rather different from the supervised settings. In the supervised case, language serves mostly as a guard against overfitting, and
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Example treasure hunting task: the agent is placed in a random environment and must collect a reward that has been hidden at a consistent offset with respect to some landmark. At language-learning time only, natural language instructions and expert policies are provided. The agent must both learn primitive navigation skills, like avoiding water, as well as the highlevel structure of the reward functions for this domain.
can be generated conditioned on a set of preprovided concept-learning observations. Here, agents are free to interact with the environment as much as they need, but receive observations only during interaction. Thus our goal here will be to build agents that can adapt quickly to new environments, rather than requiring them to immediately perform well on held-out data.</p>
<p>Why should we expect $\mathrm{L}^{3}$ to help in this setting? In reinforcement learning, we typically encourage our models to explore by injecting randomness into either the agent's action space or its underlying parameterization. But most random policies exhibit nonsensical behaviors; as a result, it is inefficient both to sample in the space of network weights and to perform policy optimization from a random starting point. Our hope is that when parameters are chosen from within a structured family, a stochastic search in this structured space will only ever consider behaviors corresponding to a reasonable final policy, and in this way discover good behavior faster than ordinary RL.</p>
<p>Here the interpretation model $f$ describes a policy that chooses actions conditioned on the current environment state and a linguistic parameterization. As the agent initially has no observations at all, we simply design the proposal model to generate unconditional samples from a prior over descriptions. Taking $x$ to be an agent's current observation of the environment state, we define a state representation network and models:
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>This parameterization assumes a discrete action space, and assigns to each action a probability proportional to a bilinear function of the encoded description and world state. $f$ is an instruction following model of a kind well-studied in natural language processing (Branavan et al., 2009); the proposal model allows it to generate its own instructions without external direction. To learn, we sample a fixed number of descriptions $w$ from $q$. For each description, we sample multiple rollouts of the policy it induces to obtain an estimate of its average reward. Finally, we take the highest-scoring description and fine-tune its induced policy.</p>
<p>At language-learning time, we assume access to both natural language descriptions of these tar-</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: Treasure hunting reward obtained by each learning algorithm across multiple evaluation environments, after language learning has already taken place (bands show 95% confidence intervals for mean performance). Multitask learns an embedding for each task, while Scratch trains on every task individually. L<sup>3</sup> rapidly discovers high-scoring policies in most environments. Dashed line indicates the end of the concept-learning phase; subsequent performance comes from fine-tuning. The max reward for this task is 3.</p>
<p>get locations provided by human annotators, as well as expert policies for navigating to the location of the treasure. The multitask model we compare to replaces these descriptions with trainable task embeddings.<sup>4</sup> The learner is trained from task-specific expert policies using DAgger (Ross et al., 2011) during the language-learning phase, and adapts to individual environments using "vanilla" policy gradient (Williams, 1992) during the concept-learning phase.</p>
<p>The environment implementation and linguistic annotations are in this case adapted from a natural language navigation dataset originally introduced by Janner et al. (2017). In our version of the problem (Figure 7), the agent begins each episode in a random position on a randomly-chosen map and must attempt to obtain the treasure. Relational concepts describing target locations are reused between language learning and concept-learning phases, but the environments themselves are distinct. For language learning the agent has access to 250 tasks, and is evaluated on an additional 50.</p>
<p>Averaged learning curves for held-out tasks are shown in Figure 8. As expected, reward for the L<sup>3</sup> model remains low during the initial exploration period, but once a description is chosen the score improves rapidly. Immediately L<sup>3</sup> achieves better reward than the multitask baseline, though it is not perfect; this suggests that the interpretation model is somewhat overfit to the pretraining environments. After fine-tuning even better results are rapidly obtained. Example rollouts are visualized in Appendix E. These results show that the model has used the structure provided by language to learn a better representation space for policies—one that facilitates sampling from a distribution over interesting and meaningful behaviors.</p>
<h2>7 Other Related Work</h2>
<p>This is the first approach we are aware of to frame a general learning problem as optimization over a space of natural language strings. However, many closely related ideas have been explored in the literature. String-valued latent variables are widely used in language processing tasks ranging from morphological analysis (Dreyer and Eisner, 2009) to sentence compression (Miao and Blunsom, 2016). Natural language annotations have been used in conjunction with training examples to guide the discovery of logical descriptions of concepts (Ling et al., 2017; Srivastava et al., 2017), and used as an auxiliary loss for training (Frome et al., 2013), analogously to the Meta+Joint baseline in this paper. Structured language-like annotations have been used to improve learning of generalizable structured policies (Oh et al., 2017; Andreas et al., 2017; Denil et al., 2017). Finally, natural language instructions available at concept-learning time (rather than language-learning time) have been used to provide side information to reinforcement learners about high-level strategy (Branavan et al., 2011), environments (Narasimhan et al., 2017) and exploration (Harrison et al., 2017).</p>
<h2>8 Conclusion</h2>
<p>We have presented an approach for learning in a space parameterized by natural language. Using simple models for representation and search in this space, we demonstrated that our approach outperforms standard baselines on classification, structured prediction and reinforcement learning tasks. We believe that these results suggest the following general conclusions:</p>
<p><strong>Language encourages compositional generalization.</strong> Standard deep learning architectures are good at recognizing new instances of familiar</p>
<p><sup>4</sup>In RL, the contribution of L<sup>3</sup> is orthogonal to that of meta-learning—one could use a technique like RL<sup>2</sup> (Duan et al., 2016) to generate candidate descriptions more efficiently, or MAML (Finn et al., 2017) rather than zero-shot reward as the training criterion for the interpretation model.</p>
<p>concepts, but not always at generalizing to new ones. By forcing decisions to pass through a linguistic bottleneck in which the underlying compositional structure of concepts is explicitly expressed, stronger generalization becomes possible.</p>
<p>Language simplifies structured exploration. Natural language scaffolding provides dramatic advantages in problems like reinforcement learning that require exploration: models with latent linguistic parameterizations can limit exploration to a class of behaviors that are likely a priori to be goal-directed and interpretable.</p>
<p>And generally, language can help learning. In multitask settings, it can even improve learning on tasks for which no language data is available at training or test time. While some of these advantages are also provided by techniques built on top of formal languages, natural language is at once more expressive and easier to obtain than formal supervision. We believe this work hints at broader opportunities for using naturally-occurring language data to improve machine learning for tasks of all kinds.</p>
<h2>Acknowledgments</h2>
<p>JA is supported by a Facebook graduate fellowship.</p>
<h2>References</h2>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. 2017. Modular multitask reinforcement learning with policy sketches. In Proceedings of the International Conference on Machine Learning.</p>
<p>Mikhail Moiseevich Bongard. 1968. The recognition problem. Technical report.
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
S.R.K. Branavan, David Silver, and Regina Barzilay. 2011. Learning to win by reading manuals in a Monte-Carlo framework. In Proceedings of the Human Language Technology Conference of the Association for Computational Linguistics.</p>
<p>Rich Caruana. 1998. Multitask learning. In Learning to learn, Springer.</p>
<p>Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 .</p>
<p>Ann A Copestake, Guy Emerson, Michael Wayne Goodman, Matic Horvat, Alexander Kuhnle, and Ewa Muszynska. 2016. Resources for building applications with dependency minimal recursion semantics. In Language Resources and Computation.</p>
<p>Misha Denil, Sergio Gómez Colmenarejo, Serkan Cabi, David Saxton, and Nando de Freitas. 2017. Programmable agents. arXiv preprint arXiv:1706.06383 .</p>
<p>Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. RobustFill: Neural program learning under noisy I/O. In Proceedings of the International Conference on Machine Learning.</p>
<p>Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the Conference on Computer Vision and Pattern Recognition.</p>
<p>Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</p>
<p>Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. 2016. RL ${ }^{2}$ : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779 .</p>
<p>Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the International Conference on Machine Learning.</p>
<p>Andrea Frome, Greg Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, MarcAurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visualsemantic embedding model. In Advances in Neural Information Processing Systems.</p>
<p>Alison Gopnik and Andrew Meltzoff. 1987. The development of categorization in the second year and its relation to other cognitive and linguistic developments. Child Development .</p>
<p>Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. ACM SIGPLAN Notices 46(1).</p>
<p>Brent Harrison, Upol Ehsan, and Mark O Riedl. 2017. Guiding reinforcement learning exploration using natural language. arXiv preprint arXiv:1707.08616</p>
<p>Michael Janner, Karthik Narasimhan, and Regina Barzilay. 2017. Representation learning for grounded spatial reasoning. Transactions of the Association for Computational Linguistics .</p>
<p>Robin Jia and Percy Liang. 2016. Data recombination for neural semantic parsing. arXiv preprint arXiv:1606.03622 .</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Proceedings of the Conference on Computer Vision and Pattern Recognition .</p>
<p>Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations.</p>
<p>Emanuel Kitzelmann and Ute Schmid. 2006. Inductive synthesis of functional programs: An explanation based generalization approach. Journal of Machine Learning Research 7.</p>
<p>Alexander Kuhnle and Ann Copestake. 2017. Shapeworld-a new test methodology for multimodal language understanding. arXiv preprint arXiv:1704.04517 .</p>
<p>Nate Kushman and Regina Barzilay. 2013. Using semantic unification to generate regular expressions from natural language. In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in ccg grammar induction for semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</p>
<p>Tessa Lau, Steven A Wolfman, Pedro Domingos, and Daniel S Weld. 2003. Programming by demonstration using version space algebra. Machine Learning 53(1).</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</p>
<p>Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sentence compression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</p>
<p>Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola. 2017. Deep transfer in reinforcement learning by language grounding. arXiv preprint arXiv:1708.00133 .</p>
<p>Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. 2017. Zero-shot task generalization with multi-task deep reinforcement learning. In Proceedings of the International Conference on Machine Learning.</p>
<p>John C Raven. 1936. Mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive. Unpublished masters thesis, University of London .</p>
<p>Stéphane Ross, Geoffrey J Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics.</p>
<p>Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. 2016. Metalearning with memory-augmented neural networks. In Proceedings of the International Conference on Machine Learning.</p>
<p>Jurgen Schmidhuber. 1987. Evolutionary principles in self-referential learning. On learning how to learn. Diploma thesis, Institut f. Informatik, Tech. Univ. Munich .</p>
<p>K Simonyan and A Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arxiv:1409.1556 .</p>
<p>Rishabh Singh and Sumit Gulwani. 2012. Learning semantic string transformations from examples. In International Conference on Very Large Databases.</p>
<p>Jake Snell, Kevin Swersky, and Richard S Zemel. 2017. Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175 .</p>
<p>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics 2.</p>
<p>Shashank Srivastava, Igor Labutov, and Tom Mitchell. 2017. Joint concept learning and semantic parsing from natural language explanations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</p>
<p>Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. A corpus of natural language for visual reasoning. In 55th Annual Meeting of the Association for Computational Linguistics, ACL.</p>
<p>Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. 2016. Matching networks for one shot learning. In Advances in Neural Information Processing Systems.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning 8(3-4).</p>
<h2>A Model and Training Details</h2>
<p>In all models, RNN encoders and decoders use gated recurrent units (Cho et al., 2014).</p>
<p>Few-shot classification Models are trained with the ADAM optimizer (Kingma and Ba, 2015) with a step size of 0.0001 and batch size of 100. The number of pretraining iterations is tuned based on subsequent concept-learning performance on the development set. Neural network hidden states, task parameters, and word vectors are all of size 512. 10 hypotheses are sampled during for each evaluation task in the concept-learning phase.</p>
<p>Programming by demonstration Training as in the classification task, but with a step size of 0.001 . Hidden states are of size 512 , task parameters of size 128 and word vectors of size 32. 100 hypotheses are sampled for concept learning.</p>
<p>Policy search DAgger (Ross et al., 2011) is used for pre-training and vanilla policy gradient (Williams, 1992) for concept learning. Both learning algorithms use ADAM with a step size of 0.001 and a batch size of 5000 samples. For imitation learning, rollouts are obtained from the expert policy on a schedule with probability $0.95^{t}$ (for $t$ the current epoch). For reinforcement learning, a discount of 0.9 is used. Because this dataset contains no development data, pretraining is run until performance on the pretraining tasks reaches a plateau. Hidden states and task embeddings are of size 64. 100 hypotheses are sampled for concept learning, and 1000 episodes (divided evenly among samples) are used to estimate hypothesis quality before fine-tuning.</p>
<h2>B Dataset Information</h2>
<p>ShapeWorld This is the only fully-synthetic dataset used in our experiments. Each scene features 4 or 5 non-overlapping entities. Descriptions refer to spatial relationships between pairs of entities identified by shape, color, or both. There are 8 colors and 8 shapes. The total vocabulary size is only 30 words, but the dataset contains 2643 distinct captions. Descriptions are on average 12.0 words long.</p>
<p>Regular expressions Annotations were collected from Mechanical Turk users. Each user was presented with the same task as the learner in this paper: they observed five strings being transformed, and had to predict how to transform a
sixth. Only after they correctly generated the heldout word were they asked for a description of the rule. Workers were additionally presented with hints like "look at the beginning of the word" or "look at the vowels". Descriptions are automatically preprocessed to strip punctuation and ensure that every character literal appears as a single token.</p>
<p>The regular expression data has a vocabulary of 1015 rules and a total of 1986 distinct descriptions. Descriptions are on average 12.3 words in length but as long as 46 words in some cases.</p>
<p>Navigation The data used was obtained from Janner et al. (2017). We created our own variant of the dataset containing collections of related tasks. Beginning with the "local" tasks in the dataset, we generated alternative goal positions at fixed offsets from landmarks as described in the main section of this paper. Natural-language descriptions were selected for each task collection from the human annotations provided with the dataset. The vocabulary size is 74 and the number of distinct hints 446. The original action space for the environment is also modified slightly: rather than simply reaching the goal cell (achieved with reasonably high frequency by a policy that takes random moves), we require the agent to commit to an individual goal cell and end the episode with a special DIG action.</p>
<p>Data augmentation Due to their comparatively small size, a data augmentation scheme (Jia and Liang, 2016) is employed for the regular expression and navigation datasets. In particular, wherever a description contains a recognizable entity name (i.e. a character literal or a landmark name), a description template is extracted. These templates are then randomly swapped in at training time on other examples with the same high-level semantics. For example, the description replace first $b$ with $e$ is abstracted to replace first CHAR1 with CHAR2, and can subsequently be specialized to, e.g., replace first $c$ with $d$. This templating is easy to implement because we have access to ground-truth structured concept representations at training time. If these were not available it would be straightforward to employ an automatic template induction system (Kwiatkowski et al., 2011) instead.</p>
<h1>C Examples: ShapeWorld</h1>
<p>(Examples in this and the following appendices were not cherry-picked.)</p>
<h2>Positive examples:</h2>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<h1>D Examples: Regular Expressions</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Example in: <br> mediaeval <br> paneling <br> wafer <br> conventions <br> handsprings</th>
<th style="text-align: center;">Example out: <br> ilediaeval <br> ilameling <br> ilafer <br> ilonventions <br> ilandsprings</th>
<th style="text-align: center;">Human description: <br> leading consonant si replaced with i l <br> Inferred description: <br> first consonant of a word is replaced with i l</th>
<th style="text-align: center;">Input: <br> chaser</th>
<th style="text-align: center;">True out: <br> ilhaser <br> Pred. out: <br> ilhaser</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">uptakes <br> pouching <br> embroidery <br> rebelliousness <br> stoplight</td>
<td style="text-align: center;">uptakes <br> punuching <br> embrunidery <br> rebelliunusness <br> stunplight</td>
<td style="text-align: center;">replace every o with u n <br> change all o to u n</td>
<td style="text-align: center;">regulation</td>
<td style="text-align: center;">regulatiunn <br> regulatinun</td>
</tr>
<tr>
<td style="text-align: center;">fluffiest <br> kidnappers <br> matting <br> griping <br> disagreements</td>
<td style="text-align: center;">fluffiest <br> kidnappers <br> eeatting <br> griping <br> disagreeeeents</td>
<td style="text-align: center;">the leter $m$ is replaced by ee <br> change every $m$ to ee</td>
<td style="text-align: center;">chartering</td>
<td style="text-align: center;">chartering <br> chartering</td>
</tr>
<tr>
<td style="text-align: center;">clandestine <br> limming <br> homes <br> lifeblood <br> inflates</td>
<td style="text-align: center;">clandestine <br> limming <br> homq <br> lifqlood <br> inflatq</td>
<td style="text-align: center;">e <br> where e appears, replace it <br> and the following letter with q</td>
<td style="text-align: center;">gratuity <br> gratuity</td>
<td style="text-align: center;">gratuity</td>
</tr>
<tr>
<td style="text-align: center;">fruitlessly <br> sandier <br> washers <br> revelries <br> dewlaps</td>
<td style="text-align: center;">fruitlessly <br> sandier <br> washemu <br> revelrimu <br> dewlamu</td>
<td style="text-align: center;">if the word ends with an $x$, replace the last two letters with $m u$ <br> change last to m u if consonant</td>
<td style="text-align: center;">prompters</td>
<td style="text-align: center;">promptemu <br> promptemu</td>
</tr>
<tr>
<td style="text-align: center;">ladylike <br> flintlocks <br> student <br> surtaxes <br> bedecks</td>
<td style="text-align: center;">ladylike <br> flintlocknl <br> studenml <br> surtaxenl <br> bedecknl</td>
<td style="text-align: center;">ending consonant is replaced with n 1 <br> drop last two and add n 1</td>
<td style="text-align: center;">initials</td>
<td style="text-align: center;">initialnl <br> initialnl</td>
</tr>
<tr>
<td style="text-align: center;">porringer <br> puddling <br> synagog <br> curtseying <br> monsieur</td>
<td style="text-align: center;">porringeer <br> puddlinge <br> synageoge <br> curtseyinge <br> monsieur</td>
<td style="text-align: center;">add e next to letter g <br> when a letter is preceded by a g, <br> e is added after that letter</td>
<td style="text-align: center;">rag</td>
<td style="text-align: center;">rage <br> rage</td>
</tr>
<tr>
<td style="text-align: center;">trivializes <br> tried <br> tearfully <br> hospitalize <br> patronizing</td>
<td style="text-align: center;">trivializes <br> tried <br> gxarfully <br> gxspitalize <br> gxtronizing</td>
<td style="text-align: center;">replace the 1st 2 letters of the word with a g x if the word begins with a consonant then a vowel <br> if the second letter is a vowel, replace the first two letters with g x</td>
<td style="text-align: center;">landlords</td>
<td style="text-align: center;">gxndlords <br> gxndlords</td>
</tr>
<tr>
<td style="text-align: center;">microseconds <br> antiviral <br> flintlock <br> appreciable <br> stricter</td>
<td style="text-align: center;">microsecnyr <br> antiviral <br> flintloyr <br> appreciabyr <br> stricter</td>
<td style="text-align: center;">replace consonants with y r <br> the last two letters are replaced by y r</td>
<td style="text-align: center;">exertion <br> exertion</td>
<td style="text-align: center;">exertion <br> exertiyr</td>
</tr>
</tbody>
</table>
<h1>E Examples: Navigation</h1>
<p>White breadcrumbs show the path taken by the agent.</p>
<h2>Human description:</h2>
<p>move to the star</p>
<h2>Inferred description:</h2>
<p>reach the star cell
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Suppressing shared parameters $\eta$ and $\lambda$ for clarity.
${ }^{3}$ Many state-of-the-art approaches to meta-learning for classification (e.g. Snell et al., 2017) are not well-defined for possibly-overlapping evaluation classes with only positive examples provideded. Here we have attempted to provide a robust implementation that is as close as possible to the other systems under evaluation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
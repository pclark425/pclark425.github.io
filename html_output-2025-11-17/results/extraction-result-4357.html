<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4357 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4357</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4357</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-280265849</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.10522v1.pdf" target="_blank">DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</a></p>
                <p><strong>Paper Abstract:</strong> We introduce DeepResearchEco, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions—enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity. Source code available at: https://github.com/sciknoworg/deep-research.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4357.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4357.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepResearch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepResearch Eco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive, agentic LLM-based system that controls breadth and depth to iteratively retrieve, summarize, and synthesize scientific literature into structured, JSON-schema-validated reports, surfacing intermediate reasoning (SERP queries, learnings, follow-ups).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DeepResearch Eco (recursive agentic LLM workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Orchestrates a recursive multi-agent pipeline initialized with a user research question and two parameters (breadth and depth). Four sub-agents run in cycles: (1) generate SERP-style queries and research goals via an LLM, (2) search using configurable providers (Firecrawl for general web full text; ORKG Ask for scholarly metadata/abstracts), (3) summarize top-k results by prompting an LLM to emit up to N concise 'learnings' and up to N follow-up questions, and (4) generate a final report that weaves accumulated learnings into a Markdown narrative validated against a JSON schema (single field: reportMarkdown). Breadth controls number of subqueries per level, depth controls recursion layers; at each recursion the LLM is prompted with prior learnings and follow-ups to refine queries. Intermediate artifacts (queries, learnings, URLs) are retained for transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT o3 and GPT o3-mini (OpenAI reasoning-capable models as reported; exact parameter sizes not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Ecology (ecological sciences literature synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>49 research questions; 196 generated reports; mean sources integrated per synthesis varied by config (e.g., up to 192.9 mean sources for d4_b4); uses ORKG Ask index (70+ million articles) or Firecrawl for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Mechanistic causal relationships, multi-step causal chains, temporal thresholds/precision (quantified time windows), quantitative effect sizes and statistical findings reported in the literature (e.g., % changes, recovery times), and cross-study pattern generalizations (context-dependent relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured JSON-schema-validated Markdown reports; structured lists of 'learnings' (concise information-dense insights), follow-up questions, and narrative text embedding numerical values and citations (no formal symbolic equation extraction described).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automated similarity metrics (ROUGE-L, BERTScore with SciBERT, Word Mover's Distance on SciBERT embeddings) for configuration consistency; a multi-dimensional, domain-specific quality scoring pipeline (six metrics: depth, breadth, domain-specific quality, rigor, innovation, information density) implemented with curated vocabularies and regex detectors; qualitative case comparisons and ablation across depth/breadth/model settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Source utilization scaling: up to 21.2× increase (from 9.1 ± 1.7 to 192.9 ± 31.2 mean sources per synthesis between d1_b1 and d4_b4); information density improved 14.9× (sources per 1,000 words); word count increased modestly (1,579 to 2,234 words, ~41.5%); best composite quality score d4_b4 = 0.577 (18% improvement vs d1_b1); BERTScore (SciBERT F1) internal consistency ~0.56 (o3) and ~0.61 (o3-mini); ROUGE-L much lower (~0.14–0.16) as expected; temporal precision and multi-step causal chains increased (d4 produces >3× more multi-step causal chains vs d1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Within-paper comparisons across two LLM variants (o3, o3-mini) and four depth/breadth settings; no direct experimental comparison to external prior systems (e.g., PaperQA/PaperQA2) reported as a quantitative baseline in experiments—positioned conceptually against feed-forward RAG pipelines in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLMs can overlook or misuse fine-grained details; model-specific generation patterns (o3 vs o3-mini) affect wording and structure; reconciling conflicting temporal detail across many sources is challenging; risk of hallucination remains (mitigated by retrieval and traceable URLs but not eliminated); high-parameter runs are computationally expensive (d4_b4 much higher resource use); diminishing returns beyond parameter thresholds; current system lacks interactive human-in-the-loop feedback integration and multimodal (figures/tables) synthesis support (not yet implemented).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4357.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular retrieval-augmented generative agent for scientific question answering that retrieves papers (via Google Scholar), builds an embedding-based chunk database, and uses an LLM to summarize and answer queries over retrieved chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrievalaugmented generative agent for scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperQA (retrieval-augmented generative pipeline for papers)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieves relevant papers using Google Scholar with keyword and year-range queries, constructs an embedding-based chunk database from PDF text, retrieves relevant chunks using maximal marginal relevance, then an LLM summarizes or marks chunks as irrelevant and generates answers using retrieved content plus the model's knowledge. The pipeline includes chunking, dense retrieval, reranking, and summarization stages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature / scholarly papers</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not explicitly described as extracting symbolic laws; used to distill numerical findings and evidence-level summaries (statistical findings and quantitative results present in source chunks).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Summaries and marked relevant/irrelevant text chunks; final generated answers (text).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed in this paper beyond its description; PaperQA applies chunk-level summarization to mitigate noise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Semantic noise and parsing errors in chunking and retrieval; need for reranking and contextual summarization to mitigate irrelevant or noisy chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4357.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of PaperQA into a multi-agent framework that separates retrieval and generation into specialized agents to support tasks beyond Q&A (e.g., citation traversal, contradiction detection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperQA2 multi-agent retrieval-synthesis pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Decomposes pipeline into agents: a paper search agent that reformulates queries and fetches PDFs, a citation traversal agent that expands corpora along citation networks, a gather-evidence agent that retrieves and summarizes text chunks via dense retrieval and reranking, and a generation agent that synthesizes answers from top-ranked evidence. Supports tasks like Wikipedia-style summarization and contradiction detection (ContraCrow).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scholarly literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Provides synthesized findings and can surface contradictory quantitative claims across papers (e.g., used in contradiction detection), but not described as extracting formal mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Summaries, synthesized answers, evidence-ranked chunks; agent-produced artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Applied to downstream tasks including contradiction detection (ContraCrow) and WikiCrow summarization (as reported in original work), but specific validation details not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Prior feed-forward pipelines (single pass) may lack recursion and iterative refinement; PaperQA2 addresses some of these limitations via agent separation but details of quantitative-law extraction are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4357.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ORKG Ask</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scholarly search and exploration system combining large-scale semantic search over a CORE-derived index and LLM-based knowledge extraction to synthesize top-ranked article evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Orkg ask: a neuro-symbolic scholarly search and exploration system.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ORKG Ask (semantic search + LLM synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Performs semantic vector search over a large scholarly index (CORE-derived, ~70+ million articles) using vector similarity (Nomic embeddings reported), returns structured metadata (titles, abstracts, URLs) for top results, and uses an LLM to generate a synthesis from the top-k results (e.g., top 5). Integrates with workflows for structured knowledge extraction and downstream synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Unspecified LLM(s) for synthesis in this paper; embeddings via Nomic embeddings are mentioned for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scholarly literature across disciplines (large-scale corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over a scholarly index of 70+ million articles (corpus size); number processed per query depends on retrieval top-k.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Used to synthesize evidence and extract summarized quantitative findings from top articles (numerical results, statistical findings), not described as producing formal mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured metadata (titles, abstracts, URLs) and LLM-generated syntheses (text summaries) of top-k results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Combined with LLMs4Synthesis and LLM-as-a-judge approaches in the literature; specific validation steps for ORKG Ask in this paper are not detailed beyond system description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Scale of corpus requires strong retrieval ranking; synthesis quality depends on retrieval quality and LLM behaviour; exact LLMs and evaluation details are context-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4357.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs4Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs4Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that structures literature synthesis tasks into distinct categories (paper-wise, methodological, thematic) and leverages LLMs with structured prompts and LLM-as-a-judge evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llms4synthesis: Leveraging large language models for scientific synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLMs4Synthesis (structured synthesis + LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Structures synthesis tasks into paper-wise, methodological, and thematic categories and generates syntheses using structured prompts. Uses an LLM-as-a-judge (reported use of GPT-4 in the referenced work) for evaluating syntheses and applies training/optimization techniques such as reinforcement learning with AI feedback (RLAIF) to fine-tune open-source models (e.g., Mistral-7B) for factuality and clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 used as an LLM-as-a-judge (reported); RLAIF applied to optimize open-source models like Mistral-7B (as reported in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature synthesis (multi-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Framework is aimed at structured synthesis of literature and improving factuality; not described as directly extracting formal quantitative laws, though it aims to surface quantitative findings and methodological comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured syntheses across predefined categories; judged/evaluated outputs (textual syntheses).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>LLM-as-a-judge evaluation (e.g., GPT-4 used to judge syntheses) and optimization via RLAIF for factuality and clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported optimization against baseline open-source models via RLAIF in the referenced work (specific numbers not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on LLM-as-a-judge which may have biases; factuality depends on prompt design, retrieval quality, and feedback signal quality.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4357.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-enabled system that leverages planning and information retrieval to iteratively diversify and refine generated research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Nova</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Nova (iterative planning + retrieval for idea diversification)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines iterative planning with retrieval to diversify generated ideas and mitigate repetition; uses LLM planning modules and retrieval to ground idea generation in literature, supporting multiple refinement stages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Research ideation across domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Primarily focused on idea generation and novelty/diversity, not explicitly on extracting quantitative laws from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Iteratively refined idea descriptions and plans (text); may reference literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Novelty and diversity metrics as reported in the original Nova work (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4357.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IdeaSynth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IdeaSynth</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that organizes evolving research ideas as canvas nodes that receive literature-grounded feedback across iterative stages to deepen exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IdeaSynth (canvas-node iterative ideation grounded in literature)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Represents ideas as nodes on a canvas that evolve through feedback loops grounded in retrieved literature; supports multi-stage refinement of ideas using LLM-generated feedback informed by retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Research ideation / literature-grounded idea synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Focused on ideation; not described as extracting formal quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured ideation artifacts (canvas nodes) and literature-grounded commentary (text).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4357.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain of Ideas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Ideas (CoI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that arranges literature into developmental chains representing how research areas evolve, supporting progressive insight and structured representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of ideas: Revolutionizing research via novel idea development with llm agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Chain of Ideas (literature developmental chains)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Organizes papers and findings into sequential/developmental chains to reflect conceptual evolution, enabling progressive insight generation and structured reasoning across literature via LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scholarly literature organization and ideation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Supports mapping development of concepts and relationships, not specifically extracting mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured chains/graphs of literature-derived ideas and summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4357.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-LLM system for scientific idea generation that recombines research facets (purpose, mechanism, evaluation) using novelty heuristics grounded in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Scideator (facet recombination for idea generation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Recombines research facets drawn from literature (e.g., purpose, mechanism, evaluation) and applies novelty heuristics to generate novel research ideas, leveraging LLMs and literature grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Research ideation grounded in scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Designed for ideation rather than formal law discovery; may surface patterns or mechanistic hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Idea proposals and recombined research facets (text).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4357.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed/implemented framework aiming to automate the end-to-end scientific discovery pipeline from idea generation to experimental design and publication writing using AI agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Scientist (end-to-end autonomous discovery system)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Autonomously coordinates stages of scientific discovery including idea generation, experimental design, execution (in some implementations), analysis, and manuscript drafting using a team of AI agents; intended to close the loop for open-ended discovery. Specific LLM architectures/implementations vary by project and are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific discovery / multiple domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Aims to discover new hypotheses and potential quantitative relationships via autonomous experimentation and analysis; details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Varies; could include experimental protocols, analysis outputs, manuscripts (text) and potentially datasets from automated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not described here; original work may include experimental/automated validation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Autonomy risks, reproducibility, and integration with real-world experimental infrastructures; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4357.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VirSci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VirSci</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A coordinated multi-agent system that generates, critiques, and revises scientific proposals collaboratively to emulate scientific team workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>VirSci (multi-agent collaborative proposal generation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Coordinates teams of virtual agents that separately generate, critique, and revise scientific proposals in a collaborative loop, leveraging LLM-based reasoning to iterate on research concepts and designs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific proposal generation and ideation across domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Supports ideation/critique rather than formal extraction of quantitative laws, but could surface patterns or hypothesized relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Proposal drafts, critiques, and revised texts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4357.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4357.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yescieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yescieval (LLM-as-a-judge for scientific QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach (referenced as 'LLM-as-a-judge') that uses LLMs to evaluate and judge the quality or correctness of syntheses or answers over scientific literature at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Yescieval: Robust llm-as-a-judge for scientific question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-as-a-judge (evaluation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses a high-capability LLM (e.g., GPT-4 reported in related work) as an automated judge to score or validate system outputs (syntheses, answers) for factuality, relevance, and other quality dimensions; employed as a scalable evaluation and feedback mechanism to improve synthesis frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reported related use of GPT-4 as judge in cited works; specific model in Yescieval reference not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific question answering and synthesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not primarily a law-extraction method; used to judge quality of extracted findings and syntheses including numeric claims.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Scores/judgements and narrative critiques (textual evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Serves as an automated validation/evaluation method; original work likely compares to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Reliance on an LLM judge can inherit model biases and evaluation blind spots; LLM-as-judge is an approximate proxy for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paperqa: Retrievalaugmented generative agent for scientific research. <em>(Rating: 2)</em></li>
                <li>Orkg ask: a neuro-symbolic scholarly search and exploration system. <em>(Rating: 2)</em></li>
                <li>Llms4synthesis: Leveraging large language models for scientific synthesis. <em>(Rating: 2)</em></li>
                <li>Paperqa: Retrievalaugmented generative agent for scientific research. <em>(Rating: 1)</em></li>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Nova <em>(Rating: 1)</em></li>
                <li>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. <em>(Rating: 1)</em></li>
                <li>Yescieval: Robust llm-as-a-judge for scientific question answering. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4357",
    "paper_id": "paper-280265849",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "DeepResearch",
            "name_full": "DeepResearch Eco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
            "brief_description": "A recursive, agentic LLM-based system that controls breadth and depth to iteratively retrieve, summarize, and synthesize scientific literature into structured, JSON-schema-validated reports, surfacing intermediate reasoning (SERP queries, learnings, follow-ups).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "DeepResearch Eco (recursive agentic LLM workflow)",
            "method_description": "Orchestrates a recursive multi-agent pipeline initialized with a user research question and two parameters (breadth and depth). Four sub-agents run in cycles: (1) generate SERP-style queries and research goals via an LLM, (2) search using configurable providers (Firecrawl for general web full text; ORKG Ask for scholarly metadata/abstracts), (3) summarize top-k results by prompting an LLM to emit up to N concise 'learnings' and up to N follow-up questions, and (4) generate a final report that weaves accumulated learnings into a Markdown narrative validated against a JSON schema (single field: reportMarkdown). Breadth controls number of subqueries per level, depth controls recursion layers; at each recursion the LLM is prompted with prior learnings and follow-ups to refine queries. Intermediate artifacts (queries, learnings, URLs) are retained for transparency.",
            "llm_model_used": "GPT o3 and GPT o3-mini (OpenAI reasoning-capable models as reported; exact parameter sizes not specified)",
            "scientific_domain": "Ecology (ecological sciences literature synthesis)",
            "number_of_papers": "49 research questions; 196 generated reports; mean sources integrated per synthesis varied by config (e.g., up to 192.9 mean sources for d4_b4); uses ORKG Ask index (70+ million articles) or Firecrawl for retrieval",
            "type_of_quantitative_law": "Mechanistic causal relationships, multi-step causal chains, temporal thresholds/precision (quantified time windows), quantitative effect sizes and statistical findings reported in the literature (e.g., % changes, recovery times), and cross-study pattern generalizations (context-dependent relationships).",
            "extraction_output_format": "Structured JSON-schema-validated Markdown reports; structured lists of 'learnings' (concise information-dense insights), follow-up questions, and narrative text embedding numerical values and citations (no formal symbolic equation extraction described).",
            "validation_method": "Automated similarity metrics (ROUGE-L, BERTScore with SciBERT, Word Mover's Distance on SciBERT embeddings) for configuration consistency; a multi-dimensional, domain-specific quality scoring pipeline (six metrics: depth, breadth, domain-specific quality, rigor, innovation, information density) implemented with curated vocabularies and regex detectors; qualitative case comparisons and ablation across depth/breadth/model settings.",
            "performance_metrics": "Source utilization scaling: up to 21.2× increase (from 9.1 ± 1.7 to 192.9 ± 31.2 mean sources per synthesis between d1_b1 and d4_b4); information density improved 14.9× (sources per 1,000 words); word count increased modestly (1,579 to 2,234 words, ~41.5%); best composite quality score d4_b4 = 0.577 (18% improvement vs d1_b1); BERTScore (SciBERT F1) internal consistency ~0.56 (o3) and ~0.61 (o3-mini); ROUGE-L much lower (~0.14–0.16) as expected; temporal precision and multi-step causal chains increased (d4 produces &gt;3× more multi-step causal chains vs d1).",
            "baseline_comparison": "Within-paper comparisons across two LLM variants (o3, o3-mini) and four depth/breadth settings; no direct experimental comparison to external prior systems (e.g., PaperQA/PaperQA2) reported as a quantitative baseline in experiments—positioned conceptually against feed-forward RAG pipelines in related work.",
            "challenges_limitations": "LLMs can overlook or misuse fine-grained details; model-specific generation patterns (o3 vs o3-mini) affect wording and structure; reconciling conflicting temporal detail across many sources is challenging; risk of hallucination remains (mitigated by retrieval and traceable URLs but not eliminated); high-parameter runs are computationally expensive (d4_b4 much higher resource use); diminishing returns beyond parameter thresholds; current system lacks interactive human-in-the-loop feedback integration and multimodal (figures/tables) synthesis support (not yet implemented).",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4357.0",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA",
            "name_full": "PaperQA",
            "brief_description": "A modular retrieval-augmented generative agent for scientific question answering that retrieves papers (via Google Scholar), builds an embedding-based chunk database, and uses an LLM to summarize and answer queries over retrieved chunks.",
            "citation_title": "Paperqa: Retrievalaugmented generative agent for scientific research.",
            "mention_or_use": "mention",
            "method_name": "PaperQA (retrieval-augmented generative pipeline for papers)",
            "method_description": "Retrieves relevant papers using Google Scholar with keyword and year-range queries, constructs an embedding-based chunk database from PDF text, retrieves relevant chunks using maximal marginal relevance, then an LLM summarizes or marks chunks as irrelevant and generates answers using retrieved content plus the model's knowledge. The pipeline includes chunking, dense retrieval, reranking, and summarization stages.",
            "llm_model_used": null,
            "scientific_domain": "General scientific literature / scholarly papers",
            "number_of_papers": null,
            "type_of_quantitative_law": "Not explicitly described as extracting symbolic laws; used to distill numerical findings and evidence-level summaries (statistical findings and quantitative results present in source chunks).",
            "extraction_output_format": "Summaries and marked relevant/irrelevant text chunks; final generated answers (text).",
            "validation_method": "Not detailed in this paper beyond its description; PaperQA applies chunk-level summarization to mitigate noise.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Semantic noise and parsing errors in chunking and retrieval; need for reranking and contextual summarization to mitigate irrelevant or noisy chunks.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.1",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2",
            "brief_description": "An extension of PaperQA into a multi-agent framework that separates retrieval and generation into specialized agents to support tasks beyond Q&A (e.g., citation traversal, contradiction detection).",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "PaperQA2 multi-agent retrieval-synthesis pipeline",
            "method_description": "Decomposes pipeline into agents: a paper search agent that reformulates queries and fetches PDFs, a citation traversal agent that expands corpora along citation networks, a gather-evidence agent that retrieves and summarizes text chunks via dense retrieval and reranking, and a generation agent that synthesizes answers from top-ranked evidence. Supports tasks like Wikipedia-style summarization and contradiction detection (ContraCrow).",
            "llm_model_used": null,
            "scientific_domain": "General scholarly literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "Provides synthesized findings and can surface contradictory quantitative claims across papers (e.g., used in contradiction detection), but not described as extracting formal mathematical laws.",
            "extraction_output_format": "Summaries, synthesized answers, evidence-ranked chunks; agent-produced artifacts.",
            "validation_method": "Applied to downstream tasks including contradiction detection (ContraCrow) and WikiCrow summarization (as reported in original work), but specific validation details not reproduced here.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Prior feed-forward pipelines (single pass) may lack recursion and iterative refinement; PaperQA2 addresses some of these limitations via agent separation but details of quantitative-law extraction are not provided here.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.2",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "ORKG Ask",
            "name_full": "ORKG Ask",
            "brief_description": "A scholarly search and exploration system combining large-scale semantic search over a CORE-derived index and LLM-based knowledge extraction to synthesize top-ranked article evidence.",
            "citation_title": "Orkg ask: a neuro-symbolic scholarly search and exploration system.",
            "mention_or_use": "mention",
            "method_name": "ORKG Ask (semantic search + LLM synthesis)",
            "method_description": "Performs semantic vector search over a large scholarly index (CORE-derived, ~70+ million articles) using vector similarity (Nomic embeddings reported), returns structured metadata (titles, abstracts, URLs) for top results, and uses an LLM to generate a synthesis from the top-k results (e.g., top 5). Integrates with workflows for structured knowledge extraction and downstream synthesis tasks.",
            "llm_model_used": "Unspecified LLM(s) for synthesis in this paper; embeddings via Nomic embeddings are mentioned for retrieval.",
            "scientific_domain": "Scholarly literature across disciplines (large-scale corpus)",
            "number_of_papers": "Operates over a scholarly index of 70+ million articles (corpus size); number processed per query depends on retrieval top-k.",
            "type_of_quantitative_law": "Used to synthesize evidence and extract summarized quantitative findings from top articles (numerical results, statistical findings), not described as producing formal mathematical laws.",
            "extraction_output_format": "Structured metadata (titles, abstracts, URLs) and LLM-generated syntheses (text summaries) of top-k results.",
            "validation_method": "Combined with LLMs4Synthesis and LLM-as-a-judge approaches in the literature; specific validation steps for ORKG Ask in this paper are not detailed beyond system description.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Scale of corpus requires strong retrieval ranking; synthesis quality depends on retrieval quality and LLM behaviour; exact LLMs and evaluation details are context-dependent.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.3",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "LLMs4Synthesis",
            "name_full": "LLMs4Synthesis",
            "brief_description": "A framework that structures literature synthesis tasks into distinct categories (paper-wise, methodological, thematic) and leverages LLMs with structured prompts and LLM-as-a-judge evaluation.",
            "citation_title": "Llms4synthesis: Leveraging large language models for scientific synthesis.",
            "mention_or_use": "mention",
            "method_name": "LLMs4Synthesis (structured synthesis + LLM-as-a-judge)",
            "method_description": "Structures synthesis tasks into paper-wise, methodological, and thematic categories and generates syntheses using structured prompts. Uses an LLM-as-a-judge (reported use of GPT-4 in the referenced work) for evaluating syntheses and applies training/optimization techniques such as reinforcement learning with AI feedback (RLAIF) to fine-tune open-source models (e.g., Mistral-7B) for factuality and clarity.",
            "llm_model_used": "GPT-4 used as an LLM-as-a-judge (reported); RLAIF applied to optimize open-source models like Mistral-7B (as reported in related work).",
            "scientific_domain": "General scientific literature synthesis (multi-domain)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Framework is aimed at structured synthesis of literature and improving factuality; not described as directly extracting formal quantitative laws, though it aims to surface quantitative findings and methodological comparisons.",
            "extraction_output_format": "Structured syntheses across predefined categories; judged/evaluated outputs (textual syntheses).",
            "validation_method": "LLM-as-a-judge evaluation (e.g., GPT-4 used to judge syntheses) and optimization via RLAIF for factuality and clarity.",
            "performance_metrics": null,
            "baseline_comparison": "Reported optimization against baseline open-source models via RLAIF in the referenced work (specific numbers not provided in this paper).",
            "challenges_limitations": "Relies on LLM-as-a-judge which may have biases; factuality depends on prompt design, retrieval quality, and feedback signal quality.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.4",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Nova",
            "name_full": "Nova",
            "brief_description": "An LLM-enabled system that leverages planning and information retrieval to iteratively diversify and refine generated research ideas.",
            "citation_title": "An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Nova",
            "mention_or_use": "mention",
            "method_name": "Nova (iterative planning + retrieval for idea diversification)",
            "method_description": "Combines iterative planning with retrieval to diversify generated ideas and mitigate repetition; uses LLM planning modules and retrieval to ground idea generation in literature, supporting multiple refinement stages.",
            "llm_model_used": null,
            "scientific_domain": "Research ideation across domains",
            "number_of_papers": null,
            "type_of_quantitative_law": "Primarily focused on idea generation and novelty/diversity, not explicitly on extracting quantitative laws from literature.",
            "extraction_output_format": "Iteratively refined idea descriptions and plans (text); may reference literature.",
            "validation_method": "Novelty and diversity metrics as reported in the original Nova work (not reproduced here).",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.5",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "IdeaSynth",
            "name_full": "IdeaSynth",
            "brief_description": "A system that organizes evolving research ideas as canvas nodes that receive literature-grounded feedback across iterative stages to deepen exploration.",
            "citation_title": "Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback.",
            "mention_or_use": "mention",
            "method_name": "IdeaSynth (canvas-node iterative ideation grounded in literature)",
            "method_description": "Represents ideas as nodes on a canvas that evolve through feedback loops grounded in retrieved literature; supports multi-stage refinement of ideas using LLM-generated feedback informed by retrieval.",
            "llm_model_used": null,
            "scientific_domain": "Research ideation / literature-grounded idea synthesis",
            "number_of_papers": null,
            "type_of_quantitative_law": "Focused on ideation; not described as extracting formal quantitative laws.",
            "extraction_output_format": "Structured ideation artifacts (canvas nodes) and literature-grounded commentary (text).",
            "validation_method": "Not detailed in this paper.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.6",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Chain of Ideas",
            "name_full": "Chain of Ideas (CoI)",
            "brief_description": "An approach that arranges literature into developmental chains representing how research areas evolve, supporting progressive insight and structured representation.",
            "citation_title": "Chain of ideas: Revolutionizing research via novel idea development with llm agents.",
            "mention_or_use": "mention",
            "method_name": "Chain of Ideas (literature developmental chains)",
            "method_description": "Organizes papers and findings into sequential/developmental chains to reflect conceptual evolution, enabling progressive insight generation and structured reasoning across literature via LLM agents.",
            "llm_model_used": null,
            "scientific_domain": "Scholarly literature organization and ideation",
            "number_of_papers": null,
            "type_of_quantitative_law": "Supports mapping development of concepts and relationships, not specifically extracting mathematical laws.",
            "extraction_output_format": "Structured chains/graphs of literature-derived ideas and summaries.",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.7",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Scideator",
            "name_full": "Scideator",
            "brief_description": "A human-LLM system for scientific idea generation that recombines research facets (purpose, mechanism, evaluation) using novelty heuristics grounded in literature.",
            "citation_title": "Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination.",
            "mention_or_use": "mention",
            "method_name": "Scideator (facet recombination for idea generation)",
            "method_description": "Recombines research facets drawn from literature (e.g., purpose, mechanism, evaluation) and applies novelty heuristics to generate novel research ideas, leveraging LLMs and literature grounding.",
            "llm_model_used": null,
            "scientific_domain": "Research ideation grounded in scientific literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "Designed for ideation rather than formal law discovery; may surface patterns or mechanistic hypotheses.",
            "extraction_output_format": "Idea proposals and recombined research facets (text).",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.8",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "AI Scientist",
            "name_full": "The AI Scientist",
            "brief_description": "A proposed/implemented framework aiming to automate the end-to-end scientific discovery pipeline from idea generation to experimental design and publication writing using AI agents.",
            "citation_title": "The AI Scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "method_name": "AI Scientist (end-to-end autonomous discovery system)",
            "method_description": "Autonomously coordinates stages of scientific discovery including idea generation, experimental design, execution (in some implementations), analysis, and manuscript drafting using a team of AI agents; intended to close the loop for open-ended discovery. Specific LLM architectures/implementations vary by project and are not detailed in this paper.",
            "llm_model_used": null,
            "scientific_domain": "General scientific discovery / multiple domains",
            "number_of_papers": null,
            "type_of_quantitative_law": "Aims to discover new hypotheses and potential quantitative relationships via autonomous experimentation and analysis; details not provided here.",
            "extraction_output_format": "Varies; could include experimental protocols, analysis outputs, manuscripts (text) and potentially datasets from automated experiments.",
            "validation_method": "Not described here; original work may include experimental/automated validation pipelines.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Autonomy risks, reproducibility, and integration with real-world experimental infrastructures; not detailed in this paper.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.9",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "VirSci",
            "name_full": "VirSci",
            "brief_description": "A coordinated multi-agent system that generates, critiques, and revises scientific proposals collaboratively to emulate scientific team workflows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "VirSci (multi-agent collaborative proposal generation)",
            "method_description": "Coordinates teams of virtual agents that separately generate, critique, and revise scientific proposals in a collaborative loop, leveraging LLM-based reasoning to iterate on research concepts and designs.",
            "llm_model_used": null,
            "scientific_domain": "Scientific proposal generation and ideation across domains",
            "number_of_papers": null,
            "type_of_quantitative_law": "Supports ideation/critique rather than formal extraction of quantitative laws, but could surface patterns or hypothesized relationships.",
            "extraction_output_format": "Proposal drafts, critiques, and revised texts.",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.10",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Yescieval",
            "name_full": "Yescieval (LLM-as-a-judge for scientific QA)",
            "brief_description": "An approach (referenced as 'LLM-as-a-judge') that uses LLMs to evaluate and judge the quality or correctness of syntheses or answers over scientific literature at scale.",
            "citation_title": "Yescieval: Robust llm-as-a-judge for scientific question answering.",
            "mention_or_use": "mention",
            "method_name": "LLM-as-a-judge (evaluation framework)",
            "method_description": "Uses a high-capability LLM (e.g., GPT-4 reported in related work) as an automated judge to score or validate system outputs (syntheses, answers) for factuality, relevance, and other quality dimensions; employed as a scalable evaluation and feedback mechanism to improve synthesis frameworks.",
            "llm_model_used": "Reported related use of GPT-4 as judge in cited works; specific model in Yescieval reference not detailed in this paper.",
            "scientific_domain": "Scientific question answering and synthesis evaluation",
            "number_of_papers": null,
            "type_of_quantitative_law": "Not primarily a law-extraction method; used to judge quality of extracted findings and syntheses including numeric claims.",
            "extraction_output_format": "Scores/judgements and narrative critiques (textual evaluation).",
            "validation_method": "Serves as an automated validation/evaluation method; original work likely compares to human judgments.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Reliance on an LLM judge can inherit model biases and evaluation blind spots; LLM-as-judge is an approximate proxy for human evaluation.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4357.11",
            "source_info": {
                "paper_title": "DeepResearchEco: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paperqa: Retrievalaugmented generative agent for scientific research.",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Orkg ask: a neuro-symbolic scholarly search and exploration system.",
            "rating": 2,
            "sanitized_title": "orkg_ask_a_neurosymbolic_scholarly_search_and_exploration_system"
        },
        {
            "paper_title": "Llms4synthesis: Leveraging large language models for scientific synthesis.",
            "rating": 2,
            "sanitized_title": "llms4synthesis_leveraging_large_language_models_for_scientific_synthesis"
        },
        {
            "paper_title": "Paperqa: Retrievalaugmented generative agent for scientific research.",
            "rating": 1,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Nova",
            "rating": 1,
            "sanitized_title": "an_iterative_planning_and_search_approach_to_enhance_novelty_and_diversity_of_llm_generated_ideas_nova"
        },
        {
            "paper_title": "Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback.",
            "rating": 1,
            "sanitized_title": "ideasynth_iterative_research_idea_development_through_evolving_and_composing_idea_facets_with_literaturegrounded_feedback"
        },
        {
            "paper_title": "Yescieval: Robust llm-as-a-judge for scientific question answering.",
            "rating": 2,
            "sanitized_title": "yescieval_robust_llmasajudge_for_scientific_question_answering"
        }
    ],
    "cost": 0.023085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DeepResearch Eco : A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</p>
<p>Jennifer D'souza jennifer.dsouza@tib.eu 
TIB Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Endres Keno Sander endres.keno.sander@stud.uni-hannover.de 
Leibniz University Hannover
Germany</p>
<p>Andrei Aioanei aaioanei@proton.me 
TIB Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>DeepResearch Eco : A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology
88025719B8BD79F0D584FD2916B8CD41
We introduce DeepResearch Eco , a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth-and breadth-controlled exploration of original research questions-enhancing search diversity and nuance in the retrieval of relevant scientific literature.Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameterdriven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor.Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words.High-parameter settings yield expert-level analytical depth and contextual diversity.Source code available at: https://github.com/sciknoworg/deep-research.</p>
<p>Introduction</p>
<p>Science requires extreme attention to detail, and large language models (LLMs) can overlook or misuse details when faced with challenging reasoning problems [1,2].Ensuring factual accuracy in LLM-generated content has thus become a key challenge.The current paradigm for eliciting factually grounded responses from LLMs is to use retrieval-augmented generation (RAG) [3,4], which supplements the model's knowledge with relevant documents from external sources.By leveraging retrieval, such agentic pipelines can explore scientific literature at a much higher throughput than human scientists-enabling comprehensive surveys that were previously impractical.However, scaling up literature exploration in this manner raises new questions about how to balance breadth (covering many sources) versus depth (deeply analyzing the evidence from each source) to produce high-quality scientific syntheses.</p>
<p>In this work, we introduce DeepResearch Eco , an agentic LLM-based system for complex scientific question answering and literature synthesis, which in this work is tested against research questions in the ecological sciences.DeepResearch employs a recursive retrieval and generation loop guided by explicit, user-controllable depth and breadth parameters.This design enables iterative broad exploration of the topic followed by targeted deep dives, effectively marrying a wide-ranging literature survey with in-depth analysis.Unlike prior feed-forward pipelines, our approach surfaces intermediate reasoning steps (e.g., search subqueries and extracted "learnings") and uses them to refine subsequent queries, yielding a transparent and traceable knowledge workflow.We integrate two variants of LLM reasoning models within this framework to assess the robustness and generality of the generated research reports across different model capabilities.The result is a flexible methodology that can be tuned to either quickly scan numerous publications or rigorously drill down into specific evidence, all within an automated agentic workflow.</p>
<p>Specifically, we explore the following empirically driven research questions in this work: RQ1: How similar are the reports generated by DeepResearch across different depth and breadth settings, using two variants of reasoning models, when evaluated with ROUGE (word-based) and embedding-based semantic similarity metrics?RQ2: How do depth and breadth parameters in automated research systems affect the quality and diversity of synthesized scientific knowledge in ecology?RQ3: Can high-parameter configurations in LLM-based systems achieve domain-specific synthesis capabilities that match or exceed expert-level integration, especially in ecological research contexts?To answer these questions, we conduct extensive experiments using DeepResearch on ecological science problems, analyzing both quantitative metrics and qualitative aspects of the generated outputs.In summary, our contributions are threefold: (1) we present a novel recursive, breadth-vs-depth controllable LLM workflow for automated scientific literature review; (2) we provide an in-depth evaluation of how exploration depth and breadth impact the quality and diversity of knowledge synthesis (finding, for example, that a high-depth configuration can automatically integrate information from 111 sources-nearly 6× more than a shallow setting-and increase coverage of key concepts by 25%); and (3) we demonstrate that carefully configured, highparameter runs can approach expert-level integration in ecology, achieving an order-of-magnitude higher information density in outputs without loss of rigor or specificity.All code and data are released under an open-source MIT license 1 to facilitate reproducibility and future research.</p>
<p>Related Work</p>
<p>Recent developments in LLMs have led to the emergence of agentic workflows for scientific question answering and synthesis.This section reviews key systems that shape the landscape of LLM-based scientific discovery, covering agentic pipelines, human-aligned synthesis frameworks, and multi-agent reasoning systems.Scientific Search and Synthesis.A growing line of work focuses on using LLMs to automate scientific search and synthesis, combining document retrieval with intelligent summarization and reasoning capabilities.</p>
<p>PaperQA [5] introduced a modular agentic pipeline for LLM-assisted scientific question answering.It begins by retrieving relevant papers through Google Scholar using keyword and year-range queries, then constructs an embedding-based chunk database.For each user question, relevant text chunks are retrieved using maximal marginal relevance.These chunks are then summarized or marked as irrelevant, helping mitigate semantic noise and parsing errors.Finally, an LLM generates a response, using its own knowledge and optionally the summarized content.PaperQA2 [1] builds on this model by introducing a full-fledged multi-agent framework.Retrieval and generation are separated into distinct agents: a paper search agent reformulates the user query, fetches PDFs, and converts them to text; a citation traversal agent expands the corpus through citation networks; a gather-evidence agent retrieves and summarizes text chunks via dense retrieval, reranking, and contextual summarization; and a generation agent synthesizes answers from the top-ranked evidence.PaperQA2 also powers use cases beyond question answering, including Wikipedia-style summarization (WikiCrow) and contradiction detection (ContraCrow), the latter benchmarking whether scientific claims contradict prior literature.</p>
<p>ORKG Ask [6] offers a complementary approach rooted in scholarly infrastructure.It combines semantic search over a 70+ million article index (from CORE [7]) with knowledge extraction via LLMs.The search interface returns top-ranked articles using vector similarity (via Nomic embeddings), and the LLM generates a synthesis of the top 5 results.This is augmented by LLMs4Synthesis [8], a framework that structures synthesis tasks into paper-wise, methodological, and thematic categories.Syntheses are generated using structured prompts and evaluated using GPT-4 as an LLM-as-a-judge.This highlights an important emerging research direction: leveraging LLM-as-a-judge [9] as a scalable and effective approach for evaluating scientific tasks.RLAIF (reinforcement learning with AI feedback) is further applied to optimize open-source models (e.g., Mistral-7B) for factuality and clarity.Iterative, Structured, and Human-Aligned Research Workflows.Beyond retrieval and synthesis, another line of research focuses on designing LLM systems that mirror human cognitive workflows-emphasizing iterative refinement, structured reasoning, and alignment with scientific practices.</p>
<p>Nova [10] and IdeaSynth [11] enable iterative refinement of research ideas.Nova leverages planning and information retrieval to diversify generated ideas, addressing the tendency of LLMs to produce repetitive outputs.IdeaSynth organizes ideas as canvas nodes that evolve through literature-grounded feedback loops, facilitating deeper exploration across multiple stages of ideation.Semantic Canvas [12] complements these efforts by introducing constraint-guided input filtering and semantic navigation, which improves output relevance and encourages user engagement.</p>
<p>Other systems focus on structuring the research ideation process.Chain of Ideas (CoI) [13] arranges literature into developmental chains to reflect how research areas evolve over time, supporting progressive insight development.Scideator [14] promotes creativity by recombining research facets-such as purpose, mechanism, and evaluation-using novelty heuristics to suggest original directions.Both systems emphasize structured representations of knowledge to align with how researchers typically generate and refine ideas.Multi-Agent Systems for End-to-End Scientific Discovery.Recent work explores autonomous multi-agent systems that aim to replicate the full scientific workflow-from ideation to publication.Going beyond search and synthesis, fully autonomous multi-agent systems have been proposed to tackle scientific discovery holistically.The AI Scientist framework [15] automates the entire pipeline from idea generation to experimental design and publication writing.VirSci [16] coordinates teams of virtual agents that generate, critique, and revise scientific proposals collaboratively.These systems demonstrate the potential of distributed agentic reasoning and underscore the growing interest in autonomous research systems.These systems underscore the growing interest in distributed agentic reasoning for scientific discovery and highlight the feasibility of closed-loop, autonomous scientific workflows.Positioning DeepResearch.While prior systems like PaperQA2 and ORKG Ask emphasize modularity and scalability, they follow largely feedforward retrieval-to-generation pipelines, often with limited configurability or recursion.In contrast, DeepResearch introduces a recursive, user-controllable exploration loop governed by explicit depth and breadth parameters.One of the essential facets of true deep research is the ability to have recursive calls to repeatedly drill down on the nuances of the question posed by researchers.This enables progressively focused or diversified reasoning, which single-pass architectures do not address.Moreover, DeepResearch surfaces intermediate reasoning steps-such as SERP-style subqueries, structured "learnings," and follow-up questions-enhancing transparency and researcher oversight.Its ability to integrate multiple search modalities and enforce structured, schema-conformant outputs positions it as a flexible tool for both exploratory synthesis and machine-readable knowledge workflows.These distinctions highlight DeepResearch's unique contribution to the emerging paradigm of agentic, iterative, and human-aligned scientific research systems.</p>
<p>Method</p>
<p>Deep Research</p>
<p>The Deep Research system orchestrates a recursive, multi-agent workflow for automated literature exploration and synthesis, as shown in Figure 1.The system is initialized with a user-defined research question and two parameters-breadth and depth-that determine how the exploration unfolds.The breadth parameter controls how many diverse SERP-style queries are generated at each level, allowing the system to branch into multiple directions.The depth parameter governs the number of recursive layers, each of which pushes the investigation deeper by refining queries based on prior learnings.Before execution begins, the environment is configured by selecting an LLM backend and a search client.full-text results in markdown, and ORKG Ask, which queries a scholarly corpus of over 80 million publications to return structured metadata including titles, abstracts, and links.The core loop is composed of four sub-agents.The generate serp queries sub-agent converts the input research question (or a follow-up from a previous round) into a set of search-compatible queries, each accompanied by a research goal.These are passed to the search sub-agent, which retrieves the top results using the selected provider.The results are then processed by the summarize result sub-agent, which merges relevant content (titles and abstracts or full text) and prompts the LLM to generate summary "learnings" and new follow-up questions.This cycle continues until the specified depth is reached.All accumulated insights are then handed off to the generate report sub-agent, which synthesizes the findings into a comprehensive Markdown report, complete with citations and structured using a validated JSON schema.Each sub-agent operates independently but in coordination, and the entire orchestration is governed by a shared system prompt that ensures coherence across the research workflow.</p>
<p>Sub-agents</p>
<p>generate serp queries.This sub-agent takes an actual research question (e.g., "What are the effects of invasive species in grasslands?") and prompts the LLM to generate SERP-style queries-i.e., search engine-compatible (SERP = search engine results page) queries-which are typically: 1) declarative or keyword-based sentences, and 2) optimized for information retrieval rather than naturalness.For example, the original question may yield the query "impact of invasive species on native grassland biodiversity."</p>
<p>On its first invocation, the sub-agent receives the user's research question, optionally enriched with feedback.In subsequent recursive calls, its input consists of the previous research goal and a set of follow-up questions, along with accumulated learnings passed from the summarize result sub-agent.The number of queries generated defaults to 3 but is configurable via the breadth parameter.With each increase in recursion depth, the number of generated queries is halved (using integer division, breadth // 2), thus progressively narrowing the scope of research exploration.Note that at this stage, the LLM is prompted not only to generate SERP-style queries, but also to produce an accompanying research goal for each query, which helps guide subsequent iterations of the research process.</p>
<p>search.This sub-agent executes each SERP-style query using one of two configurable search providers.The first is the Firecrawl API, which performs web-scale search and returns full-text web content in markdown format for up to ten retrieved pages.This mode enables broad coverage of unstructured online sources such as blogs, scientific literature, or news articles.The second is the ORKG Ask API, which queries a scholarly index of over 70 million scientific publications and returns a structured response comprising titles, abstracts, and URLs for the top-ranked results.While Firecrawl supports general-purpose web research, ORKG Ask is optimized for evidence-based synthesis from scientific literature.In both cases, the sub-agent operates asynchronously and executes queries in parallel to maximize efficiency.Retrieved content is passed unfiltered to the summarization sub-agent, and all URLs are retained for transparency and citation in downstream reporting.</p>
<p>summarize result.This sub-agent processes the raw output from the search sub-agent.For each query, it takes the top 10 returned documents (by default) and extracts their textual content.In the case of the Firecrawl provider, this content consists of markdown-formatted full text; for ORKG Ask, it is a combination of publication titles and abstracts.These are merged into a single prompt and passed to the LLM, along with the original query that triggered the search.The LLM is then instructed to produce two outputs: (i) a list of up to 3 "learnings, " meaning concise and information-dense summary insights derived from the content, and (ii) a list of up to 3 follow-up questions for further exploration.Both values are configurable via parameters.These outputs are used to inform recursive querying (generate SERP queries) and accumulate findings for the final report.The agent prompt used for this summarization is shown below.</p>
<p>generate report.This sub-agent synthesizes all accumulated learnings from previous search and summarization rounds into a comprehensive Markdown report.It takes as input the original user research question or, in the case of a recursive call, a composed prompt containing the research goal and follow-up questions.Alongside this prompt, it receives the list of learnings-information-dense insights extracted by the summarize result sub-agent-and the URLs of visited documents.The LLM is instructed to generate a detailed narrative that weaves together all findings, aiming for the length and coherence of a multi-page literature overview.The final report includes a Sources section automatically appended, listing all retrieved document URLs for transparency and traceability.The output is strictly validated against a JSON schema that enforces the presence of a single field: reportMarkdown.The exact prompt passed to the language model is shown below, illustrating how the composed query and accumulated learnings are structured to guide report generation.</p>
<p>Results and Discussion</p>
<p>This section presents our experiments with DeepResearch on ecological research questions, analyzing outcomes both quantitatively and qualitatively.</p>
<p>Experimental Settings</p>
<p>Dataset</p>
<p>We compiled a corpus of 49 ecological research questions from nine fellows of the interdisciplinary group "Mapping Evidence to Theory in Ecology." 2 The questions were collected via a Google Form with prompts such as: Your research question, Relevant ecological sub-discipline, and Purpose of the question.The dataset is publicly available at https://github.com/sciknoworg/deep-research/blob/main/data/49-questions.csv.</p>
<p>The questions span a wide range of ecological sub-domains, including restoration ecology, invasive species management, microbial ecology, and pollination ecology, as well as interdisciplinary areas involving sociology and geology.In terms of intent, 16 questions aim to explore existing hypotheses, another 16 seek to generate new ideas, and 13 aim to collect evidence.A few respondents were motivated by the need for practical insights or broad knowledge overviews.This distribution highlights the exploratory and generative nature of early-stage or interdisciplinary ecological research.</p>
<p>Experimental Setup</p>
<p>We conducted experiments using two OpenAI models: GPT o3 and GPT o3-mini.These models were selected for their ability to produce structured, schema-conformant outputs, supporting fields such as learnings, follow-up questions, and research goals extracted from unstructured LLM responses.Both models are also advertised as reasoning-capable, an essential feature for multi-step scientific synthesis.</p>
<p>The semantic search component is powered by the ORKG Ask API. 3 We evaluated the system across eight configurations defined by a Cartesian product of two reasoning models (o3-mini and o3), two synthesis depths ( ∈ {1, 4})-i.e., the number of recursive synthesis steps, where each step involves one full query-response cycle, and two breadth values ( ∈ {1, 4})-i.e., the number of subqueries issued per step.Each configuration generated 49 structured markdown reports saved with the filename pattern: <index><em><model></em><engine>_d<depth>_b<breadth>.md.All the markdown reports are available at https://github.com/sciknoworg/deep-research/tree/main/data/ecology-reports/orkg-ask.</p>
<p>Quantitative Evaluations</p>
<p>To compare DeepResearch outputs across settings, we align reports generated under different configurations by their shared indices.Let   and   denote two configuration groups (e.g., different model-depth-breadth settings), each containing 50 reports indexed by question ID .We define the aligned subset of indices as:   = {  :  exists in both   and   }.</p>
<p>This ensures that for each  ∈   , the same research question is compared under both configurations.All similarity metrics are computed over these aligned report pairs and averaged across the set   .</p>
<p>Metrics</p>
<p>We assess report similarity using three complementary metrics:</p>
<ol>
<li>
<p>ROUGE-L F 1 .ROUGE-L is a lexical metric that measures the longest common subsequence (LCS) between two texts.It reflects surface-level overlap in word order and phrasing.Given token sequences  and , with ℓ = LCS(, ), the precision, recall, and F 1 score are:
𝑃 LCS = ℓ |𝐴| , 𝑅 LCS = ℓ |𝐵| 𝐹 1 LCS = 2 𝑃 LCS 𝑅 LCS 𝑃 LCS + 𝑅 LCS .
We compute ROUGE-L using the rouge_score library with stemming enabled.While effective for capturing surface similarity, ROUGE-L does not account for paraphrasing or semantic equivalence.</p>
</li>
<li>
<p>BERTScore (SciBERT F 1 ).BERTScore compares token-level embeddings from a pre-trained language model to measure semantic similarity.Using SciBERT, we split each report into chunks of up to 510 tokens, ensuring compatibility with the model's 512-token input limit.For each chunk pair (  ,   ), we compute cosine similarity-based precision, recall, and F 1 :
𝑃 chunk = 1 |𝐴| |𝐴| ∑︁ 𝑖=1 max 𝑗 cos(a 𝑖 , b 𝑗 ), 𝑅 chunk = 1 |𝐵| |𝐵| ∑︁ 𝑗=1 max 𝑖 cos(a 𝑖 , b 𝑗 )
We then average chunk-level F 1 scores across aligned reports to obtain document-level similarity.Unlike ROUGE-L, BERTScore captures paraphrasing and semantic alignment even when wording differs.</p>
</li>
</ol>
<p>Word Mover's Distance (WMD).</p>
<p>WMD computes the minimal cumulative distance required to "transport" words from one document to another in embedding space.Each word is represented by a SciBERT embedding h  , and distances are computed as (,  ′ ) = 1 − cos(h  , h  ′ ).WMD solves the following optimal transport problem:
WMD(𝐴, 𝐵) = min 𝜋∈Π(𝐴,𝐵) ∑︁ 𝑤∈𝐴 ∑︁ 𝑤 ′ ∈𝐵 𝜋(𝑤, 𝑤 ′ ) 𝑑(𝑤, 𝑤 ′ ),
where Π(, ) denotes valid transport plans between the empirical word distributions of  and .</p>
<p>To match our similarity scale, we report 1 − WMD(, ), where higher values indicate greater similarity.Computation is performed using Gensim's WmdSimilarity on precomputed SciBERT embeddings.</p>
<p>Comparison: ROUGE-L emphasizes exact token sequence overlap, BERTScore captures contextual semantic similarity via embedding proximity, and WMD quantifies semantic dissimilarity as the transport cost between word embeddings.Together, these metrics offer a complementary, multi-faceted perspective on report similarity.</p>
<p>Results</p>
<p>To address RQ1-How similar are the reports generated by DeepResearch across different depth and breadth settings, using two variants of reasoning models, when evaluated with ROUGE (word-based) and embedding-based semantic similarity metrics?-we present results in Figure 2. The figure contains three 8×8 heatmaps showing pairwise similarity between the four o3 configurations (rows/columns 1-4) and the four o3-mini configurations (rows/columns 5-8).Each cell reports the average similarity across aligned reports with the same index.Darker shading indicates stronger similarity (higher ROUGE-L or BERTScore; lower WMD).Self-consistency.In all three heatmaps, the main diagonal-where each configuration is compared to itself-is the darkest, reflecting perfect alignment.ROUGE-L F1 and BERTScore F1 are both 1.0, and WMD similarity is also 1.0 (i.e., WMD = 0).This confirms that the similarity metrics behave as expected in the identity case.</p>
<p>Within-model consistency.The upper-left 4×4 block shows consistency across o3 configurations with different depth and breadth settings.BERTScore values average around 0.56, WMD similarity around 0.56, while ROUGE-L is lower, around 0.14.Similarly, the bottom-right 4×4 block for o3-mini configurations shows even higher internal consistency: BERTScore averages around 0.61, WMD similarity around 0.61, and ROUGE-L around 0.16.</p>
<p>The comparatively lower ROUGE-L scores are expected, as ROUGE evaluates surface-level token overlap and does not account for paraphrasing or semantic equivalence.In contrast, BERTScore and WMD rely on contextual embeddings, capturing semantic similarity even when lexical expressions differ.These embedding-based metrics thus better reflect the meaning-preserving variations typical in LLM-generated outputs.</p>
<p>Cross-model similarity.The off-diagonal blocks (rows 1-4 vs. columns 5-8 and vice versa), representing comparisons across o3 and o3-mini, are visibly lighter.Average BERTScore drops to approximately 0.54, WMD similarity to 0.54, and ROUGE-L to 0.12.Even the best-aligned configuration pair-depth 4, breadth 4 for both models-exhibits weaker similarity than within-model comparisons.</p>
<p>Summary.These results indicate that both o3 and o3-mini produce internally consistent outputs across different recursive configurations, with o3-mini showing slightly stronger stability.However, alignment between the two models is consistently weaker, suggesting that model-specific generation patterns persist despite identical prompts and retrieval settings.This highlights the influence of model architecture on the structure and wording of scientific outputs.</p>
<p>Qualitative Evaluations</p>
<p>To systematically evaluate synthesis quality across multiple dimensions, we developed a scoring framework.Our approach builds on existing frameworks for assessing scientific synthesis quality [17,18] while incorporating domain-specific considerations for ecological research.</p>
<p>Theoretical Foundation</p>
<p>Our quality assessment framework is motivated by three key principles from the literature on automated research systems: (i) human alignment (Chain of Ideas [13]), emphasizing depth, breadth, and rigor; (ii) iterative refinement (Nova [10], IdeaSynth [11]), highlighting sophisticated reasoning and broad literature integration; and (iii) collaborative knowledge integration (VirSci [16]), assessing the ability to draw connections across sources.</p>
<p>Metric Design</p>
<p>We assess the deep research generated report quality using six complementary metrics, selected to balance ecological relevance, analytical depth, and scalability for automated evaluation.Each metric targets a distinct quality axis, grounded in identifiable linguistic or structural signals and weighted by domain relevance and signal reliability.Scores are normalized to the [0,1] range using empirical thresholds from our 196-report dataset and aggregated via weighted sums reflecting their relative importance.For each metric, we define the detection strategy, assumptions, normalization scheme, and weight rationale, informed by curated vocabularies and empirical distributions.Research Depth Parameter Assessment.Research depth quantifies the mechanistic sophistication and analytical precision of synthesis outputs, distinguishing surface-level description from processlevel understanding.We define three key components: Mechanistic understanding is assessed via a curated list of 15 ecology-specific process indicators (Appendix A), such as "feedback, " "nutrient cycling, " and "trophic cascade."Matches are counted via case-insensitive substring search.Causal reasoning captures explicit cause-effect statements using predefined connectives ("because," "due to"), result indicators ("results in, " "induces"), and mechanistic verbs ("drives, " "regulates").This reflects an LLM's capacity to reason about ecological processes.Temporal precision measures the proportion of specific temporal references, such as quantified intervals ("within 6 months, " "every 3 years") and dated events ("1990-2020"), identified via regular expressions.</p>
<p>The combined score is:
𝑆_𝑑𝑒𝑝𝑡ℎ = 0.4 • min (︂ 𝑀 _𝑚𝑒𝑐ℎ 20 ,1)︂ + 0.3 • min (︂ 𝑀 _𝑐𝑎𝑢𝑠𝑎𝑙 10 ,1)︂ + 0.3 • 𝑀 _𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙 (1)
Research Breadth Parameter Assessment.Breadth evaluates the diversity of evidence synthesized across spatial, ecological, and methodological axes.It reflects generalizability and the capacity to identify patterns across contexts.We compute five normalized sub-scores: Geographic coverage (  ): count of unique biogeographic zones (e.g., "Tropical, " "Boreal") from a list of 20.Intervention diversity (  ): number of unique management practices matched to a taxonomy of 17 interventions.Biodiversity dimensions (  ): presence of terms related to taxonomic, functional, phylogenetic, and spatial diversity.Ecosystem services (  ): matches against a vocabulary aligned with the Millennium Ecosystem Assessment.Spatial scale (  ): presence of explicit scale terms ("local," "regional," "continental") and area measures.Combined:
𝑆_𝑏𝑟𝑒𝑎𝑑𝑡ℎ =; 0.25 • min (︂ 𝐺_𝑟𝑒𝑔𝑖𝑜𝑛𝑠 8 ,1)︂ + 0.25 • min (︂ 𝐼_𝑡𝑦𝑝𝑒𝑠 12 ,1)︂ + 0.25 • min (︂ 𝐷_𝑑𝑖𝑚𝑠 8 ,1)︂ + 0.15 • min (︂ 𝐸_𝑠𝑒𝑟𝑣𝑖𝑐𝑒𝑠 10 ,1
)︂ + 0.10 • min
(︂ 𝑆_𝑠𝑐𝑎𝑙𝑒𝑠 6 ,1
)︂</p>
<p>Domain-Specific Quality Assessment.This ecology-specific dimension captures alignment with pressing research themes: Conservation focus: frequency of conservation-related terms ("biodiversity, " "restoration, " "habitat loss").Climate relevance: mentions of climate-related terms across scales.Ecological complexity: use of system-level terms ("synergistic, " "nonlinear, " "interconnected").Combined:
𝑆_𝑒𝑐𝑜𝑙𝑜𝑔𝑖𝑐𝑎𝑙 = 0.4•min (︂ 𝐶_𝑐𝑜𝑛𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛 8 ,1)︂ +0.3•min (︂ 𝐶_𝑐𝑙𝑖𝑚𝑎𝑡𝑒 6 ,1)︂ +0.3•min (︂ 𝐸_𝑐𝑜𝑚𝑝𝑙𝑒𝑥𝑖𝑡𝑦 5 ,1)︂(3)
Scientific Rigor Assessment.This metric assesses evidentiary and methodological integrity across three axes: Statistical sophistication detects the use of inferential statistics and analysis techniques, reflecting quantitative depth.Citation practices are evaluated by presence of parenthetical (e.g., "(Smith et al., 2021)") or narrative citations.Uncertainty acknowledgment rewards explicit discussion of limitations ("unknown, " "limited evidence, " "unclear").</p>
<p>Combined score:
𝑆_𝑟𝑖𝑔𝑜𝑟 = 0.4 • min (︂ 𝑅_𝑠𝑡𝑎𝑡𝑖𝑠𝑡𝑖𝑐𝑎𝑙 5 , 1 )︂ + 0.4 • min (︂ 𝐶_𝑓 𝑜𝑟𝑚𝑎𝑙 20 , 1 )︂ + 0.2 • min (︂ 𝑈 _𝑎𝑐𝑘𝑛𝑜𝑤𝑙𝑒𝑑𝑔𝑚𝑒𝑛𝑡 5, 1 )︂ (4)
Innovation Capacity Assessment.We assess novelty using three linguistic signals: Speculative statements use hedging and conjecture ("might," "could," "hypothetical").Novelty indicators include self-declared innovation terms ("novel," "pioneering," "emerging").Gap identification detects explicit acknowledgment of unanswered questions ("research gap, " "understudied").Combined:
𝑆_𝑖𝑛𝑛𝑜𝑣𝑎𝑡𝑖𝑜𝑛 = 0.4 • min (︂ 𝐼_𝑠𝑝𝑒𝑐𝑢𝑙𝑎𝑡𝑖𝑣𝑒 3 ,1)︂ + 0.3 • min (︂ 𝐼_𝑖𝑛𝑑𝑖𝑐𝑎𝑡𝑜𝑟𝑠 3 ,1)︂ + 0.3 • min (︂ 𝐺_𝑟𝑒𝑠𝑒𝑎𝑟𝑐ℎ 3, 1 )︂ (5)
Information Density and Taxonomic Precision.Information density reflects synthesis efficiency:
𝑆_𝑑𝑒𝑛𝑠𝑖𝑡𝑦 = min (︂ 𝑁 _𝑠𝑜𝑢𝑟𝑐𝑒𝑠 𝑊 _𝑐𝑜𝑢𝑛𝑡/1000 • 1 50 ,1
)︂</p>
<p>Together, these dimensions enable a multifaceted, reproducible evaluation of synthesis quality grounded in both ecological expertise and computational feasibility.To facilitate reproducibility, we publicly release our qualitative evaluation pipeline and the accompanying taxonomies at https: //github.com/sciknoworg/deep-research/blob/main/scripts/README.md.</p>
<p>Results</p>
<p>Analysis of 196 syntheses across 49 ecological questions shows that depth and breadth parameters strongly shape synthesis quality, with clear implications for automated research system design.Depth Parameter Effects.In addressing RQ2 on how depth and breadth parameters shape synthesis quality and diversity, we first examine the role of depth in enhancing analytical sophistication.Increasing depth parameters transforms synthesis from surface-level generalizations to mechanistic understanding.Moving from d1 to d4 yields a 5.9-fold increase in source utilization (18.9 to 111.1 sources) without increasing content length, enabling denser, more analytical outputs.</p>
<p>At low depth (d1), syntheses are descriptive but lack causal insight.For example, a grassland analysis notes: "Extensification packages suppress herbage or milk output by 10-40%," reporting outcomes without explanation.In contrast, high-depth (d4) synthesis offers mechanistic accounts: "Nutrient withdrawal shifts competitive hierarchies from fast-growing tall grasses to stress-tolerators by (i) reducing soil NO − 3 and NH + 4 , (ii) decreasing leaf N content, and (iii) opening ground-layer light niches... " -tracing clear ecological pathways and system dynamics.</p>
<p>Research depth is formally assessed via three components that capture analytical sophistication: mechanistic understanding, causal reasoning, and temporal precision.Mechanistic understanding is measured using a curated vocabulary of 15 ecology-specific terms (Appendix A) such as "feedback," "nutrient cycling, " and "energy flow, " detected via case-insensitive substring matching.Causal reasoning is assessed through scientific connectives ("because," "due to," "leads to," "triggers," "regulates," etc.), identifying both simple and multi-step causal explanations.Temporal precision quantifies the ratio of specific time references (e.g., "5-10 years, " "within 6 months") to all temporal mentions, using regular expressions to distinguish precise from vague durations.</p>
<p>Measured via Equation 1, empirical results reflect this assessment: though raw depth scores for d1 and d4 are similar (0.494 vs. 0.500), d4 outputs contain over three times more multi-step causal chains, revealing deeper reasoning.Temporal specificity also improves: low-depth syntheses use vague terms like "several years" or "long-term, " while d4 outputs report concrete thresholds ("5-6 years" for species recovery, "≥10 years" for diversity lags).Although temporal precision scores remain close (0.583 for  1  1 vs. 0.549 for  4  4 ), this reflects the challenge of reconciling more diverse temporal information in high-depth synthesis.The ability of d4 configurations to integrate broader evidence while maintaining precision demonstrates robust synthesis capabilities under information load.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we presented DeepResearch Eco , a recursive, agentic workflow for controllable scientific synthesis, validated on 49 ecological research questions.Increasing depth and breadth parameters improves analytical rigor, evidence diversity, and ecological specificity.For instance, in Question 8-"Is there evidence that climate change and land use interact to alter biodiversity of grasslands?"-the d=1, b=1 report offers a brief generalization, while the d=4, b=4 report integrates cross-regional evidence, mechanistic pathways, and system feedbacks.Similarly, for Question 41-"What is the most common effect of fertilization on grassland plant diversity?"-thed=1, b=1 report notes a general decline, whereas the d=4, b=4 report details competitive shifts, functional group changes, and long-term nutrient effects.These cases exemplify how DeepResearch enables structured, transparent, and expert-like synthesis with tunable analytical control.</p>
<p>Future work will focus on evaluating DeepResearch across additional domains beyond ecology, such as materials science and social science, to further demonstrate its generality and adaptability.We also plan to address current limitations by implementing an interactive agent for researcher feedback integration, enabling guided refinement across recursive steps.Support for multimodal synthesisincluding figures and tables-will be explored to enhance utility in data-rich fields.Finally, we envision collaborative agentic workflows in which multiple agents co-explore subtopics or perspectives, enabling distributed synthesis across teams or disciplines.These enhancements will reinforce our commitment to scalable, human-aligned, and reproducible AI-assisted research.0.8% from d1_b1 to intermediate configurations but jump 16.1% from intermediate to d4_b4, indicating discrete capability transitions rather than smooth improvement curves.Third, the super-linear efficiency indicates that computational investment yields disproportionate returns in synthesis quality, altering the cost-benefit calculations for deploying automated research systems (Figure 5).</p>
<p>B.4. Domain-Specific Quality Validation</p>
<p>Beyond general synthesis metrics, our analysis reveals systematic patterns in ecology-specific quality dimensions that validate the system's domain expertise and demonstrate parameter-dependent specialization capabilities.These domain-specific assessments provide critical evidence that the system achieves not merely generic text synthesis but ecological knowledge integration that scales with computational investment.Conservation focus (a component of   in Equation 3) demonstrates clear parameterdependent variation, with breadth-enhanced configurations achieving superior performance (d1_b4: 9.42 ± 9.73; d4_b4: 9.33 ± 10.00) compared to depth-focused alternatives (d4_b1: 8.67 ± 8.89).This pattern reflects the inherently multi-scale, multi-stakeholder nature of conservation challenges that require integration of diverse management approaches, regional conservation strategies, and crossjurisdictional policy frameworks.The superior performance of breadth-enhanced configurations aligns with conservation biology's need to synthesize evidence across geographic regions, taxonomic groups, and intervention strategies to develop effective preservation strategies.</p>
<p>Climate relevance (another component of   in Equation 3) exhibits progressive enhancement with parameter optimization, increasing systematically from 5.88 ± 6.46 (d1_b1) to 7.33 ± 7.73 (d4_b4), representing a 25% improvement in climate integration capability.This enhancement transcends simple keyword counting to demonstrate cross-domain synthesis, as high-parameter configurations successfully identify and integrate climate considerations across diverse research contexts.The progressive improvement validates that parameter enhancement enables deeper integration of specialized research domains.</p>
<p>Ecosystem services coverage reveals nuanced patterns that illuminate the differential effects of depth versus breadth parameters.Coverage ranges from 1.32 to 1.49 across configurations, with depthenhanced configurations achieving optimal performance (d4_b1: 1.49 ± 1.40).The finding suggests that while breadth helps identify diverse services across systems, depth enables understanding of the mechanisms underlying service generation.</p>
<p>Statistical sophistication (a key component of   in Equation 4) shows progressive enhancement across parameter configurations (1.02 to 1.20), with d4_b4 achieving optimal integration of quantitative research methodologies.This improvement reflects not merely increased detection of statistical terms but enhanced capacity to synthesize quantitative findings across vastly expanded literature sets.The 53% increase in quantitative information density (related to   in Equation 6) from d1_b1 (12.16 ± 9.05) to d4_b4 (18.55 ± 9.43) demonstrates the system's enhanced content analysis capabilities that emerge under high-parameter conditions, enabling the system to identify, extract, and integrate numerical findings that would be overlooked by simpler synthesis approaches.</p>
<p>Taxonomic precision emerges as the most distinctive quality indicator, with d4_b4 configurations achieving perfect performance (1.0 ± 0.0) compared to variable results in other configurations (0.47-0.53).This improvement reflects the system's enhanced capacity to correctly identify and reference specific taxonomic entities when processing comprehensive literature sets.The pattern suggests that taxonomic accuracy benefits from the combination of broad geographic coverage (exposing the system to diverse taxa) and analytical processing (enabling correct taxonomic placement and nomenclature).</p>
<p>Ecological complexity metrics demonstrate stability across parameter configurations despite exponentially increasing source loads, with d4_b4 achieving the highest score (1.22 ± 1.36) while processing 21.2-fold more sources than the baseline.This maintained performance under information loads vali-dates the system's capacity for knowledge integration at scale, suggesting that parameter enhancement enables not just broader coverage but sustained analytical depth even as information complexity increases.</p>
<p>As illustrated in Figure 3, the overall quality score progression from 0.405 to 0.478 across configurations demonstrates consistent enhancement with parameter increases, following a logarithmic improvement pattern with diminishing returns.The d4_b4 configuration achieves an 18% quality improvement over d1_b1, providing empirical justification for the 21.2-fold increase in computational requirements.This cost-benefit relationship, while showing diminishing returns, still validates high-parameter deployment for applications demanding comprehensive, high-quality synthesis outputs, particularly in domains where synthesis quality directly impacts conservation outcomes or policy decisions (Figure 6).</p>
<p>The practical implications of these scaling relationships become clear when examining cost-benefit trade-offs (Figure 6), which reveals that optimal configuration choice depends critically on application requirements and resource constraints.</p>
<p>Figure 1 :
1
Figure 1: Deep Research Orchestration Workflow.The user provides a research question and feedback, along with recursion parameters -breadth (b) and depth (d) -to guide the exploration.The workflow recursively calls four sub-agents: (1) generate serp queries to formulate search-optimized sub-queries and research goals, (2) search to retrieve content from configurable APIs (e.g., ORKG Ask or Firecrawl), (3) summarize result to extract structured learnings and follow-up questions, and (4) generate report to produce a final markdown report.The process iterates until the maximum depth is reached.</p>
<p>Figure 2 :
2
RQ1: Similarity of reports generated by o3-mini and o3 across four depth-breadth settings.Darker cells indicate higher similarity (ROUGE-L/BERTScore) or lower distance (WMD).</p>
<p>Figure 3 :
3
Figure 3: Decomposition of quality improvements across six key dimensions.Error bars represent standard deviations across 49 ecological research questions.The domain-specific score shows strong performance for comprehensive configurations (d4_b4: 0.9+), while information density remains the primary driver of differentiation across configurations.Overall quality scores (composite of all dimensions): best configuration d4_b4 achieved 0.577, with mean across all configurations of 0.511.</p>
<p>Figure 4 :
4
Figure 4: Individual parameter effects on source utilization showing equivalent depth and breadth contributions.Bar chart comparing averaged effects of depth and breadth parameters independently: Depth 1 (18.9 sources), Depth 4 (111.1 sources), Breadth 1 (19.2 sources), and Breadth 4 (110.8sources).Both depth (5.9-fold increase) and breadth (5.8-fold increase) parameters demonstrate nearly identical individual effects when averaged across the complementary parameter.This equivalence indicates that depth and breadth contribute equally to synthesis capability when considered independently, validating the balanced parameter design.The synergistic combination of both parameters (d4_b4: 192.9 sources) exceeds the sum of individual effects, demonstrating super-linear scaling behavior.</p>
<p>Figure 5 :
5
Figure 5: Exponential scaling of synthesis capabilities with depth-breadth parameters.(Left) Source utilization demonstrates super-linear scaling from 9.1 ± 1.7 sources (d1_b1) to 192.9 ± 31.2 sources (d4_b4), representing a 21.2-fold increase.(Center) Word count shows modest 41.5% increase (1,579 to 2,234 words), indicating enhanced information integration rather than verbosity.(Right) Information density (sources per 1,000 words) exhibits 14.9-fold improvement, demonstrating that higher parameter configurations achieve fundamentally superior synthesis efficiency.Error bars represent standard deviations across 49 ecological research questions (n=196 documents total).</p>
<p>Figure 6 :
6
Figure 6: Configuration optimization analysis showing cost-benefit trade-offs and efficiency frontiers.Four-panel analysis of parameter configuration performance: (A) Quality vs. cost trade-off using source count as computational cost proxy, revealing d4_b4's superior quality despite highest resource requirements; (B) Quality efficiency (quality per source) showing d1_b1's highest efficiency for resource-constrained applications; (C) Marginal quality improvement relative to baseline (d1_b1), demonstrating diminishing returns with d4_b4 providing 18% quality improvement; (D) Composite ranking combining quality (70%) and efficiency (30%) weights to identify optimal configurations for different use cases.Analysis based on 196 documents across 49 ecological research questions, providing empirical foundation for resource allocation decisions.</p>
<p>Two search modes are currently supported: Firecrawl, which enables open web search and returns
summarize resultInitial user researchgenerate SERP queriesTitle + Abstract merged from top k resultsGenerate N learnings and N follow-up questionsq. q. or recursion-wise follow-up[user research question &amp; feedback] or [prev. research goal + follow-up questions &amp; learnings][User research1searchquestion] questions] [Feedback2Deep Research depth: d breadth: bQuery available APIs (e.g., ORKG ASK)results Top k</p>
<p>Table 1
1
Source utilization statistics across 49 ecological questions (n=196 documents)
Configuration Mean Sources Std Dev Min Maxd1_b19.11.7010d1_b428.77.01040d4_b129.34.71839d4_b4192.931.293244
https://github.com/sciknoworg/deep-research
https://www.uni-bielefeld.de/einrichtungen/zif/groups/previous/mapping-evidence/
https://api.ask.orkg.org/docs#tag/Semantic-Neural-Search/operation/semantic_search_index_search_get
AcknowledgmentsWe thank the ecologists who participated in our survey and contributed the 49 research questions that underpin this study.All participants were research fellows in the interdisciplinary ZiF research group Mapping Evidence to Theory in Ecology, hosted by the Center for Interdisciplinary Research (ZiF: Zentrum für interdisziplinäre Forschung) at Bielefeld University and led by PI Tina Heger.The first author was a fellow in this group and conducted the survey as part of the ORKG Ask project.More information on the research group is available at https://www.uni-bielefeld.de/einrichtungen/zif/groups/ongoing/mappingevidence/.We also gratefully acknowledge support from the SCINEXT project (BMBF, Grant ID: 01IS22070) and the TIB Leibniz Information Centre for Science and Technology.Breadth Parameter Effects.Breadth expansion shifts synthesis from localized analyses to globally integrated perspectives.Moving from b1 to b4 results in a 5.8-fold increase in source utilization (19.2 to 110.8), demonstrating that breadth parameters expand diversity of evidence without inflating content length.This shift manifests most clearly in geographic coverage.Low-breadth configurations (b1) average 3.7 regions, typically focused on temperate zones in Europe and North America.In contrast, b4 outputs integrate evidence from an average of 4.9 distinct regions across multiple continents-e.g., "North America, Europe, Asia, and Australia"-surfacing biogeographic variation in species response, management effectiveness, and system constraints.Such contextualization enables nuanced recommendations that are otherwise invisible in regionally constrained syntheses.Methodological diversity also improves with breadth.Low-breadth syntheses average 2.6 intervention types, often reflecting single-strategy evaluations.In contrast, high-breadth configurations incorporate an average of 3.2 distinct approaches.For example, a b4 synthesis on Phragmites control evaluates chemical (glyphosate, imazapyr), mechanical (mowing, excavation), biological (goat grazing), and hydrological (salinity manipulation) methods.This comparative framing enhances decision support by revealing trade-offs and synergies across intervention types.Applying the breadth metric (Equation2) reveals broader gains beyond geography and methodology.High-breadth (d4_b4) syntheses exhibit stronger integration of biodiversity dimensions (e.g., combining functional, phylogenetic, and spatial perspectives), more comprehensive treatment of ecosystem services (including provisioning, regulating, and cultural functions), and finer resolution of spatial scale considerations (e.g., from plot-level to continental).These collectively elevate the generalizability and ecological realism of the synthesis.Quantitatively, the breadth score rises from 0.376 (d4_b1) to 0.473 (d4_b4), affirming that breadth enables systematic identification of cross-regional patterns while accounting for boundary conditions.The increase reflects not just a higher number of sources but a richer, more multidimensional integration of evidence, supporting more robust ecological inference and transferable insights for policy and practice.Domain, Rigor, Innovation, and Density Quality Validation. Figure3presents a comprehensive decomposition of quality improvements across all six dimensions, revealing how each component responds to depth-breadth parameter configurations and demonstrating the empirical validation of our multi-dimensional quality framework.To address RQ3-whether high-parameter configurations enable domain-aware synthesis comparable to expert-level integration-we analyzed performance across four advanced quality metrics: domain specificity (  ), scientific rigor (  ), innovation capacity (  ), and information density (  ).The findings support a clear pattern: high-parameter setups (notably d4_b4) consistently outperform lower-depth/breadth configurations across all measures.Domain-specificity metrics reveal that breadth-enhanced configurations (d1_b4, d4_b4) better capture conservation-oriented themes and climate relevance, aligning with the inherently cross-scale nature of ecological policy and management.For example, conservation term frequency rises to 9.33 ± 10.00 in d4_b4, compared to 8.67 ± 8.89 in depth-focused d4_b1-consistent with the weighting of conservation and climate indicators in Equation3. Climate integration shows a 25% gain from d1_b1 to d4_b4, underscoring the role of parameter scaling in cross-domain awareness.Interestingly, ecosystem service coverage peaks in d4_b1, suggesting that depth facilitates mechanistic unpacking of service generation, while breadth ensures representational completeness.Rigorous synthesis practices also improve with parameter scaling.Statistical sophistication, a key component of   (Equation4), increases from 1.02 to 1.20 across configurations, reflecting greater incorporation of inferential analysis.Citation quality and uncertainty acknowledgment co-evolve, resulting in robust evidence presentation that mirrors academic standards.Innovation capacity (Equation5) benefits from enhanced parameterization through more frequent identification of knowledge gaps and speculative framing-signals that often underpin novel research trajectories.The most pronounced efficiency gain, however, lies in information density (Equation6), which improves 14.9-fold from d1_b1 to d4_b4 despite only modest word count increases.This validates that high-parameter configurations not only scale information volume but also preserve analytical quality and specificity.Taken together, these results confirm that LLM-based systems, when carefully configured, can approximate expert-levelDeclaration on Generative AIChatGPT was used solely to support stylistic refinement and selective text shortening.All original text and substantive content were authored by the three co-authors.A. Domain-Specific Vocabulary for Quality AssessmentThe following vocabulary was used for automated detection of ecology-specific concepts in our quality assessment framework.Terms were selected based on frequency analysis of high-impact ecology papers, expert consultation, and validation against ecology textbook indices.A.1. Mechanistic Terms"mechanism", "pathway", "feedback", "trophic", "nutrient cycling", "energy flow", "predation", "competition", "mutualism", "succession", "disturbance", "resilience", "adaptation", "selection pressure", "gene flow", "decomposition", "mineralization", "nitrification", "photosynthesis", "respiration", "herbivory", "facilitation", "inhibition"A.2. Management Interventions"fertilizer", "stocking", "mowing", "grazing", "irrigation", "organic", "controlled burn", "restoration", "reforestation", "afforestation", "rewilding", "habitat creation", "invasive species control", "predator control", "captive breeding", "protected area", "translocation"The complete vocabulary is available in machine-readable JSON format at: https://github.com/sciknoworg/deep-research/blob/main/scripts/vocab/ecology_dictionaries.jsonB. Detailed Qualitative AnalysisB.1. Depth Parameter EffectsAnalysis of depth parameter variation reveals a fundamental transition in analytical sophistication that transcends simple quantitative scaling.Enhancement from d1 to d4 produces a 5.9-fold increase in average source utilization (from 18.9 to 111.1 sources) while maintaining comparable content length, indicating that depth parameters enable qualitatively different synthesis modes characterized by enhanced analytical penetration rather than mere scope expansion.The transformation in mechanistic understanding proves most evident when comparing synthesis outputs across depth levels.Low-depth configurations (d1) typically provide broad generalizations with minimal mechanistic detail, employing descriptive language that reports empirical relationships without explaining underlying processes.A representative d1 grassland analysis exemplifies this pattern: "Extensification packages suppress herbage or milk output by 10-40%."While this statement provides useful quantitative information about management outcomes, it offers no insight into the causal pathways or ecological mechanisms driving these effects.In stark contrast, high-depth configurations (d4) deliver comprehensive mechanistic frameworks that explicitly trace causal sequences from initial interventions through intermediate processes to ultimate outcomes.The same grassland management question addressed at d4 provides detailed process-level explanations: "Nutrient withdrawal shifts competitive hierarchies from fast-growing tall grasses toward stress-tolerators by: (i) reducing soil NO − 3 and NH + 4 availability, (ii) decreasing leaf N content and photosynthetic capacity in dominants, (iii) opening ground-layer light niches through reduced canopy closure, enabling germination of small-seeded forbs." This progression from empirical observation to mechanistic understanding represents a qualitative shift in synthesis capability, with d4 documents consistently capturing biochemical pathways, ecological feedbacks, and system dynamics that remain entirely implicit or absent in lower-depth analyses.Temporal precision emerges as another critical differentiator across depth levels.Low-depth syntheses employ vague temporal descriptors such as "several years, " "long-term, " or "historically, " providing little guidance for practical implementation or hypothesis testing.High-depth configurations transform this temporal vagueness into precise quantitative thresholds essential for ecological management and prediction.D4 syntheses consistently specify exact timeframes: species richness recovery occurs within "5-6 years, " functional diversity lags require "≥10 years, " and diversity-productivity trade-offs emerge at "ca. 18-22 years." This precision extends beyond simple duration reporting to include process-specific temporal sequences, seasonal timing requirements, and critical intervention windows.The apparent stability in temporal precision metrics (0.583 for  1  1 versus 0.549 for  4  4 , as measured by the temporal component in Equation1) initially seems counterintuitive but reflects a phenomenon: as source integration increases 21-fold, maintaining comparable precision becomes increasingly challenging due to the need to reconcile conflicting temporal information across diverse studies.This pattern indicates that high-depth configurations successfully integrate temporal information from expanded source sets while preserving precision, demonstrating robust temporal synthesis capabilities under high information loads.Causal reasoning sophistication shows marked enhancement with depth parameter increases.While d1 and d4 configurations achieve similar raw depth scores (0.494 versus 0.500), calculated using Equation1), qualitative analysis reveals that d4 documents contain 3.2 times more multi-step causal sequences, linking distal causes through proximate mechanisms to ultimate ecological outcomes.This multiplication of causal chains indicates not merely more causal statements but fundamentally more sophisticated causal reasoning that captures the complex, indirect pathways characteristic of ecological systems.The depth enhancement enables synthesis outputs to move beyond simple cause-effect pairs to construct integrated causal networks that better represent ecological reality.B.2. Breadth Parameter EffectsBreadth parameter drives a systematic expansion from geographically and methodologically constrained analyses to globally comprehensive syntheses that capture the full spectrum of ecological variation.Quantitative analysis shows that progression from b1 to b4 produces a 5.8-fold increase in source utilization (from 19.2 to 110.8 sources on average) while maintaining proportional content expansion, indicating that breadth parameters facilitate the integration of diverse evidence rather than superficial coverage expansion.Geographic coverage transformation represents the most immediately evident manifestation of breadth enhancement.Low-breadth configurations (b1) typically focus on specific biogeographic regions, averaging 3.7 geographic regions, with a heavy emphasis on well-studied European or North American temperate systems.These syntheses often present detailed insights into regional management practices but offer limited applicability beyond their focal geography.The concentration on familiar systems reflects both source availability bias and the computational constraints of limited breadth parameters that prevent comprehensive geographic integration.High-breadth configurations (b4) achieve substantially enhanced global perspective, with geographic coverage expanding to an average of 4.9 regions while systematically incorporating evidence from multiple continents.A representative b4 synthesis demonstrates this transformation by integrating findings from "North America, Europe, Asia, and Australia," explicitly recognizing biogeographic variation in species responses, management effectiveness, and ecological constraints.This expanded geographic scope enables identification of context-dependencies and boundary conditions that remain entirely invisible in regionally-focused analyses, providing managers with nuanced understanding of when and where specific interventions prove effective.Methodological diversity shows parallel enhancement with breadth parameters.Low-breadth syntheses average only 2.6 intervention types, typically focusing on single management approaches or closely related intervention clusters.This methodological constraint limits the ability to compare alternative strategies or identify optimal intervention combinations.High-breadth configurations expand intervention coverage to 3.2 categories on average, systematically integrating diverse management philosophies and implementation approaches.A representative b4 Phragmites management synthesis exemplifies this comprehensiveness by evaluating "chemical (glyphosate, imazapyr), mechanical (mowing, excavation), biological (goat grazing), and hydrological (salinity manipulation) control methods," providing practitioners with comparative assessment across the intervention spectrum rather than advocacy for single approaches.The breadth enhancement particularly influences synthesis generalizability through systematic identification of context-dependencies and biogeographic patterns.Higher breadth configurations consistently achieve superior breadth scores (0.473 for d4_b4 versus 0.376 for d4_b1, calculated using Equation2), reflecting enhanced capacity to identify cross-regional patterns while explicitly acknowledging boundary conditions and regional variations.This dual capability-recognizing both generalities and exceptions-proves essential for developing robust management recommendations that maintain validity across diverse implementation contexts.B.3. Source Utilization and Synthesis EfficiencyThe relationship between parameter configuration and synthesis capability follows a precise power law ( 2 = 0.97), revealing systematic scaling properties that transcend simple linear effects.Our analysis of 196 synthesis documents demonstrates that maximum parameter configurations (d4_b4) achieve a 21.2-fold increase in source utilization compared to baseline (d1_b1), with mean source counts escalating from 9.1 ± 1.7 to 192.9 ± 31.2 sources per synthesis.This exponential scaling pattern indicates that higher parameter configurations enable qualitatively different modes of information integration.The efficiency implications prove particularly striking when examining the relationship between source utilization and content generation.While source utilization increases 21.2-fold, word count expands only 41.5% (from 1,579 to 2,234 words), yielding a dramatic 14.9-fold enhancement in information density measured as sources per 1,000 words (Equation6).This disproportionate scaling demonstrates that parameter enhancement drives analytical depth rather than content inflation, with high-parameter configurations achieving superior knowledge integration while maintaining proportional document length.Three critical characteristics define this scaling behavior.First, the relationship exhibits consistency across ecological domains, with coefficients of variation remaining stable (0.16-0.19) across all 50 research questions spanning grassland management, invasive species control, and biodiversity conservation.This domain-independent scaling suggests that the observed patterns reflect fundamental properties of the synthesis system rather than artifacts of particular research areas.Second, the scaling demonstrates clear threshold effects, with minimal improvements occurring until both parameters exceed moderate values ( ≥ 2,  ≥ 2), after which dramatic gains materialize.Overall quality scores increase by only
M D Skarlinski, S Cox, J M Laurent, J D Braza, M Hinks, M J Hammerling, M Ponnapati, S G Rodriques, A D White, arXiv:2409.13740Language agents achieve superhuman synthesis of scientific knowledge. 2024arXiv preprint</p>
<p>Large legal fictions: Profiling legal hallucinations in large language models. M Dahl, V Magesh, M Suzgun, D E Ho, Journal of Legal Analysis. 162024</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in neural information processing systems. 332020</p>
<p>Retrieval augmentation reduces hallucination in conversation. K Shuster, S Poff, M Chen, D Kiela, J Weston, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, 10.48550/arXiv.2312.07559arXiv:2312.07559Paperqa: Retrievalaugmented generative agent for scientific research. 2023arXiv preprint</p>
<p>Orkg ask: a neuro-symbolic scholarly search and exploration system. A Oelen, M Y Jaradeh, S Auer, arXiv:2412.049772024arXiv preprint</p>
<p>Core: A global aggregation service for open access papers. P Knoth, D Herrmannova, M Cancellieri, L Anastasiou, N Pontika, S Pearce, B Gyawali, D Pride, Scientific Data. 103662023</p>
<p>Llms4synthesis: Leveraging large language models for scientific synthesis. H Babaei Giglou, J Souza, S Auer, Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries. the 24th ACM/IEEE Joint Conference on Digital Libraries2024</p>
<p>Yescieval: Robust llm-as-a-judge for scientific question answering. J Souza, H B Giglou, Q Münch, arXiv:2505.142792025arXiv preprint</p>
<p>X Hu, H Fu, J Wang, Y Wang, Z Li, R Xu, Y Lu, Y Jin, L Pan, Z Lan, arXiv:2410.14255An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Nova2024arXiv preprint</p>
<p>K Pu, K Feng, T Grossman, T Hope, B D Mishra, M Latzke, J Bragg, J C Chang, P Siangliulue, arXiv:2410.04025Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. 2024arXiv preprint</p>
<p>T Sandholm, S Dong, S Mukherjee, J Feland, B A Huberman, arXiv:2411.03575Semantic navigation for ai-assisted ideation. 2024arXiv preprint</p>
<p>L Li, W Xu, J Guo, R Zhao, X Li, Y Yuan, B Zhang, Y Jiang, Y Xin, R Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024arXiv preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. M Radensky, S Shahid, R Fok, P Siangliulue, T Hope, D S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.06292The AI Scientist: Towards fully automated open-ended scientific discovery. 2024arXiv preprint</p>
<p>H Su, R Chen, S Tang, X Zheng, J Li, Z Yin, W Ouyang, N Dong, arXiv:2410.09403Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. 2024arXiv preprint</p>
<p>An effective framework for measuring the novelty of scientific articles through integrated topic modeling and cloud model. Z Wang, H Zhang, J Chen, H Chen, Journal of Informetrics. 181015872024</p>
<p>A practical guide to question formation, systematic searching and study screening for literature reviews in ecology and evolution. Y Z Foo, R E O'dea, J Koricheva, S Nakagawa, M Lagisz, 10.1111/2041-210X.13654Methods in Ecology and Evolution. 122021</p>            </div>
        </div>

    </div>
</body>
</html>
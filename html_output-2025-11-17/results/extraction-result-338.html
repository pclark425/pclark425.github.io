<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-338 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-338</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-338</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-222066988</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.395.pdf" target="_blank">Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions</a></p>
                <p><strong>Paper Abstract:</strong> The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as “put a hot piece of bread on a plate”. Currently, the best-performing models are able to complete less than 1% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information, the starting location in the virtual environment, is incorporated, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases, suggesting contextualized language models may provide strong planning modules for grounded virtual agents.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e338.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e338.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (fine-tuned visual-semantic planner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 Medium transformer (pretrained) fine-tuned to translate high-level natural language directives into sequences of {command, arg1, arg2} action triples for the ALFRED / AI2-THOR embodied task; generates plans from text alone and optionally with a single starting-location input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are unsupervised multitask learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Medium)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>325M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 Medium (24 layers, 16 attention heads, ~325M parameters) pre-trained on large text corpora and fine-tuned on directive→gold-command-sequence pairs from ALFRED; generation uses nucleus sampling (p=0.9) from a prompt of the form "<Directive> [SEP]" until an EOS marker.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED visual semantic planning (AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Convert a high-level natural language directive (e.g., "put a cold slice of lettuce on the table") into a detailed sequence of executable action triples ({command, arg1, arg2}) that accomplish the goal in an AI2-THOR virtual home environment; actions include goto, pickup, put, cool, heat, clean, slice, toggle, arguments reference objects and receptacles drawn from the scene.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / household tasks / instruction following / object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial (procedural: multi-step action sequences; object-relational: objects and receptacles and their roles in actions; spatial: location tokens, e.g., 'goto <location>' and location arguments)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (implicit knowledge) + supervised fine-tuning on ALFRED directive→command-sequence pairs; few-shot behavior examined via downsampled training</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning on paired directive→command-sequences, prompted generation from '<Directive> [SEP]' (zero-shot at inference for unseen directives within fine-tuned model); nucleus sampling used for decoding; few-shot data ablation experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in model weights (distributed language representations) with outputs realized as natural-language-encoded action sequences ({command, arg1, arg2} triples); spatial information expressed as location tokens within triples (e.g., 'goto kitchen'), object-relational information expressed as argument tokens (object and receptacle lexical items), and procedural knowledge encoded as learned sequential patterns mapping directives → ordered action triples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>per-element accuracy (command / arg1 / arg2), triple-in-position accuracy, full-sequence accuracy (exact match), both strict (exact token match) and permissive (single-token overlap in arguments) scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Text-only (no visual input): command accuracy 90.8%; arg1 strict 69.9%, arg2 strict 63.8%; triple accuracy strict 65.8%; full-sequence exact match strict 22.2% (permissive 26.1%). When the starting location (first-step location) is provided (i.e., first goto known / omitted): full-sequence strict rises to 53.4% (permissive 58.2%). Few-shot / data-ablation: with only 10% training data average accuracy falls ~24% but remains ≈44% (condition-dependent); 1% training yields ≈8% full sequences in few-shot setting.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>High accuracy predicting procedural elements (commands) and inferring objects mentioned in directives: command token prediction ≈91% and object argument errors were rare (wrong object only ≈4% of error cases); model reliably produces correct action sequences when directives explicitly contain procedural cues and object names; many plans are plausible and can contain harmless extra actions or reorderings that wouldn't prevent task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Spatial/location errors are the dominant failure mode: predicting wrong location argument accounted for ≈45% of analyzed errors, indicating weak grounding of precise spatial assignments from text alone; model also produced extra (22% of errorful cases) or missing actions (12%), causing sequence offsets; some failures derive from inconsistencies in crowdsourced gold annotations (≈17% gold directive/object mismatches, ≈13% missing subtasks).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to an RNN encoder-decoder baseline trained on same data: GPT-2 improves argument prediction by ≈+5 percentage points, triple accuracy (strict) GPT-2 65.8% vs RNN 60.2%, full-sequence strict GPT-2 22.2% vs RNN 17.1%. No random-agent baselines reported for this textual-planning-only setup.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Providing one piece of visual information (starting location / first 'goto') more than doubled full-sequence performance (strict: 22.2% → 53.4%; permissive: 26.1% → 58.2%), showing spatial cue importance; data ablation: reducing training to 10% reduces accuracy ≈24% but model retains substantial capability (~44% in reported condition); 1% training yields ≈8% full sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large pretrained LMs (GPT-2) encode rich procedural and object-relational knowledge sufficient to generate many detailed multi-step embodied plans from directives alone (≈26% full plans exact match), but they encode spatial specificity poorly: supplying a minimal spatial cue (starting location) dramatically increases correctness (~58% permissive), indicating spatial assignments are not reliably inferable from language-only pretraining and fine-tuning, while procedural sequencing and object-role assignment are robustly captured by the model's weights.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e338.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e338.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RNN encoder-decoder with attention (GLoVE-initialized)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard recurrent neural network encoder-decoder with attention, using 300-d GLoVE embeddings, trained to translate directives into sequences of {command,arg1,arg2} triples as a baseline planner for ALFRED.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation by jointly learning to align and translate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN encoder-decoder (attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder architecture using RNNs (attention-based) initialized with 300-dimensional GLoVE embeddings; serves as a baseline sequence-to-sequence model converting directives → natural-language command triples; standard seq2seq training on ALFRED pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED visual semantic planning (AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same ALFRED task as above: map a high-level natural-language directive to a full sequence of executable action triples for a virtual household environment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / household tasks / instruction following / object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial (same mixture as GPT-2, but encoded via RNN parameters and GLoVE-initialized token embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised fine-tuning on ALFRED directive→command pairs; initial lexical knowledge from GLoVE embeddings (pretrained word vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning of seq2seq RNN encoder-decoder with attention; inference by greedy/beam decoding (not detailed beyond standard seq2seq), no visual input except in variant experiments where starting location may be provided externally.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in RNN weights and GLoVE-initialized word embeddings; outputs are natural-language-encoded action sequences ({command,arg1,arg2}); spatial and object relations encoded token-wise in arguments and learned sequence patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>same as GPT-2: command / arg / triple / full-sequence accuracy, strict and permissive variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Text-only (no visual input): command accuracy 89.6%; arg1 strict 64.8%, arg2 strict 58.4%; triple accuracy strict 60.2%; full-sequence exact match strict 17.1% (permissive 23.6%). With first-location provided (Full Minus First) strict 43.6% (permissive 26.1% listed in table appears to be inconsistent with text ordering — primary reported is strict Full Minus First 43.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Accurately predicts commands (≈90%); captures common procedural templates and frequent object assignments present explicitly in directives; benefits substantially when the first-step location is known.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Lower argument prediction accuracy vs GPT-2 (Arg1/Arg2 errors higher); fails often on precise object/receptacle naming and location assignment leading to lower triple and full-sequence exact matches; suffers from sequence generation artifacts requiring heavy post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly in paper to GPT-2: RNN underperforms GPT-2 across argument, triple, and full-sequence metrics (e.g., full-sequence strict RNN 17.1% vs GPT-2 22.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Longer training required (convergence after many more epochs) and higher sensitivity to training-data size; providing first-location information increases full-sequence accuracy substantially (Full Minus First reported 43.6% strict vs 17.1% full sequence strict without that information). Data downsampling effects similar to GPT-2 but absolute performance lower.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RNN seq2seq baselines can learn procedural patterns and some object-relational mapping from directives, but they encode these less effectively than a large pre-trained transformer (GPT-2), particularly for argument (object/receptacle) prediction and full-sequence planning; both architectures struggle most with spatial specificity unless given explicit location cues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks <em>(Rating: 2)</em></li>
                <li>Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments <em>(Rating: 2)</em></li>
                <li>Shifting the baseline: Single modality performance on visual navigation & QA <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-338",
    "paper_id": "paper-222066988",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "GPT-2 planner",
            "name_full": "GPT-2 (fine-tuned visual-semantic planner)",
            "brief_description": "A GPT-2 Medium transformer (pretrained) fine-tuned to translate high-level natural language directives into sequences of {command, arg1, arg2} action triples for the ALFRED / AI2-THOR embodied task; generates plans from text alone and optionally with a single starting-location input.",
            "citation_title": "Language models are unsupervised multitask learners",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Medium)",
            "model_size": "325M",
            "model_description": "GPT-2 Medium (24 layers, 16 attention heads, ~325M parameters) pre-trained on large text corpora and fine-tuned on directive→gold-command-sequence pairs from ALFRED; generation uses nucleus sampling (p=0.9) from a prompt of the form \"&lt;Directive&gt; [SEP]\" until an EOS marker.",
            "task_name": "ALFRED visual semantic planning (AI2-THOR)",
            "task_description": "Convert a high-level natural language directive (e.g., \"put a cold slice of lettuce on the table\") into a detailed sequence of executable action triples ({command, arg1, arg2}) that accomplish the goal in an AI2-THOR virtual home environment; actions include goto, pickup, put, cool, heat, clean, slice, toggle, arguments reference objects and receptacles drawn from the scene.",
            "task_type": "multi-step planning / household tasks / instruction following / object manipulation",
            "knowledge_type": "procedural + object-relational + spatial (procedural: multi-step action sequences; object-relational: objects and receptacles and their roles in actions; spatial: location tokens, e.g., 'goto &lt;location&gt;' and location arguments)",
            "knowledge_source": "pre-training on text corpora (implicit knowledge) + supervised fine-tuning on ALFRED directive→command-sequence pairs; few-shot behavior examined via downsampled training",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning on paired directive→command-sequences, prompted generation from '&lt;Directive&gt; [SEP]' (zero-shot at inference for unseen directives within fine-tuned model); nucleus sampling used for decoding; few-shot data ablation experiments reported.",
            "knowledge_representation": "Implicit in model weights (distributed language representations) with outputs realized as natural-language-encoded action sequences ({command, arg1, arg2} triples); spatial information expressed as location tokens within triples (e.g., 'goto kitchen'), object-relational information expressed as argument tokens (object and receptacle lexical items), and procedural knowledge encoded as learned sequential patterns mapping directives → ordered action triples.",
            "performance_metric": "per-element accuracy (command / arg1 / arg2), triple-in-position accuracy, full-sequence accuracy (exact match), both strict (exact token match) and permissive (single-token overlap in arguments) scoring.",
            "performance_result": "Text-only (no visual input): command accuracy 90.8%; arg1 strict 69.9%, arg2 strict 63.8%; triple accuracy strict 65.8%; full-sequence exact match strict 22.2% (permissive 26.1%). When the starting location (first-step location) is provided (i.e., first goto known / omitted): full-sequence strict rises to 53.4% (permissive 58.2%). Few-shot / data-ablation: with only 10% training data average accuracy falls ~24% but remains ≈44% (condition-dependent); 1% training yields ≈8% full sequences in few-shot setting.",
            "success_patterns": "High accuracy predicting procedural elements (commands) and inferring objects mentioned in directives: command token prediction ≈91% and object argument errors were rare (wrong object only ≈4% of error cases); model reliably produces correct action sequences when directives explicitly contain procedural cues and object names; many plans are plausible and can contain harmless extra actions or reorderings that wouldn't prevent task completion.",
            "failure_patterns": "Spatial/location errors are the dominant failure mode: predicting wrong location argument accounted for ≈45% of analyzed errors, indicating weak grounding of precise spatial assignments from text alone; model also produced extra (22% of errorful cases) or missing actions (12%), causing sequence offsets; some failures derive from inconsistencies in crowdsourced gold annotations (≈17% gold directive/object mismatches, ≈13% missing subtasks).",
            "baseline_comparison": "Compared to an RNN encoder-decoder baseline trained on same data: GPT-2 improves argument prediction by ≈+5 percentage points, triple accuracy (strict) GPT-2 65.8% vs RNN 60.2%, full-sequence strict GPT-2 22.2% vs RNN 17.1%. No random-agent baselines reported for this textual-planning-only setup.",
            "ablation_results": "Providing one piece of visual information (starting location / first 'goto') more than doubled full-sequence performance (strict: 22.2% → 53.4%; permissive: 26.1% → 58.2%), showing spatial cue importance; data ablation: reducing training to 10% reduces accuracy ≈24% but model retains substantial capability (~44% in reported condition); 1% training yields ≈8% full sequences.",
            "key_findings": "Large pretrained LMs (GPT-2) encode rich procedural and object-relational knowledge sufficient to generate many detailed multi-step embodied plans from directives alone (≈26% full plans exact match), but they encode spatial specificity poorly: supplying a minimal spatial cue (starting location) dramatically increases correctness (~58% permissive), indicating spatial assignments are not reliably inferable from language-only pretraining and fine-tuning, while procedural sequencing and object-role assignment are robustly captured by the model's weights.",
            "uuid": "e338.0"
        },
        {
            "name_short": "RNN baseline",
            "name_full": "RNN encoder-decoder with attention (GLoVE-initialized)",
            "brief_description": "A standard recurrent neural network encoder-decoder with attention, using 300-d GLoVE embeddings, trained to translate directives into sequences of {command,arg1,arg2} triples as a baseline planner for ALFRED.",
            "citation_title": "Neural machine translation by jointly learning to align and translate",
            "mention_or_use": "use",
            "model_name": "RNN encoder-decoder (attention)",
            "model_size": null,
            "model_description": "Encoder-decoder architecture using RNNs (attention-based) initialized with 300-dimensional GLoVE embeddings; serves as a baseline sequence-to-sequence model converting directives → natural-language command triples; standard seq2seq training on ALFRED pairs.",
            "task_name": "ALFRED visual semantic planning (AI2-THOR)",
            "task_description": "Same ALFRED task as above: map a high-level natural-language directive to a full sequence of executable action triples for a virtual household environment.",
            "task_type": "multi-step planning / household tasks / instruction following / object manipulation",
            "knowledge_type": "procedural + object-relational + spatial (same mixture as GPT-2, but encoded via RNN parameters and GLoVE-initialized token embeddings)",
            "knowledge_source": "supervised fine-tuning on ALFRED directive→command pairs; initial lexical knowledge from GLoVE embeddings (pretrained word vectors).",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised fine-tuning of seq2seq RNN encoder-decoder with attention; inference by greedy/beam decoding (not detailed beyond standard seq2seq), no visual input except in variant experiments where starting location may be provided externally.",
            "knowledge_representation": "Implicit in RNN weights and GLoVE-initialized word embeddings; outputs are natural-language-encoded action sequences ({command,arg1,arg2}); spatial and object relations encoded token-wise in arguments and learned sequence patterns.",
            "performance_metric": "same as GPT-2: command / arg / triple / full-sequence accuracy, strict and permissive variants.",
            "performance_result": "Text-only (no visual input): command accuracy 89.6%; arg1 strict 64.8%, arg2 strict 58.4%; triple accuracy strict 60.2%; full-sequence exact match strict 17.1% (permissive 23.6%). With first-location provided (Full Minus First) strict 43.6% (permissive 26.1% listed in table appears to be inconsistent with text ordering — primary reported is strict Full Minus First 43.6%).",
            "success_patterns": "Accurately predicts commands (≈90%); captures common procedural templates and frequent object assignments present explicitly in directives; benefits substantially when the first-step location is known.",
            "failure_patterns": "Lower argument prediction accuracy vs GPT-2 (Arg1/Arg2 errors higher); fails often on precise object/receptacle naming and location assignment leading to lower triple and full-sequence exact matches; suffers from sequence generation artifacts requiring heavy post-processing.",
            "baseline_comparison": "Compared directly in paper to GPT-2: RNN underperforms GPT-2 across argument, triple, and full-sequence metrics (e.g., full-sequence strict RNN 17.1% vs GPT-2 22.2%).",
            "ablation_results": "Longer training required (convergence after many more epochs) and higher sensitivity to training-data size; providing first-location information increases full-sequence accuracy substantially (Full Minus First reported 43.6% strict vs 17.1% full sequence strict without that information). Data downsampling effects similar to GPT-2 but absolute performance lower.",
            "key_findings": "RNN seq2seq baselines can learn procedural patterns and some object-relational mapping from directives, but they encode these less effectively than a large pre-trained transformer (GPT-2), particularly for argument (object/receptacle) prediction and full-sequence planning; both architectures struggle most with spatial specificity unless given explicit location cues.",
            "uuid": "e338.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "rating": 2,
            "sanitized_title": "alfred_a_benchmark_for_interpreting_grounded_instructions_for_everyday_tasks"
        },
        {
            "paper_title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
            "rating": 2,
            "sanitized_title": "visionandlanguage_navigation_interpreting_visuallygrounded_navigation_instructions_in_real_environments"
        },
        {
            "paper_title": "Shifting the baseline: Single modality performance on visual navigation & QA",
            "rating": 2,
            "sanitized_title": "shifting_the_baseline_single_modality_performance_on_visual_navigation_qa"
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 1,
            "sanitized_title": "language_models_as_knowledge_bases"
        }
    ],
    "cost": 0.010487499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions
Association for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 16 -20, 2020. 2020</p>
<p>Peter A Jansen pajansen@email.arizona.edu 
School of Information
University of Arizona
TucsonAZ</p>
<p>Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions</p>
<p>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings
the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsAssociation for Computational LinguisticsNovember 16 -20, 2020. 20204412
The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as "put a hot piece of bread on a plate". Currently, the best-performing models are able to complete less than 5% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information is incorporated, namely the starting location in the virtual environment, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases. Our results suggest that contextualized language models may provide strong visual semantic planning modules for grounded virtual agents.</p>
<p>Introduction</p>
<p>Simulated virtual environments with steadily increasing fidelity are allowing virtual agents to learn to perform high-level tasks that couple language understanding, visual planning, and embodied reasoning through sensorimotor grounded representations (Gordon et al., 2018;Puig et al., 2018;Wijmans et al., 2019). The ALFRED challenge task recently proposed by Shridhar et al. (2020) requires a virtual robotic agent to complete everyday tasks (such as "put cold apple slices on the table") in one of 120 interactive virtual home environments by generating and executing complex visually-grounded semantic plans that involve movable objects, irreversible state changes, and an egocentric viewpoint. Integrating natural language task directives with one "Wash the fork and put it away" Directive Figure 1: An example of the ALFRED grounded language task. In this work, we focus on visual semantic planningfrom the textual directive alone (top), our model predicts a visual semantic plan of {command, argument} tuples (captions) that matches the gold plan without requiring visual input (images).</p>
<p>of the most complex interactive virtual agent environments to date is challenging, with the current best performing systems successfully completing less than 5% of ALFRED tasks in unseen environments 1 , while common baseline models generally complete less than 1% of tasks successfully.</p>
<p>In this work we explore the visual semantic planning task in ALFRED, where the high-level natural language task directive is converted into a detailed sequence of actions in the AI2-THOR 2.0 virtual environment  that will accomplish that goal (see Figure 1). In contrast to previous approaches to visual semantic planning (e.g. Fried et al., 2018;Fang et al., 2019), we explore the performance limits of this task solely using goals expressed in natural language as input -that is, without visual input from the virtual environment. The contributions of this work are: 1. We model visual semantic planning as a sequence-to-sequence translation problem, and demonstrate that our best-performing GPT-2 model can translate between natural language directives and sequences of gold visual semantic plans in 26% of cases without visual input.</p>
<ol>
<li>
<p>We show that when a small amount of visual input is available -namely, the starting location in the virtual environment -our best model can successfully predict 58% of unseen visual semantic plans.</p>
</li>
<li>
<p>Our detailed error analysis suggests that repairing predicted plans with correct locations and fixing artifacts in the ALFRED dataset could substantially increase performance of this and future models.</p>
</li>
</ol>
<p>Related Work</p>
<p>Models for completing multi-modal tasks can achieve surprising performance using information from only a single modality. The Room-to-Room (R2R) visual language navigation task (Anderson et al., 2018) requires agents to traverse a discrete scene graph and arrive at a destination described using natural language. In ablation studies, Thomason et al. (2019) found that models using input from a single modality (either vision or language) often performed nearly as good as or better than their multi-modal counterparts on R2R and other visual QA tasks. Similarly, Hu et al. (2019) found that two state-of-the-art multi-modal agents performed significantly worse on R2R when using both linguistic and visual input instead of a single modality, while also showing that performance can improve by combining separate-modality models into mixture-of-expert ensembles. Where R2R requires traversing a static scene graph using locomotive actions, ALFRED is a dynamic environment requiring object interaction for task completion, and has a substantially richer action sequence space that includes 8 high-level actions. This work extends these past comparisons of unimodal vs. multimodel performance by demonstrating that strong performance on visual semantic planning is possible in a vastly more complex virtual environment using language input alone, through the use of generative language models.</p>
<p>Models and Embeddings</p>
<p>We approach the task of converting a natural language directive into a visual semantic plan -a series of commands that achieve that directive in a virtual environment -as a purely textual sequenceto-sequence translation problem, similar to conversion from Text-to-SQL (e.g. Yu et al., 2018;Guo et al., 2019). Here we examine two embedding methods that encode language directives and decode command sequences.</p>
<p>RNN:</p>
<p>A baseline encoder-decoder network for sequence-to-sequence translation tasks (e.g. Bahdanau et al., 2015), implemented using recurrent neural networks (RNNs). One RNN serves as an encoder for the input sequence, here the tokens representing the natural language directive. A decoder RNN network with attention uses the context vector of the encoder network to translate into output sequences of command triples representing the visual semantic plan. Both encoder and decoder networks are pre-initialized with 300-dimensional GLoVE embeddings (Pennington et al., 2014).</p>
<p>GPT-2:</p>
<p>The OpenAI GPT-2 transformer model (Radford et al., 2019), used in a text generation capacity. We fine-tune the model on sequences of natural languge directives paired with gold command sequences separated by delimiters
(i.e. "<Directive> [SEP] <CommandTuple 1 > [CSEP] <CommandTuple 2 > [CSEP] ... [CSEP] <CommandTuple N > [EOS]"
). During evaluation we provide the prompt "<Directive> [SEP]", and the model generates a command sequence until producing the end-of-sequence (EOS) marker. We make use of nucleus sampling (Holtzman et al., 2020) to select only tokens from the set of most likely tokens during generation, with p = 0.9, but do not make use of top-K filtering (Fan et al., 2018) or penalize repetitive n-grams, which are commonly used in text generation tasks, but are inappropriate here for converting to the often repetitive (at the scale of bigrams) command sequences. For tractability we make use of the GPT-2 Medium pre-trained model, which contains 24 layers, 16 attention heads, and 325M parameters. During evaluation, task directives are sorted into samelength batches to prevent generation artifacts from padding, and maintain high generation quality. 2  </p>
<p>Experiments</p>
<p>Dataset: The ALFRED dataset contains 6,574 gold command sequences representing visual semantic plans, each paired with 3 natural language directives describing the goal of those command sequences (e.g. ''put a cold slice of lettuce on the table") authored by mechanical turkers. High-level command sequences range from 3 to 20 commands (average 7.5), and are divided into 7 high-level categories (such as examine object in light, pick two objects then place, and pick then cool then place). Commands are represented as triples that pair one of 8 actions (goto, pickup, put, cool, heat, clean, slice, and toggle) with up to two arguments, typically the object of the action (such as "slicing lettuce") and an optional receptacle (such as "putting a spoon in a mug"). Arguments can reference 58 possible objects (e.g. butter knife, chair, or apple) and 26 receptacles (e.g. fridge, microwave, or bowl). To prevent knowledge of the small unseen test set for the full task, here we redivide the large training set into three smaller train, development, and test sets of 7,793, 5,661, and 7,571 goldseparate the general formula of action sequences with specific instances of objects in action sequences, which has been shown to help in Text-to-SQL translation (Guo et al., 2019). Pilot experiments with both RNNs and transformer models yielded slightly lower results than vanilla models. Language modeling: In addition to GPT-2 we also piloted XLNET, but perplexity remained high even after significant fine-tuning. directive/command-sequence pairs, respectively.</p>
<p>Processing Pipeline: Command sequences are read in as sequences of {command, arg1, arg2} triples, converted into natural language using completion heuristics (e.g. "{put, spoon, mug}" → "put the spoon in the mug", and augmented with argument delimiters to aid parsing (e.g. "put <arg1> the spoon <arg2> in the mug"). Input directives are tokenized, but receive no other preprocessing. Generated strings from all models are post-processed for common errors in sequenceto-sequence models, including token doubling, completing missing bigrams (e.g. "pick <arg1>" → "pick up <arg1>"), and heuristics for adding missing argument tags. Post-processed output sequences are then parsed and converted back into {command, arg1, arg2} tuples for evaluation.</p>
<p>Evaluation Metrics: Performance in translating between natural language directives and sequences of command triples is evaluated in terms of accuracy at the command-element (command, argu-ment1, argument2), triple, and full-sequence level. Because our generation includes only textual input and no visual input for a given virtual environment, commands may be generated that reference objects that do not exist in a scene (such as generating an action to toggle a "lamp" to examine an object, when the environment specifically contains a "desk lamp"). As such we include two scoring metrics: a strict metric that requires exact matching of each token in an argument to be counted as correct, and a permissive metric that requires matching only a single token within an argument to be correct.</p>
<p>Strict Scoring</p>
<p>butter knife = knife Permissive Scoring desk lamp = lamp All accuracy scoring is binary. Triples receive a score of one if all elements in a given gold and predicted triple are identical, and zero oth-   </p>
<p>Results</p>
<p>Performance of the embedding models is reported in Table 1, broken down by triple components, full triples, and full sequences. Both models achieve approximately 90% accuracy in predicting the correct commands, in the correct location i in the sequence. Arguments are predicted less accurately, with the RNN model predicting 65% and 58% of first and second arguments correctly, respectively. The GPT-2 model increases performance on argument prediction by approximately +5%, reaching 70% and 64% under strict match scoring. Permissive scoring, allowing for partial matches between arguments (e.g. "lamp" and "desk lamp" are considered equivalent) further increases argument scoring to approximately 74% and 65% in the best model. Scoring by complete triples in the correct location i shows a similar pattern of performance, with the best-scoring GPT-2 model achieving 66% accuracy using strict scoring, and 69% under permissive scoring, with triple accuracy broken down by command shown in Table 2. Fully-correct predicted sequences of commands that perfectly match gold visual semantic plans using only the text directives as input, -i.e. without visual input from the virtual environment -occur in 17% of unseen test cases with the RNN model, and 22% of cases with the GPT-2 model, highlighting how detailed and accurate visual plans can be constructed from text input alone in a large subset of cases. In analyzing the visual semantic plans, the first command is typically to move the virtual agent to a starting location that contains the first object it must interact with (for example, moving to the countertop, where a potato is resting in the initialized virtual environment, to begin a directive about slicing, washing, and heating a potato slice). If we supply the model with this single piece of visual information from the environment, fullsequence prediction accuracy for all models more than doubles, increasing to 53% in the strict condition, and 58% with permissive scoring, for the best-performing GPT-2 model. Table 3 shows an analysis of common categories of errors in 100 directive/visual semantic plan pairs randomly drawn from the development set that were not answered correctly by the best-performing GPT-2 model that includes the starting location for the first step. As expected, a primary source of error is the lack of visual input in generating the visual plans, with the most common error, predicting the wrong location in an argument, occuring in 45% of errors. 4 Conversely, predicting the wrong object to interact with occurred in only 4% of errors, as this information is often implicitly or explicitly supplied in the text directive. This suggests augmenting the model with object locations from the environment could mend prediction errors in nearly half of all errorful plans.</p>
<p>Error Analysis</p>
<p>The GPT-2 model predicted additional (incorrect) actions in 22% of errorful predictions, while missing key actions in 12% of errors, causing offset errors in sequence matching that reduced overall performance in nearly a quarter of cases. In a small number of cases, the model predicted extra actions that were not harmful to completing the goal, or switched the order of sets of actions that could be completed independently (such as picking up and moving two different objects to a single location). In both cases the virtual agent would likely have been successful in completing the directive if following these plans.</p>
<p>A final significant source of error includes inconsistencies in the crowdsourced text directives or gold visual semantic plans themselves. In 17% of errors, the gold task directive had a mismatch with the objects referenced in the gold commands (e.g. the directive referenced a watering can, where the gold annotation references a tea pot), and automated scoring marked the predicted sequence as incorrect. Similarly, in 13% of cases, the task directive failed to mention one or more subtasks (e.g. the directive is "turn on a light", but the gold command sequence also includes first retrieving a specific object to examine in the light). This suggests that nearly one-third of errors may be due to issues in the evaluation data, and that overall visual semantic plan generation performance may be significantly higher.</p>
<p>Data Dependence and Few-Shot Learning</p>
<p>To examine how performance varies with the amount of training data available, we randomly downsampled the amount of training data to 25%, 10%, and 1% of its original size. This analysis, shown in Figure 2, demonstrates that relatively high performance on the visual semantic prediction task is still possible with comparatively little training data. When only 10% of the original training data is used, average prediction accuracy reduces by 24%, but still reaches 44%. In the few-shot case (1% downsampling), where each of the 7 ALFRED tasks observes only 4 gold command sequences each (for a total of 12 natural language directives ing set size (100%, 25%, 10%, or 1% of the full training set) for the GPT-2 model on the test set. Even with a large rediction in training data, the model is still able to accurrately predict a large number of visual semantic plans. Performance represents the permissive scoring metric in the "full minus first" condition in Table 1. per task) during training, the GPT-2 model is still able to generate an accurate visual semantic plan in 8% of cases. Given that large pre-trained language models have been shown to encode a variety of commonsense knowledge as-is, without finetuning (Petroni et al., 2019), it is possible that some of the model's few-shot performance on ALFRED may be due to an existing knowledge of similar common everyday tasks.</p>
<p>Conclusion</p>
<p>We empirically demonstrate that detailed gold visual semantic plans can be generated for 26% of unseen task directives in the ALFRED challenge using a large pre-trained language model without visual input from the simulated environment, where 58% can be generated if starting locations are known. We envision these plans may be used either as-is, or as an initial "hypothetical" plan of how the model believes the task might be solved in a generic environment, that is then modified based on visual or other input from a specific environment to further increase overall accuracy. We release our planner code, data, predictions, and analyses for incorporation into end-to-end systems at: http://github.com/cognitiveailab/ alfred-gpt2/ . </p>
<p>3 :
3(left) Common classes of prediction errors in the GPT-2 model, and their proportions in 100 predictions from the development set. (right) Example errors, where (G) and (P) represent subsets of gold and predicted visual semantic plans, respectively.erwise. Full-sequence scoring directly compares <CommandTuple i > for each i in the gold and predicted sequences, and receives a score of one only if all triples are identical and in identical locations i, and zero otherwise. 3</p>
<p>Figure 2 :
2Average prediction accuracy as a function of train-</p>
<p>Table 1 :
1Average prediction accuracy on the unseen test set broken down by triple components, full triples, and full visual semantic plans. Full Sequence accuracy represents the proportion of predicted visual semantic plans that perfectly match gold plans. Full Minus First represents the same, but omitting the first tuple, typically a {goto, location} that moves the agent to the starting location in the virtual environment (see description in text).Triple Components 
Full 
Entire Visual Semantic Plans 
Model 
Command 
Arg1 
Arg2 
Triples 
Full Sequence Full Minus First 
Strict Scoring 
RNN 
89.6% 
64.8% 58.4% 
60.2% 
17.1% 
43.6% 
GPT-2 
90.8% 
69.9% 63.8% 
65.8% 
22.2% 
53.4% 
Permissive Scoring 
RNN 
89.6% 
70.6% 61.4% 
65.9% 
23.6% 
26.1% 
GPT-2 
90.8% 
73.8% 65.1% 
69.4% 
26.1% 
58.2% </p>
<p>Model 
G o t o 
P i c k u p 
P u t 
C o o l 
H e a t 
C l e a n 
S l i c e 
T o g g l e A v g . </p>
<p>RNN 
59 81 60 77 69 83 67 91 66 
GPT-2 63 84 66 72 77 82 70 94 69 </p>
<p>Table 2 :
2Average triple prediction accuracy on the test set broken down into each of the 8 possible ALFRED commands. Values represent percentages. Goto has an N of 24k, Pick up an N of 11k, and Put an N of 10k. All other commands occur approximately 1000 times in the test dataset.</p>
<p>) ... slice lettuce, put knife in microwave, put lettuce in fridge, ...Prop. Error Class Description 
Example Errors </p>
<p>Incorrect Arguments 
Predicted wrong location: 
45% Predicted wrong location 
(G) ... slice lettuce, put knife on countertop, put lettuce in fridge, ... 
4% 
Predicted wrong object 
(PIncorrect Triples 
Predicted extra (not harmful) action  † , and introduced offset error  ‡ 
22% Offset due to extra/missing actions 
Instructions: Put a mug with a spoon in the sink. 
22% Predicted extra (incorrect) actions 
(G) ... pick up mug, put mug in sink basin  ‡ 
12% Predicted missed actions 
(P) ... pick up mug, go to sink basin  † , put mug in sink basin  ‡ 
7% 
Predicted extra (not harmful) actions 
5% 
Order of actions swapped </p>
<p>Instruction Errors 
Gold Instructions Incomplete: 
17% Gold Instructions Incorrect 
Instructions: Put a heated mug in the microwave. 
13% Gold Instructions Incomplete 
(G) ... go to microwave, heat mug, go to cabinet, put mug in cabinet </p>
<p>Table</p>
<p>Gould, and Anton van den Hengel. 2018. Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3683. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR). Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898.Angela 
https://leaderboard.allenai.org/ alfred/
Negative results not reported for space: We hypothesized that separating visual semantic plans into variablized action-sequence templates and variable-value assignments represented as separate decoders would help models learn to
Tuning and Computational Resources: RNN models required approximately 100k epochs of training to reach convergence over 12 hours, requiring 8GB of GPU RAM. GPT-2 models asymptoted performance at 25 epochs, requiring 6 hours of training and 16GB of GPU RAM. All experiments were conducted using an NVIDIA Titan RTX.
An unexpected source of error is that our GPT-2 planner frequently prefers to store used cutlery in either the fridge or microwave -creating a moderate fire hazard. Interestingly, this behavior appears learned from the training data, which frequently stores cutlery in unusual locations. Disagreements on discarded cutlery locations occurred in 15% of all errors.</p>
<p>. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, StephenPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen</p>
<p>Scene memory transformer for embodied agents in long-horizon tasks. Kuan Fang, Alexander Toshev, Li Fei-Fei, Silvio Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. 2019. Scene memory transformer for em- bodied agents in long-horizon tasks. In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition, pages 538-547.</p>
<p>Speaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Advances in Neural Information Processing Systems. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. 2018. Speaker-follower mod- els for vision-and-language navigation. In Advances in Neural Information Processing Systems, pages 3314-3325.</p>
<p>Iqa: Visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDaniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. 2018. Iqa: Visual question answering in in- teractive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 4089-4098.</p>
<p>Towards complex text-to-sql in cross-domain database with intermediate representation. Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, Dongmei Zhang, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsJiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang. 2019. Towards complex text-to-sql in cross-domain database with intermediate representation. In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4524- 4535.</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Are you looking? grounding to multiple modalities in vision-and-language navigation. Ronghang Hu, Daniel Fried, Anna Rohrbach, Dan Klein, Trevor Darrell, Kate Saenko, 10.18653/v1/P19-1655Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsRonghang Hu, Daniel Fried, Anna Rohrbach, Dan Klein, Trevor Darrell, and Kate Saenko. 2019. Are you looking? grounding to multiple modalities in vision-and-language navigation. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pages 6551-6557, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474arXiv preprintEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van- derBilt, Luca Weihs, Alvaro Herrasti, Daniel Gor- don, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474.</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Empirical Methods in Natural Language Processing (EMNLP). Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word rep- resentation. In Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1532-1543.</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 2463-2473.</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 8494-8502.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Computer Vision and Pattern Recognition (CVPR). Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Computer Vision and Pattern Recognition (CVPR).</p>
<p>Shifting the baseline: Single modality performance on visual navigation &amp; QA. Jesse Thomason, Daniel Gordon, Yonatan Bisk, 10.18653/v1/N19-1197Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics1MinneapolisJesse Thomason, Daniel Gordon, and Yonatan Bisk. 2019. Shifting the baseline: Single modality per- formance on visual navigation &amp; QA. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1977-1983, Min- neapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Embodied question answering in photorealistic environments with point cloud perception. Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, and Dhruv Batra. 2019. Em- bodied question answering in photorealistic environ- ments with point cloud perception. In The IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR).</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn- ing Yao, Shanelle Roman, et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 3911-3921.</p>
<p>Visual semantic planning using deep successor representations. Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionYuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, and Ali Farhadi. 2017. Visual semantic planning using deep successor representations. In Proceedings of the IEEE International Conference on Computer Vi- sion, pages 483-492.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2318 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2318</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2318</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-267200218</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.13023v2.pdf" target="_blank">Enabling global image data sharing in the life sciences</a></p>
                <p><strong>Paper Abstract:</strong> Despite the importance of imaging in biological and medical research, a large body of informative and precious image data never sees the light of day. To ensure scientific rigor as well as the reuse of data for scientific discovery, image data need to be made FAIR (findable, accessible, interoperable and reusable). Image data experts are working together globally to agree on common data formats, metadata, ontologies and supporting tools toward image data FAIRification. With this Perspective, we call on public funders to join these efforts to support their national scientists. What researchers most urgently need are openly accessible resources for image data storage that are operated under long-term commitments by their funders. Although existing resources in Australia, Japan and Europe are already collaborating to enable global image data sharing, these efforts will fall short unless more countries invest in operating and federating their own open data resources. This will allow us to harvest the enormous potential of existing image data, preventing substantial loss of unrealized value from past investments in imaging acquisition infrastructure. This Perspective provides a roadmap toward globally federated open data resources for bioimaging data.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2318.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2318.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep learning (medical imaging)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep learning for medical image analysis and computer-aided detection/diagnosis (CAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of deep neural networks and classical ML for detection, diagnosis, segmentation, prognosis, and decision support in radiology and other medical imaging domains, building on decades of CAD work and recent FDA-cleared systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Artificial intelligence in radiology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Medical imaging (radiology, diagnostic image analysis, therapy guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automatically detect, localize, classify, and quantify pathologies and imaging biomarkers to assist diagnosis, prognosis, risk assessment, and therapy planning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies: many medical images exist (DICOM standard facilitates sharing) and public repositories (TCIA, IDC, MIDRC) provide curated datasets, but labeled/annotated data are often limited and heterogeneously curated; access and governance differ across repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-resolution image data (DICOM), often multi-modal (CT/MRI/PET/US), coupled with clinical and demographic metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — large high-dimensional images, diverse acquisition protocols and parameters, non-linear relationships, variable quality and noise, and substantial preprocessing/harmonization needs; regulatory considerations add constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively mature compared to other bioimaging domains: CAD approaches since 1960s, several FDA-cleared systems exist, but rapidly evolving with deep learning advances.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — clinical deployment requires interpretability, traceable provenance, validation on target populations, and regulatory-grade evidence; 'black-box' models are discouraged without transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised deep learning (CNNs / other deep architectures) for classification, detection, segmentation; also hybrid ML/CAD pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>End-to-end or modular neural-network based systems trained on curated labeled images and clinical metadata to perform classification, detection, segmentation, or prediction; training requires careful data curation, harmonization, bias mitigation, and validation protocols tailored to the clinical claim and intended population.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (with hybrid CAD pipelines and some unsupervised/pretraining elements)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for diagnostic tasks given curated labeled datasets and standardized formats (DICOM); limitations include variability across sites, need for extensive validation, and governance/ethical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective in many diagnostic tasks (historical and recent FDA approvals cited), but performance depends strongly on data quality, representativeness, annotation standardization, and evaluation procedures; risk of 'garbage in, garbage out' if data poorly curated.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can improve detection/diagnosis, triage, quantification, and workflow efficiency; could enable scaled secondary analyses and integration with other omics/clinical data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper contrasts long-standing CAD approaches with modern deep learning — deep learning offers improved pattern learning but requires more curated data and compute; also emphasizes that models must be validated for each clinical claim and population (no off-label use).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Standardized data formats (DICOM), well-curated and diverse labeled datasets, repository access, metadata harmonization, rigorous evaluation and regulatory pathways, and infrastructure for federated/centralized data sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Trustworthy, effective deep learning in medical imaging depends less on algorithm novelty and more on access to standardized, well-annotated, interoperable DICOM datasets, rigorous metadata and evaluation, and transparent training/pipeline reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling global image data sharing in the life sciences', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2318.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2318.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noise2Void</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noise2Void — Learning denoising from single noisy images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised denoising method that trains models to remove noise using only single noisy images (no clean ground truth required), enabling denoising when paired clean data are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Noise2Void-Learning Denoising from Single Noisy Images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Microscopy / bioimage preprocessing (image denoising)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Remove noise from microscopy images to improve downstream analysis (visualization, segmentation) when clean (noise-free) ground truth images are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Can operate with abundant raw noisy image data because it does not require paired clean labels; labeled clean images are scarce/expensive, making self-supervision attractive.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Single-channel or multi-channel microscopy images (2D/3D), unpaired noisy images.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate to high — noise characteristics vary across instruments and modalities; models must handle differing SNR, non-stationary and structured noise.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging/maturing — self-supervised denoising methods have become practical and adopted in bioimaging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — interpretability is desirable to ensure denoising does not introduce artifacts that affect scientific conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Self-supervised denoising (Noise2Void)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Models are trained by masking or withholding portions of a noisy image and predicting masked pixels from surrounding pixels, thereby learning to remove noise without ever seeing a clean target image; architecture implementations typically use convolutional networks adapted for image restoration.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Self-supervised learning / unsupervised</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable where clean ground-truth images are unavailable or costly; well-suited for many microscopy denoising tasks though limited if noise violates method assumptions (e.g., highly structured signal mistaken for noise).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively valuable — allows denoising using only raw images, increasing usability of datasets and enabling downstream ML; limitations include potential removal of weak signals and sensitivity to noise model mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate to high — reduces need for expensive paired acquisitions, enables better downstream segmentation/analysis, and broadens usable data for ML training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to supervised denoising requiring clean targets (which can yield superior results when available) — Noise2Void trades some performance for applicability without paired data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sufficient quantity of raw image data, appropriate assumptions about noise, careful validation to ensure no biologically relevant signal is removed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Self-supervised denoising enables reuse of large raw microscopy datasets for downstream analysis and ML when clean ground truth is scarce, but success depends on matching the method's noise assumptions to the data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling global image data sharing in the life sciences', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2318.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2318.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Foundation segmentation models (SAM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Foundation/large-scale segmentation models exemplified by the Segment Anything Model (SAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large, generalist segmentation models trained on extremely large annotated datasets (e.g., SAM trained on billions of masks) to provide broad, promptable segmentation capabilities across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Segment Anything</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General image segmentation, with potential application to bioimaging and microscopy</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Create broadly applicable segmentation models that generalize across object types, modalities, and contexts, enabling zero/few-shot segmentation or strong starting points for domain-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires enormous annotated datasets (SAM trained on ~11 billion masks); such large-scale annotations are currently more available in natural image domains than in specialized bioimage modalities where annotations are scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Images with object masks (segmentation labels), potentially multi-modal and heterogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high — covers a vast and heterogeneous object/domain space, requiring models to handle diverse scales, contrasts, and imaging artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging and actively debated — foundational models are a recent trend; applicability to scientific imaging is promising but not yet settled.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — for scientific use, provenance of training data and interpretability/limitations must be known to avoid erroneous scientific conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Foundation models for segmentation (large-scale supervised pretraining with promptable interfaces)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Large models pre-trained on massive collections of images and segmentation masks to learn broadly applicable representations; can be used directly or fine-tuned/adapter-tuned on domain-specific data, often enabling promptable segmentation (user-provided seeds/boxes).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Foundation models / supervised pretraining with transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly promising as starting points for many segmentation tasks, but direct out-of-the-box application to specialized bioimage modalities is limited unless domain-specific fine-tuning and appropriate metadata/context are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively promising — potential to reduce annotation burden and provide baseline capability across modalities, but concerns remain about domain gaps between training data and specialized scientific images.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could dramatically accelerate segmentation workflows across domains if sufficient annotated, heterogeneous scientific training data are available and models are transparent about training provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper contrasts foundation models with many smaller specialized models: foundation models generalize more broadly but require vast annotated data and compute; smaller models provide tighter domain control and often better performance within a narrow application.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of very large, heterogeneous and well-annotated datasets; rich metadata linking imaging modality and sample context; compute resources; mechanisms for fine-tuning and transparent reporting of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Foundation segmentation models can provide broad applicability and accelerate analysis, but their scientific utility depends critically on access to diverse, well-annotated bioimage datasets and transparent training metadata to enable appropriate adaptation and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling global image data sharing in the life sciences', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2318.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2318.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cellpose (generalist segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cellpose: A generalist algorithm for cellular segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist deep-learning segmentation algorithm trained on diverse annotated datasets to perform robust cellular segmentation across different imaging conditions and sample types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cellpose: A generalist algorithm for cellular segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Cellular segmentation in microscopy images</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Segment cells and subcellular structures across diverse tissue types, staining conditions, and imaging modalities to support quantitative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained on multiple annotated datasets aggregated to improve generalization; annotations required are laborious to produce, making labeled data a limiting resource.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Microscopy images with pixel-wise segmentation masks; 2D and 3D formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — diverse cell morphologies, densities, imaging contrasts, and artifacts complicate generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Maturing — generalist models like Cellpose demonstrate that aggregated annotated datasets can enable cross-domain segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — users must understand model limitations and validation boundaries to avoid mis-segmentation affecting downstream scientific interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Generalist supervised deep-learning segmentation (Cellpose)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A convolutional neural network-based segmentation approach trained on a heterogeneous collection of manually annotated images to learn representations that generalize across cell types and imaging conditions; often used as a starting point and fine-tuned for specific datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (generalist models with transfer/fine-tuning capability)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable across many cellular imaging tasks; works better than highly specialized methods when diversity is present, but may still require fine-tuning for novel modalities or extreme domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Works well in practice for many labs, reducing annotation burden; success depends on similarity between training data distribution and new data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for routine cell segmentation tasks — enables broader adoption of automated segmentation and reproducible quantification across labs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to both small task-specific models (which can be more accurate in narrow domains) and to foundation models (broader but more resource-intensive); Cellpose occupies a middle ground as a practical generalist.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Diverse annotated training corpus, community adoption, tools for fine-tuning, and availability through model repositories and easy-to-use interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Aggregating heterogeneous, high-quality annotations enables practical generalist segmentation models that reduce annotation burden and broaden accessibility, but domain-shift and modality gaps still necessitate fine-tuning and metadata-aware validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling global image data sharing in the life sciences', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2318.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2318.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Smart microscopy (real-time AI control)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smart microscopy: AI/ML modules for real-time detection and adaptive acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of AI/ML modules integrated with microscope hardware to detect rare events or regions of interest (ROIs) in real time and adapt acquisition strategies (e.g., change imaging mode or focus) to capture relevant data efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Microscopy acquisition and experimental control</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify rare events/ROIs on-the-fly to adapt imaging parameters, maximize signal from events of interest, and reduce unnecessary data acquisition/storage.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on streaming image data produced during experiments; labeled examples of rare events are often scarce, making training challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Real-time image streams / time series (2D/3D), often multi-channel and high frame-rate.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — stringent real-time/low-latency requirements, high data throughput, variable SNR and imaging artifacts, and need for robust generalization to unseen events.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging — methods exist but require integration with hardware and easy-to-use 'easy buttons' for adoption by non-expert users.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — decisions affect experiment acquisition; understanding model behavior is important to avoid missed events or biased sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Real-time object/event detection and decision modules (typically lightweight supervised/online ML models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Lightweight detection/classification models deployed on acquisition pipelines to evaluate incoming frames/volumes and trigger adaptive actions (e.g., higher-resolution imaging), often with constraints on latency and compute; may include online learning or heuristics to handle rare-event scarcity.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised/online learning with real-time deployment constraints</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and valuable where adaptive acquisition can substantially increase scientific return (e.g., catching transient events), but constrained by compute/latency, data labeling, and integration complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively beneficial for capturing rare or dynamic events and reducing data volume, but widespread adoption limited by usability, integration effort, and required tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for experiments where rare events are informative — can greatly increase experiment efficiency and ROI of expensive instruments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over manual ROI selection and open-loop acquisition by enabling dynamic, data-driven imaging decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Low-latency compute at the microscope, robust models that generalize across conditions, labelled examples or self-supervision strategies, and user-friendly integration tools ('easy buttons').</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding robust, low-latency AI modules into acquisition pipelines can dramatically increase experimental efficiency and capture rate of rare events, but success requires tooling, compute at the edge, and curated examples or self-supervision strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling global image data sharing in the life sciences', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2318.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2318.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioImage Model Zoo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A community resource hosting trained models tailored to bioimage analysis, linking models to training data, metadata, and tools, aimed at improving reproducibility and ease-of-use for life scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Model sharing, reproducibility, and deployment in bioimage analysis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide FAIR access to pretrained models, provenance, training data links, and tools to increase reproducibility and lower barriers to applying AI to bioimaging tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Hosts models and links to the training data where available; increases accessibility to pretrained models especially where raw annotated datasets are not directly shareable.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Model artifacts, metadata schemas, links to image datasets and annotations; models designed for image input/output (segmentation masks, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Organizational and technical complexity around metadata, model provenance, interoperability and packaging rather than algorithmic complexity per se.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Growing infrastructure — community-driven resources increasingly recognized as critical for adoption and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — requires metadata that describe intended use-cases and limitations so users understand when a pretrained model is appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Model repository and packaging infrastructure (hosting pretrained deep learning models with metadata and provenance)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A platform to host model weights, standardized metadata (including links to training datasets and usage instructions), and connectors to common bioimage analysis tools so users can run, evaluate, and fine-tune models reproducibly.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Infrastructure / reproducibility enabler (supports supervised/unsupervised/foundation models)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable as an enabler — reduces duplication, fosters reproducibility, and speeds adoption of AI methods by providing ready-to-use models and clear provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective in improving accessibility, reproducibility, and credit attribution for model creators; fosters community standards for model metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — accelerates method adoption, enables reproducible comparisons, and lowers the barrier for experimentalists to use ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over ad-hoc model sharing (e.g., individual GitHub repos) by standardizing metadata, linking to data and training pipelines, and integrating with tool ecosystems.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Community buy-in, standardized metadata schemas, links to training data and pipelines, and programmatic access to models and metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Standardized, FAIR model repositories materially increase the impact and reuse of AI methods in bioimaging by making pretrained models and their provenance readily accessible to both developers and experimentalists.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling global image data sharing in the life sciences', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2318.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2318.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIDRC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Medical Imaging and Data Resource Commons (MIDRC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-institutional, interoperable data resource created to collect, curate, and share medical imaging and associated clinical data to foster machine learning innovation (initiated during COVID-19).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Medical imaging data infrastructure for ML</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide large-scale, interoperable collections of clinical imaging and metadata to accelerate ML development, evaluation, and dissemination for medical imaging use cases.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed to be generalizable, scalable, and interoperable; aims to provide researchers with extensive imaging and clinical data resources, though governance and siloing issues can complicate integration with other resources.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>DICOM medical images with associated clinical and demographic metadata; potentially curated and indexed for ML use.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High organizational complexity — harmonizing clinical governance, privacy, and interoperability across institutions and jurisdictions; technical complexity in curation and indexing large imaging collections.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively new (initiated in late summer 2020) but built on established clinical imaging standards (DICOM); serving as a growing centralized resource.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for downstream clinical ML models — provenance, metadata, and rigorous validation are necessary to ensure safe clinical claims.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Data commons / federated/shared data infrastructure to enable ML</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Provides a platform and curated datasets that ML practitioners can use to train, validate, and benchmark algorithms; designed to be interoperable and scalable to multiple imaging use cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Data infrastructure enabling supervised and other ML paradigms</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable as a centralized resource to accelerate medical imaging ML, but must be integrated carefully to avoid isolated silos and to ensure harmonization with other commons.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively valuable — provides treasures of data and standards that facilitate ML development, though harmonization with other national/international resources is a recognized challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can accelerate ML innovation in medical imaging by providing large-scale interoperable datasets, but benefits depend on governance, accessibility, and integration with broader data ecosystems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>An intentional, centralized/data-commons approach compared to multiple siloed resources; paper warns siloing risks and advocates for federated/interoperable approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Clear governance, interoperability standards, metadata harmonization (DICOM and richer metadata), funding for curation, and integration with other national/international resources.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Data commons like MIDRC can catalyze medical imaging ML by aggregating interoperable clinical imaging and metadata, but their ultimate utility depends on harmonized governance, FAIR metadata, and integration into a federated global ecosystem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling global image data sharing in the life sciences', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Noise2Void-Learning Denoising from Single Noisy Images <em>(Rating: 2)</em></li>
                <li>Segment Anything <em>(Rating: 2)</em></li>
                <li>Cellpose: A generalist algorithm for cellular segmentation. <em>(Rating: 2)</em></li>
                <li>BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis. <em>(Rating: 2)</em></li>
                <li>Artificial intelligence in radiology <em>(Rating: 2)</em></li>
                <li>Toward fairness in artificial intelligence for medical image analysis: Identification and mitigation of potential biases in the roadmap from data collection to model deployment. <em>(Rating: 2)</em></li>
                <li>Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2318",
    "paper_id": "paper-267200218",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Deep learning (medical imaging)",
            "name_full": "Deep learning for medical image analysis and computer-aided detection/diagnosis (CAD)",
            "brief_description": "Use of deep neural networks and classical ML for detection, diagnosis, segmentation, prognosis, and decision support in radiology and other medical imaging domains, building on decades of CAD work and recent FDA-cleared systems.",
            "citation_title": "Artificial intelligence in radiology",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Medical imaging (radiology, diagnostic image analysis, therapy guidance)",
            "problem_description": "Automatically detect, localize, classify, and quantify pathologies and imaging biomarkers to assist diagnosis, prognosis, risk assessment, and therapy planning.",
            "data_availability": "Varies: many medical images exist (DICOM standard facilitates sharing) and public repositories (TCIA, IDC, MIDRC) provide curated datasets, but labeled/annotated data are often limited and heterogeneously curated; access and governance differ across repositories.",
            "data_structure": "High-resolution image data (DICOM), often multi-modal (CT/MRI/PET/US), coupled with clinical and demographic metadata.",
            "problem_complexity": "High — large high-dimensional images, diverse acquisition protocols and parameters, non-linear relationships, variable quality and noise, and substantial preprocessing/harmonization needs; regulatory considerations add constraints.",
            "domain_maturity": "Relatively mature compared to other bioimaging domains: CAD approaches since 1960s, several FDA-cleared systems exist, but rapidly evolving with deep learning advances.",
            "mechanistic_understanding_requirements": "High — clinical deployment requires interpretability, traceable provenance, validation on target populations, and regulatory-grade evidence; 'black-box' models are discouraged without transparency.",
            "ai_methodology_name": "Supervised deep learning (CNNs / other deep architectures) for classification, detection, segmentation; also hybrid ML/CAD pipelines",
            "ai_methodology_description": "End-to-end or modular neural-network based systems trained on curated labeled images and clinical metadata to perform classification, detection, segmentation, or prediction; training requires careful data curation, harmonization, bias mitigation, and validation protocols tailored to the clinical claim and intended population.",
            "ai_methodology_category": "Supervised learning (with hybrid CAD pipelines and some unsupervised/pretraining elements)",
            "applicability": "Highly applicable for diagnostic tasks given curated labeled datasets and standardized formats (DICOM); limitations include variability across sites, need for extensive validation, and governance/ethical constraints.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective in many diagnostic tasks (historical and recent FDA approvals cited), but performance depends strongly on data quality, representativeness, annotation standardization, and evaluation procedures; risk of 'garbage in, garbage out' if data poorly curated.",
            "impact_potential": "High — can improve detection/diagnosis, triage, quantification, and workflow efficiency; could enable scaled secondary analyses and integration with other omics/clinical data.",
            "comparison_to_alternatives": "Paper contrasts long-standing CAD approaches with modern deep learning — deep learning offers improved pattern learning but requires more curated data and compute; also emphasizes that models must be validated for each clinical claim and population (no off-label use).",
            "success_factors": "Standardized data formats (DICOM), well-curated and diverse labeled datasets, repository access, metadata harmonization, rigorous evaluation and regulatory pathways, and infrastructure for federated/centralized data sharing.",
            "key_insight": "Trustworthy, effective deep learning in medical imaging depends less on algorithm novelty and more on access to standardized, well-annotated, interoperable DICOM datasets, rigorous metadata and evaluation, and transparent training/pipeline reporting.",
            "uuid": "e2318.0",
            "source_info": {
                "paper_title": "Enabling global image data sharing in the life sciences",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Noise2Void",
            "name_full": "Noise2Void — Learning denoising from single noisy images",
            "brief_description": "A self-supervised denoising method that trains models to remove noise using only single noisy images (no clean ground truth required), enabling denoising when paired clean data are unavailable.",
            "citation_title": "Noise2Void-Learning Denoising from Single Noisy Images",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Microscopy / bioimage preprocessing (image denoising)",
            "problem_description": "Remove noise from microscopy images to improve downstream analysis (visualization, segmentation) when clean (noise-free) ground truth images are unavailable.",
            "data_availability": "Can operate with abundant raw noisy image data because it does not require paired clean labels; labeled clean images are scarce/expensive, making self-supervision attractive.",
            "data_structure": "Single-channel or multi-channel microscopy images (2D/3D), unpaired noisy images.",
            "problem_complexity": "Moderate to high — noise characteristics vary across instruments and modalities; models must handle differing SNR, non-stationary and structured noise.",
            "domain_maturity": "Emerging/maturing — self-supervised denoising methods have become practical and adopted in bioimaging.",
            "mechanistic_understanding_requirements": "Medium — interpretability is desirable to ensure denoising does not introduce artifacts that affect scientific conclusions.",
            "ai_methodology_name": "Self-supervised denoising (Noise2Void)",
            "ai_methodology_description": "Models are trained by masking or withholding portions of a noisy image and predicting masked pixels from surrounding pixels, thereby learning to remove noise without ever seeing a clean target image; architecture implementations typically use convolutional networks adapted for image restoration.",
            "ai_methodology_category": "Self-supervised learning / unsupervised",
            "applicability": "Applicable where clean ground-truth images are unavailable or costly; well-suited for many microscopy denoising tasks though limited if noise violates method assumptions (e.g., highly structured signal mistaken for noise).",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively valuable — allows denoising using only raw images, increasing usability of datasets and enabling downstream ML; limitations include potential removal of weak signals and sensitivity to noise model mismatch.",
            "impact_potential": "Moderate to high — reduces need for expensive paired acquisitions, enables better downstream segmentation/analysis, and broadens usable data for ML training.",
            "comparison_to_alternatives": "Compared implicitly to supervised denoising requiring clean targets (which can yield superior results when available) — Noise2Void trades some performance for applicability without paired data.",
            "success_factors": "Sufficient quantity of raw image data, appropriate assumptions about noise, careful validation to ensure no biologically relevant signal is removed.",
            "key_insight": "Self-supervised denoising enables reuse of large raw microscopy datasets for downstream analysis and ML when clean ground truth is scarce, but success depends on matching the method's noise assumptions to the data.",
            "uuid": "e2318.1",
            "source_info": {
                "paper_title": "Enabling global image data sharing in the life sciences",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Foundation segmentation models (SAM)",
            "name_full": "Foundation/large-scale segmentation models exemplified by the Segment Anything Model (SAM)",
            "brief_description": "Large, generalist segmentation models trained on extremely large annotated datasets (e.g., SAM trained on billions of masks) to provide broad, promptable segmentation capabilities across domains.",
            "citation_title": "Segment Anything",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General image segmentation, with potential application to bioimaging and microscopy",
            "problem_description": "Create broadly applicable segmentation models that generalize across object types, modalities, and contexts, enabling zero/few-shot segmentation or strong starting points for domain-specific fine-tuning.",
            "data_availability": "Requires enormous annotated datasets (SAM trained on ~11 billion masks); such large-scale annotations are currently more available in natural image domains than in specialized bioimage modalities where annotations are scarce.",
            "data_structure": "Images with object masks (segmentation labels), potentially multi-modal and heterogeneous.",
            "problem_complexity": "Very high — covers a vast and heterogeneous object/domain space, requiring models to handle diverse scales, contrasts, and imaging artifacts.",
            "domain_maturity": "Emerging and actively debated — foundational models are a recent trend; applicability to scientific imaging is promising but not yet settled.",
            "mechanistic_understanding_requirements": "Medium to high — for scientific use, provenance of training data and interpretability/limitations must be known to avoid erroneous scientific conclusions.",
            "ai_methodology_name": "Foundation models for segmentation (large-scale supervised pretraining with promptable interfaces)",
            "ai_methodology_description": "Large models pre-trained on massive collections of images and segmentation masks to learn broadly applicable representations; can be used directly or fine-tuned/adapter-tuned on domain-specific data, often enabling promptable segmentation (user-provided seeds/boxes).",
            "ai_methodology_category": "Foundation models / supervised pretraining with transfer learning",
            "applicability": "Highly promising as starting points for many segmentation tasks, but direct out-of-the-box application to specialized bioimage modalities is limited unless domain-specific fine-tuning and appropriate metadata/context are provided.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively promising — potential to reduce annotation burden and provide baseline capability across modalities, but concerns remain about domain gaps between training data and specialized scientific images.",
            "impact_potential": "High — could dramatically accelerate segmentation workflows across domains if sufficient annotated, heterogeneous scientific training data are available and models are transparent about training provenance.",
            "comparison_to_alternatives": "Paper contrasts foundation models with many smaller specialized models: foundation models generalize more broadly but require vast annotated data and compute; smaller models provide tighter domain control and often better performance within a narrow application.",
            "success_factors": "Availability of very large, heterogeneous and well-annotated datasets; rich metadata linking imaging modality and sample context; compute resources; mechanisms for fine-tuning and transparent reporting of training data.",
            "key_insight": "Foundation segmentation models can provide broad applicability and accelerate analysis, but their scientific utility depends critically on access to diverse, well-annotated bioimage datasets and transparent training metadata to enable appropriate adaptation and validation.",
            "uuid": "e2318.2",
            "source_info": {
                "paper_title": "Enabling global image data sharing in the life sciences",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Cellpose (generalist segmentation)",
            "name_full": "Cellpose: A generalist algorithm for cellular segmentation",
            "brief_description": "A generalist deep-learning segmentation algorithm trained on diverse annotated datasets to perform robust cellular segmentation across different imaging conditions and sample types.",
            "citation_title": "Cellpose: A generalist algorithm for cellular segmentation.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Cellular segmentation in microscopy images",
            "problem_description": "Segment cells and subcellular structures across diverse tissue types, staining conditions, and imaging modalities to support quantitative analyses.",
            "data_availability": "Trained on multiple annotated datasets aggregated to improve generalization; annotations required are laborious to produce, making labeled data a limiting resource.",
            "data_structure": "Microscopy images with pixel-wise segmentation masks; 2D and 3D formats.",
            "problem_complexity": "High — diverse cell morphologies, densities, imaging contrasts, and artifacts complicate generalization.",
            "domain_maturity": "Maturing — generalist models like Cellpose demonstrate that aggregated annotated datasets can enable cross-domain segmentation.",
            "mechanistic_understanding_requirements": "Medium — users must understand model limitations and validation boundaries to avoid mis-segmentation affecting downstream scientific interpretations.",
            "ai_methodology_name": "Generalist supervised deep-learning segmentation (Cellpose)",
            "ai_methodology_description": "A convolutional neural network-based segmentation approach trained on a heterogeneous collection of manually annotated images to learn representations that generalize across cell types and imaging conditions; often used as a starting point and fine-tuned for specific datasets.",
            "ai_methodology_category": "Supervised learning (generalist models with transfer/fine-tuning capability)",
            "applicability": "Applicable across many cellular imaging tasks; works better than highly specialized methods when diversity is present, but may still require fine-tuning for novel modalities or extreme domain shifts.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Works well in practice for many labs, reducing annotation burden; success depends on similarity between training data distribution and new data.",
            "impact_potential": "High for routine cell segmentation tasks — enables broader adoption of automated segmentation and reproducible quantification across labs.",
            "comparison_to_alternatives": "Compared conceptually to both small task-specific models (which can be more accurate in narrow domains) and to foundation models (broader but more resource-intensive); Cellpose occupies a middle ground as a practical generalist.",
            "success_factors": "Diverse annotated training corpus, community adoption, tools for fine-tuning, and availability through model repositories and easy-to-use interfaces.",
            "key_insight": "Aggregating heterogeneous, high-quality annotations enables practical generalist segmentation models that reduce annotation burden and broaden accessibility, but domain-shift and modality gaps still necessitate fine-tuning and metadata-aware validation.",
            "uuid": "e2318.3",
            "source_info": {
                "paper_title": "Enabling global image data sharing in the life sciences",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Smart microscopy (real-time AI control)",
            "name_full": "Smart microscopy: AI/ML modules for real-time detection and adaptive acquisition",
            "brief_description": "Use of AI/ML modules integrated with microscope hardware to detect rare events or regions of interest (ROIs) in real time and adapt acquisition strategies (e.g., change imaging mode or focus) to capture relevant data efficiently.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Microscopy acquisition and experimental control",
            "problem_description": "Identify rare events/ROIs on-the-fly to adapt imaging parameters, maximize signal from events of interest, and reduce unnecessary data acquisition/storage.",
            "data_availability": "Relies on streaming image data produced during experiments; labeled examples of rare events are often scarce, making training challenging.",
            "data_structure": "Real-time image streams / time series (2D/3D), often multi-channel and high frame-rate.",
            "problem_complexity": "High — stringent real-time/low-latency requirements, high data throughput, variable SNR and imaging artifacts, and need for robust generalization to unseen events.",
            "domain_maturity": "Emerging — methods exist but require integration with hardware and easy-to-use 'easy buttons' for adoption by non-expert users.",
            "mechanistic_understanding_requirements": "Medium — decisions affect experiment acquisition; understanding model behavior is important to avoid missed events or biased sampling.",
            "ai_methodology_name": "Real-time object/event detection and decision modules (typically lightweight supervised/online ML models)",
            "ai_methodology_description": "Lightweight detection/classification models deployed on acquisition pipelines to evaluate incoming frames/volumes and trigger adaptive actions (e.g., higher-resolution imaging), often with constraints on latency and compute; may include online learning or heuristics to handle rare-event scarcity.",
            "ai_methodology_category": "Supervised/online learning with real-time deployment constraints",
            "applicability": "Applicable and valuable where adaptive acquisition can substantially increase scientific return (e.g., catching transient events), but constrained by compute/latency, data labeling, and integration complexity.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively beneficial for capturing rare or dynamic events and reducing data volume, but widespread adoption limited by usability, integration effort, and required tooling.",
            "impact_potential": "High for experiments where rare events are informative — can greatly increase experiment efficiency and ROI of expensive instruments.",
            "comparison_to_alternatives": "Improves over manual ROI selection and open-loop acquisition by enabling dynamic, data-driven imaging decisions.",
            "success_factors": "Low-latency compute at the microscope, robust models that generalize across conditions, labelled examples or self-supervision strategies, and user-friendly integration tools ('easy buttons').",
            "key_insight": "Embedding robust, low-latency AI modules into acquisition pipelines can dramatically increase experimental efficiency and capture rate of rare events, but success requires tooling, compute at the edge, and curated examples or self-supervision strategies.",
            "uuid": "e2318.4",
            "source_info": {
                "paper_title": "Enabling global image data sharing in the life sciences",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "BioImage Model Zoo",
            "name_full": "BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis",
            "brief_description": "A community resource hosting trained models tailored to bioimage analysis, linking models to training data, metadata, and tools, aimed at improving reproducibility and ease-of-use for life scientists.",
            "citation_title": "BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Model sharing, reproducibility, and deployment in bioimage analysis",
            "problem_description": "Provide FAIR access to pretrained models, provenance, training data links, and tools to increase reproducibility and lower barriers to applying AI to bioimaging tasks.",
            "data_availability": "Hosts models and links to the training data where available; increases accessibility to pretrained models especially where raw annotated datasets are not directly shareable.",
            "data_structure": "Model artifacts, metadata schemas, links to image datasets and annotations; models designed for image input/output (segmentation masks, etc.).",
            "problem_complexity": "Organizational and technical complexity around metadata, model provenance, interoperability and packaging rather than algorithmic complexity per se.",
            "domain_maturity": "Growing infrastructure — community-driven resources increasingly recognized as critical for adoption and reproducibility.",
            "mechanistic_understanding_requirements": "Medium — requires metadata that describe intended use-cases and limitations so users understand when a pretrained model is appropriate.",
            "ai_methodology_name": "Model repository and packaging infrastructure (hosting pretrained deep learning models with metadata and provenance)",
            "ai_methodology_description": "A platform to host model weights, standardized metadata (including links to training datasets and usage instructions), and connectors to common bioimage analysis tools so users can run, evaluate, and fine-tune models reproducibly.",
            "ai_methodology_category": "Infrastructure / reproducibility enabler (supports supervised/unsupervised/foundation models)",
            "applicability": "Highly applicable as an enabler — reduces duplication, fosters reproducibility, and speeds adoption of AI methods by providing ready-to-use models and clear provenance.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective in improving accessibility, reproducibility, and credit attribution for model creators; fosters community standards for model metadata.",
            "impact_potential": "High — accelerates method adoption, enables reproducible comparisons, and lowers the barrier for experimentalists to use ML models.",
            "comparison_to_alternatives": "Improves over ad-hoc model sharing (e.g., individual GitHub repos) by standardizing metadata, linking to data and training pipelines, and integrating with tool ecosystems.",
            "success_factors": "Community buy-in, standardized metadata schemas, links to training data and pipelines, and programmatic access to models and metadata.",
            "key_insight": "Standardized, FAIR model repositories materially increase the impact and reuse of AI methods in bioimaging by making pretrained models and their provenance readily accessible to both developers and experimentalists.",
            "uuid": "e2318.5",
            "source_info": {
                "paper_title": "Enabling global image data sharing in the life sciences",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MIDRC",
            "name_full": "Medical Imaging and Data Resource Commons (MIDRC)",
            "brief_description": "A multi-institutional, interoperable data resource created to collect, curate, and share medical imaging and associated clinical data to foster machine learning innovation (initiated during COVID-19).",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Medical imaging data infrastructure for ML",
            "problem_description": "Provide large-scale, interoperable collections of clinical imaging and metadata to accelerate ML development, evaluation, and dissemination for medical imaging use cases.",
            "data_availability": "Designed to be generalizable, scalable, and interoperable; aims to provide researchers with extensive imaging and clinical data resources, though governance and siloing issues can complicate integration with other resources.",
            "data_structure": "DICOM medical images with associated clinical and demographic metadata; potentially curated and indexed for ML use.",
            "problem_complexity": "High organizational complexity — harmonizing clinical governance, privacy, and interoperability across institutions and jurisdictions; technical complexity in curation and indexing large imaging collections.",
            "domain_maturity": "Relatively new (initiated in late summer 2020) but built on established clinical imaging standards (DICOM); serving as a growing centralized resource.",
            "mechanistic_understanding_requirements": "High for downstream clinical ML models — provenance, metadata, and rigorous validation are necessary to ensure safe clinical claims.",
            "ai_methodology_name": "Data commons / federated/shared data infrastructure to enable ML",
            "ai_methodology_description": "Provides a platform and curated datasets that ML practitioners can use to train, validate, and benchmark algorithms; designed to be interoperable and scalable to multiple imaging use cases.",
            "ai_methodology_category": "Data infrastructure enabling supervised and other ML paradigms",
            "applicability": "Highly applicable as a centralized resource to accelerate medical imaging ML, but must be integrated carefully to avoid isolated silos and to ensure harmonization with other commons.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively valuable — provides treasures of data and standards that facilitate ML development, though harmonization with other national/international resources is a recognized challenge.",
            "impact_potential": "High — can accelerate ML innovation in medical imaging by providing large-scale interoperable datasets, but benefits depend on governance, accessibility, and integration with broader data ecosystems.",
            "comparison_to_alternatives": "An intentional, centralized/data-commons approach compared to multiple siloed resources; paper warns siloing risks and advocates for federated/interoperable approaches.",
            "success_factors": "Clear governance, interoperability standards, metadata harmonization (DICOM and richer metadata), funding for curation, and integration with other national/international resources.",
            "key_insight": "Data commons like MIDRC can catalyze medical imaging ML by aggregating interoperable clinical imaging and metadata, but their ultimate utility depends on harmonized governance, FAIR metadata, and integration into a federated global ecosystem.",
            "uuid": "e2318.6",
            "source_info": {
                "paper_title": "Enabling global image data sharing in the life sciences",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Noise2Void-Learning Denoising from Single Noisy Images",
            "rating": 2,
            "sanitized_title": "noise2voidlearning_denoising_from_single_noisy_images"
        },
        {
            "paper_title": "Segment Anything",
            "rating": 2,
            "sanitized_title": "segment_anything"
        },
        {
            "paper_title": "Cellpose: A generalist algorithm for cellular segmentation.",
            "rating": 2,
            "sanitized_title": "cellpose_a_generalist_algorithm_for_cellular_segmentation"
        },
        {
            "paper_title": "BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis.",
            "rating": 2,
            "sanitized_title": "bioimage_model_zoo_a_communitydriven_resource_for_accessible_deep_learning_in_bioimage_analysis"
        },
        {
            "paper_title": "Artificial intelligence in radiology",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_in_radiology"
        },
        {
            "paper_title": "Toward fairness in artificial intelligence for medical image analysis: Identification and mitigation of potential biases in the roadmap from data collection to model deployment.",
            "rating": 2,
            "sanitized_title": "toward_fairness_in_artificial_intelligence_for_medical_image_analysis_identification_and_mitigation_of_potential_biases_in_the_roadmap_from_data_collection_to_model_deployment"
        },
        {
            "paper_title": "Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning.",
            "rating": 2,
            "sanitized_title": "wholecell_segmentation_of_tissue_images_with_humanlevel_performance_using_largescale_data_annotation_and_deep_learning"
        }
    ],
    "cost": 0.020497249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>White Paper on Enabling Global Image Data Sharing in the Life Sciences</p>
<p>Peter Bajcsy peter.bajcsy@nist.gov 
Sreenivas Bhattiprolu sreenivas.bhattiprolu@zeiss.com 
Katy Borner 
Lucy Collinson lucy.collinson@crick.ac.uk 
Jan Ellenberg jan.ellenberg@embl.de 
Reto Fiolka reto.fiolka@utsouthwestern.edu 
Maryellen Giger m-giger@uchicago.edu 
Wojtek Goscinski 
Matthew Hartley matthewh@ebi.ac.uk 
Nathan Hotaling nathan.hotaling@nih.gov 
Rick Horwitz rickh@alleninstitute.org 
Florian Jug flotian.jug@fht.org 
Anna Kreshuk anna.kreshuk@embl.de 
Emma Lundberg emmalu@stanford.edu 
Aastha Mathur aastha.mathur@eurobioimaging.eu 
Kedar Narayan narayank@mail.nih.gov 
Shuichi Onami sonami@riken.jp 
Anne Plant anne.plant@nist.gov 
Fred Prior fwprior@uams.edu 
Jason Swedlow jrswedlow@dundee.ac.uk 
Adam Taylor adam.taylor@sagebase.org </p>
<p>National Institute of Standards and Technology
GaithersburgUSA</p>
<p>ZEISS Microscopy Customer Center
DublinUSA</p>
<p>Intelligent Systems Engineering
Indiana University
BloomingtonUSA</p>
<p>Imaging Platform
Broad Institute
CambridgeMAUSA</p>
<p>Electron Microscopy Science Technology Platform
Francis Crick Institute
1 Midland RoadLondonUK</p>
<p>European Molecular Biology Laboratory
HeidelbergGermany</p>
<p>Lyda Hill Department of Bioinformatics
UT Southwestern Medical Center
DallasTexasUSA</p>
<p>Department of Radiology and Committee on Medical Physics
University of Chicago
ChicagoILUSA</p>
<p>National Imaging Facility
BrisbaneAustralia</p>
<p>European Molecular Biology Laboratory
European Bioinformatics Institute
Hinxton, CambridgeUK</p>
<p>National Center for Advancing Translational Science
National Institutes of Health
RockvilleUSA</p>
<p>Allen Institute for Cell Science
SeattleUSA</p>
<p>Fondazione Human Technopole
MilanItaly</p>
<p>US and SciLifeLab
Stanford University
California</p>
<p>KTH Royal Institute of Technology
StockholmSweden</p>
<p>Center for Molecular Microscopy
Center for Cancer Research
National Cancer Institute
National Institutes of Health
Bethesda USA. and Cancer Research Technology Program
Frederick National Laboratory for Cancer Research
Frederick USA</p>
<ol>
<li>RIKEN Center for Biosystems Dynamics Research
KobeJapan</li>
</ol>
<p>Department of Biomedical Informatics
Department of Radiology
University of Arkansas for Medical Sciences
Little RockUSA</p>
<p>Divisions of Computational Biology and Molecular, Cell and Developmental Biology
University of Dundee
UK</p>
<p>Sage Bionetworks
SeattleWAUSA</p>
<p>National Institute of Standards and Technology
20899GaithersburgMDUSA</p>
<p>ZEISS Microscopy Customer Center Dublin
94568CAUSA</p>
<p>Intelligent Systems Engineering
Indiana University
47408BloomingtonINUSA</p>
<p>Beth Cimini Imaging Platform
Broad Institute
CambridgeMAUSA</p>
<p>Electron Microscopy Science Technology Platform
Francis Crick Institute
1 Midland RoadNW1 1ATLondonUK</p>
<p>European Molecular Biology Laboratory
HeidelbergGermany</p>
<p>Lyda Hill Department of Bioinformatics
UT Southwestern Medical Center
DallasTexasUSA</p>
<p>Department of Radiology and Committee on Medical Physics
University of Chicago
ChicagoILUSA</p>
<p>National Imaging Facility
Australia</p>
<p>European Molecular Biology Laboratory
European Bioinformatics Institute
Hinxton, CambridgeUK</p>
<p>National Center for Advancing Translational Science
National Institutes of Health Rockville
MD</p>
<p>Allen Institute for Cell Science Seattle
WA</p>
<p>Fondazione Human Technopole
MilanItaly</p>
<p>EMBL
HeidelbergGermany</p>
<p>European Molecular Biology Laboratory
HeidelbergGermany</p>
<p>Stanford University
CaliforniaUS</p>
<p>SciLifeLab
KTH Royal Institute of Technology
StockholmSweden</p>
<p>Euro-BioImaging ERIC Bio-Hub, at EMBL
HeidelbergGermany</p>
<p>1Center for Molecular Microscopy
Center for Cancer Research
National Cancer Institute
National Institutes of Health
20892Bethesda, MarylandUSA</p>
<p>2Cancer Research Technology Program
Frederick National Laboratory for Cancer Research
21702Frederick, MarylandUSA</p>
<p>RIKEN Center for Biosystems Dynamics Research
KobeJapan</p>
<p>National Institute of Standards and Technology
20899GaithersburgMDUSA</p>
<p>Department of Biomedical Informatics Department of Radiology
University of Arkansas for Medical Sciences Little Rock
72205ARUSA</p>
<p>Divisions of Computational Biology and Molecular, Cell and Developmental Biology
University of Dundee
UK</p>
<p>Sage Bionetworks
SeattleWAUSA</p>
<p>White Paper on Enabling Global Image Data Sharing in the Life Sciences
C88E12A88E36316A28783EFAB39E9EDD2401.13023) is published with a closely related companion entitled, Harmonizing the Generation and Pre-publication Stewardship of FAIR Image Data, which can be found at the following link, arXiv:2401.13022.
Coordinated collaboration is essential to realize the added value of and infrastructure requirements for global image data sharing in the life sciences.In this White Paper, we take a first step at presenting some of the most common use cases as well as critical/emerging use cases of (including the use of artificial intelligence for) biological and medical image data, which would benefit tremendously from better frameworks for sharing (including technical, resourcing, legal, and ethical aspects).In the second half of this paper, we paint an 'ideal world scenario' for how global image data sharing could work and benefit all life sciences and beyond.As this is still a long way off, we conclude by suggesting several concrete measures directed toward our institutions, existing imaging communities and data initiatives, and national funders, as well as publishers.Our vision is that within the next ten years, most researchers in the world will be able to make their datasets openly available and use quality image data of interest to them for their research and benefit.This paper is published in parallel with a companion White Paper entitled "Harmonizing the Generation and Pre-publication Stewardship of FAIR Image Data", which addresses challenges and opportunities related to producing well-documented and high-quality image data that is ready to be shared.The driving goal is to address remaining challenges and democratize access to everyday practices and tools for a spectrum of biomedical researchers, regardless of their expertise, access to resources, and geographical location.Motivation for White PaperPublic, reference data are one of the most important foundational resources in the modern life and biomedical sciences.Over the last 40 years, the public release and availability of genomic and macromolecule structural databases accelerated discovery, sparked the development of wholly new fields of science, spawned multi-billion dollar industries, and led to a revolution in drug development and disease treatments.The major next step is the development of public reference biological and medical imaging data repositories, which promises at least an equal, if not larger impact on discovery and society.The motivation driving this White Paper is the timely and critical need for infrastructure supporting image acquisition, management, and analysis, that is not only coordinated within the US but also across the world.Australia, Japan, and the European Union have established nationally funded, coordinated research infrastructures (RIs) for life science and biomedical imaging (collectively "BioImaging").These BioImaging RIs are the output of concerted long-term strategic planning efforts from academic, funding, political as well as industrial partners, i.e., the key stakeholders in life sciences and biomedical research.In 2023, national and transnational BioImaging RIs are in operation and deliver technology and community support for quality BioImage data acquisition, management, analysis, and publication for their stakeholders.</p>
<p>As just one example, Euro-BioImaging ERIC, the European BioImaging RI1 , provides imaging technology platforms and data services to the European life and biomedical sciences community.One of the key provisions of Euro-BioImaging Image Data Services is the commitment to open sharing of data following the FAIR (Findable, Accessible, Interoperable, Reuseable) principles (Wilkinson et al., 2016).The work on Euro-BioImaging initiated in 2008 and the infrastructure finally became operational in 2019, requiring over a decade to coordinate such diverse, dynamic activities across a large number of countries and scientific communities.Nonetheless, the result is a coordinated effort where image data acquisition at advanced technology facilities across the continent are linked to public data resources, i.e., the BioImage Archive and the Image Data Resource, constructed and operated with significant investments from the UK and EU (Table 1).Publicly funded projects and community initiatives are also actively building and supporting image data formats2 and applications for managing, analysing, and sharing data using the most advanced technologies, including AI3 .Collaboration between academic and commercial organizations is maturing and provides a powerful ecosystem for development and eventual scaling of new technologies and products.Whereas Euro-BioImaging's Image Data Services are well-developed, they are also evolving and maturing to match the innovation of the rapidly developing field of BioImaging.Additionally, these services are being incorporated into the landscape of FAIR and interoperable data and services across Europe, by utilising, and hence constituting, a crucial component of the European Open Science Cloud (EOSC4 ) -a shared resource that aims to federate valuable digital resources and outputs across disciplinary and national boundaries.</p>
<p>Looking at the landscape in North America, there are early examples of engagement of the biological and medical imaging community e.g.BioImaging North America (BINA), Association of Biomolecular Resource Facilities (ABRF), and others (Table 3).However, at present, there is no route towards a common data infrastructure for coordinated image data management and analysis resources for the life sciences and bioimaging communities in the US or Canada.Because of this a trans-continental commitment to a common infrastructure for BioImaging is not on the horizon.Indeed, NIH alone has created several siloed biological and medical imaging data resources through their Common Fund and extramural programs.In fact, recently, the NIBIB of the NIH created MIDRC (the Medical Imaging and Data Resource Center), which is a multiinstitutional collaborative initiative driven by the medical imaging community that was initiated in late summer 2020 to help combat the global COVID-19 health emergency.MIDRC is generalizable, scalable, and interoperable for multiple imaging use cases.Its aim is to foster machine learning innovation through data sharing for rapid and flexible collection, analysis, and dissemination of imaging and associated clinical data by providing researchers with unparalleled resources.However, once established resources like these become hard to reconcile, coordinate, and harmonize with other existing or future data resources in, as well as outside, the USA.Therefore, opportunities for discoveries through data integration, one of the key outputs from genomic and structural data resources, are missed and the return on large research investments reduced.Furthermore, there is a risk that contributions from North American science to the emerging technologies enabled by generative AI are reduced or possibly even missed all together.</p>
<p>To move from isolated efforts to a coordinated design and strategy for a sustainable image data resource, the academic and industrial community of imaging scientists need to join forces and engage with the national stakeholders and funding agencies to develop a common understanding of the scientific community's requirements and the desired outcomes of the regional and national funding institutions.Such a coordinated collaboration is similar to the agreement by the medical imaging industry to use the DICOM format (Bidgood et al., 1997), making medical images obtained from around the world accessible to all.In addition, the lesson of the European (and indeed the Australian, the Japanese, and several other countries) experience is that a connected, coordinated effort is required to build the image data research infrastructures that provide services to the wider community.Assuming that individual groups will provide sustainable infrastructure for the whole scientific community using standardized responsive funding mechanisms seems naïve and has been shown to be inadequate many times.</p>
<p>Here, we suggest that imaging scientists, funders, publishers, technology developers, and vendors in North America come together to create this infrastructure and the training needed to support it.In our experience the establishment of several proof-of-concept (PoC) projects provides a rapid and powerful way to test different ideas for technology development, delivery, access, training, etc. and build the foundation for the future development of an accessible and useful data infrastructure.We include several specific use cases that would be candidates for these PoCs.It is notable that the priorities of each use case differ in detail, as the scope of the scientific applications that drive requirements in biological and medical fields varies.This demonstrates the challenge and opportunity of building a common BioImaging Data Infrastructure.</p>
<p>The European and other international communities of imaging scientists stand ready to support, advise, exchange ideas, and even to contribute to these efforts as the North American community initiates this essential journey for a common foundational BioImaging Data Infrastructure.</p>
<p>Background</p>
<p>Image data is the fastest growing data resource in the life sciences.The data are complex and deep in information, and the scientific community is only at the beginning to tap into this exponentially growing resource.Sharing FAIR quality-managed image data (Kemmer et al., 2023) widely supports open science.It increases reproducibility and transparency, facilitates collaboration, allows combining and analyzing datasets in new ways, inspires novel research questions and approaches, and most importantly, leads to innovation and new discoveries.</p>
<p>Many of the most exciting opportunities for deriving value from the reuse of biological and medical imaging data at scale rely on the ability to curate and aggregate those data so that they can be addressed together as a coherent whole.This aggregation enables very large scale AI/ML model training, as well as the creation of cross-modality and cross-domain benchmarks and reference datasets.It also opens the path for widely distributed computing, flexible movement of compute power to data or vice-versa, and mirroring of highly-accessed datasets to enable fast regional access, among other opportunities.These gains are not possible when data are siloed and fragmented in many different places, often with no public access or consistent organization.Access to data is easier when the data are aggregated in a small number of locations that are connected and follow compatible standards for data and metadata storage, i.e., interoperable.As outlined in the use cases described below, the international imaging community needs to address several limiting factors to promote global image data sharing:</p>
<p>• Engage with national and regional funders about the value added by resources that enable image data storage and sharing • Educate on the importance of quality-managed and harmonized data and rewarding those engaged in the laborious process of producing and curating the data • Address the technical challenges of moving extremely large datasets • Find common legal and ethical solutions for sharing medical image datasets that must remain in their country of origin Balancing these constraints in the long term is possible through a federated model whereby a network of data resources dedicated to particular use cases or domains are stored and made available through a small number of centralized repositories that provide both direct data hosting, as well as indexing and search capabilities.Enabling this model requires development of shared core metadata models for interoperability based on international guidelines such as REMBI (Sarkans et al., 2021), extendable for domain specific applications.Making effective use of the data also requires presentation in standardized formats, particularly those designed for large scale cloud-ready consumption such as OME-Zarr (Moore et al., 2021).</p>
<p>Together, these approaches would allow petabytes to exabytes of immensely valuable scientific data, representing enormous funder investments, to be maximally useful for the global community.Allowing these data to be easily located, accessed through consistent mechanisms in standardised formats and coupled with rich metadata unlocks valuable large-scale applications.These include training universal image classifiers to support automated quality control pipelines for researchers and instrument manufacturers, automated segmentation of common cell structures for use as biomarkers of disease or to better understand mechanisms of action, and prediction of physiological or biomolecular properties of cells to speed up biomanufacturing, reduce assay variability in labs and reduce consumable use.</p>
<p>However, there are several challenges, which make it more difficult than sharing genomic data or protein structures, for example.One is that individual files may be very large (100 GBs to 10 TBs, and beyond5 ) and require either high-speed internet connections to be uploaded and downloaded or physically transported on external drives.This makes data management and sharing expensive, and it requires storage and maintenance of large datasets over time.A robust technical infrastructure, including large data storage capacity, and powerful computing resources is hence required, as many research institutions still do not have the necessary infrastructure to support image data sharing.Another key challenge is that careful management and organization is needed to ensure that the image data becomes accessible and ideally reusable by other researchers.While medical images typically are easily shared since most are acquired in DICOM format, biological imaging data sharing is still limited because of the lack of standardisation in image data formats and metadata.This makes it difficult for researchers to compare and analyse data from different sources.Ensuring that the data are properly labelled, annotated, and stored in a standardised format can require significant time and resources (Swedlow et al., 2021), which is an essential, yet often underappreciated process.In addition, proper curation and harmonization of imaging data and associated metadata are needed to ensure future merging of data sets for analyses at scale.Finally, depending on the context intellectual property issues, questions of ownership and data privacy concerns may arise.Because of this, researchers may be reluctant to share their image data, and in case of medical imaging data, they must take steps to ensure that the data are de-identified and cannot be traced back to individual patients.Finally, there may be cultural barriers, such as a lack of trust or reluctance to share image data due to concerns about competition or ownership.</p>
<p>Addressing these challenges requires a concerted effort and endurance from researchers, institutions, funding agencies, publishers, and industry to establish internationally recognized best practices for sharing image data in the life sciences.It will also require investments in technical infrastructure and resources, and a shift in the cultural attitudes towards data sharing and collaboration in the scientific community.</p>
<p>In this White Paper, we take a first step at presenting some of the most common use cases as well as critical/emerging use cases of (including use of artificial intelligence for) biological and medical image data, which would benefit tremendously from better frameworks for sharing (including technical, resourcing, legal, and ethical aspects).In the second half of this paper, we paint an 'ideal world scenario' for how global image data sharing could work and benefit all life sciences and beyond.As this is still a long way off, we conclude by suggesting several concrete measures directed towards our institutions, existing imaging communities and data initiatives, and national funders as well as publishers.Our vision is that within the next ten years most researchers in this world will be in a position to make datasets openly available as well as access and use quality image data of interest for their research and benefit.</p>
<p>Use cases representing different image data types and their challenges and status for sharing</p>
<p>Here, we present different use cases of image data and their potential and challenges in the context of image data sharing.We deliberately choose to present different data types spanning the broad scope from light microscopy and electron to medical imaging, we and conclude with a use case on artificial intelligence being applied to image data.We are convinced that sharing of FAIR image data from all these domains will significantly increase interoperability among the different research domains in the life sciences.Most importantly, it will allow a comprehensive view of biological processes at different levels of organization, from molecular to cellular to whole organism and thereby will advance both basic biological as well as applied health research.The integration of light microscopy and medical imaging can provide a better understanding of disease mechanisms, particularly in the context of complex diseases that involve multiple organ systems and cellular processes.Preclinical and clinical imaging techniques, such as magnetic resonance imaging (MRI), provide valuable data on anatomical structures and physiological functions.</p>
<p>Integrating this data with light microscopy images can provide a more accurate and detailed understanding of the underlying biology.This will trigger advancements in drug discovery and development and help to identify potential therapeutic targets, as well as to evaluate drug efficacy and safety.In summary, if we foster image data sharing in the domains of each of the following use cases (and beyond), then we will enable not only promote open science but also new discoveries by being able to link different types of image data sets across the entire spectrum of life.</p>
<p>Use case 1: Light microscopy</p>
<p>The scale and complexity of light microscopy data area is increasing exponentially as applications become more advanced and automation hardware (motorized stages, objectives, filters, etc.) becomes more affordable and more commoditized.With this increase, so does the challenges of sharing these data.Where images are very large (e.g., light sheet microscopy, intra-vital imaging, hyperspectral imaging, etc.) or datasets have expanded (high content screening and whole slide pathology) on-going discussions as to what the minimal "raw" data storage requirements should be and what "raw" data even means are helping shape the storage landscape for the future.For the majority of the community, the cost of storing data in a shareable, open, and standardized format generally outweighs the cost needed to re-perform the experiment, if it is even possible, e.g., rare patient samples.The richness of these image datasets make them not only valuable for reproducibility, but also ripe for auxiliary analysis, testing other hypotheses, and training AI/ML algorithms.But without clear and easy to use portals that are free or heavily subsidized, individual labs bearing the storage costs and complexities of making data secure and FAIR often see this cost-benefit quite differently from the scientific community at large.</p>
<p>Several repositories support various mixes of data types, metadata requirements, and cost models (Table 1).However, small labs or those that do not have appropriate domain expertise, face large hurdles in using these portals, and often funding is a barrier for long term archiving/sharing of data at the scale that can now be generated by an automated microscope in any life science lab.For those repositories that do not require ongoing user-supported storage cost, alternative funding models must be identified for the long-term health of their resources, likely with help from national or international research organizations and science agencies.</p>
<p>Excellent work is being done to create standards for image metadata and organization schema by groups such as the Open Microscopy Environment (OME7 ), QUAREP-LiMi8 , BINA 9 and others (see Table 2); create open file format specifications for data files that range in size from megabytes to petabytes (OME-tiff/OME-Zarr, N510 ) that are optimal for a variety of storage backends including cloud, parallel file systems, and network attached storage; and consolidate the global community around a common set of standards.Currently, these efforts are supported in a grant/cyclic funding cycle that does not lend itself well to stable and continuous development.</p>
<p>New/updated funding models/approaches are needed to keep these critical projects healthy, so that the entire community can stand on a strong foundation.</p>
<p>The metadata standardization effort is challenging in light microscopy because metadata standards must be detailed, yet flexible enough to support vastly different experimental types.For example, the variety of light microscopy modalities, from transmitted light, epifluorescent, confocal, multi-photon and light sheet imaging to spectral methods like Raman and Mass-Spectrometry imaging, require complex equipment, settings, and reagents (often custom created in a lab) to fully capture sufficient information to interpret and reproduce the measurement.This results in the need to capture a large number of parameters, risking researcher exhaustion and highlights the need for heavy investment in simplified tooling for metadata capture and sharing.</p>
<p>Fortunately, the research community is beginning to define aspects of "experimental design capture".For example, the Research Resource Identification system (RRID)11 allows researchers to pull persistent unique identifiers (PUIs) for organisms, biological samples, reagents, and even tools, enabling identification of what was done specifically in a particular experiment.FPbase (Lambert, 2019) has a similar setup for microscopes and optical configurations.However, these tools are still beyond the scope of most researchers to be able to "just use" and thus heavy investment in training and education is needed for these tools (much less new tools to address other gaps).</p>
<p>Directing researchers on which aspects of experimental design must be captured has been approached by the REMBI, which attempts to enumerate all relevant fields in bioimaging and bioimage analysis metadata, and complements other valuable ontologies including the 4DN-BINA-OME standard (Hammer et al., 2021), Biological imaging methods ontology (fbbi12 ) NCBI taxonomy13 , the EDAM-Bioimaging ontology (Matúš Kalaš et al., 2020).These efforts in conjunction are starting points around which the community can converge.</p>
<p>The next challenge is how to collect this information.Researchers are busy, typically juggling many competing priorities and demands on their time; therefore, systems that minimize the work needed to capture metadata are critical.A number of tools either have been or are being developed for the capture of microscope hardware details; they include -Micro-Meta App (Rigano et al., 2021), MicCheck (Montero Llopis et al., 2021), FPbase, MethodsJ2 (Ryan et al., 2021), MDEMic (Kunis et al., 2021).A major need is tools to link experimental and image metadata, and to easily capture experimental metadata in a structured and reproducible format.For microscopy metadata capture, an ideal tool would have the ability to a) collect data in structures proposed by international guidelines such as REMBI and use persistent identifiers such as RRIDs, b) easily generate subsets and templates for ease of reuse, and c) easily import and export data.Additional features might include integration with electronic lab notebook (ELN) tools, integration with tools like barcode scanners to allow the inclusion of physical reagents, integration with protocol repositories, integration with figure creation tools, and direct export to systems designed to store (repositories) and/or report on (journals) the data produced.</p>
<p>Finally, we need to ensure that researchers will use such a system.The benefits to data generating labs include improved internal quality control, visibility, and the cross-referencing of one's work by generating reusable data, as well as data access for educational institutions.</p>
<p>Training will be required to learn how to use these systems and any capital costs involved.Therefore, strong support and commitment from funding agencies will be required to modernize and implement the "sample, to image, to FAIR data'' pipeline.</p>
<p>Use case 2: Volume electron microscopy</p>
<p>Volume electron microscopy (volume EM or vEM) describes a set of high-resolution imaging techniques used in biomedical research to reveal the 3D ultrastructure of cells, tissues, and small model organisms at nanometer resolution (Collinson et al., 2023).Typically, heavy metal-stained and resin-embedded specimens are sectioned, imaged, and computationally reconstructed to generate information-rich 3D image volumes that capture the ultrastructure of large fields of view.</p>
<p>In the field of connectomics, neuronal "wiring diagrams" derived from up to petabyte-sized image volumes have profoundly advanced our understanding of the brain.vEM has similarly transformed cell and developmental biology in health and disease in various experimental systems.In 2023, Nature cited vEM as a top technology to watch14 .</p>
<p>In contrast to imaging technologies where recorded signals originate primarily from labelled targets (e.g., fluorescence microscopy), vEM non-specifically captures all resolvable heavy metal-stained features in the volume -membranes, cytoskeletal elements, chromatin structures and large protein complexes.Thus, vEM datasets intrinsically lend themselves to sharing and reuse, since any publication accompanying such data interrogates but a fraction of the information acquired.vEM instrumentation is complex and expensive to run, with imaging experiments lasting days, weeks or even years, so re-use of these data increases return-on-investment.However, vEM comprises multiple related-but-distinct imaging modalities; vEM experiments are often combined with each other and with fluorescence and X-ray microscopy, and they typically include image registration, segmentation and model generation steps.Meaningful and accurate observations require that all these large and disparate datasets as well as related raster-and vector-based files be linked in a reliable and spatially coherent manner.This calls for an improvement on traditional file types, simplistic data sharing models, and visualization methods.Furthermore, vEM datasets range from &lt;1GB to &gt;1PB, so current strategies of storing, sharing and interacting with image data break down.</p>
<p>Without specific resources, support, and clear guidelines and targets, a mandate of data sharing is not feasible.Multimodal vEM data can be rendered meaningless unless it is correctly described at multiple levels beyond just the image file, e.g., biosample, sample, specimen, image layers (label maps, correlated images etc.At the same time, the most advanced and rigorous data sharing models will flounder unless they are accompanied by accessible resources and user interfaces.A recent survey of the vEM community showed that a majority of members are comfortable with no more than point-and-click or text entries/ commands; such findings can help calibrate expectations and shape strategic interventions.It appears that, at least for vEM, unless data sharing requirements are accompanied by an "easy button", resource and time-strapped users will be disincentivized or simply unable to share their valuable data.</p>
<p>The current status of data sharing within the vEM community can be roughly split into two camps: on the one hand, large, well-funded consortia (especially within the connectomics community) have done a remarkable job of uploading gigantic datasets15 , along with intuitive browser-based user interfaces and visualization software16 -although it is too early to evaluate the true extent of re-use of these data.On the other hand, sharing by smaller groups is improving17 but still patchy, with many investigators uncomfortable with "giving away free data" and/ or unfamiliar with processes for data upload to public archives.These cultural roadblocks are reinforced by a lack of tangible incentives and formal recognition for data sharers, an entrenched hierarchy between data producing "technical" and data consuming "research" positions (and publications), and a lack of vendor support to ease the structured curation and transfer of images.</p>
<p>There has been progress: connectomics researchers have built paradigm-shifting hardware, file format, and software advances to share massive datasets with interactive tools.Practitioners of "cell biology vEM'' are also building plugins and tools that allow for data streaming and easier sharing (Pape et al., 2023).There are several repositories for vEM data, with EMPIAR (hosted at EMBL-EBI) emerging as a primary resource for vEM data that accompany manuscripts.Over a series of meetings, workshops, and efforts from working groups, the vEM community is coalescing around an understanding of the image data and accompanying metadata to be shared based on the REMBI recommendations.A nimble implementation of REMBI for vEM will lay the groundwork for facile and meaningful data sharing amongst various research groups, and indeed some institutions have begun this process to cohere disparate data streams from imaging core facilities.</p>
<p>Use case 3: Medical imaging</p>
<p>Medical imaging enables visualization of human anatomy and function, usually in a non-invasive manner.Depending on the targeted anatomic region and the specific label, nuclear medicine systems can yield quantitative information on physiology, such as metabolism.Ultrasound uses sound waves (not ionizing radiation) to non-invasively image regions of the body.Ultrasound probes (transducers) are implemented on the skin externally; however, they can also be used internally such as in the vaginal cavity to obtain better quality images.Most recently, they are used during surgery to assess disease extent.Ultrasound can image structure as well as function such as echogenicity, tissue stiffness (elastography), and blood velocity.MRI is a non-ionizing 3D imaging technique that incorporates a magnetic field that aligns protons within the body and a radiofrequency current that stimulates the protons, which when turned off, the protons realign with the magnetic field releasing energy subsequently detected by MRI sensors.MRIs can give information on structure, the biochemical nature of tissue, and information on the local environment.Use of intravenous contrast agents are used with temporal MRI acquisitions to yield dynamic-contrast enhanced MRI to yield images with information on vascular uptake and tumor angiogenesis.</p>
<p>Computer vision and AI of medical images has been studied since the 1960's, with the 1980's bringing in computer-aided detection/diagnosis (CAD) as means to extract and merge information from medical images to aid radiologists in their interpretation (Giger et al., 2013;Sahiner et al., 2019).These efforts led to the first FDA-approved CADe system in 1998 (in mammography) (Freer &amp; Ulissey, 2001) and the first FDA-cleared CADx system to aid in cancer diagnosis in 2017 (in breast MRI) (Jiang et al., 2021;Yanase &amp; Triantaphyllou, 2019).Many developments have accompanied the improvements in compute power, storage, and deep-learning technologies.It is important to note that such developments have been greatly facilitated by having medical images acquired in the DICOM format, a world-wide, industry agreement to standardize the acquisition of medical images (Mustra et al., 2008).</p>
<p>Multiple repositories exist for medical images -varying in terms of level of curation, governance, accessibility, and interoperability.Examples include The Cancer Image Archive (TCIA), the Image Data Commons (IDC), and Medical Imaging and Data Resource Commons (MIDRC) (Table 1).</p>
<p>To enable the development of trustworthy AI of medical images, best practices in terms of the collection, data models, harmonization, diversity, annotations, and training/testing protocols are critical (Hosny et al., 2018).These are being addressed by current repositories, some of which include educational information on bias and on metrology for AI investigators.It is necessary to avoid "garbage in, garbage out".Note that data collection typically entails diagnostic quality images, which can vary from low quality to high quality images.It is important that the data are curated and organized via a data model so that the end user/developer understands how realistic the data are and from which cohorts can be selected.Many imaging repositories include images and some metadata (clinical and demographic data).However, to conduct multi-modal AI, interoperability of image-based data commons with other data commons (such as those with her, genomics, etc) are necessary.Often such interoperability can technically be accomplished, however, the varying governances of different data commons can hinder efficient implementation.</p>
<p>To increase imaging studies available to developers, a federation of data commons and sources may be necessary, potentially using combined centralized and federated commons.Ultimately, medical imaging AI algorithms need to be appropriately evaluated for their specific clinical question, specific clinical claim, and their intended population to move through regulatory (FDA) to clinical practice.It is important to note that AI-enabled medical devices cannot be used "offlabel".</p>
<p>Use case 4: Artificial Intelligence with scientific image data</p>
<p>During the past few years, the analysis of scientific image data has become ever more reliant on machine learning (ML) methods, and more generally, on modern methods from artificial intelligence (AI).Practically at the microscope, "smart microscopy" and other hardware control applications use AI/ML modules to detect rare events and/or regions of interest (ROIs) at which an imaging device should alter its mode of operation.Downstream from image acquisition, virtually all kinds of analyses now depend on AI/ML techniques, either via methods that are pure ML approaches (e.g.image denoising or segmentation), or hybrid approaches that combine AI/ML modules with additional computational components (e.g.cell or object tracking).Common to all successful applications of AI is the need for adequate amounts of image data and suitable data annotations for AI model training and validation.</p>
<p>Hence, the successful application of AI methods to data-driven life science applications hinges on the availability and accessibility of suitable image and label data.While some applications can be trained through self-supervision, i.e. on raw image data only, like Noise2Void (Krull et al., 2019), most models require external supervision that can stem from a concurrent imaging modality (Bai et al., 2023;Wagner et al., 2021) or from laborious manual annotation by human experts (Greenwald et al., 2022;Stringer et al., 2021).To give one example, the Segment Anything Model (Kirillov et al., 2023) was trained on 11 billion object masks, which someone had to create, collect, and curate before the final model could be trained.The benefits and drawbacks of the two main strategies: training multiple smaller models with a limited range of application or large, foundational models that can be applied to a much broader range of data modalities are still heavily debated in the community.The benefit of smaller models is much better control of the data domain they can operate on, but smaller models must be created again and again for the changing context of life science projects.In contrast, foundational models promise to be more widely applicable to a large range of biological problems at the cost of much higher data and computational resources requirements in training.Additionally, foundational models can be an excellent starting point for more specialized models via processes like fine-tuning, where the original model gets slightly modified for the sake of performing even better on a more specific task at hand.</p>
<p>Another aspect to this debate is that the heterogeneity of training data is key to improved generalisation performance of AI models.As we have learned from large language models such as the GPT model series (Brown et al., 2020;Radford et al., 2019), training on an unfathomable amount of data can yield generalization performance that drives applications like Chat-GPT (OpenAI, 2023).Still, such paradigm shifting foundational models can only be trained after enough sufficiently well annotated data is collected and made available.Hence, the key to truly changing the landscape of foundational model training for scientific image data processing and analysis is a central resource that collects image data from different sources and all relevant scientific image data modalities and that is annotated not only with metadata regarding the imaging process but also with metadata describing the sample and sample preparation.</p>
<p>Metadata of interest extends beyond the aspects mentioned above.Once analysis methods and/or AI models are established, metadata must also include specific image processing information that enables others to reuse a method or model in a valid way (some methods, for example, require an input image to have a number of pixels per dimension that is divisible by a specific number, or they can only be applied to large images when a certain amount of pixel context is given, etc.).Hence, a consistent metadata collection is also important to determine the applicability of pretrained models to new data.Such consistency is crucial to ensure fair and unbiased AI models, especially when dealing with medical image analysis (Drukker et al., 2023).</p>
<p>Once AI models are trained, uniform and standardized (FAIR) access to these pre-trained models for scientific image data analysis needs to be provided.While many collections of trained models have been created for method developers (eg: PyTorch model zoo18 ), other initiatives do also have a user and tool developer perspective in mind (e.g., Hugging Face19 , Bioimage Model Zoo20 ).The Bioimage Model Zoo (Table 1) is a solution built to meet the needs of the life science imaging community, hosting trained models, while also giving credit to all involved parties, linking to training data, teaching material, and consumer software tools that can run the hosted models.Such initiatives will increase the reproducibility and reusability of AI methods and aid their adoption and adaptation by life scientists to their specific analysis needs.</p>
<p>Another important aspect is that models, data, and metadata (see above) need to be programmatically accessible to computer scientists, method and tool developers, and bioimage analysts according to the FAIR principles.This will increase the utility of such data, reduce the duplication of datasets across sites, and therefore save energy, money, and resources.Ideally, we could, as a community, put additional incentives in place such as contributor badges or open calls to the imaging community with prizes or awards to close the gaps in publicly available data.</p>
<p>In the long-term, though, we would hope that the reuse of deposited data, data labels, metadata, or trained AI models would lead to resource citations, which would directly fuel the existing performance indicator values used in academia.While we start to see good examples of thorough work combining heterogeneous data with detailed metadata for training ML/DL models (Conrad &amp; Narayan, 2023), recognition metrics will further support a positive change in mindset.</p>
<p>In summary, we believe that the FAIR availability of data and AI models through suitable open data resources, based on community standards for metadata and programmatic access, will greatly increase the rate of new method development.Furthermore, life science experimentalists and imaging scientists will reap the benefits of improved usability and ease of deployment of the new and old AI/ML methods and, through contribution of additional data, will in turn motivate the closing of the remaining gaps in unaddressed tasks and imaging modalities.</p>
<p>Towards global image data sharing</p>
<p>Creating an ecosystem for global image data sharing requires engagement with the relevant stakeholders:</p>
<p>New funding mechanisms must be considered for various critical but non-traditional work in this space."Plumbing" to develop, maintain, and upgrade image data sharing pipelines into seamless data ecosystems must be funded.These include not only software developers who create modules -and importantly, "easy buttons'' for users -but also data wranglers and curators who will perform key under-the-hood work, much of which is unlikely to be published.Funders must also identify and support key archives and repositories in various fields, and then enforce image data and metadata formats for users.An international federated system is an appealing model that allows resiliency and easier access/response.For key areas (disease models for example), funding of architectures and benchmark datasets with features such as correlated multimodal images and versioned segmentation will allow data mining and standardization of future experiments in the area.Finally, in the spirit of true democratization, funders must support cloud resources for smaller, resource strapped labs, while training and educating researchers on the benefits of and tools for most efficiently sharing data for maximal impact.Conceptually, for grant proposals, the creation and sharing of meaningful data specifically for re-use must be treated on par with scientific manuscripts -or alternatively, alternative funding streams must be created for such data streams.On a larger scale, funders should consider novel approaches to image data acquisition and sharing, such as national or regional networks.Given the expense of instrumentation and upkeep, advanced imaging lends itself to shared facilities to maximise returnon-investment.To prevent existing and new facilities from devolving into a patchwork of unconnected and variably capable labs, funders must move beyond the traditional "pay for play" core facility and toward imaginative models along the lines of existing national imaging infrastructures as they can be found under the legal entity of Euro-BioImaging ERIC.</p>
<p>Journals must require meaningful data sharing and must provide authors with resources to do so.For example, tools to quality control datasets and metadata are critical, so that these resources do not devolve into data dumps.Perhaps similar to Methods and Protocol journals, further support and recognition of "Data Journals" (Kindling &amp; Strecker, 2022), that publish key datasets with some accompanying research to establish utility and quality, will help drive trust and eventual re-use of data This will also pave the way to publish quality data and gain credit, by being able to cite shared data at par with method and protocol papers.</p>
<p>Today's imaging technologies are inseparable from high-end instrumentation and downstream computation.Given the expensive nature of these tools, the relatively slow rate and potentially high volume of data generation, and the intrinsic reusability of imaging data, vendors' responsibilities must extend beyond the point of data creation (i.e., writing detector recordings to disk).The image data must be accompanied by metadata that allow thorough descriptions of the images and ideally also experiments leading to them, and the metadata itself should be well structured according to accepted formats.Critically, with recent investments in developing commercial "complete pipelines", users must have the option to easily exit vendor-created silos and export these data and metadata into open formats.</p>
<p>All these ideas gain potency only when there is active adoption at the user and community level.The awkward truth is that there are ingrained habits about sharing data; on top of a general reluctance to openly share data that may be re-mined by others, there is also an unfortunate tendency to "check the boxes" during publication and upload a poorly annotated dataset, which is of little use.Some of these behaviours are understandable given the scarcity and modes of funding -too much emphasis on new data generation, and not enough on data sharing and data re-use.That said, it is undeniable that the community must get over these qualms and share data meaningfully -anecdotal evidence suggests that such data actually engenders collaborations and new discoveries.The community can also play a critical role in enabling useful data sharing by coming together to agree on metadata standards.Implementations of recommendations that allow easy metadata field population and simultaneously enable project tracking may incentivize uptake of such tools.Luckily, many international imaging initiatives such as Global BioImaging (here: International Working Group on Image Data), QUAREP-LiMi, BINA and others (see Table 2), have been launched in the recent past, which in close collaboration are positioned to take on this critical role and speak on behalf of their communities.</p>
<p>Towards Global Image Data Sharing:</p>
<p>A to-do list for various stakeholders</p>
<p>• Launch, identify and support key data repositories.An international federated archival system with some software/compute resources is an appealing model that supports resilience and access.• Support education of researchers on benefits of image data sharing and reuse, FAIR data, as well as training and dissemination of tools to do the same in a facile manner.• Develop metrics to recognize and incentivize high-quality, structured data generation that allows easy sharing and re-use.• Provide resources to build useful, accessible, and community recommendationbased implementations of metadata standards to incentivize uptake and enable enforcement.• Develop new funding mechanisms for critical but non-traditional work in this space e.g., to develop, maintain, and upgrade data sharing pipelines and ecosystems, as well as for software developers, data wranglers, data curators, and FAIR image data stewards.• Fund data architectures and benchmark datasets in key areas (e.g., disease models).• Develop and implement data streaming approaches to support navigation of massive data, such as vEM and correlative/ multimodal image datasets.• Establish guidelines for journals and develop additional specialist "Data Journals" to support re-use and citation of high value datasets, similar to Methods and Protocols journals.• Develop novel approaches to image data acquisition and sharing, such as coordinated national or regional facility networks, to maximise return-on-investment. • Engage with vendors to mandate that image data be accompanied by wellstructured metadata in accepted formats and within data models that thoroughly describe images, with the option to export both image data and metadata into open formats.• Support development of guardrails for AI in image data acquisition and analysis to clarify data provenance and discourage "black box" solutions, e.g., models and training pipelines must be transparent, and results or images generated by AI models must be clearly marked as such.</p>
<p>Table 1 : Existing public image data repositories for open data sharing Type Name Link
1Public image dataBioImage Archivehttps://www.ebi.ac.uk/bioimarepositoriesge-archive/(Hartley et al., 2022)Public image dataImage Data Resourcehttps://idr.openmicroscopy.orrepositoriesg/(Williams et al., 2017)Public image dataSSBD, a platform for Systemshttps://ssbd.riken.jprepositoriesScience of Biological Dynamics(Tohsato et al., 2016)Public image dataMedical Imaging and Datahttps://midrc.org andrepositoriesResource Commons, MIDRChttps://data.midrc.org/Public image dataEMPIAR, the Electronrepositories and for publicMicroscopy Public ImagedownloadArchive
https://www.ebi.ac.uk/empiar/ (Iudin et al., 2022) For public download The Cancer Image Archive https://www.cancerimagingarchive.net/Cloud-based secure access to cancer imaging data Imaging Data Commons (IDC) https://datacommons.cancer.gov/repository/imaging-datacommons</p>
<p>Table 2 : Relevant publications, recommendations, and guidelines on image data management and sharing (non-exhaustive).
2TopicTitleLinkPublic image dataA call for public archives forhttps://www.nature.com/articlrepositoriesbiological image data (Ellenberges/s41592-018-0195-8.pdfet al., 2018)Public image dataA global view of standards forhttps://doi.org/10.1038/s4159repositories; standards foropen image data formats and2-021-01113-7image data formatsrepositories (Swedlow et al.,2021)Data infrastructure (local) Position statement: Biologistshttps://arxiv.org/abs/2108.07need modern data infrastructure631on campus (Andreev et al.,2021)FAIR image dataBuilding a FAIR image dataecosystem for microscopy(Kemmer et al., 2023)
https://zenodo.org/record/7788899#.ZE-gJ8FByw4 Image data file formats OME-NGFF: a next-generation file format for expanding https://www.nature.com/articles/s41592-021-01326-w bioimaging data-access strategies (Moore et al., 2021) https://www.midrc.org/and https://data.midrc.org/European FAIR data and service infrastructure European Open Science Cloud https://www.eosc.eu/</p>
<p>Typically housed in Radiology and Radiation Therapy departments, medical images are critical in early detection, diagnosis, prognosis, risk assessment, assessing response to therapy, guiding therapeutic interventions, and most recently, theranostics.Typical imaging modalities include radiographs, computed tomography (CT), ultrasound, magnetic resonance imaging (MRI), and nuclear medicine (e.g., PET and SPECT).Almost all medical images are obtained or can be readily converted into DICOM format thus enabling effective and efficient data transportation, archival, and machine learning.</p>
<p>Radiographic imaging (often called "x-rays") and CT use ionizing radiation to measure the attenuation of kilovoltage photons as they pass through the body.Common examples include chest radiographs, full-field digital mammography (FFDM), and tomosynthesis.CT acquires images at multiple angles allowing for the generation of cross-sectional images, yielding "slices" through the body.Acquisition parameters of energy, slice thickness, projections and angles, and whether contrast is employed (and what type) are examples of data elements needed for harmonization of scans across patients, institutions, and protocols.Nuclear medicine techniques include positron emission tomography (PET) and single-photon emission computed tomography (SPECT) scans.Such imaging acquisitions involve radioactive-labeled agents, with the emitting photons being detected by energy-specific sensors.</p>
<p>www.eurobioimaging.eu <br />
https://ngff.openmicroscopy.org/
https://ai4life.eurobioimaging.eu/
https://www.eosc.eu/
https://h01-release.storage.googleapis.com/landing.html
https://www.nibib.nih.gov/
https://www.openmicroscopy.org/
https://quarep.org/
https://www.bioimagingnorthamerica.org/
https://github.com/saalfeldlab/n5
https://www.rrids.org/
https://www.ebi.ac.uk/ols4/ontologies/fbbi
https://www.ncbi.nlm.nih.gov/taxonomy
https://www.nature.com/articles/s41592-023-01861-8
https://h01-release.storage.googleapis.com/landing.html
https://h01-release.storage.googleapis.com/gallery.html
http://nanotomy.org/ AND https://openorganelle.janelia.org/datasets
https://pytorch.org/serve/model_zoo.html
https://huggingface.co/
https://bioimage.io/#/
Finally, a word about artificial intelligence, which is disrupting a wide variety of scientific and non-scientific fields at an astonishing rate. It is clear that in the near future, much of the querying of large image data will be done by AI, so the design of data sharing platforms and schemas must have this in mind. Creating large image datasets that can be mined for AI algorithm/model training is enticing due to the potential to reveal new insights, but without guardrails, these non-moral computational processes and results can have profound and unforeseeable impacts to the imaging community and beyond, most easily intuited with possible AI-based diagnostic or predictive tools in the clinic. We do not delve deep into the ethics of AI, but two rules of thumb could be: 1. For any model trained on publicly available data, the model and full training pipeline must be transparent and public to prevent "black box" predictions and synthesised data (AI model cards 21 or nutritious labels 22 ). 2. Results or images that were generated by AI models must be clearly marked as such. Growing efforts in this direction, like elucidated in the Use Case 4, will also need dedicated and sustained support to grow into a global resource.21 https://arxiv.org/abs/1810.03993 22 https://arxiv.org/abs/1805.03677
AcknowledgementsDisclaimer: Commercial products are identified in this document in order to specify the experimental procedure adequately.Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the products identified are necessarily the best available for the purpose.Kedar Narayan is funded by Federal funds from the National Cancer Institute, National Institutes of Health, under Contract No. 75N91019D00024.The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government.
A Andreev, T Morrell, K Briney, S Gesing, U Manor, 10.48550/arXiv.2108.07631arXiv:2108.07631Biologists need modern data infrastructure on campus. 2021</p>
<p>The new era of quantitative cell imaging-Challenges and opportunities. N Bagheri, A E Carpenter, E Lundberg, A L Plant, R Horwitz, 10.1016/j.molcel.2021.12.024Molecular Cell. 8222022</p>
<p>Deep learning-enabled virtual histological staining of biological samples. B Bai, X Yang, Y Li, Y Zhang, N Pillar, A Ozcan, 10.1038/s41377-023-01104-7Light: Science &amp; Applications. 1212023</p>
<p>Understanding and Using DICOM, the Data Interchange Standard for Biomedical Imaging. W D Bidgood, S C Horii, F W Prior, D E Van Syckle, Journal of the American Medical Informatics Association. 431997</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, D Amodei, 10.48550/arXiv.2005.14165arXiv:2005.14165Language Models are Few-Shot Learners. 2020</p>
<p>Volume EM: A quiet revolution takes shape. L M Collinson, C Bosch, A Bullen, J J Burden, R Carzaniga, C Cheng, M C Darrow, G Fletcher, E Johnson, K Narayan, C J Peddie, M Winn, C Wood, A Patwardhan, G J Kleywegt, P Verkade, 10.1038/s41592-023-01861-8Nature Methods. 2062023</p>
<p>Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model trained on a diverse dataset. R Conrad, K Narayan, 10.1016/j.cels.2022.12.006Cell Systems. 1412023</p>
<p>Toward fairness in artificial intelligence for medical image analysis: Identification and mitigation of potential biases in the roadmap from data collection to model deployment. K Drukker, W Chen, J W Gichoya, N P Gruszauskas, J Kalpathy-Cramer, S Koyejo, K J Myers, R C Sá, B Sahiner, H M Whitney, Z Zhang, M L Giger, 10.1117/1.JMI.10.6.061104Journal of Medical Imaging. 106611042023</p>
<p>A call for public archives for biological image data. J Ellenberg, J R Swedlow, M Barlow, C E Cook, U Sarkans, A Patwardhan, A Brazma, E Birney, 10.1038/s41592-018-0195-8Nature Methods. 15112018</p>
<p>Screening mammography with computer-aided detection: Prospective study of 12,860 patients in a community breast center. T W Freer, M J Ulissey, 10.1148/radiol.2203001282Radiology. 22032001</p>
<p>Breast image analysis for risk assessment, detection, diagnosis, and treatment of cancer. M L Giger, N Karssemeijer, J A Schnabel, 10.1146/annurev-bioeng-071812-152416Annual Review of Biomedical Engineering. 152013</p>
<p>Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning. N F Greenwald, G Miller, E Moen, A Kong, A Kagel, T Dougherty, C C Fullaway, B J Mcintosh, K X Leow, M S Schwartz, C Pavelchek, S Cui, I Camplisson, O Bar-Tal, J Singh, M Fong, G Chaudhry, Z Abraham, J Moseley, D Van Valen, 10.1038/s41587-021-01094-0Nature Biotechnology. 4042022</p>
<p>Towards community-driven metadata standards for light microscopy: Tiered specifications extending the OME model. M Hammer, M Huisman, A Rigano, U Boehm, J J Chambers, N Gaudreault, A J North, J A Pimentel, D Sudar, P Bajcsy, C M Brown, A D Corbett, O Faklaris, J Lacoste, A Laude, G Nelson, R Nitschke, F Farzam, C S Smith, C Strambio-De-Castillia, 10.1038/s41592-021-01327-9Nature Methods. 18122021</p>
<p>The BioImage Archive -Building a Home for Life-Sciences Microscopy Data. M Hartley, G J Kleywegt, A Patwardhan, U Sarkans, J R Swedlow, A Brazma, 10.1016/j.jmb.2022.167505Journal of Molecular Biology. 2022. 167505</p>
<p>Artificial intelligence in radiology. A Hosny, C Parmar, J Quackenbush, L H Schwartz, H J W L Aerts, 10.1038/s41568-018-0016-5Nature Reviews Cancer. 1882018</p>
<p>EMPIAR: The Electron Microscopy Public Image Archive. A Iudin, P K Korir, S Somasundharam, S Weyand, C Cattavitello, N Fonseca, O Salih, G J Kleywegt, A Patwardhan, 10.1093/nar/gkac1062Nucleic Acids Research. 2022. gkac1062</p>
<p>Artificial Intelligence Applied to Breast MRI for Improved Diagnosis. Y Jiang, A V Edwards, G M Newstead, 10.1148/radiol.2020200292Radiology. 29812021</p>
<p>Building a FAIR image data ecosystem for microscopy communities. I Kemmer, A Keppler, B Serrano-Solano, A Rybina, B Özdemir, J Bischof, A El Ghadraoui, J E Eriksson, A Mathur, 10.1007/s00418-023-02203-7Histochemistry and Cell Biology. 2023</p>
<p>M Kindling, D Strecker, List of data journals. 2022Data set</p>
<p>. 10.5281/ZENODO.7082126</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollár, R Girshick, 10.48550/arXiv.2304.02643arXiv:2304.02643Segment Anything. 2023</p>
<p>A Krull, T.-O Buchholz, F Jug, 10.48550/arXiv.1811.10980arXiv:1811.10980Noise2Void-Learning Denoising from Single Noisy Images. 2019</p>
<p>MDEmic: A metadata annotation tool to facilitate management of FAIR image data in the bioimaging community. S Kunis, S Hänsch, C Schmidt, F Wong, C Strambio-De-Castillia, S Weidtkamp-Peters, 10.1038/s41592-021-01288-zNature Methods. 2021</p>
<p>FPbase: A community-editable fluorescent protein database. T J Lambert, 10.1038/s41592-019-0352-8Nature Methods. 1642019</p>
<p>Matúš Kalaš, L Plantard, J Lindblad, M Jones, Nataša Sladoje, M A Kirschmann, A Chessel, L Scholz, F Rössler, L N Sáenz, E G D Mariscal, J Bogovic, A Dufour, X Heiligenstein, D Waithe, Marie-Charlotte Domart, Matthia Karreman, Raf Van De, Plas, R Haase, We, 10.7490/F1000RESEARCH.1117826.1EDAM-bioimaging: The ontology of bioimage informatics operations, topics, data, and formats. 2020. 2020</p>
<p>Best practices and tools for reporting reproducible fluorescence microscopy methods. P Montero Llopis, R A Senft, T J Ross-Elliott, R Stephansky, D P Keeley, P Koshar, G Marqués, Y.-S Gao, B R Carlson, T Pengo, M A Sanders, L A Cameron, M S Itano, 10.1038/s41592-021-01156-wNature Methods. 18122021</p>
<p>J Moore, C Allan, S Besson, J.-M Burel, E Diel, D Gault, K Kozlowski, D Lindner, M Linkert, T Manz, W Moore, C Pape, C Tischer, J R Swedlow, OME-NGFF: Scalable format strategies for interoperable bioimaging data. 2021Preprint</p>
<p>. 10.1101/2021.03.31.437929Bioinformatics. </p>
<p>Overview of the DICOM standard. M Mustra, K Delac, M Grgic, 50th International Symposium ELMAR. 2008. 20081</p>
<p>. Openai, 10.48550/arXiv.2303.08774arXiv:2303.087742023GPT-4 Technical Report</p>
<p>W Ouyang, F Beuttenmueller, E Gómez-De-Mariscal, C Pape, T Burke, C Garcia-López-De-Haro, C Russell, L Moya-Sans, C De-La-Torre-Gutiérrez, D Schmidt, D Kutra, M Novikov, M Weigert, U Schmidt, P Bankhead, G Jacquemet, D Sage, R Henriques, A Muñoz-Barrutia, A Kreshuk, 10.1101/2022.06.07.49510206.07.495102BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis. 20222022</p>
<p>MoBIE: A Fiji plugin for sharing and exploration of multi-modal cloud-hosted big image data. C Pape, K Meechan, E Moreva, M Schorb, N Chiaruttini, V Zinchenko, Martinez, H Vergara, G Mizzon, J Moore, D Arendt, A Kreshuk, Y Schwab, C Tischer, 10.1038/s41592-023-01776-4Nature Methods. 2042023</p>
<p>Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019</p>
<p>Micro-Meta App: An interactive software tool to facilitate the collection of microscopy metadata based on community-driven specifications. A Rigano, S Ehmsen, S U Ozturk, J Ryan, A Balashov, M Hammer, K Kirli, K Bellve, U Boehm, C M Brown, J J Chambers, R A Coleman, A Cosolo, O Faklaris, K Fogarty, T Guilbert, A B Hamacher, M S Itano, D P Keeley, C Strambio-De-Castillia, 10.1101/2021.05.31.44638205.31.44638220212021</p>
<p>MethodsJ2: A software tool to capture metadata and generate comprehensive microscopy methods text. J Ryan, T Pengo, A Rigano, P M Llopis, M S Itano, L A Cameron, G Marqués, C Strambio-De-Castillia, M A Sanders, C M Brown, 10.1038/s41592-021-01290-5Nature Methods. 2021</p>
<p>Deep learning in medical imaging and radiation therapy. B Sahiner, A Pezeshk, L M Hadjiiski, X Wang, K Drukker, K H Cha, R M Summers, M L Giger, 10.1002/mp.13264Medical Physics. 4612019</p>
<p>REMBI: Recommended Metadata for Biological Images-enabling reuse of microscopy data in biology. U Sarkans, W Chiu, L Collinson, M C Darrow, J Ellenberg, D Grunwald, J.-K Hériché, A Iudin, G G Martins, T Meehan, K Narayan, A Patwardhan, M R G Russell, H R Saibil, C Strambio-De-Castillia, J R Swedlow, C Tischer, V Uhlmann, P Verkade, A Brazma, 10.1038/s41592-021-01166-8Nature Methods. 18122021</p>
<p>C Schmied, M Nelson, S Avilov, G.-J Bakker, C Bertocchi, J Bischof, U Boehm, J Brocher, M Carvalho, C Chiritescu, J Christopher, B Cimini, E Conde-Sousa, M Ebner, R Ecker, K Eliceiri, J Fernandez-Rodriguez, N Gaudreault, L Gelman, H K Jambor, 10.48550/arXiv.2302.07005arXiv:2302.07005Community-developed checklists for publishing images and image analysis. 2023</p>
<p>Cellpose: A generalist algorithm for cellular segmentation. C Stringer, T Wang, M Michaelos, M Pachitariu, 10.1038/s41592-020-01018-xNature Methods. 1812021</p>
<p>A global view of standards for open image data formats and repositories. J R Swedlow, P Kankaanpää, U Sarkans, W Goscinski, G Galloway, L Malacrida, R P Sullivan, S Härtel, C M Brown, C Wood, A Keppler, F Paina, B Loos, S Zullino, D L Longo, S Aime, S Onami, 10.1038/s41592-021-01113-7Nature Methods. 18122021</p>
<p>SSBD: A database of quantitative data of spatiotemporal dynamics of biological phenomena. Y Tohsato, K H L Ho, K Kyoda, S Onami, 10.1093/bioinformatics/btw417Bioinformatics. 32222016</p>
<p>Deep learning-enhanced light-field imaging with continuous validation. N Wagner, F Beuttenmueller, N Norlin, J Gierten, J C Boffi, J Wittbrodt, M Weigert, L Hufnagel, R Prevedel, A Kreshuk, 10.1038/s41592-021-01136-0Nature Methods. 1852021</p>
<p>The FAIR Guiding Principles for scientific data management and stewardship. M D Wilkinson, M Dumontier, Ij J Aalbersberg, G Appleton, M Axton, A Baak, N Blomberg, J.-W Boiten, L B Da Silva Santos, P E Bourne, J Bouwman, A J Brookes, T Clark, M Crosas, I Dillo, O Dumon, S Edmunds, C T Evelo, R Finkers, B Mons, 10.1038/sdata.2016.18Scientific Data. 312016</p>
<p>Image Data Resource: A bioimage data integration and publication platform. E Williams, J Moore, S W Li, G Rustici, A Tarkowska, A Chessel, S Leo, B Antal, R K Ferguson, U Sarkans, A Brazma, R E Carazo Salas, J R Swedlow, 10.1038/nmeth.4326Nature Methods. 1482017</p>
<p>A systematic survey of computer-aided diagnosis in medicine: Past and present developments. J Yanase, E Triantaphyllou, 10.1016/j.eswa.2019.112821Expert Systems with Applications. 1381128212019</p>            </div>
        </div>

    </div>
</body>
</html>
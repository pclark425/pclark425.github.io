<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3252 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3252</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3252</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-003ef1cd670d01af05afa0d3c72d72228f494432</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/003ef1cd670d01af05afa0d3c72d72228f494432" target="_blank">LLM+P: Empowering Large Language Models with Optimal Planning Proficiency</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> LLM+P is the first framework that incorporates the strengths of classical planners into large language models, and is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most Problems.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\footnote{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3252.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3252.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model + Planner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Framework introduced in this paper that uses an LLM (GPT-4) to translate natural-language planning problems into PDDL, calls a classical planner (Fast-Downward) to produce an (often optimal) plan, and uses the LLM to translate the plan back to natural language; relies critically on an example (problem,PDDL) pair provided in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM+P (GPT-4 + Fast-Downward)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-4 (temperature=0, deterministic response) is used as a translator between natural language and PDDL; Fast-Downward planner (aliases SEQ-OPT-FDSS-1 for optimal and LAMA for sub-optimal) is used to search for plans. Domain PDDL files are provided by a human expert and used as fixed world models.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Robot planning benchmark (7 domains: BlockSWORLD, BARMAN, Floortile, GRIPPERS, Storage, TERMES, TyReWorld) from automatically generated PDDL tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given natural-language descriptions of initial states and goals for planning problems in robot domains, generate a correct (and often optimal) symbolic plan that achieves the goals when executed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>in-context / external fixed domain specification (context window as episodic/example memory + provided domain PDDL as external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Memory is implemented as (1) an in-context demonstration consisting of an example natural-language problem paired with its PDDL (placed in the LLM prompt), and (2) a provided domain PDDL file (fixed external specification) that acts as persistent declarative memory of actions, preconditions and effects. The LLM reads these prompt-context tokens (within the model context window) to produce problem PDDL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>With context (example problem + problem-PDDL) LLM+P achieves high success rates per Table I: BARMAN 20% (100% when sub-optimal alias included), BlockSWORLD 90%, Floortile 0%, GRIPPERS 95% (100%), Storage 85%, TERMES 20%, TyReWorld 10% (90%); overall substantially higher success than without context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without the in-context example (LLM+P^-), the reported success rates are effectively 0% across the evaluated domains in Table I (LLM+P^- column shows 0 for all domains).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Providing an in-context demonstration (example problem paired with PDDL) is crucial: it transforms the LLM's output from unusable/mis-specified PDDL into planner-solvable PDDL, leading to large gains in correctness and optimality. The fixed domain PDDL provides reliable action semantics enabling the planner to guarantee sound/optimal plans.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Failures primarily arise from mis-specified or incomplete problem PDDL generated by the LLM (e.g., missing initial facts, wrong predicates), which are exacerbated when demonstration context is absent. The approach also depends on the LLM context window capacity (prompt size limits) and human-provided domain PDDL.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Include a concise example (natural-language problem paired with correct problem-PDDL) in the prompt for in-context learning; provide a correct domain PDDL beforehand; keep prompts complete (include all initial conditions); use the classical planner for search/optimality rather than relying on LLM plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3252.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3252.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-AS-P (LLM-as-planner)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-Planner baseline (natural-language planning by LLM alone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where the LLM is prompted to directly produce a natural-language plan (or action sequence) for the planning task without invoking a symbolic planner; includes an evaluated adaptation using Tree of Thoughts (ToT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-AS-P (GPT-4 outputs plans directly); LLM-AS-P (ToT) = Tree-of-Thoughts adaptation using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-4 is prompted to directly generate action sequences or natural-language plans; for ToT adaptation, the LLM is called repeatedly to expand a search tree of partial plans and to evaluate/rank partial plans (breadth-first ToT adaptation with time budget).</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Same robot planning benchmark (7 domains listed above).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Directly produce a plan in natural language that achieves the goal from the described initial state (no symbolic planner in the loop).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>No explicit external memory mechanism is used beyond the standard prompt/context tokens; ToT calls repeatedly use the LLM's internal state and the provided prompt at each tree node but do not maintain an external memory store or retrieval component.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Overall poor: Table I shows LLM-AS-P (no context) and LLM-AS-P (with context) often produce infeasible plans. Example: BlockSWORLD LLM outputs 15% correct (30% including sub-optimal plans) but frequently produces plans that violate preconditions; many domains show 0% or low success. Tree-of-Thoughts (ToT) adaptation also times out or yields low success (mostly 0%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Not applicable; performance suggests that direct LLM plan generation without external memory/solver struggles to track state and preconditions over long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Fails to maintain or update symbolic state properties (e.g., ON, CLEAR) across long horizons; often violates preconditions; ToT adaptation causes a very large number of LLM calls and frequently times out, so it's computationally expensive and still fails to reliably detect goal achievement.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Do not rely solely on LLMs to produce long-horizon executable plans; instead use LLMs to translate problem descriptions into formal representations consumable by sound planners, or augment LLMs with external state-tracking/verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3252.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3252.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning (context as memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning / demonstration-based prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a small number of input–output examples (here: an example natural-language problem and its corresponding PDDL) in the prompt so the LLM generalizes to produce correct outputs for new inputs; treated as a form of prompt-level episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>In-context demonstration (example problem + PDDL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompt engineering technique where one or several exemplars are prepended to the user query so the LLM uses the exemplars as templates to generate the desired structured output; in this paper used with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Applied to the same robot planning benchmark (PDDL generation task).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Help the LLM produce correct problem PDDL files from natural-language problem descriptions by providing an exemplar mapping in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-based episodic memory / demonstration memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>The exemplar (natural-language problem + target PDDL) is included in the model prompt; the LLM conditions on those tokens when producing the PDDL for the new problem. This relies on the LLM's context window to serve as ephemeral memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Crucial improvement: without the exemplar LLM+P produced incorrect/missing PDDL and the planner could not solve problems; with the exemplar success rates rose dramatically (see LLM+P results: many domains went from 0% to high success).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without the exemplar in the prompt, LLM+P generated improperly specified PDDL (missing predicates/initial conditions) and planner success rates were essentially 0% (LLM+P^- column).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Enables correct and solver-compatible translations from NL to PDDL; significantly increases downstream planner success and optimality by ensuring problem encodings are complete and syntactically correct.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Limited by the LLM context window size (cannot include arbitrarily many or very large exemplars), and exemplar quality matters; if the exemplar is itself incomplete or mismatched, the LLM can reproduce those errors.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Always include a concise, correct exemplar (problem,PDDL) when asking an LLM to output structured planning representations; keep exemplars focused and include required initial-state predicates explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3252.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3252.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-assisted Prompt Editing (MemPrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced human-in-the-loop system where a growing memory of errors and user feedback is stored and appended to prompts to improve future LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory-assisted prompt editing to improve gpt-3 after deployment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemPrompt (referenced method)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A system that stores past errors and corrective feedback and appends relevant memory entries to the prompt to bias the LLM away from repeating mistakes; cited as related work on augmenting LLMs with memory.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not applied in this paper's experiments; mentioned as an external memory augmentation approach that could improve LLM downstream behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>growing error-feedback memory appended to prompts (retrieval-augmented prompt memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Described in citation as a human-in-the-loop memory bank of past errors and corrections that are included in future prompts (no implementation details or experimental results in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Mentioned as improving LLM accuracy by supplying past corrective examples to the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Not evaluated here; potential limitations include prompt size growth and reliance on curated feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Consider retrieval/append strategies of corrective exemplars to prompts, but manage prompt size and memory selection carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3252.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3252.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REPLUG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REPLUG (Retrieval-augmented black-box language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced retrieval-augmentation paradigm that treats an LLM as a black box and augments it with a tunable retrieval model to provide relevant external context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Replug: Retrieval-augmented black-box language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REPLUG (referenced method)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented setup that retrieves exemplars or documents and appends them to LLM prompts to improve downstream performance; cited in related work as an external memory/augmentation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not applied in experiments; referenced as a relevant external-memory technique.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (document/exemplar retrieval appended to prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Cited conceptually only; typical implementation retrieves nearest-neighbor exemplars from a datastore and appends them to the model input, but the paper does not implement REPLUG.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Cited as a means to augment black-box LLMs with external knowledge and past examples, improving downstream outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Not evaluated in this work; general limitations include retrieval quality dependence and prompt length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>If applying retrieval augmentation, ensure retrieved exemplars are aligned to the target output format (e.g., PDDL) and manage context-window budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3252.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3252.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced method where LMs learn to decide when and how to call external tools (APIs) by augmenting prompts with tool calls; mentioned as related work on augmenting LLMs with external modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ToolFormer (referenced method)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A method that fine-tunes models to insert tool calls into generated text, enabling use of external modules such as calculators or search APIs; cited as an example of LLMs using external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not applied in this paper; mentioned as related work demonstrating tool-use augmentation for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external tool-call augmentation (not memory per se, but tool-augmented reasoning and retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Cited only; typical ToolFormer setups decide to call tools (e.g., calculator, search) and incorporate tool outputs into generation. The present paper uses a planner (external tool) but does not use ToolFormer's learned self-instrumentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Illustrates benefit of incorporating external modules (e.g., calculators, planners) into LLM workflows; the current paper follows the same philosophy by using a symbolic planner.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Not evaluated here; general caveats include the need to decide when tool calls are appropriate and to manage tool-call formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Memory-assisted prompt editing to improve gpt-3 after deployment <em>(Rating: 2)</em></li>
                <li>Replug: Retrieval-augmented black-box language models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Large language models still can't plan (a benchmark for llms on planning and reasoning about change) <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3252",
    "paper_id": "paper-003ef1cd670d01af05afa0d3c72d72228f494432",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "LLM+P",
            "name_full": "Large Language Model + Planner",
            "brief_description": "Framework introduced in this paper that uses an LLM (GPT-4) to translate natural-language planning problems into PDDL, calls a classical planner (Fast-Downward) to produce an (often optimal) plan, and uses the LLM to translate the plan back to natural language; relies critically on an example (problem,PDDL) pair provided in the prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM+P (GPT-4 + Fast-Downward)",
            "agent_description": "GPT-4 (temperature=0, deterministic response) is used as a translator between natural language and PDDL; Fast-Downward planner (aliases SEQ-OPT-FDSS-1 for optimal and LAMA for sub-optimal) is used to search for plans. Domain PDDL files are provided by a human expert and used as fixed world models.",
            "game_or_benchmark_name": "Robot planning benchmark (7 domains: BlockSWORLD, BARMAN, Floortile, GRIPPERS, Storage, TERMES, TyReWorld) from automatically generated PDDL tasks",
            "task_description": "Given natural-language descriptions of initial states and goals for planning problems in robot domains, generate a correct (and often optimal) symbolic plan that achieves the goals when executed.",
            "uses_memory": true,
            "memory_type": "in-context / external fixed domain specification (context window as episodic/example memory + provided domain PDDL as external memory)",
            "memory_implementation_details": "Memory is implemented as (1) an in-context demonstration consisting of an example natural-language problem paired with its PDDL (placed in the LLM prompt), and (2) a provided domain PDDL file (fixed external specification) that acts as persistent declarative memory of actions, preconditions and effects. The LLM reads these prompt-context tokens (within the model context window) to produce problem PDDL.",
            "performance_with_memory": "With context (example problem + problem-PDDL) LLM+P achieves high success rates per Table I: BARMAN 20% (100% when sub-optimal alias included), BlockSWORLD 90%, Floortile 0%, GRIPPERS 95% (100%), Storage 85%, TERMES 20%, TyReWorld 10% (90%); overall substantially higher success than without context.",
            "performance_without_memory": "Without the in-context example (LLM+P^-), the reported success rates are effectively 0% across the evaluated domains in Table I (LLM+P^- column shows 0 for all domains).",
            "has_performance_comparison": true,
            "memory_benefits": "Providing an in-context demonstration (example problem paired with PDDL) is crucial: it transforms the LLM's output from unusable/mis-specified PDDL into planner-solvable PDDL, leading to large gains in correctness and optimality. The fixed domain PDDL provides reliable action semantics enabling the planner to guarantee sound/optimal plans.",
            "memory_limitations_or_failures": "Failures primarily arise from mis-specified or incomplete problem PDDL generated by the LLM (e.g., missing initial facts, wrong predicates), which are exacerbated when demonstration context is absent. The approach also depends on the LLM context window capacity (prompt size limits) and human-provided domain PDDL.",
            "best_practices_or_recommendations": "Include a concise example (natural-language problem paired with correct problem-PDDL) in the prompt for in-context learning; provide a correct domain PDDL beforehand; keep prompts complete (include all initial conditions); use the classical planner for search/optimality rather than relying on LLM plan generation.",
            "uuid": "e3252.0",
            "source_info": {
                "paper_title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "LLM-AS-P (LLM-as-planner)",
            "name_full": "LLM-as-Planner baseline (natural-language planning by LLM alone)",
            "brief_description": "Baseline approach where the LLM is prompted to directly produce a natural-language plan (or action sequence) for the planning task without invoking a symbolic planner; includes an evaluated adaptation using Tree of Thoughts (ToT).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LLM-AS-P (GPT-4 outputs plans directly); LLM-AS-P (ToT) = Tree-of-Thoughts adaptation using GPT-4",
            "agent_description": "GPT-4 is prompted to directly generate action sequences or natural-language plans; for ToT adaptation, the LLM is called repeatedly to expand a search tree of partial plans and to evaluate/rank partial plans (breadth-first ToT adaptation with time budget).",
            "game_or_benchmark_name": "Same robot planning benchmark (7 domains listed above).",
            "task_description": "Directly produce a plan in natural language that achieves the goal from the described initial state (no symbolic planner in the loop).",
            "uses_memory": false,
            "memory_type": null,
            "memory_implementation_details": "No explicit external memory mechanism is used beyond the standard prompt/context tokens; ToT calls repeatedly use the LLM's internal state and the provided prompt at each tree node but do not maintain an external memory store or retrieval component.",
            "performance_with_memory": null,
            "performance_without_memory": "Overall poor: Table I shows LLM-AS-P (no context) and LLM-AS-P (with context) often produce infeasible plans. Example: BlockSWORLD LLM outputs 15% correct (30% including sub-optimal plans) but frequently produces plans that violate preconditions; many domains show 0% or low success. Tree-of-Thoughts (ToT) adaptation also times out or yields low success (mostly 0%).",
            "has_performance_comparison": false,
            "memory_benefits": "Not applicable; performance suggests that direct LLM plan generation without external memory/solver struggles to track state and preconditions over long horizons.",
            "memory_limitations_or_failures": "Fails to maintain or update symbolic state properties (e.g., ON, CLEAR) across long horizons; often violates preconditions; ToT adaptation causes a very large number of LLM calls and frequently times out, so it's computationally expensive and still fails to reliably detect goal achievement.",
            "best_practices_or_recommendations": "Do not rely solely on LLMs to produce long-horizon executable plans; instead use LLMs to translate problem descriptions into formal representations consumable by sound planners, or augment LLMs with external state-tracking/verifiers.",
            "uuid": "e3252.1",
            "source_info": {
                "paper_title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "In-context learning (context as memory)",
            "name_full": "In-context learning / demonstration-based prompting",
            "brief_description": "Using a small number of input–output examples (here: an example natural-language problem and its corresponding PDDL) in the prompt so the LLM generalizes to produce correct outputs for new inputs; treated as a form of prompt-level episodic memory.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "agent_name": "In-context demonstration (example problem + PDDL)",
            "agent_description": "A prompt engineering technique where one or several exemplars are prepended to the user query so the LLM uses the exemplars as templates to generate the desired structured output; in this paper used with GPT-4.",
            "game_or_benchmark_name": "Applied to the same robot planning benchmark (PDDL generation task).",
            "task_description": "Help the LLM produce correct problem PDDL files from natural-language problem descriptions by providing an exemplar mapping in the prompt.",
            "uses_memory": true,
            "memory_type": "prompt-based episodic memory / demonstration memory",
            "memory_implementation_details": "The exemplar (natural-language problem + target PDDL) is included in the model prompt; the LLM conditions on those tokens when producing the PDDL for the new problem. This relies on the LLM's context window to serve as ephemeral memory.",
            "performance_with_memory": "Crucial improvement: without the exemplar LLM+P produced incorrect/missing PDDL and the planner could not solve problems; with the exemplar success rates rose dramatically (see LLM+P results: many domains went from 0% to high success).",
            "performance_without_memory": "Without the exemplar in the prompt, LLM+P generated improperly specified PDDL (missing predicates/initial conditions) and planner success rates were essentially 0% (LLM+P^- column).",
            "has_performance_comparison": true,
            "memory_benefits": "Enables correct and solver-compatible translations from NL to PDDL; significantly increases downstream planner success and optimality by ensuring problem encodings are complete and syntactically correct.",
            "memory_limitations_or_failures": "Limited by the LLM context window size (cannot include arbitrarily many or very large exemplars), and exemplar quality matters; if the exemplar is itself incomplete or mismatched, the LLM can reproduce those errors.",
            "best_practices_or_recommendations": "Always include a concise, correct exemplar (problem,PDDL) when asking an LLM to output structured planning representations; keep exemplars focused and include required initial-state predicates explicitly.",
            "uuid": "e3252.2",
            "source_info": {
                "paper_title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MemPrompt",
            "name_full": "Memory-assisted Prompt Editing (MemPrompt)",
            "brief_description": "A referenced human-in-the-loop system where a growing memory of errors and user feedback is stored and appended to prompts to improve future LLM responses.",
            "citation_title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "mention_or_use": "mention",
            "agent_name": "MemPrompt (referenced method)",
            "agent_description": "A system that stores past errors and corrective feedback and appends relevant memory entries to the prompt to bias the LLM away from repeating mistakes; cited as related work on augmenting LLMs with memory.",
            "game_or_benchmark_name": "",
            "task_description": "Not applied in this paper's experiments; mentioned as an external memory augmentation approach that could improve LLM downstream behavior.",
            "uses_memory": true,
            "memory_type": "growing error-feedback memory appended to prompts (retrieval-augmented prompt memory)",
            "memory_implementation_details": "Described in citation as a human-in-the-loop memory bank of past errors and corrections that are included in future prompts (no implementation details or experimental results in this paper).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "memory_benefits": "Mentioned as improving LLM accuracy by supplying past corrective examples to the prompt.",
            "memory_limitations_or_failures": "Not evaluated here; potential limitations include prompt size growth and reliance on curated feedback.",
            "best_practices_or_recommendations": "Consider retrieval/append strategies of corrective exemplars to prompts, but manage prompt size and memory selection carefully.",
            "uuid": "e3252.3",
            "source_info": {
                "paper_title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "REPLUG",
            "name_full": "REPLUG (Retrieval-augmented black-box language models)",
            "brief_description": "A referenced retrieval-augmentation paradigm that treats an LLM as a black box and augments it with a tunable retrieval model to provide relevant external context.",
            "citation_title": "Replug: Retrieval-augmented black-box language models",
            "mention_or_use": "mention",
            "agent_name": "REPLUG (referenced method)",
            "agent_description": "A retrieval-augmented setup that retrieves exemplars or documents and appends them to LLM prompts to improve downstream performance; cited in related work as an external memory/augmentation approach.",
            "game_or_benchmark_name": "",
            "task_description": "Not applied in experiments; referenced as a relevant external-memory technique.",
            "uses_memory": true,
            "memory_type": "retrieval-augmented external memory (document/exemplar retrieval appended to prompt)",
            "memory_implementation_details": "Cited conceptually only; typical implementation retrieves nearest-neighbor exemplars from a datastore and appends them to the model input, but the paper does not implement REPLUG.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "memory_benefits": "Cited as a means to augment black-box LLMs with external knowledge and past examples, improving downstream outputs.",
            "memory_limitations_or_failures": "Not evaluated in this work; general limitations include retrieval quality dependence and prompt length constraints.",
            "best_practices_or_recommendations": "If applying retrieval augmentation, ensure retrieved exemplars are aligned to the target output format (e.g., PDDL) and manage context-window budget.",
            "uuid": "e3252.4",
            "source_info": {
                "paper_title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ToolFormer",
            "name_full": "ToolFormer",
            "brief_description": "Referenced method where LMs learn to decide when and how to call external tools (APIs) by augmenting prompts with tool calls; mentioned as related work on augmenting LLMs with external modules.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "agent_name": "ToolFormer (referenced method)",
            "agent_description": "A method that fine-tunes models to insert tool calls into generated text, enabling use of external modules such as calculators or search APIs; cited as an example of LLMs using external tools.",
            "game_or_benchmark_name": "",
            "task_description": "Not applied in this paper; mentioned as related work demonstrating tool-use augmentation for LLMs.",
            "uses_memory": true,
            "memory_type": "external tool-call augmentation (not memory per se, but tool-augmented reasoning and retrieval)",
            "memory_implementation_details": "Cited only; typical ToolFormer setups decide to call tools (e.g., calculator, search) and incorporate tool outputs into generation. The present paper uses a planner (external tool) but does not use ToolFormer's learned self-instrumentation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "memory_benefits": "Illustrates benefit of incorporating external modules (e.g., calculators, planners) into LLM workflows; the current paper follows the same philosophy by using a symbolic planner.",
            "memory_limitations_or_failures": "Not evaluated here; general caveats include the need to decide when tool calls are appropriate and to manage tool-call formatting.",
            "uuid": "e3252.5",
            "source_info": {
                "paper_title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "rating": 2
        },
        {
            "paper_title": "Replug: Retrieval-augmented black-box language models",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.01531375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency</h1>
<p>Bo Liu ${ }^{<em> \dagger}$, Yuqian Jiang ${ }^{</em> \dagger}$, Xiaohan Zhang ${ }^{\ddagger}$, Qiang Liu ${ }^{\dagger}$, Shiqi Zhang ${ }^{\ddagger}$, Joydeep Biswas ${ }^{\dagger}$, Peter Stone ${ }^{\dagger \S}$</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon robot planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from robot planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems. We also show LLM+P enables a home robot to solve a complex manipulation task that is specified by the user in natural language. ${ }^{\dagger}$</p>
<h2>I. INTRODUCTION</h2>
<p>Ever since the birth of the field, AI researchers have sought to create programs that can converse in natural language with the same grace and flexibility as people. While even relatively simple models, such as Eliza from 1966 [1], can generate responses to some prompts that seem reasonable, it has always been relatively easy to generate prompts that expose their weaknesses compared to people - their lack of true "understanding."</p>
<p>While large language models (LLMs) such as GPT-4 [2] and ChatGPT [3] have far surpassed expectations of just a few years ago, they are no different in this respect. Indeed the internet is now awash with examples of people reveling in getting ChatGPT to generate output that even a 5-year-old human child would know to be ill-advised.</p>
<p>Given how LLMs are designed and trained, this phenomenon should come as no surprise. They are not specifically built to demonstrate understanding. They are trained</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to generate sequences of words that might be plausible to a human given a prior context. In the terms of Mahowald et al. [4] LLMs have become amazingly proficient at linguistic competence - knowing how to say things; but they are not nearly as good at functional competence - knowing what to say. Specifically, they can be (relatively) easily fooled by, for example, asking for the result of a straightforward arithmetic problem that does not appear in their training corpus or by asking for the solution to a novel planning problem that requires knowledge about how the world works.</p>
<h2>A Failure Example of GPT-4 in Planning</h2>
<p>Problem (P1): You have 5 blocks. One cannot place more than one block on another block. b5 is on top of b3. b4 is on top of b2. b2 is on top of b1. b3 is on top of b4. b1 is on the table. b5 is clear. Your arm is empty.
Your goal is to move the blocks. b1 should be on top of b2. b3 should be on top of b5. b4 should be on top of b1.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>GPT-4:</p>
<ol>
<li>Move b5 from b3 to the table.</li>
<li>Move b3 from b4 to b5.</li>
<li>Move b4 from b2 to the table.</li>
<li>Move b2 from b1 to b5.</li>
<li>Move b1 from the table to b2.</li>
<li>Move b4 from the table to b1.</li>
</ol>
<p>Does that mean that we should increase efforts to include all arithmetic and planning problems in their training corpus? Clearly, that is a fool's errand. On the other hand, why should it be necessary? We already have calculators and generalpurpose symbolic planners that are guaranteed to produce correct answers. Thus a natural alternative approach, and one that we are admittedly not the first to explore, is to connect LLMs to such tools.</p>
<p>With this motivation in mind, the objective of the research reported in this paper is, for the first time, to enable LLMs to solve planning problems correctly. We aim to do so without altering the LLMs themselves, even with finetuning [5], [6]. Rather, we introduce a methodology, called LLM+P by which, when posed a natural language description of a planning problem, the LLM:</p>
<p>1) outputs a problem description suitable as input to a</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 1: LLM+P makes use of a large language model (LLM) to produce the PDDL description of the given problem, then leverages a classical planner for finding an <em>optimal</em> plan, then translates the raw plan back to natural language using the LLM again.</p>
<p>general-purpose planner;</p>
<p>2) solves the problem using the general-purpose planner; and</p>
<p>3) converts the output of the planner back to natural language (or connects to action executors of a robot).</p>
<p>Our extensive empirical evaluations indicate that LLM+P is able to generate correct solutions to many more planning problems than are LLMs on their own. While demonstrated in this paper on planning problems, this general methodology can be applied to any class of problems for which we have a sound and complete solver, such as arithmetic problems (by leveraging calculators).</p>
<p><strong>Limitation:</strong> In this paper, we do not ask the LLM to <em>recognize</em> that it has been posed a prompt that is suitable for processing using the proposed LLM+P pipeline. A valuable future research direction will be to consider recognizing when a prompt should be processed by LLM+P.</p>
<h2>II. BACKGROUND</h2>
<p>This section introduces the notation we use for representing a planning problem to be solved by LLMs, and recaps the standard representation of classical planners.</p>
<h3>A. The Classical Planning Problem</h3>
<p>Formally, the input of a planning problem <em>P</em> is defined by a tuple $\langle\mathscr{S}, s^{init},\mathscr{S}^{G},\mathscr{A}, f\rangle$:</p>
<ul>
<li>$\mathscr{S}$ is a finite and discrete set of states used to describe the world's state (i.e., state space). We assume a factored state space such that each state $s \in \mathscr{S}$ is defined by the values of a fixed set of variables.</li>
<li>$s^{init} \in \mathscr{S}$ is an initial world state.</li>
<li>$\mathscr{S}^{G} \subset \mathscr{S}$ is a set of goal states. $\mathscr{S}^{G}$ are usually specified as a list of <em>goal conditions</em>, all of which must hold in a goal state.</li>
<li>$\mathscr{A}$ is a set of symbolic actions.</li>
<li>$f$ is the underlying state transition function. $f$ takes the current state and an action as input and outputs the corresponding next state.</li>
</ul>
<p>A solution to a planning problem <em>P</em> is a symbolic plan $\pi$ in the form of $\langle a_1, a_2, \ldots, a_N \rangle$, such that the preconditions of $a_1$ hold in $s^{init}$, the preconditions of $a_2$ hold in the state that results from applying $a_1$, and so on, with the goal conditions all holding in the state that results after applying $a_N$.</p>
<h3>B. Planning Domain Definition Language (PDDL)</h3>
<p>The planning domain definition language (PDDL) serves as a standardized encoding of classical planning problems [7], [8]. The PDDL representation of a planning problem <em>P</em> is separated into two files: a domain file and a problem file. The domain PDDL file provides a lifted representation of the underlying rules of the world. It includes a set of predicates that define the state space $\mathscr{S}$ and the actions (i.e., $\mathscr{A}$) with their preconditions and effects (i.e., the transition function $f$). The problem PDDL file provides a list of objects to ground the domain, the problem's initial state $s^{init}$ and goal conditions $\mathscr{S}^{G}$. There exists a rich set of symbolic planners that implement efficient search algorithms to solve planning problems formalized in PDDL. In this work, we aim to take a natural language prompt which describes the initial state $s^{init}$ and goal conditions $\mathscr{S}^{G}$, formulate it in PDDL,</p>
<p>and leverage symbolic planners to output correct plans. We assume the domain rules are available (See the assumptions in Section III).</p>
<h2>III. Method</h2>
<p>This section first uses a running example to investigate ways for formulating planning prompts in PDDL and then introduces the LLM+P method.</p>
<h2>A. Large Language Model as a PDDL Writer</h2>
<p>LLMs are bad at planning (or long-horizon reasoning) [9] but they are good at describing and translating textual inputs, including re-writing planning prompts in the PDDL format. The intuition is that we can view PDDL as a different language than English, so re-writing planning prompts in PDDL is essentially a "machine translation" task that LLMs are known to excel at. Here we provide an example of a PDDL problem file written by GPT-4 without any promptengineering. As we see, the generated file appears to have the correct PDDL syntax but uses a made-up predicate (empty) and misses the initial condition that b1 is on the table.</p>
<h2>An Example PDDL Problem File Written by GPT-4</h2>
<p>Prompt: Description of Problem (P1) + Provide me with the problem PDDL file that describes the planning problem directly without further explanations.</p>
<h2>GPT-4 (the generated problem PDDL):</h2>
<p>(:objects b1 b2 b3 b4 b5 - block)
(:init (on b5 b3) (on b4 b2) (on b2
b1) (on b3 b4) (clear b5) (empty))
(:goal (and (on b1 b2) (on b3 b5)
(on b4 b1)))</p>
<h2>B. In-Context Learning</h2>
<p>LLMs are known to be capable of in-context learning without finetuning their parameters. By in-context learning, we mean LLMs' ability to perform unseen downstream tasks by simply conditioning on a few input-label pairs (demonstrations) [10]. Here is a pair of problem descriptions in natural language and in PDDL provided to GPT-4 as context. When the context is included with the prompt from the example above, the resulting PDDL problem file is directly solvable by the planner.</p>
<h2>An Example PDDL Problem File Written by GPT-4 with Context</h2>
<h2>Context (shaded):</h2>
<p>An example planning problem is: You have 5 blocks. b2 is on top of b5. b5 is on top of b1. b1 is on top of b4. b3 is on top of b2. b4 is on the table. b3 is clear. Your arm is empty. Your goal is to move the blocks. b4 should be on top of b3. The problem PDDL is:
(:objects b1 b2 b3 b4 b5)
(:init (arm-empty) (on b1 b4) (on
b2 b5) (on b3 b2)
(on-table b4) (on b5 b1)
(clear b3))
(:goal (and (on b4 b3))))
Prompt: Description of Problem (P1) + Context + Provide me with the problem PDDL file that describes the planning problem directly without further explanations.</p>
<h2>GPT-4 (the generated problem PDDL):</h2>
<p>(:objects b1 b2 b3 b4 b5 )
(:init (arm-empty) (on-table b1)
(on b2 b1) (on b3 b4)
(on b4 b2) (on b5 b3) (clear
b5)))
(:goal (and (on b1 b2) (on b3 b5)
(on b4 b1)))</p>
<h2>Planner:</h2>
<p>(unstack b5 b3) (putdown b5)
(unstack b3 b4) (stack b3 b5)
(unstack b4 b2) (putdown b4)
(unstack b2 b1) (putdown b2)
(pickup b1) (stack b1 b2) (pickup
b4) (stack b4 b1)</p>
<h2>C. Large Language Model + Classical Planner (LLM+P)</h2>
<p>Having introduced the LLM's ability to encode problems in PDDL and in-context learning, we are ready to introduce the proposed LLM+P solution (the bottom of Fig. 1). In particular, we assume that for each problem domain, a human expert can provide a domain description (i.e. action preconditions and effects) that will be fixed for all problem instances that happen in that domain. While the problem of automatically generating the description is another valuable research question, in this proposed work, we assume that the description is available as a PDDL domain file. The LLM+P method is directly applicable as a natural language interface for giving tasks to robot systems. For instance, assume we want a robot to act as a bartender to make cocktails. It is reasonable to tell it what actions it can take, but leave itself to infer how to make new cocktails most efficiently given a set of ingredients to combine. Moreover, we assume the agent is provided with a minimal example that demonstrates what an example problem PDDL looks like for a simple</p>
<p>problem inside that domain. Next, the agent is provided with a new (potentially quite complicated) problem $(P)$. The LLM then uses the in-context learning to infer the problem PDDL file corresponding to $P$. Once the problem PDDL file is generated, we feed it into any classical planner, together with the provided domain PDDL file, to generate a PDDL plan [11]. In the end, the LLM translates the PDDL plan back into the natural language to finish up the LLM+P pipeline. To summarize, the assumptions we need for LLM+P are:</p>
<ol>
<li>A robot knows when to trigger LLM+P based on its conversation with a human user.</li>
<li>A domain PDDL is provided to define the actions that the robot is capable of. This specification is taskagnostic - the entities relevant to the task are specified in the LLM-generated problem PDDL.</li>
<li>A simple problem description in natural language and its corresponding problem PDDL file are also provided.</li>
</ol>
<h2>IV. Related Work</h2>
<p>This section first provides a brief overview of classical planning algorithms. Then it summarizes recent advances in using large language models for planning tasks. It concludes with a discussion of recent research on augmenting LLMs with external modules.</p>
<h2>A. Classical Planning</h2>
<p>Automated planning (or classical planning) techniques can be used for computing a sequence of actions that achieves a given goal [12], [13], [14]. Automated planning algorithms have been widely used in robot systems. Shakey is the first robot that was equipped with a planning component, which was constructed using STRIPS [15]. Some previous generalpurpose planning architectures were also demonstrated to be useful for robot planning, such as PRODIGY [16] and HTN [17]. Recent classical planning systems designed for robotics frequently use planning domain description language (PDDL) or answer set programming (ASP) as the underlying action language for the planners [18], [19], [20], [21]. For example, researchers have used classical planning algorithms for sequencing actions for a mobile robot working on delivery tasks [22], reasoning about safe and efficient urban driving behaviors for autonomous vehicles [23], and planning actions for a team of mobile robots [24]. Task and motion planning (TAMP) is a hierarchical planning framework that combines classical planning in discrete spaces and robot motion planning in continuous space [25], [26].</p>
<p>Most of the above-mentioned planning methods require domain-specific programming languages as the underlying representation of the problems and their solutions. LLM+P, on the other hand, takes advantage of LLMs and serves as a natural language interface for robots to solve complex planning tasks. The main feature that motivates us to use such classical planning systems is that most of these planners are sound and complete, meaning that they are guaranteed to be logically correct and will output a plan if one exists. Many are also able to find optimal (shortest) plans, at least if given sufficient time.</p>
<h2>B. Planning with Large Language Models</h2>
<p>Various large language models (LLMs) have been developed in recent years, such as Bert [27], CodeX [28], Opt [29], GPT-3 [10], ChatGPT [30], GPT-4 [2], Llama [31], Llama2 [32], and PaLM [33]. As LLMs are pretrained with a tremendous amount of offline text data, they can emerge with surprising zero-shot generalization ability, which can be leveraged for robot planning tasks [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45]. Several recent methods had successes in extracting task knowledge from LLMs to decompose commands or instructions for robots in natural language. For instance, the work of Huang et al. showed that LLMs can be used for task planning in household domains by iteratively augmenting prompts [38]. SayCan is another approach that enabled robot planning with affordance functions to account for action feasibility, where the service requests are specified in natural language [34]. Vemprala et al. recently studied how ChatGPT can be applied to generalized robotics domains [3].</p>
<p>However, a major drawback of existing LLMs is their lack of long-horizon reasoning ability for complex tasks (See [9], [46] and Section 8.2 from [2]). Specifically, the output they produce when presented with such a task is often incorrect in the sense that following the output plan will not actually solve the task. Therefore, in this work, we focus on resolving this issue by leveraging the properties of classical planners. Similarly, some recent work also investigates approaches for combining classical planning with LLMs [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57]. They either use prompting or fine-tuning to make LLMs capable of solving PDDL planning problems. Improvements to longhorizon planning capabilities have also been made by iteratively querying LLMs, as demonstrated in Minecraft [58]. In contrast, we do not solely rely on LLM as the problem solver, but are more into taking the advantage of both the planner (i.e., generating accurate and optimal plans) and the LLM itself (i.e., 1-shot generalization for translating naturallanguage problem descriptions into PDDL).</p>
<h2>C. Augmenting LLMs with External Modules</h2>
<p>Recently developed methods have shown that the performance of downstream tasks of LLMs can be improved by combining them with external modules. For instance, WebGPT [59] is a fine-tuned version of GPT-3 by combining web knowledge to answer open-ended questions. Lazaridou et al. studied how search engines like Google can be utilized as external tools for LLMs [60]. MemPrompt [61] presented a human-in-the-loop system where a growing memory of errors and user feedback is served as past experience adding to the prompts for more accurately answering new questions. REPLUG [62] is another retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. Specifically, people have investigated using calculators for computation [63], [64]. In very recent work related to ours, Schick et al. trained a model called ToolFormer that can decide when and how to call certain tool APIs by in-line</p>
<p>augmentation on prompts for LLMs [65]. In this work, we propose that classical planners can be another particularly useful external module. In comparison, LLM+P, does not rely on any fine-tuning or re-training of LLMs. By simply incorporating knowledge from classical planners, LLM+P incorporates long-horizon reasoning and planning capabilities into existing LLMs.</p>
<p>The authors are informed that a concurrent work [66] presents preliminary results of integrating LLMs with PDDL using the SayCan dataset [34]. However, the SayCan dataset has a limited scope, as it contains only three predefined actions. Consequently, all model variants evaluated in the original paper achieved a success rate of approximately $90 \%$. Due to the homogeneity of the SayCan dataset, Lyu et al. did not necessitate a rigorous definition of the domain PDDL, which can lead to infeasible plans. As a result, we consider our LLM+P method as a more comprehensive investigation into enhancing LLMs with optimal planning proficiency.</p>
<h2>V. EXPERIMENTS</h2>
<p>We conduct experiments to answer these questions:</p>
<p>1) How well does LLM-AS-P work? To what extent can state-of-the-art LLMs and LLM-based reasoning methods be directly used for planning? (Not at all)
2) How well does LLM+P work compare to LLM-AS-P? (Much better)
3) What role does the context play in the success of LLM+P? (It's crucial)
4) Can LLM+P help make service robots more efficient on realistic tasks? (Yes)</p>
<h2>A. Benchmark Problems</h2>
<p>We present seven robot planning domains borrowed from past International Planning Competitions and 20 automatically generated tasks for each domain [67]. Below is a list of the planning domains, along with a brief summary of each.</p>
<p>1) BlockSWORLD: Given a set of piles of blocks on a table, a robot is tasked with rearranging them into a specified target configuration.
2) BARMAN: A robot bartender is tasked with creating cocktails for a customer's order, utilizing the available ingredients and containers.
3) Floortile: A set of robots are tasked to use paint color patterns on floor tiles. Robots can move around and change colors but cannot step on painted tiles.
4) GRIPPERS: A set of robots with two grippers is given a task to move objects among different rooms.
5) Storage: Given a set of hoists, the goal is to lift and drop crates using the hoists into a depot. Crates are initially stored in different areas and hoists can be moved among storage areas.
6) TERMES: A robot is tasked to build complex structures by carrying and placing blocks, and also climbing on them so that it can build towers.
7) TyReWorld: The robot is given a task to replace flat tires by, for example, inflating tires, tightening nuts,
and moving tools back to the boot when done, all in the proper order.
For each problem $P, P$ comes with a natural language description and a ground-truth problem PDDL file. Each domain also includes an example problem description, a corresponding PDDL file, and a plan description, used as context in various approaches. We assume each problem domain has its own domain PDDL file given by the user or a domain expert prior to addressing any planning problems in that domain. This dataset is made publicly available in our codebase for reproducibility.</p>
<h2>B. Experiment Setup</h2>
<p>We leverage the GPT-4 model provided by OpenAI ${ }^{2}$ for all experiments. We set the temperature to 0 , and use the top probability response. As a result, the response returned from the LLM is deterministic. Once a text PDDL response is generated, we feed it into the FAST-DOWNWARD planner ${ }^{3}$ and try both aliases SEQ-OPT-FDSS-1 (guaranteed optimal) and LAMA (not guaranteed optimal) with a maximum search time of 200 seconds. We report the success rate of the optimal alias, and for the domains that time out, we show the success rate of the sub-optimal alias in parentheses. For the baseline methods, we manually count the number of optimal plans, and report the number of correct plans in parentheses (if there are any sub-optimal plans).</p>
<p>We also evaluate a recent LLM-based approach for deliberate reasoning called Tree of Thoughts [68], referred to as LLM-AS-P (ToT). We adapt the breadth-first-search algorithm from the original ToT implementation ${ }^{4}$ for planning. The LLM is prompted to expand the search tree from allowed actions and evaluate the paths on their likelihood of reaching the goal. The same time limit of 200 seconds is applied.</p>
<h2>C. Results and Analysis</h2>
<p>The results of applying LLM-AS-P and LLM+P across 7 domains are provided in Table I.</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Success Rate \%</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>LLM $^{-}$</td>
<td>LLM</td>
<td>LLM $^{T o T}$</td>
<td>LLM+P $^{-}$</td>
<td>LLM+P</td>
</tr>
<tr>
<td>BARMAN</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>20 (100)</td>
</tr>
<tr>
<td>BlockSWORLD</td>
<td>20</td>
<td>15 (30)</td>
<td>0 (5)</td>
<td>0</td>
<td>90</td>
</tr>
<tr>
<td>Floortile</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>GRIPPERS</td>
<td>25 (60)</td>
<td>35 (50)</td>
<td>10 (20)</td>
<td>0</td>
<td>95 (100)</td>
</tr>
<tr>
<td>Storage</td>
<td>0</td>
<td>0 (25)</td>
<td>0</td>
<td>0</td>
<td>85</td>
</tr>
<tr>
<td>TERMES</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>20</td>
</tr>
<tr>
<td>TyReWorld</td>
<td>5</td>
<td>15</td>
<td>0</td>
<td>0</td>
<td>10 (90)</td>
</tr>
</tbody>
</table>
<p>TABLE I: Success rate \% of applying LLM-AS-P without context (LLM ${ }^{-}$), LLM-AS-P (LLM), Tree of Thoughts (LLM ${ }^{T o T}$ ), LLM+P without context (LLM ${ }^{-}$), and LLM+P.</p>
<p>Findings (LLM-AS-P):</p>
<p>1) We observe that though LLM-AS-P provides a plan in natural language for every problem, most of these</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{2}$ We use the most recent model as of September 2023. https:// platform.openai.com/docs/models/gpt-4 ${ }^{3}$ https://github.com/aibasel/downward/tree/ release-22.12.0 ${ }^{4}$ https://github.com/princeton-nlp/ tree-of-thought-1lm/</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 2: Demonstration of the optimal tidy-up plan. The robot starts at the coffee table and 1) picks up the bottle, 2) navigates to a room with the side table and the recycle bin, 3) puts down the bottle, 4) grasps the soup can, 5) puts the soup can in the recycle bin, 6) re-grasps the bottle, 7) navigates to the kitchen, 8) places the bottle in the pantry.</p>
<p>plans are not feasible. The main reason is that LLM-AS-P lacks the ability to reason about preconditions.</p>
<p>2) In most cases, LLM-AS-P fails in the same way with or without the example plan as context. In particular, in the BlockSwordD domain, LLM-AS-P cannot keep track of properties like ON and Clear. In the BARMAN domain, LLM-AS-P's plans fail to clean shot glasses before using them again.</p>
<p>3) The hardest domains are the ones with complex spatial relationship. The LLM-AS-P methods (with or without context) completely fail at this type of problems. In the FLOORTILE domain, LLM-AS-P generates "move right to tile.0-4 and paint tile.1-2 black" but the robot can only paint neighboring tiles. In TERMES and STORAGE, LLM-AS-P ignores the requirement that the robot cannot unload the block/crate at the same position it occupies.</p>
<p>4) LLM-AS-P (ToT) calls the LLM at each tree node to provide a list of available actions, and then calls the LLM to evaluate each new path on the tree as a partial plan. We find that the LLM is able to give reasonable rankings on the partial plans, but it often fails to recognize whether the plan reaches the goal. LLM-AS-P (ToT) times out in most cases due to the large number of LLM calls, so it is not suitable for solving long-horizon problems.</p>
<h3>Findings (LLM+P):</h3>
<p>1) The proposed LLM+P produces an optimal plan for the majority of problems. Most failed cases are due to mis-specified problem files, such as missing one of the initial conditions (e.g. leaving the tiles disconnected in FLOORTILE), causing the planning problem to be unsolvable.</p>
<p>2) Without the context (i.e., an example problem and its corresponding problem PDDL), we observe that LLMs fail to produce correct problem PDDL files. Therefore, the context is important for LLM+P to work.</p>
<h3><em>D. Robot Demonstration</em></h3>
<p>We verify that LLM+P can efficiently solve realistic service robot problems by deploying it on a real robot tasked with tidying up a home. The user asks the robot to move a mustard bottle from the coffee table to the pantry, and throws away the empty soup can from the side table. Since the side table and the recycle bin are on the way from the coffee table to the pantry, the optimal plan is to take the mustard bottle to the side table, and re-grasps it after throwing away the soup can, with a total cost of 22. Fig. 2 shows the optimal plan found by LLM+P. Parts of the prompt and the generated PDDL are shown below. LLM-AS-P outputs a sub-optimal plan which takes the bottle to the pantry first and travels back for the soup can, with a total cost of 31.</p>
<h3><strong>Tidy-Up Problem PDDL Generated by LLM+P</strong></h3>
<p><strong>Problem (P):</strong> You are a home robot with one gripper. The distance between coffee table and side table is 10. The distance between coffee table and pantry is 20... You are at the coffee table. There is a mustard bottle... Your goal is to move objects to their destinations...</p>
<h3><strong>Problem PDDL generated by LLM+P:</strong></h3>
<div class="codehilite"><pre><span></span><code>(:objects coffee-table side-table
recycle-bin pantry - location
mustard-bottle soup-can - object)
(:init (= (total-cost) 0) (= (distance coffee-table side-table)
10) (= (distance coffee-table
pantry) 20) ... (robot-at
coffee-table) (at mustard-bottle
coffee-table) (at soup-can
side-table) (hand-empty) )
(:goal (and (at mustard-bottle
pantry) (at soup-can recycle-bin)))
(:metric minimize (total-cost)) )
</code></pre></div>

<h3>VI. CONCLUSION AND FUTURE WORK</h3>
<p>In this work, we propose to leverage classical planners to empower large language models with optimal planning capabilities. The key design choice of the proposed LLM+P framework is to focus LLMs on translating the planning problem from natural language to structured PDDL format. Moreover, we show that it is important to also make LLMs aware of a simple (problem, PDDL) pair as a demonstration (or the context) for in-context learning. Some interesting directions to further extend the LLM+P framework include: 1) enabling the LLM to auto-detect when and how to apply LLM+P; and 2) reducing LLM+P's dependency on information by humans, potentially involving finetuning.</p>
<h2>REFERENCES</h2>
<p>[1] J. Weizenbaum, "Eliza-a computer program for the study of natural language communication between man and machine," Communications of the ACM, vol. 9, no. 1, pp. 36-45, 1966.
[2] OpenAI, "Opt-4 technical report," 2023.
[3] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, "Chatgpt for robotics: Design principles and model abilities," Microsoft, Tech. Rep. MSR-TR-2023-8, February 2023. [Online]. Available: https://www.microsoft.com/en-us/research/publication/ chatgpt-for-robotics-design-principles-and-model-abilities/
[4] K. Mahowald, A. A. Ivanova, I. A. Blank, N. Kanwisher, J. B. Tenenbaum, and E. Fedorenko, "Dissociating language and thought in large language models: a cognitive perspective," arXiv preprint arXiv:2301.06627, 2023.
[5] C. Lee, K. Cho, and W. Kang, "Mixout: Effective regularization to finetune large-scale pretrained language models," arXiv preprint arXiv:1909.11299, 2019.
[6] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," arXiv preprint arXiv:2109.01652, 2021.
[7] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram, M. Veloso, D. Weld, and D. Wilkins, "Pddl-the planning domain definition language," 1998.
[8] P. Haslum, N. Lipovetzky, D. Magazzeni, and C. Muise, "An introduction to the planning domain definition language," Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 13, no. 2, pp. $1-187,2019$.
[9] K. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati, "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)," arXiv preprint arXiv:2206.10498, 2022.
[10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[11] M. Helmert, "The fast downward planning system," Journal of Artificial Intelligence Research, vol. 26, pp. 191-246, 2006.
[12] T. Bylander, "The computational complexity of propositional STRIPS planning," Artificial Intelligence, vol. 69, no. 1-2, pp. 165-204, 1994.
[13] J. McCarthy, "Situations, actions, and causal laws," Stanford University Technical Report, Tech. Rep., 1963.
[14] R. E. Fikes and N. J. Nilsson, "Strips: A new approach to the application of theorem proving to problem solving," Artificial intelligence, vol. 2, no. 3-4, pp. 189-208, 1971.
[15] N. J. Nilsson et al., "Shakey the robot," 1984.
[16] J. Carbonell, O. Etzioni, Y. Gil, R. Joseph, C. Knoblock, S. Minton, and M. Veloso, "Prodigy: An integrated architecture for planning and learning," ACM SIGART Bulletin, vol. 2, no. 4, pp. 51-55, 1991.
[17] D. S. Nau, T.-C. Au, O. Ilghami, U. Kuter, J. W. Murdock, D. Wu, and F. Taman, "Shop2: An htn planning system," Journal of artificial intelligence research, 2003.
[18] Y.-q. Jiang, S.-q. Zhang, P. Khandelwal, and P. Stone, "Task planning in robotics: an empirical comparison of pddl-and asp-based systems," Frontiers of Information Technology \&amp; Electronic Engineering, vol. 20, pp. 363-373, 2019.
[19] G. Brewka, T. Eiter, and M. Truszczyński, "Answer set programming at a glance," Communications of the ACM, vol. 54, no. 12, pp. 92-103, 2011.
[20] V. Lifschitz, "Answer set programming and plan generation," Artificial Intelligence, vol. 138, no. 1-2, pp. 39-54, 2002.
[21] M. Fox and D. Long, "Pddl2. 1: An extension to pddl for expressing temporal planning domains," Journal of artificial intelligence research, vol. 20, pp. 61-124, 2003.
[22] S. Zhang, F. Yang, P. Khandelwal, and P. Stone, "Mobile robot planning using action language bc with an abstraction hierarchy," in International Conference on Logic Programming and Nonmonotonic Reasoning. Springer, 2015, pp. 502-516.
[23] Y. Ding, X. Zhang, X. Zhan, and S. Zhang, "Task-motion planning for safe and efficient urban driving," in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.
[24] Y. Jiang, H. Yedidsion, S. Zhang, G. Sharon, and P. Stone, "Multi-robot planning with conflicts and synergies," Autonomous Robots, vol. 43, no. 8, pp. 2011-2032, 2019.
[25] F. Lagriffoul, N. T. Dantam, C. Garrett, A. Akbari, S. Srivastava, and L. E. Kavraki, "Platform-independent benchmarks for task and motion planning," IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3765-3772, 2018.
[26] L. P. Kaelbling and T. Lozano-Pérez, "Integrated task and motion planning in belief space," The International Journal of Robotics Research, vol. 32, no. 9-10, pp. 1194-1227, 2013.
[27] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[28] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al., "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021.
[29] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al., "Opt: Open pre-trained transformer language models," arXiv preprint arXiv:2205.01068, 2022.
[30] OpenAI, "Chatgpt," Accessed: 2023-02-08, 2023, cit. on pp. 1, 16. [Online]. Available: https://openai.com/blog/chatgpt/
[31] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.
[32] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.
[33] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[34] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al., "Do as i can, not as i say: Grounding language in robotic affordances," arXiv preprint arXiv:2204.01691, 2022.
[35] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, "Task and motion planning with large language models for object rearrangement," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023.
[36] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., "Palm-e: An embodied multimodal language model," arXiv preprint arXiv:2303.03378, 2023.
[37] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al., "Inner monologue: Embodied reasoning through planning with language models," arXiv preprint arXiv:2207.05608, 2022.
[38] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," in International Conference on Machine Learning. PMLR, 2022, pp. 9118-9147.
[39] Y. Kant, A. Ramachandran, S. Yenamandra, I. Gilitschenski, D. Batra, A. Szot, and H. Agrawal, "Housekeep: Tidying virtual households using commonsense reasoning," in Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIX. Springer, 2022, pp. 355-373.
[40] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, "Progprompt: Generating situated robot task plans using large language models," arXiv preprint arXiv:2209.11302, 2022.
[41] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg, "Text2motion: From natural language instructions to feasible plans," arXiv preprint arXiv:2303.12353, 2023.
[42] Y. Yang, J.-R. Gaglione, C. Neary, and U. Topcu, "Automaton-based representations of task knowledge from generative language models," arXiv preprint arXiv:2212.01944, 2023.
[43] Y. Ding, X. Zhang, S. Amiri, N. Cao, H. Yang, A. Kaminski, C. Esselink, and S. Zhang, "Integrating action knowledge and llms for task planning and situation handling in open worlds," arXiv preprint arXiv:2305.17590, 2023.
[44] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, J. Varley, et al., "Robots that ask for help: Uncertainty alignment for large language model planners," arXiv preprint arXiv:2307.01928, 2023.
[45] Y. Chen, J. Arkin, Y. Zhang, N. Roy, and C. Fan, "Autotamp:</p>
<p>Autoregressive task and motion planning with llms as translators and checkers," arXiv preprint arXiv:2306.06531, 2023.
[46] K. Valmeekam, S. Sreedharan, M. Marquez, A. Olmo, and S. Kambhampati, "On the planning abilities of large language models (a critical investigation with a proposed benchmark)," arXiv preprint arXiv:2302.06706, 2023.
[47] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. LozanoPérez, and L. P. Kaelbling, "PDDL planning with pretrained large language models," in NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. [Online]. Available: https: //openreview.net/forum?id=1QMMUB4zfl
[48] V. Pallagani, B. Muppasani, K. Murugesan, F. Rossi, L. Horesh, B. Srivastava, F. Fabiano, and A. Loreggia, "Plansformer: Generating symbolic plans using transformers," arXiv preprint arXiv:2212.08681, 2022.
[49] D. Arora and S. Kambhampati, "Learning and leveraging verifiers to improve planning capabilities of pre-trained language models," arXiv preprint arXiv:2305.17077, 2023.
[50] L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati, "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning," arXiv preprint arXiv:2305.14909, 2023.
[51] T. Silver, S. Dan, K. Srinivas, J. B. Tenenbaum, L. P. Kaelbling, and M. Katz, "Generalized planning in pddl domains with pretrained large language models," arXiv preprint arXiv:2305.11014, 2023.
[52] V. Pallagani, B. Muppasani, K. Murugesan, F. Rossi, B. Srivastava, L. Horesh, F. Fabiano, and A. Loreggia, "Understanding the capabilities of large language models for automated planning," arXiv preprint arXiv:2305.16151, 2023.
[53] K. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati, "On the planning abilities of large language models-a critical investigation," arXiv preprint arXiv:2305.15771, 2023.
[54] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, "Translating natural language to planning goals with large-language models," arXiv preprint arXiv:2302.05128, 2023.
[55] R. Hazra, P. Z. D. Martires, and L. De Raedt, "Saycanpay: Heuristic planning with large language models using learnable domain knowledge," arXiv preprint arXiv:2308.12682, 2023.
[56] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf, "Sayplan: Grounding large language models using 3d scene graphs for scalable task planning," arXiv preprint arXiv:2307.06135, 2023.
[57] Z. Zhou, J. Song, K. Yao, Z. Shu, and L. Ma, "Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning," arXiv preprint arXiv:2308.13724, 2023.
[58] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents," arXiv preprint arXiv:2302.01560, 2023.
[59] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., "Webgpt: Browserassisted question-answering with human feedback," arXiv preprint arXiv:2112.09332, 2021.
[60] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev, "Internet-augmented language models through few-shot prompting for open-domain question answering," arXiv preprint arXiv:2203.05115, 2022.
[61] A. Madaan, N. Tandon, P. Clark, and Y. Yang, "Memory-assisted prompt editing to improve gpt-3 after deployment," 2023.
[62] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih, "Replug: Retrieval-augmented black-box language models," arXiv preprint arXiv:2301.12652, 2023.
[63] W. Chen, X. Ma, X. Wang, and W. W. Cohen, "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks," arXiv preprint arXiv:2211.12588, 2022.
[64] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, "Pal: Program-aided language models," arXiv preprint arXiv:2211.10435, 2022.
[65] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," arXiv preprint arXiv:2302.04761, 2023.
[66] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apidianaki, and C. Callison-Burch, "Faithful chain-of-thought reasoning," arXiv preprint arXiv:2301.13379, 2023.
[67] J. Seipp, Á. Torralba, and J. Hoffmann, "PDDL generators," https: //doi.org/10.5281/zenodo.6382173, 2022.
[68] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," arXiv preprint arXiv:2305.10601, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.
${ }^{\dagger}$ Department of Computer Science, The University of Texas at Austin {bliu, lqiang, joydeep, pstone}@cs.utexas.edu, jiangyuqian@utexas.edu
${ }^{\ddagger}$ Department of Computer Science, State University of New York at Binghamton {xzhan244, zhangs}@binghamton.edu
${ }^{\S}$ Sony AI
${ }^{\dagger}$ The code and results are publicly available at https://github. com/Cranial-XIX/llm-pddl.git.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
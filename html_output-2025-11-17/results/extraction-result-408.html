<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-408 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-408</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-408</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-29288308</p>
                <p><strong>Paper Title:</strong> A Computational Cognition and Visual Servoing based Methodology to Design Automatic Manipulative Tasks</p>
                <p><strong>Paper Abstract:</strong> In the last decades, robotics has exerted an important role in the research on diverse knowledge domains, such as, artificial intelligence, biology, neuroscience and psychology. In particular, the study of knowledge representation and thinking, has led to the proposal of cognitive architectures; capturing essential structures and processes of cognition and behavior. Robotists have also attempted to design automatic systems using these proposals. Though, certain difficulties have been reported for obtaining efficient low-level processing while sensing or controlling the robot. The main challenges involve the treatment of the differences between the computational paradigms employed by the cognitive and the robotic architectures. The objective of this work, is to propose a methodology for designing robotic systems capable of decision making and learning when executing manipulative tasks. The development of a system called the Cognitive Reaching Robot (CRR) will be reported. CRR combines the advantages of using a psychologically-oriented cognitive architecture, with efficient low-level behavior implementations through the visual servoing control technique.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e408.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e408.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognitive Reaching Robot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular hybrid robotic system that combines a psychologically-oriented symbolic cognitive architecture (Soar) for high-level decision making and reinforcement learning with an imperative visual-servoing based visuomotor module for low-level control, integrated via ROS topics to perform reaching, grasping and releasing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cognitive Reaching Robot (CRR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CRR is a modular hybrid system composed of three modules (cognitive, auditory, visuomotor) connected via ROS topics. The cognitive module is implemented in the Soar cognitive architecture and implements symbolic representations (working memory, semantic memory, episodic memory) and production rules that implement an MDP and Q-learning. The visuomotor module implements image-based visual servoing (IBVS) using ViSP and low-level joint-space control (including null-space projection for secondary tasks such as joint-limit avoidance). An auditory module handles speech recognition. Continuous sensory data (vision, proprioception, audio) are converted into symbolic/semantic structures (e.g., object centroids, 2D features, stimulus valences) that the cognitive module uses to select high-level actions; those actions trigger procedural visuomotor behaviors executed by the visuomotor controller. The architecture emphasizes a clear division of labor: symbolic deliberation and episodic/semantic memory for high-level decisions and learning, and efficient procedural control for sensorimotor execution.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Soar cognitive architecture (Soar 9.3.2) providing symbolic representations: working memory (WM), semantic memory (smem), episodic memory (epmem), and production-rule procedural memory (57 productions defined for this task). The MDP and Q-values are represented and updated within Soar's symbolic structures.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Imperative procedural components include: image-based visual servoing (IBVS) implemented with ViSP (interaction matrices, Moore–Penrose pseudoinverse control law), joint-space control with null-space projection for secondary tasks, classical perception pipelines (OpenCV HSV thresholding and image moments for centroid extraction), and a Q-learning reinforcement learning algorithm (embedded within Soar's productions as procedural updates). No neural-network components are used; imperative elements are algorithmic controllers and RL.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular message-passing integration over ROS topics: sensory topics (visual, auditory, proprioceptive) publish processed sensory features to the cognitive module; the cognitive Soar module queries semantic/episodic memory and production rules to select high-level actions (MDP actions), which are sent as command topics to the visuomotor and other modules. Integration relies on explicit representation conversions (feature extraction: binarization, image moments → symbolic feature tokens) and a division-of-labor (symbolic high-level planning + RL inside Soar; imperative low-level servoing). There is no end-to-end differentiable coupling; integration is via interfaces and symbolic/feature translations and episodic/semantic queries.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combined capabilities observed include: adaptive decision-making across trials via RL (learning to follow or wait based on auditory cues), robustness to calibration/modeling errors because IBVS measures task error directly (visual feedback closes the loop), ability to remember and exploit episodic information (Soar episodic memory) to perform structured behaviors such as alternating release locations, and real-time operation of sensorimotor control (~66 Hz loop). The hybrid yields behaviors not present in either alone: symbolic planning/remembering to decide when to engage the visuomotor controller, and visuomotor robustness enabling the symbolic planner to rely on accurate execution despite modeling uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Interactive manipulative task: reaching, grasping and releasing with auditory cues ("wait" vs "go") modeled as an MDP with states S0..S10 and actions a0..a8; two experimental sets (ES1 learn when to reach vs sleep; ES2 learn to alternate drop locations). Evaluated with reinforcement learning trials and comparisons to a random policy baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Visuomotor control loop: ~66 Hz (≈15 ms per iteration) without GUI display (drops to ≈20 Hz when camera view is displayed). Learning performance (ES1): RL policy success rates reported around 0.85–0.90 (fraction of successful trials) versus random policy success rates around 0.40–0.60 (reported per-case success fractions). ES2 results: learned policies resulted in repeated selection of correct think+release actions; example Q-value-like rewards reported for actions (e.g., think-release-loc-2-B reward ≈ 6.9741, think-release-loc-1-A reward ≈ 6.9741).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Authors report improved adaptability to task variations and system changes (wear, calibration errors) because visual servoing provides closed-loop compensation and RL reduces the need for manual reprogramming. No quantitative out-of-distribution or compositional generalization experiments are reported; claims about generalization are qualitative (robustness to modeling/calibration errors, ability to adapt policies through reinforcement learning).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability at the symbolic level: decisions and learned Q-values are stored in Soar's symbolic structures (semantic/episodic memories and productions), enabling inspection and episodic retrieval (queries placed on epmem/smem attributes). Low-level imperative control (IBVS and control laws) is algorithmic and interpretable mathematically but not represented symbolically for explanation; overall the hybrid permits traceability of high-level choices and memory retrievals but not end-to-end symbolic explanations of low-level control dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limitations identified include: reliance on simple threshold-based vision (HSV binarization) which assumes constant illumination (sensitivity to lighting changes), potential difficulty in producing robust symbolic representations from noisy sensors, lack of end-to-end integrated learning between perception, decision, and control, limited scalability of hand-authored productions (57 productions used for the studied task), no quantitative comparisons to purely symbolic or purely procedural baselines, and reduced control frequency when debugging GUI is enabled (66 Hz → 20 Hz). Joint singularities and local minima can occur in the absence of secondary-task null-space control (mitigated here by threshold-activated secondary tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Explicit division-of-labor complementary framework: symbolic/psychologically-oriented cognitive architectures are used for high-level planning, memory and deliberative learning (MDP + RL), while imperative procedural controllers (visual servoing, joint-space control) provide efficient, robust low-level sensorimotor execution; the hybrid is motivated by combining complementary strengths (interpretability and structured memory from symbolic systems; robustness and efficiency from procedural controllers).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e408.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e408.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>hybrid CA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid Cognitive Architecture (symbolic + sub-symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of cognitive architectures that combine symbolic (declarative, rule-based, graph-like representations) and sub-symbolic (connectionist, procedural, low-level) paradigms to leverage strengths of both: symbolic systems for high-level reasoning and sub-symbolic for handling low-level uncertainty and perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid Cognitive Architecture (symbolic + sub-symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Conceptual hybrid CA described in the paper as combining symbolic cognitive architectures (graph-based semantic memory and production rules for planning and procedural knowledge) with sub-symbolic architectures (networks of processing nodes, connectionist memory organizations) to form systems capable of both high-level cognition and robust low-level processing. The paper frames the hybrid CA as a motivation for integrating Soar-like symbolic decision-making with visual servoing, rather than presenting a distinct fully-specified hybrid algorithmic template.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic cognitive architecture elements: semantic memory, episodic memory, working memory, rule-based productions, graph/tree-like knowledge representations (as exemplified by Soar in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Sub-symbolic/imperative elements described conceptually: networks of processing nodes (connectionist), localist/globalist memory organizations, and procedural sensorimotor controllers (the paper specifically uses visual servoing and algorithmic RL as concrete imperative examples).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Described at the conceptual level as a hybrid that uses division-of-labor: symbolic systems handle planning and higher-order tasks, sub-symbolic systems handle context-specific low-level processing and uncertainties. The paper advocates modular integration (message passing, software bus/topics) and feature extraction layers to translate continuous sensor data into symbols rather than end-to-end differentiable integration.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>At the conceptual level the hybrid is claimed to yield: better handling of high-level planning along with context-specific sensorimotor robustness, useful emergent behaviors like adaptation and generalization to noisy sensory inputs, and the ability to exhibit rich episodic/semantic memory-guided behavior. The paper notes emergent properties in sub-symbolic systems generally (e.g., generalization and interference) and argues hybrids can exploit these while retaining symbolic strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Conceptual/general — no single benchmark; paper applies the hybrid notion to a manipulative robotics task (reaching/grasping/releasing) implemented as the CRR case study.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper states sub-symbolic systems generalize well to novel stimuli but suffer interference when learning new items; hybridization aims to combine that generalization with symbolic strengths. No quantitative generalization benchmarks provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic components confer interpretability and the ability to query/inspect memories and productions; sub-symbolic components are characterized as less transparent. The hybrid is presented as a way to retain interpretability for high-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper highlights conceptual challenges: incompatibility of computational paradigms (symbolic vs sub-symbolic), difficulties in generating reliable symbols from noisy sensors, and architectural/software integration issues. No detailed failure-mode study is provided for the general hybrid concept.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Principle of complementary strengths / division-of-labor: symbolic architectures for structured, abstract cognition and explanations; sub-symbolic/procedural architectures for robust, context-dependent perception and motor control. Hybrid CAs are motivated by combining these to achieve both robust low-level performance and psychologically-plausible high-level cognition.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A cognitive robotic system based on the Soar cognitive architecture for mobile robot navigation, search, and mapping missions <em>(Rating: 2)</em></li>
                <li>Neurorobotics: From vision to action <em>(Rating: 2)</em></li>
                <li>Cognitive architectures: Where do we go from here? <em>(Rating: 2)</em></li>
                <li>A brief introduction to current software frameworks in cognitive robotics integrating different computational paradigms <em>(Rating: 2)</em></li>
                <li>Reinforcement learning: A survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-408",
    "paper_id": "paper-29288308",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "CRR",
            "name_full": "Cognitive Reaching Robot",
            "brief_description": "A modular hybrid robotic system that combines a psychologically-oriented symbolic cognitive architecture (Soar) for high-level decision making and reinforcement learning with an imperative visual-servoing based visuomotor module for low-level control, integrated via ROS topics to perform reaching, grasping and releasing tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Cognitive Reaching Robot (CRR)",
            "system_description": "CRR is a modular hybrid system composed of three modules (cognitive, auditory, visuomotor) connected via ROS topics. The cognitive module is implemented in the Soar cognitive architecture and implements symbolic representations (working memory, semantic memory, episodic memory) and production rules that implement an MDP and Q-learning. The visuomotor module implements image-based visual servoing (IBVS) using ViSP and low-level joint-space control (including null-space projection for secondary tasks such as joint-limit avoidance). An auditory module handles speech recognition. Continuous sensory data (vision, proprioception, audio) are converted into symbolic/semantic structures (e.g., object centroids, 2D features, stimulus valences) that the cognitive module uses to select high-level actions; those actions trigger procedural visuomotor behaviors executed by the visuomotor controller. The architecture emphasizes a clear division of labor: symbolic deliberation and episodic/semantic memory for high-level decisions and learning, and efficient procedural control for sensorimotor execution.",
            "declarative_component": "Soar cognitive architecture (Soar 9.3.2) providing symbolic representations: working memory (WM), semantic memory (smem), episodic memory (epmem), and production-rule procedural memory (57 productions defined for this task). The MDP and Q-values are represented and updated within Soar's symbolic structures.",
            "imperative_component": "Imperative procedural components include: image-based visual servoing (IBVS) implemented with ViSP (interaction matrices, Moore–Penrose pseudoinverse control law), joint-space control with null-space projection for secondary tasks, classical perception pipelines (OpenCV HSV thresholding and image moments for centroid extraction), and a Q-learning reinforcement learning algorithm (embedded within Soar's productions as procedural updates). No neural-network components are used; imperative elements are algorithmic controllers and RL.",
            "integration_method": "Modular message-passing integration over ROS topics: sensory topics (visual, auditory, proprioceptive) publish processed sensory features to the cognitive module; the cognitive Soar module queries semantic/episodic memory and production rules to select high-level actions (MDP actions), which are sent as command topics to the visuomotor and other modules. Integration relies on explicit representation conversions (feature extraction: binarization, image moments → symbolic feature tokens) and a division-of-labor (symbolic high-level planning + RL inside Soar; imperative low-level servoing). There is no end-to-end differentiable coupling; integration is via interfaces and symbolic/feature translations and episodic/semantic queries.",
            "emergent_properties": "Combined capabilities observed include: adaptive decision-making across trials via RL (learning to follow or wait based on auditory cues), robustness to calibration/modeling errors because IBVS measures task error directly (visual feedback closes the loop), ability to remember and exploit episodic information (Soar episodic memory) to perform structured behaviors such as alternating release locations, and real-time operation of sensorimotor control (~66 Hz loop). The hybrid yields behaviors not present in either alone: symbolic planning/remembering to decide when to engage the visuomotor controller, and visuomotor robustness enabling the symbolic planner to rely on accurate execution despite modeling uncertainty.",
            "task_or_benchmark": "Interactive manipulative task: reaching, grasping and releasing with auditory cues (\"wait\" vs \"go\") modeled as an MDP with states S0..S10 and actions a0..a8; two experimental sets (ES1 learn when to reach vs sleep; ES2 learn to alternate drop locations). Evaluated with reinforcement learning trials and comparisons to a random policy baseline.",
            "hybrid_performance": "Visuomotor control loop: ~66 Hz (≈15 ms per iteration) without GUI display (drops to ≈20 Hz when camera view is displayed). Learning performance (ES1): RL policy success rates reported around 0.85–0.90 (fraction of successful trials) versus random policy success rates around 0.40–0.60 (reported per-case success fractions). ES2 results: learned policies resulted in repeated selection of correct think+release actions; example Q-value-like rewards reported for actions (e.g., think-release-loc-2-B reward ≈ 6.9741, think-release-loc-1-A reward ≈ 6.9741).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Authors report improved adaptability to task variations and system changes (wear, calibration errors) because visual servoing provides closed-loop compensation and RL reduces the need for manual reprogramming. No quantitative out-of-distribution or compositional generalization experiments are reported; claims about generalization are qualitative (robustness to modeling/calibration errors, ability to adapt policies through reinforcement learning).",
            "interpretability_properties": "High interpretability at the symbolic level: decisions and learned Q-values are stored in Soar's symbolic structures (semantic/episodic memories and productions), enabling inspection and episodic retrieval (queries placed on epmem/smem attributes). Low-level imperative control (IBVS and control laws) is algorithmic and interpretable mathematically but not represented symbolically for explanation; overall the hybrid permits traceability of high-level choices and memory retrievals but not end-to-end symbolic explanations of low-level control dynamics.",
            "limitations_or_failures": "Limitations identified include: reliance on simple threshold-based vision (HSV binarization) which assumes constant illumination (sensitivity to lighting changes), potential difficulty in producing robust symbolic representations from noisy sensors, lack of end-to-end integrated learning between perception, decision, and control, limited scalability of hand-authored productions (57 productions used for the studied task), no quantitative comparisons to purely symbolic or purely procedural baselines, and reduced control frequency when debugging GUI is enabled (66 Hz → 20 Hz). Joint singularities and local minima can occur in the absence of secondary-task null-space control (mitigated here by threshold-activated secondary tasks).",
            "theoretical_framework": "Explicit division-of-labor complementary framework: symbolic/psychologically-oriented cognitive architectures are used for high-level planning, memory and deliberative learning (MDP + RL), while imperative procedural controllers (visual servoing, joint-space control) provide efficient, robust low-level sensorimotor execution; the hybrid is motivated by combining complementary strengths (interpretability and structured memory from symbolic systems; robustness and efficiency from procedural controllers).",
            "uuid": "e408.0"
        },
        {
            "name_short": "hybrid CA",
            "name_full": "Hybrid Cognitive Architecture (symbolic + sub-symbolic)",
            "brief_description": "A class of cognitive architectures that combine symbolic (declarative, rule-based, graph-like representations) and sub-symbolic (connectionist, procedural, low-level) paradigms to leverage strengths of both: symbolic systems for high-level reasoning and sub-symbolic for handling low-level uncertainty and perception.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Hybrid Cognitive Architecture (symbolic + sub-symbolic)",
            "system_description": "Conceptual hybrid CA described in the paper as combining symbolic cognitive architectures (graph-based semantic memory and production rules for planning and procedural knowledge) with sub-symbolic architectures (networks of processing nodes, connectionist memory organizations) to form systems capable of both high-level cognition and robust low-level processing. The paper frames the hybrid CA as a motivation for integrating Soar-like symbolic decision-making with visual servoing, rather than presenting a distinct fully-specified hybrid algorithmic template.",
            "declarative_component": "Symbolic cognitive architecture elements: semantic memory, episodic memory, working memory, rule-based productions, graph/tree-like knowledge representations (as exemplified by Soar in the paper).",
            "imperative_component": "Sub-symbolic/imperative elements described conceptually: networks of processing nodes (connectionist), localist/globalist memory organizations, and procedural sensorimotor controllers (the paper specifically uses visual servoing and algorithmic RL as concrete imperative examples).",
            "integration_method": "Described at the conceptual level as a hybrid that uses division-of-labor: symbolic systems handle planning and higher-order tasks, sub-symbolic systems handle context-specific low-level processing and uncertainties. The paper advocates modular integration (message passing, software bus/topics) and feature extraction layers to translate continuous sensor data into symbols rather than end-to-end differentiable integration.",
            "emergent_properties": "At the conceptual level the hybrid is claimed to yield: better handling of high-level planning along with context-specific sensorimotor robustness, useful emergent behaviors like adaptation and generalization to noisy sensory inputs, and the ability to exhibit rich episodic/semantic memory-guided behavior. The paper notes emergent properties in sub-symbolic systems generally (e.g., generalization and interference) and argues hybrids can exploit these while retaining symbolic strengths.",
            "task_or_benchmark": "Conceptual/general — no single benchmark; paper applies the hybrid notion to a manipulative robotics task (reaching/grasping/releasing) implemented as the CRR case study.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Paper states sub-symbolic systems generalize well to novel stimuli but suffer interference when learning new items; hybridization aims to combine that generalization with symbolic strengths. No quantitative generalization benchmarks provided.",
            "interpretability_properties": "Symbolic components confer interpretability and the ability to query/inspect memories and productions; sub-symbolic components are characterized as less transparent. The hybrid is presented as a way to retain interpretability for high-level reasoning.",
            "limitations_or_failures": "Paper highlights conceptual challenges: incompatibility of computational paradigms (symbolic vs sub-symbolic), difficulties in generating reliable symbols from noisy sensors, and architectural/software integration issues. No detailed failure-mode study is provided for the general hybrid concept.",
            "theoretical_framework": "Principle of complementary strengths / division-of-labor: symbolic architectures for structured, abstract cognition and explanations; sub-symbolic/procedural architectures for robust, context-dependent perception and motor control. Hybrid CAs are motivated by combining these to achieve both robust low-level performance and psychologically-plausible high-level cognition.",
            "uuid": "e408.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A cognitive robotic system based on the Soar cognitive architecture for mobile robot navigation, search, and mapping missions",
            "rating": 2,
            "sanitized_title": "a_cognitive_robotic_system_based_on_the_soar_cognitive_architecture_for_mobile_robot_navigation_search_and_mapping_missions"
        },
        {
            "paper_title": "Neurorobotics: From vision to action",
            "rating": 2,
            "sanitized_title": "neurorobotics_from_vision_to_action"
        },
        {
            "paper_title": "Cognitive architectures: Where do we go from here?",
            "rating": 2,
            "sanitized_title": "cognitive_architectures_where_do_we_go_from_here"
        },
        {
            "paper_title": "A brief introduction to current software frameworks in cognitive robotics integrating different computational paradigms",
            "rating": 2,
            "sanitized_title": "a_brief_introduction_to_current_software_frameworks_in_cognitive_robotics_integrating_different_computational_paradigms"
        },
        {
            "paper_title": "Reinforcement learning: A survey",
            "rating": 1,
            "sanitized_title": "reinforcement_learning_a_survey"
        }
    ],
    "cost": 0.011952999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Computational Cognition and Visual Servoing based Methodology to Design Automatic Manipulative Tasks</p>
<p>Hendry Ferreira 
Robotics Team of the Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN)
NantesFrance</p>
<p>Philippe Martinet 
Robotics Team of the Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN)
NantesFrance</p>
<p>A Computational Cognition and Visual Servoing based Methodology to Design Automatic Manipulative Tasks
7BA8008BFD90C577AD2064F10390721510.5220/0004480802130220Cognitive RoboticsComputational CognitionArtificial IntelligenceVisual Servoing
In the last decades, robotics has exerted an important role in the research on diverse knowledge domains, such as, artificial intelligence, biology, neuroscience and psychology.In particular, the study of knowledge representation and thinking, has led to the proposal of cognitive architectures; capturing essential structures and processes of cognition and behavior.Robotists have also attempted to design automatic systems using these proposals.Though, certain difficulties have been reported for obtaining efficient low-level processing while sensing or controlling the robot.The main challenges involve the treatment of the differences between the computational paradigms employed by the cognitive and the robotic architectures.The objective of this work, is to propose a methodology for designing robotic systems capable of decision making and learning when executing manipulative tasks.The development of a system called the Cognitive Reaching Robot (CRR) will be reported.CRR combines the advantages of using a psychologically-oriented cognitive architecture, with efficient low-level behavior implementations through the visual servoing control technique.</p>
<p>INTRODUCTION</p>
<p>In the last decades, with the venue of fields of study such as cybernetics, artificial intelligence, neuroscience and psychology; remarkable progresses have been made in the understanding of what is required to create artificial life evolving in real-world environments (Arbib et al., 2008).Still, one of the remaining challenges is to create new cognitive models that would replicate high-level capabilities; such as, perception and information processing, reasoning, planning, learning, and adaptation to new situations.</p>
<p>The study of knowledge representation and thinking has led to the proposal of Cognitive Architectures.A Cognitive Architecture (CA) can be conceived as a broadly-scoped, domain-generic computational cognitive model, which captures essential structures and processes of the mind, to be used for a broad, multiple-level, multiple-domain analysis of cognition and behavior (Newell, 1994).For cognitive science (i.e., in relation to understanding the human mind) a CA provides a concrete mechaniscist framework for more detailed modeling of cognitive phenomena; through specifying essential structures, divisions of modules, relations between modules, and so on (Duch et al., 2008).</p>
<p>A robot that employs a CA to select its next ac-tion, is derived from integrated models of the cognition of humans or animals.Its control system is designed using that integrated CA and is structurally coupled to its underlying mechanisms (Sun, 2009).However, there are challenges associated with using these architectures in real environments; in particular, for performing efficient low-level processing (Hanford and Long, 2011).It can be hard, thus, to generate meaningful and trustful symbols from potentially noisy sensor measurements, or to exert control over actuators using the representation of knowledge employed by the CA.</p>
<p>Cognitive models are derived from a large spectrum of computational paradigms that are not necessarily compatible when considering underlying software architecture requirements.Scientists in cognition research, and actually higher-level robotic applications, develop their programs, models and experiments using a language grounded in an ontology based on general principles (Huelse and Hild, 2008).Hence, they expect reasonable and scalable performance for general domains and problem spaces.</p>
<p>On the side of cognitive robotists, it would not be reasonable to replace already existing robust mechanisms ensuring sensory-motor control by less efficient ones.Such is the case of the visual servoing technique which uses computer vision data to control the motion of the robot's effector (Corke, 2011).This approach has the advantage of allowing the control by directly measuring the error on the effector's interaction with the environment; making it robust to inaccuracies in estimates of the system parameters (Chaumette and Hutchinson, 2006).</p>
<p>This research seeks to contribute to the debate standing from the point of view of cognitive roboticists.It can be conceived as an effort to assess to what extent it is feasible to build cognitive systems making use of the benefits of a psychologically-oriented CA; without leaving behind efficient control strategies such as visual servoing.The aim is to verify the potential benefits of creating an interactive platform under these technologies; and to analyze the resulting flexibility in automating manipulative tasks.</p>
<p>COGNITIVE ARCHITECTURES</p>
<p>According to (Kelley, 2003), two key design properties that underlie the development of any CA are memory and learning.Various types of memory serve as a repository for background knowledge about the world, the current episode, the activity, and oneself; while learning is the main process that shapes this knowledge.Based on these two features, different approaches can be gathered in three groups: symbolic, non-symbolic, and hybrid models.</p>
<p>A symbolic CA has the ability to input, output, store and alter symbolic entities; executing appropriate actions in order to reach goals (Newell, 1994).The majority of these architectures employ a centralized control over the information flow from sensory inputs, through memory; to motor outputs.This approach stresses the working memory executive functions, with an access to semantic memory; where knowledge generally has a graph-based representation.Rule-based representations of perceptions / actions in the procedural memory, embody the logical reasoning of human experts.</p>
<p>Inspired by connectionist ideas, a sub-symbolic CA is composed by a network of processing nodes (Duch et al., 2008).These nodes interact with each other in specific ways changing the internal state of the system.As a result, interesting emergent properties are revealed.There are two complementary approaches to memory organization, globalist and localist.In these architectures, the generalization of learned responses to novel stimuli is usually good, but learning new items may lead to problematic interference with existent knowledge (O'Reilly and Munakata, 2000).</p>
<p>A hybrid CA combines the relative strengths of the first two paradigms (Kelley, 2003).In this sense, symbolic systems are good approaches to process and executing high-level cognitive tasks; such as, planning and deliberative reasoning, resembling human expertise.But they are not the best approach to represent low-level information.Sub-symbolic systems are better suited for capturing the context-specificity and handling low-level information and uncertainties.Yet, their main shortcoming are difficulties for representing and handling higher-order cognitive tasks.</p>
<p>VISUAL SERVOING</p>
<p>The task in visual servoing (VS) is to use visual features, extracted from an image, to control the pose of the robot's end-effector in relation to a target.The camera may be carried by the end-effector (a configuration known by eye-in-hand) or fixed in (eye-tohand) (Corke, 2011).The aim of all vision-based control schemes is to minimize an error e(t), which is typically defined by e(t) = s(m(t), a) − s * (1) The vector m(t) is a set of image measurements used to compute a vector of k visual features s(m(t), a), based on a set of parameters a representing potential additional knowledge about the system (i.e., the camera intrinsic parameters, or a 3-D model of the target).The vector s * contains the desired values of the features.</p>
<p>Depending on the characteristics of the task, a fixed goal can be considered where changes in s depend only on the camera's motion.A more general situation can also be modeled, where the target is moving and the resulting image depends both on the camera's and the target's motion.In any case, VS schemes mainly differ in the way s is designed.For image-based visual servo control (IBVS), s consists of a set of features that are immediately available in the image data.For position-based visual servo control (PBVS), s consists of a set of 3D parameters, which must be estimated from image measurements.Once s is selected, a velocity controller relating its time variation to the camera velocity is given by
ṡ = L s V c
(2) The spatial velocity of the camera is denoted by V c = (v c , ω c ), with v c the instantaneous linear velocity of the origin of the camera frame and ω c the instantaneous angular velocity of the camera frame.L s ∈ R 6×k is named the interaction matrix related to s.</p>
<p>Using ( 1) and ( 2), the relation between the camera velocity and the time variation of e can be defined by
ICINCO2013-10thInternationalConferenceonInformaticsinControl,AutomationandRobotics ė = L e V c
(3) Considering V c as the input to the controller, if an exponential decoupled decrease of e is desired, from (3) the velocity of the camera can be expressed by
V c = −λL + e e(4)
where L + ∈ R 6×k is chosen as the Moore-Penrose pseudoinverse of L e , that is L + e = (L e t L e ) −1 L e t when L e is of full rank 6.In case k = 6 and det(L e ) = 0, it is possible to invert L e giving the control V c = −λL e −1 e.Following (4), the six components of V c are given as input to the controller.For robots with less than six degrees of freedom, the control scheme may be expressed in the joint space by q = −λ(J e + e + P e e s ) − J e + ∂e ∂t (5)</p>
<p>where J e is the feature Jacobian matrix associated with the primary task e, P e = (I 6 − J e + J e ) is the gradient projection on the null space of the primary task to accomplish a secondary task e s , and ∂e ∂t models the motion of the target.An example of VS is presented in Figure 1.</p>
<p>THE CRR PROPOSAL</p>
<p>The Cognitive Reaching Robot (CRR) is a system designed to perform interactive manipulative tasks.When compared to non-cognitive approaches, CRR has the advantage of being adaptive to variations of the task; since the reinforcement learning mechanism reduces the need for explicitly reprogramming the behavior of the robot.Furthermore, CRR is robust to changes in the robotic system due to wear.It is tolerant to calibration errors by employing visual servoing; where modeling errors are compensated in the control loop (the camera directly measures the task errors).</p>
<p>The platform presents a modular organization (as shown in Figure 2) and is composed by three modules.The cognitive module is responsible for symbolic decision making and learning.The auditory module processes speech recognition.The visuomotor module is in charge of applying the VS control.To enable inter-modular communication, six topics were defined.Topics are named buses over which modules exchange messages.According to the sensory modalities that compose CRR, auditory, proprioceptive and visual topics were defined.The aim of these topics is sending sensory information to the cognitive module.</p>
<p>Similarly, the cognitive module sends commands to the auditory, visual and proprioceptive modules.Software Components.Three criteria grounded the choice for software technologies: source availability, efficiency and continuity of the development community.The sole exception was the use of SYMORO+ (Khalil and Creusot, 1997), a proprietary automatic symbolic modeling tool for robots.CRR was developed under Ubuntu Oneiric Ocelot and relied on the Voce Library 0.9.1, ViSP 2.6.2, the symbolic CA Soar 9.3.2, and ROS Electric.Eclipse Juno 4.2 was used for testing the algorithms.</p>
<p>CASE STUDY</p>
<p>The experimental situation designed, consisted in a reaching, grasping, and releasing task, involving reinforcement learning.From the inputs received, and based on the rewards or punishments obtained, the robot must learn the optimal sequence policy π : S → A to execute the task, and thus, to maximize the reward obtained.</p>
<p>Task Definition.The experimenter is positioned in front of the robot for every trial and presents it an object accompanied by a verbal auditory cue ("wait" or "go").The robot has to choose between sleeping or reaching the object.If the object is reached after a "wait" or the robot goes sleeping after a "go", the experimenter sends an auditory verbal cue representing punishment ("stop") and the trial ends.On the contrary, if the robot goes sleeping after getting a "wait"or follows the object after a "go", it receives an auditory verbal cue representing reward ("great").After being rewarded for following the object, the experiment enters the releasing phase.If the robot alternated the location for dropping the object it is rewarded, otherwise it is punished.Figure 3 presents the reinforcement algorithm.The robot has two main goals in the experiment.It is required to learn when reaching or sleeping in the presence of the object; and if the object is grasped, to learn to drop it alternatively in one of two containers.Summarizing, the robot is required of perceptive abilities (recognizing the object and speech), visuomotor coordination, and decision making (while remembering events).</p>
<p>Perception</p>
<p>Object Recognition.The recognition of the object was accomplished using OpenCV 2.4.The partition of the image into meaningful regions was achievement in two steps.The classification steps includes a decision process applied to each pixel assigning it to one of C classes C ∈ {0...C −1}.For CRR a particular case using C = 2 known as binarization (Pratt, 2007) was used.Formally, it is conceived as a monadic operation taking an image of size I W ×H as input, and producing an image O W ×H as output; such as
O[u, v] = f (I[u, v]), ∀(u, v) ∈ I (6)
The color image I is processed in HSV color space, and the f function used was
f (I[u, v]) = 1 if ε i &lt; I[u, v] &lt; ε f 0 otherwise (7)
The choice of f was based on simplicity and ease of implementation; however, it assumes constant illumination conditions throughout the experiment (which is the case since the environment is illuminated artificially).The thresholds ε were set to recognize red objects.</p>
<p>In the description phase the represented sets S are characterized in terms of scalar or vector-valued features such as size, location and shape.A particularly useful class of image features are moments (Corke, 2011), which are easy to compute and can be used to find the location of an object (centroid).For a binary image B[x, y] the (p + q) th order moment is defined by
m pq = y max ∑ y=0 x max ∑ x=0 x p y q B(x, y)(8)
Moments can be given a physical interpretation by regarding the image function as a mass distribution.Thus m 00 is the total mass of the region, and the centroid of the region is given by
x c = m 10 m 00 , y c = m 01 m 00 (9)
After the centroid is obtained, the last step consisted in proportionally defining two points beside it, forming an imaginary line of −45 • slope.These two points are the output of the object recognition algorithm, later entered to ViSP to define 2D features and performing the VS control.</p>
<p>Speech Recognition.CCR used the Voce Library 0.9.1 to process speech.It required no additional efforts than changing the grammar configuration file to include the vocabulary to be recognized.</p>
<p>Visuomotor Control</p>
<p>In order to perform visuomotor coordination to reach the object, an IBVS strategy was chosen given its robustness to modeling uncertainties (Chaumette and Hutchinson, 2006).The camera was located in the ICINCO2013-10thInternationalConferenceonInformaticsinControl,AutomationandRobotics effector of the robot (eye-in-hand), thus the J e component of ( 5) is defined by
J e = L e c V n n J(q) (10)
Two visuomotor subtasks were defined: reaching the object and avoiding joint limits.</p>
<p>Primary Task.The subtask e consisted in positioning the end-effector in front of the object for grasping it.The final orientation of the effector was not important (assuming a spherical object), therefore, only 3 DOF were required to perform the task.Two 2D point features were used given its simplicity, each of them allowing to control 2 DOF.The resulting interaction matrix L e i was defined by the error vector for the primary task can be expressed by
L ei = −1/Z e 0e t = (x s1 − x s1 * ) (y s1 − y s1 * ) (x s2 − x s2 * ) (y s2 − y s2 * ) (12)
and L 4×6 e is given by
L e = L e 1 L e 2 t (13)
Secondary Task.The remaining 3 DOF were used to perform the secondary task of avoiding joint limits.The strategy adopted was activation thresholds (Marchand et al., 1996).The secondary task is required only if one (or several) joint is in the vicinity of a joint limit.Thus, thresholds can be defined by q i min = q i min + ρ(q i max − q i min ) q i max = q i max − ρ(q i max − q i min ) (14
)
where 0 &lt; ρ &lt; 1/2.The vector e s had 6 components, each defined by
e s i =        β(q i − q imax ) q imax −q i min if q i &gt; q i max β(q i − q i min ) q imax −q i min if q i &lt; q i min 0 else (15)
with the scalar constant β regulating the amplitude of the control law due to the secondary task.</p>
<p>Decision Making</p>
<p>Markov Decision Process (MDP) provided the mathematical framework for modeling decision making.The task space was represented by a set of S = {S 0 , ..., S 10 } states, A = {a 0 , ..., a 8 } actions and Pa(s, s ) = {α 0 , ..., α 14 } action-transition probabilities.The simplified MDP representation of the agent is given in Figure 4.The procedural knowledge implementation in Soar can be conceived as a mapping between an in-put to an output semantic structure.To develop the case study, it was necessary to define three types of productions: maintenance, action and learning rules.The first category includes rules that process inputs and outputs to maintain a consistent state in WM; a typical task is clearing or putting data into the slots in order to access the modules functionalities.The second category includes rules related to the robot's task, such as, managing the MDP state transitions.The last group involves rules that guarantee the correct functioning of RL; it includes tasks like maintaining the operators' Q-values, or registering rewards and punishments.Figure 5 presents a qualitative view of the contents of the procedural memory.For modeling the case study, a total of 57 productions were defined.</p>
<p>Remembrance of Events.Functionalities in Soar are accessed through testing the current semantic structure of WM.The same principle applies for querying data in the long term memory.In order to access the episodic or semantic memory, the programmer must define rules placing the query attributes and values on the attribute epmem (for episodic retrieval) or smem (for semantic retrieval).After each decision cycle, Soar checks the epmem.commandnode to match conditions for episodic retrieval.A copy of the most recent match (if found) will be available on the epmem.resultfor the next decision cycle.</p>
<p>Remembrance of Facts.Facts about the world can be modeled through semantic structures.For the case study, the agent must know what are the stimuli received, or at least, how it feels like in relation to them.Thus, semantic information concerning stimuli was added to the system.The resulting graph was equivalent to a tree of height two (Figure 6).A stimulus has a name, a sensory modality (visual, auditory or proprioceptive) and a valence (positive, negative or neutral).Reinforcement Learning.The learning by reinforcement can be considered as equivalent to mapping situations to actions, so as to maximize a numerical reward signal (Kaelbling et al., 1996).The learner is not told which actions to take, but instead it must discover which actions yield the most reward by trying them.The RL module of Soar is based on the Q-learning algorithm (Kaelbling et al., 1996).In the case study a reward is applied whenever the state is not neutral.Figure 7 illustrates the processing of the stimuli.When an input arrives, procedural rules query the semantic memory to determine the valence associated with the stimulus.Following an analogy with respect to humans, the agent continues to work if it doesn't feel happy or sad about what it has done; if so, it stops to think about it.</p>
<p>RESULTS</p>
<p>The implementation of the functionalities of CRR took place incrementally.Given the independence between the different modules, each component could be developed and tested individually.The modules were connected to the platform through ROS Etectric; a comprehensive simulation was done, and the results obtained are presented below.</p>
<p>System Performance.The performance of the visuomotor module is quite acceptable for real-time control applications.The module was designed to operate in four different modalities.In the VS mode, only visual servoing is available.In the VSI mode, it is possible to have a real-time view of the camera.</p>
<p>In the VSL mode, the system generates log files for joint positions and velocities, feature errors, and camera velocities.Finally, a combination of the last three is allowed in the VSIL mode.As it can be seen in Figure 8, a freq.near to 66 Hz (approx.15 ms per iteration) can be reached.If the camera view is displayed (which can be useful for debugging but has no importance for execution) the freq.drops to 20 Hz.</p>
<p>Joint Limit Avoidance.In order to test the joint limit avoidance property of the system, a simple simulation was designed.The robot was positioned in the configuration displayed in Figure 9a.An object is assumed to be presented to the robot, rotated −10 • in ICINCO2013-10thInternationalConferenceonInformaticsinControl,AutomationandRobotics Figure 9: Robot configuration for testing joint limits avoidance.a) Joint positions in deg: q 1 = 0, q 2 = 90, q 3 = −90, q 4 = 0, q 5 = 0, q 6 = 0. b) Simulated view, dots are the current feature locations and crosses are the desired locations.</p>
<p>the z axis of the camera frame.The simulated camera view is shown in Figure 9b.The primary task (moving the robot to the desired view of the features) can be solved in infinite ways given the current singularity between joint frames 4 and 6.For testing the limit avoidance control law, limits of q 6 min = −5 • and q 6 max = 5 • were set to joint 6.As it is shown in Figure 10, if just the primary task is performed, the control law generated will mostly operate q 6 and the task will fall in local minima, since q 6 min will be reached.On the contrary, as shown in Figure 11, setting a threshold ρ = 0.5 (which means it will be active when q 6 &lt; −2.5 • or q 6 &gt; 2.5 • ) solves the problem and the joint limit is avoided.</p>
<p>Task Learning.The task designed to run over CRR had two learning phases.In order to assess the correctness of the cognitive model and the learning algorithm; two experimental sets were defined.In the experimental set one (ES1), the objective was to teach the robot to identify when reaching the target.The ES1 evaluation consisted of five test cases varying the order of presentation of the clues "wait" and "go".In all conditions the robot started without prior knowl-  edge (the RL module was reset).The comparison between a RL and a random police is given in Table 1; as it can be seen, the robot was able to learn the task.The experimental set two (ES2) assumes ES1 was accomplished, so the agent properly grasped the object and must now learn where to drop it.The ES2 evaluation showed the agent was able to quickly learn the task using RL, and the resulting Q-values are presented in Table 2.For each test case of both ES1 and ES2, the first 20 responses of the robot were registered.</p>
<p>CONCLUSIONS</p>
<p>This work started from the interest in developing cognitive robotic systems for executing manipulative Table 2: ES2 evaluation results.The robot attempted to release the object without remembering 5 times (taking the release-loc-1 and release-loc-2 actions).However, it learned to maximize the reward by tacking the think-Remember action, which was selected 15 times.Finally, after recalling the last location, the agent learned to alternate between the think-release-loc-2-B and think-release-loc-1-A actions.</p>
<p>Action</p>
<p>Freq.Reward think-Remember 15 4.9302 think-release-loc-2-A 1 -2.2800 think-release-loc-2-B 7 6.9741 think-release-loc-1-A 7 6.9741 think-release-loc-1-B 0 0.0000 release-loc-2 2 0.6840 release-loc-1 3 0.4332 tasks.To this purpose, an approach emphasizing multidisciplinary theoretical and technical formulations was adopted.A methodological proposal for integrating a psychologically-oriented cognitive architecture to the visual servoing control technique has been presented; and resulted in the development of a modular system capable of auditory and visual perception, decision making, learning and visuomotor coordination.</p>
<p>The evaluation of the case study, showed that CRR is a system whose operation is adequate for real-time interactive manipulative applications.</p>
<p>Figure 1 :
1
Figure 1: IBVS example.(a) Image points trajectories (c) 3-D trajectory of the camera optical center (Chaumette and Hutchinson, 2007).</p>
<p>Figure 2: The CRR architecture.The boxes represent modules and the ovals indicate the libraries wrapped inside the modules.The links between modules indicate topics.AUS: auditory sensory, PRS: proprioceptive sensory, VIS: visual sensory, AUC: Auditory command, VIC: Visual command, PRC: Proprioceptive command.Hardware Components.The design of CRR aimed to praise the reusability of equipments, so its hardware components were chosen according to a criteria of accessibility in the robotic lab.The project considered a Stäubli TX-40 robot manipulator, an AVT MARLIN F-131C camera, and a DELL Vostro 1500 laptop (Intel Core 2 Duo 1.8GHz 800Mhz FSB, 4.0GB DDR2 667MHz RAM, 256MB NVIDIA GeForce 8600M GT).</p>
<p>Figure 3 :
3
Figure 3: Task reinforcement algorithm.</p>
<p>Figure 4 :Figure 5 :
45
Figure4: The MDP task model.* = (α, ρ), where α is the transition probability from s to s when taking the action, and ρ is the reward associated with the state.From all actions there is a link to S 0 (omitted for clarity) modeling errors on the process with probability 1 − α.The states are: S 0 : Started, S 1 : Initialized, S 2 : Object located, S 3 : Object reached, S 4 : Sleeping, S 5 : Object grasped, S 6 : Object released in location 1, S 7 : Object released in location 2, S 8 : Thinking, S 9 : Object released in location 1 after thinking, S 10 : Object released in location 2 after thinking.The action a 0 initializes the system, a 1 signals the localization of the object, a 2 signals the robot to reach the object, a 3 puts the robot in sleeping mode, a 4 signals the robot to close the gripper, a 5 explores past events, a 6 and a 7 signal the robot to release the object at location 1 or 2 respectively, and a 8 restarts the system.If a state receives a negative feedback from the user ρ i = −4 (punishment).In case of positive feedback, ρ i = 2 (reward).Procedural Knowledge Modeling.Cognitive models in Soar 9.3.2 are stored in long-term production memory as productions.A production has a set of conditions and actions.If the conditions match the current state of working memory (WM), the production fires and the actions are performed.Some attributes of the state are defined by Soar (i.e., io, input-link and name) ensuring the operation of the architecture.The modeler has the choice to define custom attributes, which derives in a great control over the state.Procedural memory</p>
<p>Figure 6 :
6
Figure 6: Stimulus semantic knowledge.A: Auditory, V: Visual, P: Proprioceptive.</p>
<p>Figure 7 :
7
Figure 7: Stimulus processing and reinforcement.</p>
<p>Figure 8: Visuomotor module computing time.</p>
<p>Figure 10: Simulation of VS primary task.</p>
<p>Figure 11 :
11
Figure 11: Simulation of VS avoiding joint limits.</p>
<p>Table 1 :
1Test RL-S RL-C R-S R-CC1170.8580.40C2180.9011 0.55C3170.8512 0.60C4180.9090.45C5180.9010 0.50
ES1 evaluation results.RL-S: number of successes applying a RL policy, RL-C: RL-S/attempts, R-S: number of successes applying a random policy, R-C: R-S/attempts.</p>
<p>ACKNOWLEDGEMENTSThis research was accomplished thanks to the founding of the National Agency of Research through the EQUIPEX ROBOTEX project (ANR-10-EQX-44), of the European Union through the FEDER ROBO-TEX project 2011-2015, and of the Ecole Centrale of Nantes.
Proceedings of the IROS workshop on Current software frameworks in cognitive robotics integrating different computational paradigms. 22 SeptemberNice, France</p>
<p>Neurorobotics: From vision to action. M A Arbib, G Metta, P P Van Der Smagt, Springer Handbook of Robotics. 2008</p>
<p>Visual servo control, part i: Basic approaches. F Chaumette, S Hutchinson, IEEE Robotics and Automation Magazine. 132006</p>
<p>Visual servo control, part ii: Advanced approaches. F Chaumette, S Hutchinson, IEEE Robotics and Automation Magazine. 1412007</p>
<p>P I Corke, Robotics, Vision &amp; Control: Fundamental Algorithms in Matlab. Springer2011</p>
<p>Cognitive architectures: Where do we go from here?. W Duch, R J Oentaryo, M Pasquier, 2008. 2008. 22 SeptemberNice, FranceIn (iro</p>
<p>A cognitive robotic system based on the Soar cognitive architecture for mobile robot navigation, search, and mapping missions. S Hanford, L Long, Pa, M Usa. Huelse, M Hild, Aerospace Engineering. 222011. 2008. 2008University Park,PhD thesisA brief introduction to current software frameworks in cognitive robotics integrating different computational paradigms. In (iro</p>
<p>. September , Nice, France</p>
<p>Reinforcement learning: A survey. L Kaelbling, M Littman, A Moore, Journal of Artificial Intelligence Research. 41996</p>
<p>Symbolic and sub-symbolic representations in computational models of human cognition: What can be learned from biology? Theory &amp; Psychology. T D Kelley, 200313</p>
<p>Symoro+: A system for the symbolic modelling of robots. W Khalil, D Creusot, Robotica. 1521997</p>
<p>Using the task function approach to avoid robot joint limits and kinematic singularities in visual servoing. E Marchand, F Chaumette, A Rizzo, IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'96. Osaka, Japan. Newell, A.Harvard University Press1996. 19943Unified Theories of Cognition</p>
<p>Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain. R O'reilly, Y Munakata, 2000Bradford Books. Mit Press</p>
<p>W Pratt, Digital Image Processing: PIKS Scientific Inside. Wiley-Interscience publication. Wiley2007</p>
<p>Multi-agent systems for society. chapter Cognitive Architectures and Multi-agent Social Simulation. R Sun, 2009AutomationandRoboticsBerlin, Heidelberg</p>            </div>
        </div>

    </div>
</body>
</html>
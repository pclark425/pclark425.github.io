<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-475 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-475</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-475</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-271212127</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.10239v2.pdf" target="_blank">What is Reproducibility in Artificial Intelligence and Machine Learning Research?</a></p>
                <p><strong>Paper Abstract:</strong> In the rapidly evolving fields of Artificial Intelligence (AI) and Machine Learning (ML), the reproducibility crisis underscores the urgent need for clear validation methodologies to maintain scientific integrity and encourage advancement. The crisis is compounded by the prevalent confusion over validation terminology. In response to this challenge, we introduce a framework that clarifies the roles and definitions of key validation efforts: repeatability, dependent and independent reproducibility, and direct and conceptual replicability. This structured framework aims to provide AI/ML researchers with the necessary clarity on these essential concepts, facilitating the appropriate design, conduct, and interpretation of validation studies. By articulating the nuances and specific roles of each type of validation study, we aim to enhance the reliability and trustworthiness of research findings and support the community's efforts to address reproducibility challenges effectively.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e475.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e475.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>data_leakage_ml_pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information leakage in machine-learning experimental pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Undesired flow of information from test/holdout data into training or model selection, often caused by incorrect cross-validation or preprocessing order, resulting in over-optimistic performance estimates and non-reproducible claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning of neural representations of suicide and emotion concepts identifies suicidal youth</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>machine learning experiment pipeline (neuroimaging classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end ML pipeline for a supervised classification task (fMRI features → preprocessing → feature selection → cross-validation → classifier training → evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment code / ML pipeline scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>data leakage / missing preprocessing contract (implementation differs from described evaluation protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The published methods described cross-validation and holdout evaluation, but the implemented pipeline leaked information from the holdout/test partition into feature selection or hyperparameter tuning steps, effectively tuning features to the specific dataset and inflating reported accuracy (reported 91% later shown to be overestimated).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing and cross-validation / model selection steps</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>independent reproducibility study and manual inspection of pipeline (reproduction attempt revealed leakage), re-analysis of code and data flow</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparison of reported performance to corrected performance after removing leakage; identification of inappropriate use of holdout set in cross-validation; counts of affected studies (Kapoor & Narayanan found 294 studies across 17 fields affected by leakage)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Severely inflated performance metrics; led to correction and eventual retraction in the Just et al. example; general consequence is over-optimistic claims that do not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Documented as widespread in surveys and case studies; Kapoor & Narayanan (2023) found 294 studies across 17 fields with leakage issues (paper reports this as a notable prevalence).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or incomplete natural-language descriptions of evaluation protocol and preprocessing order, omitted implementation details, and insufficient independent validation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require explicit, step-by-step documentation of data splits and preprocessing order; share runnable code and data; perform independent reproducibility checks focused on data partitioning; enforce checklist reviews during peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>In the cited case, independent reproduction exposed the flaw and led to retraction — demonstrating high effectiveness of independent code review, though no broad quantitative reduction number is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / neuroimaging (broader ML applications)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is Reproducibility in Artificial Intelligence and Machine Learning Research?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e475.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e475.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>random_seed_sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-seed dependence and under-reporting in deep learning experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High sensitivity of deep learning outcomes to random initializations and nondeterministic factors; when seeds are unreported or single-run results are reported, results may not be reproducible or representative.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Problems and opportunities in training deep learning software systems: An analysis of variance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>deep neural network training runs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Repeated training of the same architecture/hyperparameters (e.g., LeNet5) where randomness (initial weights, data order, nondeterministic ops) influences final model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / hyperparameter listing</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training scripts / experiment harness</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>undocumented random-seed dependence / incomplete reporting of stochastic control</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often report results from single training runs without specifying seeds or reporting run-to-run variance; Pham et al. demonstrated that 16 identical training runs of LeNet5 produced accuracies from 8.6% to 99.0% (a 90.4% spread) under supposedly identical hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure (weight initialization, data shuffling, nondeterministic ops)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical repeated-execution experiments (multiple independent training runs) and variance analysis</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>range and variance of final accuracy across repeated runs (example: 8.6%–99.0% accuracy range; compute spread and descriptive statistics over multiple seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can produce extremely large differences in reported performance, undermining claims based on single runs and making reproduction unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Highlighted as common in deep learning literature; specific example provided (Pham et al.) demonstrates extreme effect but no universal prevalence percentage given.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Omission of random seeds and run-to-run variability in natural-language method descriptions; reliance on single-run reporting; implicit assumptions about determinism.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report random seeds, execute and report multiple runs with summary statistics (mean ± std), use deterministic settings where possible, and include variance analyses in publications.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not numerically quantified in the paper, but described as effective for reducing irreproducible single-run claims when adopted (standard practice recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is Reproducibility in Artificial Intelligence and Machine Learning Research?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e475.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e475.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>artifact_execution_failure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Failure to execute published artifacts due to incomplete documentation or unavailable authors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attempts to run published code or re-execute experiments frequently fail because papers omit necessary details, required artifacts, or authors are unavailable to assist, reducing practical reproducibility of results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Repeatability in computer systems research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>research artifacts and execution environment</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The collection of code, data, environment specs, and documentation intended to enable re-execution of experiments from published papers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section and supplementary materials</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>archive of research code and scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing artifacts / environment mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Collberg & Proebsting's attempts to execute code from 601 papers classified by available time/author availability show low success rates: 32.3% (no author contact), 48.3% (ample time, no author), and 54.0% (ample time with author). These failures reflect gaps between natural-language descriptions and runnable implementations or environment requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experiment implementation and environment specification (dependencies, build/run instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>systematic attempts to execute published code artifacts under controlled categories of time and author availability</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>execution success/failure rates across categorized attempts (reported percentages from Collberg & Proebsting study)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantially reduces the ability of others to reproduce or validate published findings; even when code exists, execution failure prevents independent verification.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Empirically measured in the Collberg & Proebsting study: success in 32.3% / 48.3% / 54.0% depending on conditions; indicates frequent occurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Insufficient documentation, missing runtime environment details, fragile build steps, and lack of accessible artifacts or author support.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require complete artifact packaging (code, data, environment specs, containerization), encourage use of reproducibility badges and artifact review, and provide clear run instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The paper cites standards and tools (e.g., artifact badging) as helpful but does not report quantitative before/after effectiveness; Collberg results show author availability improves success but still leaves many failures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>computer systems research / machine learning experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is Reproducibility in Artificial Intelligence and Machine Learning Research?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e475.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e475.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>dataset_confounding_spurious_features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model exploitation of dataset-specific artifacts (spurious features) versus described clinical signal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When a model learns to predict dataset provenance or spurious correlates (e.g., hospital ID) rather than the intended clinical signal, producing apparently high in-domain performance but poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CNN chest x-ray diagnostic model</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Convolutional neural network trained for pneumonia detection using hospital-sourced radiographs with associated metadata and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (dataset description and evaluation protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model training and evaluation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset confounding / mismatch between claimed predictive signal and what implementation actually leverages</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although papers described training models to detect pathology, investigation found the CNNs were primarily detecting hospital-specific features rather than pathology (models could identify source hospital with >99.95% accuracy), causing dramatic performance degradation when applied to external hospital datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset composition / feature sources exploited by model (data provenance and label distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>direct replication and cross-site evaluation; auxiliary analysis to predict hospital/source from images</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>AUC performance drop across external sites and measurement of hospital-prediction accuracy (reported 99.95%+ ability to identify source hospital), demonstrating confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Models that appear to perform well on held-out in-domain test sets fail to generalize across institutions; reported in-paper claims of performance do not hold under replication.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Demonstrated in the Zech et al. pneumonia study; highlights potential for similar issues wherever datasets vary by site or acquisition protocol, no global prevalence provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Insufficiently detailed dataset provenance and inadequate external validation; ambiguous reporting about dataset heterogeneity and preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Perform external validation across multiple data sources, include provenance metadata, stratified evaluation by site, and report potential confounders explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>External validation reveals the problem; mitigation reduces false generalization but paper provides no quantitative reduction metric beyond the case example.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning for medical imaging / supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is Reproducibility in Artificial Intelligence and Machine Learning Research?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e475.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e475.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>rl_env_underspecification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement-learning environment and evaluation underspecification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Key environment details (random seeds, reward shaping, termination conditions, episode handling) are often under-specified in papers, causing large variance in reported RL benchmark results and hampering reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility of benchmarked deep reinforcement learning tasks for continuous control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>reinforcement learning benchmark experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RL agents trained and evaluated against simulated environments for continuous control tasks (benchmarks), where environment and training hyperparameters determine performance.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / benchmark description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>environment code, agent training scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete environment specification / undocumented stochasticity and termination conditions</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often omit critical environment-level details such as exact reward implementations, episode termination rules, and randomness controls; Islam et al. documented wide variation in reported baseline performance across implementations and attributed this to stochasticity and underreported hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>environment definition, reward function implementation, episode termination, and training hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproduction attempts across independent teams and comparative analysis of reported benchmark outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparative spread in reported results across independent implementations and analysis attributing variance to different environment or termination implementations (Islam et al. reported wide ranges though no single numeric spread is given in this paper excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Creates inconsistent baseline performance across papers and undermines comparisons between algorithms; makes independent reproduction difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as a notable cause for variability in RL literature (Islam et al. study cited); exact prevalence not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous descriptions of environment specifics and omitted low-level implementation choices in natural-language methods sections.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Publish environment code, specify reward and termination rules verbatim, fix or report seeds, and report multiple runs with variance statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Described as necessary and effective strategy; no numeric evaluation provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>reinforcement learning / machine learning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is Reproducibility in Artificial Intelligence and Machine Learning Research?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e475.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e475.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>incomplete_artifacts_conflict</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conflict between incomplete original artifacts and independent reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Situations where original code/data are incomplete or incorrect, yet independent reimplementation reproduces reported results, creating tension in definitions of reproducibility and faithfulness between description and implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A statistical definition for reproducibility and replicability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>reproduction workflow (dependent vs independent reproduction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two pathways to reproduce results: dependent reproduction using original artifacts, and independent reproduction by reimplementation from textual description.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods + artifact package (code/data)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>original provided code/data vs independently written reimplementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/incorrect original artifacts vs faithful independent reimplementation (mismatch between published artifacts and descriptive protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Patil et al.'s example: original code/data were incomplete or incorrect, but independent reimplementation (new code) reproduced the original results. This did not satisfy Patil et al.'s technical reproducibility definition, exposing a gap between the natural-language description, the original artifacts, and what actually constitutes reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experiment implementation artifacts and their fidelity to published description</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>independent reproduction attempts and comparison between results of original artifacts (when runnable) and independently implemented pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Binary classification against a reproducibility definition (Patil et al. judged the case as not meeting their technical reproducibility criterion despite successful independent reproduction); no numerical metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Creates ambiguity in reproducibility claims and can mislead assessments of whether published artifacts suffice for reproduction; complicates reproducibility taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Presented as an edge-case in definitional debates; prevalence not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incomplete or incorrect original artifacts, ambiguous textual descriptions, and differing reproducibility definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require complete, correct artifact release and clearer reproducibility taxonomies; encourage independent reproduction as a stronger validation pathway.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Independent reproduction revealed correctness of conclusions despite faulty artifacts, showing independent reimplementation is effective for validation; no broad quantitative effectiveness provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>experimental reproducibility methodology / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is Reproducibility in Artificial Intelligence and Machine Learning Research?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e475.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e475.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>automation_vs_correctness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated one-click reproduction vs actual validation (convenience-correctness gap)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automating result regeneration (gold standard) improves repeatability but may not detect incorrect implementations or methodological flaws; ease of re-execution does not guarantee validity of conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility standards for machine learning in the life sciences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>automated experiment regeneration platforms / artifact automation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Platforms or packaging approaches that enable fully automated regeneration of results (containerized, scripted, one-click reproducibility).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>standards document / methodological recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>fully packaged reproducible artifact (container + scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>automation-convenience vs correctness gap (gold-standard automation without methodological validation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Heil's bronze/silver/gold automation standard shows that a 'gold' fully-automated reproduction may still only provide repeatability; it does not substitute for deeper reproducibility checks that validate implementation correctness or detect conceptual/design flaws.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>validation workflow (reproduction vs substantive correctness checks)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>conceptual analysis and comparative framework discussion (observational, not empirical in this paper excerpt)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No quantitative measurement provided; argument based on conceptual hierarchy of validation rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>May create false confidence that results are scientifically validated when they are simply easy to re-run; does not prevent propagation of flawed findings.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as a common misconception in reproducibility discussions; no prevalence metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Overemphasis on ease of re-execution in documentation and tooling versus insufficient focus on independent validation and methodological correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Complement automated regeneration with independent reproducibility studies, code review that examines correctness, and higher-level replication efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Argued qualitatively to be necessary; no quantitative effectiveness reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning reproducibility infrastructure / scientific standards</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is Reproducibility in Artificial Intelligence and Machine Learning Research?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Machine learning of neural representations of suicide and emotion concepts identifies suicidal youth <em>(Rating: 2)</em></li>
                <li>Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study <em>(Rating: 2)</em></li>
                <li>Problems and opportunities in training deep learning software systems: An analysis of variance <em>(Rating: 2)</em></li>
                <li>Repeatability in computer systems research <em>(Rating: 2)</em></li>
                <li>Reproducibility of benchmarked deep reinforcement learning tasks for continuous control <em>(Rating: 2)</em></li>
                <li>Leakage and the reproducibility crisis in machine-learning-based science <em>(Rating: 2)</em></li>
                <li>A statistical definition for reproducibility and replicability <em>(Rating: 2)</em></li>
                <li>Replicability is not reproducibility: nor is it good science <em>(Rating: 1)</em></li>
                <li>A step toward quantifying independently reproducible machine learning research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-475",
    "paper_id": "paper-271212127",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "data_leakage_ml_pipeline",
            "name_full": "Information leakage in machine-learning experimental pipelines",
            "brief_description": "Undesired flow of information from test/holdout data into training or model selection, often caused by incorrect cross-validation or preprocessing order, resulting in over-optimistic performance estimates and non-reproducible claims.",
            "citation_title": "Machine learning of neural representations of suicide and emotion concepts identifies suicidal youth",
            "mention_or_use": "mention",
            "system_name": "machine learning experiment pipeline (neuroimaging classifier)",
            "system_description": "End-to-end ML pipeline for a supervised classification task (fMRI features → preprocessing → feature selection → cross-validation → classifier training → evaluation).",
            "nl_description_type": "research paper methods section",
            "code_implementation_type": "experiment code / ML pipeline scripts",
            "gap_type": "data leakage / missing preprocessing contract (implementation differs from described evaluation protocol)",
            "gap_description": "The published methods described cross-validation and holdout evaluation, but the implemented pipeline leaked information from the holdout/test partition into feature selection or hyperparameter tuning steps, effectively tuning features to the specific dataset and inflating reported accuracy (reported 91% later shown to be overestimated).",
            "gap_location": "data preprocessing and cross-validation / model selection steps",
            "detection_method": "independent reproducibility study and manual inspection of pipeline (reproduction attempt revealed leakage), re-analysis of code and data flow",
            "measurement_method": "comparison of reported performance to corrected performance after removing leakage; identification of inappropriate use of holdout set in cross-validation; counts of affected studies (Kapoor & Narayanan found 294 studies across 17 fields affected by leakage)",
            "impact_on_results": "Severely inflated performance metrics; led to correction and eventual retraction in the Just et al. example; general consequence is over-optimistic claims that do not generalize.",
            "frequency_or_prevalence": "Documented as widespread in surveys and case studies; Kapoor & Narayanan (2023) found 294 studies across 17 fields with leakage issues (paper reports this as a notable prevalence).",
            "root_cause": "Ambiguous or incomplete natural-language descriptions of evaluation protocol and preprocessing order, omitted implementation details, and insufficient independent validation.",
            "mitigation_approach": "Require explicit, step-by-step documentation of data splits and preprocessing order; share runnable code and data; perform independent reproducibility checks focused on data partitioning; enforce checklist reviews during peer review.",
            "mitigation_effectiveness": "In the cited case, independent reproduction exposed the flaw and led to retraction — demonstrating high effectiveness of independent code review, though no broad quantitative reduction number is provided in the paper.",
            "domain_or_field": "machine learning / neuroimaging (broader ML applications)",
            "reproducibility_impact": true,
            "uuid": "e475.0",
            "source_info": {
                "paper_title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "random_seed_sensitivity",
            "name_full": "Random-seed dependence and under-reporting in deep learning experiments",
            "brief_description": "High sensitivity of deep learning outcomes to random initializations and nondeterministic factors; when seeds are unreported or single-run results are reported, results may not be reproducible or representative.",
            "citation_title": "Problems and opportunities in training deep learning software systems: An analysis of variance",
            "mention_or_use": "mention",
            "system_name": "deep neural network training runs",
            "system_description": "Repeated training of the same architecture/hyperparameters (e.g., LeNet5) where randomness (initial weights, data order, nondeterministic ops) influences final model performance.",
            "nl_description_type": "research paper methods section / hyperparameter listing",
            "code_implementation_type": "training scripts / experiment harness",
            "gap_type": "undocumented random-seed dependence / incomplete reporting of stochastic control",
            "gap_description": "Papers often report results from single training runs without specifying seeds or reporting run-to-run variance; Pham et al. demonstrated that 16 identical training runs of LeNet5 produced accuracies from 8.6% to 99.0% (a 90.4% spread) under supposedly identical hyperparameters.",
            "gap_location": "training procedure (weight initialization, data shuffling, nondeterministic ops)",
            "detection_method": "empirical repeated-execution experiments (multiple independent training runs) and variance analysis",
            "measurement_method": "range and variance of final accuracy across repeated runs (example: 8.6%–99.0% accuracy range; compute spread and descriptive statistics over multiple seeds)",
            "impact_on_results": "Can produce extremely large differences in reported performance, undermining claims based on single runs and making reproduction unreliable.",
            "frequency_or_prevalence": "Highlighted as common in deep learning literature; specific example provided (Pham et al.) demonstrates extreme effect but no universal prevalence percentage given.",
            "root_cause": "Omission of random seeds and run-to-run variability in natural-language method descriptions; reliance on single-run reporting; implicit assumptions about determinism.",
            "mitigation_approach": "Report random seeds, execute and report multiple runs with summary statistics (mean ± std), use deterministic settings where possible, and include variance analyses in publications.",
            "mitigation_effectiveness": "Not numerically quantified in the paper, but described as effective for reducing irreproducible single-run claims when adopted (standard practice recommendation).",
            "domain_or_field": "deep learning / machine learning",
            "reproducibility_impact": true,
            "uuid": "e475.1",
            "source_info": {
                "paper_title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "artifact_execution_failure",
            "name_full": "Failure to execute published artifacts due to incomplete documentation or unavailable authors",
            "brief_description": "Attempts to run published code or re-execute experiments frequently fail because papers omit necessary details, required artifacts, or authors are unavailable to assist, reducing practical reproducibility of results.",
            "citation_title": "Repeatability in computer systems research",
            "mention_or_use": "mention",
            "system_name": "research artifacts and execution environment",
            "system_description": "The collection of code, data, environment specs, and documentation intended to enable re-execution of experiments from published papers.",
            "nl_description_type": "research paper methods section and supplementary materials",
            "code_implementation_type": "archive of research code and scripts",
            "gap_type": "incomplete specification / missing artifacts / environment mismatch",
            "gap_description": "Collberg & Proebsting's attempts to execute code from 601 papers classified by available time/author availability show low success rates: 32.3% (no author contact), 48.3% (ample time, no author), and 54.0% (ample time with author). These failures reflect gaps between natural-language descriptions and runnable implementations or environment requirements.",
            "gap_location": "experiment implementation and environment specification (dependencies, build/run instructions)",
            "detection_method": "systematic attempts to execute published code artifacts under controlled categories of time and author availability",
            "measurement_method": "execution success/failure rates across categorized attempts (reported percentages from Collberg & Proebsting study)",
            "impact_on_results": "Substantially reduces the ability of others to reproduce or validate published findings; even when code exists, execution failure prevents independent verification.",
            "frequency_or_prevalence": "Empirically measured in the Collberg & Proebsting study: success in 32.3% / 48.3% / 54.0% depending on conditions; indicates frequent occurrence.",
            "root_cause": "Insufficient documentation, missing runtime environment details, fragile build steps, and lack of accessible artifacts or author support.",
            "mitigation_approach": "Require complete artifact packaging (code, data, environment specs, containerization), encourage use of reproducibility badges and artifact review, and provide clear run instructions.",
            "mitigation_effectiveness": "The paper cites standards and tools (e.g., artifact badging) as helpful but does not report quantitative before/after effectiveness; Collberg results show author availability improves success but still leaves many failures.",
            "domain_or_field": "computer systems research / machine learning experiments",
            "reproducibility_impact": true,
            "uuid": "e475.2",
            "source_info": {
                "paper_title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "dataset_confounding_spurious_features",
            "name_full": "Model exploitation of dataset-specific artifacts (spurious features) versus described clinical signal",
            "brief_description": "When a model learns to predict dataset provenance or spurious correlates (e.g., hospital ID) rather than the intended clinical signal, producing apparently high in-domain performance but poor generalization.",
            "citation_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study",
            "mention_or_use": "mention",
            "system_name": "CNN chest x-ray diagnostic model",
            "system_description": "Convolutional neural network trained for pneumonia detection using hospital-sourced radiographs with associated metadata and labels.",
            "nl_description_type": "research paper methods section (dataset description and evaluation protocol)",
            "code_implementation_type": "model training and evaluation code",
            "gap_type": "dataset confounding / mismatch between claimed predictive signal and what implementation actually leverages",
            "gap_description": "Although papers described training models to detect pathology, investigation found the CNNs were primarily detecting hospital-specific features rather than pathology (models could identify source hospital with &gt;99.95% accuracy), causing dramatic performance degradation when applied to external hospital datasets.",
            "gap_location": "dataset composition / feature sources exploited by model (data provenance and label distribution)",
            "detection_method": "direct replication and cross-site evaluation; auxiliary analysis to predict hospital/source from images",
            "measurement_method": "AUC performance drop across external sites and measurement of hospital-prediction accuracy (reported 99.95%+ ability to identify source hospital), demonstrating confounding.",
            "impact_on_results": "Models that appear to perform well on held-out in-domain test sets fail to generalize across institutions; reported in-paper claims of performance do not hold under replication.",
            "frequency_or_prevalence": "Demonstrated in the Zech et al. pneumonia study; highlights potential for similar issues wherever datasets vary by site or acquisition protocol, no global prevalence provided.",
            "root_cause": "Insufficiently detailed dataset provenance and inadequate external validation; ambiguous reporting about dataset heterogeneity and preprocessing.",
            "mitigation_approach": "Perform external validation across multiple data sources, include provenance metadata, stratified evaluation by site, and report potential confounders explicitly.",
            "mitigation_effectiveness": "External validation reveals the problem; mitigation reduces false generalization but paper provides no quantitative reduction metric beyond the case example.",
            "domain_or_field": "deep learning for medical imaging / supervised learning",
            "reproducibility_impact": true,
            "uuid": "e475.3",
            "source_info": {
                "paper_title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "rl_env_underspecification",
            "name_full": "Reinforcement-learning environment and evaluation underspecification",
            "brief_description": "Key environment details (random seeds, reward shaping, termination conditions, episode handling) are often under-specified in papers, causing large variance in reported RL benchmark results and hampering reproducibility.",
            "citation_title": "Reproducibility of benchmarked deep reinforcement learning tasks for continuous control",
            "mention_or_use": "mention",
            "system_name": "reinforcement learning benchmark experiments",
            "system_description": "RL agents trained and evaluated against simulated environments for continuous control tasks (benchmarks), where environment and training hyperparameters determine performance.",
            "nl_description_type": "research paper methods section / benchmark description",
            "code_implementation_type": "environment code, agent training scripts",
            "gap_type": "incomplete environment specification / undocumented stochasticity and termination conditions",
            "gap_description": "Papers often omit critical environment-level details such as exact reward implementations, episode termination rules, and randomness controls; Islam et al. documented wide variation in reported baseline performance across implementations and attributed this to stochasticity and underreported hyperparameters.",
            "gap_location": "environment definition, reward function implementation, episode termination, and training hyperparameters",
            "detection_method": "reproduction attempts across independent teams and comparative analysis of reported benchmark outcomes",
            "measurement_method": "Comparative spread in reported results across independent implementations and analysis attributing variance to different environment or termination implementations (Islam et al. reported wide ranges though no single numeric spread is given in this paper excerpt).",
            "impact_on_results": "Creates inconsistent baseline performance across papers and undermines comparisons between algorithms; makes independent reproduction difficult.",
            "frequency_or_prevalence": "Reported as a notable cause for variability in RL literature (Islam et al. study cited); exact prevalence not quantified in the paper.",
            "root_cause": "Ambiguous descriptions of environment specifics and omitted low-level implementation choices in natural-language methods sections.",
            "mitigation_approach": "Publish environment code, specify reward and termination rules verbatim, fix or report seeds, and report multiple runs with variance statistics.",
            "mitigation_effectiveness": "Described as necessary and effective strategy; no numeric evaluation provided in the paper.",
            "domain_or_field": "reinforcement learning / machine learning benchmarks",
            "reproducibility_impact": true,
            "uuid": "e475.4",
            "source_info": {
                "paper_title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "incomplete_artifacts_conflict",
            "name_full": "Conflict between incomplete original artifacts and independent reproduction",
            "brief_description": "Situations where original code/data are incomplete or incorrect, yet independent reimplementation reproduces reported results, creating tension in definitions of reproducibility and faithfulness between description and implementation.",
            "citation_title": "A statistical definition for reproducibility and replicability",
            "mention_or_use": "mention",
            "system_name": "reproduction workflow (dependent vs independent reproduction)",
            "system_description": "Two pathways to reproduce results: dependent reproduction using original artifacts, and independent reproduction by reimplementation from textual description.",
            "nl_description_type": "research paper methods + artifact package (code/data)",
            "code_implementation_type": "original provided code/data vs independently written reimplementation",
            "gap_type": "incomplete/incorrect original artifacts vs faithful independent reimplementation (mismatch between published artifacts and descriptive protocol)",
            "gap_description": "Patil et al.'s example: original code/data were incomplete or incorrect, but independent reimplementation (new code) reproduced the original results. This did not satisfy Patil et al.'s technical reproducibility definition, exposing a gap between the natural-language description, the original artifacts, and what actually constitutes reproducibility.",
            "gap_location": "experiment implementation artifacts and their fidelity to published description",
            "detection_method": "independent reproduction attempts and comparison between results of original artifacts (when runnable) and independently implemented pipelines",
            "measurement_method": "Binary classification against a reproducibility definition (Patil et al. judged the case as not meeting their technical reproducibility criterion despite successful independent reproduction); no numerical metric provided.",
            "impact_on_results": "Creates ambiguity in reproducibility claims and can mislead assessments of whether published artifacts suffice for reproduction; complicates reproducibility taxonomy.",
            "frequency_or_prevalence": "Presented as an edge-case in definitional debates; prevalence not quantified in the paper.",
            "root_cause": "Incomplete or incorrect original artifacts, ambiguous textual descriptions, and differing reproducibility definitions.",
            "mitigation_approach": "Require complete, correct artifact release and clearer reproducibility taxonomies; encourage independent reproduction as a stronger validation pathway.",
            "mitigation_effectiveness": "Independent reproduction revealed correctness of conclusions despite faulty artifacts, showing independent reimplementation is effective for validation; no broad quantitative effectiveness provided.",
            "domain_or_field": "experimental reproducibility methodology / machine learning",
            "reproducibility_impact": true,
            "uuid": "e475.5",
            "source_info": {
                "paper_title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "automation_vs_correctness",
            "name_full": "Automated one-click reproduction vs actual validation (convenience-correctness gap)",
            "brief_description": "Automating result regeneration (gold standard) improves repeatability but may not detect incorrect implementations or methodological flaws; ease of re-execution does not guarantee validity of conclusions.",
            "citation_title": "Reproducibility standards for machine learning in the life sciences",
            "mention_or_use": "mention",
            "system_name": "automated experiment regeneration platforms / artifact automation",
            "system_description": "Platforms or packaging approaches that enable fully automated regeneration of results (containerized, scripted, one-click reproducibility).",
            "nl_description_type": "standards document / methodological recommendations",
            "code_implementation_type": "fully packaged reproducible artifact (container + scripts)",
            "gap_type": "automation-convenience vs correctness gap (gold-standard automation without methodological validation)",
            "gap_description": "Heil's bronze/silver/gold automation standard shows that a 'gold' fully-automated reproduction may still only provide repeatability; it does not substitute for deeper reproducibility checks that validate implementation correctness or detect conceptual/design flaws.",
            "gap_location": "validation workflow (reproduction vs substantive correctness checks)",
            "detection_method": "conceptual analysis and comparative framework discussion (observational, not empirical in this paper excerpt)",
            "measurement_method": "No quantitative measurement provided; argument based on conceptual hierarchy of validation rigor.",
            "impact_on_results": "May create false confidence that results are scientifically validated when they are simply easy to re-run; does not prevent propagation of flawed findings.",
            "frequency_or_prevalence": "Described as a common misconception in reproducibility discussions; no prevalence metric provided.",
            "root_cause": "Overemphasis on ease of re-execution in documentation and tooling versus insufficient focus on independent validation and methodological correctness.",
            "mitigation_approach": "Complement automated regeneration with independent reproducibility studies, code review that examines correctness, and higher-level replication efforts.",
            "mitigation_effectiveness": "Argued qualitatively to be necessary; no quantitative effectiveness reported in the paper.",
            "domain_or_field": "machine learning reproducibility infrastructure / scientific standards",
            "reproducibility_impact": true,
            "uuid": "e475.6",
            "source_info": {
                "paper_title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Machine learning of neural representations of suicide and emotion concepts identifies suicidal youth",
            "rating": 2,
            "sanitized_title": "machine_learning_of_neural_representations_of_suicide_and_emotion_concepts_identifies_suicidal_youth"
        },
        {
            "paper_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study",
            "rating": 2,
            "sanitized_title": "variable_generalization_performance_of_a_deep_learning_model_to_detect_pneumonia_in_chest_radiographs_a_crosssectional_study"
        },
        {
            "paper_title": "Problems and opportunities in training deep learning software systems: An analysis of variance",
            "rating": 2,
            "sanitized_title": "problems_and_opportunities_in_training_deep_learning_software_systems_an_analysis_of_variance"
        },
        {
            "paper_title": "Repeatability in computer systems research",
            "rating": 2,
            "sanitized_title": "repeatability_in_computer_systems_research"
        },
        {
            "paper_title": "Reproducibility of benchmarked deep reinforcement learning tasks for continuous control",
            "rating": 2,
            "sanitized_title": "reproducibility_of_benchmarked_deep_reinforcement_learning_tasks_for_continuous_control"
        },
        {
            "paper_title": "Leakage and the reproducibility crisis in machine-learning-based science",
            "rating": 2,
            "sanitized_title": "leakage_and_the_reproducibility_crisis_in_machinelearningbased_science"
        },
        {
            "paper_title": "A statistical definition for reproducibility and replicability",
            "rating": 2,
            "sanitized_title": "a_statistical_definition_for_reproducibility_and_replicability"
        },
        {
            "paper_title": "Replicability is not reproducibility: nor is it good science",
            "rating": 1,
            "sanitized_title": "replicability_is_not_reproducibility_nor_is_it_good_science"
        },
        {
            "paper_title": "A step toward quantifying independently reproducible machine learning research",
            "rating": 1,
            "sanitized_title": "a_step_toward_quantifying_independently_reproducible_machine_learning_research"
        }
    ],
    "cost": 0.01503025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What is Reproducibility in Artificial Intelligence and Machine Learning Research?
30 Mar 2025</p>
<p>Abhyuday Desai 
Ready Tensor, Inc</p>
<p>Mohamed Abdelhamid 
Ready Tensor, Inc</p>
<p>Nakul R Padalkar 
Administrative Sciences
Metropolitan College
Boston University</p>
<p>What is Reproducibility in Artificial Intelligence and Machine Learning Research?
30 Mar 2025FBEEC4D81C21A2147E4DA62F8662D7EFarXiv:2407.10239v2[cs.CY]
In the rapidly evolving fields of Artificial Intelligence (AI) and Machine Learning (ML), the reproducibility crisis underscores the urgent need for clear validation methodologies to maintain scientific integrity and encourage advancement.The crisis is compounded by the prevalent confusion over validation terminology.In response to this challenge, we introduce a framework that clarifies the roles and definitions of key validation efforts: repeatability, dependent and independent reproducibility, and direct and conceptual replicability.This structured framework aims to provide AI/ML researchers with the necessary clarity on these essential concepts, facilitating the appropriate design, conduct, and interpretation of validation studies.By articulating the nuances and specific roles of each type of validation study, we aim to enhance the reliability and trustworthiness of research findings and support the community's efforts to address reproducibility challenges effectively.</p>
<p>Introduction</p>
<p>The AI/ML domain has witnessed explosive growth in research publications over the past few years.With major conferences receiving thousands of submissions annually, the sheer volume of research output has made it challenging to ensure consistent reproducibility.Conferences like NeurIPS, ICML, ICLR, and AAAI have seen a significant growth in paper submissions.The number of papers submitted to these four conferences has increased by 169% between 2018 and 2023.Figure 1 shows the number of articles submitted and accepted at the top conferences.</p>
<p>Although the number of publications has increased, attempts to reproduce the findings of these papers have often been met with challenges.Numerous studies have highlighted this critical issue in recent years.For example, a survey of 1,576 researchers in the Nature journal indicated that more than 70% of researchers have tried and failed in their attempt to reproduce another researcher's experiments, and more than half have failed to reproduce their own experiments (Baker, 2016).Collberg and Proebsting (2016) attempted to execute the code from 601 papers from computer systems research.This study only involved at-tempts to execute the code, not verify the correctness of the published results.These attempts to re-execute the code were divided into three categories: category 1: time to examine a research artifact is limited and communicating with the author is not an option; category 2 involves the scenario where ample time is available, but the lead author is not available for consultation; category 3 represents the case where ample time is available, and the author is available to correspond.They achieved success in 32.3% in category 1 attempts, 48.3% in category 2, and 54.0% in category 3. Raff (2019) attempted to reproduce results of 255 papers between 1984 and 2017 with a success rate of 63.5%.This study involved independent reproduction attempts, where the original authors' code, even if available, was not used.Islam et al. (2017) investigated the reproducibility of benchmarked deep reinforcement learning tasks and found a wide range of results reported in the literature for the same baseline algorithms, highlighting the difficulty of reproduction attempts.These variations were attributed to factors such as external randomness, under-reporting of hyperparameters, and a narrow range of tasks under the benchmark.Pham et al. (2020) revealed a striking statistic that highlights reproducibility challenges in deep Figure 1: Total number of papers submitted and accepted at NeurIPS, ICML, ICLR and AAAI from 2018 to 2023 learning research.They reported that for 16 identical training runs for a popular deep learning network architecture called LeNet5, the accuracy of the resulting 16 models ranged from 8.6% to 99.0% -a difference of 90.4% across the runs.</p>
<p>The reproducibility challenges in AI/ML research highlighted above, spanning from code execution issues to the variability in experimental outcomes, emphasize the critical need for a systematic approach to ensure effective and reliable validation efforts.</p>
<p>Terminology Confusion</p>
<p>Addressing the reproducibility crisis in AI and ML research requires not only clear validation methods but also a precise understanding of fundamental terms, like 'repeatability', 'reproducibility', and 'replicability'.Despite their significance in establishing the trustworthiness of research, there exists a notable confusion over these terms' meanings.For instance, Hunold (2015) highlighted findings from a survey conducted at the Euro-Par conference on reproducibility in parallel computing, where only 32% of respondents indicated they could accurately differentiate between replicability, repeatability, and reproducibility, pointing to a significant gap in understanding that complicates the validation process.Gundersen (2021) highlights this issue further by reviewing the 34 diverse definitions and interpretations of 'reproducibility', 'replication', and related terms across numerous research papers, concluding that a single, agreed-upon definition does not exist.Barba (2018) work on the disparities in terminologies for reproducible research reveals how the application and understanding of these terms can vary significantly across researchers in different scientific fields, contributing to ongoing debates and misunderstandings.Barba's study identifies three distinct approaches for using these terms:</p>
<p>• Approach A: No distinction is made between 'reproducibility,' 'replicability,' and 'repeatability.'</p>
<p>• Approach B1: 'Reproducibility' is defined as using the original data and code to regenerate the results, whereas 'replicability' refers to generating similar scientific findings with new data.</p>
<p>• Approach B2: Conversely, 'reproducibility' implies that independent researchers achieve the same results using their own methods and data, while 'replicability' involves using the original study's artifacts.</p>
<p>The most prominent example of category A is the Open Science Collaboration (2012), an initiative focusing on promoting open science practices across disciplines.Here is an excerpt from chapter 11: "narrowly, reproducibility is the repetition of a simulation or data analysis of existing data by re-executing a program.More broadly, reproducibility refers to direct replication, an attempt to replicate the original observation using the same methods of a previous investigation but collecting new [data]."</p>
<p>Under category B1, there are often-cited works by Claerbout and Karrenbach (1992), Buckheit and Donoho (1995); are credited for being pioneering works in defining reproducibility.While Peng et al. (2006) is cited for distinguishing the term replication.The works under category B2 include Cartwright (1991), Drummond (2009), andPlesser (2018).These researchers directly swapped the meaning of the terms reproducibility and replicability compared to Category B1.</p>
<p>There have been attempts to resolve the debate on terminology.Patil et al. (2016) provide their definition of reproducibility and different forms of replicability.However, they encountered conflicts with their definition of reproducibility involving the case where the data and code from the original study were incomplete and/or incorrect.The reproducibility study team implemented new code, which indeed reproduced the original results.The authors noted that this didn't meet their technical definition of reproducibility.Goodman et al. (2016) presents a new lexicon for research reproducibility, introducing terms like methods reproducibility, results reproducibility, and inferential reproducibility.These terms avoid the ambiguity resulting from the different interpretations of the words "reproducible", "replicable," and "repeatable" in common usage.Gundersen (2021) also proposes new terms representing three degrees of reproducibility: outcome, analysis, and interpretation reproducibility.The authors also define four reproducibility types driven by documentation that is used to conduct the reproducibility study: R1 (only text is used as reference), R2 (text and code are used), R3 (text and data are used), and R4 (text, data, and code are used).</p>
<p>Motivation for Our Framework</p>
<p>The terminological confusion in reproducibility research hampers scientific progress.Rather than introducing new terms, our framework refines existing ones to establish a common language that researchers already intuitively understand.Current approaches to reproducibility often emphasize convenience by sharing code and data to facilitate regeneration.While transparency through artifact sharing is valuable, it does not inherently enhance scientific trust.True validation occurs when findings persist despite deliberate variations in experimental elements.</p>
<p>We propose a hierarchical validation framework that places different types of validation efforts on a spectrum of increasing scientific rigor.While repeatability (obtaining consistent results under identical conditions) forms the baseline, it represents only a minimal validation standard.Reproducibility, which involves external researchers validating the original experiment's correctness, elevates this standard further.</p>
<p>The highest levels of validation come through replication efforts:</p>
<p>direct replicability tests whether findings hold when implementation details change, while conceptual replicability examines whether conclusions remain valid across entirely different experimental approaches.</p>
<p>This hierarchy challenges the research community to move beyond making repetition more convenient toward facilitating higher forms of validation.It also provides clear guidance when validation studies yield conflicting results: findings from higher-level validation efforts should generally take precedence over those from lower levels.This precedence relationship, often missing from existing frameworks, offers researchers a clearer path through the complex landscape of validation efforts.</p>
<p>AI/ML Research Context and Reproducibility Landscape</p>
<p>Scope and Definitions: AI and ML in Context</p>
<p>To establish a common understanding and provide context for the discussion on reproducibility, we first define the key terms relevant to this paper: Artificial Intelligence (AI), Machine Learning (ML), Supervised Learning, Unsupervised Learning, Reinforcement Learning, and Generative AI.These definitions help clarify the scope of our framework, especially as each paradigm presents unique characteristics and requirements.Artificial Intelligence (AI): refers to computer systems capable of performing tasks that typically require human intelligence.</p>
<p>Machine Learning (ML): is a subset of AI that focuses on systems that learn from data without explicit programming, improving performance through experience.</p>
<p>Our paper focuses on empirical, data-centric AI/ML research, where the central goal is to in-vestigate how models learn from data to perform tasks.This encompasses most contemporary AI/ML research published in venues such as the Conference on Neural Information Processing Systems (NeurIPS), the International Conference on Machine Learning (ICML), the International Conference on Learning Representations (ICLR), and the AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Within the empirical domain, several paradigms have emerged, each with distinct validation challenges.These paradigms represent different approaches to leveraging data for building intelligent systems, and our framework aims to provide validation guidance across this spectrum.</p>
<p>Machine Learning</p>
<p>Supervised Learning involves training on labeled data pairs.Common reproducibility issues include data leakage (i.e.unintentional inclusion of test data during training), inconsistent data-splitting protocols, unreported preprocessing steps, and variations in cross-validation implementations-all of which can significantly impact model performance.</p>
<p>Unsupervised Learning discovers patterns in unlabeled data.Reproducibility challenges stem from sensitivity to initialization methods, ambiguous convergence criteria, inconsistently applied evaluation metrics, and unreported parameter choices that can lead to substantially different discovered patterns.</p>
<p>Reinforcement Learning (RL) trains agents through interaction and feedback from the environment.As noted in Islam et al. (2017), the reproducibility of RL is complicated by environmental stochasticity, variations in the implementation of reward functions, inconsistent episode termination conditions, and differences in exploration strategies -factors often not adequately documented in research articles.</p>
<p>Deep Learning</p>
<p>Deep learning, a subset of machine learning, has powered breakthroughs in computer vision, natural language processing, by leveraging multi-layer neural networks.Reproducibility challenges include sensitivity to weight initialization schemes, dependencies on specific hardware/software configurations, undocumented hyper-parameter tuning protocols, and random seed dependencies.For example, Pham et al. (2020) reported drastically different performance outcomes (8.6% to 99.0%) when training LeNet5 under identical hyperparameters but with different random seeds.</p>
<p>Foundation Models and Generative AI</p>
<p>Foundation Models and Generative AI are further subsets within deep learning.Foundation models (e.g., large language models) are trained on vast datasets using substantial computational resources.Reproducing or fully retraining such models is often infeasible, leading researchers to rely on downstream evaluations or partial reimplementations for validation.Reproducibility challenges include incomplete documentation of training data, limited access to computational infrastructure, and difficulties in accurately comparing different implementations of the same model architecture.As noted by Miikkulainen (2024), these models represent a significant shift in AI development that challenges traditional evaluation practices, requiring new approaches to validation.</p>
<p>Reproducibility Landscape: From Concepts to Implementation</p>
<p>The reproducibility landscape in AI/ML research can be categorized into conceptual frameworks, standards/certification systems, and practical tools/platforms.Understanding these categories helps position our contribution within existing efforts.</p>
<p>Conceptual Frameworks and Definitions</p>
<p>At the foundational level, researchers have proposed various conceptual frameworks defining what reproducibility means.For example, Goodman et al. (2016) proposed "methods reproducibility," "results reproducibility," and "inferential reproducibility" to distinguish different aspects of validation.Similarly, Gundersen (2021) offered a taxonomy based on documentation types (R1-R4) and degrees of reproducibility (outcome, analysis, interpretation).These conceptual frameworks provide the theoretical foundation for reproducibility efforts.The proposed reproducibility framework falls under this category.</p>
<p>Standards and Certification Systems</p>
<p>Building on conceptual foundations, standards and certification systems provide operational criteria for evaluating reproducibility.</p>
<p>Implementation Tools and Platforms</p>
<p>The third category comprises practical tools and platforms supporting reproducible practices.For example, Papers with Code (2025) links research papers to implementation code, creating verifiable connections between claims and implementations.Other platforms like Weights &amp; Biases (2025) and Neptune.ai (2025) provide experiment tracking infrastructure that automatically documents research artifacts.</p>
<p>The Reproducibility Framework</p>
<p>Components of Research Studies</p>
<p>Gundersen (2021) proposes his definitions for reproducibility terms by referring to the general scientific method in research studies.We use Gundersen's approach while narrowing the focus on the key components incorporated in a research publication.</p>
<p>A research publication consists of the components, as visualized in Figure 2: 1. Claim/hypothesis: Represents the central assertion or prediction that the study aims to investigate.It is the result that the authors are trying to prove.</p>
<p>2.</p>
<p>Experiment design: This is the structured plan devised to test the claim or hypothesis.It encompasses the selection of methodologies for data collection, data pre-processing, model building, and validation.This stage establishes the research protocol, detailing how the experiment will be conducted to ensure valid and reliable results.</p>
<ol>
<li>
<p>Experiment implementation: Refers to the practical execution of the experiment according to the predefined design.This phase involves the operational aspects of the study, such as collecting data, coding and scripting for analysis or model building, and utilizing necessary hardware or software resources.</p>
</li>
<li>
<p>Experiment outcomes: These are the raw results obtained from the experiment.These outcomes are the direct observations or data points collected during the implementation phase, before any analysis or interpretation.</p>
</li>
<li>
<p>Analysis of outcomes: This is the step of examining and interpreting the experiment outcomes through various analytical techniques.This may include statistical testing, the creation of summary tables and charts, and other methods to extract insights and understand the data's implications regarding the hypothesis.</p>
</li>
</ol>
<p>Study conclusion:</p>
<p>Refers to the final assessment of the hypothesis based on the analyzed outcomes.This stage involves synthesizing the findings to determine whether the initial claim is supported or refuted by the experimental evidence, culminating in a clear statement about the validity of the claim.</p>
<p>The Validation Spectrum</p>
<p>We now introduce our reproducibility framework by defining key types of validation studies summarized in Table 1.</p>
<p>Repeatability Repeatability refers to the ability to obtain consistent results by the same team using the same experimental setup, including the claim or hypothesis, experiment design, implementation, outcomes, analysis of outcomes, and conclusions.This means that when the original researchers re-execute their experiment under the same conditions, they achieve the same findings.</p>
<p>Reproducibility: Reproducibility involves external researchers validating the correctness of an original experiment's findings by following the documented experimental setup.This can be achieved through direct use of the original data and code, called dependent reproducibility, or by reimplementing the experiment, titled independent reproducibility, ensuring the results are reliable and applicable across different teams.Importantly, validating the correctness of the implementation in both dependent and independent repro-  Direct Replicability: Direct Replicability is when an independent team intentionally varies the implementation of an experiment-while keeping the hypothesis and experimental design consistent with the original study-to verify the results.This deliberate alteration may entail using different datasets, methodologies, or analytical approaches.The objective is to affirm the original findings under slightly altered experimental conditions, but not entirely new ones, ensuring that the study's conclusions are not solely contingent on the original set of experimental parameters.</p>
<p>Conceptual Replicability: Conceptual Replicability refers to the process where an independent team tests the same hypothesis through a fundamentally new experimental approach.Unlike direct replicability, which alters only the implementation, conceptual replicability involves redesigning the experimental setup itself.This approach aims to validate the hypothesis in broader or different contexts by significantly deviating from the original study's design.Let us now explore these validation types in more detail.</p>
<p>The Role of Repeatability</p>
<p>Repeatability is the exercise of ensuring that results are consistently achievable under the same experimental conditions by the original researchers.When different researchers repeat the experiment without verifying its correctness, we term this external repeatability.</p>
<p>External repeatability, while extending the concept to include independent researchers, does not materially enhance the trustworthiness or reliability of the findings beyond what is achieved by the original team's repetition.The absence of a thorough examination of the experiment's design or its outcomes means that this effort remains a basic repetition rather than a deepened form of validation.</p>
<p>Reproducibility for Validating Experiment Correctness</p>
<p>Reproducibility transcends the mere blind regeneration of results; it fundamentally involves a critical validation of the original study's implementation correctness.This process ensures that, when an experiment implementation is as described and without material flaws, it consistently leads to the same conclusions, affirming the reliability of the original findings.</p>
<p>Clarifying the Scope of Reproducibility</p>
<p>Reproducibility focuses on validating the correct implementation of the experiment design and its ability to consistently lead to the original study's conclusions.It does not assess whether the experimental design is the most appropriate or the only option for testing the hypothesis; this validation is categorized under replicability in our framework.</p>
<p>Pathways to Achieving Reproducibility</p>
<p>There are two main pathways to pursue a reproducibility study:</p>
<ol>
<li>
<p>Dependent Reproducibility: This approach uses the original study's code and data to recreate the results and relies on the availability of these original artifacts.</p>
</li>
<li>
<p>Independent Reproducibility: Entails a researcher independently reimplementing the experiment based on the published descriptions, without reliance on the original code and data.</p>
</li>
</ol>
<p>Independent reproduction stands as a more rigorous validation method compared to dependent reproduction due to its requirement for the reproducer to independently parse and execute the original experiment's design.This method deepens the validation process, making it more probable to identify flaws or limitations in the original findings than when relying on dependent reproduction.</p>
<p>Navigating Common Scenarios</p>
<p>For further clarity around our definition of reproducibility, consider the following scenarios:</p>
<p>Discovering Flaws in the Original Study: Reproduction attempts of AI/ML research may reveal flaws in the original study, such as data leakage or insufficient control of experimental variables, affecting outcomes significantly.</p>
<p>If corrections to these flaws change the study's conclusions, it indicates a failure in reproducibility, pointing to problems with the original implementation's accuracy.</p>
<p>This underscores that reproducibility goes beyond mere re-execution of an experiment by different individuals.It requires an independent verification of the experiment's implementation for correctness.</p>
<p>Adherence to Experiment Methodology:</p>
<p>Reproducibility is achieved by closely following the original experiment's methodology, where minor modifications are permitted but should not be confused with direct replication.For instance, translating code from TensorFlow to PyTorch, while a technical modification, falls under the reproduction category if it follows the original experiment design and hypothesis testing.These minor changes do not constitute direct replication, which involves a deliberate alteration in the experiment's implementation to test the robustness of findings under varied conditions.</p>
<p>Replicability for Robustness and Generalization</p>
<p>The overarching goal of replicability, encompassing both direct and conceptual approaches, is to validate the robustness and generalization of a study's findings.By testing the original results through intentional alterations in experiment implementation, called direct replicability, or entirely new experimental designs, titled conceptual replicability, researchers aim to confirm that the conclusions drawn are not merely artifacts of specific experimental setups but hold across different contexts and approaches.Replicability can help understand the boundaries within which the original findings hold.Direct replicability focuses on intentional modifications to an experiment's implementation while adhering to its original design and hypothesis.These modifications may include using alternative datasets, analytical methods, computational tools, or statistical techniques, aiming to test the stability of the findings under slight but deliberate variations.</p>
<p>Conversely, conceptual replicability represents a more extensive departure from the original experiment by adopting entirely new designs to explore the same hypothesis.This approach allows for testing the hypothesis under significantly different conditions or assumptions, potentially uncovering new insights, or challenging the original study's conclusions.</p>
<p>It is important to distinguish conceptual replicability from corroboration.While corroboration involves gathering supportive evidence from multiple studies to reinforce a theory, conceptual replicability specifically examines the hypothesis's validity across diverse experimental designs, without necessarily aiming to support the overarching theory directly.</p>
<p>Validation Hierarchy and Scientific Rigor</p>
<p>Let us now explore the implications of the different types of validation studies on scientific rigor and reliability.As illustrated in Figure 3, the validation hierarchy from repeatability to conceptual replicability represents a continuum of increasing scientific rigor and reliability.Repeatability establishes the baseline integrity of research findings, ensuring that results are not artifacts of chance.It is the essential first check for any study, affirming internal consistency.</p>
<p>Dependent Reproducibility and Independent Reproducibility elevate scrutiny by testing if the findings hold when the experiment is recreated.Dependent reproducibility involves using the original materials and validating the correctness of the implementation as described in the study.Independent reproducibility is achieved by reconstructing the experiment based on the original This validation hierarchy illustrates that while repeatability and reproducibility are fundamental for trust in the original experiment's findings, direct and conceptual replicability are crucial for demonstrating the findings' robustness and generalization to methodological changes.Importantly, each higher level of validation can supersede the previous level.For instance, if a study's results are independently reproducible-even if not dependently reproducible-or directly replicable in the absence of the ability to reproduce using the original dataset, it underscores the reliability of the findings at a higher degree of scientific scrutiny.Each step increases the rigor and provides a deeper understanding of what the research achieves and its limitations in the face of expanding scientific scrutiny.</p>
<p>Framework Analysis and Validation Through Case Studies</p>
<p>Our framework both builds upon existing reproducibility approaches and provides novel contributions to address the terminological confusion in the field.This section analyzes how our framework relates to prominent existing approaches and presents case studies demonstrating its practical application.</p>
<p>Comparative Analysis with Existing Frameworks</p>
<p>Gundersen's Outcome, Analysis, and Interpretation Reproducibility: Gundersen's categorization focuses on which aspects of a result are validated: outcomes (same results), analysis (same analysis despite different outcomes), or interpretation (same conclusions despite different outcomes and analyses).</p>
<p>In contrast, our framework emphasizes how validation is conducted-through repeatability, reproducibility, or replicability-regardless of which aspects are confirmed.</p>
<p>Integrating these approaches yields a more comprehensive evaluation system.For example, an independent reproduction study could achieve outcome reproducibility (identical results), analysis reproducibility (different outcomes, but same analysis), or inferential reproducibility (same conclusions through different outcomes and analysis).This integration clarifies both the validation method employed and the specific aspects confirmed, providing a richer description of scientific reliability.</p>
<p>Gundersen's R1-R4 Documentation Levels: Gundersen's R1-R4 levels categorize studies based on documentation completeness: R1 (description only), R2 (description and code), R3 (description and data), and R4 (description, data, and code).These levels determine what validation types are practically feasible.Gundersen's framework complements ours by highlighting the prerequisites for different validation types.</p>
<p>Goodman's Reproducibility Lexicon: Goodman's framework distinguishes methods reproducibility (same procedures), results reproducibility (same outcomes with new data), and inferential reproducibility (same conclusions).This parallels Gundersen's outcome-analysisinterpretation categorization but with different terminology.The key distinction is our emphasis on the hierarchy of validation rigor, establishing that conceptual replicability provides stronger evidence than direct replicability and dependent reproducibility surpasses repetition.</p>
<p>Heil's Bronze, Silver, Gold Standards: Heil's standards focus on automating reproduction: bronze (manual effort required), silver (semiautomated), and gold (fully automated one-click regeneration of results).A critical distinction in our framework is that repeatability, even when fully automated per Heil's gold standard, offers limited scientific assurance without thorough investigation of implementation correctness.The ease of regenerating results does not substitute for critical evaluation of methodological soundness.</p>
<p>Reproducibility Framework in Practice</p>
<p>This section presents case studies demonstrating validation attempts across our framework's hierarchy.These examples illustrate how different validation levels from repeatability to conceptual replicability provide increasing scientific evidence and reveal distinct insights, while adapting to diverse research contexts from traditional ML studies to foundation models where conceptual replicability may be the only viable approach.</p>
<p>Example 1: From Repeatability to Reproducibility</p>
<p>The critical distinction between repeatability and reproducibility is starkly illustrated by a highprofile study by Just et al. (2017) that claimed to identify suicide risk from brain scans using machine learning.The paper reported achieving 91% accuracy in identifying suicidal ideation from fMRI data, garnering significant media attention and clinical interest.The authors provided code and a portion of their study data for public access.</p>
<p>When Verstynen and Kording (2023) attempted to reproduce study findings, they discovered fundamental flaws in the methodology.Their examination revealed information leakage in the machine learning pipeline; the features of the classifier were effectively tuned to the particular dataset, with information from the holdout set inappropriately influencing the cross-validation process.This methodological flaw meant the reported 91% accuracy was a substantial overestimation, eventually leading to the paper's retraction.</p>
<p>This type of data leakage is not an isolated problem.A comprehensive survey by Kapoor and Narayanan (2023) identified 294 studies across 17 scientific fields affected by various forms of data leakage, often resulting in significantly overoptimistic conclusions.In a case study of civil war prediction, they found that when leakage errors were corrected, complex ML models showed no substantive performance advantage over decadesold logistic regression approaches-contradicting previously published findings.</p>
<p>This case exemplifies why reproducibility involves more than merely re-executing experiments.By critically examining the implementation's correctness, reproduction efforts revealed fatal methodological flaws that invalidated the study's conclusions.This underscores why our framework distinguishes between repeatability and reproducibility, with the latter providing substantially stronger scientific evidence through methodological validation.</p>
<p>Example 2: From Reproducibility to Replicability</p>
<p>The distinction between reproducibility and direct replicability is clearly demonstrated in a study by Zech et al. (2018) examining deep learning models for pneumonia detection.The researchers developed convolutional neural networks (CNNs) that achieved excellent performance (AUC up to 0.931) when validated on test sets from the same hospital systems used for training.However, when attempting direct replication by applying these models to other hospital systems with different demographic distributions, performance degraded significantly.</p>
<p>Further investigation revealed the models were primarily detecting hospital-specific features rather than generalizable medical findings, with CNNs identifying the source hospital with 99.95%+ accuracy.Differences in disease prevalence between institutions created confounding variables that the models exploited but failed to generalize across systems.</p>
<p>This case illustrates why validation must progress beyond reproducibility to replicability.While the pneumonia detection models satisfied reproducibility criteria by performing consistently within their original context, they failed at direct replicability when the implementation context changed.This limitation became apparent only through direct replication efforts, highlighting how our validation hierarchy provides increasingly valuable insights about the robustness and generalizability of AI/ML research by placing higher importance on direct and conceptual replicability.</p>
<p>Example 3: Conceptual Replicability Revealing Limitations</p>
<p>The Synthetic Minority Over-sampling Technique (SMOTE) by Chawla et al. (2002) provides an instructive example where successful replication studies refined understanding of a widelyadopted technique's limitations.SMOTE, which generates synthetic examples for minority classes to address class imbalance, gained widespread adoption after its introduction, with numerous studies reproducing its reported benefits across various classification tasks.</p>
<p>Conceptual replication studies, which tested the same hypothesis using fundamentally different experimental designs, revealed important nuances in SMOTE's effectiveness.For instance, Van Hulse et al. (2007) showed that simpler alternatives like random undersampling sometimes outperformed SMOTE, particularly for certain classifier types and evaluation metrics.A comprehensive study by Abdelhamid and Desai (2024) discovered substantial variability in performance of class imbalance handling techniques across diverse datasets, emphasizing the need for dataset-specific analysis in choosing the best class imbalance handling technique.</p>
<p>This case demonstrates how conceptual replication -the highest level in our validation hierarchy -can reveal nuanced understanding beyond what reproduction or direct replication might uncover.By testing the central hypothesis through diverse experimental approaches, these studies identified boundary conditions and comparative limitations that refined the research community's understanding of when and how to apply SMOTE effectively.</p>
<p>Example 4: Conceptual Replicability for Foundation Models</p>
<p>Large Language Models (LLMs) and other generative AI systems present unique validation challenges that necessitate conceptual replicability as the primary validation approach.As Miikkulainen (2024) notes, these models represent a paradigm shift in AI development where "each such model is a one-off -not unlike a human that is shaped by their experience...We have to learn to do science with a sample of one."</p>
<p>The spectrum of model accessibility, from proprietary such as ChatGPT-4 from OpenAI ( 2023), to open-weights such as Llama 3 by Meta AI (2024), to fully open-source, makes traditional reproducibility approaches often impractical or impossible.The extraordinary computational re-quirements, proprietary training data, and sheer complexity of these systems mean that even with complete documentation, dependent reproduction would remain infeasible for most research teams.For such cases, validation adapts through behavioral testing, capability verification on smaller datasets, architectural validation at reduced scales, and comprehensive process documentation.</p>
<p>In response, the research community has developed standardized benchmarks and evaluation frameworks that validate model capabilities without requiring access to implementation details.Platforms like the Open LLM Leaderboard by Hugging Face (2023), HELM by Liang et al. (2022), MMLU by Hendrycks et al. (2020), and Chatbot Arena by Chiang et al. (2024) represent conceptual replicability approaches, as they test the central hypotheses about model capabilities through experimental designs different from those used during development.</p>
<p>This case demonstrates the critical role of conceptual replicability in modern AI validation.When other validation approaches become technically or economically infeasible, conceptual replicability provides a path for maintaining scientific rigor by ensuring claims about AI capabilities remain testable and falsifiable, even when working with systems whose inner workings remain inaccessible.</p>
<p>Limitations and Boundaries of the Framework</p>
<p>While our reproducibility framework provides structured guidance for validation in AI/ML research, it has important limitations regarding its scope and application.</p>
<p>Limited Applicability to Non-Empirical Research</p>
<p>Our framework is designed primarily for empirical, data-centric AI/ML research and has limited applicability to theoretical work (focused on mathematical proofs), algorithmic innovations without extensive experimental validation, hardwarespecific research requiring proprietary equipment, and AI ethics or philosophical inquiries.For these categories, different validation approaches would be necessary.</p>
<p>Beyond Correctness: What Validation Does Not Address</p>
<p>Our framework validates correctness, robustness, and generalizability but does not address several critical dimensions of AI/ML research: fairness and equity across demographic groups, ethical implications including potential misuse, broader societal consequences, and alignment with human values.These considerations require complementary frameworks beyond traditional scientific validation.Reproducibility is necessary but insufficient for responsible AI/ML research.</p>
<p>Broader Reward Systems and Incentives</p>
<p>While our framework clarifies definitions, terminological confusion is only one factor in the reproducibility crisis.Broader academic and industry reward systems significantly influence reproducibility practices, including publication incentives favoring novelty, resource constraints discouraging thorough documentation, and commercial considerations limiting data sharing.Addressing these systemic challenges is beyond our scope but essential for comprehensive resolution of the reproducibility crisis.</p>
<p>Conclusions</p>
<p>In response to the challenges posed by the reproducibility crisis within the artificial intelligence and machine learning domain, our work has established a clear framework and definitions for critical terms such as repeatability, reproducibility, and replicability.By differentiating between these key concepts and exploring their implications for scientific rigor and reliability, we aim to clarify their meanings and underline their vital role in enhancing trust in scientific research.Ultimately, our goal is to foster a scientific environment where findings are not only repeatable and reproducible but also robustly replicable across various contexts, thereby advancing the field with trustworthy and verifiable knowledge.</p>
<p>Figure 2 :
2
Figure 2: The research publication workflow</p>
<p>Figure 3 :
3
Figure 3: Hierarchy of validation studies in research</p>
<p>Table 1 :
1
Defining types of validation studies
ResearchersClaim /ExperimentExperimentExperimentAnalysisConclusionsHypothesisDesignImplemen-OutcomesoftationOutcomesRepeatabilitysamesamesamesamesamesamesameReproducibilitydifferentsamesamesamesamesamesameDirectdifferentsamesamedifferentdifferentsame /sameReplicabilitydifferentConceptualdifferentsamedifferentdifferentdifferentsame /sameReplicabilitydifferentducibility distinguishes these efforts from mere re-peatability, enhancing the study's scientific rigor.</p>
<p>Balancing the scales: A comprehensive study on tackling class imbalance in binary classification. M Abdelhamid, A Desai, arXiv:2409.19751Artifact review and badging. Association for Computing Machinery2024. 2023arXiv preprint</p>
<p>1,500 scientists lift the lid on reproducibility. M Baker, Nature. 53376042016</p>
<p>L A Barba, arXiv:1802.03311Terminologies for reproducible research. 2018arXiv preprint</p>
<p>Wavelab and reproducible research. J B Buckheit, D L Donoho, 1995Springer</p>
<p>Replicability, reproducibility, and robustness: comments on harry collins. N Cartwright, History of Political Economy. 2311991</p>
<p>Smote: synthetic minority over-sampling technique. N V Chawla, K W Bowyer, L O Hall, W P Kegelmeyer, Journal of artificial intelligence research. 162002</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. W.-L Chiang, L Zheng, Y Sheng, A N Angelopoulos, T Li, D Li, B Zhu, H Zhang, M Jordan, J E Gonzalez, Forty-first International Conference on Machine Learning. 2024</p>
<p>Electronic documents give reproducible research a new meaning. J F Claerbout, M Karrenbach, SEG technical program expanded abstracts 1992. 1992Society of Exploration Geophysicists</p>
<p>An open, large-scale, collaborative effort to estimate the reproducibility of psychological science. O S Collaboration, Perspectives on Psychological Science. 762012</p>
<p>Repeatability in computer systems research. C Collberg, T A Proebsting, Communications of the ACM. 5932016</p>
<p>Replicability is not reproducibility: nor is it good science. C Drummond, Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML. the Evaluation Methods for Machine Learning Workshop at the 26th ICMLNational Research Council of Canada Montreal, Canada20091</p>
<p>Collective knowledge. G Fursin, Philosophical Transactions: Mathematical, Physical and Engineering Sciences. 3792021. 2197</p>
<p>What does research reproducibility mean?. S N Goodman, D Fanelli, J P , Science translational medicine. 83412016Ioannidis</p>
<p>The fundamental principles of reproducibility. O E Gundersen, Philosophical Transactions of the Royal Society A. 379202002102021. 2197</p>
<p>Reproducibility standards for machine learning in the life sciences. B J Heil, M M Hoffman, F Markowetz, S.-I Lee, C S Greene, S C Hicks, Nature Methods. 18102021</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Open llm leaderboard. 2020. 2023arXiv preprint</p>
<p>S Hunold, arXiv:1511.04217.ICLRICLR 2022 Author Guide and Reproducibility Statement Requirement. 2015. 2022. February 28, 2025arXiv preprintA survey on reproducibility in parallel computing</p>
<p>Icml, ICML 2024 Paper Guidelines. 2024. February 28, 2025</p>
<p>Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. R Islam, P Henderson, M Gomrokchi, D Precup, arXiv:1708.041332017arXiv preprint</p>
<p>Machine learning of neural representations of suicide and emotion concepts identifies suicidal youth. M A Just, L Pan, V L Cherkassky, D L Mcmakin, C Cha, M K Nock, D Brent, Nature Human Behaviour. 1122017</p>
<p>Leakage and the reproducibility crisis in machine-learningbased science. S Kapoor, A Narayanan, Patterns. 492023</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Llama 3: Next-Generation Large Language Model. A I Meta, 2024. February 28, 2025URL or arXiv if/when available</p>
<p>Generative ai: An ai paradigm shift in the making?. R Miikkulainen, AI Magazine. 4512024</p>
<p>Neptune, neptune.ai -Metadata Store for MLOps, Experiment Tracking, and Model Registry. 2025. February 28, 2025</p>
<p>NeurIPS 2021 Paper Checklist. Neurips, 2021. February 28, 2025</p>
<p>ChatGPT: Optimizing Language Models for Dialogue. Openai, 2023. February 28, 2025</p>
<p>Papers with Code. 2025. February 28, 2025</p>
<p>A statistical definition for reproducibility and replicability. P Patil, R Peng, J Leek, 2016, 07</p>
<p>Reproducible epidemiologic research. R D Peng, F Dominici, S L Zeger, American journal of epidemiology. 16392006</p>
<p>Problems and opportunities in training deep learning software systems: An analysis of variance. H V Pham, S Qian, J Wang, T Lutellier, J Rosenthal, L Tan, Y Yu, N Nagappan, Proceedings of the 35th IEEE/ACM international conference on automated software engineering. the 35th IEEE/ACM international conference on automated software engineering2020</p>
<p>Reproducibility vs. replicability: a brief history of a confused terminology. H E Plesser, Frontiers in neuroinformatics. 11762018</p>
<p>A step toward quantifying independently reproducible machine learning research. E Raff, Advances in Neural Information Processing Systems. 201932</p>
<p>Experimental perspectives on learning from imbalanced data. J Van Hulse, T M Khoshgoftaar, A Napolitano, Proceedings of the 24th international conference on Machine learning. the 24th international conference on Machine learning2007</p>
<p>Overfitting to 'predict'suicidal ideation. T Verstynen, K P Kording, Nature human behaviour. 752023</p>
<p>Weights, Biases, Weights &amp; BiasesExperiment Tracking, Visualization, and Collaboration for Machine Learning. 2025. February 28, 2025</p>
<p>Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. J R Zech, M A Badgeley, M Liu, A B Costa, J J Titano, E K Oermann, PLoS medicine. 1511e10026832018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4558 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4558</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4558</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-72faa912691bc2d8dc342fc1c9b53cccc9ba56fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/72faa912691bc2d8dc342fc1c9b53cccc9ba56fc" target="_blank">LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context</a></p>
                <p><strong>Paper TL;DR:</strong> LiveIdeaBench is introduced, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts, and it is revealed that the scientific idea generation capabilities measured by this benchmark, are poorly predicted by standard metrics of general intelligence.</p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4558.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4558.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveIdeaBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic benchmark and evaluation framework that measures LLMs' divergent scientific idea generation from single-keyword prompts using a multi-model judge ensemble across five dimensions drawn from Guilford's creativity theory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LiveIdeaBench (Guilford-based divergent thinking benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LiveIdeaBench prompts idea-generating LLMs with single scientific keywords (minimal context) and collects short (target 100-word) scientific ideas. A dynamic panel of judge LLMs (top 10 LiveBench models, subject to diversity constraints) evaluates each idea. The benchmark operationalizes five dimensions (originality, feasibility, clarity, fluency, flexibility). Originality, feasibility and clarity are scored on a 1–10 integer scale by three randomly sampled judge LLMs per idea and averaged; fluency is assessed by comparing multiple outputs from the same model/keyword using a qualitative A–D grade mapped to 10/7/4/1 (see FluencyGradeMapping); flexibility is computed as the 30th percentile of the composite distribution (average of originality, feasibility, clarity and fluency) across keywords to capture a model's performance floor. The system enforces judge independence (exclude the evaluated model from judges), organization-level representation limits (max 20% per org in the panel), response length standardization (<=100 words target, 200 words hard cap), refusal-handling fallback prompts, and periodic refresh of keywords and judge models to reduce data staleness and contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality (novelty of idea), Feasibility (technical/practical implementability and scientific soundness), Clarity (concise, coherent articulation within length constraint), Fluency (diversity/non-redundancy of multiple ideas from same prompt), Flexibility (consistency/generalization across domains measured as 30th percentile of composite scores).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (22 scientific disciplines; dynamic set of 1,180 keywords used in reported run)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Divergent scientific idea generation / hypothesis & research-idea proposals (high-level hypotheses, conceptual ideas, experimental directions)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied to 41 state-of-the-art models over 1,180 keywords. Key quantitative findings reported: weak but significant correlation between LiveIdeaBench (idea-generation) and LiveBench (general intelligence) scores r=0.357, p=0.038, N=41; human expert validation in the PDE domain showed strong alignment for originality (r ≈ 0.82). Pareto analysis revealed trade-offs between originality and feasibility (e.g., claude-3.7-sonnet:thinking high originality/moderate feasibility; nova-pro-v1 high feasibility/lower originality). Some smaller or lower-general-intelligence models (e.g., qwq-32b-preview, mistral-small) scored comparably to much larger/high-LB models on ideation. Idea length had a very weak positive correlation with idea quality (r=0.096, p<0.0001). Each idea was scored by at least 3 judges for originality/feasibility/clarity; fluency comparisons used single sampled judge per model-keyword pair.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Primarily automated (LLM-as-a-judge ensemble) with targeted human expert validation (PDE domain) to assess judge reliability; ensemble averaging (3 judges per idea) used to reduce individual model bias.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Multi-judge sampling and averaging; judge-panel diversity rules and exclusion to avoid self-evaluation; empirical human expert validation on a domain subset (PDE) showing correlation r ≈ 0.82 for originality; statistical reporting (95% CIs) and correlation tests versus LiveBench/general-intelligence metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Temporal comparisons are confounded by a dynamic judge panel and evolving models; LLM-as-judge can be biased (sycophancy, safety alignment) compressing score ranges; judge knowledge boundaries may mis-evaluate genuinely novel ideas; safety refusals can reduce creativity scores (handled with fallback prompts but still problematic); potential for hallucinated ideas requiring human oversight; dynamic updates complicate reproducibility; environmental cost of large-scale runs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>HuggingFace dataset '6cf/liveideabench-v2' (dynamic keyword set of 1,180 keywords across 22 domains at time of reported run); public leaderboard at https://liveideabench.com/.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4558.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4558.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge jury panel / ensemble evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation approach that uses multiple state-of-the-art LLMs as critic-judges to score other models' outputs, leveraging sampling and ensemble averaging to approximate human-like assessments at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-a-Judge ensemble (jury) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Form a judge panel from top-performing LLMs (here: top 10 LiveBench models, subject to org-diversity constraints). For each candidate idea, randomly sample k judges (k=3 for originality/feasibility/clarity) to produce independent scores; average judges' numeric scores to obtain final per-dimension values. For fluency, use a single sampled judge to compare the set of ideas produced by one generator for a keyword. Use strict independence (exclude the candidate model from the judge pool for that evaluation) and ensemble/consensus mechanisms to mitigate idiosyncratic biases of individual judge models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same five dimensions used in LiveIdeaBench (originality, feasibility, clarity, fluency, flexibility) when applied as the judge mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain; judge models expected to have broad scientific coverage but limited by their knowledge boundaries</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation framework for generated scientific ideas/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper reports ensemble judge configuration and internal statistics: each idea evaluated by 3 judges for numeric dimensions; LLM-judge ensemble produced scores with which model rankings and trade-offs (e.g., Pareto front) were computed. Human expert validation in PDE indicated good alignment (r ≈ 0.82 for originality) between LLM-judge scores and human expert judgments in that domain.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-based) evaluation with limited human validation; ensemble of LLM judges approximates human scoring at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Random sampling of multiple judge models per idea, averaging to reduce single-model bias; empirical comparison to human expert ratings in a domain-specific validation (PDE).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Judge models may exhibit sycophancy and alignment-induced biases, compress score ranges or over/under-rate novelty; single-judge fluency assessments can introduce variance; judge knowledge gaps can mis-evaluate truly novel ideas; dynamic judge panel complicates longitudinal comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4558.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4558.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CriticPrompt-1to10</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Originality & Feasibility Critic Prompt (1–10 scoring system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized system prompt used by critic LLMs instructing them to behave as demanding scientific reviewers and return brief analyses plus JSON scores (1–10) for originality, feasibility and clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Critic prompt with 1–10 scoring (three-judge averaging)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Judge LLMs receive a structured system prompt: act as a demanding scientific reviewer and provide (1) a concise textual analysis (<100 words) and (2) a JSON block with integer scores from 1 to 10 for originality, feasibility and clarity. Score ranges are semantically anchored (1–3 poor, 4–6 average, 7–10 excellent). For each idea, three judges are randomly sampled and their numeric scores averaged to compute the final per-dimension score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality, Feasibility, Clarity each scored 1–10.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / cross-domain (applied to ideas across 22 domains in the benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation rubric for plausibility/novelty/clarity of conceptual scientific proposals</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used throughout LiveIdeaBench; provided stable per-dimension numeric signals and enabled cross-model comparisons. Averaging over 3 judges per idea used to produce more robust scores.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM-based scoring; designed to mimic high-standard human peer review via instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Judge sampling (≥3 judges per idea), averaging, and domain-specific human expert validation (PDE) to check alignment for at least originality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on judges' internal scientific knowledge; judges may miss novelty outside their knowledge; alignment/safety constraints can bias judgement; numeric anchors (1–10) may still yield compressed distributions depending on judge calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4558.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4558.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FluencyGradeMapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fluency grading (A–D mapped to integer scale)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A qualitative-to-quantitative mapping to measure idea diversity (fluency) among multiple outputs for the same keyword: D (identical) to A (completely different) mapped to 1/4/7/10.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fluency grading with A–D → 10/7/4/1 mapping</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For a given model and keyword, multiple generated ideas are compared by a sampled judge who assigns one of four qualitative grades: D = academically identical ideas, C = similar ideas addressing similar problems, B = different ideas addressing similar problems, A = completely different ideas addressing different problems. These grades are linearly mapped to an integer 1–10 scale: D→1, C→4, B→7, A→10, harmonizing fluency with the numeric scale used for other dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Diversity and substantive distinctness of ideas produced from the same prompt (fluency).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Metric for diversity of generated hypotheses/ideas</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Implemented across model-keyword pairs; used as one input to composite scores and Pareto visualizations (fluency represented as bubble size/color in figures). Single judge per model-keyword pair used for fluency assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (single sampled LLM judge per model-keyword pair) to compare and grade diversity across outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Qualitative grade mapping chosen for interpretability and then mapped to integer scale for aggregation; no separate large-scale human calibration of mapping reported (human validation performed elsewhere for other metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Single-judge assessment for fluency can introduce variance; discrete 4-level mapping may not capture fine-grained diversity; quality of judge's semantic discrimination affects reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4558.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4558.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flexibility30Pct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flexibility measured as 30th percentile of composite scores</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conservative metric of cross-domain generality: flexibility is computed as the 30th percentile of the per-keyword composite scores (average of originality, feasibility, clarity, fluency) to represent a model's performance floor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Flexibility = 30th percentile of composite per-keyword scores</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute a composite score for each keyword (mean of originality, feasibility, clarity, fluency). The model's flexibility metric is defined as the 30th percentile of that composite-score distribution across all keywords, intended to measure reliable, lower-bound cross-domain performance while avoiding extremes and outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Cross-domain consistency/generalizability; focuses on weaker-performing subset of domains to estimate performance floor.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (22 disciplines evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Aggregate/generalization metric for idea-generation capability</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used in model profiling and comparisons to identify models that sustain reasonable ideation quality across domains versus those with domain-specific strengths; employed in final rankings and visualizations (e.g., Fig. 2, Fig. 5).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated aggregation of LLM-based scores.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Statistical rationale: selecting 30th percentile to capture performance floor without outlier sensitivity; no alternative percentile sensitivity analysis reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Choice of percentile is heuristic and conservative; may understate performance breadth for models with bimodal distributions; sensitive to keyword set composition and dynamic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4558.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4558.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JudgePanelProtocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judge LLM Panel Formation and Independence Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational rules for assembling and applying the judge panel to reduce correlated biases and circular evaluation artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Judge panel formation and sampling protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Panel comprises top 10 LiveBench models with constraints: maximum two models per organization (≤20% share) to reduce vendor bias; when multiple temporal variants exist for the same base model, only the single chosen representative (highest LiveBench score) is selected to avoid redundancy; when evaluating a model, that model is excluded from the judge pool. For each idea, three judges are randomly sampled for originality/feasibility/clarity; a single judge is sampled for fluency comparisons. This protocol aims to ensure judge diversity, independence and reduce circularity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Robustness and independence of judgments; reduction of correlated/systematic biases across judges.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Protocol governing evaluation jury composition and sampling</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Protocol implemented in all reported evaluations; intended to limit organization-level dominance and self-evaluation. The paper notes this design reduces some biases but introduces temporal comparability issues because the panel composition evolves.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated judge selection rules governing automated LLM judging.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Design rationale and implementation; empirical checks via ensemble averaging and downstream human validation in a domain-specific subset (PDE).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dynamic panel changes make longitudinal comparisons difficult; replacement policy may still not fully remove correlated biases given shared training data across models; excluding evaluated model reduces but does not eliminate cross-model influence if judges share datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4558.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4558.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanPDEValidation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert validation (Partial Differential Equations domain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted human-expert validation study used to assess alignment between LLM-judge scores and human expert ratings in a specialized scientific domain (PDEs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert validation for judge alignment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A subset of generated ideas in the Partial Differential Equations (PDE) domain was rated by domain experts and compared to the LLM-judge scores (particularly originality). Correlation analysis quantified alignment between automated LLM-judge scores and human expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Alignment of LLM-judge originality scores with human expert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied mathematics / Partial Differential Equations (PDE)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Validation of evaluation judgments for scientific ideas (domain-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported strong alignment for originality: correlation r ≈ 0.82 between LLM-judge scores and human expert judgments in the PDE validation subset.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated LLM-judge scoring compared against human expert ratings for validation purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation analysis between averaged LLM-judge scores and human expert ratings in domain-specific sample.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Validation limited to a single domain (PDE); cannot guarantee similar alignment in other, especially highly specialized or cutting-edge domains; sample size and expert selection details are in supplementary materials (not fully described in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4558.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4558.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanLLMComparisons_Refs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior human vs. LLM studies (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior empirical studies that compare LLM-generated creative outputs to human outputs and analyze differences in novelty and diversity; these are mentioned for contextual comparison but not executed within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Various prior human vs. LLM comparative studies (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The paper references several prior works: e.g., Rafner et al. (creativity tests comparing LLMs and humans), Meincke et al. (product ideation comparing GPT-4 vs humans), large-scale studies of LLM performance on creativity tasks (100,000 participant-style evaluations), and human-evaluation studies comparing LLM-generated research proposals with researcher proposals. These prior studies use combinations of human raters, experimental comparisons, and task-specific metrics to compare LLM and human creative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Typical criteria in referenced works include novelty/originality, feasibility/practicality, novelty vs. diversity at group level, human preference ratings, and sometimes statistical measures of semantic distance or diversity (e.g., DAT, Divergent Association Task).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General creativity and domain-specific ideation (various domains cited)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Empirical comparative studies of generative outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Summarized findings in the paper's related-work: some studies report LLMs perform comparably or exceed humans on specific creative tasks (novelty, DAT), while other work highlights LLMs' lower group-level diversity (higher homogeneity) and domain-specific limitations. The current paper positions LiveIdeaBench as complementary and more specialized for minimal-context scientific ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Referenced studies use human evaluation (expert or crowd) and automated metrics depending on task; not executed in this paper but used as motivation and context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Varies by cited study (human ratings, semantic-distance metrics, expert review, large-scale experiments); not reproduced in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>The paper notes mixed evidence across prior studies (some show LLM advantage in novelty, others show reduced group diversity), motivating the need for domain- and task-specific benchmarks like LiveIdeaBench.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4558",
    "paper_id": "paper-72faa912691bc2d8dc342fc1c9b53cccc9ba56fc",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "LiveIdeaBench",
            "name_full": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
            "brief_description": "A dynamic benchmark and evaluation framework that measures LLMs' divergent scientific idea generation from single-keyword prompts using a multi-model judge ensemble across five dimensions drawn from Guilford's creativity theory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LiveIdeaBench (Guilford-based divergent thinking benchmark)",
            "evaluation_method_description": "LiveIdeaBench prompts idea-generating LLMs with single scientific keywords (minimal context) and collects short (target 100-word) scientific ideas. A dynamic panel of judge LLMs (top 10 LiveBench models, subject to diversity constraints) evaluates each idea. The benchmark operationalizes five dimensions (originality, feasibility, clarity, fluency, flexibility). Originality, feasibility and clarity are scored on a 1–10 integer scale by three randomly sampled judge LLMs per idea and averaged; fluency is assessed by comparing multiple outputs from the same model/keyword using a qualitative A–D grade mapped to 10/7/4/1 (see FluencyGradeMapping); flexibility is computed as the 30th percentile of the composite distribution (average of originality, feasibility, clarity and fluency) across keywords to capture a model's performance floor. The system enforces judge independence (exclude the evaluated model from judges), organization-level representation limits (max 20% per org in the panel), response length standardization (&lt;=100 words target, 200 words hard cap), refusal-handling fallback prompts, and periodic refresh of keywords and judge models to reduce data staleness and contamination.",
            "evaluation_criteria": "Originality (novelty of idea), Feasibility (technical/practical implementability and scientific soundness), Clarity (concise, coherent articulation within length constraint), Fluency (diversity/non-redundancy of multiple ideas from same prompt), Flexibility (consistency/generalization across domains measured as 30th percentile of composite scores).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cross-domain (22 scientific disciplines; dynamic set of 1,180 keywords used in reported run)",
            "theory_type": "Divergent scientific idea generation / hypothesis & research-idea proposals (high-level hypotheses, conceptual ideas, experimental directions)",
            "human_comparison": false,
            "evaluation_results": "Applied to 41 state-of-the-art models over 1,180 keywords. Key quantitative findings reported: weak but significant correlation between LiveIdeaBench (idea-generation) and LiveBench (general intelligence) scores r=0.357, p=0.038, N=41; human expert validation in the PDE domain showed strong alignment for originality (r ≈ 0.82). Pareto analysis revealed trade-offs between originality and feasibility (e.g., claude-3.7-sonnet:thinking high originality/moderate feasibility; nova-pro-v1 high feasibility/lower originality). Some smaller or lower-general-intelligence models (e.g., qwq-32b-preview, mistral-small) scored comparably to much larger/high-LB models on ideation. Idea length had a very weak positive correlation with idea quality (r=0.096, p&lt;0.0001). Each idea was scored by at least 3 judges for originality/feasibility/clarity; fluency comparisons used single sampled judge per model-keyword pair.",
            "automated_vs_human_evaluation": "Primarily automated (LLM-as-a-judge ensemble) with targeted human expert validation (PDE domain) to assess judge reliability; ensemble averaging (3 judges per idea) used to reduce individual model bias.",
            "validation_method": "Multi-judge sampling and averaging; judge-panel diversity rules and exclusion to avoid self-evaluation; empirical human expert validation on a domain subset (PDE) showing correlation r ≈ 0.82 for originality; statistical reporting (95% CIs) and correlation tests versus LiveBench/general-intelligence metrics.",
            "limitations_challenges": "Temporal comparisons are confounded by a dynamic judge panel and evolving models; LLM-as-judge can be biased (sycophancy, safety alignment) compressing score ranges; judge knowledge boundaries may mis-evaluate genuinely novel ideas; safety refusals can reduce creativity scores (handled with fallback prompts but still problematic); potential for hallucinated ideas requiring human oversight; dynamic updates complicate reproducibility; environmental cost of large-scale runs.",
            "benchmark_dataset": "HuggingFace dataset '6cf/liveideabench-v2' (dynamic keyword set of 1,180 keywords across 22 domains at time of reported run); public leaderboard at https://liveideabench.com/.",
            "uuid": "e4558.0",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge jury panel / ensemble evaluation",
            "brief_description": "An automated evaluation approach that uses multiple state-of-the-art LLMs as critic-judges to score other models' outputs, leveraging sampling and ensemble averaging to approximate human-like assessments at scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-as-a-Judge ensemble (jury) evaluation",
            "evaluation_method_description": "Form a judge panel from top-performing LLMs (here: top 10 LiveBench models, subject to org-diversity constraints). For each candidate idea, randomly sample k judges (k=3 for originality/feasibility/clarity) to produce independent scores; average judges' numeric scores to obtain final per-dimension values. For fluency, use a single sampled judge to compare the set of ideas produced by one generator for a keyword. Use strict independence (exclude the candidate model from the judge pool for that evaluation) and ensemble/consensus mechanisms to mitigate idiosyncratic biases of individual judge models.",
            "evaluation_criteria": "Same five dimensions used in LiveIdeaBench (originality, feasibility, clarity, fluency, flexibility) when applied as the judge mechanism.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cross-domain; judge models expected to have broad scientific coverage but limited by their knowledge boundaries",
            "theory_type": "Evaluation framework for generated scientific ideas/hypotheses",
            "human_comparison": null,
            "evaluation_results": "Paper reports ensemble judge configuration and internal statistics: each idea evaluated by 3 judges for numeric dimensions; LLM-judge ensemble produced scores with which model rankings and trade-offs (e.g., Pareto front) were computed. Human expert validation in PDE indicated good alignment (r ≈ 0.82 for originality) between LLM-judge scores and human expert judgments in that domain.",
            "automated_vs_human_evaluation": "Automated (LLM-based) evaluation with limited human validation; ensemble of LLM judges approximates human scoring at scale.",
            "validation_method": "Random sampling of multiple judge models per idea, averaging to reduce single-model bias; empirical comparison to human expert ratings in a domain-specific validation (PDE).",
            "limitations_challenges": "Judge models may exhibit sycophancy and alignment-induced biases, compress score ranges or over/under-rate novelty; single-judge fluency assessments can introduce variance; judge knowledge gaps can mis-evaluate truly novel ideas; dynamic judge panel complicates longitudinal comparability.",
            "benchmark_dataset": "",
            "uuid": "e4558.1",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CriticPrompt-1to10",
            "name_full": "Originality & Feasibility Critic Prompt (1–10 scoring system)",
            "brief_description": "A standardized system prompt used by critic LLMs instructing them to behave as demanding scientific reviewers and return brief analyses plus JSON scores (1–10) for originality, feasibility and clarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Critic prompt with 1–10 scoring (three-judge averaging)",
            "evaluation_method_description": "Judge LLMs receive a structured system prompt: act as a demanding scientific reviewer and provide (1) a concise textual analysis (&lt;100 words) and (2) a JSON block with integer scores from 1 to 10 for originality, feasibility and clarity. Score ranges are semantically anchored (1–3 poor, 4–6 average, 7–10 excellent). For each idea, three judges are randomly sampled and their numeric scores averaged to compute the final per-dimension score.",
            "evaluation_criteria": "Originality, Feasibility, Clarity each scored 1–10.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General / cross-domain (applied to ideas across 22 domains in the benchmark)",
            "theory_type": "Evaluation rubric for plausibility/novelty/clarity of conceptual scientific proposals",
            "human_comparison": null,
            "evaluation_results": "Used throughout LiveIdeaBench; provided stable per-dimension numeric signals and enabled cross-model comparisons. Averaging over 3 judges per idea used to produce more robust scores.",
            "automated_vs_human_evaluation": "Automated LLM-based scoring; designed to mimic high-standard human peer review via instruction.",
            "validation_method": "Judge sampling (≥3 judges per idea), averaging, and domain-specific human expert validation (PDE) to check alignment for at least originality scores.",
            "limitations_challenges": "Relies on judges' internal scientific knowledge; judges may miss novelty outside their knowledge; alignment/safety constraints can bias judgement; numeric anchors (1–10) may still yield compressed distributions depending on judge calibration.",
            "uuid": "e4558.2",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "FluencyGradeMapping",
            "name_full": "Fluency grading (A–D mapped to integer scale)",
            "brief_description": "A qualitative-to-quantitative mapping to measure idea diversity (fluency) among multiple outputs for the same keyword: D (identical) to A (completely different) mapped to 1/4/7/10.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Fluency grading with A–D → 10/7/4/1 mapping",
            "evaluation_method_description": "For a given model and keyword, multiple generated ideas are compared by a sampled judge who assigns one of four qualitative grades: D = academically identical ideas, C = similar ideas addressing similar problems, B = different ideas addressing similar problems, A = completely different ideas addressing different problems. These grades are linearly mapped to an integer 1–10 scale: D→1, C→4, B→7, A→10, harmonizing fluency with the numeric scale used for other dimensions.",
            "evaluation_criteria": "Diversity and substantive distinctness of ideas produced from the same prompt (fluency).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cross-domain",
            "theory_type": "Metric for diversity of generated hypotheses/ideas",
            "human_comparison": null,
            "evaluation_results": "Implemented across model-keyword pairs; used as one input to composite scores and Pareto visualizations (fluency represented as bubble size/color in figures). Single judge per model-keyword pair used for fluency assessment.",
            "automated_vs_human_evaluation": "Automated (single sampled LLM judge per model-keyword pair) to compare and grade diversity across outputs.",
            "validation_method": "Qualitative grade mapping chosen for interpretability and then mapped to integer scale for aggregation; no separate large-scale human calibration of mapping reported (human validation performed elsewhere for other metrics).",
            "limitations_challenges": "Single-judge assessment for fluency can introduce variance; discrete 4-level mapping may not capture fine-grained diversity; quality of judge's semantic discrimination affects reliability.",
            "uuid": "e4558.3",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Flexibility30Pct",
            "name_full": "Flexibility measured as 30th percentile of composite scores",
            "brief_description": "A conservative metric of cross-domain generality: flexibility is computed as the 30th percentile of the per-keyword composite scores (average of originality, feasibility, clarity, fluency) to represent a model's performance floor.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Flexibility = 30th percentile of composite per-keyword scores",
            "evaluation_method_description": "Compute a composite score for each keyword (mean of originality, feasibility, clarity, fluency). The model's flexibility metric is defined as the 30th percentile of that composite-score distribution across all keywords, intended to measure reliable, lower-bound cross-domain performance while avoiding extremes and outliers.",
            "evaluation_criteria": "Cross-domain consistency/generalizability; focuses on weaker-performing subset of domains to estimate performance floor.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cross-domain (22 disciplines evaluated)",
            "theory_type": "Aggregate/generalization metric for idea-generation capability",
            "human_comparison": null,
            "evaluation_results": "Used in model profiling and comparisons to identify models that sustain reasonable ideation quality across domains versus those with domain-specific strengths; employed in final rankings and visualizations (e.g., Fig. 2, Fig. 5).",
            "automated_vs_human_evaluation": "Automated aggregation of LLM-based scores.",
            "validation_method": "Statistical rationale: selecting 30th percentile to capture performance floor without outlier sensitivity; no alternative percentile sensitivity analysis reported in main text.",
            "limitations_challenges": "Choice of percentile is heuristic and conservative; may understate performance breadth for models with bimodal distributions; sensitive to keyword set composition and dynamic updates.",
            "uuid": "e4558.4",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "JudgePanelProtocol",
            "name_full": "Judge LLM Panel Formation and Independence Protocol",
            "brief_description": "Operational rules for assembling and applying the judge panel to reduce correlated biases and circular evaluation artifacts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Judge panel formation and sampling protocol",
            "evaluation_method_description": "Panel comprises top 10 LiveBench models with constraints: maximum two models per organization (≤20% share) to reduce vendor bias; when multiple temporal variants exist for the same base model, only the single chosen representative (highest LiveBench score) is selected to avoid redundancy; when evaluating a model, that model is excluded from the judge pool. For each idea, three judges are randomly sampled for originality/feasibility/clarity; a single judge is sampled for fluency comparisons. This protocol aims to ensure judge diversity, independence and reduce circularity.",
            "evaluation_criteria": "Robustness and independence of judgments; reduction of correlated/systematic biases across judges.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cross-domain",
            "theory_type": "Protocol governing evaluation jury composition and sampling",
            "human_comparison": null,
            "evaluation_results": "Protocol implemented in all reported evaluations; intended to limit organization-level dominance and self-evaluation. The paper notes this design reduces some biases but introduces temporal comparability issues because the panel composition evolves.",
            "automated_vs_human_evaluation": "Automated judge selection rules governing automated LLM judging.",
            "validation_method": "Design rationale and implementation; empirical checks via ensemble averaging and downstream human validation in a domain-specific subset (PDE).",
            "limitations_challenges": "Dynamic panel changes make longitudinal comparisons difficult; replacement policy may still not fully remove correlated biases given shared training data across models; excluding evaluated model reduces but does not eliminate cross-model influence if judges share datasets.",
            "uuid": "e4558.5",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "HumanPDEValidation",
            "name_full": "Human expert validation (Partial Differential Equations domain)",
            "brief_description": "A targeted human-expert validation study used to assess alignment between LLM-judge scores and human expert ratings in a specialized scientific domain (PDEs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human expert validation for judge alignment",
            "evaluation_method_description": "A subset of generated ideas in the Partial Differential Equations (PDE) domain was rated by domain experts and compared to the LLM-judge scores (particularly originality). Correlation analysis quantified alignment between automated LLM-judge scores and human expert judgments.",
            "evaluation_criteria": "Alignment of LLM-judge originality scores with human expert ratings.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Applied mathematics / Partial Differential Equations (PDE)",
            "theory_type": "Validation of evaluation judgments for scientific ideas (domain-specific)",
            "human_comparison": true,
            "evaluation_results": "Reported strong alignment for originality: correlation r ≈ 0.82 between LLM-judge scores and human expert judgments in the PDE validation subset.",
            "automated_vs_human_evaluation": "Hybrid: automated LLM-judge scoring compared against human expert ratings for validation purposes.",
            "validation_method": "Correlation analysis between averaged LLM-judge scores and human expert ratings in domain-specific sample.",
            "limitations_challenges": "Validation limited to a single domain (PDE); cannot guarantee similar alignment in other, especially highly specialized or cutting-edge domains; sample size and expert selection details are in supplementary materials (not fully described in main text).",
            "benchmark_dataset": "",
            "uuid": "e4558.6",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "HumanLLMComparisons_Refs",
            "name_full": "Prior human vs. LLM studies (mentioned)",
            "brief_description": "Cited prior empirical studies that compare LLM-generated creative outputs to human outputs and analyze differences in novelty and diversity; these are mentioned for contextual comparison but not executed within this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Various prior human vs. LLM comparative studies (referenced)",
            "evaluation_method_description": "The paper references several prior works: e.g., Rafner et al. (creativity tests comparing LLMs and humans), Meincke et al. (product ideation comparing GPT-4 vs humans), large-scale studies of LLM performance on creativity tasks (100,000 participant-style evaluations), and human-evaluation studies comparing LLM-generated research proposals with researcher proposals. These prior studies use combinations of human raters, experimental comparisons, and task-specific metrics to compare LLM and human creative outputs.",
            "evaluation_criteria": "Typical criteria in referenced works include novelty/originality, feasibility/practicality, novelty vs. diversity at group level, human preference ratings, and sometimes statistical measures of semantic distance or diversity (e.g., DAT, Divergent Association Task).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General creativity and domain-specific ideation (various domains cited)",
            "theory_type": "Empirical comparative studies of generative outputs",
            "human_comparison": true,
            "evaluation_results": "Summarized findings in the paper's related-work: some studies report LLMs perform comparably or exceed humans on specific creative tasks (novelty, DAT), while other work highlights LLMs' lower group-level diversity (higher homogeneity) and domain-specific limitations. The current paper positions LiveIdeaBench as complementary and more specialized for minimal-context scientific ideation.",
            "automated_vs_human_evaluation": "Referenced studies use human evaluation (expert or crowd) and automated metrics depending on task; not executed in this paper but used as motivation and context.",
            "validation_method": "Varies by cited study (human ratings, semantic-distance metrics, expert review, large-scale experiments); not reproduced in detail here.",
            "limitations_challenges": "The paper notes mixed evidence across prior studies (some show LLM advantage in novelty, others show reduced group diversity), motivating the need for domain- and task-specific benchmarks like LiveIdeaBench.",
            "benchmark_dataset": "",
            "uuid": "e4558.7",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.018607,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LiveIdeaBench: Evaluating LLMs’ Divergent Thinking for Scientific Idea Generation with Minimal Context</h1>
<p>Kai Ruan ${ }^{1}$, Xuan Wang ${ }^{2}$, Jixiang Hong ${ }^{1}$, Peng Wang ${ }^{3}$, Yang Liu ${ }^{4,5}$, Hao Sun ${ }^{1, <em>}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China<br>${ }^{2}$ ZJU-UIUC Institute, Zhejiang University, Haining, China<br>${ }^{3}$ Bank of China, Beijing, China<br>${ }^{4}$ School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China<br>${ }^{5}$ State Key Laboratory of Nonlinear Mechanics, Institute of Mechanics, Chinese Academy of Sciences, Beijing, China<br></em>Corresponding author</p>
<h4>Abstract</h4>
<p>While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs’ scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.</p>
<h2>Introduction</h2>
<p>The advancement of scientific knowledge relies heavily on creative thinking and the generation of novel hypotheses. The ability to envision new possibilities and formulate testable explanations is crucial for scientific progress. In recent years, LLMs have demonstrated remarkable capabilities in various scientific tasks, from literature analysis to experimental design, suggesting their potential as powerful tools for augmenting scientific discovery [1-3]. Concurrently, machine learning techniques are being applied to forecast future research directions by analyzing the structure and evolution of scientific knowledge networks [4]. As Rafner et al. [5] note, these generative AI systems now</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Overall design of the LiveIdeaBench benchmark. a. Over 1,000 scientific keywords, representing diverse domains, are used in prompts for the Idea LLMs, encouraging divergent thinking and the generation of novel scientific ideas. b. Sampled Judge LLMs evaluate the generated ideas across three primary dimensions: originality, feasibility and clarity, assigning numerical scores to each idea. c. The evaluation panel comprises the top 10 state-of-the-art models selected from LiveBench, ensuring robust assessment through sampling and ensemble scoring. d. Fluency scores are derived by analyzing the diversity and substantive differences among ideas generated from the same keyword (using a randomly sampled judge), while originality, feasibility, and clarity metrics are combined for integrated evaluation. e. Following Guilford's creativity theory, the evaluation methodology assesses five critical dimensions: originality, feasibility, clarity, fluency, and flexibility, with flexibility computed as the 30th percentile of the averaged scores across the other four dimensions. f. The LiveIdeaBench benchmark provides a comprehensive dataset of generated ideas, evaluation metrics, and a dynamic leaderboard tracking the performance of over 40 models, available at https://huggingface.co/datasets/6cf/liveideabench-v2 and https://liveideabench.com/.</p>
<p>perform comparably to humans on some creativity tests and could potentially enhance the creative capabilities of knowledge workers worldwide.</p>
<p>The landscape of artificial intelligence has undergone dramatic transformation since the emergence of ChatGPT in late 2022, with LLMs demonstrating increasingly sophisticated capabilities in scientific contexts. As we approach late 2024, these models have achieved superhuman performance across multiple dimensions - from language comprehension to mathematical reasoning and code generation. The recent success of AlphaGeometry in solving complex geometric problems at IMO gold medalist level [6] exemplifies this remarkable trajectory. Yet, these advances in general intelligence prompt a fundamental question: does the potential for scientific idea generation in LLMs grow in tandem with their analytical capabilities, particularly in the realm of scientific discovery?</p>
<p>This question becomes particularly pertinent when evaluating LLMs' potential contributions to scientific innovation. Current evaluation benchmarks for LLMs in scientific contexts predominantly</p>
<p>rely on rich contextual inputs, such as research paper titles, abstracts, or complete articles. While these approaches effectively assess models' ability to comprehend and synthesize existing knowledge, they are not designed to systematically evaluate a crucial aspect of scientific thinking - the capacity to generate novel ideas from limited information. This limitation is particularly significant, as many groundbreaking scientific discoveries originate from unexpected connections and conceptual leaps. Shi et al. [2] found that the most surprising scientific breakthroughs often emerge when scientists from one disciplinary background present problems to audiences with different perspectives, suggesting value in approaches that can generate diverse conceptual connections.</p>
<p>The need for a specialized benchmark to assess idea generation from minimal context is further underscored by research showing the distinct nature of creative ideation processes. Recent studies in cognitive psychology have questioned earlier assumptions about the relationship between general cognitive abilities and creative performance. While traditional views suggested a threshold relationship between intelligence and creativity [7], meta-analyses by Kim [8] found only a small relationship between these constructs, and more recent work by Weiss et al. [9] concluded that the relationship is likely linear with no evidence supporting a threshold. Given the complex and debated relationship between general intelligence and creative potential in humans, where they appear distinct yet interconnected, it is crucial to investigate whether analogous distinctions exist in LLMs. Evaluating divergent thinking separately allows us to probe if enhancing idea generation capabilities in these systems requires different approaches.</p>
<p>To address this gap, we propose LiveIdeaBench (Fig. 1), a novel evaluation benchmark designed to assess LLMs' capabilities in divergent thinking for scientific idea generation under constrained conditions. Unlike existing benchmarks that predominantly evaluate convergent thinking by requiring models to derive single correct solutions from rich contextual information, LiveIdeaBench is explicitly grounded in Guilford's seminal theory of divergent production [10]. Our methodology operationalizes key principles from this framework: we use single-keyword prompts as minimal context. This constraint encourages models to generate connections and concepts primarily from their internal knowledge representations, rather than synthesizing readily available information from detailed prompts (e.g., abstracts, or complete articles), thus probing their capacity for less scaffolded, more internally-driven ideation relevant to initial brainstorming. This approach is analogous to the broad stimuli used in classic divergent thinking tasks (e.g., the "Utility Test" described by Guilford), to elicit the generation of multiple and varied potential ideas, fostering a "ready flow of ideas" rather than convergence towards a predefined outcome. This approach necessitates that models "produce their own answers," a hallmark of divergent production assessment distinct from selection-based or highly constrained convergent thinking tasks. Furthermore, our evaluation directly measures core divergent production dimensions like fluency (quantity and diversity of ideas, reflecting the capacity to generate a "number of varied responses") and originality (novelty of ideas), as conceptualized by Guilford. While acknowledging that comprehensive scientific creativity involves an interplay of both divergent and convergent processes, LiveIdeaBench focuses on and evaluates the foundational, generative phase: the ability to produce diverse scientific concepts from sparse cues. This capacity is critical for sparking innovation but is often overlooked by conventional benchmarks. Thus, our focus is on assessing this specific aspect of creative potential relevant to scientific discovery, rather than evaluating the entire scientific process, which also involves deep domain expertise and subsequent validation.</p>
<p>We posit that such benchmarking serves as a crucial initial step towards developing effective human-AI hybrid intelligence systems [11-13] for scientific discovery. By systematically assessing LLMs' performance on idea generation tasks, we aim to provide insights into how these models can best function as collaborative tools or creative sparks for human researchers within the scientific process.</p>
<p>Our work makes several key contributions to the field:</p>
<ol>
<li>We introduce a systematic benchmark for evaluating scientific idea generation in LLMs under minimal contextual conditions, framing this evaluation as a foundational step for understanding their potential role in human-AI scientific discovery workflows.</li>
<li>We conduct comprehensive evaluations across 41 state-of-the-art models spanning various architectures and parameter scales, using a diverse set of over 1,000 scientific keywords from 22 distinct scientific domains;</li>
<li>We demonstrate that an LLM's general intelligence metrics do not necessarily correlate with its capacity for idea generation in scientific contexts. This highlights the need for specialized evaluation benchmarks to understand how to best leverage LLMs to complement human capabilities within AI-assisted scientific innovation.</li>
</ol>
<p>Through extensive experimentation, we reveal significant insights into the relationship between models' general intelligence and their idea generation patterns. Our findings suggest that while LLMs have achieved remarkable benchmarks in general intelligence tasks, their performance on idea generation tasks in scientific contexts requires distinct evaluation methodologies. Some models with lower performance on general intelligence benchmarks demonstrate surprisingly strong idea generation capabilities, while others show the opposite pattern. This specialized assessment complements existing benchmarks and provides new insights into how current LLM systems might effectively support human researchers in scientific innovation, for instance, by acting as idea generators or brainstorming copilots.</p>
<h1>Related Works</h1>
<h2>Human Creativity Research</h2>
<p>Early theoretical work on human creativity offers valuable insights for our investigation. In their seminal 1962 work, Getzels and Jackson [14] examined the relationship between intelligence and creativity, particularly among "gifted" students. Their study yielded two crucial findings: first, high IQ does not necessarily equate to high creativity; and second, creativity and intelligence function as relatively independent traits. These observations led to the threshold theory [10, 14], proposing that intelligence is necessary but not sufficient for creativity, with its influence diminishing beyond a certain threshold.</p>
<p>However, subsequent research has yielded mixed evidence for this theory. While Gralewski et al. [15] and Jauk et al. [7] found some empirical support under certain conditions, some studies found no support $[8,16]$, challenging the threshold concept. More recent work by Weiss et al. [9] concluded that the relationship is likely linear with no evidence supporting a threshold. These varying findings suggest that the development of idea generation capabilities might be underpinned by different cognitive mechanisms than general intelligence, a pattern potentially relevant to LLM development as well. It's also noteworthy that the threshold debate has nuances regarding potential versus achievement; some research suggests that while a threshold might (or might not) apply to divergent thinking ability, intelligence may remain related $[7,17]$ or even become more important for translating potential into real-world creative achievements across the entire range [18, 19]. These early studies primarily focused on the relationship between general cognitive abilities and creative potential, often measured via divergent thinking tasks. While much of this debate centers on general creativity, the specific relationship between intelligence and scientific creativity, which heavily</p>
<p>relies on deep domain knowledge and rigorous methodology alongside ideation, remains an area requiring nuanced investigation, though the general finding of creativity and intelligence being distinct capabilities provides a useful starting point for examining LLMs.</p>
<p>Another theoretical strand addresses creativity assessment methodologies. Guilford's influential theoretical framework $[10,20]$ introduced the distinction between divergent thinking (generating varied responses to open-ended prompts) and convergent thinking (finding optimal solutions to well-defined problems). Guilford identified four key aspects of divergent thinking: the ability to generate a large number of ideas (fluency), the capacity to think across different categories (flexibility), the generation of novel ideas (originality), and the development of detailed and refined ideas (elaboration). More recent research by Cortes et al. [21] has noted that traditional creativity assessments typically involve elements of both divergent and convergent thinking rather than isolating either process. Their findings indicate that purportedly divergent and convergent tasks show limited correlation, questioning whether they reflect different components of the same higher-level construct (creativity). This work highlights the complexity of measuring creative processes and informs our approach to evaluating idea generation in LLMs.</p>
<p>Broader theoretical frameworks further enrich this picture. For instance, Margaret Boden [22] distinguishes between psychological creativity (P-creativity, novel to the individual) and historical creativity (H-creativity, novel to humanity), while Teresa Amabile's [23] componential theory highlights the interplay of domain-relevant skills, creativity-relevant processes (including divergent thinking), and intrinsic task motivation. Melvin Rhodes' [24] "4 P's" (Person, Process, Press, Product) provide a holistic view, reminding us that creativity involves the individual creator, the cognitive processes, the environmental influences, and the resulting outcome. While LiveIdeaBench primarily assesses aspects of the creative "process" (specifically idea generation fluency and diversity under minimal constraints), understanding these broader theories helps contextualize its scope and limitations within the larger landscape of scientific creativity, which ultimately demands valuable and impactful products.</p>
<p>These theoretical frameworks from human creativity research have informed modern approaches to evaluating LLMs' idea generation capabilities and guide our assessment methodology in LiveIdeaBench. Contemporary creativity research has expanded assessment methodologies beyond traditional paper-based tests, with Rafner et al. [25] developing CREA, a comprehensive game-based tool for measuring divergent and convergent thinking. Their validation studies across diverse populations have demonstrated the effectiveness of these novel assessment approaches. Benedek's [19] recent work clarifies that creative potential (measured through divergent thinking tasks) explains only limited variance in creative achievement, emphasizing complex interactions between potential, behavior, and environmental factors.</p>
<h1>Evaluating Idea Generation in LLMs</h1>
<p>Recent research has systematically explored LLMs' potential for tasks analogous to human creative processes through various methodological approaches. Rafner et al. [5] provided a critical analysis of creativity in the age of generative AI, noting that while these systems now perform comparably to humans on some creativity tests, important qualitative differences remain. Their analysis emphasized the importance of integrating psychological science with computational approaches to develop more effective creativity support tools, highlighting both the promising capabilities and important limitations of current systems.</p>
<p>Empirical assessments in creative contexts have shown promising results, exploring LLMs' potential through various applications. For instance, Meincke et al. [26] focused on product ideation, comparing the quality and novelty of ideas generated by GPT-4 against those generated by humans.</p>
<p>Comprehensive human evaluation studies [27] have compared LLM-generated research proposals with those from NLP researchers, revealing statistically significant advantages in novelty while maintaining comparable feasibility metrics. Complementing these approaches, Lee et al. [28] conducted an empirical study investigating the impact of using ChatGPT on human creative performance, finding that it significantly enhanced the articulation and creativity of human-generated ideas.</p>
<p>Studies directly examining divergent thinking abilities in LLMs have employed various methodological approaches. Cropley [29] analyzed ChatGPT on the Divergent Association Task and found that while GPT-4 demonstrated strong semantic distance capabilities in generating unrelated words, its inconsistency and predictability highlighted important differences from human creative processes. This research emphasized the distinction between divergent thinking abilities and true creativity, suggesting the importance of understanding both the capabilities and limitations of these systems. Marrone, Cropley, and Medeiros [30] further examined how narrow AI impacts human creativity, identifying both supportive functions and important limitations in creative processes.</p>
<p>Extensive empirical evidence from a study involving 100,000 participants [31] has demonstrated LLMs' competitive performance, sometimes exceeding human levels, on certain general creative tasks like the Divergent Association Task (DAT) and creative composition, validating the assessment of such capabilities but not addressing the specific demands of scientific ideation. However, Wenger and Kenett [32] found that while LLMs performed on par with humans in individual creativity tests, they exhibited significantly lower response diversity at the group level, showing high homogeneity in creative outputs even across different model families. This creative homogeneity represents an important limitation in current systems. Confirming this potential downside, Doshi et al. [33], in an experimental study on story writing, found that while access to generative AI ideas caused stories to be evaluated as more creative individually (especially for less creative writers), these AI-enabled stories were significantly more similar to each other than stories produced by humans alone, pointing to a potential reduction in collective novelty. These findings collectively underscore the importance of evaluating not just the quality but also the diversity and novelty of AI-generated creative content.</p>
<p>The assessment methodology for LLM creativity has evolved through several sophisticated frameworks designed for broad evaluation. For instance, Lu et al. [34] evaluated text-to-code creativity using programming challenges combined with prompting strategies designed to force novel solutions by restricting previously used techniques, though potential reliance on static problem sets raises limitations. Building upon foundational creativity theories [10], Zhao et al. [35] proposed a comprehensive, albeit static, framework adapting established psychological test tasks to assess general creativity across multiple dimensions. These frameworks offer valuable, comprehensive assessments for their respective areas (coding creativity, general creativity tasks), often incorporating both divergent and convergent aspects or multiple task types with richer contextual inputs. In contrast, LiveIdeaBench adopts a more focused approach, specifically designed to probe the initial divergent idea generation phase under conditions of minimal context, thus complementing these broader evaluations by foregrounding the assessment of the capacity for generating diverse possibilities from sparse cues relevant to scientific discovery. The minimal context inherent in our approach makes it particularly suited for probing creativity, relevant to sparking novel research directions.</p>
<p>Recent applications specifically for scientific ideation provide important context for our benchmark. Wang and colleagues' [36] SCIMON system leverages literature retrieval and iterative novelty enhancement to generate scientific ideas. Similarly, Gu and Krenn [37, 38] demonstrated approaches using knowledge graphs and LLMs to suggest research directions that might not emerge through traditional human ideation. These developments highlight the growing interest in LLMs' potential contribution to scientific idea generation and the need for specialized evaluation methods like LiveIdeaBench.</p>
<h1>Chain-of-Thoughts \&amp; Brainstorming of LLMs</h1>
<p>Chain-of-thought (CoT) prompting has emerged as a powerful technique for enhancing LLMs' reasoning capabilities. Following the initial introduction of CoT prompting [39], several advanced variants have been developed [40, 41], enabling models to explore multiple reasoning paths simultaneously. Recent investigations into programming applications [42] have demonstrated that incorporating brainstorming significantly improves LLMs' performance, achieving a more than $50 \%$ increase in solving competition-level problems.</p>
<p>A common thread among these approaches is their implicit reliance on divergent thinking - the ability to generate multiple distinct solutions or paths from a single starting point. While these methods have proven effective at enhancing model performance, they highlight a crucial gap in our understanding: we lack systematic ways to evaluate LLMs' fundamental capacity for divergent thinking. This gap becomes particularly significant as these prompting strategies increasingly depend on models' ability to generate and explore multiple possibilities.</p>
<p>Our work addresses this need through LiveIdeaBench, which specifically evaluates LLMs' divergent thinking capabilities in scientific contexts. By assessing models' ability to generate multiple novel ideas from minimal input, we provide a benchmark for understanding the foundational cognitive abilities that underpin the success of methods like CoT, Tree-of-Thoughts (ToT), and brainstorming. This evaluation becomes increasingly critical as these techniques continue to evolve and rely more heavily on models' creative thinking capabilities.</p>
<h2>Specialized Idea Generation Agents</h2>
<p>Recent years have seen significant advances in automated scientific systems aimed at aiding or accelerating discovery, each offering unique approaches. The AI Scientist framework [43] demonstrated potential for end-to-end research automation, while Nova [44] introduced iterative planning mechanisms for idea development. Systems like ResearchAgent [45] and Scideator [46] have further refined these approaches through knowledge graph integration and systematic recombination of research elements. Pushing the boundaries further, the recent AI co-scientist framework from Google [47] showcases a sophisticated multi-agent system designed to generate novel research hypotheses and detailed research proposals. The system employs specialized agents for generation, reflection, ranking, evolution, proximity analysis, and meta-review to iteratively refine scientific hypotheses through a self-improving cycle, with generated hypotheses later validated through real-world laboratory experiments conducted by human researchers.</p>
<p>In contrast to LiveIdeaBench, these specialized systems typically rely heavily on extensive knowledge bases, complex contextual inputs, or planning processes to generate and refine ideas or research plans. While highly effective for their intended applications, this dependence on rich context and complex architectures makes it challenging to isolate and evaluate their fundamental capabilities for initial idea generation from minimal cues, particularly their capacity for divergent thinking. Additionally, many such systems inherently focus on convergent processes - integrating information to find optimal solutions or designs within existing knowledge frameworks - rather than specifically probing the ability to explore truly novel conceptual combinations from sparse starting points. This limitation is particularly evident in systems like SciPIP [48] and IdeaSynth [49], whose structure encourages integrating existing information (existing papers or ideas) rather than generating diverse possibilities from minimal input.</p>
<p>Our work with LiveIdeaBench takes a fundamentally different approach by evaluating idea generation capabilities from minimal input, allowing us to assess models' raw potential for divergent scientific thinking independent of complex knowledge retrieval or multi-step planning abilities. This</p>
<p>distinction is crucial for understanding the foundational idea generation capabilities of LLMs and their potential role in sparking novel directions early in the scientific discovery process.</p>
<h1>LLMs-as-a-judge</h1>
<p>The challenges in evaluating creative output at scale necessitate automated assessment methods that can maintain human-level judgment quality.</p>
<p>The emergence of LLMs as evaluation tools has opened new possibilities for assessing model outputs at scale. Various evaluation approaches [50-55] have demonstrated the feasibility of using LLMs to evaluate other models' responses, offering advantages in terms of efficiency and cost-effectiveness compared to human evaluation [56]. Recent advances, such as the jury-based framework [57] and reference-guided verdict method [58], have shown promising results in reducing individual model biases and achieving high agreement with human judgments (Cohen's $\kappa$ up to 0.93 ). The development of specialized evaluation models like Prometheus 2 [59] further underscores the growing sophistication in this area.</p>
<p>However, existing LLM-based evaluation approaches face several limitations when applied to creative tasks. First, most approaches focus on evaluating responses against predetermined criteria or reference answers, making them better suited for convergent thinking tasks than divergent thinking assessment. Second, the evaluation of scientific creativity presents unique challenges that go beyond traditional metrics, requiring simultaneous assessment of originality, feasibility, clarity, fluency, and flexibility.</p>
<p>LiveIdeaBench addresses these limitations through a novel evaluation methodology specifically designed for scientific creativity. Our approach combines multiple LLMs in a specialized jury system that evaluates ideas across five key dimensions derived from Guilford's creativity theory: fluency, flexibility, originality, clarity and feasibility. By incorporating domain-specific scientific knowledge and employing a multi-model consensus mechanism, our benchmark aims to achieve more robust and nuanced evaluations of scientific ideas while maintaining the efficiency advantages of automated assessment.</p>
<h2>Results</h2>
<p>We assessed the performance of over 40 language models across multiple dimensions of scientific ideation using our comprehensive evaluation benchmark. A detailed description of the generated idea dataset is provided in Supplementary Note 2, and its constituent fields are listed in Supplementary Table S.1. The evaluation results are visualized in Fig. 2. For detailed outcomes, including raw scores and examples, please refer to Supplementary Table S.2; aggregated performance metrics and model rankings are presented in Supplementary Table S.3.</p>
<p>Diverse Model Performance Across Scientific Domains Our quantitative assessment and comparative analysis reveal distinct variations in model capabilities across scientific disciplines. (Fig. 3) While gemini-2.0-flash-thinking-exp, deepseek-r1, and claude-3.7-sonnet:thinking demonstrate high overall scores, the pattern of their relative strengths exhibits domain specificity. Importantly, although larger and more recent architectures generally achieve superior results, their advantage is not uniform across disciplines, suggesting that domain expertise and reasoning capabilities are not solely determined by model scale. For example, gemini-2.0-flash-thinkingexp excelled in anthropology ideation; claude-3.7-sonnet:thinking performed best in chemistry, medicine, and data science; while deepseek-r1 demonstrated stronger relative performance in physics.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Performance comparison of models evaluated on LiveIdeaBench. a. Dimensional scores (originality, feasibility, fluency, clarity, and flexibility) and overall performance (red line) for open-weight and proprietary models, with $95 \%$ confidence intervals. b. Multidimensional performance profiles of representative models across the five evaluation dimensions. c. Word cloud visualization of scientific keywords. For detailed scores and $95 \%$ CIs for each model, see Supplementary Table S.3.</p>
<p>Distinction between General Intelligence and Scientific Idea Generation The comparison between LiveIdeaBench and LiveBench metrics (see Fig. 4 and Supplementary Figure S.4) uncovers a notable disconnect between general intelligence and scientific idea generation capabilities, revealing contrasting results patterns across models. While Claude-3.7-sonnet:thinking leads in general intelligence (LiveBench) and also shows high effectiveness on scientific ideation (LiveIdeaBench), its idea generation capabilities are matched by models like qwq-32b-preview (ranked 8/41 in LiveIdeaBench). Notably, qwq-32b-preview exhibits low general intelligence scores, representing a profile of low general intelligence but high scientific ideation capability. As Fig. 4 illustrates, other patterns exist, such as high general intelligence paired with lower scientific ideation capability (e.g.,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Model Performance on LiveIdeaBench Across Scientific Categories. The heatmap displays average performance scores with $95 \%$ confidence intervals for model-discipline combinations. Scientific categories were classified using SciBERT [60] through semantic similarity computation following the framework from [61]. Higher scores (darker blue) indicate better idea generation ability within each discipline. Numbers in parentheses following each scientific category indicate the keyword count associated with that discipline. Categories are sorted by keyword count.
o3-mini-high). These contrasting profiles underscore that scientific idea generation capability, as measured by LiveIdeaBench, is poorly predicted by general intelligence, necessitating specialized benchmarks like LiveIdeaBench for evaluating this aspect of LLM potential.</p>
<p>Trade-offs in Scientific Originality and Feasibility The Pareto front visualization (see Fig. 5) illustrates clear trade-offs between feasibility and originality. While claude-3.7-sonnet:thinking achieves the highest originality with moderate feasibility, nova-pro-v1 demonstrates the opposite pattern. Models like deepseek-r1, qwq-32b, and gemini-2.0-flash-exp exhibit balanced effectiveness between these two dimensions. Particularly, deepseek-r1 stands out for its exceptional all-round capabilities across all measured dimensions of scientific idea generation, demonstrating that balanced effectiveness is achievable despite common tendencies toward specialization. Additionally, while models like claude-3.7-sonnet:thinking and several Gemini variants show strong fluency and flexibility, the mediocre results of o3-mini-high, known for logical reasoning, highlight the specific nature of the capabilities measured by this benchmark.</p>
<p>Independence of Idea Quality from Length As shown in Fig. 6, while the majority of models generally adhered well to the 100-word limit specified in the prompt (Fig. 6b), their mean originality, feasibility, and clarity scores demonstrated significant variations across different models</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Comparison of LiveIdeaBench (upper half, assessing scientific ideation) and LiveBench (lower half, assessing general intelligence, denoted as LB) across various evaluation metrics for the same set of models. The middle rows show contrasting trends between models in average performance. While Claude-3.7-sonnet:thinking achieves the highest average score on LiveBench, its performance on scientific ideation tasks (LiveIdeaBench) is comparable to qwq-32b-preview, which ranks fourth from last on general intelligence metrics. The arrows highlight three representative patterns: Claude-3.7-sonnet:thinking (left) exemplifies high general intelligence combined with high scientific ideation capability; o3-mini-high (middle) shows high general intelligence but low scientific ideation capability; and qwq-32b-preview (right) demonstrates low general intelligence but high scientific ideation capability. These contrasting patterns highlight that LLMs' scientific idea generation capability, as measured by LiveIdeaBench, is distinct from their general intelligence capabilities (e.g., reasoning, coding, and math), underscoring the necessity of LiveIdeaBench for evaluating scientific ideation potential.
(Fig. 6a,c-e ). Further analysis of idea length (see Supplementary Figs. S.1 and S.2) reveals a statistically significant but very weak positive correlation with idea quality ( $r=0.096, p&lt;0.0001$ ). Even for models specifically designed for reasoning, as detailed in Supplementary Note 8, the relationship between the length of their thoughts and the generated idea quality remains very weak. This observation lends further support to the notion that effective idea generation is not merely a function of extensive logical elaboration, distinguishing it from certain aspects of general reasoning.</p>
<h1>Discussion</h1>
<p>To the best of our knowledge, LiveIdeaBench represents the first comprehensive benchmark specifically designed to evaluate LLMs' divergent thinking capabilities in scientific innovation. Existing LLM benchmarks predominantly focus on problem-solving tasks such as logical reasoning, mathematical computation, and code generation. These benchmarks inherently assess convergent</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Pareto front visualization of model performance on LiveIdeaBench. This illustrates the trade-off between feasibility and originality across different language models, with bubble size, color gradient and edge width representing fluency, flexibility and clarity scores, respectively. The distribution reveals a clear Pareto frontier, where claude-3.7-sonnet:thinking achieves the highest originality but moderate feasibility, while nova-pro-v1 demonstrates the opposite pattern. Models such as deepseek-r1, qwq-32b, and gemini-2.0-flash-exp exhibit relatively balanced performance between these two dimensions. In terms of fluency and flexibility, claude-3.7-sonnet, claude-3.7-sonnet:thinking, gemini-2.0-flash-exp, gemini-2.0-pro-exp-02-05 and gemini-2.0-flash-thinking-exp show particularly strong performance. Notably, deepseek-r1 stands out for its exceptional all-round performance across all dimensions of scientific idea generation, suggesting strong performance across the dimensions measured by this benchmark. Moreover, while the o3-mini-high is renowned for its proficiency in logical and mathematical reasoning, it delivered a mediocre performance on this benchmark. For detailed scores and 95% CIs for each model, see Supplementary Table S.3.
thinking-the ability to arrive at predetermined correct answers through structured problem-solving (e.g., selecting the right option in multiple choice questions, completing text with expected words, or fixing code to match specific requirements). This stands in contrast to divergent thinking, which involves generating diverse, novel solutions from minimal contextual input.</p>
<p>Furthermore, our benchmark incorporates mechanisms to address potential data contamination and overfitting issues that commonly plague static benchmarks. Traditional evaluation methods may encourage models to perform well on specific test cases without developing generalizable creative</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Comprehensive Analysis of Idea Quality, Length, and Component Scores. a. Mean composite idea scores (average of Originality, Feasibility, and Clarity) with 95% confidence intervals for different language models. b. Mean idea length in words across models with specific word counts labeled. c-e. Detailed breakdown of performance on individual dimensions: Originality (c), Feasibility (d), and Clarity (e) scores with 95% confidence intervals. For the regression plot between clarity scores and originality, feasibility scores across all generated ideas, see Supplementary Figure S.5.</p>
<p>thinking abilities. Our approach employs a dynamic judge panel comprising multiple state-of-the-art models, randomly sampling multiple LLMs for evaluation and employing ensemble scoring methods. This design not only minimizes individual model biases but also leverages the continually updated knowledge bases of judge models, effectively preventing the limitations associated with fixed benchmarks. This methodology aligns with recent advances in live benchmarking [62, 63], which similarly address data contamination and overfitting concerns through dynamic evaluation mechanisms.</p>
<p>Our analysis through LiveIdeaBench yields several notable insights into LLMs' scientific idea generation capabilities. Most notably, we find that a model's performance on these divergent thinking tasks is not strongly coupled with its performance on general intelligence benchmarks.</p>
<p>For instance, qwq-32b-preview achieves comparable divergence thinking performance to top-tier models despite significantly lower scores on general intelligence benchmarks (confirming that high general intelligence is not a prerequisite for strong performance here). This statistically significant but weak positive correlation ( $r=0.357, p=0.038, N=41$; see Supplementary Figure S.4) suggests that fostering scientific idea generation capabilities in these systems may benefit from distinct development approaches compared to enhancing general problem-solving skills. The varying strengths we observe across different model architectures-particularly in originality versus feasibility trade-offs-point to potential complementarity in scientific applications.</p>
<p>Our findings in Fig. 4 illustrate the relationship between general intelligence and scientific idea generation capabilities. The comparison reveals three distinct performance profiles: models like claude-3.7-sonnet:thinking excelling in both areas; models like o3-mini-high showing high general intelligence but comparatively limited idea generation capability; and crucially, models like qwq-32b-preview exhibiting remarkable scientific divergent thinking capabilities despite relatively lower general intelligence scores. This latter pattern, in particular, highlights that the capabilities measured by LiveIdeaBench are distinct from, and poorly predicted by, general intelligence scores.</p>
<p>These findings indicate that LLMs' divergent thinking capabilities for scientific ideation under minimal context operate largely independently from the convergent thinking abilities typically measured by problem-solving tasks. This distinction could be influenced by several factors, potentially including variations in the relevance of pre-training data to scientific tasks, differences in posttraining methodologies applied, and inherent architectural properties of the models. This observed difference between metrics underscores the importance of specialized evaluation benchmarks like LiveIdeaBench that specifically target these idea generation capabilities, rather than attempting to infer them from general intelligence assessments.</p>
<p>We believe this phenomenon is closely tied to models' pretraining data and post-training procedures. From Fig. 4, we can also observe that mistral-small, which ranks near the bottom in general intelligence, demonstrates remarkably high scientific divergent thinking capabilities. Even more interestingly, the earlier released mistral-small model scored significantly higher in creativity than the newer same-size, same-family model mistral-small-24b-instruct-2501. We also observe that models from the same family—qwq-32b and qwq-32b-preview-score similarly in creativity while differing dramatically in general intelligence. These two pairs of examples strongly suggest that a model's divergent thinking and convergent thinking abilities cannot reliably predict each other.</p>
<p>This pattern holds true even when comparing models from the same family with different parameter counts. In Fig. 4, focusing on mistral-large-2411 and mistral-small, we see that mistral-small still slightly outperforms mistral-large-2411 in scientific divergent think-ing-despite the latter having 123 billion parameters, far exceeding the former's 24 billion. This further suggests that parameter count alone does not appear to be the primary determinant of scientific divergence capabilities, suggesting other factors like training data or architectural nuances play significant roles.</p>
<p>However, several limitations warrant consideration. The use of contemporary state-of-theart models as judges makes temporal comparisons difficult. When the judge panel composition changes with model updates, direct performance comparisons across different evaluation periods become unreliable. While this limitation mirrors challenges faced by other dynamic benchmarks like LiveBench, it limits our ability to track longitudinal trends in model capabilities. Furthermore, our evaluation includes several proprietary, closed-source models (e.g., GPT-40). Access to these models is typically via APIs which may be subject to change, and the underlying models can be updated without public notice, potentially impacting exact reproducibility. While including these models is essential for a comprehensive assessment of the current state-of-the-art, we acknowledge this inherent</p>
<p>limitation regarding reproducibility compared to evaluations focused solely on open-weight models. Moreover, the LLM-as-a-judge approach itself introduces a potential source of bias. Aligned models can exhibit sycophancy [64-66], a tendency to agree with the user or produce agreeable outputs, which may lead to inflated absolute scores and a compressed score range, further reinforcing the need to focus on relative rather than absolute performance rankings. Additionally, we observe an inherent tension between models' safety constraints and creative evaluation. Some models exemplify this behavior by declining to generate ideas for potentially sensitive keywords (e.g., "ecotoxicology"). While such safety measures are crucial, they can negatively impact creativity scores, potentially undervaluing models with stronger ethical constraints. The potential for hallucinations in the generated ideas themselves [67] also underscores the necessity of human oversight in practical applications.</p>
<p>Furthermore, we recognize a fundamental challenge in the reliability of LLM-as-a-Judge approaches. When evaluating scientific ideas containing concepts outside the judge models' knowledge boundaries, these models might misunderstand novel concepts and consequently misjudge their originality or feasibility. While our use of a dynamic panel of state-of-the-art judge models likely provides broader and more current knowledge coverage than static panels, this fundamental limitation persists. While our multi-model ensemble approach mitigates this issue to some extent, more comprehensive solutions could involve retrieval-augmented generation (RAG) approaches that incorporate up-to-date scientific literature. By augmenting LLM judges with the ability to retrieve and reference the latest scientific papers, such approaches could significantly enhance evaluation accuracy, particularly for highly specialized or cutting-edge scientific domains, although implementing effective RAG systems for this purpose presents its own challenges regarding retrieval relevance and integration with the judging process. This represents a promising direction for future work that could further strengthen the reliability of LLM-based creativity assessments. To empirically assess this reliability, we conducted a human expert validation focused on the Partial Differential Equations (PDE) domain (see Supplementary Note 7). The results showed encouraging alignment between human experts and LLM judgments, particularly for originality ( $r \approx 0.82$ ), lending empirical support to the LLM-as-a-judge approach, at least within this specific domain.</p>
<p>Looking ahead, several research directions emerge. To address temporal comparability issues, normalized scoring mechanisms could maintain meaningful cross-temporal comparisons while preserving the advantages of dynamic evaluation. The comprehensive dataset generated through our evaluations offers opportunities for training scientific language models, mining ideation patterns, and investigating novel scientific ideas. Future work must also tackle the challenge of fairly evaluating creative potential while accounting for ethical constraints, possibly through domain-specific scoring adjustments or separate evaluation tracks for models with different safety priorities.</p>
<p>The implications of our findings extend beyond model evaluation to the broader landscape of AI-assisted scientific discovery. As LLMs demonstrate increasingly sophisticated idea generation capabilities, they hold promise as powerful tools for accelerating scientific innovation - from hypothesis generation to experimental design. Our benchmark provides a foundation for understanding and improving these capabilities, potentially enabling more effective human-AI collaboration [11-13] and informing the design of human-aware AI systems for science [68] in pushing the boundaries of scientific knowledge. The creative strengths we observed in different model architectures suggest that a diverse ecosystem of AI tools, each with complementary capabilities, could support different aspects of the scientific process.</p>
<p>These findings and challenges point toward a broader research agenda: understanding how to nurture and evaluate machine creativity while maintaining essential safety guardrails, ultimately in service of advancing scientific discovery. Through continued refinement, LiveIdeaBench aims to serve as a key tool in this evolving landscape of AI capability assessment and scientific innovation.</p>
<p>Finally, a practical consideration is the environmental cost associated with the extensive LLM usage required for comprehensive benchmarking [69]. Using the "EcoLogits Calculator" [70] to estimate the carbon footprint of our benchmark evaluations, we calculate a total emission of approximately $3074 \mathrm{~kgCO}_{2}$ eq for the full evaluation run reported here. While necessary for rigorous assessment, this highlights the significant energy demands of current AI systems. A detailed breakdown of the estimated emissions per model and per role (idea generator vs. judge) can be found in Supplementary Table S.4.</p>
<h1>Methods</h1>
<p>Building upon Guilford's foundational theory of divergent thinking, we develop a comprehensive evaluation methodology (see Fig. 1) that quantitatively assesses five fundamental dimensions in scientific idea generation. While Guilford's original theoretical framework provides theoretical underpinnings, we extend and operationalize these concepts specifically for evaluating LLMs' scientific idea generation capabilities within our LiveIdeaBench benchmark. It is important to note that our methodology evaluates an essential but not exhaustive aspect of scientific creativity, focusing primarily on divergent thinking capabilities.</p>
<h2>Dimensions of Evaluation</h2>
<p>Originality Originality assessment focuses on the uniqueness and novelty of generated ideas. We implement this through our critic system, where judge LLMs evaluate each idea's originality independently (see Fig. 1b and Supplementary Note 1.2). The final originality score for each model is computed as the mean evaluation across all scientific keywords and generated ideas, providing an absolute score that reflects the model's capacity for novel ideation. To ensure assessment reliability, each generated idea is evaluated by a minimum of three randomly assigned critic LLMs from our panel. This multiple-evaluator approach mitigates potential assessment bias that could arise from relying on a single model's judgment, thereby enhancing the objectivity and reliability of our evaluation benchmark.</p>
<p>Feasibility In the context of scientific innovation, the practical implementability and scientific soundness of an idea are paramount. Therefore, our evaluation includes a distinct feasibility dimension, assessing whether a proposed idea is technically achievable and aligns with established scientific principles and constraints. This aligns with the instructions given to the LLMs, which noted feasibility as a key characteristic of a good scientific idea (see Supplementary Note 1.1). Similar to originality and clarity, feasibility scores are determined by our critic system and averaged across all keywords and ideas to produce an absolute metric (see Supplementary Note 1.2). This ensures our benchmark evaluates the practical viability crucial for scientific progress.</p>
<p>Clarity Our evaluation benchmark incorporates a clarity dimension, directly informed by the prompt provided to the idea-generating LLMs, which notes that good scientific ideas should be clearly articulated (see Supplementary Note 1.1). This dimension assesses the quality of the idea's expression, focusing on its coherence, logical flow, and comprehensibility, particularly given the constraint of the 100-word limit which demands concise articulation. While conceptually related to the elaboration aspect in Guilford's creativity theory, our assessment prioritizes effective and understandable communication within the specified format. Like originality and feasibility, clarity scores are determined by our critic system (see Fig. 1b and Supplementary Note 1.2), involving</p>
<p>multiple judges per idea and averaging the results. Assessing clarity acknowledges that the potential impact of a scientific idea depends not only on its novelty and feasibility but also on how effectively it is communicated [28].</p>
<p>Fluency Fluency assessment examines the model's capacity to generate diverse, non-redundant ideas using identical keywords (see Fig. 1d). Through our judge panel, we evaluate the distinctiveness of generated outputs using a letter-grade scoring system: D indicates academically identical ideas; C represents similar ideas addressing similar problems; B denotes different ideas addressing similar problems; and A signifies completely different ideas addressing different problems. To align with the 1-10 integer scale used for all evaluation dimensions, these four qualitative grades are mapped linearly to the integer scores 1 (for D), 4 (for C), 7 (for B), and 10 (for A), respectively. This mapping ensures consistent scaling across dimensions and maintains equal intervals between the assessed qualitative distinctness levels, enabling precise measurement of genuine idea diversity versus surface-level variations (see Supplementary Note 1.3 for prompts). While simpler diversity metrics examining syntax or semantics would require fewer computational resources, we chose LLM-as-a-Judge for its ability to better capture the nuanced differences between genuinely distinct scientific ideas versus superficial variations. For a benchmark specifically designed to evaluate scientific divergent thinking capabilities, this precision is essential.</p>
<p>Flexibility Flexibility measurement evaluates the model's ability to maintain consistent performance across different scientific domains and contexts. Rather than treating flexibility as an independent metric, we derive it from the distribution of the combined scores (averaging originality, feasibility, clarity, and fluency) across various keywords. Following the principle that a system's overall effectiveness is constrained by its weakest performing components, we focus on the 30th percentile of this composite score distribution (see Fig. 1e). This percentile choice provides a robust measure of a model's performance floor while avoiding extreme outliers, enabling us to assess whether its scientific creativity can genuinely generalize to less common or niche domains. The resulting metric identifies models that maintain reliable performance across diverse scientific contexts rather than those exhibiting domain-specific excellence, thus providing a conservative estimate of cross-domain capabilities.</p>
<h1>Scientific Keyword Selection</h1>
<p>Our evaluation leverages a dynamic, continuously updated set of scientific keywords sourced from a real-time analytics database [71]. The current keywords set (as of December 16, 2024) comprises 1,180 high-impact scientific keywords (Fig. 2c) across 22 distinct scientific disciplines, selected based on current search engine engagement metrics. Unlike static benchmarks, LiveIdeaBench updates its keyword database monthly to maintain alignment with emerging scientific trends and research frontiers. This automated refresh mechanism ensures the benchmark consistently reflects contemporary scientific discourse and technological advancement, making it particularly valuable for evaluating LLMs' ability to engage with cutting-edge scientific concepts rather than just established knowledge.</p>
<h2>Model Selection</h2>
<p>LiveIdeaBench maintains a continuously evolving roster of evaluated models by automatically incorporating the top 41 performers from the most recent LiveBench evaluations [62]. This dynamic selection process ensures our benchmark always tests the latest advancements in language model</p>
<p>capabilities. We implement a dual-role system where all models serve as idea generators, while the top 10 performers additionally function as our judge panel (critics), subject to the diversity constraints outlined in the Experimental Protocol. This approach creates a self-updating evaluation benchmark that evolves alongside rapid developments in AI, ensuring that both idea generation and assessment standards reflect current state-of-the-art capabilities. The automatic monthly refresh of both models and evaluation criteria through LiveBench integration helps prevent benchmark staleness and potential gaming of the system, maintaining LiveIdeaBench's relevance as a contemporary measure of scientific creativity.</p>
<p>Our evaluation benchmark currently encompasses 41 state-of-the-art LLMs based on LiveBench's March 2025 results. This includes models from major developers such as Anthropic (claude-3.7sonnet:thinking, claude-3.7-sonnet, claude-3.5-sonnet, claude-3-opus, claude-3.5-haiku20241022) [72]; OpenAI (o3-mini-high, gpt-4.5-preview, o1, o3-mini, o1-mini, gpt-4o-202411-20, gpt-4-turbo, gpt-4o-mini) [73]; Google (gemini-2.0-flash-thinking-exp, gemini-2.0-pro-exp-02-05, gemini-2.0-flash-exp, gemini-pro-1.5, gemini-2.0-flash-lite-001, gemma-227b-it) [74]; Qwen (qwq-32b, qwen-max, qwen2.5-dracarys2-72b, qwen-2.5-72b-instruct, qwen2.5-coder-32b-instruct, qwq-32b-preview, qwen-2.5-7b-instruct) [75-77]; DeepSeek (deepseekr1, deepseek-chat (v3), deepseek-r1-distill-llama-70b, deepseek-r1-distill-qwen-32b) [78-80]; Meta (llama-3.1-405b-instruct, llama-3.3-70b-instruct, llama-3.1-70b-instruct) [81]; Mistral (mistral-large-2411, mistral-small-24b-instruct-2501, mistral-small (v2409)) [82]; Amazon (nova-pro-v1, nova-lite-v1) [83]; StepFun (step-2-16k-202411) [84]; xAI (grok-21212) [85]; and Microsoft (phi-4) [86]. This comprehensive set includes both proprietary and openweight models, spanning diverse architectures, parameter scales, and training methodologies.</p>
<h1>Experimental Protocol</h1>
<p>We implemented several methodological controls to ensure rigorous evaluation:</p>
<ul>
<li>Model Selection Criteria for Idea Generation: To prevent redundancy in the pool of ideagenerating models, we selected only the most recent version of models with multiple temporal variants (e.g., GPT-4o series). Models exhibiting API instability during the evaluation period were also excluded to maintain data quality consistency across all evaluated models.</li>
<li>Judge LLM Panel Formation, Independence, and Application: The judge panel, comprising the top 10 models from LiveBench, is formed while applying specific diversity constraints to mitigate potential correlated biases. To ensure broader representation, we limit the contribution from any single organization to a maximum of $20 \%$ (i.e., 2 models) of the panel; if the initial top 10 includes more than two models from one organization, the lower-ranked ones are replaced by the next highest-ranked eligible models from different organizations. Additionally, when considering model pairs with identical base models that differ primarily in "reasoning effort" (e.g., o3-mini vs. o3-mini-high), we select only one representative for the judge panel (prioritizing the variant with higher general intelligence scores on LiveBench) to avoid redundancy and the potential amplification of biases inherent to that specific base model. Furthermore, to prevent circular dependency during evaluation, we implement strict independence: when evaluating any specific model's generated ideas, that model is explicitly excluded from serving on the judge panel for that evaluation round, ensuring independent assessment free from self-evaluation. This established panel is then utilized through specific sampling procedures for evaluation: For assessing originality, feasibility, and clarity, each individual generated idea is evaluated by a subset of 3 judges randomly sampled from the remaining eligible panel members. The final score for each of these dimensions is the average</li>
</ul>
<p>of the scores provided by these three judges, enhancing assessment robustness. For assessing fluency, which evaluates the diversity of ideas generated for the same keyword by a given model, the comparison is performed by a single judge randomly sampled from the eligible panel members for each keyword-model pair.</p>
<ul>
<li>Response Standardization: All models were prompted to generate ideas within a 100-word target length, with a maximum allowable threshold of 200 words (see Supplementary Note 1.1). Responses exceeding this limit were excluded from analysis to ensure comparative validity across models.</li>
<li>Special Implementations: The reasoning-centric architecture of qwq-32b-preview necessitated a modified protocol, incorporating a " $<em> </em>$ Final Idea: $<em> </em>$ " delimiter for response parsing (see Supplementary Note 1.4). In cases where parsing failed, critic LLMs evaluated the complete reasoning output to maintain assessment comprehensiveness.</li>
<li>Handling Refused Responses: To fairly assess models, especially those with strong safety alignments that might refuse prompts for sensitive keywords, we implemented a two-step refusal handling protocol. If an initial idea generation request is refused (detected via specific keywords detailed in Supplementary Note 5), a fallback prompt reframing the task within an academic context is used for a second attempt. This ensures models are not unduly penalized for safety constraints when they might still be capable of generating relevant scientific ideas under appropriate framing. Further details and refusal rates are provided in Supplementary Note 5 .</li>
</ul>
<h1>Data availability</h1>
<p>All the datasets used to test the methods in this study are available on Huggingface at https: //huggingface.co/datasets/6cf/liveideabench-v2. The dynamic leaderboard is available at https://liveideabench.com/</p>
<h2>Code availability</h2>
<p>All the source codes used to reproduce the results in this study are available on GitHub at https://github.com/x66ccff/liveideabench.</p>
<p>Acknowledgements: The work is supported by the National Natural Science Foundation of China (No. 92270118, No. 62276269), the Beijing Natural Science Foundation (No. 1232009), and the Strategic Priority Research Program of the Chinese Academy of Sciences (No. XDB0620103). In addition, H.S and Y.L. would like to acknowledge the support from the Fundamental Research Funds for the Central Universities (No. 202230265 and No. E2EG2202X2).</p>
<p>Author contributions: K.R., X.W., J.H., P.W., Y.L., H.S. contributed to the ideation and design of the research; K.R., X.W. and J.H. performed the research; H.S. supervised the project; all authors contributed to the research discussions, writing, and editing of the paper.</p>
<p>Corresponding author: Hao Sun (haosun@ruc.edu.cn).</p>
<p>Competing interests: The authors declare no competing interests.
Supplementary information: The supplementary information is attached.</p>
<h1>Supplementary Information for:</h1>
<h2>LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context</h2>
<h2>1 Prompts</h2>
<h3>1.1 Idea Prompt</h3>
<p>I'll be submitting your next responses to a "Good Scientific Idea" expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: "{{keywords}}". You may provide background information. The idea MUST be concisely expressed within 100 words total (including any background information). (Note: good scientific ideas should be original (novel contribution), feasible (technically implementable), clearly articulated, and address meaningful problems in the field.).</p>
<h2>Fallback Prompt (If the first request is refused):</h2>
<p>I'll be submitting your next responses to a "Good Scientific Idea" expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: "{{keywords}}". You may provide background information. The idea MUST be concisely expressed within 100 words total (including any background information). This is a research study comparing different AI models on their ability to generate scientific ideas. Your suggestions will only be used for academic research purposes and not for any harmful applications. Please respond with a creative scientific idea related to the keyword provided. (Note: good scientific ideas should be original (novel contribution), feasible (technically implementable), clearly articulated, and address meaningful problems in the field.).</p>
<h3>1.2 Originality \&amp; Feasibility Critic Prompt</h3>
<h2>SYSTEM:</h2>
<p>You are an extremely demanding scientific reviewer with the highest critical standards, like those at Nature or Science. When evaluating scientific ideas, you will assess them on three key dimensions:</p>
<ol>
<li>originality: Novel contribution to unexplored areas or innovative approaches to existing problems</li>
<li>feasibility: Technical implementation and practicality</li>
<li>clarity: How well-articulated and easy to understand the idea is</li>
</ol>
<p>Your response should consist of two parts: a text analysis followed by a JSON score block. First, provide your brief analysis (less than 100 words) of the idea. Then, for each dimension, provide a score from 1 to 10 where $1-3=$ poor, $4-6=$ average, $7-10=$ excellent. For example:
" json
{
"originality": <score_1_to_10>,
"feasibility": <score_1_to_10>,
"clarity": <score_1_to_10>
}
・.
USER:
Please evaluate the following scientific idea and give your scores directly: {{idea}}</p>            </div>
        </div>

    </div>
</body>
</html>
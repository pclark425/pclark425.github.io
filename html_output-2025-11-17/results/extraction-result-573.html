<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-573 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-573</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-573</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-d079a2f877f554e00f71a6975435d8325987bdf5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d079a2f877f554e00f71a6975435d8325987bdf5" target="_blank">Return of Frustratingly Easy Domain Adaptation</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL), which minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels.</p>
                <p><strong>Paper Abstract:</strong> 
 
 Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ``frustratingly easy'' to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.
 
</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e573.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e573.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORrelation ALignment (CORAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple unsupervised domain adaptation method that aligns second-order statistics by whitening source features and re-coloring them with the target covariance so classifiers trained on source apply to target without target labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Covariance alignment via whitening and re-coloring (CORAL)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute covariance matrices of source and target feature sets (after zero-mean normalization), regularize them by adding identity*lambda, whiten the source by multiplying by C_S^{-1/2}, then re-color the whitened source by multiplying by C_T^{1/2}. Train a supervised classifier on the transformed source features and apply it to target features. Analytical derivation uses SVD/pseudoinverse for low-rank covariances; in practice classical regularized whitening/coloring with λ=1 is used for stability and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data transformation (feature alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical feature preprocessing / linear-algebraic whitening (general ML / signal processing)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>unsupervised domain adaptation across application contexts (computer vision object recognition; natural language sentiment classification)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (statistical whitening/recoloring repurposed as asymmetric domain-alignment transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Regularized covariance estimation (add identity*λ with λ=1) to make matrices full-rank and numerically stable; practical implementation uses classical whitening and coloring instead of the full SVD-based analytical low-rank solution for speed and stability; applied as an asymmetric transform (source->target) rather than symmetric transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - CORAL achieves substantial improvements in benchmark tasks: e.g., on Office-Caltech10 with SURF features average accuracy increased from 37.8% (no adaptation) to 46.7% (Table 1); on Office with deep features CORAL-FT7 average 69.4% vs NA-FT7 65.5% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Covariance matrices can be low-rank (data lie on lower-dimensional manifolds), causing instability in inverse square roots; direct SVD-based solutions can be unstable/slow; feature sparsity (e.g., bag-of-words) reduces correlation signal and limits gains; domain shift can place source and target on different subspaces so naive symmetric whitening fails.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Underlying shared statistical principle (matching second-order statistics) makes transfer straightforward; availability of unlabeled target data to estimate target covariance; features with strong correlations (deep image features) amplify CORAL's effectiveness; computational simplicity (linear transforms) enables easy application.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires feature vectors from source and target of same dimensionality; enough samples in each domain to estimate covariance reliably; ability to compute matrix inverse-square-root (or stable regularized alternatives); for best results, features should have meaningful second-order structure (not extremely sparse).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within ML contexts: demonstrated on both computer vision and NLP benchmarks and on shallow and deep features; likely applicable to any domain where feature covariances capture domain differences, but less effective for extremely sparse/un-correlated features.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (statistical moment-matching / linear algebra)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Return of Frustratingly Easy Domain Adaptation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e573.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e573.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORAL -> Sentiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application of CORAL to sentiment analysis (bag-of-words features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct application of the CORAL covariance-alignment procedure to text bag-of-words features for cross-domain sentiment classification, using dimensionality reduction to top words and regularized covariance operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>CORAL applied to bag-of-words sentiment features</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Reduce bag-of-words dimensionality (paper used preprocessed top-400 words), normalize features to zero-mean/unit-variance per dimension, compute regularized covariances for source and target, whiten source and recolor with target covariance (λ regularization), train linear SVM on transformed source and test on target.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data preprocessing applied between domains</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision / general ML method development (method originally demonstrated on image features)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>natural language processing — cross-domain sentiment classification (Amazon review domains)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with minimal preprocessing (dimensionality reduction and regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied to 400-dimensional bag-of-words vectors (preprocessed dataset); used the same regularization λ=1 for covariance matrices; used standard SVM model-selection on source as in vision experiments. No structural change to CORAL algorithm beyond standard preprocessing for text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - CORAL produced modest improvements over baselines on sentiment benchmarks: average accuracy improved from 76.7% (no adaptation baseline) to 78.0% (CORAL) on the Amazon review tasks (Table 5). Some state-of-the-art manifold methods performed worse than NA on this sparse text data, while CORAL still improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Bag-of-words features are sparse and weakly correlated, which reduces the discriminative signal in covariance alignment; covariance estimates can be noisy for high-dimensional sparse text data; gains are smaller compared to image/deep-feature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of processed low-dimensional bag-of-words (top-400) reduced noise and made covariance estimation feasible; CORAL's simplicity made direct application straightforward.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires dimensionality reduction or sufficient samples to estimate covariances reliably for sparse text; standard preprocessing (zero-mean/unit-variance) and cross-validated SVM training on source.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate — demonstrated that CORAL can transfer from image-feature scenarios to NLP sentiment tasks, but effectiveness depends on feature correlation structure; likely applicable to other text tasks if feature representations have informative second-order statistics (e.g., dense embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (data preprocessing and linear transform) and empirical know-how (regularization λ, dimensionality reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Return of Frustratingly Easy Domain Adaptation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e573.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e573.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORAL -> Deep Nets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application and integration of CORAL to deep neural network features and architectures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying CORAL to deep network activations (e.g., fc6/fc7 of AlexNet) and proposing integration as additional transformation layers to align internal covariances across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Layer-wise covariance alignment for deep features / networks</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Treat activations of a network layer φ_k as feature vectors; compute layer activation covariances for source and target, regularize, whiten source activations and recolor with target covariance. Practically CORAL is applied to final-layer activations (fc6/fc7) extracted from pre-trained or fine-tuned networks; authors also describe how the linear transform A_l could be implemented as extra layers to perform multi-layer CORAL in-network.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method integrated into deep learning architectures</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>feature-level domain adaptation on shallow features / classical ML</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>deep learning internal representation adaptation (computer vision CNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied to internal layer activations; suggested implementation as network layers)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied CORAL to precomputed fc6/fc7 activations and to fine-tuned networks (CORAL-fc6/fc7, CORAL-FT6/FT7); used λ-regularized covariance estimation; for efficiency suggested applying equivalent transform to linear classifier weights rather than features when many target examples exist; proposed implementing transforms A_l as extra network layers for multilayer alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - consistent improvements on standard Office deep-feature benchmarks: e.g., CORAL-FT7 average accuracy 69.4% vs NA-FT7 65.5% (Table 2). CORAL applied to deep features outperformed or matched several recent deep adaptation methods (DAN, DDC, ReverseGrad) on many reported settings.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Integration into end-to-end training requires inserting differentiable whitening/coloring layers or approximations; naive whitening of both domains fails when source and target lie on different subspaces; deep adaptation baselines can require complex multi-loss training and hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Deep features have strong inter-feature correlations (large singular values), which makes second-order alignment particularly effective; availability of precomputed deep activations and fine-tuning pipelines; CORAL's linearity makes it cheap to apply as a post-processing step or add as layers.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to layer activations from source and target (unlabeled target) to compute covariances; computational routines for matrix square-roots and inverse square-roots; for in-network use, need to implement differentiable versions or approximations of the whitening/coloring transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within deep-learning contexts; authors argue CORAL is especially valuable with deep features and can be integrated into networks (single- or multi-layer), but practical end-to-end differentiable implementation would require further engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural know-how (how to compute/apply transforms to activations) and instrumental/technical skills (integrating transforms into network architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Return of Frustratingly Easy Domain Adaptation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e573.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e573.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subspace / Manifold Adaptation Methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subspace projection and alignment methods for unsupervised domain adaptation (e.g., GFK, SA, geodesic subspace mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of unsupervised adaptation techniques that project source and target into lower-dimensional subspaces (manifolds) and align those subspaces via geodesic flows or linear maps to bridge domain shift.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Subspace projection and alignment (GFK, SA, geodesic flow methods)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Project high-dimensional features into lower-dimensional subspaces (top-k eigenvectors), then compute mappings that align source and target subspaces — examples include Geodesic Flow Kernel (integrating over subspaces along the geodesic manifold) and Subspace Alignment (computing linear maps minimizing Frobenius norm between subspace bases). These methods focus on aligning bases rather than full covariance distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / manifold learning applied to domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>manifold learning / subspace methods (applied historically in ML and computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>unsupervised domain adaptation for object recognition and related tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>mention (prior art transferred manifold ideas into domain adaptation literature)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>These prior works adapt manifold/subspace machinery for domain adaptation by selecting subspace dimensionality k and deriving mapping/kernels to connect source and target subspaces; require hyperparameter selection for k and often expensive subspace computations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>used successfully in prior literature for some tasks (GFK, SA reported good performance), but in this paper they are reported to be outperformed by CORAL in many benchmarks and to require more hyperparameter tuning and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Require selecting subspace dimensionality hyperparameter k; expensive SVD/projection operations; they align bases but not eigenvalue distributions, so distributions of projected points can still differ substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Manifold geometry provides principled way to model smooth transitions between domains; works well where dominant subspace captures most variance and corresponding eigenvalues are similar.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Reliable estimation of top-k eigenvectors (sufficient data), hyperparameter selection for subspace dimension, computational resources for repeated subspace computations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate — effective when data lie on well-defined low-dimensional subspaces, but less general than covariance-alignment approaches because of hyperparameter sensitivity and focus on bases rather than full second-order statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles (manifold geometry) and explicit computational procedures (subspace projection and mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Return of Frustratingly Easy Domain Adaptation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Frustratingly easy domain adaptation <em>(Rating: 2)</em></li>
                <li>Domain adaptation via transfer component analysis <em>(Rating: 2)</em></li>
                <li>Geodesic flow kernel for unsupervised domain adaptation <em>(Rating: 2)</em></li>
                <li>Domain adaptation for object recognition: An unsupervised approach <em>(Rating: 2)</em></li>
                <li>Unsupervised domain adaptation by backpropagation <em>(Rating: 2)</em></li>
                <li>Learning transferable features with deep adaptation networks <em>(Rating: 2)</em></li>
                <li>Batch normalization: Accelerating deep network training by reducing internal covariate shift <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-573",
    "paper_id": "paper-d079a2f877f554e00f71a6975435d8325987bdf5",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "CORAL",
            "name_full": "CORrelation ALignment (CORAL)",
            "brief_description": "A simple unsupervised domain adaptation method that aligns second-order statistics by whitening source features and re-coloring them with the target covariance so classifiers trained on source apply to target without target labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Covariance alignment via whitening and re-coloring (CORAL)",
            "procedure_description": "Compute covariance matrices of source and target feature sets (after zero-mean normalization), regularize them by adding identity*lambda, whiten the source by multiplying by C_S^{-1/2}, then re-color the whitened source by multiplying by C_T^{1/2}. Train a supervised classifier on the transformed source features and apply it to target features. Analytical derivation uses SVD/pseudoinverse for low-rank covariances; in practice classical regularized whitening/coloring with λ=1 is used for stability and efficiency.",
            "procedure_type": "computational method / data transformation (feature alignment)",
            "source_domain": "statistical feature preprocessing / linear-algebraic whitening (general ML / signal processing)",
            "target_domain": "unsupervised domain adaptation across application contexts (computer vision object recognition; natural language sentiment classification)",
            "transfer_type": "adapted/modified for new context (statistical whitening/recoloring repurposed as asymmetric domain-alignment transformation)",
            "modifications_made": "Regularized covariance estimation (add identity*λ with λ=1) to make matrices full-rank and numerically stable; practical implementation uses classical whitening and coloring instead of the full SVD-based analytical low-rank solution for speed and stability; applied as an asymmetric transform (source-&gt;target) rather than symmetric transforms.",
            "transfer_success": "successful - CORAL achieves substantial improvements in benchmark tasks: e.g., on Office-Caltech10 with SURF features average accuracy increased from 37.8% (no adaptation) to 46.7% (Table 1); on Office with deep features CORAL-FT7 average 69.4% vs NA-FT7 65.5% (Table 2).",
            "barriers_encountered": "Covariance matrices can be low-rank (data lie on lower-dimensional manifolds), causing instability in inverse square roots; direct SVD-based solutions can be unstable/slow; feature sparsity (e.g., bag-of-words) reduces correlation signal and limits gains; domain shift can place source and target on different subspaces so naive symmetric whitening fails.",
            "facilitating_factors": "Underlying shared statistical principle (matching second-order statistics) makes transfer straightforward; availability of unlabeled target data to estimate target covariance; features with strong correlations (deep image features) amplify CORAL's effectiveness; computational simplicity (linear transforms) enables easy application.",
            "contextual_requirements": "Requires feature vectors from source and target of same dimensionality; enough samples in each domain to estimate covariance reliably; ability to compute matrix inverse-square-root (or stable regularized alternatives); for best results, features should have meaningful second-order structure (not extremely sparse).",
            "generalizability": "High within ML contexts: demonstrated on both computer vision and NLP benchmarks and on shallow and deep features; likely applicable to any domain where feature covariances capture domain differences, but less effective for extremely sparse/un-correlated features.",
            "knowledge_type": "explicit procedural steps and theoretical principles (statistical moment-matching / linear algebra)",
            "uuid": "e573.0",
            "source_info": {
                "paper_title": "Return of Frustratingly Easy Domain Adaptation",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "CORAL -&gt; Sentiment",
            "name_full": "Application of CORAL to sentiment analysis (bag-of-words features)",
            "brief_description": "Direct application of the CORAL covariance-alignment procedure to text bag-of-words features for cross-domain sentiment classification, using dimensionality reduction to top words and regularized covariance operations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "CORAL applied to bag-of-words sentiment features",
            "procedure_description": "Reduce bag-of-words dimensionality (paper used preprocessed top-400 words), normalize features to zero-mean/unit-variance per dimension, compute regularized covariances for source and target, whiten source and recolor with target covariance (λ regularization), train linear SVM on transformed source and test on target.",
            "procedure_type": "computational method / data preprocessing applied between domains",
            "source_domain": "computer vision / general ML method development (method originally demonstrated on image features)",
            "target_domain": "natural language processing — cross-domain sentiment classification (Amazon review domains)",
            "transfer_type": "direct application with minimal preprocessing (dimensionality reduction and regularization)",
            "modifications_made": "Applied to 400-dimensional bag-of-words vectors (preprocessed dataset); used the same regularization λ=1 for covariance matrices; used standard SVM model-selection on source as in vision experiments. No structural change to CORAL algorithm beyond standard preprocessing for text.",
            "transfer_success": "partially successful - CORAL produced modest improvements over baselines on sentiment benchmarks: average accuracy improved from 76.7% (no adaptation baseline) to 78.0% (CORAL) on the Amazon review tasks (Table 5). Some state-of-the-art manifold methods performed worse than NA on this sparse text data, while CORAL still improved performance.",
            "barriers_encountered": "Bag-of-words features are sparse and weakly correlated, which reduces the discriminative signal in covariance alignment; covariance estimates can be noisy for high-dimensional sparse text data; gains are smaller compared to image/deep-feature settings.",
            "facilitating_factors": "Availability of processed low-dimensional bag-of-words (top-400) reduced noise and made covariance estimation feasible; CORAL's simplicity made direct application straightforward.",
            "contextual_requirements": "Requires dimensionality reduction or sufficient samples to estimate covariances reliably for sparse text; standard preprocessing (zero-mean/unit-variance) and cross-validated SVM training on source.",
            "generalizability": "Moderate — demonstrated that CORAL can transfer from image-feature scenarios to NLP sentiment tasks, but effectiveness depends on feature correlation structure; likely applicable to other text tasks if feature representations have informative second-order statistics (e.g., dense embeddings).",
            "knowledge_type": "explicit procedural steps (data preprocessing and linear transform) and empirical know-how (regularization λ, dimensionality reduction)",
            "uuid": "e573.1",
            "source_info": {
                "paper_title": "Return of Frustratingly Easy Domain Adaptation",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "CORAL -&gt; Deep Nets",
            "name_full": "Application and integration of CORAL to deep neural network features and architectures",
            "brief_description": "Applying CORAL to deep network activations (e.g., fc6/fc7 of AlexNet) and proposing integration as additional transformation layers to align internal covariances across domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Layer-wise covariance alignment for deep features / networks",
            "procedure_description": "Treat activations of a network layer φ_k as feature vectors; compute layer activation covariances for source and target, regularize, whiten source activations and recolor with target covariance. Practically CORAL is applied to final-layer activations (fc6/fc7) extracted from pre-trained or fine-tuned networks; authors also describe how the linear transform A_l could be implemented as extra layers to perform multi-layer CORAL in-network.",
            "procedure_type": "computational method integrated into deep learning architectures",
            "source_domain": "feature-level domain adaptation on shallow features / classical ML",
            "target_domain": "deep learning internal representation adaptation (computer vision CNNs)",
            "transfer_type": "adapted/modified for new context (applied to internal layer activations; suggested implementation as network layers)",
            "modifications_made": "Applied CORAL to precomputed fc6/fc7 activations and to fine-tuned networks (CORAL-fc6/fc7, CORAL-FT6/FT7); used λ-regularized covariance estimation; for efficiency suggested applying equivalent transform to linear classifier weights rather than features when many target examples exist; proposed implementing transforms A_l as extra network layers for multilayer alignment.",
            "transfer_success": "successful - consistent improvements on standard Office deep-feature benchmarks: e.g., CORAL-FT7 average accuracy 69.4% vs NA-FT7 65.5% (Table 2). CORAL applied to deep features outperformed or matched several recent deep adaptation methods (DAN, DDC, ReverseGrad) on many reported settings.",
            "barriers_encountered": "Integration into end-to-end training requires inserting differentiable whitening/coloring layers or approximations; naive whitening of both domains fails when source and target lie on different subspaces; deep adaptation baselines can require complex multi-loss training and hyperparameter tuning.",
            "facilitating_factors": "Deep features have strong inter-feature correlations (large singular values), which makes second-order alignment particularly effective; availability of precomputed deep activations and fine-tuning pipelines; CORAL's linearity makes it cheap to apply as a post-processing step or add as layers.",
            "contextual_requirements": "Access to layer activations from source and target (unlabeled target) to compute covariances; computational routines for matrix square-roots and inverse square-roots; for in-network use, need to implement differentiable versions or approximations of the whitening/coloring transforms.",
            "generalizability": "High within deep-learning contexts; authors argue CORAL is especially valuable with deep features and can be integrated into networks (single- or multi-layer), but practical end-to-end differentiable implementation would require further engineering.",
            "knowledge_type": "explicit procedural know-how (how to compute/apply transforms to activations) and instrumental/technical skills (integrating transforms into network architectures)",
            "uuid": "e573.2",
            "source_info": {
                "paper_title": "Return of Frustratingly Easy Domain Adaptation",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Subspace / Manifold Adaptation Methods",
            "name_full": "Subspace projection and alignment methods for unsupervised domain adaptation (e.g., GFK, SA, geodesic subspace mapping)",
            "brief_description": "A family of unsupervised adaptation techniques that project source and target into lower-dimensional subspaces (manifolds) and align those subspaces via geodesic flows or linear maps to bridge domain shift.",
            "citation_title": "",
            "mention_or_use": "mention",
            "procedure_name": "Subspace projection and alignment (GFK, SA, geodesic flow methods)",
            "procedure_description": "Project high-dimensional features into lower-dimensional subspaces (top-k eigenvectors), then compute mappings that align source and target subspaces — examples include Geodesic Flow Kernel (integrating over subspaces along the geodesic manifold) and Subspace Alignment (computing linear maps minimizing Frobenius norm between subspace bases). These methods focus on aligning bases rather than full covariance distributions.",
            "procedure_type": "computational method / manifold learning applied to domain adaptation",
            "source_domain": "manifold learning / subspace methods (applied historically in ML and computer vision)",
            "target_domain": "unsupervised domain adaptation for object recognition and related tasks",
            "transfer_type": "mention (prior art transferred manifold ideas into domain adaptation literature)",
            "modifications_made": "These prior works adapt manifold/subspace machinery for domain adaptation by selecting subspace dimensionality k and deriving mapping/kernels to connect source and target subspaces; require hyperparameter selection for k and often expensive subspace computations.",
            "transfer_success": "used successfully in prior literature for some tasks (GFK, SA reported good performance), but in this paper they are reported to be outperformed by CORAL in many benchmarks and to require more hyperparameter tuning and computational cost.",
            "barriers_encountered": "Require selecting subspace dimensionality hyperparameter k; expensive SVD/projection operations; they align bases but not eigenvalue distributions, so distributions of projected points can still differ substantially.",
            "facilitating_factors": "Manifold geometry provides principled way to model smooth transitions between domains; works well where dominant subspace captures most variance and corresponding eigenvalues are similar.",
            "contextual_requirements": "Reliable estimation of top-k eigenvectors (sufficient data), hyperparameter selection for subspace dimension, computational resources for repeated subspace computations.",
            "generalizability": "Moderate — effective when data lie on well-defined low-dimensional subspaces, but less general than covariance-alignment approaches because of hyperparameter sensitivity and focus on bases rather than full second-order statistics.",
            "knowledge_type": "theoretical principles (manifold geometry) and explicit computational procedures (subspace projection and mapping)",
            "uuid": "e573.3",
            "source_info": {
                "paper_title": "Return of Frustratingly Easy Domain Adaptation",
                "publication_date_yy_mm": "2015-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Frustratingly easy domain adaptation",
            "rating": 2
        },
        {
            "paper_title": "Domain adaptation via transfer component analysis",
            "rating": 2
        },
        {
            "paper_title": "Geodesic flow kernel for unsupervised domain adaptation",
            "rating": 2
        },
        {
            "paper_title": "Domain adaptation for object recognition: An unsupervised approach",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised domain adaptation by backpropagation",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable features with deep adaptation networks",
            "rating": 2
        },
        {
            "paper_title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "rating": 1
        }
    ],
    "cost": 0.01628725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Return of Frustratingly Easy Domain Adaptation</h1>
<p>Baochen Sun<br>Department of Computer Science<br>University of Massachusetts Lowell<br>Lowell, MA 01854, USA<br>bsun@cs.uml.edu</p>
<p>Jiashi Feng<br>Department of EECS, UC Berkeley, USA \&amp; Department of ECE, National University of Singapore, Singapore<br>elefjia@nus.edu.sg</p>
<p>Kate Saenko<br>Department of Computer Science<br>University of Massachusetts Lowell<br>Lowell, MA 01854, USA<br>saenko@cs.uml.edu</p>
<h4>Abstract</h4>
<p>Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being "frustratingly easy" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple-it can be implemented in four lines of Matlab code-CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.</p>
<p>"Everything should be made as simple as possible, but not simpler."</p>
<p>Albert Einstein</p>
<h2>1 Introduction</h2>
<p>Machine learning is very different from human learning. Humans are able to learn from very few labeled examples and apply the learned knowledge to new examples in novel conditions. In contrast, supervised machine learning methods only perform well when the given extensive labeled data are from the same distribution as the test distribution. Both theoretical (Ben-David et al. 2007; Blitzer, Dredze, and Pereira 2007) and practical results (Saenko et al. 2010; Torralba and Efros 2011) have shown that the test error of supervised methods generally increases in proportion to the "difference" between the distributions of training and test examples. For example, Donahue et al. (2014) showed that even state-of-the-art Deep Convolutional Neural Network features learned on a dataset of $1.2 M$ images are susceptible to domain shift. Addressing domain shift is undoubtedly critical for successfully applying machine learning methods in real world applications.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Two Domain Shift Scenarios: object recognition across visual domains (left) and sentiment prediction across text domains (right). When data distributions differ across domains, applying classifiers trained on one domain directly to another domain is likely to cause a significant performance drop.</p>
<p>To compensate for the degradation in performance due to domain shift, many domain adaptation algorithms have been developed, most of which assume that some labeled examples in the target domain are provided to learn the proper model adaptation. Daume III (2007) proposed a supervised domain adaptation approach notable for its extreme simplicity: it merely changes the features by making domainspecific and common copies, then trains a supervised classifier on the new features from both domains. The method performs very well, yet is "frustratingly easy" to implement. However, it cannot be applied in the situations where the target domain is unlabeled, which unfortunately are quite common in practice.</p>
<p>In this work, we present a "frustratingly easy" unsupervised domain adaptation method called CORrelation ALignment (CORAL). CORAL aligns the input feature distributions of the source and target domains by exploring their second-order statistics. More concretely, CORAL aligns the distributions by re-coloring whitened source features with the covariance of the target distribution. CORAL is simple and efficient, as the only computations it needs are (1) computing covariance statistics in each domain and (2) applying the whitening and re-coloring linear transformation to the source features. Then, supervised learning proceeds as usual-training a classifier on the transformed source features.</p>
<p>Despite being "frustratingly easy", CORAL offers surprisingly good performance on standard adaptation tasks. We apply it to two tasks: object recognition and sentiment prediction (Figure 1), and show that it outperforms many existing methods. For object recognition, we demonstrate that it works well with both standard "flat" bag-ofwords features and with state-of-the-art deep CNN features (Krizhevsky, Sutskever, and Hinton 2012), outperforming existing methods, including recent deep CNN adaptation approaches (Tzeng et al. 2014; Ganin and Lempitsky 2015; Long et al. 2015). The latter approaches are quite complex and expensive, requiring re-training of the network and tuning of many hyperparameters such as the structure of the hidden adaptation layers. In contrast, CORAL only needs to compute the covariance of the source and target features.</p>
<h2>2 Related Work</h2>
<p>Domain shift is a fundamental problem in machine learning, and has also attracted a lot of attention in the speech, natural language and vision communities (Blitzer, Dredze, and Pereira 2007; Gopalan, Li, and Chellappa 2011; Sun and Saenko 2014). For supervised adaptation, a variety of techniques have been proposed. Some consider the source domain as a prior that regularizes the learning problem in the sparsely labeled target domain, e.g., (Yang, Yan, and Hauptmann 2007). Others minimize the distance between the target and source domains, either by re-weighting the domains or by changing the feature representation according to some explicit distribution distance metric (Borgwardt et al. 2006). Some learn a transformation on features using a contrastive loss (Saenko et al. 2010). Arguably the simplest and most prominent supervised approach is the "frustratingly easy" feature replication (Daume III 2007). Given a feature vector $\boldsymbol{x}$, it defines the augmented feature vector $\tilde{\boldsymbol{x}}=(\boldsymbol{x} ; \boldsymbol{x} ; \mathbf{0})$ for data points in the source and $\tilde{\boldsymbol{x}}=(\boldsymbol{x} ; \mathbf{0} ; \boldsymbol{x})$ for data points in the target. A classifier is then trained on augmented features. This approach is simple, however, it requires labeled target examples, which are often not available in real world applications.</p>
<p>Early techniques for unsupervised adaptation consisted of re-weighting the training point losses to more closely reflect those in the test distribution (Jiang and Zhai 2007; Huang et al. 2006). Dictionary learning methods (Shekhar et al. 2013; Huang and Wang 2013) try to learn a dictionary where the difference between the source and target domain is minimized in the new representation. Recent state-of-the-art unsupervised approaches (Gopalan, Li, and Chellappa 2011; Gong et al. 2012; Long et al. 2014; Caseiro et al. 2015) have pursued adaptation by projecting the source and target distributions into a lower-dimensional manifold, and finding a transformation that brings the subspaces closer together. Geodesic methods find a path along the subspace manifold, and either project source and target onto points along that path (Gopalan, Li, and Chellappa 2011), or find a closedform linear map that projects source points to target (Gong et al. 2012). Alternatively, the subspaces can be aligned by computing the linear map that minimizes the Frobenius norm of the difference between them (Harel and Mannor 2011; Fernando et al. 2013). However, these approaches
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a-c) Illustration of CORrelation ALignment (CORAL) for Domain Adaptation: (a) The original source and target domains have different distribution covariances, despite the features being normalized to zero mean and unit standard deviation. This presents a problem for transferring classifiers trained on source to target. (b) The same two domains after source decorrelation, i.e. removing the feature correlations of the source domain. (c) Target re-correlation, adding the correlation of the target domain to the source features. After this step, the source and target distributions are well aligned and the classifier trained on the adjusted source domain is expected to work well in the target domain. (d) One might instead attempt to align the distributions by whitening both source and target. However, this will fail since the source and target data are likely to lie on different subspaces due to domain shift. (Best viewed in color)
only align the bases of the subspaces, not the distribution of the projected points. They also require expensive subspace projection and hyperparameter selection.</p>
<p>Adaptive deep neural networks have recently been explored for unsupervised adaptation. DLID (Chopra, Balakrishnan, and Gopalan 2013) trains a joint source and target CNN architecture, but is limited to two adaptation layers. ReverseGrad (Ganin and Lempitsky 2015), DAN (Long et al. 2015), and DDC (Tzeng et al. 2014) directly optimize the deep representation for domain invariance, using additional loss layers designed for this purpose. Training with this additional loss is costly and can be sensitive to initialization, network structure, and other optimization settings. Our approach, applied to deep features (top layer activations), achieves better or comparable performance to these more complex methods, and can be incorporated directly into the network structure.</p>
<h2>3 Correlation Alignment for Unsupervised Domain Adaptation</h2>
<p>We present an extremely simple domain adaptation methodCORrelation ALignment (CORAL)-which works by align-</p>
<p>ing the distributions of the source and target features in an unsupervised manner. We propose to match the distributions by aligning the second-order statistics, namely, the covariance.</p>
<h3>3.1 Formulation and Derivation</h3>
<p>We describe our method by taking a multi-class classification problem as the running example. Suppose we are given source-domain training examples $D_{S}=\left{\vec{x}<em S="S">{i}\right}, \vec{x} \in \mathbb{R}^{D}$ with labels $L</em>}=\left{y_{i}\right}, y \in{1, \ldots, L}$, and target data $D_{T}=\left{\vec{u<em s="s">{i}\right}, \vec{u} \in \mathbb{R}^{D}$. Here both $\vec{x}$ and $\vec{u}$ are the $D$ dimensional feature representations $\phi(I)$ of input $I$. Suppose $\mu</em>$.}, \mu_{t}$ and $C_{S}, C_{T}$ are the feature vector means and covariance matrices. As illustrated in Figure 2, $\mu_{t}=\mu_{s}=0$ after feature normalization while $C_{S} \neq C_{T</p>
<p>To minimize the distance between the second-order statistics (covariance) of the source and target features, we apply a linear transformation $A$ to the original source features and use the Frobenius norm as the matrix distance metric:</p>
<p>$$
\begin{aligned}
&amp; \min <em _hat_S="\hat{S">{A}\left|C</em>\right|}}-C_{T<em A="A">{F}^{2} \
&amp; =\min </em>
\end{aligned}
$$}\left|A^{\top} C_{S} A-C_{T}\right|_{F}^{2</p>
<p>where $C_{\hat{S}}$ is covariance of the transformed source features $D_{s} A$ and $|\cdot|_{F}^{2}$ denotes the matrix Frobenius norm.</p>
<p>If $\operatorname{rank}\left(C_{S}\right) \geq \operatorname{rank}\left(C_{T}\right)$, then an analytical solution can be obtained by choosing $A$ such that $C_{\hat{S}}=C_{T}$. However, the data typically lie on a lower dimensional manifold (Harel and Mannor 2011; Gong et al. 2012; Fernando et al. 2013), and so the covariance matrices are likely to be low rank (Hariharan, Malik, and Ramanan 2012). We derive a solution for this general case, using the following lemma.
Lemma 1. (Cai, Candès, and Shen 2010) Let $Y$ be a real matrix of rank $r_{Y}$ and $X$ a real matrix of rank at most $r$, where $r \leqslant r_{Y}$; let $Y=U_{Y} \Sigma_{Y} V_{Y}$ be the SVD of $Y$, and $\Sigma_{Y[1: r]}, U_{Y[1: r]}, V_{Y[1: r]}$ be the largest $r$ singular values and the corresponding left and right singular vectors of $Y$ respectively. Then, $X^{<em>}=U_{Y[1: r]} \Sigma_{Y[1: r]} V_{Y[1: r]}{ }^{\top}$ is the optimal solution to the problem of $\min <em F="F">{X}|X-Y|</em>$.
Theorem 1. Let $\Sigma^{+}$be the Moore-Penrose pseudoinverse of $\Sigma, r_{C_{S}}$ and $r_{C_{T}}$ denote the rank of $C_{S}$ and $C_{T}$ respectively. Then, $A^{}^{2</em>}=U_{S} \Sigma_{S}^{+\frac{1}{2}} U_{S}{ }^{\top} U_{T[1: r]} \Sigma_{T[1: r]}{ }^{\frac{1}{2}} U_{T[1: r]}{ }^{\top}$ is the optimal solution to the problem in Equation (1) with $r=\min \left(r_{C_{S}}, r_{C_{T}}\right)$.</p>
<p>Proof. Since $A$ is a linear transformation, $A^{\top} C_{S} A$ does not increase the rank of $C_{S}$. Thus, $r_{C_{S}} \leqslant r_{C_{T}}$. Since $C_{S}$ and $C_{T}$ are symmetric matrices, conducting SVD on $C_{S}$ and $C_{T}$ gives $C_{S}=U_{S} \Sigma_{S} U_{S}{ }^{\top}$ and $C_{T}=U_{T} \Sigma_{T} U_{T}^{\top}$ respectively. We first find the optimal value of $C_{\hat{S}}$ through considering the following two cases:
Case 1. $r_{C_{S}}&gt;r_{C_{T}}$. The optimal solution is $C_{\hat{S}}=C_{T}$. Thus, $C_{\hat{S}}=U_{T} \Sigma_{T} U_{T}{ }^{\top}=U_{T[1: r]} \Sigma_{T[1: r]} U_{T[1: r]}{ }^{\top}$ is the optimal solution to Equation (1) where $r=r_{C_{T}}$.</p>
<p>Case 2. $r_{C_{S}} \leqslant r_{C_{T}}$. Then, according to Lemma 1, $C_{\hat{S}}=$ $U_{T[1: r]} \Sigma_{T[1: r]} U_{T[1: r]}{ }^{\top}$ is the optimal solution to Equation (1) where $r=r_{C_{S}}$.</p>
<p>Combining the results in the above two cases yields that $C_{\hat{S}}=U_{T[1: r]} \Sigma_{T[1: r]} U_{T[1: r]}{ }^{\top}$ is the optimal solution to Equation (1) with $r=\min \left(r_{C_{S}}, r_{C_{T}}\right)$. We then proceed to solve for $A$ based on the above result. Let $C_{\hat{S}}=A^{\top} C_{S} A$, and we get:</p>
<p>$$
A^{\top} C_{S} A=U_{T[1: r]} \Sigma_{T[1: r]} U_{T[1: r]}{ }^{\top}
$$</p>
<p>Since $C_{S}=U_{S} \Sigma_{S} U_{S}{ }^{\top}$, we have</p>
<p>$$
A^{\top} U_{S} \Sigma_{S} U_{S}{ }^{\top} A=U_{T[1: r]} \Sigma_{T[1: r]} U_{T[1: r]}{ }^{\top}
$$</p>
<p>This gives:</p>
<p>$$
\left(U_{S}^{\top} A\right)^{\top} \Sigma_{S}\left(U_{S}^{\top} A\right)=U_{T[1: r]} \Sigma_{T[1: r]} U_{T[1: r]}{ }^{\top}
$$</p>
<p>Let $E=\Sigma_{S}^{+\frac{1}{2}} U_{S}{ }^{\top} U_{T[1: r]} \Sigma_{T[1: r]}{ }^{\frac{1}{2}} U_{T[1: r]}{ }^{\top}$, then the right hand side of the above equation can be re-written as $E^{\top} \Sigma_{S} E$. This gives</p>
<p>$$
\left(U_{S}^{\top} A\right)^{\top} \Sigma_{S}\left(U_{S}^{\top} A\right)=E^{\top} \Sigma_{S} E
$$</p>
<p>By setting $U_{S}{ }^{\top} A$ to $E$, we get the optimal solution of $A$ as</p>
<p>$$
\begin{aligned}
A^{*} &amp; =U_{S} E \
&amp; =\left(U_{S} \Sigma_{S}^{+\frac{1}{2}} U_{S}^{\top}\right)\left(U_{T[1: r]} \Sigma_{T[1: r]}{ }^{\frac{1}{2}} U_{T[1: r]}{ }^{\top}\right)
\end{aligned}
$$</p>
<h3>3.2 Algorithm</h3>
<p>We can think of transformation $A$ in this way intuitively: the first part $U_{S} \Sigma_{S}^{+\frac{1}{2}} U_{S}{ }^{\top}$ whitens the source data while the second part $U_{T[1: r]} \Sigma_{T[1: r]}{ }^{\frac{1}{2}} U_{T[1: r]}{ }^{\top}$ re-colors it with the target covariance. This is illustrated in Figure 2(b) and Figure 2(c) respectively. The traditional whitening is adding a small regularization parameter $\lambda$ to the diagonal elements of the covariance matrix to explicitly make it full rank and then multiply the original feature by the inverse square root (or square root for coloring) of it. The whitening and re-coloring here are slightly different from them since the data are likely to lie on a lower dimensional space and the covariance matrices could be low rank.</p>
<p>In practice, for the sake of efficiency and stability, we can perform the classical whitening and coloring. This is advantageous because: (1) it is faster (e.g., the whole CORAL transformation takes less than one minute on a regular laptop for $D_{S} \in \mathbb{R}^{795 \times 4096}$ and $D_{T} \in \mathbb{R}^{2817 \times 4096}$ ) and more stable, as SVD on the original covariance matrices might not be stable and might slow to converge; (2) as illustrated in Figure 3, the performance is similar to the analytical solution in Equation (2) and very stable with respect to $\lambda$. In this paper, we set $\lambda$ to 1 . The final algorithm can be written in four lines of MATLAB code as illustrated in Algorithm 1.</p>
<p>One might instead attempt to align the distributions by whitening both source and target. As shown in Figure 2(d), this will fail as the source and target data are likely to lie on</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Sensitivity of Covariance Regularization Parameter $\lambda$ with $\lambda \in{0,0.001,0.01,0.1,1}$. When $\lambda=0$, there is no regularization and we use the analytical solution in Equation (2). Please refer to Section 4.1 for details of tasks.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">CORAL</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Unsupervised</span><span class="w"> </span><span class="n">Domain</span><span class="w"> </span><span class="n">Adaptation</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">Source</span><span class="w"> </span><span class="n">Data</span><span class="w"> </span>\<span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">Target</span><span class="w"> </span><span class="n">Data</span><span class="w"> </span>\<span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">T</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Output</span><span class="p">:</span><span class="w"> </span><span class="n">Adjusted</span><span class="w"> </span><span class="n">Source</span><span class="w"> </span><span class="n">Data</span><span class="w"> </span>\<span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">s</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">C_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">=</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">cov</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">+</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">eye</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">size</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">},</span><span class="w"> </span><span class="mi">2</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">C_</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="o">=</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">cov</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">T</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">+</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">eye</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">size</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">T</span><span class="p">},</span><span class="w"> </span><span class="mi">2</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">=</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">C_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="p">}}</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="o">%</span>\<span class="p">)</span><span class="w"> </span><span class="n">whitening</span><span class="w"> </span><span class="n">source</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="o">=</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">C_</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="o">^</span><span class="p">{</span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="p">}}</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="o">%</span>\<span class="p">)</span><span class="w"> </span><span class="n">re</span><span class="o">-</span><span class="n">coloring</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">covariance</span>
</code></pre></div>

<p>different subspaces due to domain shift. An alternative approach would be whitening the target and then re-coloring it with the source covariance. However, as demonstrated in (Harel and Mannor 2011; Fernando et al. 2013) and our experiments, transforming data from source to target space gives better performance. This might be due to the fact that by transforming the source to target space the classifier was trained using both the label information from the source and the unlabelled structure from the target.</p>
<p>After CORAL transforms the source features to the target space, a classifier $f_{\vec{w}}$ parametrized by $\vec{w}$ can be trained on the adjusted source features and directly applied to target features. For a linear classifier $f_{\vec{w}}(I)=\vec{w}^{T} \phi(I)$, we can apply an equivalent transformation to the parameter vector $\vec{w}$ instead of the features $u$. This results in added efficiency when the number of classifiers is small but the number and dimensionality of target examples is very high.</p>
<p>Since correlation alignment changes the features only, it can be applied to any base classifier. Due to its efficiency, it can also be especially advantageous when the target domains are changing rapidly, e.g., due to scene changes over the course of a long video stream.</p>
<h3>3.3 Relationship to Existing Methods</h3>
<p>Relationship to Feature Normalization It has long been known that input feature normalization improves many machine learning methods, e.g., (Ioffe and Szegedy 2015). However, CORAL does not simply perform feature normalization, but rather aligns two different distributions. Standard feature normalization (zero mean and unit variance) does not address this issue, as illustrated in Figure 2(a). In this example, although the features are normalized to have zero mean and unit variance in each dimension, the differences in correlations present in the source and target domains cause the distributions to be different.</p>
<p>Relationship to Manifold Methods Recent state-of-theart unsupervised approaches project the source and target distributions into a lower-dimensional manifold and find a transformation that brings the subspaces closer together (Gopalan, Li, and Chellappa 2011; Gong et al. 2012; Fernando et al. 2013; Harel and Mannor 2011). CORAL avoids subspace projection, which can be costly and requires selecting the hyper-parameter that controls the dimensionality of the subspace. We note that subspace-mapping approaches (Harel and Mannor 2011; Fernando et al. 2013) only align the top $k$ (subspace dimensionality) eigenvectors of the source and target covariance matrices. On the contrary, CORAL aligns the covariance matrices, which can only be re-constructed using all eigenvectors and eigenvalues. Even though the eigenvectors can be aligned well, the distributions can still differ a lot due to the difference of eigenvalues between the corresponding eigenvectors of the source and target data. CORAL is a more general and much simpler method than the above two as it takes into account both eigenvectors and eigenvalues of the covariance matrix without the burden of subspace dimensionality selection.</p>
<p>Relationship to MMD methods Maximum Mean Discrepancy (MMD) based methods (e.g.,TCA (Pan et al. 2009), DAN (Long et al. 2015)) for domain adaptation can be interpreted as "moment matching" and can express arbitrary statistics of the data. Minimizing MMD with polynomial kernel $(k(x, y)=\left(1+x^{\prime} y\right)^{d}$ with $d=2)$ is similar to the CORAL objective, however, no previous work has used this kernel for domain adaptation nor proposed a closed form solution to the best of our knowledge. The other difference is that MMD based approaches usually apply the same transformation to both the source and target domain. As demonstrated in (Kulis, Saenko, and Darrell 2011; Harel and Mannor 2011; Fernando et al. 2013), asymmetric transformations are more flexible and often yield better performance for domain adaptation tasks. Intuitively, symmetric transformations find a space that "ignores" the differences between the source and target domain while asymmetric transformations try to "bridge" the two domains.</p>
<h3>3.4 Application to Deep Neural Networks</h3>
<p>Suppose $\phi(I)$ was computed by a multilayer neural network, then the inputs to each layer $\phi_{k}$ can suffer from covariate shift as well. Batch Normalization (Ioffe and Szegedy 2015) tries to compensate for internal covariate shift by normalizing each mini-batch to be zero-mean and unitvariance. However, as illustrated in Figure 2, such normalization might not be enough. Even if used with full whitening, Batch Normalization may not compensate for external covariate shift: the layer activations will be decorrelated for a source point but not for a target point. What's more, as mentioned in Section 3.2, whitening both domains still does not work. Our method can be easily integrated into a deep architecture by treating layers as features (e.g., fc6 or fc7 of AlexNet (Krizhevsky, Sutskever, and Hinton 2012)). Although we experiment only with CORAL applied to one hid-</p>
<p>den layer at each time, multilayer CORAL could be used by implementing the transformations $A_{l}$ as extra layers which follow each original layer $l$.</p>
<h2>4 Experiments</h2>
<p>We evaluate our method on object recognition (Saenko et al. 2010) and sentiment analysis (Blitzer, Dredze, and Pereira 2007) with both shallow and deep features, using standard benchmarks and protocols. In all experiments we assume the target domain is unlabeled.</p>
<p>We follow the standard procedure (Fernando et al. 2013; Donahue et al. 2014) and use a linear SVM as the base classifier. The model selection approach of (Fernando et al. 2013) is used to set the $C$ parameter for the SVM by doing cross-validation on the source domain. Since there are no other hyperparameters (except the common regularization parameter $\lambda$ for whitening and coloring, which we discussed in Section 3.2 and Figure 3) required for our method, the results in this paper can be easily reproduced. To compare to published methods, we use the accuracies reported by their authors or conduct experiments using the source code provided by the authors.</p>
<h3>4.1 Object Recognition</h3>
<p>In this set of experiments, domain adaptation is used to improve the accuracy of an object classifier on novel image domains. Both the standard Office (Saenko et al. 2010) and extended Office-Caltech10 (Gong et al. 2012) datasets are used as benchmarks in this paper. Office-Caltech10 contains 10 object categories from an office environment (e.g., keyboard, laptop, etc.) in 4 image domains: Webcam, $D S L R$, Amazon, and Caltech256. The standard Office dataset contains 31 (the same 10 categories from OfficeCaltech10 plus 21 additional ones) object categories in 3 domains: Webcam, $D S L R$, and Amazon. Later, we also conduct a larger (more data and categories) scale evaluation on Office-Caltech10 and the Cross-Dataset Testbed (Tommasi and Tuytelaars 2014) dataset.</p>
<p>Object Recognition with Shallow Features We follow the standard protocol of (Gong et al. 2012; Fernando et al. 2013; Gopalan, Li, and Chellappa 2011; Kulis, Saenko, and Darrell 2011; Saenko et al. 2010) and conduct experiments on the Office-Caltech10 dataset with shallow features (SURF). The SURF features were encoded with 800 -bin bag-of-words histograms and normalized to have zero mean and unit standard deviation in each dimension. Since there are four domains, there are 12 experiment settings, namely, $\mathrm{A} \rightarrow \mathrm{C}$ (train classifier on (A)mazon, test on (C)altech), $\mathrm{A} \rightarrow \mathrm{D}$ (train on (A)mazon, test on (D)SLR), $\mathrm{A} \rightarrow \mathrm{W}$, and so on. We follow the standard protocol and conduct experiments in 20 randomized trials for each domain shift and average the accuracy over the trials. In each trial, we use the standard setting (Gong et al. 2012; Fernando et al. 2013; Gopalan, Li, and Chellappa 2011; Kulis, Saenko, and Darrell 2011; Saenko et al. 2010) and randomly sample the same number ( 20 for Amazon, Caltech, and Webcam; 8 for $D S L R$ as there are only 8 images per category in the $D S L R$ domain) of labelled images in the source domain as training
set, and use all the unlabelled data in the target domain as the test set.</p>
<p>In Table 1, we compare our method to five recent published methods: SVMA (Duan, Tsang, and Xu 2012), DAM (Duan et al. 2009), GFK (Gong et al. 2012), SA (Fernando et al. 2013), and TCA (Pan et al. 2009) as well as the no adaptation baseline (NA). GFK, SA, and TCA are manifold based methods that project the source and target distributions into a lower-dimensional manifold. GFK integrates over an infinite number of subspaces along the subspace manifold using the kernel trick. SA aligns the source and target subspaces by computing a linear map that minimizes the Frobenius norm of their difference. TCA performs domain adaptation via a new parametric kernel using feature extraction methods by projecting data onto the learned transfer components. DAM introduces smoothness assumption to enforce the target classifier share similar decision values with the source classifiers. Even though these methods are far more complicated than ours and require tuning of hyperparameters (e.g., subspace dimensionality), our method achieves the best average performance across all the 12 domain shifts. Our method also improves on the no adaptation baseline (NA), in some cases increasing accuracy significantly (from $56 \%$ to $86 \%$ for $\mathrm{D} \rightarrow \mathrm{W}$ ).</p>
<p>Object Recognition with Deep Features We follow the standard protocol of (Donahue et al. 2014; Tzeng et al. 2014; Long et al. 2015; Ganin and Lempitsky 2015) and conduct experiments on the standard Office dataset with deep features. DLID (Chopra, Balakrishnan, and Gopalan 2013) trains a joint source and target CNN architecture with an "interpolating path" between the source and target domain. DANN (Ghifary, Kleijn, and Zhang 2014) incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization to reduce the distribution mismatch. DA-NBNN (Tommasi and Caputo 2013) presents an NBNN-based domain adaptation algorithm that iteratively learns a class metric while inducing a large margin separation among classes. DECAF (Donahue et al. 2014) uses AlexNet (Krizhevsky, Sutskever, and Hinton 2012) pretrained on ImageNet (Deng et al. 2009) and extracts the fc6 or fc7 layers in the source domains as features to train a classifier. It then applies the classifier to the target domain directly. DDC (Tzeng et al. 2014) adds a domain confusion loss to AlexNet (Krizhevsky, Sutskever, and Hinton 2012) and fine-tunes it on both the source and target domain.</p>
<p>DAN (Long et al. 2015) and ReverseGrad (Ganin and Lempitsky 2015) are the two most recent domain adaptation approaches based on deep architectures. DAN is similar to DDC but utilizes a multi-kernel selection method for better mean embedding matching and adapts in multiple layers. ReverseGrad introduces a gradient reversal layer to allow direct optimization through back-propagation. Both DDC and ReverseGrad add a new binary classification task by treating the source and target domain as two classes. They maximize the binary classification loss to obtain invariant features.</p>
<p>To have a fair comparison, we apply CORAL to both the pre-trained AlexNet (CORAL-fc6 and CORAL-fc7) and to AlexNet fine-tuned on the source (CORAL-FT6</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">A $\rightarrow$ C</th>
<th style="text-align: center;">A $\rightarrow$ D</th>
<th style="text-align: center;">A $\rightarrow$ W</th>
<th style="text-align: center;">C $\rightarrow$ A</th>
<th style="text-align: center;">C $\rightarrow$ D</th>
<th style="text-align: center;">C $\rightarrow$ W</th>
<th style="text-align: center;">D $\rightarrow$ A</th>
<th style="text-align: center;">D $\rightarrow$ C</th>
<th style="text-align: center;">D $\rightarrow$ W</th>
<th style="text-align: center;">W $\rightarrow$ A</th>
<th style="text-align: center;">W $\rightarrow$ C</th>
<th style="text-align: center;">W $\rightarrow$ D</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NA</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: left;">SVMA</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">41.0</td>
</tr>
<tr>
<td style="text-align: left;">DAM</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">40.2</td>
</tr>
<tr>
<td style="text-align: left;">GFK</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: left;">TCA</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">$\mathbf{3 9 . 1}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 1}$</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">$\mathbf{4 1 . 4}$</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">$\mathbf{4 0 . 2}$</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: left;">SA</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">$\mathbf{4 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{3 5 . 0}$</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: left;">CORAL</td>
<td style="text-align: center;">$\mathbf{4 0 . 3}$</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">$\mathbf{4 7 . 2}$</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">$\mathbf{3 9 . 2}$</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">$\mathbf{8 5 . 9}$</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">$\mathbf{3 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Object recognition accuracies of all 12 domain shifts on the Office-Caltech10 dataset (Gong et al. 2012) with SURF features, following the protocol of (Gong et al. 2012; Fernando et al. 2013; Gopalan, Li, and Chellappa 2011; Kulis, Saenko, and Darrell 2011; Saenko et al. 2010).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">A $\rightarrow$ D</th>
<th style="text-align: center;">A $\rightarrow$ W</th>
<th style="text-align: center;">D $\rightarrow$ A</th>
<th style="text-align: center;">D $\rightarrow$ W</th>
<th style="text-align: center;">W $\rightarrow$ A</th>
<th style="text-align: center;">W $\rightarrow$ D</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NA-fc6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">62.2</td>
</tr>
<tr>
<td style="text-align: left;">NA-fc7</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">64.4</td>
</tr>
<tr>
<td style="text-align: left;">NA-FT6</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: left;">NA-FT7</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: left;">SA-fc6</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">49.1</td>
</tr>
<tr>
<td style="text-align: left;">SA-fc7</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: left;">SA-FT6</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">53.7</td>
</tr>
<tr>
<td style="text-align: left;">SA-FT7</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: left;">GFK-fc6</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">49.1</td>
</tr>
<tr>
<td style="text-align: left;">GFK-fc7</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">59.1</td>
</tr>
<tr>
<td style="text-align: left;">GFK-FT6</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">59.7</td>
</tr>
<tr>
<td style="text-align: left;">GFK-FT7</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">63.7</td>
</tr>
<tr>
<td style="text-align: left;">TCA-fc6</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: left;">TCA-fc7</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">53.1</td>
</tr>
<tr>
<td style="text-align: left;">TCA-FT6</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">52.8</td>
</tr>
<tr>
<td style="text-align: left;">TCA-FT7</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: left;">DLID</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DANN</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">39.3</td>
</tr>
<tr>
<td style="text-align: left;">DA-NBNN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DECAF-fc6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DECAF-fc7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DDC</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DAN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ReverseGrad</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{6 7 . 3}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">CORAL-fc6</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">64.0</td>
</tr>
<tr>
<td style="text-align: left;">CORAL-fc7</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">$\mathbf{5 1 . 1}$</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">66.9</td>
</tr>
<tr>
<td style="text-align: left;">CORAL-FT6</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">$\mathbf{9 7 . 1}$</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">$\mathbf{9 9 . 5}$</td>
<td style="text-align: center;">68.5</td>
</tr>
<tr>
<td style="text-align: left;">CORAL-FT7</td>
<td style="text-align: center;">$\mathbf{6 2 . 2}$</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">$\mathbf{4 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Object recognition accuracies of all 6 domain shifts on the standard Office dataset (Saenko et al. 2010) with deep features, following the protocol of (Donahue et al. 2014; Tzeng et al. 2014; Ganin and Lempitsky 2015).
and CORAL-FT7). However, the fine-tuning procedures of DDC, DAN, and ReverseGrad are very complicated as there is more than one loss and hyper-parameters are needed to combine them. They also require adding new layers and data from both source and target domains. We use standard fine-tuning on the source domain only to get the baseline NA results (NA-FT6 and NA-FT7). Since there are three domains, there are 6 experiment settings. We follow the protocol of (Donahue et al. 2014; Tzeng et al. 2014; Ganin and Lempitsky 2015) and conduct experiments on 5 random training/test splits and get the mean accuracy for each domain shift.</p>
<p>In Table 2 we compare our method to the 11 baseline methods discussed before. Again, our method outperforms all of these techniques in almost all cases, sometimes by a very large margin. Note that most of the deep structures based methods report results only on some settings. We find that the higher level fc7/FT7 features lead to better
performance than fc6/FT6. What's more, the NA baselines also achieve very good performance, even better than all the manifold methods and some deep methods. However, CORAL outperforms it consistently and is the only method achieves better AVG performance across all the 6 shifts. It also achieves better peformance than the two latest deep methods (DAN and ReverseGrad) in 2 out of the 3 shifts they reported.</p>
<p>One interesting finding is that, although fine-tuning on the source domain only (NA-FT6 and NA-FT7) does not achieve better performance on the target domain compared to the pre-trained network (NA-fc6 and NA-fc7), applying CORAL to the fine-tuned network (CORAL-FT6 and CORAL-FT7) achieves much better performance than applying CORAL to the pre-trained network (CORAL-fc6 and CORAL-fc7). One possible explanation is that the pretrained network might be underfitting while the fine-tuned network is overfitting. Since CORAL aligns the source feature distribution to target distribution, overfitting becomes less of a problem.</p>
<p>A Larger Scale Evaluation In this section, we repeat the evaluation on a larger scale. We conduct two sets of experiments to investigate how the dataset size and number of classes will affect the performance of domain adaptation methods. In both sets of experiments, we use the "full training" protocol, where all the source data are used for training, compared to the standard subsampling protocol in the previous two sections. Since all the target data are used in the previous two sections, the only difference between these two settings is the training dataset size of the source domain. To have a direct comparison to Table 1, we conduct the first set of experiments on the Office-Caltech10 dataset with SURF features. To investigate the effect of the number of classes, we conduct the second set of experiments on the Cross-Dataset Testbed (Tommasi and Tuytelaars 2014) dataset, with 3847 images for Caltech256 (Gregory, Alex, and Pietro 2007), 4000 images for ImageNet (Deng et al. 2009), and 2626 images for SUN (Xiao et al. 2010) over 40 classes, using the only publicly available deep features (DECAF-fc7).</p>
<p>In Tables 3 and 4, we compare CORAL to SA, GFK, TCA which have available source code as well as the NA baseline. Table 3 shows the result of the Office-Caltech10 dataset and Table 4 shows the result on the Cross-Dataset Testbed dataset. In both experiments, CORAL outperforms all the baseline methods and again the margin on deep features is</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{A} \rightarrow \mathrm{C}$</th>
<th style="text-align: center;">$\mathrm{A} \rightarrow \mathrm{D}$</th>
<th style="text-align: center;">$\mathrm{A} \rightarrow \mathrm{W}$</th>
<th style="text-align: center;">$\mathrm{C} \rightarrow \mathrm{A}$</th>
<th style="text-align: center;">$\mathrm{C} \rightarrow \mathrm{D}$</th>
<th style="text-align: center;">$\mathrm{C} \rightarrow \mathrm{W}$</th>
<th style="text-align: center;">$\mathrm{D} \rightarrow \mathrm{A}$</th>
<th style="text-align: center;">$\mathrm{D} \rightarrow \mathrm{C}$</th>
<th style="text-align: center;">$\mathrm{D} \rightarrow \mathrm{W}$</th>
<th style="text-align: center;">$\mathrm{W} \rightarrow \mathrm{A}$</th>
<th style="text-align: center;">$\mathrm{W} \rightarrow \mathrm{C}$</th>
<th style="text-align: center;">$\mathrm{W} \rightarrow \mathrm{D}$</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">$\mathbf{4 4 . 6}$</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">41.1</td>
</tr>
<tr>
<td style="text-align: center;">SA</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">41.5</td>
</tr>
<tr>
<td style="text-align: center;">GFK</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">$\mathbf{5 6 . 0}$</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">$\mathbf{3 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 5}$</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">46.4</td>
</tr>
<tr>
<td style="text-align: center;">TCA</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">$\mathbf{5 2 . 2}$</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr>
<td style="text-align: center;">CORAL</td>
<td style="text-align: center;">$\mathbf{4 5 . 1}$</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">$\mathbf{4 4 . 4}$</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">$\mathbf{4 6 . 4}$</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">$\mathbf{8 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 6}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Object recognition accuracies of all 12 domain shifts on the Office-Caltech10 dataset (Gong et al. 2012) with SURF features, using the "full training" protocol.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\mathrm{C} \rightarrow \mathrm{I}$</th>
<th style="text-align: center;">$\mathrm{C} \rightarrow \mathrm{S}$</th>
<th style="text-align: center;">$\mathrm{I} \rightarrow \mathrm{C}$</th>
<th style="text-align: center;">$\mathrm{I} \rightarrow \mathrm{S}$</th>
<th style="text-align: center;">$\mathrm{S} \rightarrow \mathrm{C}$</th>
<th style="text-align: center;">$\mathrm{S} \rightarrow \mathrm{I}$</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NA</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: left;">SA</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">25.8</td>
</tr>
<tr>
<td style="text-align: left;">GFK</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">31.3</td>
</tr>
<tr>
<td style="text-align: left;">TCA</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">26.6</td>
</tr>
<tr>
<td style="text-align: left;">CORAL</td>
<td style="text-align: center;">$\mathbf{6 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{2 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Object recognition accuracies of all 6 domain shifts on the Testbed Cross-Dataset (Tommasi and Tuytelaars 2014) dataset with DECAF-fc7 features, using the "full training" protocol.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\mathrm{K} \rightarrow \mathrm{D}$</th>
<th style="text-align: center;">$\mathrm{D} \rightarrow \mathrm{B}$</th>
<th style="text-align: center;">$\mathrm{B} \rightarrow \mathrm{E}$</th>
<th style="text-align: center;">$\mathrm{E} \rightarrow \mathrm{K}$</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NA</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">76.7</td>
</tr>
<tr>
<td style="text-align: left;">TCA</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">63.0</td>
</tr>
<tr>
<td style="text-align: left;">SA</td>
<td style="text-align: center;">$\mathbf{7 6 . 4}$</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: left;">GFK</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">69.6</td>
</tr>
<tr>
<td style="text-align: left;">GFK</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">SCL</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">76.7</td>
</tr>
<tr>
<td style="text-align: left;">KMM</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">$\mathbf{7 8 . 6}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 9}$</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">77.8</td>
</tr>
<tr>
<td style="text-align: left;">CORAL</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">$\mathbf{8 3 . 6}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Review classification accuracies of the 4 standard domain shifts (Gong, Grauman, and Sha 2013) on the Amazon dataset (Blitzer, Dredze, and Pereira 2007) with bag-ofwords features.
much larger than on shallow features. Comparing Table 3 to Table 1, we can say that the performance difference between NA and other methods is smaller as more source data is used. This may be due to the fact that as more training data is used, the classifier is stronger and can generalize better to other domains.</p>
<h3>4.2 Sentiment Analysis</h3>
<p>We also evaluate our method on sentiment analysis using the standard Amazon review dataset (Blitzer, Dredze, and Pereira 2007; Gong, Grauman, and Sha 2013). We use the processed data from (Gong, Grauman, and Sha 2013), in which the dimensionality of the bag-of-words features was reduced to keep the top 400 words without losing performance. This dataset contains Amazon reviews on 4 domains: Kitchen appliances, DVD, Books, and Electronics. For each domain, there are 1000 positive and 1000 negative reviews. We follow the standard protocol of (Gong, Grauman, and Sha 2013) and conduct experiments on 20 random training/test splits and report the mean accuracy for each domain shift.</p>
<p>In Table 5, we compare our method to five published methods: TCA (Pan et al. 2009), GFS (Gopalan, Li, and Chellappa 2011), GFK (Gong et al. 2012), SCL (Blitzer, McDonald, and Pereira 2006), and KMM (Huang et al.</p>
<p>2006) as well as the no adaptation baseline (NA). GFS is a precursor of GFK and interpolates features using a finite number of subspaces. SCL introduces structural correspondence learning to automatically induce correspondences among features from different domains. KMM presents a nonparametric method to directly produce re-sampling weights without distribution estimation. One interesting observation is that, for this sentiment analysis task, three state-of-the-art methods (TCA, GFS, and GFK) actually perform worse than the no adaptation baseline (NA). Despite the difficulty of this task, CORAL still performs well and achieves the best average classification accuracy across the 4 standard domain shifts.</p>
<h2>5 Discussion</h2>
<p>One interesting result is that the margin between CORAL and other published methods is much larger on deep features (e.g. 64.0 of CORAL-fc6 compared to 49.1 of SAfc6 in Table 2) than on bag-of-words features. This could be because deep features are more strongly correlated than bag-of-words features (e.g. the largest singular value of the covariance matrix of Amazon-fc6 is 354 compared to 27 of Amazon-SURF). Similarly, the improvement on images (Tables 1-4) is much larger than text (Table 5), possibly because bag-of-words text features are extremely sparse and less correlated than image features. As demonstrated in (Mahendran and Vedaldi 2015), high level deep features are more "parts" or "objects'. Intuitively, "parts" or "objects" should be more strongly correlated than "edges" (e.g., arm and head of a person are more likely to appear jointly).</p>
<p>These findings suggest that CORAL is extremely valuable in the era of deep learning. Applying CORAL to deep text features is part of future work.</p>
<h2>6 Conclusion</h2>
<p>In this article, we proposed an simple, efficient and effective method for domain adaptation. The method is "frustratingly easy" to implement: the only computation involved is recoloring the whitened source features with the covariance of the target domain.</p>
<p>Extensive experiments on standard benchmarks demonstrate the superiority of our method over many existing state-of-the-art methods. These results confirm that CORAL is applicable to multiple features types, including highlyperforming deep features, and to different tasks, including computer vision and natural language processing.</p>
<h2>7 Acknowledgments</h2>
<p>The authors would like to thank Mingsheng Long, Judy Hoffman, and Trevor Darrell for helpful discussions and suggestions; the reviewers for their valuable comments. The Tesla K40 used for this research was donated by the NVIDIA Corporation. This research was supported by NSF Awards IIS-1451244 and IIS-1212928.</p>
<h2>References</h2>
<p>Ben-David, S.; Blitzer, J.; Crammer, K.; and Pereira, F. 2007. Analysis of representations for domain adaptation. In NIPS.
Blitzer, J.; Dredze, M.; and Pereira, F. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In ACL.
Blitzer, J.; McDonald, R.; and Pereira, F. 2006. Domain adaptation with structural correspondence learning. In EMNLP.
Borgwardt, K. M.; Gretton, A.; Rasch, M. J.; Kriegel, H.-P.; Schölkopf, B.; and Smola, A. J. 2006. Integrating structured biological data by kernel maximum mean discrepancy. In Bioinformatics.
Cai, J.-F.; Candès, E. J.; and Shen, Z. 2010. A singular value thresholding algorithm for matrix completion. SIAM J. on Optimization 20(4):1956-1982.
Caseiro, R.; Henriques, J. F.; Martins, P.; and Batista, J. 2015. Beyond the shortest path : Unsupervised domain adaptation by sampling subspaces along the spline flow. In CVPR.
Chopra, S.; Balakrishnan, S.; and Gopalan, R. 2013. Dlid: Deep learning for domain adaptation by interpolating between domains. In ICML Workshop.
Daume III, H. 2007. Frustratingly easy domain adaptation. In ACL.</p>
<p>Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In CVPR.
Donahue, J.; Jia, Y.; Vinyals, O.; Hoffman, J.; Zhang, N.; Tzeng, E.; and Darrell, T. 2014. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML.
Duan, L.; Tsang, I. W.; Xu, D.; and Chua, T. 2009. Domain adaptation from multiple sources via auxiliary classifiers. In ICML.
Duan, L.; Tsang, I. W.; and Xu, D. 2012. Domain transfer multiple kernel learning. TPAMI 34(3):465-479.
Fernando, B.; Habrard, A.; Sebban, M.; and Tuytelaars, T. 2013. Unsupervised visual domain adaptation using subspace alignment. In ICCV.
Ganin, Y., and Lempitsky, V. 2015. Unsupervised domain adaptation by backpropagation. In ICML.
Ghifary, M.; Kleijn, W. B.; and Zhang, M. 2014. Domain adaptive neural networks for object recognition. In PRICAI.
Gong, B.; Shi, Y.; Sha, F.; and Grauman, K. 2012. Geodesic flow kernel for unsupervised domain adaptation. In CVPR.
Gong, B.; Grauman, K.; and Sha, F. 2013. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In ICML.
Gopalan, R.; Li, R.; and Chellappa, R. 2011. Domain adaptation for object recognition: An unsupervised approach. In ICCV.</p>
<p>Gregory, G.; Alex, H.; and Pietro, P. 2007. Caltech 256 object category dataset. In Tech. Rep. UCB/CSD-04-1366, California Institue of Technology.
Harel, M., and Mannor, S. 2011. Learning from multiple outlooks. In ICML.
Hariharan, B.; Malik, J.; and Ramanan, D. 2012. Discriminative decorrelation for clustering and classification. In ECCV.
Huang, D.-A., and Wang, Y.-C. 2013. Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition. In ICCV.
Huang, J.; Smola, A. J.; Gretton, A.; Borgwardt, K. M.; and Schölkopf, B. 2006. Correcting sample selection bias by unlabeled data. In NIPS.
Ioffe, S., and Szegedy, C. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML.
Jiang, J., and Zhai, C. 2007. Instance Weighting for Domain Adaptation in NLP. In ACL.
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. In NIPS.
Kulis, B.; Saenko, K.; and Darrell, T. 2011. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In CVPR.
Long, M.; Wang, J.; Ding, G.; Sun, J.; and Yu, P. 2014. Transfer joint matching for unsupervised domain adaptation. In CVPR.
Long, M.; Cao, Y.; Wang, J.; and Jordan, M. I. 2015. Learning transferable features with deep adaptation networks. In ICML. Mahendran, A., and Vedaldi, A. 2015. Understanding deep image representations by inverting them. In CVPR.
Pan, S. J.; Tsang, I. W.; Kwok, J. T.; and Yang, Q. 2009. Domain adaptation via transfer component analysis. In IJCAI.
Saenko, K.; Kulis, B.; Fritz, M.; and Darrell, T. 2010. Adapting visual category models to new domains. In ECCV.
Shekhar, S.; Patel, V. M.; Nguyen, H. V.; and Chellappa, R. 2013. Generalized domain-adaptive dictionaries. In CVPR.</p>
<p>Sun, B., and Saenko, K. 2014. From virtual to reality: Fast adaptation of virtual object detectors to real domains. In $B M V C$.
Tommasi, T., and Caputo, B. 2013. Frustratingly easy NBNN domain adaptation. In ICCV.
Tommasi, T., and Tuytelaars, T. 2014. A testbed for crossdataset analysis. In ECCV TASK-CV Workshop.
Torralba, A., and Efros, A. A. 2011. Unbiased look at dataset bias. In CVPR.
Tzeng, E.; Hoffman, J.; Zhang, N.; Saenko, K.; and Darrell, T. 2014. Deep domain confusion: Maximizing for domain invariance. CoRR abs/1412.3474.
Xiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba, A. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR.
Yang, J.; Yan, R.; and Hauptmann, A. 2007. Adapting SVM classifiers to data with shifted distributions. In ICDM Workshop.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright (C) 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
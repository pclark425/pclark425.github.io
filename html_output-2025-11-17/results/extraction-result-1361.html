<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1361 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1361</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1361</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-b55de6a119991437d58072baa5240edc8895adcd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b55de6a119991437d58072baa5240edc8895adcd" target="_blank">Exploration Based Language Learning for Text-Based Games</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work presents an exploration and imitation-learning-based agent capable of state-of-the-art performance in playing text-based computer games and shows that the learned policy can generalize better than existing solutions to unseen games without using any restriction on the action space.</p>
                <p><strong>Paper Abstract:</strong> This work presents an exploration and imitation-learning-based agent capable of state-of-the-art performance in playing text-based computer games. 

These games are of interest as they can be seen as a testbed for language understanding, problem-solving, and language generation by artificial agents. Moreover, they provide a learning setting in which these skills can be acquired through interactions with an environment rather than using fixed corpora. 

One aspect that makes these games particularly challenging for learning agents is the combinatorially large action space. Existing methods for solving text-based games are limited to games that are either very simple or have an action space restricted to a predetermined set of admissible actions.

In this work, we propose to use the exploration approach of Go-Explore for solving text-based games. More specifically, in an initial exploration phase, we first extract trajectories with high rewards, after which we train a policy to solve the game by imitating these trajectories.

Our experiments show that this approach outperforms existing solutions in solving text-based games, and it is more sample efficient in terms of the number of interactions with the environment.

Moreover, we show that the learned policy can generalize better than existing solutions to unseen games without using any restriction on the action space.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1361.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1361.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoinCollector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoinCollector (text-based navigation toy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of text-based navigation games in which an agent must navigate connected rooms to find and take a single coin; used to evaluate long-horizon, sparse-reward exploration in text worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counting to explore and generalize in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CoinCollector</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic text-adventure navigation domain consisting of connected rooms; objective is to travel through rooms and take a coin located in a specific room (single +1 terminal reward). Domain: abstract/textual maze navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Described as a set of connected rooms (implicit maze-like connectivity). Sparse connectivity typical of maze-like layouts; specifics (average degree etc.) not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Max rooms reported: 90 (CoinCollector statistic in paper). Hard instances require at least ~30 actions to reach the coin (trajectory length ~30).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Go-Explore (Phase 1)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Archive-based exploration that discretizes observations into cells (sum of room token embeddings + cumulative reward), selects under-explored cells, returns to them using simulator determinism, and performs random exploration from there; action space restricted to admissible actions during Phase 1.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of environment interactions (frames) and trajectory length (steps to goal); sample efficiency (frames to find reward).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Phase 1 Go-Explore finds an optimal trajectory (~30 steps) with approximately half the environment interactions (frames) compared to DQN++/DRQN++ baselines (exact frame counts not reported in text). Trajectory length found by Go-Explore: optimal 30 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Archive-driven exploration (return-to-cell) for finding high-reward trajectories; then imitation (Seq2Seq) for generalization — memory of trajectories and state-conditioned policy beneficial for long-horizon sparse reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Longer shortest paths / longer required trajectories (hard CoinCollector instances require ~30 steps) make exploration much harder for naive RL; archive-based exploration that returns to promising states markedly improves sample efficiency and finds shorter/optimal trajectories. Specific topological metrics (diameter, clustering coefficient) are not numerically reported, but path length (trajectory length) is explicitly tied to exploration difficulty and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that can leverage a return-to-state mechanism (archive + deterministic reset) and exploit trajectory memory are more effective in long-horizon, sparse-reward navigation; reactive/tabular approaches struggle when required path length grows. The paper reports Go-Explore produces optimal shorter trajectories, implying benefit from trajectory-centric policy structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration Based Language Learning for Text-Based Games', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1361.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1361.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CookingWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CookingWorld (First TextWorld Problems challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large set of procedurally generated text-based games where the agent must gather ingredients and perform multi-step cooking actions according to a recipe across multiple rooms; used to evaluate language-conditioned planning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>First textworld problems: A reinforcement and language learning challenge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CookingWorld (TextWorld-generated)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated cooking-themed text-adventure games (222 levels, 4,440 games) where agents collect ingredients/objects and perform multi-step recipe actions across rooms; domain: household/cooking tasks in text-world maps.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Implicit room maps (room connectivity varies across generated games); described as room maps with varying connectivity (sparse to moderate). Specific graph statistics (degree, clustering) not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Max rooms reported per game: 12 (CookingWorld statistic in paper). 4,440 total games spanning 222 distinct map/recipe templates.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Go-Explore Phase 1 and Go-Explore Seq2Seq (imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Phase 1: archive-based exploration using binned sum-of-embedding room descriptions + cumulative reward as cells, exploring from selected cells using admissible actions. Phase 2: Seq2Seq encoder-decoder trained via imitation on extracted high-reward trajectories to produce actions token-by-token (no admissible actions at test time).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Sum of environment steps (trajectory steps), number of games solved (wins), and total accumulated score across games; sample efficiency measured as steps per solved game or frames to find winning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Go-Explore Phase 1 (Single setting) achieved total score 19,530 with 47,562 steps and found winning trajectories in 4,279/4,440 games (approx. 96.3% wins). Go-Explore Seq2Seq (imitation) achieved score 17,507 with 68,571 steps and 3,757 wins in Single setting. Baselines (DRRN, LSTM-DQN variants) required more steps or achieved lower scores (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Phase 1: 4279 wins out of 4440 games (~96.34%) in Single setting; Seq2Seq imitation: 3757/4440 (~84.63%) wins in Single setting. Joint and Zero-Shot settings show lower success rates (see paper Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Two-stage approach: (1) exploration-focused archive-based search to extract high-reward trajectories; (2) sequence-generation (Seq2Seq) imitation policy for generalization. Ranking admissible-action scorers (DRRN) perform well when admissible actions are available; imitation yields better zero-shot generalization without admissible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>The paper links exploration difficulty to long horizons and sparse rewards induced by the environment layout and tasks; smaller maps (max rooms=12) still require complex multi-step strategies, and overlapping room descriptions across different games harm generalization (same observation may require different actions in different maps). No quantitative analysis of topological measures (diameter/clustering) is provided, but empirical findings show that archive-based exploration enables finding high-quality trajectories across diverse generated map topologies and that Seq2Seq imitation generalizes better across unseen maps than some RL baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Successful policies combine explicit exploration memory (archive of cells/trajectories) and state-conditioned sequence-generation for action production. The authors note that ambiguous/overlapping observations across different map layouts (topology-driven perceptual aliasing) degrade performance in the Joint setting and recommend richer state encoders (hierarchical models or knowledge-graph-based representations) to disentangle similar observations from different underlying map/topology contexts. Go-Explore's ability to return to archived states (rather than re-exploring from scratch) is highlighted as critical for efficiently discovering long-horizon solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration Based Language Learning for Text-Based Games', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Go-explore: a new approach for hard-exploration problems <em>(Rating: 2)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Transfer in deep reinforcement learning using knowledge graphs <em>(Rating: 2)</em></li>
                <li>Towards solving text-based games by producing adaptive action spaces <em>(Rating: 1)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1361",
    "paper_id": "paper-b55de6a119991437d58072baa5240edc8895adcd",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "CoinCollector",
            "name_full": "CoinCollector (text-based navigation toy)",
            "brief_description": "A family of text-based navigation games in which an agent must navigate connected rooms to find and take a single coin; used to evaluate long-horizon, sparse-reward exploration in text worlds.",
            "citation_title": "Counting to explore and generalize in text-based games",
            "mention_or_use": "use",
            "environment_name": "CoinCollector",
            "environment_description": "Synthetic text-adventure navigation domain consisting of connected rooms; objective is to travel through rooms and take a coin located in a specific room (single +1 terminal reward). Domain: abstract/textual maze navigation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Described as a set of connected rooms (implicit maze-like connectivity). Sparse connectivity typical of maze-like layouts; specifics (average degree etc.) not reported.",
            "environment_size": "Max rooms reported: 90 (CoinCollector statistic in paper). Hard instances require at least ~30 actions to reach the coin (trajectory length ~30).",
            "agent_name": "Go-Explore (Phase 1)",
            "agent_description": "Archive-based exploration that discretizes observations into cells (sum of room token embeddings + cumulative reward), selects under-explored cells, returns to them using simulator determinism, and performs random exploration from there; action space restricted to admissible actions during Phase 1.",
            "exploration_efficiency_metric": "Number of environment interactions (frames) and trajectory length (steps to goal); sample efficiency (frames to find reward).",
            "exploration_efficiency_value": "Phase 1 Go-Explore finds an optimal trajectory (~30 steps) with approximately half the environment interactions (frames) compared to DQN++/DRQN++ baselines (exact frame counts not reported in text). Trajectory length found by Go-Explore: optimal 30 steps.",
            "success_rate": null,
            "optimal_policy_type": "Archive-driven exploration (return-to-cell) for finding high-reward trajectories; then imitation (Seq2Seq) for generalization — memory of trajectories and state-conditioned policy beneficial for long-horizon sparse reward tasks.",
            "topology_performance_relationship": "Longer shortest paths / longer required trajectories (hard CoinCollector instances require ~30 steps) make exploration much harder for naive RL; archive-based exploration that returns to promising states markedly improves sample efficiency and finds shorter/optimal trajectories. Specific topological metrics (diameter, clustering coefficient) are not numerically reported, but path length (trajectory length) is explicitly tied to exploration difficulty and performance.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Policies that can leverage a return-to-state mechanism (archive + deterministic reset) and exploit trajectory memory are more effective in long-horizon, sparse-reward navigation; reactive/tabular approaches struggle when required path length grows. The paper reports Go-Explore produces optimal shorter trajectories, implying benefit from trajectory-centric policy structure.",
            "uuid": "e1361.0",
            "source_info": {
                "paper_title": "Exploration Based Language Learning for Text-Based Games",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "CookingWorld",
            "name_full": "CookingWorld (First TextWorld Problems challenge)",
            "brief_description": "A large set of procedurally generated text-based games where the agent must gather ingredients and perform multi-step cooking actions according to a recipe across multiple rooms; used to evaluate language-conditioned planning and generalization.",
            "citation_title": "First textworld problems: A reinforcement and language learning challenge",
            "mention_or_use": "use",
            "environment_name": "CookingWorld (TextWorld-generated)",
            "environment_description": "Procedurally generated cooking-themed text-adventure games (222 levels, 4,440 games) where agents collect ingredients/objects and perform multi-step recipe actions across rooms; domain: household/cooking tasks in text-world maps.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Implicit room maps (room connectivity varies across generated games); described as room maps with varying connectivity (sparse to moderate). Specific graph statistics (degree, clustering) not reported.",
            "environment_size": "Max rooms reported per game: 12 (CookingWorld statistic in paper). 4,440 total games spanning 222 distinct map/recipe templates.",
            "agent_name": "Go-Explore Phase 1 and Go-Explore Seq2Seq (imitation)",
            "agent_description": "Phase 1: archive-based exploration using binned sum-of-embedding room descriptions + cumulative reward as cells, exploring from selected cells using admissible actions. Phase 2: Seq2Seq encoder-decoder trained via imitation on extracted high-reward trajectories to produce actions token-by-token (no admissible actions at test time).",
            "exploration_efficiency_metric": "Sum of environment steps (trajectory steps), number of games solved (wins), and total accumulated score across games; sample efficiency measured as steps per solved game or frames to find winning trajectories.",
            "exploration_efficiency_value": "Go-Explore Phase 1 (Single setting) achieved total score 19,530 with 47,562 steps and found winning trajectories in 4,279/4,440 games (approx. 96.3% wins). Go-Explore Seq2Seq (imitation) achieved score 17,507 with 68,571 steps and 3,757 wins in Single setting. Baselines (DRRN, LSTM-DQN variants) required more steps or achieved lower scores (see Table 3).",
            "success_rate": "Phase 1: 4279 wins out of 4440 games (~96.34%) in Single setting; Seq2Seq imitation: 3757/4440 (~84.63%) wins in Single setting. Joint and Zero-Shot settings show lower success rates (see paper Table 3).",
            "optimal_policy_type": "Two-stage approach: (1) exploration-focused archive-based search to extract high-reward trajectories; (2) sequence-generation (Seq2Seq) imitation policy for generalization. Ranking admissible-action scorers (DRRN) perform well when admissible actions are available; imitation yields better zero-shot generalization without admissible actions.",
            "topology_performance_relationship": "The paper links exploration difficulty to long horizons and sparse rewards induced by the environment layout and tasks; smaller maps (max rooms=12) still require complex multi-step strategies, and overlapping room descriptions across different games harm generalization (same observation may require different actions in different maps). No quantitative analysis of topological measures (diameter/clustering) is provided, but empirical findings show that archive-based exploration enables finding high-quality trajectories across diverse generated map topologies and that Seq2Seq imitation generalizes better across unseen maps than some RL baselines.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Successful policies combine explicit exploration memory (archive of cells/trajectories) and state-conditioned sequence-generation for action production. The authors note that ambiguous/overlapping observations across different map layouts (topology-driven perceptual aliasing) degrade performance in the Joint setting and recommend richer state encoders (hierarchical models or knowledge-graph-based representations) to disentangle similar observations from different underlying map/topology contexts. Go-Explore's ability to return to archived states (rather than re-exploring from scratch) is highlighted as critical for efficiently discovering long-horizon solutions.",
            "uuid": "e1361.1",
            "source_info": {
                "paper_title": "Exploration Based Language Learning for Text-Based Games",
                "publication_date_yy_mm": "2020-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Go-explore: a new approach for hard-exploration problems",
            "rating": 2
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Transfer in deep reinforcement learning using knowledge graphs",
            "rating": 2
        },
        {
            "paper_title": "Towards solving text-based games by producing adaptive action spaces",
            "rating": 1
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 2
        }
    ],
    "cost": 0.01126,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploration Based Language Learning for Text-Based Games</h1>
<p>Andrea Madotto ${ }^{1 <em>}$, Mahdi Namazifar ${ }^{2}$, Joost Huizinga ${ }^{2}$, Piero Molino ${ }^{2}$, Adrien Ecoffet $^{2}$, Huaixiu Zheng ${ }^{3 </em>}$, Dian Yu ${ }^{4 <em>}$, Alexandros Papangelis ${ }^{2}$, Chandra Khatri ${ }^{2}$, Gokhan Tur $^{5 </em>}$<br>${ }^{1}$ The Hong Kong University Of Science and Technology<br>${ }^{2}$ Uber AI,<br>${ }^{3}$ Google Brain,<br>${ }^{4}$ UC-Davis,<br>${ }^{5}$ Amazon Alexa AI<br>amadotto@connect.ust.hk, dianyu@ucdavis.edu, huaixiu.zheng@google.com, {mahdin,jhuizinga,piero,adrienle,apapangelis,chandrak}@uber.com, gokhan.tur@ieee.org</p>
<h4>Abstract</h4>
<p>This work presents an exploration and imitation-learning-based agent capable of state-of-theart performance in playing text-based computer games. These games are of interest as they can be seen as a test bed for language understanding, problem-solving, and language generation by artificial agents. Moreover, they provide a learning setting in which these skills can be acquired through interactions with an environment rather than using fixed corpora. One aspect that makes these games particularly challenging for learning agents is the combinatorially large action space. Existing methods for solving text-based games are limited to games that are either very simple or have an action space restricted to a predetermined set of admissible actions. In this work, we propose to use the exploration approach of Go-Explore [Ecoffet et al., 2019] for solving text-based games. More specifically, in an initial exploration phase, we first extract trajectories with high rewards, after which we train a policy to solve the game by imitating these trajectories. Our experiments show that this approach outperforms existing solutions in solving text-based games, and it is more sample efficient in terms of number of interactions with the environment. Moreover, we show that the learned policy can generalize better than existing solutions to unseen games without using any restriction on the action space.</p>
<h2>1 Introduction</h2>
<p>Text-based games became popular in the mid 80s with the game series Zork [Anderson and Galley, 1985] resulting in many different text-based games being produced and published [Spaceman, 2019]. These games use a plain text description of the environment and the player has to interact</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>with them by writing natural-language commands. Recently, there has been a growing interest in developing agents that can automatically solve text-based games [Côté et al., 2018] by interacting with them. Since the actions in these games are commands that are in natural language form, the major obstacle is the extremely large action space of the agent, which leads to a combinatorially large exploration problem. In fact, with a vocabulary of $N$ words (e.g. 20K) and the possibility of producing sentences with at most $m$ words (e.g. 7 words), the total number of actions is $O\left(N^{m}\right)$ (e.g. $20 \mathrm{~K}^{7}=1.28 e^{30}$ ). To avoid this large action space, several existing solutions focus on simpler text-based games with very small vocabularies where the action space is constrained to verb-object pairs [Narasimhan et al., 2015]. Moreover, many existing works rely on using predetermined sets of admissible actions [He et al., 2015; Tessler et al., 2019; Zahavy et al., 2018]. However, a more ideal, and still under explored, alternative would be an agent that can operate in the full, unconstrained action space of natural language that can systematically generalize to new text-based games with no or few interactions with the environment.</p>
<p>To address this challenge, we propose to adapt the recently proposed Go-Explore [Ecoffet et al., 2019] algorithm. Specifically, we propose to first extract high reward trajectories of states and actions in the game using the exploration methodology proposed in Go-Explore and then train a policy using a Seq2Seq [Sutskever et al., 2014] model that maps observations to actions, in an imitation learning fashion. To show the effectiveness of our proposed methodology, we first benchmark the exploration ability of our Go-Explore variant on the family of text-based games called CoinCollector [Yuan et al., 2018]. Then we use the 4,440 games from the popular "First TextWorld Problems" [Côté, 2018], challenge, which are generated using TextWorld [Côté et al., 2018], to show the generalization ability of our proposed methodology. In the former experiment we show that our Go-Explore variant finds winning trajectories faster than existing solutions, and in the latter, we show that training a Seq2Seq model on the trajectories found during exploration results in stronger generalization, as suggested by the stronger performance on un-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dsc.</th>
<th style="text-align: left;">-= garden $=-$ well, here we are in <br> a garden. There is a roasted red <br> apple and a red onion on the floor</th>
<th style="text-align: left;">Admissible. Com.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Inv.</td>
<td style="text-align: left;">You are carrying: a black pepper</td>
<td style="text-align: left;">- drop black pepper</td>
</tr>
<tr>
<td style="text-align: center;">Prev.A.</td>
<td style="text-align: left;">Open screen door</td>
<td style="text-align: left;">- eat black pepper</td>
</tr>
<tr>
<td style="text-align: center;">Quest.</td>
<td style="text-align: left;">Gather all following ingredients <br> and follow the directions to <br> prepare this tasty meal. <br> Ingredients: black pepper, red <br> apple salt. Directions: chop the <br> red apple, roast the red apple, <br> prepare meal</td>
<td style="text-align: left;">- examine red apple <br> - examine red onion <br> - go north <br> - look <br> - take red apple <br> - take red onion</td>
</tr>
<tr>
<td style="text-align: center;">Feed.</td>
<td style="text-align: left;">That 's already open</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Example of the observations provided by the CookingWorld environment.
seen games, compared to existing competitive baselines.
To summarize, our contributions include: 1) first use of Go-Explore beyond the Atari setting; 2) substantial modification to the original Go-Explore by including the reward in the cell representation, as well as, using imitation learning in the second phase, instead of PPO [Schulman et al., 2017], which make the algorithm work in a combinatorial action space; 3) state-of-the-art performance in several text-based games; 4) first to test the generalization of the learned agents both across games and on unseen games.</p>
<h2>2 Related work</h2>
<p>Reinforcement Learning Based Approaches for TextBased Games Among reinforcement learning based efforts to solve text-based games two approaches are prominent. The first approach assumes an action as a sentence of a fixed number of words, and associates a separate $Q$-function [Watkins, 1989; Mnih et al., 2015] with each word position in this sentence. This method was demonstrated with two-word sentences consisting of a verb-object pair (e.g. take apple) [Narasimhan et al., 2015; Yuan et al., 2018]. In the second approach, one $Q$-function that scores all possible actions (i.e. sentences) is learned and used to play the game [He et al., 2015; Tessler et al., 2019; Zahavy et al., 2018]. The first approach is quite limiting since a fixed number of words must be selected in advance and no sequential dependency is enforced between words (e.g. lack of language modelling). In the second approach, on the other hand, the number of possible actions can become exponentially large if the admissible actions (a predetermined low cardinality set of actions that the agent can take) are not provided to the agent. A possible solution to this issue has been proposed by [Tao et al., 2018]: a hierarchical pointer-generator is used to first produce the set of admissible actions given the observation, and subsequently to choose one element from this set as the action to perform. However, in our experiments we show that even in settings where the true set of admissible actions is provided by the environment, a $Q$-scorer [He et al., 2015] does not generalize well on unseen games (Section 5.2 Zero-Shot) and we would expect performance to degrade even further if the admissible actions were generated by a separate model. Less common are models that either learn to reduce a large set of actions into a smaller set of admissible actions by eliminat-
ing actions [Zahavy et al., 2018] or by compressing them in a latent space [Tessler et al., 2019].
Exploration in Reinforcement Learning In most textbased games rewards are sparse, since the size of the action space makes the probability of observing a reward extremely low when taking only random actions. Sparse reward environments are particularly challenging for reinforcement learning as they require longer term planning. Many exploration based solutions have been proposed to address the challenges associated with reward sparsity. Among these exploration approaches are novelty search [Achiam and Sastry, 2017; Burda et al., 2018], intrinsic motivation [Oudeyer and Kaplan, 2009; Barto, 2013], and curiosity based rewards [Schmidhuber, 2006; Schmidhuber, 1991; Pathak et al., 2017]. For text based games exploration methods have been studied by [Yuan et al., 2018], where the authors showed the effectiveness of the episodic discovery bonus [Gershman and Daw, 2017] in environments with sparse rewards. This exploration method can only be applied in games with very small action and state spaces, since their counting methods rely on the state in its explicit raw form.</p>
<h2>3 Methodology</h2>
<p>Go-Explore [Ecoffet et al., 2019] differs from the exploration-based algorithms discussed above in that it explicitly keeps track of under-explored areas of the state space and in that it utilizes the determinism of the simulator in order to return to those states, allowing it to explore sparse-reward environments in a sample efficient way (see [Ecoffet et al., 2019] as well as section 5.1). Go-Explore is composed of two phases. In phase 1 (also referred to as the "exploration" phase) the algorithm explores the state space while keeping track of previously visited states by maintaining an archive. During this phase, instead of resuming the exploration from scratch, the algorithm starts exploring from promising states in the archive to find high performing trajectories. In phase 2 (also referred to as the "robustification" phase, while in our variant we will call it "generalization") the algorithm trains a policy using the trajectories found in phase 1. Following this framework, we define the Go-Explore phases for text-based games. Let us first define text-based games using the same notation as [Yuan et al., 2018]. A text-based game can be framed as a discrete-time Partially Observable Markov Decision Process (POMDP) [Kaelbling et al., 1998] defined by $(S, T, A, \Omega, O, R)$, where: $S$ is the set of the environment states, $T$ is the state transition function that defines the next state probability, i.e. $T\left(s_{t+1} \mid a_{t} ; s_{t}\right) \forall s_{t} \in S, A$ is the set of actions, which in our case is all the possible sequences of tokens, $\Omega$ is the set of observations, i.e. text observed by the agent every time it has to take an action in the game (i.e. dialogue turn) which is controlled by the conditional observation probability $O$, i.e. $O\left(o_{t} \mid s_{t}, a_{t-1}\right)$, and, finally, $R$ is the reward function i.e. $r=R(s, a)$.</p>
<p>Next we define the observation $o_{t} \in \Omega$ and the action $a_{t} \in A$. Text-based games provide some information in plain text at each turn and, without loss of generality, we define an observation $o_{t}$ as the sequence of tokens $\left{o_{t}^{0}, \cdots, o_{t}^{n}\right}$ that form that text. Similarly, we define the tokens of an action</p>
<p>$a_{t}$ as the sequence $\left{a_{t}^{0}, \cdots, a_{t}^{m}\right}$. Furthermore, we define the set of admissible actions $\mathcal{A}<em t="t">{t} \in A$ as $\mathcal{A}</em>$.}=\left{a_{0}, \cdots, a_{z}\right}$, where each $a_{i}$, which is a sequence of tokens, is grammatically correct and admissible with reference to the observation $o_{t</p>
<h3>3.1 Phase 1: Exploration</h3>
<p>In phase 1, Go-Explore builds an archive of cells, where a cell is defined as a set of observations that are mapped to the same, discrete representation by some mapping function $f(o)$. Each cell is associated with meta-data including the trajectory towards that cell, the length of that trajectory, and the cumulative reward of that trajectory. New cells are added to the archive when they are encountered in the environment, and existing cells are updated with new meta-data when the trajectory towards that cells is higher scoring or equal scoring but shorter.</p>
<p>At each iteration the algorithm selects a cell from this archive based on meta-data of the cell (e.g. the accumulated reward, etc.) and starts to randomly explore from the end of the trajectory associated with the selected cell. Phase 1 requires three components: the observation-to-cell mapping function $f(o)$, the cell selection criterion (based on cumulative rewardin our case), and the way actions are selected when exploring from a selected cell (randomly in our case). In our variant of the algorithm, $f(o)$ is defined as follows: given an observation, we compute the sum of the word embedding for each token in this observation, and then concatenate this sum with the current cumulative reward to construct the cell representation. The resulting vectors are subsequently compressed and discretized by binning them in order to map similar observations to the same cell. This way, the cell representation, which is the key of the archive, incorporates information about the current observation of the game. Adding the current cumulative reward to the cell representation is new to our GoExplore variant, as in the original algorithm only down-scaled image pixels were used. This choice was fundamental in increasing the speed at which high reward trajectories are discovered, making the exploration feasible. In phase 1, we restrict the action space to the set of admissible actions $\mathcal{A}<em g="g">{t}$ that are provided by the game at every step of the game ${ }^{1}$. This too is particularly important for the exploration to find a high reward trajectory faster. Finally, we denote the trajectory found in phase 1 for game $g$ as $\mathcal{T}</em>\right)\right]$.}=\left[\left(o_{0}, a_{0}, r_{0}\right), \cdots,\left(o_{t}, a_{t}, r_{t</p>
<h3>3.2 Phase 2: Generalization</h3>
<p>Phase 2 of Go-Explore uses the trajectories found in phase 1 and trains a policy based on those trajectories. The goal of this phase in the original Go-Explore algorithm is to turn the fragile policy of playing a trajectory of actions in sequence into a more robust, state-conditioned policy that can thus deal</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>with environmental stochasticity. In our variant of the algorithm the purpose of the second phase is generalization, although in our environment there is no stochasticity, our goal is to learn a general policy that can be applied across different games and generalizes to unseen games. In the original Go-Explore implementation, the authors used the backward Proximal Policy Optimization algorithm (PPO) [Salimans and Chen, 2018; Schulman et al., 2017] to train this policy. In this work we opt for a simple but effective Seq2Seq imitation learning approach that does not use the reward directly in the loss and copes with the vast action space generating one token at a time sequentially. More specifically, given the trajectory $\mathcal{T}<em t="t">{g}$, we train a Seq2Seq model to minimize the negative log-likelihood of the action $a</em>}$ given the observation $o_{t}$. We define a word embedding matrix $E \in \mathbb{R}^{d \times|V|}$, where $d$ is the embedding size and $|V|$ is the cardinality of the vocabulary, which maps the input token to an embedded vector. Then, we define an encoder $\mathrm{LSTM<em _dec="{dec" _text="\text">{\text {enc }}$ and a decoder $\mathrm{LSTM}</em>}}$. Every token of $o_{t}$ from the trajectory $\mathcal{T<em _enc="{enc" _text="\text">{g}$ is embedded using $E$ and the sequence of these embedding vectors is passed through $\mathrm{LSTM}</em>$ :}</p>
<p>$$
h_{i}^{e n c}=\operatorname{LSTM}<em t="t">{e n c}\left(E\left(o</em>\right)
$$}^{i}\right), h_{i-1}^{e n c</p>
<p>The last hidden state $h_{|o_{t}|}^{e n c}$ is used as the initial hidden state of the decoder which generates the action $a_{t}$ token by token. Specifically, given the sequence of hidden states $H \in \mathbb{R}^{d \times\left|o_{t}\right|}$ of the encoder, tokens $a_{t}^{j}$ are generated as follows:</p>
<p>$$
\begin{aligned}
h_{j}^{d e c} &amp; =\operatorname{LSTM}<em t="t">{d e c}\left(E\left(a</em>\right) \
c_{j} &amp; =\operatorname{Softmax}\left(h_{t}^{d e c} H\right) H^{T} \
\operatorname{dist}}^{(j-1)}\right), h_{j-1}^{d e c<em j="j">{t}^{j} &amp; =\operatorname{Softmax}\left(W\left[h</em>\right]\right)
\end{aligned}
$$}^{d e c} ; c_{j</p>
<p>where $W \in \mathbb{R}^{2 d \times|V|}$ is a matrix that maps the decoder hidden state, concatenated with the context vector, into a vocabularysized vector. During training, the parameters of the model are trained by minimizing:</p>
<p>$$
L_{P\left(a_{t} \mid o_{t}\right)}=-\sum_{k}^{\left|a_{t}\right|} \log \left(\operatorname{dist}<em t="t">{k}^{j}\left(a</em>\right)\right)
$$</p>
<p>which is the sum of the negative log likelihood of each token in $a_{t}$ (using teacher forcing [Williams and Zipser, 1989]). However, at test time the model produces the sequence in an auto-regressive manner using greedy search.</p>
<h2>4 Experiments</h2>
<h3>4.1 Games and experiments setup</h3>
<p>[Yuan et al., 2018; Côté et al., 2018; Narasimhan et al., 2015] proposes a set of commonly used standard benchmarks for agents that play text-based games that require no more than two words in each step to solve the game and have a very limited number of admissible actions per observation. While simple, this setting limits the agent's ability to fully express natural language and learn more complex ways to speak. In this paper, we embrace more challenging environments where multiple words are needed at each step to solve the games and the reward is particularly sparse. Hence, we have selected the following environments:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CoinCollector</th>
<th style="text-align: center;">CookingWorld</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vocabulary $(</td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">)$</td>
</tr>
<tr>
<td style="text-align: center;">Action Space</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$20,000^{5}$</td>
</tr>
<tr>
<td style="text-align: center;"># Level</td>
<td style="text-align: center;">1 (Hard)</td>
<td style="text-align: center;">222</td>
</tr>
<tr>
<td style="text-align: center;">Max Room</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;"># Tok. Description</td>
<td style="text-align: center;">$64 \pm 9$</td>
<td style="text-align: center;">$97 \pm 49$</td>
</tr>
<tr>
<td style="text-align: center;"># Adm. Actions</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$14 \pm 13$</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of the two families of text-based games used in the experiments. The average is among the different games in CookingWorld and among different instance of the same game for CoinCollector.</p>
<ul>
<li>CoinCollector [Yuan et al., 2018] is a class of text-based games where the objective is to find and collect a coin from a specific location in a given set of connected rooms. The agent wins the game after it collects the coin, at which point (for the first and only time) a reward of +1 is provided. In this game the environment parses only five admissible commands made of two words: go north, go east, go south, go west, and take coin.</li>
<li>CookingWorld [Côté, 2018] in this challenge, there are 4,440 games with 222 different levels of difficulty, with 20 games per level of difficulty, each with different entities and maps. The goal of each game is to cook and eat food from a given recipe, which includes the task of collecting ingredients (e.g. tomato, potato, etc.), objects (e.g. knife), and processing them according to the recipe (e.g. cook potato, slice tomato, etc.). The parser of each game accepts 18 verbs and 51 entities with a predefined grammar, but the overall size of the vocabulary of the observations is 20,000 .
Table 2 sumurizes the statistics of the enviroments used in the experiments. In our experiments, we try to address two major research questions. First, we want to benchmark the exploration power of phase 1 of Go-Explore in comparison to existing exploration approaches used in text-based games. For this purpose, we generate 10 CoinCollector games with the hardest setting used by [Yuan et al., 2018], i.e. hard-level 30 and use them as a benchmark. In fact, CoinCollector requires many actions (at least 30 on hard games) to find a reward, which makes it suitable for testing the exploration capabilities of different algorithms. Secondly, we want to verify the generalization ability of our model in creating complex strategies using natural language. CoinCollector has a very limited action space, and is mainly designed to benchmark models on their capability of dealing with sparse rewards. Therefore we use the more complex CookingWorld games to evaluate the generalization capabilities of our proposed approach. We design three different settings for CookingWorld: 1) Single: treat each game independently, which means we train and test one agent for each game to evaluate how robust different models are across different games.; 2) Joint: training and testing a single policy on all the 4,440 CookingWorld games at the same time to verify that models can learn to play multiple games at the same time; 3) Zero-Shot: split the games into training, validation, and test sets, and then train our policy on the training games and test it on the unseen test games. This setting is the hardest among all, since it requires generalization to unseen games.
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ul>
<p>Figure 1: CoinCollector results of DQN++ and DRQN++ versus Go-Explore Phase 1, i.e. just exploration.</p>
<p>In both CoinCollector and CookingWorld games an observation $o_{t}$ provided by the environment consists of a room description $D$, inventory information $I$, quest $Q$, previous action $P$ and feedback $F$ provided in the previous turn. Table 1 shows an example for each of these components. In our experiments for phase 1 of Go-Explore we only use $D$ as the observation.</p>
<h3>4.2 Baselines</h3>
<p>For the CoinCollector games, we compared Go-Explore with the episodic discovery bonus [Gershman and Daw, 2017] that was used by [Yuan et al., 2018] to improve two Q-learningbased baselines: DQN++ and DRQN++. We used the code provided by the authors and the same hyper-parameters ${ }^{2}$.</p>
<p>For the CookingWorld games, we implemented four different treatments based on two existing methods:</p>
<ul>
<li>LSTM-DQN [Narasimhan et al., 2015; Yuan et al., 2018]: An LSTM based state encoder with a separate $Q$-functions for each component (word) of a fixed pattern of Verb, Adjective1, Noun1, Adjective2, and Noun2. In this approach, given the observation $o_{t}$, the tokens are first converted into embeddings, then an LSTM is used to extract a sequence of hidden states $H_{\text {dqn }} \in \mathbb{R}^{d \times|o_{t}|}$. A mean-pool layer is applied to $H_{\text {dqn }}$ to produce a single vector $h_{o^{\prime}}$ that represents the whole sequence. Next, a linear transformation $W_{\text {type }} \in \mathbb{R}^{d \times\left|V_{\text {type }}\right|}$ is used to generate each of the Q values, where $\left|V_{\text {type }}\right| \ll|V|$ is the subset of the original vocabulary restricted to the word type of a particular game (e.g for Verb type: take, drop, etc.). Formally, we have:</li>
</ul>
<p>$$
\begin{aligned}
&amp; Q\left(o_{t}, a_{\text {type }}\right)=h_{o^{\prime}} W_{\text {type }} \
&amp; \text { where, type } \in{\text { Verb, Obj, Noun, Obj2,Noun2 }}
\end{aligned}
$$</p>
<p>Next, all the $Q$-functions are jointly trained using the DQN algorithm with $\epsilon$-greedy exploration [Watkins, 1989;</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Mnih et al., 2015]. At evaluation time, the argmax of each $Q$-function is concatenated to produce $a_{t}$. Importantly, in $V_{\text {type }}$ a special token $&lt;\mathrm{s}&gt;$ is used to denote the absence of a word, so the model can produce actions with different lengths.</p>
<ul>
<li>LSTM-DQN+ADM: It is the same model as LSTM-DQN, except that the random actions for $\epsilon$-greedy exploration are sampled from the set of admissible actions instead of creating them by sampling each word separately.</li>
<li>LSTM-DQN+ADM+CNT: It is the same model as the previous case, but with the exploration based reward as in DQN++ [Yuan et al., 2018].</li>
<li>DRRN [He et al., 2015]: In this approach a model learns how to score admissible actions instead of directly generating the action token by token. The policy uses an LSTM for encoding the observation and actions are represented as the sum of the embedding of the word tokens they contain. Then, the $Q$ value is defined as the dot product between the embedded representations of the observation and the action. Following the aforementioned notation, $h_{o^{i}}$ is generated as in the LSTM-DQN baseline. Next, we define its embedded representation as $c_{i}=\sum_{k}^{\left|a_{i}\right|} E\left(a_{i}^{k}\right)$, where $E$ is an embedding matrix as in Equation 1. Thus, the $Q$ function is defined as:</li>
</ul>
<p>$$
Q\left(o_{t}, a_{i}\right)=h_{o^{i}} c_{i}
$$</p>
<p>At test time the action with the highest $Q$ value is chosen.</p>
<h3>4.3 Hyper-parameters</h3>
<p>In all the games the maximum number of steps has been set to 50. As mentioned earlier, the cell representation used in the Go-Explore archive is computed as the binning of the sum of embeddings of the room description tokens concatenated with the current cumulative reward. The sum of embeddings is computed using 50 dimensional pre-trained GloVe [Pennington et al., 2014] vectors. In the CoinCollector baselines we use the same hyper-parameters as in the original paper. In CookingWorld all the baselines use pre-trained GloVe of dimension 100 for the single setting and 300 for the joint one. The LSTM hidden state has been set to 300 for all the models.</p>
<h2>5 Results</h2>
<h3>5.1 CoinCollector</h3>
<p>In this setting, we compare the number of actions played in the environment (frames) and the score achieved by the agent (i.e. +1 reward if the coin is collected). In Go-Explore we also count the actions used to restore the environment to a selected cell, i.e. to bring the agent to the state represented in the selected cell. This allows a one-to-one comparison of the exploration efficiency between Go-Explore and algorithms that use a count-based reward in text-based games. Importantly, [Yuan et al., 2018] showed that DQN and DRQN, without such counting rewards, could never find a successful trajectory in hard games such as the ones used in our experiments. Figure 1 shows the number of interactions with the environment (frames) versus the maximum score obtained, averaged over 10 games of the same difficulty. As shown by
[Yuan et al., 2018], DRQN++ finds a trajectory with the maximum score faster than DQN++. On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment. Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively.</p>
<h3>5.2 CookingWorld</h3>
<p>In CookingWorld, we compared models in the three settings mentioned earlier, namely, Single, Joint, and Zero-shot. In all experiments, we measured the sum of the final scores of all the games and their trajectory length (number of steps). Table 3 summarizes the results in these three settings. Phase 1 of Go-Explore on single games achieves a total score of 19,530 (sum over all games), which is very close to the maximum possible points (i.e. 19,882), with 47,562 steps. A winning trajectory was found in 4,279 out of the total of 4,440 games. This result confirms again that the exploration strategy of GoExplore is effective in text-based games. Next, we evaluate the effectiveness and the generalization ability of the imitation learning policy trained using the extracted trajectories in phase 1 of Go-Explore in the three settings mentioned above.
Single In this setting, each model is trained from scratch in each of the 4,440 games based on the trajectory found in phase 1 of Go-Explore (previous step). As shown in Table 3, the LSTM-DQN [Narasimhan et al., 2015; Yuan et al., 2018] approach without the use of admissible actions performs poorly. One explanation for this could be that it is difficult for this model to explore both language and game strategy at the same time; it is hard for the model to find a reward signal before it has learned to model language, since almost none of its actions will be admissible, and those reward signals are what is necessary in order to learn the language model. As we see in Table 3, however, by using the admissible actions in the $\epsilon$-greedy step the score achieved by the LSTM-DQN increases dramatically (+ADM row in Table 3). DRRN [He et al., 2015] achieves a very high score, since it explicitly learns how to rank admissible actions (i.e. a much simpler task than generating text). Finally, our approach of using a Seq2Seq model trained on the single trajectory provided by phase 1 of Go-Explore achieves the highest score among all the methods, even though we do not use admissible actions in this phase. However, in this experiment the Seq2Seq model cannot perfectly replicate the provided trajectory and the total achieved score that is $9.4 \%$ lower compared to the total score achieved by phase 1 of Go-Explore.</p>
<p>Joint In this setting, a single model is trained on all the games at the same time, to test whether one agent can learn to play multiple games. Overall, our proposed model greatly outperforms the baselines, although, as expected, all the evaluated models achieved a lower performance compared to the single game setting. One reason for this could be that learning multiple games at the same time leads to a situation where the agent encounters similar observations in different games, and the correct action to take in different games may be different. Furthermore, it is important to note that the order in which games are presented greatly affects the performance of</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Single</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Joint</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Zero-Shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;">Win</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;">Win</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;">Win</td>
</tr>
<tr>
<td style="text-align: center;">LSTM-DQN</td>
<td style="text-align: center;">2206</td>
<td style="text-align: center;">201832</td>
<td style="text-align: center;">412</td>
<td style="text-align: center;">473</td>
<td style="text-align: center;">213618</td>
<td style="text-align: center;">172</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">21480</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">+ADM</td>
<td style="text-align: center;">10360</td>
<td style="text-align: center;">140940</td>
<td style="text-align: center;">1770</td>
<td style="text-align: center;">623</td>
<td style="text-align: center;">210283</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">21530</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">16075</td>
<td style="text-align: center;">78856</td>
<td style="text-align: center;">3195</td>
<td style="text-align: center;">4560</td>
<td style="text-align: center;">184888</td>
<td style="text-align: center;">216</td>
<td style="text-align: center;">451</td>
<td style="text-align: center;">17243</td>
<td style="text-align: center;">37</td>
</tr>
<tr>
<td style="text-align: center;">Go-Explore Seq2Seq</td>
<td style="text-align: center;">17507</td>
<td style="text-align: center;">68571</td>
<td style="text-align: center;">3757</td>
<td style="text-align: center;">11167</td>
<td style="text-align: center;">85967</td>
<td style="text-align: center;">2326</td>
<td style="text-align: center;">1038</td>
<td style="text-align: center;">10805</td>
<td style="text-align: center;">207</td>
</tr>
<tr>
<td style="text-align: center;">Go-Explore Phase 1</td>
<td style="text-align: center;">19530</td>
<td style="text-align: center;">47562</td>
<td style="text-align: center;">4279</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Max Possible</td>
<td style="text-align: center;">19882</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4440</td>
<td style="text-align: center;">19882</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4440</td>
<td style="text-align: center;">2034</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">444</td>
</tr>
</tbody>
</table>
<p>Table 3: CookingWorld results on the three evaluated settings single, joint and zero-shot. LSTM-DQN+ADM+CNT has not been reported in the table since adding the exploration based reward greatly reduce the performance of LSTM-DQN+ADM.</p>
<p>LSTM-DQN and DRRN. In our experiments, we tried both an easy-to-hard curriculum (i.e. sorting the games by increasing level of difficulty) and a shuffled curriculum. Shuffling the games at each epoch resulted in far better performance, thus we only report the latter.</p>
<p>Zero-Shot In this setting the 4,440 games are split into training, validation, and test games. The split is done randomly, but in a way that different difficulty levels (recipes 1, 2 and 3), are represented with equal ratios in all the 3 splits, i.e. stratified by difficulty. As shown in Table 3, the zeroshot performance of the RL baselines is poor, which could be attributed to the same reasons why RL baselines underperform in the Joint case. Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model, even though the DRRN model has access to the admissible actions at test time, while the GoExplore Seq2Seq model (as well as the LSTM-DQN model) has to construct actions token-by-token from the entire vocabulary of 20,000 tokens. On the other hand, Go-Explore Seq2Seq shows promising results by solving almost half of the unseen games. These results demonstrate both the relative effectiveness of training a Seq2Seq model on Go-Explore trajectories, but they also indicate that additional effort is needed for designing reinforcement learning algorithms that effectively generalize to unseen games.</p>
<h2>6 Discussion</h2>
<p>Experimental results show that our proposed Go-Explore exploration strategy is a viable methodology for extracting highperforming trajectories in text-based games. This method allows us to train supervised models that can outperform existing models in the experimental settings that we study. Finally, there are still challenges and limitations that both our methodology and previous solutions do not fully address yet. For instance:</p>
<p>State Representation The state representation is the main limitation of our proposed imitation learning model. In fact, by examining the observations provided in different games, we notice a large overlap in the descriptions $(D)$ of the games. This overlap leads to a situation where the policy receives very similar observations, but is expected to imitate two different actions. This shows especially in the Joint setting of CookingWorld, where the 222 games are repeated 20 times with different entities and room maps. In this work, we opted for a simple Seq2Seq model for our policy, since our goal is to show the effectiveness of our proposed exploration methods.</p>
<p>However, a more complex Hierarchical-Seq2Seq model [Sordoni et al., 2015] or a better encoder representation based on knowledge graphs [Ammanabrolu and Riedl, 2019a; Ammanabrolu and Riedl, 2019b] would likely improve the performance.
Language Based Exploration In Go-Explore, the given admissible actions are used during random exploration. However, in more complex games, e.g. Zork I and in general the Z-Machine games, these admissible actions are not provided. In such settings, the action space would explode in size, and thus Go-Explore, even with an appropriate cell representation, would have a hard time finding good trajectories. To address this issue one could leverage general language models to produce a set of grammatically correct actions. Alternatively one could iteratively learn a policy to sample actions, while exploring with Go-Explore. Both strategies are viable, and a comparison is left to future work.</p>
<p>It is worth noting that a hand-tailored solution for the CookingWorld games has been proposed in the "First TextWorld Problems" competition [Côté et al., 2018]. This solution managed to obtain up to $91.9 \%$ of the maximum possible score across the 514 test games on an unpublished dataset. However, this solution relies on entity extraction and template filling, which we believe limits its potential for generalization.</p>
<h2>7 Conclusion</h2>
<p>In this paper we presented a novel methodology for solving text-based games which first extracts high-performing trajectories using a modified version of phase 1 of Go-Explore and then trains a simple Seq2Seq model that maps observations to actions using the extracted trajectories. Our experiments show state-of-the-art results in three settings, with improved generalization and sample efficiency compared to existing methods. Finally, we discussed the limitations and possible improvements of our methodology, which lead to new research challenges in text-based games.</p>
<h2>References</h2>
<p>[Achiam and Sastry, 2017] Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning. arXiv preprint arXiv:1703.01732, 2017.
[Ammanabrolu and Riedl, 2019a] Prithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graphbased deep reinforcement learning. In NAACL, Volume 1 (Long and Short Papers), pages 3557-3565, 2019.</p>
<p>[Ammanabrolu and Riedl, 2019b] Prithviraj Ammanabrolu and Mark O Riedl. Transfer in deep reinforcement learning using knowledge graphs. arXiv preprint arXiv:1908.06556, 2019.
[Anderson and Galley, 1985] Tim Anderson and Stu Galley. The history of zork. The New Zork Times, 4(1-3), 1985.
[Barto, 2013] Andrew G Barto. Intrinsic motivation and reinforcement learning. In Intrinsically motivated learning in natural and artificial systems, pages 17-47. Springer, 2013.
[Burda et al., 2018] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.
[Côté et al., 2018] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. arXiv preprint arXiv:1806.11532, 2018.
[Côté, 2018] Marc-Alexandre Côté. First textworld problems: A reinforcement and language learning challenge. In NeurIPS Workshop, 2018.
[Ecoffet et al., 2019] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.
[Gershman and Daw, 2017] Samuel J Gershman and Nathaniel D Daw. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annual review of psychology, 68:101-128, 2017.
[He et al., 2015] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. arXiv preprint arXiv:1511.04636, 2015.
[Kaelbling et al., 1998] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134, 1998.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
[Narasimhan et al., 2015] Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for textbased games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.
[Oudeyer and Kaplan, 2009] Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 1:6, 2009.
[Pathak et al., 2017] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven
exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16-17, 2017.
[Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, pages 1532-1543, 2014.
[Salimans and Chen, 2018] Tim Salimans and Richard Chen. Learning montezuma's revenge from a single demonstration. arXiv preprint arXiv:1812.03381, 2018.
[Schmidhuber, 1991] Jürgen Schmidhuber. Curious modelbuilding control systems. In IJCNN, pages 1458-1463, 1991.
[Schmidhuber, 2006] Jürgen Schmidhuber. Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Connection Science, 18(2):173-187, 2006.
[Schulman et al., 2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[Sordoni et al., 2015] Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob Grue Simonsen, and Jian-Yun Nie. A hierarchical recurrent encoderdecoder for generative context-aware query suggestion. In CIKM, pages 553-562. ACM, 2015.
[Spaceman, 2019] Rural Spaceman. List of text-based computer games, 2019. [Online; accessed 21-Sep-2019].
[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NeurIPS, pages 3104-3112, 2014.
[Tao et al., 2018] Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, and Layla El Asri. Towards solving text-based games by producing adaptive action spaces. arXiv preprint arXiv:1812.00855, 2018.
[Tessler et al., 2019] Chen Tessler, Tom Zahavy, Deborah Cohen, Daniel J Mankowitz, and Shie Mannor. Action assembly: Sparse imitation learning for text based games with combinatorial action spaces. arXiv preprint arXiv:1905.09700, 2019.
[Watkins, 1989] Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, King's College, Cambridge, 1989.
[Williams and Zipser, 1989] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270280, 1989.
[Yuan et al., 2018] Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. Counting to explore and generalize in text-based games. arXiv preprint arXiv:1806.11525, 2018.
[Zahavy et al., 2018] Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In NeurIPS, pages 3562-3573, 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The authors released their code at https://github.com/xingdie-ric-yuan/TextWorld-Coin-Collector.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
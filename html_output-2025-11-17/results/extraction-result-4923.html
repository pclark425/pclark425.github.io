<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4923 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4923</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4923</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-60441391</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1902.04259v2.pdf" target="_blank">NAIL: A General Interactive Fiction Agent</a></p>
                <p><strong>Paper Abstract:</strong> Interactive Fiction (IF) games are complex textual decision making problems. This paper introduces NAIL, an autonomous agent for general parser-based IF games. NAIL won the 2018 Text Adventure AI Competition, where it was evaluated on twenty unseen games. This paper describes the architecture, development, and insights underpinning NAIL's performance.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4923.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4923.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL: Navigate, Acquire, Interact and Learn</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular, competition-winning parser-based interactive fiction agent that uses specialized Decision Modules and an explicit Knowledge Graph to play unseen IF games; uses a small n-gram language model to rank candidate actions and a FastText Validity Detector to judge action success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular agent composed of Decision Modules (Examiner, Hoarder, Interactor, Navigator, etc.). The Interactor uses a 5-gram language model trained on web page titles to rank candidate actions; a FastText-based Validity Detector classifies game responses as success/failure and gates updates to an explicit Knowledge Graph that records locations, entities, inventory, connections and action records.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>5-gram language model (n-gram LM trained on web page titles); FastText classifier for validity (not a modern transformer LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho / parser-based Interactive Fiction (20-game competition test set; 56-game development set)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Autonomously play unseen parser-based IF games (maximize normalized game score / satisfy fine-grained Dependencies) within a limited number of steps (1000 steps per game in competition).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Explicit external structured knowledge graph (map/episodic memory) storing locations, connection graph, entities, inventory, action records, and unrecognized words.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured facts: location names and long descriptions, connection graph (map), entity names and descriptions, entity states and attributes (Open/Closed, Locked/Unlocked, On/Off, used flags), inventory list, action history records (action + game response + success probability), list of unrecognized words.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Updated online after each relevant interaction: Navigator adds new Location objects and updates Connection Graph when movement is classified as successful; Examiner adds Entities and fills entity Descriptions upon successful 'examine' responses; Hoarder updates Inventory on successful 'take' actions; Action Record and Unrecognized Words are updated after each attempted action using the Validity Detector.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Decision Modules consult the Knowledge Graph via direct lookup and heuristic matching: e.g., Navigator prefers untried or previously-successful directions using stored attempted-action records and fuzzy string matching on location descriptions; Interactor enumerates actions over Entities present in the current Location; eagerness computations use KG contents. Retrieval is heuristic/lookup-based (no attention over embeddings reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>NAIL won the 2018 Text-Based Adventure AI Competition (evaluated on 20 unseen games, 1000 steps per game). Reported average normalized score on the test set: 2.56% (SD 0.33) and % non-zero ≈ 45.5%. Ablation analysis shows large gains attributable to modules that rely on the Knowledge Graph: baseline (only 'look') 0.53% → Navigator 1.2% → Hoarder 1.6% → Examiner 2.6% → Interactor 3.5% (cumulative effects shown in ablation plot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Sequential ablation: starting from a minimal agent (only 'look', 0.53%), adding Navigator, Hoarder, Examiner, Interactor produced stepwise increases (to 1.2%, 1.6%, 2.6%, 3.5% respectively in the ablation figure). The paper attributes substantial benefit to the explicit knowledge graph (human-interpretable map/state) and Examiner/Validity Detector pipeline for avoiding interacting with unrecognized objects. There is no direct experiment that removes only the Knowledge Graph while leaving other modules intact.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Games are highly partially observable and often alter location descriptions (necessitating fuzzy matching). The Knowledge Graph relies on manually defined object states and attributes; non-common action effects are not learned and are only recorded. No learning procedure for effects of uncommon actions was implemented. The map and KG could be further leveraged (future work).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors recommend an explicit, debuggable Knowledge Graph for IF agents, exhaustive examination of candidate objects (Examiner module), and a learned Validity Detector to decide action success before updating memory; they suggest future use of the produced map for efficient revisits and potentially learning effects of uncommon actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4923.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Graph (NAIL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL Knowledge Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The explicit structured memory used by NAIL to record discovered locations, entities, inventory, connections, action records and unrecognized words; used by decision modules to plan and avoid repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL (component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Structured external memory component of NAIL storing per-location descriptions, lists of entities and their attributes/states, inventory, connection graph (map), action records, and unrecognized words; used to compute module eagerness and to avoid repeating failed or redundant actions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho / parser-based Interactive Fiction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Serve as the agent's external memory for tracking world state across timesteps to support exploration, interaction and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Explicit structured external memory / episodic map & fact store</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured objects (Locations, Entities) with fields: Name, Description, Entities-in-location, Action Record; Connection Graph edges between Locations; Inventory as list of objects; Unrecognized Words list; Entity-specific attributes and states.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Updates are triggered by Decision Modules after actions judged by the Validity Detector: Navigator adds locations/connections on successful move; Examiner populates Entities and descriptions after successful 'examine'; Hoarder updates Inventory after successful 'take'; action attempts are appended to Action Record; unrecognized words updated after matching game failure text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Direct lookups and simple heuristics: Decision Modules query the KG for entities at current location, previously attempted actions, and map neighbors; fuzzy string matching is used to compare location descriptions; eagerness is computed from KG contents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation of Decision Modules (many of which rely on the KG) shows substantial gains; authors explicitly credit KG as improving generality and debuggability, but no single ablation where KG alone is removed is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>KG relies on heuristics and manually defined object attributes/states; non-common action effects are not encoded; games may mutate location descriptions, requiring fuzzy matching. Learning dynamics for uncommon actions and more sophisticated map usage are left for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use an explicit, interpretable knowledge graph for IF agents; gate KG updates with a learned validity classifier; exhaustively 'examine' candidate noun phrases to populate KG with only parser-recognized entities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4923.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-DQN (Narasimhan et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning agent that constructs a state representation by processing narrative text with an LSTM and uses that representation for action selection (introduced for text-based games).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL agent that encodes the current narrative/observation using an LSTM to produce a state representation which feeds into a DQN-style action-value estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LSTM-based recurrent network (RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Text-based games (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Create a state representation from narrative text to enable RL policies in text-game environments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Implicit recurrent memory (LSTM hidden state) that captures recent observation history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hidden state vectors (LSTM cell/hidden states) summarizing recent observations/text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>LSTM hidden state updated on each received observation/timestep by processing the new text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Policy conditions on the LSTM hidden state (no explicit external retrieval mechanism described in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper is cited for introducing the idea of processing narrative text with a recurrent network to build state representations; no detailed ablation about memory is provided in this NAIL paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed in detail here; raised generally as part of related work illustrating approaches to representing partial observability in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Using recurrence to encode textual observations into state representations can provide useful memory over recent observations for RL in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4923.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-DRQN (Yuan et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension that adds a second-level recurrence over states (deep recurrent Q-network) to enable reasoning over locations visited in the past, improving memory over longer time spans in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counting to explore and generalize in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RL architecture augmenting LSTM-based observation encoding with a second-level recurrence across states (DRQN-style) to capture longer-term dependencies and histories (e.g., visited locations).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LSTM-based hierarchical recurrent network</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Text-based games (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve exploration and generalization in text games by adding a state-level recurrence that maintains longer-term memory across timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Hierarchical recurrent memory (observation-level LSTM + state-level recurrence / DRQN).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hierarchical hidden state vectors encoding both recent observations and longer-term state history such as visited locations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Recurrent hidden states updated each timestep; second-level recurrence updates higher-level state memory across transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Policy conditions on hierarchical recurrent states; no external retrieval mechanism is reported in NAIL's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned as demonstrating the ability to reason over previously visited locations; the NAIL paper does not report the DRQN paper's performance numbers or ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not elaborated in NAIL beyond the general tradeoffs of recurrent memories in partially observable environments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Hierarchy of recurrence can increase the effective memory horizon and help reasoning about locations visited in the past.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4923.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ammanabrolu&Riedl-Graph-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning (Ammanabrolu & Riedl, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end learning approach that represents game state as a knowledge graph, integrating structured external memory into a deep RL agent for text-adventure games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph-based deep RL agent (Ammanabrolu & Riedl)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL agent that constructs and uses a knowledge-graph representation of the game state as its internal/explicit memory, enabling reasoning over structured state for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Text-adventure games (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Represent and use structured knowledge of the environment (knowledge graph) within an end-to-end learned RL framework for playing text-adventure games.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Explicit structured knowledge graph (external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Nodes and edges encoding game entities, relations and state (knowledge graph).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Graph is updated as the agent processes observations during gameplay (as described in the cited work and summarized in NAIL).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>The learned policy uses the graph representation to inform action selection (no specific retrieval mechanism reported in the NAIL summary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Cited as demonstrating end-to-end learning with a knowledge-graph state; NAIL references this as related work but does not reproduce experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed in detail within the NAIL paper; implies value of structured memory but tradeoffs depend on the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Using an explicit knowledge-graph state representation is a promising approach for encoding long-lived facts in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4923.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARL (competition agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An IF agent that uses skip-thought vector encodings of observations to classify text as location or action-effect descriptions, extracts nouns to propose commands and stores visited states/actions in a hash table to avoid repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CARL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that processes observations with skip-thought vectors to detect whether an observation contains location information or action effects; for location-containing observations, nouns are extracted and verb candidates generated (using BYU Agent's embedding approach); visited states and actions are tracked in a hash table to prevent repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Skip-thought vectors (sentence encoder) used for observation representation</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Interactive Fiction / competition games (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect location-relevant text, extract interactive nouns, and generate suitable verb-noun commands while avoiding previously attempted/visited state-action pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Hash-table-based visited-state/action memory (explicit episodic memory to avoid repetition)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Skip-thought vector encodings of observations (for classification) and hash-table entries marking visited states and attempted actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Visited states/actions are recorded in the hash table as they are encountered/attempted.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Lookup in the hash table to avoid generating/repeating actions that were already attempted at a given visited state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Described in related work; NAIL notes CARL's use of skip-thought vectors and visit-tracking but does not reproduce its experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not detailed in NAIL; CARL's reliance on skip-thought vectors and hashing is described as part of prior-art tradeoffs between generality and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4923.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BYU'16 Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BYU Agent (Fulda et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that uses word-embedding relationships to infer affordances (which verbs pair with which objects) and then exhaustively enumerates possible actions; effective but sample-inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What can you do with a rock? affordance extraction via word embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BYU Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Affordance-driven agent that analyzes relationships in word-embedding space to decide which verb-object pairs are plausible, then enumerates and issues them (exhaustive search over candidate actions).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Word-embedding based affordance model (not an LLM per se)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Interactive Fiction (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify plausible verb-object affordances and generate candidate actions accordingly to interact with game objects.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>NAIL cites BYU's approach as effective but less efficient (in terms of number of in-game steps) due to exhaustive action enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Exhaustive enumeration leads to inefficiency in step-limited settings; no explicit memory mechanism is discussed in NAIL's description.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Affordance extraction from embeddings can guide action generation but must be balanced against exploration/sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4923.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Golovin (Kostka et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An IF agent that generates actions from a large set of predefined command patterns extracted from walkthroughs/tutorials and uses specialized command generators plus an RNN to identify likely interactive objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-based adventures of the golovin AI agent.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that relies on a large library of command templates mined from human resources, specialized sub-generators for common tasks (fighting, collecting, exploring) and an RNN to predict which objects are likely interactable in a scene.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>RNN used to identify interactive objects (not a large pretrained LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Interactive Fiction (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate high-coverage, reasonable commands for common IF tasks by using template-based generation guided by an RNN object-predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>NAIL compares statistics like % of actions devoted to 'examine' across agents; Golovin uses fewer examines than NAIL. No detailed memory analysis is provided in NAIL for Golovin.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Template-based generation can be effective but may lack generality for unseen verbs/objects; NAIL highlights differences in examine usage as a distinguishing factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4923.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4923.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Validity Detector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Validity Detector (FastText classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A FastText-based classifier trained to predict whether the game's textual response to an action indicates that the action was recognized and successful; used as a gate for Knowledge Graph updates and by Decision Modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL (component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Binary classifier (trained on ~1337 successful and 705 failed responses) that labels game responses as indicating success vs failure; used to decide whether to add Entities/Locations to the Knowledge Graph and to judge action outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>FastText text classifier</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho / parser-based IF</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify game responses to agent actions as successful or failed so that the agent only updates memory (KG) on valid effects.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>NAIL credits the Validity Detector as key to correctly populating the Knowledge Graph and enabling decision modules to reason about success; classifier performed well despite small training set because many games share common failure responses.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Small labeled dataset but sufficient due to standardized failure messages across games; classifier decisions are crucial—incorrect labels can lead to incorrect KG updates.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Gate KG updates with a learned validity classifier to avoid populating memory with artifacts from unrecognized actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NAIL: A General Interactive Fiction Agent', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for text-based games using deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Counting to explore and generalize in text-based games. <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>The text-based adventure ai competition. <em>(Rating: 2)</em></li>
                <li>What can you do with a rock? affordance extraction via word embeddings. <em>(Rating: 1)</em></li>
                <li>Text-based adventures of the golovin AI agent. <em>(Rating: 1)</em></li>
                <li>Learning how not to act in text-based games. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4923",
    "paper_id": "paper-60441391",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "NAIL",
            "name_full": "NAIL: Navigate, Acquire, Interact and Learn",
            "brief_description": "A modular, competition-winning parser-based interactive fiction agent that uses specialized Decision Modules and an explicit Knowledge Graph to play unseen IF games; uses a small n-gram language model to rank candidate actions and a FastText Validity Detector to judge action success.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NAIL",
            "agent_description": "Modular agent composed of Decision Modules (Examiner, Hoarder, Interactor, Navigator, etc.). The Interactor uses a 5-gram language model trained on web page titles to rank candidate actions; a FastText-based Validity Detector classifies game responses as success/failure and gates updates to an explicit Knowledge Graph that records locations, entities, inventory, connections and action records.",
            "llm_model_name": "5-gram language model (n-gram LM trained on web page titles); FastText classifier for validity (not a modern transformer LLM)",
            "game_or_benchmark_name": "Jericho / parser-based Interactive Fiction (20-game competition test set; 56-game development set)",
            "task_description": "Autonomously play unseen parser-based IF games (maximize normalized game score / satisfy fine-grained Dependencies) within a limited number of steps (1000 steps per game in competition).",
            "memory_used": true,
            "memory_type": "Explicit external structured knowledge graph (map/episodic memory) storing locations, connection graph, entities, inventory, action records, and unrecognized words.",
            "memory_representation": "Structured facts: location names and long descriptions, connection graph (map), entity names and descriptions, entity states and attributes (Open/Closed, Locked/Unlocked, On/Off, used flags), inventory list, action history records (action + game response + success probability), list of unrecognized words.",
            "memory_update_mechanism": "Updated online after each relevant interaction: Navigator adds new Location objects and updates Connection Graph when movement is classified as successful; Examiner adds Entities and fills entity Descriptions upon successful 'examine' responses; Hoarder updates Inventory on successful 'take' actions; Action Record and Unrecognized Words are updated after each attempted action using the Validity Detector.",
            "memory_retrieval_mechanism": "Decision Modules consult the Knowledge Graph via direct lookup and heuristic matching: e.g., Navigator prefers untried or previously-successful directions using stored attempted-action records and fuzzy string matching on location descriptions; Interactor enumerates actions over Entities present in the current Location; eagerness computations use KG contents. Retrieval is heuristic/lookup-based (no attention over embeddings reported).",
            "performance_with_memory": "NAIL won the 2018 Text-Based Adventure AI Competition (evaluated on 20 unseen games, 1000 steps per game). Reported average normalized score on the test set: 2.56% (SD 0.33) and % non-zero ≈ 45.5%. Ablation analysis shows large gains attributable to modules that rely on the Knowledge Graph: baseline (only 'look') 0.53% → Navigator 1.2% → Hoarder 1.6% → Examiner 2.6% → Interactor 3.5% (cumulative effects shown in ablation plot).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Sequential ablation: starting from a minimal agent (only 'look', 0.53%), adding Navigator, Hoarder, Examiner, Interactor produced stepwise increases (to 1.2%, 1.6%, 2.6%, 3.5% respectively in the ablation figure). The paper attributes substantial benefit to the explicit knowledge graph (human-interpretable map/state) and Examiner/Validity Detector pipeline for avoiding interacting with unrecognized objects. There is no direct experiment that removes only the Knowledge Graph while leaving other modules intact.",
            "challenges_or_limitations": "Games are highly partially observable and often alter location descriptions (necessitating fuzzy matching). The Knowledge Graph relies on manually defined object states and attributes; non-common action effects are not learned and are only recorded. No learning procedure for effects of uncommon actions was implemented. The map and KG could be further leveraged (future work).",
            "best_practices_or_recommendations": "Authors recommend an explicit, debuggable Knowledge Graph for IF agents, exhaustive examination of candidate objects (Examiner module), and a learned Validity Detector to decide action success before updating memory; they suggest future use of the produced map for efficient revisits and potentially learning effects of uncommon actions.",
            "uuid": "e4923.0",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Knowledge Graph (NAIL)",
            "name_full": "NAIL Knowledge Graph",
            "brief_description": "The explicit structured memory used by NAIL to record discovered locations, entities, inventory, connections, action records and unrecognized words; used by decision modules to plan and avoid repetition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NAIL (component)",
            "agent_description": "Structured external memory component of NAIL storing per-location descriptions, lists of entities and their attributes/states, inventory, connection graph (map), action records, and unrecognized words; used to compute module eagerness and to avoid repeating failed or redundant actions.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Jericho / parser-based Interactive Fiction",
            "task_description": "Serve as the agent's external memory for tracking world state across timesteps to support exploration, interaction and navigation.",
            "memory_used": true,
            "memory_type": "Explicit structured external memory / episodic map & fact store",
            "memory_representation": "Structured objects (Locations, Entities) with fields: Name, Description, Entities-in-location, Action Record; Connection Graph edges between Locations; Inventory as list of objects; Unrecognized Words list; Entity-specific attributes and states.",
            "memory_update_mechanism": "Updates are triggered by Decision Modules after actions judged by the Validity Detector: Navigator adds locations/connections on successful move; Examiner populates Entities and descriptions after successful 'examine'; Hoarder updates Inventory after successful 'take'; action attempts are appended to Action Record; unrecognized words updated after matching game failure text.",
            "memory_retrieval_mechanism": "Direct lookups and simple heuristics: Decision Modules query the KG for entities at current location, previously attempted actions, and map neighbors; fuzzy string matching is used to compare location descriptions; eagerness is computed from KG contents.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Ablation of Decision Modules (many of which rely on the KG) shows substantial gains; authors explicitly credit KG as improving generality and debuggability, but no single ablation where KG alone is removed is reported.",
            "challenges_or_limitations": "KG relies on heuristics and manually defined object attributes/states; non-common action effects are not encoded; games may mutate location descriptions, requiring fuzzy matching. Learning dynamics for uncommon actions and more sophisticated map usage are left for future work.",
            "best_practices_or_recommendations": "Use an explicit, interpretable knowledge graph for IF agents; gate KG updates with a learned validity classifier; exhaustively 'examine' candidate noun phrases to populate KG with only parser-recognized entities.",
            "uuid": "e4923.1",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "LSTM-DQN",
            "name_full": "LSTM-DQN (Narasimhan et al., 2015)",
            "brief_description": "A reinforcement-learning agent that constructs a state representation by processing narrative text with an LSTM and uses that representation for action selection (introduced for text-based games).",
            "citation_title": "Language understanding for text-based games using deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "LSTM-DQN",
            "agent_description": "Deep RL agent that encodes the current narrative/observation using an LSTM to produce a state representation which feeds into a DQN-style action-value estimator.",
            "llm_model_name": "LSTM-based recurrent network (RNN)",
            "game_or_benchmark_name": "Text-based games (related work)",
            "task_description": "Create a state representation from narrative text to enable RL policies in text-game environments.",
            "memory_used": true,
            "memory_type": "Implicit recurrent memory (LSTM hidden state) that captures recent observation history.",
            "memory_representation": "Hidden state vectors (LSTM cell/hidden states) summarizing recent observations/text.",
            "memory_update_mechanism": "LSTM hidden state updated on each received observation/timestep by processing the new text.",
            "memory_retrieval_mechanism": "Policy conditions on the LSTM hidden state (no explicit external retrieval mechanism described in this paper's mention).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Paper is cited for introducing the idea of processing narrative text with a recurrent network to build state representations; no detailed ablation about memory is provided in this NAIL paper's discussion.",
            "challenges_or_limitations": "Not discussed in detail here; raised generally as part of related work illustrating approaches to representing partial observability in text games.",
            "best_practices_or_recommendations": "Using recurrence to encode textual observations into state representations can provide useful memory over recent observations for RL in text games.",
            "uuid": "e4923.2",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "LSTM-DRQN",
            "name_full": "LSTM-DRQN (Yuan et al., 2018)",
            "brief_description": "An extension that adds a second-level recurrence over states (deep recurrent Q-network) to enable reasoning over locations visited in the past, improving memory over longer time spans in text games.",
            "citation_title": "Counting to explore and generalize in text-based games.",
            "mention_or_use": "mention",
            "agent_name": "LSTM-DRQN",
            "agent_description": "RL architecture augmenting LSTM-based observation encoding with a second-level recurrence across states (DRQN-style) to capture longer-term dependencies and histories (e.g., visited locations).",
            "llm_model_name": "LSTM-based hierarchical recurrent network",
            "game_or_benchmark_name": "Text-based games (related work)",
            "task_description": "Improve exploration and generalization in text games by adding a state-level recurrence that maintains longer-term memory across timesteps.",
            "memory_used": true,
            "memory_type": "Hierarchical recurrent memory (observation-level LSTM + state-level recurrence / DRQN).",
            "memory_representation": "Hierarchical hidden state vectors encoding both recent observations and longer-term state history such as visited locations.",
            "memory_update_mechanism": "Recurrent hidden states updated each timestep; second-level recurrence updates higher-level state memory across transitions.",
            "memory_retrieval_mechanism": "Policy conditions on hierarchical recurrent states; no external retrieval mechanism is reported in NAIL's discussion.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Mentioned as demonstrating the ability to reason over previously visited locations; the NAIL paper does not report the DRQN paper's performance numbers or ablations.",
            "challenges_or_limitations": "Not elaborated in NAIL beyond the general tradeoffs of recurrent memories in partially observable environments.",
            "best_practices_or_recommendations": "Hierarchy of recurrence can increase the effective memory horizon and help reasoning about locations visited in the past.",
            "uuid": "e4923.3",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Ammanabrolu&Riedl-Graph-RL",
            "name_full": "Playing text-adventure games with graph-based deep reinforcement learning (Ammanabrolu & Riedl, 2018)",
            "brief_description": "An end-to-end learning approach that represents game state as a knowledge graph, integrating structured external memory into a deep RL agent for text-adventure games.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "Graph-based deep RL agent (Ammanabrolu & Riedl)",
            "agent_description": "An RL agent that constructs and uses a knowledge-graph representation of the game state as its internal/explicit memory, enabling reasoning over structured state for action selection.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Text-adventure games (related work)",
            "task_description": "Represent and use structured knowledge of the environment (knowledge graph) within an end-to-end learned RL framework for playing text-adventure games.",
            "memory_used": true,
            "memory_type": "Explicit structured knowledge graph (external memory)",
            "memory_representation": "Nodes and edges encoding game entities, relations and state (knowledge graph).",
            "memory_update_mechanism": "Graph is updated as the agent processes observations during gameplay (as described in the cited work and summarized in NAIL).",
            "memory_retrieval_mechanism": "The learned policy uses the graph representation to inform action selection (no specific retrieval mechanism reported in the NAIL summary).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Cited as demonstrating end-to-end learning with a knowledge-graph state; NAIL references this as related work but does not reproduce experiments.",
            "challenges_or_limitations": "Not discussed in detail within the NAIL paper; implies value of structured memory but tradeoffs depend on the cited paper.",
            "best_practices_or_recommendations": "Using an explicit knowledge-graph state representation is a promising approach for encoding long-lived facts in text games.",
            "uuid": "e4923.4",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "CARL",
            "name_full": "CARL (competition agent)",
            "brief_description": "An IF agent that uses skip-thought vector encodings of observations to classify text as location or action-effect descriptions, extracts nouns to propose commands and stores visited states/actions in a hash table to avoid repetition.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "CARL",
            "agent_description": "Agent that processes observations with skip-thought vectors to detect whether an observation contains location information or action effects; for location-containing observations, nouns are extracted and verb candidates generated (using BYU Agent's embedding approach); visited states and actions are tracked in a hash table to prevent repetition.",
            "llm_model_name": "Skip-thought vectors (sentence encoder) used for observation representation",
            "game_or_benchmark_name": "Interactive Fiction / competition games (related work)",
            "task_description": "Detect location-relevant text, extract interactive nouns, and generate suitable verb-noun commands while avoiding previously attempted/visited state-action pairs.",
            "memory_used": true,
            "memory_type": "Hash-table-based visited-state/action memory (explicit episodic memory to avoid repetition)",
            "memory_representation": "Skip-thought vector encodings of observations (for classification) and hash-table entries marking visited states and attempted actions.",
            "memory_update_mechanism": "Visited states/actions are recorded in the hash table as they are encountered/attempted.",
            "memory_retrieval_mechanism": "Lookup in the hash table to avoid generating/repeating actions that were already attempted at a given visited state.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Described in related work; NAIL notes CARL's use of skip-thought vectors and visit-tracking but does not reproduce its experiments.",
            "challenges_or_limitations": "Not detailed in NAIL; CARL's reliance on skip-thought vectors and hashing is described as part of prior-art tradeoffs between generality and sample efficiency.",
            "uuid": "e4923.5",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "BYU'16 Agent",
            "name_full": "BYU Agent (Fulda et al., 2017)",
            "brief_description": "Agent that uses word-embedding relationships to infer affordances (which verbs pair with which objects) and then exhaustively enumerates possible actions; effective but sample-inefficient.",
            "citation_title": "What can you do with a rock? affordance extraction via word embeddings.",
            "mention_or_use": "mention",
            "agent_name": "BYU Agent",
            "agent_description": "Affordance-driven agent that analyzes relationships in word-embedding space to decide which verb-object pairs are plausible, then enumerates and issues them (exhaustive search over candidate actions).",
            "llm_model_name": "Word-embedding based affordance model (not an LLM per se)",
            "game_or_benchmark_name": "Interactive Fiction (related work)",
            "task_description": "Identify plausible verb-object affordances and generate candidate actions accordingly to interact with game objects.",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "NAIL cites BYU's approach as effective but less efficient (in terms of number of in-game steps) due to exhaustive action enumeration.",
            "challenges_or_limitations": "Exhaustive enumeration leads to inefficiency in step-limited settings; no explicit memory mechanism is discussed in NAIL's description.",
            "best_practices_or_recommendations": "Affordance extraction from embeddings can guide action generation but must be balanced against exploration/sample efficiency.",
            "uuid": "e4923.6",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Golovin",
            "name_full": "Golovin (Kostka et al., 2017)",
            "brief_description": "An IF agent that generates actions from a large set of predefined command patterns extracted from walkthroughs/tutorials and uses specialized command generators plus an RNN to identify likely interactive objects.",
            "citation_title": "Text-based adventures of the golovin AI agent.",
            "mention_or_use": "mention",
            "agent_name": "Golovin",
            "agent_description": "Agent that relies on a large library of command templates mined from human resources, specialized sub-generators for common tasks (fighting, collecting, exploring) and an RNN to predict which objects are likely interactable in a scene.",
            "llm_model_name": "RNN used to identify interactive objects (not a large pretrained LLM)",
            "game_or_benchmark_name": "Interactive Fiction (related work)",
            "task_description": "Generate high-coverage, reasonable commands for common IF tasks by using template-based generation guided by an RNN object-predictor.",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "NAIL compares statistics like % of actions devoted to 'examine' across agents; Golovin uses fewer examines than NAIL. No detailed memory analysis is provided in NAIL for Golovin.",
            "challenges_or_limitations": "Template-based generation can be effective but may lack generality for unseen verbs/objects; NAIL highlights differences in examine usage as a distinguishing factor.",
            "uuid": "e4923.7",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Validity Detector",
            "name_full": "Validity Detector (FastText classifier)",
            "brief_description": "A FastText-based classifier trained to predict whether the game's textual response to an action indicates that the action was recognized and successful; used as a gate for Knowledge Graph updates and by Decision Modules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NAIL (component)",
            "agent_description": "Binary classifier (trained on ~1337 successful and 705 failed responses) that labels game responses as indicating success vs failure; used to decide whether to add Entities/Locations to the Knowledge Graph and to judge action outcomes.",
            "llm_model_name": "FastText text classifier",
            "game_or_benchmark_name": "Jericho / parser-based IF",
            "task_description": "Classify game responses to agent actions as successful or failed so that the agent only updates memory (KG) on valid effects.",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "NAIL credits the Validity Detector as key to correctly populating the Knowledge Graph and enabling decision modules to reason about success; classifier performed well despite small training set because many games share common failure responses.",
            "challenges_or_limitations": "Small labeled dataset but sufficient due to standardized failure messages across games; classifier decisions are crucial—incorrect labels can lead to incorrect KG updates.",
            "best_practices_or_recommendations": "Gate KG updates with a learned validity classifier to avoid populating memory with artifacts from unrecognized actions.",
            "uuid": "e4923.8",
            "source_info": {
                "paper_title": "NAIL: A General Interactive Fiction Agent",
                "publication_date_yy_mm": "2019-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games.",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "The text-based adventure ai competition.",
            "rating": 2
        },
        {
            "paper_title": "What can you do with a rock? affordance extraction via word embeddings.",
            "rating": 1
        },
        {
            "paper_title": "Text-based adventures of the golovin AI agent.",
            "rating": 1
        },
        {
            "paper_title": "Learning how not to act in text-based games.",
            "rating": 1
        }
    ],
    "cost": 0.020827,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NAIL: A General Interactive Fiction Agent</p>
<p>Matthew Hausknecht 
Microsoft Research AI</p>
<p>Ricky Loynd riloynd@microsoft.com 
Microsoft Research AI</p>
<p>Greg Yang gregyang@microsoft.com 
Microsoft Research AI</p>
<p>Adith Swaminathan 
Microsoft Research AI</p>
<p>Jason D Williams 
Microsoft Research AI</p>
<p>Apple 
Microsoft Research AI</p>
<p>NAIL: A General Interactive Fiction Agent</p>
<p>Interactive Fiction (IF) games are complex textual decision making problems. This paper introduces NAIL, an autonomous agent for general parser-based IF games. NAIL won the 2018 Text Adventure AI Competition, where it was evaluated on twenty unseen games.</p>
<p>Introduction</p>
<p>Interactive Fiction games are rich simulation environments where players issue text commands to control their character and progress through the narrative. Unlike modern video games, there are no graphics; text is the sole modality of interaction. Of the varieties of IF games, this paper is concerned with parser-based IF games, environments in which the player may issue arbitrary textual commands which the game's parser attempts to interpret and execute. Despite this daunting interface, parser-based IF games were highly successful in the early 1980s. Classic games like Zork ( Figure 1) and Hitchhiker's Guide to the Galaxy sparked the imaginations of many and created a community of IF enthusiasts who continue to play and create new games to this day.</p>
<p>As a testbed for autonomous agents, IF games promote natural language understanding, commonsense reasoning, problem solving, and planning. Formally, IF games can be understood as Partially Observable Markov Decision Processes (POMDPs), since they are governed by an underlying system but provide only partial observations of the underlying game state. Game playing agents need to be capable of generating valid actions that make sense in the context of the current story. These actions often take the form of 1) information gathering about the player's surroundings, 2) interaction with nearby objects or people, and 3) navigation between game locations. In return for solving puzzles, the player is rewarded with game score that quantifies advancement through the story.</p>
<p>IF games present several unique challenges to learning agents: First, IF games feature a combinatorial language-based action space. Learning agents have been extensively studied in both discrete and continuous action spaces, but not the repeated discrete space defining IF actions. Second, IF games expect the player to understand how to interact with everyday objects like doors and mailboxes. This form of commonsense reasoning is difficult for learning agents lacking embodiment. Finally, IF games are extremely partially observable -they commonly feature tens to hundreds of unique locations, and the player only receives the description of its current location. Humans often construct a maps to remember how to navigate between rooms and keep track of which objects are in each location. For these reasons, learning agents that excel in graphical video games aren't directly applicable to IF games. Instead, a new type of agent is needed. NAIL (Navigate, Acquire, Interact and Learn) is an autonomous agent designed to play arbitrary human-made IF games. NAIL competed in the 2018 Text-Based Adventure AI Competition (Atkinson et al., 2019), where it was evaluated on twenty unknown IF games, with only one-thousand steps of interaction per game. To excel in this context, NAIL needed to be truly general, with the capability of encountering a new game and quickly accruing as much score as possible. For this reason, we designed NAIL with strong heuristics for exploring the game, interacting with objects, and building an internal representation of the game world. The remainder of this paper describes NAIL's architecture, development process, and innovations.</p>
<p>Related Work</p>
<p>NAIL draws inspiration from previous generations of IF game-playing agents:</p>
<p>BYU'16 Agent (Fulda et al., 2017) analyzes relationships in word-embedding space (Mikolov et al., 2013) to reason about which objects are possible to interact with and which verbs can likely be paired with each interactive object. Armed with this knowledge, the agent exhaustively enumerates possible actions. While effective, this approach is less efficient in terms of the number of in-game steps required.</p>
<p>Golovin (Kostka et al., 2017) generates actions using a large set of predefined command patterns extracted through walkthroughs, tutorials, and decompilation of games. Additionally, it employs specialized command generators for common tasks such as fighting, collecting items, managing inventory, and exploring. An recurrent neural network is used to identify the objects most likely to be interactable in a scene's description.</p>
<p>CARL (Atkinson et al., 2019) was developed by the creators of the BYU Agent and uses skip-thought vectors (Kiros et al., 2015) as representations of the observation text. These vectors are classified as either containing location information or relating to the effects of an action. For those containing location information, commands are generated by extracting nouns from the text and using the BYU Agent's word embedding approach to find likely verbs. A hash table is used to keep track of visited states and actions to avoid repetition.</p>
<p>There is also a growing body of work studying reinforcement learning agents in textual environments: LSTM-DQN (Narasimhan et al., 2015) introduced the idea of building a representation for the current state by processing narrative text with a recurrent network. Building on this idea, LSTM-DRQN (Yuan et al., 2018) added a second level recurrence over states, which was demonstrated to give the agent an ability to reason over locations visited in the past. In the realm of choice-based games, He et al. (2016); Zelinka (2018) explored architectural choices for scoring different pre-defined actions. Another recent innovation included training a separate network to eliminate consideration of invalid actions (Haroush et al., 2018). Finally, Ammanabrolu and Riedl (2018) demonstrated end-to-end learning using an architecture that represents game state as a knowledge graph.</p>
<p>Response p(success) I didn't understand that sentence. 0 You can't go that way. 0 You can't use multiple objects with that verb.</p>
<p>.0008 You try to push past, but vines block your way.</p>
<p>.0009 I don't know the word xyzzy.</p>
<p>.1145 Even with a lamp, you would not chance these stairs in the darkness. .6366 The gentle tapping sounds again.</p>
<p>.9387 Help! You hurtle through the cave opening! .9835 The grating opens.</p>
<p>.9998 The cyclops seems somewhat agitated.</p>
<p>.9998 Table 1: Validity Detector predicts if the game's response to an action indicates success or failure.</p>
<p>Predictions are only concerned with whether or not the action was recognized and effected change in the game. Responses are classified as successful even if they indicate that the player has done something wrong or is likely to die.</p>
<p>Architecture</p>
<p>NAIL was designed with a modular architecture that decouples the decision making from the knowledge acquisition. At the highest level, NAIL utilizes a set of Decision Modules to interact with the game and accumulates information in its knowledge graph. Further separation is achieved by decomposing the overall decision making problem into specialized task-specific Decision Modules, which take control of the agent to perform a particular job when the right context arises. For example, the darkness decision module activates in response to game narratives that include key phrases "pitch black" and "too dark too see," and attempts to provide light by issuing a "turn on" command. Separating decision making from knowledge acquisition enables NAIL's capabilities to be augmented by adding more decision modules while being able to inspect and debug the information accumulated in the knowledge graph. Figure 2 depicts NAIL's architecture, which we discuss in detail below.  </p>
<p>Validity Detector</p>
<p>The game's responses to the agent's actions provide a continual stream of feedback. NAIL processes game responses to determine whether an action was 1) unrecognized by the game, 2) recognized but failed, or 3) recognized and successful. This information is crucial to determining how the Knowledge Graph is populated with Locations and Entities.</p>
<p>The Validity Detector, a FastText (Joulin et al., 2017) classifier, predicts whether the game's response to an action indicates success or failure. The Validity Detector was trained on a manually created dataset of 1337 successful responses and 705 failed responses. Despite the small size of the training dataset, classifier performance was surprisingly good, as shown by the example predictions in Table  1. Many games use a common set of failure responses, simplifying the classification task.</p>
<p>The Validity Detector underlies each of NAIL's core components: it is used by Decision Modules to decide when actions have succeeded and it serves as a gatekeeper to the knowledge graph by disallowing the creation of invalid Entities and Locations.</p>
<p>Decision Modules</p>
<p>NAIL is composed of many decision modules (Fig. 2) which can be thought of as sub-policies designed to accomplish individual tasks. NAIL's core decision modules are the Examiner, Hoarder, Interactor, and Navigator. Respectively they are responsible for identifying relevant objects, acquiring objects, interacting with identified objects, and changing locations. Before delving into the specifics of these core modules, we first discuss the process for synchronizing control between modules.</p>
<p>Choosing Between Decision Modules: Eagerness</p>
<p>Each decision module has the ability to take control of the agent and generate textual actions. Since only a single decision module can have control over the agent at any given timestep, NAIL employs a winner-take-all competition in which decision modules report how eager they are to have control over the agent given the current context. Game context consists of various factors such as information contained within the game narrative and knowledge graph. Different modules are eager in different contexts: the Examiner module is eager to take control when a new location is discovered, while the Interactor is most eager when new objects are added to the knowledge graph. By convention, eagerness values are real numbers constrained to the range (0, 1).</p>
<p>The most eager decision module generates actions until it relinquishes control. Currently there is no mechanism for decision modules to interrupt each other, as this would require the interrupted module to resume control in a different context than before the interrupt. Instead, modules are designed to accomplish many short tasks and relinquish control so as to provide more eager modules many chances to take control.</p>
<p>Examiner Decision Module</p>
<p>One of the fundamental steps in playing IF games is reading the current narrative and deciding which parts of the surroundings are capable of being interacted with. This is the job of the Examiner, a decision module that outputs a high eagerness upon visiting a new location 4 and seeks to identify objects to add to the knowledge graph. Identification of objects proceeds in two steps: first, candidate noun phrases are extracted from the narrative text using Spacy (Honnibal and Montani, 2017). Second, "examine X" commands are issued for each of the candidate noun phrases. If the game's response to the examine command indicates the object is recognized (often in the form of a longer description of that object) the Examiner will add the object to the knowledge graph. Conversely if the response indicates the game doesn't recognize the object, it is not added to the knowledge graph and will not be interacted with in the future. The Validity Detector is used to decide which responses are valid.</p>
<p>Hoarder Decision Module</p>
<p>Acquiring objects such as weapons, treasures, and consumables is a necessity in nearly all IF games. The Hoarder outputs the highest possible eagerness upon reaching a new location. After taking control it issues a single "take all" command. Many games support this shorthand and will reward the player by transporting all portable objects directly into the player's inventory. 5 For example, in Hitchhiker's Guide to the Galaxy:  Figure 3: The Examiner decision module is responsible for extracting game objects from narrative text and adding them to the knowledge graph. Subsequent modules like the Interactor use the objects in the knowledge graph to generate further actions.</p>
<blockquote>
<p>take all telephone: You lunge for it, but the room spins nauseatingly away. flathead screwdriver: It slips through your fumbling fingers and hits the carpet with a nerve-shattering bang. toothbrush: You lunge for it, but the room spins nauseatingly away. your gown: Luckily, this is large enough for you to get hold of. You notice something in the pocket.</p>
</blockquote>
<p>The response indicates four valid objects are present, of which we succeeded in acquiring only the gown. The Hoarder uses a custom parser to add all detected objects to the knowledge graph and the successfully acquired objects to the player's inventory. The Validity Detector decides whether a response indicates the item has been successfully acquired.</p>
<p>Interactor Decision Module</p>
<p>A core challenge of IF games is deciding how to effectively interact with the objects at the current location. Exhaustive search is rarely tractable as even a restricted subset of the English language can produce a combinatorially large set of possible actions. To narrow this search, NAIL uses a 5-gram language model (LM), trained on 1.5 billion n-grams collected from web page titles, 6 to impose a prior over generated actions. As shown in Figure 4, this prior gives preference to common, sensible actions like "open the door" or "light the torch" over less reasonable actions like "light the door" or "open the torch." The Interactor works through the prioritized list of actions in descending order, executing a single action, judging whether it succeeded, and reporting an eagerness score proportional to how highly ranked the next action is.</p>
<p>In more detail, starting from the set of objects present in the player's current location, the Interactor generates an unranked list of candidate actions. These candidate actions are enumerated from two templates: a verb-object template and a verb-object-preposition-object template. 7 Verb-object commands use verbs selected from a manually curated list of 561 commonly recognized IF-game verbs. These verbs are paired with the objects present at the current location and inventory; the LM is then used to compute the joint probability of each generated verb-object pair.</p>
<p>Verb-object-preposition-object commands are also created by iterating over objects x, y present at the current location or in the player's inventory. Ten fixed templates (such as put x in y and open x with y) provide verbs and prepositions for each object pair. These candidate commands are then ranked along with the verb-object actions according to their LM-based joint probabilities. 8 Figure 4: The Interactor ranks candidate actions according to their log-probabilities. These logprobabilities are proportional to the Interactor's eagerness.</p>
<p>Navigator Decision Module</p>
<p>The Navigator is invoked to move the player to a new location. Outputting a consistently low eagerness, the Navigator is intended to activate only after the Hoarder, Examiner, and Interactor have exhausted the useful actions at the current location. After gaining control, the Navigator applies one of the twelve canonical navigation actions: North, South, West, East, Northwest, Northeast, Southwest, Southeast, Up, Down, Enter, and Exit. To avoid repeating failed navigation actions, the Navigator keeps track of all previously attempted navigational actions from each location, and prefers actions that have either not yet been attempted or have succeeded in changing locations in the past. Additionally, since location descriptions frequently tell the player where exits are (e.g. "there is an open door to the west"), the Navigator performs a simple string match and prioritizes directional actions that are present in the description.</p>
<p>After attempting a navigational action the Navigator must decide whether or not the action succeeded and a new location was reached. To do this, it first checks the knowledge graph to see if there are any locations whose textual description matches the game's response. If no known locations are found, the Navigator issues a "look" command and compares the resulting description to the description of the previous location. High similarity between location descriptions indicates that the move action likely failed; low similarity indicates that a new location has likely been discovered.</p>
<p>A fuzzy string match (FuzzyWuzzy, 2011) between location descriptions is necessary since many games randomly alter location descriptions. For example, in the forested locations of Zork 1, the game will occasionally append "You hear in the distance the chirping of a song bird" to the location's description. Similarly, dropped objects will be reported in a location's description.</p>
<p>Finally, to finish a location change, the Navigator updates the Knowledge Graph by adding a new location and connecting it to the previous location. This step results in an ad-hoc map of the game world. In future work, this map could be used to efficiently revisit previous locations or to guide future exploration.</p>
<p>Specialized Decision Modules</p>
<p>NAIL uses several highly specialized decision modules, mainly ensure the agent doesn't get stuck while playing a game.</p>
<p>• Darkness: Emits a "turn on" action in response to observation text containing phrases "pitch black" or "too dark to see".</p>
<p>• Restart: Emits a "restart" action to restart the game in response to an observation containing "restart", "restore", and "quit" being observed.</p>
<p>• Yes-No: Randomly emits either "Yes" or "No" in response to common game prompts asking the player to answer a yes-no question.</p>
<p>• YouHaveTo: Attempts to take advantage of in-game hints by using a set of custom regular expressions to parse suggested actions from the game's response. For example: "You'll have to get out of bed first" will emit "get out of bed" as the next action. • Idler: As a fallback when no other decision module is eager for control, the Idler randomly composes actions from hundreds of common verb phrases combined with nearby objects. This exhaustive exploration over possible actions sometimes produces combinations that can get the agent unstuck.</p>
<p>Due to the flexibility of NAIL's architecture it's quite easy to create new decision modules to handle different situations.</p>
<p>Decision Modules as Python Generators</p>
<p>A key design choice was to implement decision modules as Python generators. Using Python's yield to generate actions allows a straight-line implementation of the logic within each DM. Consider the case of a MorningRoutine decision module. Using a generator, the implementation is straightforward:</p>
<p>class In other words, generators allow the agent to be written as if it has direct access to the environment's step function when, in reality, it is being invoked as a library and does not have direct access to the environment. This is commonly the case in competition settings.</p>
<p>Knowledge Graph</p>
<p>NAIL accumulates knowledge about the game world as the agent interacts with the game. Specifically, NAIL's knowledge graph keeps track of objects, past interactions, locations, connections between locations, object states, and unrecognized words. This information is used by decision modules to compute eagerness and generate actions. In turn, the decision modules modify the knowledge graph to reflect the consequences of their actions.</p>
<p>At the top level, the knowledge graph is organized as follows:</p>
<p>• Current Location: Player's current location, updated by the Navigator upon successful movement.</p>
<p>• Locations: List of all discovered locations.</p>
<p>• Connection Graph: Graph of connections between discovered locations. Updated by the Navigator upon successful movement. • Inventory: List of objects in the player's inventory. Updated by any decision module that issues take/drop commands. • Unrecognized Words: List of words not recognized by the game. NAIL avoids taking actions containing any unrecognized words. Updated after each action by matching the game's response against a custom list of unrecognized responses (e.g. "That's not a verb I recognise.").</p>
<p>Each Location contains the following information:</p>
<p>• Name: The short name of the location (e.g. "West of House").</p>
<p>• Description: The full-length description of the location, as returned by a "look" command. Populated by the Navigator upon discovering the location. • Entities: List of entities (interactive objects or characters) present at that location. Populated by the Examiner. • Action Record: List of all actions NAIL has attempted at this location along with the game's response and NAIL's estimate of how likely the response indicates success. This information is optionally used by decision modules to avoid repeating actions that previously failed.</p>
<p>Finally, each Entity contains the following information:</p>
<p>• Names: List of discovered names for this entity. Many games are flexible when referring to entities -e.g. the Brass Lantern in Zork may be alternatively referred to as "lantern" or even just "brass." • Description: Long-form description of the entity -as given by "examine entity." Populated by Examiner. • Entities: List of contained entities -e.g. in the case of a container such as a chest.</p>
<p>• State: Keeps track of a list of manually-defined states: Open/Closed, Locked/Unlocked, On/Off. Also keeps track of whether the item has been used -in the case of consumable items. • Attributes: A manually-defined list of object attributes: Openable, Lockable, Switchable.</p>
<p>Attributes inform which verbs are expected to succeed on a particular object.</p>
<p>Beyond its use to Decision Modules, the knowledge graph also provides an interpretable representation of NAIL's understanding of the game. By comparing the knowledge graph to the published map for well documented games like Zork, it was possible to track down bugs in NAIL's decision modules.</p>
<p>Encoding Action Effects</p>
<p>The core interaction for many games relies heavily on a small set of common actions -take, drop, turn on, push, pull, go north, etc. Furthermore, the effects of these common actions are reasonably general across games. For example, the take action, if successful, will move an object from the player's current location to the player's inventory.</p>
<p>For these common actions, the expected changes to the knowledge graph are manually implemented and associated with the action. This association allows the effects of the action to be implemented once, and subsequently used by many different decision modules. For non-common actions, we do not make any changes to the knowledge graph, aside from recording the action and its probability of success. In future work, it may be possible to learn the effects of uncommon actions.</p>
<p>Text-Adventure Competition</p>
<p>To meet the needs of generality and efficient exploration of unseen games, we developed and evaluated NAIL on a set of fifty-six IF games (full list in Table 3) using the Jericho (2018) Learning Environment.</p>
<p>Jericho was a ideal learning environment because of its ability to introspect and provide ground truth knowledge of the game state. Our primary metric was normalized game score averaged over all games. However, due to the sparse rewards in most games, improvements to the NAIL agent often weren't reflected in game score. To address this problem, we created Dependencies, a fine-grained metric that quantifies progress towards the first point of score on each game. Specifically this metric manually defines the locations needed to be visited, the items that need to be acquired, entities that need to be detected, and the key actions that must be performed. The following snippet shows the Dependencies for the game Balances:</p>
<p>analyzer To satisfy the Entity Dependency (EntDep), the agent must detect an Entity called "furniture" or "wooden furniture" at Location 49. This dependency is verified by using Jericho's introspection feature to detect when the agent visits the location corresponding to world object number 49, and looking into the Knowledge Graph for an Entity named furniture at the KG's current Location. The Action Dependency (ActDep) "search furniture" is satisfied upon receiving a new observation that contains the text "you come across an old box." This is verified simply by monitoring the incoming observations through the event stream. Simple text matching is sufficient to recognize the results of key actions that progress the game. The Location Dependency (LocDep) is satisfied when the agent visits location number 53 (aka Pocket Valley). Locations are verified by using Jericho to inspect the game and return the world object number corresponding to the player's actual location. Finally the Inventory Dependency (InvDep) is satisfied when the world object number 62 exists in the player's inventory. This is verified using Jericho to access the list of world objects belonging to the player. We implemented Dependencies for each of the fifty-six games in the Jericho suite. Figure 5 shows that these Dependencies help quantify progress and more importantly, help pinpoint exactly which parts of the agent need to be improved: if the agent is failing ActDeps, then perhaps the Interactor needs to use a different set of verbs, conversely if NavDeps are failing, a bug may have entered the Navigator.</p>
<p>Results</p>
<p>NAIL won first place in the 2018 Text-Based Adventure AI Competition Atkinson et al. (2019), where it was evaluated on a set of twenty unknown parser-based IF games. Designed to assess an agent's ability to achieve human-like performance, the competition only allowed one thousand steps of interaction per game, comparable to a few hours of playtime for a human. Each agent's scores were normalized by the maximum possible score per game, then averaged over all games to obtain the final scores shown in Table 2 Table 2: Performance on the test set of 20 games in (unless stated otherwise) 1000 time steps per game. "% completion" is the average score percentage an agent achieved over all games and runs; "% non-zero" is the percentage of games in which an agent achieved any score, averaged over all runs. Standard deviations (SD), wherever given, refer to 10 runs over all games. Where they are not given, only 1 run could be completed. </p>
<p>Analysis</p>
<p>As apparent from the competition results, agents have a long way to go towards solving unseen games. However, the progress over the past three years of the competition is encouraging. NAIL advances the state-of-the-art in comparison to other agents in several ways:</p>
<ol>
<li>
<p>NAIL maintains an explicit Knowledge Graph which tracks relevant game information and builds a map of the game world. The information contained within is both used and populated by decision modules as the game progresses. This knowledge representation is human-interpretable and debuggable given ground-truth information about the game.</p>
</li>
<li>
<p>Unlike prior agents, NAIL leverages the intuition that interactive objects can be examined, and extensively uses its Examiner decision module as a gatekeeper for deciding which objects are worth interacting with. Across the training set of 56 games, 26% of NAIL's actions are Examines, versus 8% for CARL, 2% for Golovin, and only 0.2% for BYU. By exhaustively examining candidate objects, NAIL can focus actions on only the objects that are recognized by the game's parser.</p>
</li>
<li>
<p>NAIL is the first agent to use a Validity Detector, a learned model, to decide whether actions have succeeded. This model is key to correctly populating the Knowledge Graph and is used extensively by individual decision modules to reason about the success of their actions.</p>
</li>
</ol>
<p>To further understand NAIL's performance, we selectively ablate NAIL's decision modules. Figure 6 shows the average normalized score of NAIL across Jericho games as decision modules are sequentially added. Without any decision modules, NAIL is capable of only performing the "look" command and gets a score of 0.53%, since some games start with small positive score. Adding the Navigator allows the agent to locomote and create a map of the game world. Using this module, scores go up to 1.2%, primarily due to games that reward visiting new locations. Next, the Hoarder issues "take all" commands at each new location it visits to collect items. Scores increase to 1.6% since many games reward the player for acquiring treasures or key items. Adding the Examiner module allows the agent to more deeply search the environment and reason about which objects are interactive, boosting the score to 2.6%. Leveraging the objects identified by the Examiner, the Interactor uses its language model to generate likely actions for application to those objects. These interactions are key to solving puzzles and boost the score to 3.5%. Together, these modules make up NAIL's core and account for the lion's share of the score. A 0.2% gain is added by the Idler which exhaustively generates common IF actions when no other module is eager to take control. Finally, the specialized decision modules together contribute 0.03%.</p>
<p>Discussion</p>
<p>Interactive Fiction games are rich narrative adventures that challenge even skilled human players. We presented NAIL, an open-source, 9 competition-winning IF agent. More than just a baseline for future comparison, we expect that NAIL's extendable architecture can serve as a starting point for future IF agents.    : Analysis of walkthroughs reveals that over 90% of actions are one and two words in length. Among these actions, there were 530 unique verbs used, but the 100 most common account for 95% of all actions.</p>
<p>A Analysis of Parser-Based IF Games</p>
<p>Perhaps the least friendly user interface of all time, parser-based IF games accept any natural language string as input, and use a parser to interpret the player's action. The difficulty of using the interface stems from the fact that many natural language strings are not recognized by the parser and result in failed actions. For example, many games will produce canned responses such as "I don't know the word x." or "You can't y." Since the parser is hidden, players often need to read a manual and experiment with the game to discover what types of actions are recognized. Fortunately, parsers for many popular IF games are similar in the types of actions they accept and the responses they produce for unrecognized actions. This standardization reduces the burden on learning agents, as they do not have to generate arbitrarily complex natural language.</p>
<p>To better understand the complex action space of parser-based games, we analyzed human-created walkthroughs for 188 games. From these walkthroughs we extracted 20,263 natural language actions. As shown in Figure 7 (left), most actions are one or two words in length, with a maximum of five words 10 . Further analysis reveals that these actions have extensive structure: single-word actions are often shortcuts provided by the game for navigation ("north" moves the player north), examination ("look" describes the current location), and item management ("inventory" lists the objects carried). Two-word actions take the form of verb-object ("climb tree", "eat apple"). Three-word actions are commonly verb-preposition-object ("search under bed"), but can occasionally take on different patterns ("turn dial left"). Though uncommon, four-word actions include the pattern verb-objectpreposition-object ("unlock chest with key", "ask bolitho about ghost"). Five-word actions commonly used multiple words to describe an object: "attack troll with brass lantern."</p>
<p>The verb distribution shows that the majority of actions stem from a compact set of verbs focused on navigation, item acquisition, and environment examination. However, the distribution in Figure  7 (right) has a long tail corresponding to a diverse set of verbs used to interact with objects in the environment. The nouns used in these commands are highly varied from game to game. Some games even go so far as to create their own proper nouns for special objects and spells. Such words are not in any English dictionary and need to be remembered from the observation text.</p>
<p>Altogether, this analysis indicates that the action generation task in IF games is significantly more structured than generating free-form dialog. Thus, while learning agents still need to understand arbitrary free-form text presented by the game, they only need to generate a compact subset of language.</p>
<p>Figure 1 :
1Interactive fiction games made Infocom the dominant computer game company of the early 1980s. Right: Transcript of Zork, with player actions in green.</p>
<p>Figure 2 :
2NAIL consists of multiple Decision Modules, which are designed to perform specialized tasks. One decision module at a time may be active. The active module is responsible for generating actions and updating the knowledge graph with the effects of its actions. The knowledge graph builds a structured representation of the objects, locations, and interactions observed in the game so far and is used by the decision modules to select actions.</p>
<p>. deps = [ EntDep ([ ' wooden furniture ' , ' furniture '] , loc =49) , ActDep ( ' search furniture ' , ' you come across an old box ') , LocDep ( ' pocket valley ' , loc =53) , EntDep ([ ' pile of oats ' , ' oats ' , ' pile '] , loc =53) , ActDep ( ' search oats ' , ' You find a shiny scroll ! ') , InvDep ( ' shiny scroll ' , 62) ]</p>
<p>Figure 5 :
5Development of NAIL on fifty-six human-made IF games: The blue line tracks normalized game score while the orange shows the percentage of dependencies that are satisfied. The x-axis tracks the 32 commits made to NAIL after the implementation of game-specific Dependencies.</p>
<p>Figure 6 :
6Ablation of decision modules: Largest performance increases come from NAIL's core decision modules: the Navigator, the Examiner, and the Interactor.</p>
<p>Figure 7
7Figure 7: Analysis of walkthroughs reveals that over 90% of actions are one and two words in length. Among these actions, there were 530 unique verbs used, but the 100 most common account for 95% of all actions.</p>
<p>Without a generator the logic for the same DM becomes quite a bit more complex:MorningRoutine ( DecisionModule ): 
def take_control ( self ): 
obs = yield 
obs = yield ' get out of bed ' 
obs = yield ' turn on light ' 
obs = yield ' brush teeth ' </p>
<p>class MorningRoutine ( DecisionModule ): 
def <strong>init</strong> ( self ): 
self . out_of_bed = False 
self . turned_on_light = False 
self . brushed_teeth = False </p>
<p>def take_control ( self , obs ): 
if not self . out_of_bed : 
self . out_of_bed = True 
return ' get out of bed ' 
elif not self . turned_on_light : 
self . turned_on_light = True 
return ' turn on light ' 
elif not self . brushed_teeth : 
self . brushed_teeth = True 
return ' brush teeth ' </p>
<p>.Agent 
% completion % non-zero </p>
<p>M 
SD 
M 
SD </p>
<p>BYUAGENT 2016 
0.79 
-
15 
-
GOLOVIN 
1.45 
0.09 
31 3.94 
CARL (BYUAGENT 2017) 1.59 
-
30 
-
NAIL 
2.56 
0.33 45.5 2.84 </p>
<p>GOLOVIN (100 steps) 
0.99 
0.24 17.5 3.53 
NAIL (100 steps) 
0.95 
0.19 
26 2.11 
GOLOVIN (10k steps) 
1.44 
0.10 32.5 4.25 </p>
<p>RandomAgent 
1.66 
0.15 
34 2.11 </p>
<p>Table duplicatedfromAtkinson et al. (2019).</p>
<p>Table 3 :
3Raw scores for Jericho-supported games, averaged over sixteen runs. The Random agent selects actions from the set north/south/east/west/up/down/look/inventory/take all/drop/yes.
More precisely upon a new Location object being added to the knowledge graph. 5 For the games that don't support the take all command, objects may still be acquired individually using the Interactor decision module.
Although language used in web titles is definitely not the same as actions in IF games, in terms of both speed and perplexity, LMs trained on titles have been reported to outperform LMs trained on document body text on general information retrieval tasks(Wang et al., 2010).7  As discussed in Appendix A, the vast majority of actions taken by human players can be expressed by one of these two templates.
LM probabilities for two-word actions are nearly always higher than four-word actions. Fortunately, as shown inFigure 7, humans also prefer shorter actions.
All actions with six words or longer were reducible to equivalent shorter actions.
AcknowledgementsThe authors would like to thank Marc-Alexandre Côté, Xingdi Yuan, and Alekh Agarwal for their comments and suggestions. Additional thanks to Shuohang Wang for testing models for learning priorities over examined objects.
Playing text-adventure games with graph-based deep reinforcement learning. CoRR. Prithviraj Ammanabrolu, Mark O Riedl, abs/1812.01628Prithviraj Ammanabrolu and Mark O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. CoRR, abs/1812.01628, 2018. URL http://arxiv.org/abs/1812. 1628.</p>
<p>The text-based adventure ai competition. T Atkinson, H Baier, T Copplestone, S Devlin, J Swan, 10.1109/TG.2019.2896017IEEE Transactions on Games. T. Atkinson, H. Baier, T. Copplestone, S. Devlin, and J. Swan. The text-based adventure ai competition. IEEE Transactions on Games, pages 1-1, 2019. ISSN 2475-1502. doi: 10.1109/TG.2019.2896017.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, 10.24963/ijcai.2017/144doi: 10.24963/ ijcai.2017/144IJCAI. Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock? affordance extraction via word embeddings. In IJCAI, pages 1039-1045, 2017. doi: 10.24963/ ijcai.2017/144. URL https://doi.org/1 .24963/ijcai.2 17/144.</p>
<p>Fuzzy string matching in python. Fuzzywuzzy, FuzzyWuzzy. Fuzzy string matching in python. https://github.com/seatgeek/fuzzywuzzy, 2011.</p>
<p>Learning how not to act in text-based games. Matan Haroush, Tom Zahavy, Daniel J Mankowitz, Shie Mannor, Matan Haroush, Tom Zahavy, Daniel J. Mankowitz, and Shie Mannor. Learning how not to act in text-based games, 2018. URL https://openreview.net/forum?id=B1-tVX1Pz.</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, ACL. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In ACL, 2016.</p>
<p>spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. Matthew Honnibal, Ines Montani, To appear, 2017. 9 NAIL's source code is available atMatthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embed- dings, convolutional neural networks and incremental parsing. To appear, 2017. 9 NAIL's source code is available at https://github.com/Microsoft/nail_agent. 11</p>
<p>A learning environment for interactive fiction games. Jericho, Jericho, Jericho. Jericho: A learning environment for interactive fiction games. https://github.com/ Microsoft/jericho, 2018.</p>
<p>Bag of tricks for efficient text classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, EACL. Association for Computational LinguisticsArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In EACL, pages 427-431. Association for Computational Linguistics, April 2017.</p>
<p>Skip-thought vectors. Ryan Kiros, Yukun Zhu, R Ruslan, Richard Salakhutdinov, Raquel Zemel, Antonio Urtasun, Sanja Torralba, Fidler, NIPS. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. GarnettCurran Associates, IncRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, NIPS, pages 3294-3302. Curran Associates, Inc., 2015. URL http: //papers.nips.cc/paper/595 -skip-thought-vectors.pdf.</p>
<p>Text-based adventures of the golovin AI agent. CoRR, abs/1705.05637. Bartosz Kostka, Jaroslaw Kwiecien, Jakub Kowalski, Pawel Rychlikowski, Bartosz Kostka, Jaroslaw Kwiecien, Jakub Kowalski, and Pawel Rychlikowski. Text-based adventures of the golovin AI agent. CoRR, abs/1705.05637, 2017. URL http://arxiv.org/abs/17 5. 5637.</p>
<p>Efficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, abs/1301.3781CoRR. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representa- tions in vector space. CoRR, abs/1301.3781, 2013. URL http://arxiv.org/abs/13 1.3781.</p>
<p>Language understanding for textbased games using deep reinforcement learning. Karthik Narasimhan, Tejas D Kulkarni, Regina Barzilay, EMNLP. Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for text- based games using deep reinforcement learning. In EMNLP, pages 1-11, 2015. URL http: //aclweb.org/anthology/D/D15/D15-1 1.pdf.</p>
<p>Multi-style language model for web scale information retrieval. Kuansan Wang, Xiaolong Li, Jianfeng Gao, SIGIR'10. Kuansan Wang, Xiaolong Li, and Jianfeng Gao. Multi-style language model for web scale information retrieval. In SIGIR'10, pages 467-474, 2010. URL https://dl.acm.org/citation.cfm?id= 1835528.</p>
<p>Counting to explore and generalize in text-based games. CoRR, abs/1806.11525. Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet, Matthew J Combes, Adam Hausknecht, Trischler, Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525, 2018. URL http://arxiv.org/abs/18 6.11525.</p>
<p>Using reinforcement learning to learn how to play text-based games. Mikulás Zelinka, abs/1801.01999Mikulás Zelinka. Using reinforcement learning to learn how to play text-based games. CoRR, abs/1801.01999, 2018. URL http://arxiv.org/abs/18 1. 1999.</p>            </div>
        </div>

    </div>
</body>
</html>
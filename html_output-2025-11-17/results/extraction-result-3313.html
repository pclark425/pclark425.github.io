<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3313 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3313</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3313</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-8a9e11addba791860a9dbf15de75cc28d2cf844c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8a9e11addba791860a9dbf15de75cc28d2cf844c" target="_blank">Are We on the Right Way for Evaluating Large Vision-Language Models?</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> MMStar is presented, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples.</p>
                <p><strong>Paper Abstract:</strong> Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.9% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3313.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3313.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>0-shot/2-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot and Two-shot In-context Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies used to elicit reasoning from LLMs: 0-shot provides no examples, 2-shot provides two in-context exemplars to stabilize outputs and align answer formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic LLM evaluation (applies to many LLMs in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In-context prompting paradigms applied to a wide range of LLMs (closed-source and open-source) to evaluate their text-only ability on multi-modal benchmarks; 2-shot was used to reduce refusals and to align answer formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['0-shot prompting', '2-shot in-context prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>0-shot: present the question without examples; 2-shot: present two example question+answer pairs before the target question to guide the model's output format and reduce refusal. Implemented by the authors when evaluating many LLMs on benchmarks (Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles — only simple prompting variations (0-shot vs 2-shot) were used; no deeper multi-style reasoning interventions (e.g., chain-of-thought) were performed in this paper's primary evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista, MMStar (used for LLM evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A set of multi-modal benchmarks (some intended to require images) used to probe LLMs' reasoning and world-knowledge capabilities in a text-only setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>The paper reports many 0-shot and 2-shot numbers. Example: GeminiPro 0-shot MMMU 42.9 (Table 1); GeminiPro 2-shot MMMU 42.7 (Table 2). Authors report that 2-shot is more stable, reduces refusal and aligns output formatting; for MMStar LLM evaluation they used 2-shot because 0-shot performed poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>2-shot in-context prompting is described as more stable and reduces refusals compared to 0-shot. The authors used 2-shot for LLM inspectors in coarse filtering and for LLM evaluation on MMStar to minimize refusal and standardize formats. No experiments using diverse reasoning styles (e.g., chain-of-thought vs direct answer) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>2-shot prompting is preferred by the authors for more stable LLM behavior and consistent answer formats; however, 2-shot does not address the core problem identified (visual-unnecessary items and data leakage).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For many LLMs, performance differences between 0-shot and 2-shot are small (see GeminiPro example above), indicating that simply adding two exemplars does not fundamentally change whether models rely on visual content or leaked memorized answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We on the Right Way for Evaluating Large Vision-Language Models?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3313.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3313.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LVLM-text vs LLM vs LVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation modes: LLM base, LVLM-text (no image), and LVLM (with image)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparative evaluation modes used to detect visual necessity and data leakage: LLM base (text only), LVLM-text (an LVLM run without image inputs), and LVLM (full vision+language); used to compute Multi-modal Gain (MG) and Multi-modal Leakage (ML).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic (applied to 16 LVLMs and 20 LLMs in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper evaluates each LVLM in three modes: (1) its LLM base (text-only), (2) LVLM run with image inputs removed or replaced (LVLM-text), and (3) LVLM with images. The differences are used to quantify multi-modal contribution and training leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['text-only reasoning (LLM base)', 'LVLM-text (LVLM architecture but with images removed)', 'multi-modal reasoning (LVLM with images)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM base: the underlying language model answering without any multi-modal training effects; LVLM-text: the LVLM evaluated with image tokens removed (or grayscale images substituted) to test whether multi-modal training has caused memorization/leakage; LVLM: normal multi-modal inference with images provided. Implemented across models in Tables 3, 5 and the MG/ML calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles — the comparison is across input modalities (text-only vs text+image) and not across diverse internal reasoning algorithms; diversity arises from different model architectures and their training data, not from differing explicit reasoning prompting styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMStar and six popular multi-modal benchmarks (MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks designed to test visual understanding and multimodal reasoning across coarse/fine perception, instance reasoning, logic, science & technology, and math.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports S_t (LLM), S_wv (LVLM-text), S_v (LVLM) and computes MG = S_v - S_wv and ML = max(0, S_wv - S_t). Examples from tables: InternLM-XC2 average MG 28.1 and ML 9.4 (Table 6 avg); GPT4V average MG 25.9 and ML 5.3; GeminiPro-Vision average MG 22.8 and ML 0.2; Sphinx-X-MoE average MG 16.2 and ML 12.3 (Table 6, model-average columns).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Explicit comparisons are across the three modes: Many LVLMs show non-zero ML (LVLM-text > LLM base), indicating the LVLM's multi-modal training leaked evaluation items into the text-only capability. MG quantifies actual performance gain from images; ML quantifies spurious gains due to leakage. The paper demonstrates cases where LVLM-text exceeds its LLM base (evidence of leakage), and cases where LVLM (with images) substantially outperforms LVLM-text (real multi-modal gain).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using LVLM-text vs LLM base exposes unintentional leakage: several LVLMs can answer image-requiring questions without images, indicating data memorization; MG/ML metrics disentangle real multi-modal benefit from leakage. Benchmarks differ: MMBench shows large average MG (50.1) indicating overlap with training data; MMStar shows low average ML (1.9) indicating robustness to leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some LVLMs have LVLM-text scores substantially higher than their LLM bases (high ML), so naively comparing LVLMs by final accuracy can mislead; the authors show LVLM-text sometimes even surpasses some LVLMs that access images (i.e., strong leakage can distort ranking). No experiment indicates that varying internal reasoning algorithms (e.g., different chain-of-thought styles) was performed to mitigate leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We on the Right Way for Evaluating Large Vision-Language Models?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3313.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3313.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 / GPT-4V(ision) (GPT4-Turbo and GPT4V variants used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 family evaluated both as a text-only LLM (GPT4-Turbo) and as a vision-capable LVLM (GPT4V), with low- and high-resolution vision settings showing notable performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-Turbo (LLM) / GPT4V (LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large language / vision-language models from OpenAI used as both LLM inspectors and LVLMs; GPT4V evaluated with two image resolutions (low/high) to test effect of more image tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['0-shot and 2-shot prompting (LLM evaluations)', 'Multi-resolution visual input (low-resolution vs high-resolution image tokens for GPT4V)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM prompting: GPT4-Turbo evaluated in 0-shot/2-shot modes like other LLMs. GPT4V: the LVLM was run with different image token resolutions; higher resolution means more image tokens and improved visual information for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles — variations are in prompting and image-resolution (input modality richness), not in different internal reasoning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista, MMStar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-modal benchmarks; GPT4V evaluated on MMStar with low- and high-resolution settings to measure sensitivity to image token amount.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>From Table 5 (MMStar): GPT4V (low) average = 46.1; GPT4V (high) average = 57.1. Reported MG (on MMStar) for GPT4V (high) = 43.6 and ML = 1.3 (Table 5). As an LLM, GPT4-Turbo 0-shot on MMMU = 41.2 (Table 1) and 2-shot on MMMU = 42.0 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Increasing image resolution and number of image tokens for GPT4V increased average score on MMStar by ~11 percentage points (46.1 -> 57.1), showing that richer visual input boosts multi-modal reasoning performance. GPT4V also shows substantial multi-modal gain while maintaining low ML on MMStar.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-resolution visual inputs materially improve GPT4V's performance on a challenging, vision-dependent benchmark; GPT4V attains the best MMStar score in the paper (57.1) and shows relatively small leakage (low ML) compared to some open-source LVLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although GPT4V improves with resolution, the paper notes that even the best LVLMs (including GPT4V) still struggle in several core axes (FP, LR, ST, MA) and do not uniformly exceed 60% in those dimensions on MMStar.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We on the Right Way for Evaluating Large Vision-Language Models?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3313.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3313.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeminiPro / GeminiPro-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeminiPro (LLM) / GeminiPro-Vision (LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's GeminiPro was used as an LLM inspector and GeminiPro-Vision as an LVLM; GeminiPro achieves high text-only scores on some benchmarks, and GeminiPro-Vision shows multi-modal gain with negligible leakage on MMStar.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GeminiPro (LLM) / GeminiPro-Vision (LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large models from the Gemini family used across evaluations; GeminiPro evaluated as text-only LLM, and GeminiPro-Vision evaluated as LVLM and in LVLM-text mode.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['0-shot and 2-shot prompting for LLM evaluation', 'LVLM with image inputs and LVLM-text (images removed) for leakage analysis']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same general techniques as other models: the LLM was prompted 0/2-shot, and GeminiPro-Vision was evaluated in three modes (LLM base, LVLM-text, LVLM) to compute MG and ML.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles — the paper applies prompting and modality ablations but does not present multiple explicit internal reasoning strategies for Gemini.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista, MMStar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-modal benchmark suite; GeminiPro used as an LLM inspector, GeminiPro-Vision used to measure multi-modal gain and leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Examples in paper: GeminiPro (0-shot) average across six benchmarks = 41.4 (Table 1). GeminiPro-Vision on MMStar average = 42.6 with MG (MMStar) = 27.4 and ML = 0.0 (Table 5). GeminiPro (LLM) on MMMU = 42.9 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>GeminiPro shows strong text-only performance on several benchmarks, highlighting that many benchmark items are answerable without images; GeminiPro-Vision shows a sizeable multi-modal gain on MMStar while having essentially zero measured ML there, implying its multi-modal training did not substantially leak those MMStar items into text-only outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GeminiPro's strong LLM-only performance underscores the prevalence of vision-unnecessary items in existing benchmarks. GeminiPro-Vision's low ML on MMStar suggests careful multi-modal training data practices can limit leakage while achieving multi-modal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>GeminiPro as a text-only model can outperform some LVLMs that access images on benchmarks that include vision-independent or leaked questions, demonstrating that final accuracy alone can mislead unless MG/ML are considered.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We on the Right Way for Evaluating Large Vision-Language Models?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3313.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3313.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen1.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen1.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 72B open-source language model (Qwen series) that attains high text-only performance on multiple multi-modal benchmarks, demonstrating that LLM-only world knowledge can solve many supposedly vision-required items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen1.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large open-source LLM (72B) used as an LLM inspector in 0-shot and 2-shot modes; used to show that LLMs can answer many multi-modal benchmark questions without images.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['0-shot prompting', '2-shot in-context prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated with standard prompting (0/2-shot) on multi-modal benchmarks as a text-only model; served as an inspector in the coarse filter process and as a baseline in tables (Table 1/2).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style — only typical prompting strategies are applied; no alternative internal reasoning styles are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-modal benchmarks: authors show Qwen1.5-72B's text-only results to illustrate vision-unnecessary and leaked items.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 2 (2-shot) reports Qwen1.5-72B: MMMU 42.4, MMB 21.1, ScienceQA 70.1, AI2D 60.9, SEED 40.7, MathVista 26.3, Avg. 43.6 (2-shot). This text-only performance often exceeds random choice and some LVLMs with images.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Qwen1.5-72B's strong LLM-only performance is used as evidence that many benchmark items are solvable without images or have been leaked into LLM training data. The authors compare these text-only scores to LVLMs' image-enabled scores to show potential overestimation of multi-modal capability when relying only on final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large, well-trained LLMs (e.g., Qwen1.5-72B) can attain high accuracy on several multi-modal benchmarks without image inputs, demonstrating the need to filter vision-unnecessary items and to measure leakage (ML) and real image-derived gains (MG).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with high text-only accuracy, Qwen1.5-72B does not necessarily perform well on the curated MMStar dataset (authors constructed MMStar so LLMs perform near random), indicating that properly vision-dependent benchmarks mitigate this problem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We on the Right Way for Evaluating Large Vision-Language Models?', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learn to explain: Multimodal reasoning via thought chains for science question answering <em>(Rating: 2)</em></li>
                <li>Visual instruction tuning <em>(Rating: 1)</em></li>
                <li>MMBench: Is your multi-modal model an all-around player? <em>(Rating: 2)</em></li>
                <li>MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3313",
    "paper_id": "paper-8a9e11addba791860a9dbf15de75cc28d2cf844c",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "0-shot/2-shot prompting",
            "name_full": "Zero-shot and Two-shot In-context Prompting",
            "brief_description": "Prompting strategies used to elicit reasoning from LLMs: 0-shot provides no examples, 2-shot provides two in-context exemplars to stabilize outputs and align answer formats.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Generic LLM evaluation (applies to many LLMs in the paper)",
            "model_description": "In-context prompting paradigms applied to a wide range of LLMs (closed-source and open-source) to evaluate their text-only ability on multi-modal benchmarks; 2-shot was used to reduce refusals and to align answer formatting.",
            "model_size": null,
            "reasoning_methods": [
                "0-shot prompting",
                "2-shot in-context prompting"
            ],
            "reasoning_methods_description": "0-shot: present the question without examples; 2-shot: present two example question+answer pairs before the target question to guide the model's output format and reduce refusal. Implemented by the authors when evaluating many LLMs on benchmarks (Tables 1 and 2).",
            "diversity_of_methods": "similar styles — only simple prompting variations (0-shot vs 2-shot) were used; no deeper multi-style reasoning interventions (e.g., chain-of-thought) were performed in this paper's primary evaluations.",
            "reasoning_task_name": "MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista, MMStar (used for LLM evaluations)",
            "reasoning_task_description": "A set of multi-modal benchmarks (some intended to require images) used to probe LLMs' reasoning and world-knowledge capabilities in a text-only setting.",
            "performance_by_method": "The paper reports many 0-shot and 2-shot numbers. Example: GeminiPro 0-shot MMMU 42.9 (Table 1); GeminiPro 2-shot MMMU 42.7 (Table 2). Authors report that 2-shot is more stable, reduces refusal and aligns output formatting; for MMStar LLM evaluation they used 2-shot because 0-shot performed poorly.",
            "comparison_of_methods": "2-shot in-context prompting is described as more stable and reduces refusals compared to 0-shot. The authors used 2-shot for LLM inspectors in coarse filtering and for LLM evaluation on MMStar to minimize refusal and standardize formats. No experiments using diverse reasoning styles (e.g., chain-of-thought vs direct answer) are reported.",
            "key_findings": "2-shot prompting is preferred by the authors for more stable LLM behavior and consistent answer formats; however, 2-shot does not address the core problem identified (visual-unnecessary items and data leakage).",
            "counter_examples_or_negative_results": "For many LLMs, performance differences between 0-shot and 2-shot are small (see GeminiPro example above), indicating that simply adding two exemplars does not fundamentally change whether models rely on visual content or leaked memorized answers.",
            "uuid": "e3313.0",
            "source_info": {
                "paper_title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LVLM-text vs LLM vs LVLM",
            "name_full": "Evaluation modes: LLM base, LVLM-text (no image), and LVLM (with image)",
            "brief_description": "Comparative evaluation modes used to detect visual necessity and data leakage: LLM base (text only), LVLM-text (an LVLM run without image inputs), and LVLM (full vision+language); used to compute Multi-modal Gain (MG) and Multi-modal Leakage (ML).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Generic (applied to 16 LVLMs and 20 LLMs in experiments)",
            "model_description": "The paper evaluates each LVLM in three modes: (1) its LLM base (text-only), (2) LVLM run with image inputs removed or replaced (LVLM-text), and (3) LVLM with images. The differences are used to quantify multi-modal contribution and training leakage.",
            "model_size": null,
            "reasoning_methods": [
                "text-only reasoning (LLM base)",
                "LVLM-text (LVLM architecture but with images removed)",
                "multi-modal reasoning (LVLM with images)"
            ],
            "reasoning_methods_description": "LLM base: the underlying language model answering without any multi-modal training effects; LVLM-text: the LVLM evaluated with image tokens removed (or grayscale images substituted) to test whether multi-modal training has caused memorization/leakage; LVLM: normal multi-modal inference with images provided. Implemented across models in Tables 3, 5 and the MG/ML calculations.",
            "diversity_of_methods": "similar styles — the comparison is across input modalities (text-only vs text+image) and not across diverse internal reasoning algorithms; diversity arises from different model architectures and their training data, not from differing explicit reasoning prompting styles.",
            "reasoning_task_name": "MMStar and six popular multi-modal benchmarks (MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista)",
            "reasoning_task_description": "Benchmarks designed to test visual understanding and multimodal reasoning across coarse/fine perception, instance reasoning, logic, science & technology, and math.",
            "performance_by_method": "Paper reports S_t (LLM), S_wv (LVLM-text), S_v (LVLM) and computes MG = S_v - S_wv and ML = max(0, S_wv - S_t). Examples from tables: InternLM-XC2 average MG 28.1 and ML 9.4 (Table 6 avg); GPT4V average MG 25.9 and ML 5.3; GeminiPro-Vision average MG 22.8 and ML 0.2; Sphinx-X-MoE average MG 16.2 and ML 12.3 (Table 6, model-average columns).",
            "comparison_of_methods": "Explicit comparisons are across the three modes: Many LVLMs show non-zero ML (LVLM-text &gt; LLM base), indicating the LVLM's multi-modal training leaked evaluation items into the text-only capability. MG quantifies actual performance gain from images; ML quantifies spurious gains due to leakage. The paper demonstrates cases where LVLM-text exceeds its LLM base (evidence of leakage), and cases where LVLM (with images) substantially outperforms LVLM-text (real multi-modal gain).",
            "key_findings": "Using LVLM-text vs LLM base exposes unintentional leakage: several LVLMs can answer image-requiring questions without images, indicating data memorization; MG/ML metrics disentangle real multi-modal benefit from leakage. Benchmarks differ: MMBench shows large average MG (50.1) indicating overlap with training data; MMStar shows low average ML (1.9) indicating robustness to leakage.",
            "counter_examples_or_negative_results": "Some LVLMs have LVLM-text scores substantially higher than their LLM bases (high ML), so naively comparing LVLMs by final accuracy can mislead; the authors show LVLM-text sometimes even surpasses some LVLMs that access images (i.e., strong leakage can distort ranking). No experiment indicates that varying internal reasoning algorithms (e.g., different chain-of-thought styles) was performed to mitigate leakage.",
            "uuid": "e3313.1",
            "source_info": {
                "paper_title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 / GPT-4V",
            "name_full": "GPT-4 / GPT-4V(ision) (GPT4-Turbo and GPT4V variants used in this paper)",
            "brief_description": "OpenAI's GPT-4 family evaluated both as a text-only LLM (GPT4-Turbo) and as a vision-capable LVLM (GPT4V), with low- and high-resolution vision settings showing notable performance differences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4-Turbo (LLM) / GPT4V (LVLM)",
            "model_description": "Closed-source large language / vision-language models from OpenAI used as both LLM inspectors and LVLMs; GPT4V evaluated with two image resolutions (low/high) to test effect of more image tokens.",
            "model_size": null,
            "reasoning_methods": [
                "0-shot and 2-shot prompting (LLM evaluations)",
                "Multi-resolution visual input (low-resolution vs high-resolution image tokens for GPT4V)"
            ],
            "reasoning_methods_description": "LLM prompting: GPT4-Turbo evaluated in 0-shot/2-shot modes like other LLMs. GPT4V: the LVLM was run with different image token resolutions; higher resolution means more image tokens and improved visual information for reasoning.",
            "diversity_of_methods": "similar styles — variations are in prompting and image-resolution (input modality richness), not in different internal reasoning algorithms.",
            "reasoning_task_name": "MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista, MMStar",
            "reasoning_task_description": "Same multi-modal benchmarks; GPT4V evaluated on MMStar with low- and high-resolution settings to measure sensitivity to image token amount.",
            "performance_by_method": "From Table 5 (MMStar): GPT4V (low) average = 46.1; GPT4V (high) average = 57.1. Reported MG (on MMStar) for GPT4V (high) = 43.6 and ML = 1.3 (Table 5). As an LLM, GPT4-Turbo 0-shot on MMMU = 41.2 (Table 1) and 2-shot on MMMU = 42.0 (Table 2).",
            "comparison_of_methods": "Increasing image resolution and number of image tokens for GPT4V increased average score on MMStar by ~11 percentage points (46.1 -&gt; 57.1), showing that richer visual input boosts multi-modal reasoning performance. GPT4V also shows substantial multi-modal gain while maintaining low ML on MMStar.",
            "key_findings": "High-resolution visual inputs materially improve GPT4V's performance on a challenging, vision-dependent benchmark; GPT4V attains the best MMStar score in the paper (57.1) and shows relatively small leakage (low ML) compared to some open-source LVLMs.",
            "counter_examples_or_negative_results": "Although GPT4V improves with resolution, the paper notes that even the best LVLMs (including GPT4V) still struggle in several core axes (FP, LR, ST, MA) and do not uniformly exceed 60% in those dimensions on MMStar.",
            "uuid": "e3313.2",
            "source_info": {
                "paper_title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GeminiPro / GeminiPro-Vision",
            "name_full": "GeminiPro (LLM) / GeminiPro-Vision (LVLM)",
            "brief_description": "Google's GeminiPro was used as an LLM inspector and GeminiPro-Vision as an LVLM; GeminiPro achieves high text-only scores on some benchmarks, and GeminiPro-Vision shows multi-modal gain with negligible leakage on MMStar.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GeminiPro (LLM) / GeminiPro-Vision (LVLM)",
            "model_description": "Closed-source large models from the Gemini family used across evaluations; GeminiPro evaluated as text-only LLM, and GeminiPro-Vision evaluated as LVLM and in LVLM-text mode.",
            "model_size": null,
            "reasoning_methods": [
                "0-shot and 2-shot prompting for LLM evaluation",
                "LVLM with image inputs and LVLM-text (images removed) for leakage analysis"
            ],
            "reasoning_methods_description": "Same general techniques as other models: the LLM was prompted 0/2-shot, and GeminiPro-Vision was evaluated in three modes (LLM base, LVLM-text, LVLM) to compute MG and ML.",
            "diversity_of_methods": "similar styles — the paper applies prompting and modality ablations but does not present multiple explicit internal reasoning strategies for Gemini.",
            "reasoning_task_name": "MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista, MMStar",
            "reasoning_task_description": "Multi-modal benchmark suite; GeminiPro used as an LLM inspector, GeminiPro-Vision used to measure multi-modal gain and leakage.",
            "performance_by_method": "Examples in paper: GeminiPro (0-shot) average across six benchmarks = 41.4 (Table 1). GeminiPro-Vision on MMStar average = 42.6 with MG (MMStar) = 27.4 and ML = 0.0 (Table 5). GeminiPro (LLM) on MMMU = 42.9 (Table 1).",
            "comparison_of_methods": "GeminiPro shows strong text-only performance on several benchmarks, highlighting that many benchmark items are answerable without images; GeminiPro-Vision shows a sizeable multi-modal gain on MMStar while having essentially zero measured ML there, implying its multi-modal training did not substantially leak those MMStar items into text-only outputs.",
            "key_findings": "GeminiPro's strong LLM-only performance underscores the prevalence of vision-unnecessary items in existing benchmarks. GeminiPro-Vision's low ML on MMStar suggests careful multi-modal training data practices can limit leakage while achieving multi-modal gains.",
            "counter_examples_or_negative_results": "GeminiPro as a text-only model can outperform some LVLMs that access images on benchmarks that include vision-independent or leaked questions, demonstrating that final accuracy alone can mislead unless MG/ML are considered.",
            "uuid": "e3313.3",
            "source_info": {
                "paper_title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Qwen1.5-72B",
            "name_full": "Qwen1.5-72B",
            "brief_description": "A 72B open-source language model (Qwen series) that attains high text-only performance on multiple multi-modal benchmarks, demonstrating that LLM-only world knowledge can solve many supposedly vision-required items.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen1.5-72B",
            "model_description": "Large open-source LLM (72B) used as an LLM inspector in 0-shot and 2-shot modes; used to show that LLMs can answer many multi-modal benchmark questions without images.",
            "model_size": "72B",
            "reasoning_methods": [
                "0-shot prompting",
                "2-shot in-context prompting"
            ],
            "reasoning_methods_description": "Evaluated with standard prompting (0/2-shot) on multi-modal benchmarks as a text-only model; served as an inspector in the coarse filter process and as a baseline in tables (Table 1/2).",
            "diversity_of_methods": "single/similar style — only typical prompting strategies are applied; no alternative internal reasoning styles are reported.",
            "reasoning_task_name": "MMMU, MMBench, ScienceQA, AI2D, SEED, MathVista",
            "reasoning_task_description": "Same multi-modal benchmarks: authors show Qwen1.5-72B's text-only results to illustrate vision-unnecessary and leaked items.",
            "performance_by_method": "Table 2 (2-shot) reports Qwen1.5-72B: MMMU 42.4, MMB 21.1, ScienceQA 70.1, AI2D 60.9, SEED 40.7, MathVista 26.3, Avg. 43.6 (2-shot). This text-only performance often exceeds random choice and some LVLMs with images.",
            "comparison_of_methods": "Qwen1.5-72B's strong LLM-only performance is used as evidence that many benchmark items are solvable without images or have been leaked into LLM training data. The authors compare these text-only scores to LVLMs' image-enabled scores to show potential overestimation of multi-modal capability when relying only on final accuracy.",
            "key_findings": "Large, well-trained LLMs (e.g., Qwen1.5-72B) can attain high accuracy on several multi-modal benchmarks without image inputs, demonstrating the need to filter vision-unnecessary items and to measure leakage (ML) and real image-derived gains (MG).",
            "counter_examples_or_negative_results": "Even with high text-only accuracy, Qwen1.5-72B does not necessarily perform well on the curated MMStar dataset (authors constructed MMStar so LLMs perform near random), indicating that properly vision-dependent benchmarks mitigate this problem.",
            "uuid": "e3313.4",
            "source_info": {
                "paper_title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "rating": 2
        },
        {
            "paper_title": "Visual instruction tuning",
            "rating": 1
        },
        {
            "paper_title": "MMBench: Is your multi-modal model an all-around player?",
            "rating": 2
        },
        {
            "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI",
            "rating": 2
        }
    ],
    "cost": 0.019501249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Are We on the Right Way for Evaluating Large Vision-Language Models?</h1>
<p>Lin Chen ${ }^{1,3 <em>}$ Jinsong $\mathbf{L i}^{2,3 </em>}$ Xiaoyi Dong ${ }^{2,3}$ Pan Zhang ${ }^{3}$ Yuhang Zang ${ }^{3}$<br>Zehui Chen ${ }^{1}$ Haodong Duan ${ }^{3}$ Jiaqi Wang ${ }^{3 \dagger}$ Yu Qiao ${ }^{3}$ Dahua Lin ${ }^{2,3}$ Feng Zhao ${ }^{1 \dagger}$<br>${ }^{1}$ University of Science and Technology of China<br>${ }^{2}$ The Chinese University of Hong Kong<br>${ }^{3}$ Shanghai AI Laboratory<br>https://mmstar-benchmark.github.io/</p>
<h4>Abstract</h4>
<p>Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves $42.9 \%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over $24 \%$ on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets $43.6 \%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \%$. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.</p>
<h2>1 Introduction</h2>
<p>Encouraged by the rapid development of large language models (LLMs) [47, 4, 8, 9, 13, 1, 43], integrating visual modality into LLMs to enhance models' interactivity capabilities has witnessed ever-changing advances in recent days [54, 26, 24, 11, 52, 2, 48, 31, 5, 12]. These large visionlanguage models (LVLMs) showcase powerful visual perception and understanding capabilities, enabling them to accept image inputs from users and engage in dialogues, thereby offering a more enriched interactive experience. These achievements have further inspired the research community</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be "recalled" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.</p>
<p>To develop a variety of multi-modal benchmarks [21, 14, 27, 38, 50, 51, 29, 20, 30], constructed to explore the powerful capabilities emerging from LVLMs and provide a comprehensive and objective platform for quantitatively comparing the continually evolving models. Despite the race among existing evaluation works to construct as many axes as possible to assess the capabilities of LVLMs, we have identified two primary issues upon delving into existing evaluation samples and processes.</p>
<p>First, visual content is unnecessary for many samples. A qualified multi-modal evaluation sample should compel LVLMs to understand and reason with the visual content for correct answers. Otherwise, the evaluation sample would degrade into assessing the textual capabilities of LLM bases. Unfortunately, we have identified numerous samples across multiple popular benchmarks [27, 21, 51, 30, 20] where answers can be correctly deduced without relying on visual content. As shown in Figure 1 (a) and (b), some samples have answers directly included within the questions (e.g., What is the shape of the round dirt circle?), while others can be effortlessly answered by leveraging the rich world knowledge embedded within the LLM bases (e.g., What is the capital of Nebraska?). With a comprehensive quantitative analysis of 25 LLMs on 6 benchmarks, we observe this phenomenon is prevalent and serious. For example, more than 50% questions of ScienceQA and 20% questions of MMMU can be solved by most LLMs directly. For the powerful close source LLM GeminiPro, it achieves 42.9% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks by over 24% on average.</p>
<p>Taking aside the inappropriate samples in evaluation, we also observed strange results that LLM and LVLM could still answer some visual-necessary questions without visual content (Figure 1 (c) and (d)). A plausible explanation for this could be the inadvertent memorization of these samples during the large-scale training process, suggesting the presence of unintentional data leakage in the training of LLM and LVLM. Through a detailed study of 22 LVLMs on the 6 benchmarks, we find the unexpected leaking problem during the LVLM training is particularly serious. For example, we find Yi-VL-34B gets 15.0% higher performance than its LLM backbone on ScienceQA, Sphinx-X-MoE gets 43.6% on MMMU <em>without</em> accessing images, surpassing its LLM backbone with 17.9%, even surpassing many leading LVLMs with accessing images.</p>
<p>The existence of inappropriate questions and data leaking would lead to misjudgments of actual multi-modal performance gains and potentially misguide the study of LVLM. In pursuit of a more accurate and comprehensive evaluation, we introduce the MMStar Benchmark. MMStar is a premier, vision-critical multi-modal benchmark that includes 1,500 challenging samples, each rigorously validated by humans. It is structured to test 6 fundamental capabilities and 18 specific dimensions.</p>
<p>mensions, aiming to evaluate the multi-modal capacities of LVLMs with a carefully balanced and purified selection of samples.</p>
<p>The MMStar is a new benchmark that “Stands on the shoulders of giants”. Samples are first roughly selected from current benchmarks with an automated pipeline. In detail, we use eight powerful LLMs as candidates inspectors for visual dependency and LLM leakage, including two closed-source APIs (GPT4-Turbo [34], and GeminiPro [41]) and six leading open-source models (e.g., LLaMA-70B [43], Qwen-1.5-72B [1], and Mixtral-8x7B [19]). Samples that could be answered by more than 2 of the 8 LLMs are excluded as they may exist leaking or visual-unnecessary problems. Then we use 16 leading LVLMs (e.g., GPT4V [35], GeminiPro-Vision [41], LLaVA series [24, 26]) to gauge the difficulty of the samples and split them to four levels. Ultimately, based on the difficulty of the rough-filtered samples, strict manual review and selection are applied to curate 1,500 high-quality multimodal evaluation samples. As shown in Figure 5, these samples span 6 core multimodal capability dimensions and 18 detailed axes, aiming to probe LVLMs’ advanced multimodal capabilities with a purified and high-quality set of samples. Moreover, we design the multi-modal gain (MG) and multi-modal leakage (ML) metrics to probe LVLMs’ actual performance gain and data leakage degrees derived from multi-modal training in a benchmark-specific manner.</p>
<p>We evaluate the accuracy, MG, and ML of 16 leading LVLMs on our MMStar benchmark, the high-resolution version of GPT-4V ranks first with $57.1\%$ accuracy, showcasing its superb multi-modal capability. GPT-4V also gets the best MG and a small ML, indicating its effective multi-modal training strategy and has less data leaking.</p>
<p>In a nutshell, our contributions are threefold:</p>
<ul>
<li>We delve into existing evaluation benchmarks and processes and identify two key issues: (1) Visual content is unnecessary for many samples. (2) Unintentional data leakage exists in LLM and LVLM training. Both lead to misjudgment of LVLM capability and may misguide the following study.</li>
<li>We curate MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 challenge samples meticulously selected by humans. MMStar covers samples from diverse tasks and difficulties, aiming to evaluate the actual multi-modal capacities of LVLMs.</li>
<li>Based on MMStar, we evaluate LVLMs with Accuracy and two newly proposed metrics: multimodal gain and multi-modal leakage. The high-resolution version of GPT-4V outperforms the 16 leading LLMs and ranks first.</li>
</ul>
<h2>2 Related Work</h2>
<p>Large Vision-Language Models. As large language models (LLMs) [8, 43, 43, 47, 42, 34, 36, 9] rapidly advance, a growing fraction of the research community is focusing on integrating visual content into LLMs to build a powerful intelligent assistant with more interactive ways. Central to these large vision-language models (LVLMs) are the seminal works in modality alignment within the vision-language learning area [37, 17]. The foundation work CLIP [37] exemplifies the alignment of vision and language modalities through contrastive learning on extensive image-text pairs. Built upon the CLIP image encoder which is somewhat aligned with the language modality, current LVLMs typically utilize vast image-text pairs to connect the vision encoder and LLM, enabling LLM to receive and understand visual content [54, 26, 24, 11, 52, 2, 48, 31, 5]. For example, MiniGPT4 [54] and LLaVA [26] directly connect the vision encoder and LLM with QFormer [22] and MLP [40], showing proficiency in multi-modal dialogues. Subsequent works have further enhanced LVLMs by improving the multi-modal instruction data [24, 48, 5, 44] and designing novel modules [2, 23, 45, 28, 15, 12] for more sufficient modality alignment.</p>
<p>Evaluations of LVLMs. To probe the true capabilities of the emerging LVLMs, the research community has developed many multi-modal benchmarks encompassing a wide range of evaluation axes [27, 14, 38, 51, 39, 21, 26, 50, 46]. Early single-task benchmarks, such as VQA [16], MS-COCO [39], and OK-VQA [38], fail to holistically assess LVLMs’ general multi-modal perception and reasoning capabilities. To address this issue, comprehensive multi-modal benchmarks have been constructed [26, 21, 51, 14, 27, 7, 46]. For example, SEED [21] and MMBench [27] cover 12 and 20 evaluation dimensions respectively, while MMMU [51] spans 30 college-level subjects, providing some competitive arenas for a comprehensive comparison of cutting-edge LVLMs. However, existing evaluations of LVLMs overlook some critical issues. On the one hand, they do not guar</p>
<p>Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks. We employ a 0-shot inference strategy for evaluating all LLMs. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Strategy</th>
<th>MMMU</th>
<th>MMB</th>
<th>ScienceQA</th>
<th>AI2D</th>
<th>SEED</th>
<th>MathVista</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random Choice</td>
<td>-</td>
<td>22.1</td>
<td>0.0</td>
<td>24.2</td>
<td>23.8</td>
<td>24.3</td>
<td>17.9</td>
<td>18.7</td>
</tr>
<tr>
<td>Closed-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT4-Turbo[34]</td>
<td>0-shot</td>
<td>41.2</td>
<td>12.2</td>
<td>64.3</td>
<td>59.7</td>
<td>10.1</td>
<td>24.2</td>
<td>35.3</td>
</tr>
<tr>
<td>GeminiPro[41]</td>
<td>0-shot</td>
<td>42.9</td>
<td>18.4</td>
<td>68.9</td>
<td>59.2</td>
<td>35.5</td>
<td>23.3</td>
<td>41.4</td>
</tr>
<tr>
<td>Open-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen1.5-1.8B[1]</td>
<td>0-shot</td>
<td>29.0</td>
<td>10.0</td>
<td>54.3</td>
<td>37.9</td>
<td>28.9</td>
<td>20.4</td>
<td>30.1</td>
</tr>
<tr>
<td>Phi2-2.7B[32]</td>
<td>0-shot</td>
<td>20.0</td>
<td>7.2</td>
<td>47.1</td>
<td>38.7</td>
<td>26.4</td>
<td>22.0</td>
<td>26.9</td>
</tr>
<tr>
<td>Yi-6B[49]</td>
<td>0-shot</td>
<td>25.7</td>
<td>9.5</td>
<td>58.1</td>
<td>39.1</td>
<td>27.4</td>
<td>21.2</td>
<td>30.2</td>
</tr>
<tr>
<td>LLaMA2-7B[43]</td>
<td>0-shot</td>
<td>23.6</td>
<td>11.5</td>
<td>56.8</td>
<td>43.5</td>
<td>31.7</td>
<td>24.1</td>
<td>31.9</td>
</tr>
<tr>
<td>Qwen-7B[1]</td>
<td>0-shot</td>
<td>19.8</td>
<td>8.4</td>
<td>52.7</td>
<td>42.6</td>
<td>7.6</td>
<td>20.5</td>
<td>25.3</td>
</tr>
<tr>
<td>Deepseek-7B[3]</td>
<td>0-shot</td>
<td>21.6</td>
<td>8.4</td>
<td>56.3</td>
<td>38.1</td>
<td>13.4</td>
<td>20.6</td>
<td>26.4</td>
</tr>
<tr>
<td>InternLM2-7B[42]</td>
<td>0-shot</td>
<td>32.8</td>
<td>8.9</td>
<td>64.0</td>
<td>48.3</td>
<td>31.9</td>
<td>18.9</td>
<td>34.1</td>
</tr>
<tr>
<td>Qwen1.5-7B[1]</td>
<td>0-shot</td>
<td>25.0</td>
<td>11.4</td>
<td>62.3</td>
<td>49.4</td>
<td>19.4</td>
<td>19.9</td>
<td>31.2</td>
</tr>
<tr>
<td>Vicuna-v1.5-7B[8]</td>
<td>0-shot</td>
<td>29.9</td>
<td>10.3</td>
<td>58.9</td>
<td>42.5</td>
<td>32.6</td>
<td>22.0</td>
<td>32.7</td>
</tr>
<tr>
<td>Baichuan2-7B[47]</td>
<td>0-shot</td>
<td>25.7</td>
<td>10.5</td>
<td>52.7</td>
<td>44.0</td>
<td>29.2</td>
<td>20.8</td>
<td>30.5</td>
</tr>
<tr>
<td>Mistral-7B[18]</td>
<td>0-shot</td>
<td>30.0</td>
<td>13.2</td>
<td>63.4</td>
<td>48.5</td>
<td>34.3</td>
<td>22.6</td>
<td>35.3</td>
</tr>
<tr>
<td>LLaMA2-13B[43]</td>
<td>0-shot</td>
<td>24.4</td>
<td>10.1</td>
<td>59.1</td>
<td>45.0</td>
<td>33.6</td>
<td>23.8</td>
<td>32.7</td>
</tr>
<tr>
<td>Vicuna-v1.5-13B[8]</td>
<td>0-shot</td>
<td>28.3</td>
<td>11.6</td>
<td>59.5</td>
<td>45.0</td>
<td>26.3</td>
<td>19.6</td>
<td>31.7</td>
</tr>
<tr>
<td>Baichuan2-13B[47]</td>
<td>0-shot</td>
<td>22.1</td>
<td>4.7</td>
<td>51.1</td>
<td>32.8</td>
<td>25.4</td>
<td>20.3</td>
<td>26.1</td>
</tr>
<tr>
<td>InternLM2-20B[42]</td>
<td>0-shot</td>
<td>32.2</td>
<td>15.9</td>
<td>63.8</td>
<td>55.7</td>
<td>26.0</td>
<td>21.3</td>
<td>35.8</td>
</tr>
<tr>
<td>Yi-34B[49]</td>
<td>0-shot</td>
<td>37.1</td>
<td>10.5</td>
<td>53.6</td>
<td>57.3</td>
<td>37.3</td>
<td>21.7</td>
<td>36.3</td>
</tr>
<tr>
<td>Mixtral-8x7B[19]</td>
<td>0-shot</td>
<td>25.7</td>
<td>8.6</td>
<td>57.2</td>
<td>48.7</td>
<td>13.5</td>
<td>23.4</td>
<td>29.5</td>
</tr>
<tr>
<td>Deepseek-67B[3]</td>
<td>0-shot</td>
<td>30.9</td>
<td>14.8</td>
<td>64.3</td>
<td>57.5</td>
<td>17.1</td>
<td>23.2</td>
<td>34.6</td>
</tr>
<tr>
<td>LLaMA2-70B[43]</td>
<td>0-shot</td>
<td>28.9</td>
<td>12.3</td>
<td>62.2</td>
<td>48.6</td>
<td>34.3</td>
<td>25.2</td>
<td>35.3</td>
</tr>
<tr>
<td>Qwen1.5-72B[1]</td>
<td>0-shot</td>
<td>21.4</td>
<td>10.1</td>
<td>57.5</td>
<td>44.2</td>
<td>8.8</td>
<td>19.5</td>
<td>26.9</td>
</tr>
</tbody>
</table>
<p>antee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multimodal capabilities brought by multi-modal training.</p>
<h1>3 Two Overlooked Issues for Evaluating LVLMs</h1>
<p>In this section, we delve into two commonly overlooked issues in current LVLM evaluation works. Moreover, we present detailed experimental results to further substantiate our observations. First issue: visual content is unnecessary for many evaluation samples. The key distinction between evaluating LLMs and LVLMs lies in the necessity for LVLM evaluations to strictly ensure that the correct answers can only be derived based on a thorough understanding of visual content. Without this, evaluating LVLMs' multi-modal capabilities degrades to merely assessing their LLM backbones' uni-modal abilities. However, upon examining samples from some popular LVLM benchmarks, we find many samples lack vital visual dependency and can yield correct answers even without the image inputs! Through analysis of these failure samples, we categorize them into two groups: (1) Answers can be directly obtained from the world knowledge embedded in LLMs, owing to the LLMs' extensive pertaining on the large corpus of data. For example, as illustrated in Figure 1(a), the question "What is the capital of Nebraska?" already provides the key information "Nebraska", eliminating the need for extracting relevant location information from visual content. A more appropriate question is "What is the capital of the highlighted area in the image?" to emphasize</p>
<p>Table 2: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2-shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Strategy</th>
<th>MMMU</th>
<th>MMB</th>
<th>ScienceQA</th>
<th>AI2D</th>
<th>SEED</th>
<th>MathVista</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random Choice</td>
<td>-</td>
<td>22.1</td>
<td>0.0</td>
<td>24.2</td>
<td>23.8</td>
<td>24.3</td>
<td>17.9</td>
<td>18.7</td>
</tr>
<tr>
<td>Closed-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT4-Turbo[34]</td>
<td>2-shot</td>
<td>42.0</td>
<td>15.5</td>
<td>67.5</td>
<td>61.3</td>
<td>26.8</td>
<td>25.6</td>
<td>39.8</td>
</tr>
<tr>
<td>GeminiPro[41]</td>
<td>2-shot</td>
<td>42.7</td>
<td>18.7</td>
<td>69.3</td>
<td>60.1</td>
<td>38.1</td>
<td>25.5</td>
<td>42.4</td>
</tr>
<tr>
<td>Open-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen1.5-1.8B[1]</td>
<td>2-shot</td>
<td>33.0</td>
<td>8.6</td>
<td>55.6</td>
<td>41.3</td>
<td>32.1</td>
<td>22.7</td>
<td>32.2</td>
</tr>
<tr>
<td>Phi2-2.7B[32]</td>
<td>2-shot</td>
<td>19.9</td>
<td>4.3</td>
<td>50.8</td>
<td>41.7</td>
<td>6.9</td>
<td>18.4</td>
<td>23.7</td>
</tr>
<tr>
<td>Yi-6B[49]</td>
<td>2-shot</td>
<td>32.9</td>
<td>16.0</td>
<td>64.6</td>
<td>51.5</td>
<td>36.7</td>
<td>24.5</td>
<td>37.7</td>
</tr>
<tr>
<td>LLaMA2-7B[43]</td>
<td>2-shot</td>
<td>25.9</td>
<td>7.7</td>
<td>57.9</td>
<td>42.8</td>
<td>32.8</td>
<td>22.8</td>
<td>31.7</td>
</tr>
<tr>
<td>Qwen-7B[1]</td>
<td>2-shot</td>
<td>30.6</td>
<td>15.0</td>
<td>63.0</td>
<td>50.0</td>
<td>32.6</td>
<td>21.0</td>
<td>35.4</td>
</tr>
<tr>
<td>Deepseek-7B[3]</td>
<td>2-shot</td>
<td>28.7</td>
<td>11.6</td>
<td>61.9</td>
<td>46.0</td>
<td>34.1</td>
<td>21.7</td>
<td>34.0</td>
</tr>
<tr>
<td>InternLM2-7B[42]</td>
<td>2-shot</td>
<td>33.6</td>
<td>11.4</td>
<td>63.6</td>
<td>52.1</td>
<td>34.4</td>
<td>20.4</td>
<td>35.9</td>
</tr>
<tr>
<td>Qwen1.5-7B[1]</td>
<td>2-shot</td>
<td>33.3</td>
<td>13.1</td>
<td>65.1</td>
<td>52.1</td>
<td>32.1</td>
<td>22.8</td>
<td>36.4</td>
</tr>
<tr>
<td>Vicuna-v1.5-7B[8]</td>
<td>2-shot</td>
<td>31.3</td>
<td>9.5</td>
<td>58.9</td>
<td>45.5</td>
<td>32.0</td>
<td>20.7</td>
<td>33.0</td>
</tr>
<tr>
<td>Baichuan2-7B[47]</td>
<td>2-shot</td>
<td>28.2</td>
<td>13.7</td>
<td>58.1</td>
<td>44.1</td>
<td>32.3</td>
<td>21.7</td>
<td>33.0</td>
</tr>
<tr>
<td>Mistral-7B[18]</td>
<td>2-shot</td>
<td>29.8</td>
<td>17.2</td>
<td>66.1</td>
<td>50.0</td>
<td>34.4</td>
<td>13.4</td>
<td>35.2</td>
</tr>
<tr>
<td>LLaMA2-13B[43]</td>
<td>2-shot</td>
<td>32.9</td>
<td>10.1</td>
<td>58.9</td>
<td>43.8</td>
<td>32.1</td>
<td>24.8</td>
<td>33.8</td>
</tr>
<tr>
<td>Vicuna-v1.5-13B[8]</td>
<td>2-shot</td>
<td>31.3</td>
<td>12.8</td>
<td>63.0</td>
<td>46.8</td>
<td>33.6</td>
<td>20.8</td>
<td>34.7</td>
</tr>
<tr>
<td>Baichuan2-13B[47]</td>
<td>2-shot</td>
<td>32.2</td>
<td>13.1</td>
<td>61.0</td>
<td>47.1</td>
<td>35.2</td>
<td>23.4</td>
<td>35.3</td>
</tr>
<tr>
<td>InternLM2-20B[42]</td>
<td>2-shot</td>
<td>35.6</td>
<td>17.4</td>
<td>66.4</td>
<td>55.9</td>
<td>30.4</td>
<td>20.8</td>
<td>37.8</td>
</tr>
<tr>
<td>Yi-34B[49]</td>
<td>2-shot</td>
<td>35.8</td>
<td>15.8</td>
<td>67.9</td>
<td>59.6</td>
<td>37.2</td>
<td>26.9</td>
<td>40.5</td>
</tr>
<tr>
<td>Mixtral-8x7B[19]</td>
<td>2-shot</td>
<td>35.1</td>
<td>17.3</td>
<td>66.3</td>
<td>55.1</td>
<td>35.8</td>
<td>22.7</td>
<td>38.7</td>
</tr>
<tr>
<td>Deepseek-67B[3]</td>
<td>2-shot</td>
<td>38.3</td>
<td>17.2</td>
<td>68.3</td>
<td>59.7</td>
<td>37.3</td>
<td>23.4</td>
<td>40.7</td>
</tr>
<tr>
<td>LLaMA2-70B[43]</td>
<td>2-shot</td>
<td>30.4</td>
<td>17.2</td>
<td>63.4</td>
<td>49.3</td>
<td>34.9</td>
<td>24.2</td>
<td>36.6</td>
</tr>
<tr>
<td>Qwen1.5-72B[1]</td>
<td>2-shot</td>
<td>42.4</td>
<td>21.1</td>
<td>70.1</td>
<td>60.9</td>
<td>40.7</td>
<td>26.3</td>
<td>43.6</td>
</tr>
</tbody>
</table>
<p>the importance of visual understanding. (2) Answers are directly included in the textual questions. As shown in Figure 1(b), LLMs can derive the correct answer "circle" through simple reasoning based on the question "What is the shape of the round dirt circle?".</p>
<p>To quantitatively substantiate our findings, we further experiment to gauge the proportion of these two types of samples in existing benchmarks. Specifically, we evaluate several benchmarks with two closed-source LLMs (GPT4Turbo [34], and GeminiPro [41]) and six opensource heavy LLMs (InternLM2-20B [42], Yi34B [49], Mixtral-8x7B [19], Deepseek-67B [3], LLaMA2-70B [43], and Qwen1.5-72B [1]), recording the hit count for each question. Here, the 'hit' refers to the ability of an LLM to correctly answer the question without relying on visual input. We then calculate the percentage of samples with a hit count of six or more (representing $80 \%$ ) against the total number of samples to determine the abnormal hit rate for each benchmark. As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \%$ and $46.2 \%$, respectively. Based on our observations, most multi-modal benchmarks have yet to fully assess the multi-modal capabilities of LVLMs.</p>
<p>Table 3: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the "strategy" column, "LLM" refers to evaluating using the corresponding LLM base of the LVLM, while "LVLM-text" denotes evaluating LVLMs without accessing images. We employ the 0-shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs’ results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Param.</th>
<th>Strategy</th>
<th>MMMU</th>
<th>MMB</th>
<th>ScienceQA</th>
<th>AI2D</th>
<th>SEED</th>
<th>MathVista</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random Choice</td>
<td>-</td>
<td>-</td>
<td>22.1</td>
<td>0.0</td>
<td>24.2</td>
<td>23.8</td>
<td>24.3</td>
<td>17.9</td>
<td>18.7</td>
</tr>
<tr>
<td>Closed-source LVLMs and corresponding LLM bases</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT4V [35]</td>
<td>-</td>
<td>LLM</td>
<td>41.2</td>
<td>12.2</td>
<td>64.3</td>
<td>59.7</td>
<td>10.1</td>
<td>24.2</td>
<td>35.3</td>
</tr>
<tr>
<td>(GPT4-Turbo [34])</td>
<td>-</td>
<td>LVLM-text</td>
<td>45.1</td>
<td>17.6</td>
<td>68.2</td>
<td>62.5</td>
<td>28.4</td>
<td>25.4</td>
<td>41.2</td>
</tr>
<tr>
<td></td>
<td>-</td>
<td>LVLM</td>
<td>53.6</td>
<td>69.6</td>
<td>81.4</td>
<td>75.3</td>
<td>71.6</td>
<td>44.7</td>
<td>66.0</td>
</tr>
<tr>
<td>GeminiPro-Vision [41]</td>
<td>-</td>
<td>LLM</td>
<td>42.9</td>
<td>18.4</td>
<td>68.9</td>
<td>59.2</td>
<td>35.5</td>
<td>23.3</td>
<td>41.4</td>
</tr>
<tr>
<td>(GeminiPro) [41]</td>
<td>-</td>
<td>LVLM-text</td>
<td>39.4</td>
<td>16.7</td>
<td>66.3</td>
<td>54.5</td>
<td>27.9</td>
<td>24.5</td>
<td>38.2</td>
</tr>
<tr>
<td></td>
<td>-</td>
<td>LVLM</td>
<td>44.4</td>
<td>68.1</td>
<td>80.6</td>
<td>68.0</td>
<td>64.3</td>
<td>36.0</td>
<td>60.2</td>
</tr>
<tr>
<td>Open-source LVLMs and corresponding LLM bases</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TinyLLaVA [53]</td>
<td>3B</td>
<td>LLM</td>
<td>20.0</td>
<td>7.2</td>
<td>47.1</td>
<td>38.7</td>
<td>26.4</td>
<td>22.0</td>
<td>26.9</td>
</tr>
<tr>
<td>(Phi2-2.7B [32])</td>
<td></td>
<td>LVLM-text</td>
<td>30.0</td>
<td>21.0</td>
<td>62.3</td>
<td>51.9</td>
<td>37.2</td>
<td>23.5</td>
<td>37.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>36.0</td>
<td>66.9</td>
<td>69.1</td>
<td>62.4</td>
<td>70.1</td>
<td>28.9</td>
<td>55.6</td>
</tr>
<tr>
<td>LLaVA-1.5 [24]</td>
<td>7B</td>
<td>LLM</td>
<td>29.9</td>
<td>10.3</td>
<td>58.9</td>
<td>42.5</td>
<td>32.6</td>
<td>22.0</td>
<td>32.7</td>
</tr>
<tr>
<td>(Vicuna-v1.5-7B [8])</td>
<td></td>
<td>LVLM-text</td>
<td>29.9</td>
<td>19.5</td>
<td>64.1</td>
<td>48.7</td>
<td>37.5</td>
<td>20.3</td>
<td>36.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>34.4</td>
<td>65.0</td>
<td>68.7</td>
<td>55.6</td>
<td>65.6</td>
<td>23.6</td>
<td>52.2</td>
</tr>
<tr>
<td>InternLM2-XC2 [12]</td>
<td>7B</td>
<td>LLM</td>
<td>32.8</td>
<td>8.9</td>
<td>64.0</td>
<td>48.3</td>
<td>31.9</td>
<td>18.9</td>
<td>34.1</td>
</tr>
<tr>
<td>(InternLM2-7B [42])</td>
<td></td>
<td>LVLM-text</td>
<td>34.2</td>
<td>26.2</td>
<td>71.9</td>
<td>63.3</td>
<td>38.1</td>
<td>29.4</td>
<td>43.9</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>41.7</td>
<td>79.6</td>
<td>96.7</td>
<td>81.4</td>
<td>74.9</td>
<td>57.4</td>
<td>72.0</td>
</tr>
<tr>
<td>Monkey-Chat [23]</td>
<td>10B</td>
<td>LLM</td>
<td>19.8</td>
<td>8.4</td>
<td>52.7</td>
<td>42.6</td>
<td>7.6</td>
<td>20.5</td>
<td>25.3</td>
</tr>
<tr>
<td>(Qwen-7B [1])</td>
<td></td>
<td>LVLM-text</td>
<td>32.4</td>
<td>15.6</td>
<td>71.1</td>
<td>56.8</td>
<td>36.1</td>
<td>25.0</td>
<td>39.5</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>37.1</td>
<td>71.0</td>
<td>82.4</td>
<td>68.5</td>
<td>69.1</td>
<td>34.0</td>
<td>60.4</td>
</tr>
<tr>
<td>CogVLM-Chat [45]</td>
<td>17B</td>
<td>LLM</td>
<td>29.9</td>
<td>10.3</td>
<td>58.9</td>
<td>42.5</td>
<td>32.6</td>
<td>22.0</td>
<td>32.7</td>
</tr>
<tr>
<td>(Vicuna-v1.5-7B [8])</td>
<td></td>
<td>LVLM-text</td>
<td>30.1</td>
<td>15.5</td>
<td>54.6</td>
<td>52.5</td>
<td>36.7</td>
<td>25.0</td>
<td>35.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>34.2</td>
<td>63.4</td>
<td>66.3</td>
<td>63.3</td>
<td>68.7</td>
<td>34.7</td>
<td>55.1</td>
</tr>
<tr>
<td>Yi-VL [49]</td>
<td>34B</td>
<td>LLM</td>
<td>37.1</td>
<td>10.5</td>
<td>53.6</td>
<td>57.3</td>
<td>37.3</td>
<td>21.7</td>
<td>36.3</td>
</tr>
<tr>
<td>(Yi-34B [49])</td>
<td></td>
<td>LVLM-text</td>
<td>37.3</td>
<td>23.2</td>
<td>68.6</td>
<td>59.9</td>
<td>41.0</td>
<td>22.7</td>
<td>42.1</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>43.2</td>
<td>71.5</td>
<td>75.3</td>
<td>65.9</td>
<td>68.1</td>
<td>25.6</td>
<td>58.3</td>
</tr>
<tr>
<td>InternVL-Chat-v1.2 [6]</td>
<td>40B</td>
<td>LLM</td>
<td>37.6</td>
<td>20.1</td>
<td>69.4</td>
<td>60.2</td>
<td>35.0</td>
<td>17.9</td>
<td>40.0</td>
</tr>
<tr>
<td>(NH2-Yi-34B [33])</td>
<td></td>
<td>LVLM-text</td>
<td>41.7</td>
<td>23.9</td>
<td>70.3</td>
<td>65.0</td>
<td>40.5</td>
<td>24.0</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>49.1</td>
<td>82.4</td>
<td>82.5</td>
<td>78.5</td>
<td>75.4</td>
<td>47.7</td>
<td>69.3</td>
</tr>
<tr>
<td>Sphinx-X-MoE [15]</td>
<td>57B</td>
<td>LLM</td>
<td>25.7</td>
<td>8.6</td>
<td>57.2</td>
<td>48.7</td>
<td>13.5</td>
<td>23.4</td>
<td>29.5</td>
</tr>
<tr>
<td>(Mixtral-8x7B [19])</td>
<td></td>
<td>LVLM-text</td>
<td>43.6</td>
<td>20.5</td>
<td>68.4</td>
<td>61.1</td>
<td>39.9</td>
<td>28.4</td>
<td>43.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LVLM</td>
<td>44.8</td>
<td>69.2</td>
<td>72.2</td>
<td>65.0</td>
<td>71.1</td>
<td>38.1</td>
<td>60.1</td>
</tr>
</tbody>
</table>
<p>Second issue: unintentional data leaking exists in LLM and LVLM training. Although the community has the trend towards developing new multi-modal benchmarks to assess LVLMs’ capabilities from various dimensions, there is scant consideration for fairness and reliability during evaluation. Training LLMs and LVLMs requires vast and diverse data, inevitably leading to the leakage of evaluation samples. Such incidents are usually unintended, as it’s impractical to predict which data will be used in future evaluation benchmarks during the preparation for training corpus.</p>
<p>Figure 1 (c) showcases an evaluation sample leaked by LLMs. Though the question requires an understanding of image content, 16 out of 22 tested LLMs astonishingly provide the correct response by "recalling" their training data. To quantitatively support our observations, we evaluate 22 leading LLMs across 6 popular benchmarks and report the 0-shot results in Table 1 and 2-shot results in Table 2. Specifically, we find the 2-shot evaluation strategy is more stable than the 0-shot to reduce refusal for answering and align answer formats. Under the impact of vision-independent samples and data leakage from LLMs, GeminiPro [41] and Qwen1.5-72B [1] achieve a remarkable average accuracy of 41.4% and 43.6% under the 2-shot setting, outperforming random choice by 20.4% and 22.6%, respectively. Furthermore, Qwen1.5-72B achieves a score of 42.4% on MMMU [51], even surpassing the performance of the majority of LVLMs with accessing images. This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLMLVLM ${ }^{t \times \neq 1}$ pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLMLVLM ${ }^{t \times \neq 1}$ pairs, underscoring the issue of data leakage during the multi-modal training process.</p>
<p>In Figure 1 (d) and Figure 3, we showcase some examples where original LLMs fail, but LVLMs without accessing images succeed. Despite these questions requiring image content for accurate answers, the LVLMs without accessing images are capable of correctly answering these questions which stump original LLMs. To further support our hypotheses of data leakage during LVLMs' multi-modal training, we conduct an intriguing experiment. We remove the image inputs for LVLMs and only utilize textual questions and options for evaluation, with the results reported in Table 3. We compare the performance gains of LVLMs set to receive only text inputs (LVLM-text) against their corresponding LLM bases (LLM) to quantitatively assess the degree of data leakage in LVLMs' multi-modal training. As shown in Table 3, most LVLMs exhibit varying degrees of data leakage during multi-modal training. For example, the LLMs of Sphinx-X-8x7B [15] and Monkey-Chat [23], show a respective average performance gain of $14.1 \%$ and $14.2 \%$ compared to their original LLMs.</p>
<p>Drawing from our observations, we posit that the issue of data leakage in multi-modal datasets is a significant concern that warrants attention. For research to be transparent and equitable, it is imperative to account for and mitigate such leakage. This will ensure that the performance of models is evaluated on their true ability to integrate and interpret multi-modal data, rather than their capacity to memorize specific samples. A proper benchmark would be a crucial step toward advancing the field of multi-modal language model research.</p>
<h1>4 MMStar</h1>
<p>With the aforementioned analysis, we present an elite vision-dependent multi-modal benchmark, dubbed as MMStar. In Section 4.1, we elaborate on the data curation process of MMStar. Section</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.</p>
<p>4.2 provides a detailed analysis of the constructed MMStar benchmark. In Section 4.3, we introduce two benchmark-specific metrics developed to evaluate the degree of data leakage as well as the actual performance gains in multimodal capabilities from the multi-modal training.</p>
<h3>4.1 Data Curation Process</h3>
<p><strong>Criteria for data curation.</strong> The evaluation samples for constructing the MMStar benchmark should meet three fundamental criteria: 1) <strong>Visual dependency.</strong> The collected samples can be correctly answered only based on understanding the visual content; 2) <strong>Minimal data leakage.</strong> The collected samples should minimize the risk of unintentional inclusion in LLMs' training corpus, or be effectively transformed from uni-modal to multi-modal formats to prevent LLMs from "recalling" the correct answers; 3) <strong>Requiring advanced multi-modal capabilities for resolution.</strong> In addition to ensuring fairness and reliability by adhering to the above criteria, we also aim for samples to cover various difficulty levels. We expect to comprehensively capture LVLMs' multi-modal capabilities with succinct high-quality samples.</p>
<p><strong>Data filter.</strong> We first choose two benchmarks [27, 21] focused on natural images and four centered on scientific and technical knowledge [51, 30, 20, 29] for our sample collection. We then develop an automated pipeline to preliminarily filter out samples that do not meet the first two criteria. Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors. These open-source LLMs are applied with a 2-shot in-context inference strategy to minimize response refusals and ensure consistency in answer formatting. Following this, we evaluate the sample pool with these LLM inspectors, documenting the hit frequency for each evaluation sample. Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around 75% of LLM inspectors fail to provide the correct answer. As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607.</p>
<p><strong>Manual review.</strong> After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution. To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough (0-3). Finally, after considering both the diversity of capability dimensions and difficulty levels, we manually curated <strong>1,500</strong> high-quality samples from the coarsely filtered set. Figure 4 showcases the detailed composition of data sources for our final selection of samples.</p>
<h3>4.2 Core Capabilities</h3>
<p>We select and consolidate the dimensions used for assessing LVLMs' multi-modal capabilities in existing benchmarks and identify six core capability dimensions along with eighteen detailed axes.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Distribution of capability dimensions on the MMStar benchmark. In MMStar, we display 6 core capabilities in the inner ring, with 18 detailed axes presented in the outer ring. The middle ring showcases the number of samples for each detailed dimension. Each core capability contains a meticulously balanced 250 samples. We further ensure a relatively even distribution across the 18 detailed axes.</p>
<p>In Figure 5, we provide statistics for each core capability and their detailed axes on the MMStar benchmark.</p>
<p>Coarse Perception (CP). This core dimension refers to the capability to understand and interpret the overarching characteristics and themes of an image without delving into the finer details. It encompasses a broad, holistic view of the visual content, enabling the identification of: 1) image style \&amp; quality; 2) image scene \&amp; topic; and 3) image emotion.</p>
<p>Fine-grained Perception (FP). This core dimension represents a sophisticated level of image understanding that focuses on the detailed and nuanced aspects of visual content. It involves a deep dive into the specifics of images: 1) attribute \&amp; celebrity recognition; 2) object location; and 3) object counting. This core dimension unveils the subtle intricacies that coarse perception might overlook.</p>
<p>Instance Reasoning (IR). It encapsulates a set of advanced cognitive capabilities focused on understanding and interpreting individual and collective object attributes and interrelations within an image. This process goes beyond mere recognition, delving into the analytical assessment of: 1) single-instance attribute reasoning; 2) cross-instance attribute comparison; and 3) cross-instance relation reasoning. It is a critical component for systems requiring a deep semantic understanding of visual content, enabling nuanced interaction with and response to complex visual content.</p>
<p>Logical Reasoning (LR). This core dimension encompasses a sophisticated framework of cognitive processes designed to interpret, deduce, and infer conclusions from visual content through a structured approach to logic and reasoning. This multi-faceted capability marries the intuitive understanding of visual content with the structured rigor of logical deduction, enabling: 1) diagram reasoning; 2) code \&amp; sequence reasoning; and 3) common reasoning.</p>
<p>Science \&amp; Technology (ST). It consists of a comprehensive framework for the application and integration of knowledge across a broad spectrum of science and technology. This domain combines the theoretical underpinnings and practical applications of various fields: 1) natural science; 2) engineering; and 3) geography \&amp; earth science.</p>
<p>Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \&amp; calculation; 2) geometry; and 3) statistical analysis.</p>
<h1>4.3 Multi-modal Gain/Leakage</h1>
<p>Given our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.</p>
<p>To calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:</p>
<p>$$
M G=S_{v}-S_{w v}
$$</p>
<p>To calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:</p>
<p>$$
M L=\max \left(0, S_{w v}-S_{t}\right)
$$</p>
<h2>5 Experiments</h2>
<p>In this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.</p>
<h3>5.1 Experimental Setups</h3>
<p>Evaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].</p>
<p>Implementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2-shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.</p>
<p>Table 4: LLMs failed to solve problems in MMStar and performed close to random guessing, visual content is necessary to solve MMStar. We evaluate various LLMs on MMStar with the 2-shot inference strategy. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \&amp; technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>CP</th>
<th>FP</th>
<th>IR</th>
<th>LR</th>
<th>ST</th>
<th>MA</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random Choice</td>
<td>23.7</td>
<td>24.5</td>
<td>25.3</td>
<td>24.3</td>
<td>24.8</td>
<td>25.1</td>
<td>24.6</td>
</tr>
<tr>
<td>Closed-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT4-Turbo[34]</td>
<td>2.4</td>
<td>4.0</td>
<td>9.6</td>
<td>18.0</td>
<td>13.6</td>
<td>25.6</td>
<td>12.2</td>
</tr>
<tr>
<td>Gemini-Pro[41]</td>
<td>$\underline{\mathbf{1 6 . 8}}$</td>
<td>$\underline{\mathbf{1 3 . 6}}$</td>
<td>$\underline{\mathbf{2 0 . 4}}$</td>
<td>$\underline{\mathbf{2 4 . 4}}$</td>
<td>$\underline{\mathbf{1 9 . 6}}$</td>
<td>$\underline{\mathbf{2 8 . 8}}$</td>
<td>$\underline{\mathbf{2 0 . 6}}$</td>
</tr>
<tr>
<td>Open-source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen1.5-1.8B[1]</td>
<td>28.4</td>
<td>28.4</td>
<td>25.6</td>
<td>23.2</td>
<td>$\underline{\mathbf{2 3 . 2}}$</td>
<td>29.6</td>
<td>$\underline{\mathbf{2 6 . 4}}$</td>
</tr>
<tr>
<td>Phi2-2.7B[32]</td>
<td>11.2</td>
<td>11.2</td>
<td>15.2</td>
<td>10.8</td>
<td>11.6</td>
<td>12.0</td>
<td>12.0</td>
</tr>
<tr>
<td>Yi-6B-Chat[49]</td>
<td>23.6</td>
<td>19.2</td>
<td>28.4</td>
<td>25.2</td>
<td>12.4</td>
<td>29.6</td>
<td>23.1</td>
</tr>
<tr>
<td>LLaMA2-7B[43]</td>
<td>28.0</td>
<td>$\underline{\mathbf{3 0 . 4}}$</td>
<td>26.0</td>
<td>18.0</td>
<td>18.8</td>
<td>21.6</td>
<td>23.8</td>
</tr>
<tr>
<td>Qwen-7B[1]</td>
<td>11.6</td>
<td>5.6</td>
<td>12.8</td>
<td>5.6</td>
<td>7.2</td>
<td>0.4</td>
<td>7.2</td>
</tr>
<tr>
<td>Deepseek-7B[3]</td>
<td>26.8</td>
<td>16.0</td>
<td>28.4</td>
<td>21.6</td>
<td>$\underline{\mathbf{2 3 . 2}}$</td>
<td>25.6</td>
<td>23.6</td>
</tr>
<tr>
<td>InternLM2-7B[42]</td>
<td>22.0</td>
<td>14.8</td>
<td>22.0</td>
<td>21.6</td>
<td>15.2</td>
<td>23.2</td>
<td>19.8</td>
</tr>
<tr>
<td>Qwen1.5-7B[1]</td>
<td>15.6</td>
<td>8.0</td>
<td>9.2</td>
<td>9.2</td>
<td>15.2</td>
<td>9.2</td>
<td>11.1</td>
</tr>
<tr>
<td>Vicuna-v1.5-7B[8]</td>
<td>22.0</td>
<td>27.6</td>
<td>29.6</td>
<td>26.4</td>
<td>18.0</td>
<td>24.4</td>
<td>24.7</td>
</tr>
<tr>
<td>Baichuan2-7B[47]</td>
<td>20.8</td>
<td>18.4</td>
<td>27.6</td>
<td>18.8</td>
<td>18.8</td>
<td>21.2</td>
<td>20.9</td>
</tr>
<tr>
<td>Mistral-7B[18]</td>
<td>20.0</td>
<td>23.6</td>
<td>24.4</td>
<td>23.6</td>
<td>20.0</td>
<td>27.2</td>
<td>23.1</td>
</tr>
<tr>
<td>LLaMA2-13B[43]</td>
<td>23.6</td>
<td>23.6</td>
<td>28.0</td>
<td>21.2</td>
<td>16.4</td>
<td>10.4</td>
<td>20.5</td>
</tr>
<tr>
<td>Vicuna-v1.5-13B[8]</td>
<td>$\underline{\mathbf{3 2 . 8}}$</td>
<td>24.0</td>
<td>$\underline{\mathbf{2 8 . 8}}$</td>
<td>17.6</td>
<td>22.0</td>
<td>14.4</td>
<td>23.3</td>
</tr>
<tr>
<td>Baichuan2-13B[47]</td>
<td>26.4</td>
<td>18.0</td>
<td>28.0</td>
<td>20.4</td>
<td>21.2</td>
<td>25.6</td>
<td>23.3</td>
</tr>
<tr>
<td>InternLM2-20B[42]</td>
<td>18.2</td>
<td>17.8</td>
<td>22.6</td>
<td>23.8</td>
<td>17.8</td>
<td>13.4</td>
<td>18.9</td>
</tr>
<tr>
<td>Yi-34B[49]</td>
<td>20.4</td>
<td>18.0</td>
<td>24.0</td>
<td>24.0</td>
<td>14.4</td>
<td>$\underline{\mathbf{3 0 . 8}}$</td>
<td>21.9</td>
</tr>
<tr>
<td>Mixtral-8x7B[19]</td>
<td>24.4</td>
<td>17.6</td>
<td>19.2</td>
<td>$\underline{\mathbf{2 8 . 0}}$</td>
<td>16.0</td>
<td>33.6</td>
<td>23.1</td>
</tr>
<tr>
<td>Deepseek-67B[3]</td>
<td>29.2</td>
<td>22.4</td>
<td>18.4</td>
<td>26.0</td>
<td>20.4</td>
<td>22.4</td>
<td>23.1</td>
</tr>
<tr>
<td>LLaMA2-70B[43]</td>
<td>22.4</td>
<td>20.0</td>
<td>19.6</td>
<td>14.4</td>
<td>7.2</td>
<td>9.6</td>
<td>15.5</td>
</tr>
<tr>
<td>Qwen1.5-72B[1]</td>
<td>21.6</td>
<td>16.0</td>
<td>21.2</td>
<td>14.0</td>
<td>17.2</td>
<td>27.2</td>
<td>19.5</td>
</tr>
</tbody>
</table>
<h1>5.2 Results Analysis of MMStar</h1>
<p>In this section, we present a comprehensive comparison of various LLMs and LVLMs performed on our MMStar benchmark and summarize our key observations in the following parts.</p>
<p>Observation from LLMs. We comprehensively evaluate 2 closed-source LLMs and 20 open-source LLMs of varying sizes and architectures on the MMStar benchmark and report the results in Table 4. Encouragingly, the performance of these LLMs is almost indistinguishable from random choice, effectively validating that the evaluation samples of our MMStar exhibit significant visual dependency and minimal data leakage from LLMs. Notably, the smallest model, Qwen1.5-1.8B, achieves the best score. We conjecture this is due to it suffering the least stringent safety restrictions, thereby reducing instances of refusal to answer. Moreover, among the six core capabilities of MMStar, science \&amp; technology (ST) prove to be the most challenging dimension for LLMs. The best score on ST is only $23.2 \%$, significantly lower than the best scores of around $30 \%$ in other dimensions. We speculate this may be that samples within the ST dimension have the least degree of data leakage from LLMs' training data.</p>
<p>Observation from LVLMs. We evaluate 2 closed-source and 14 open-source LVLMs on our MMStar, with the results reported in Table 5. As shown in the table, GPT4V[35] with a high-resolution</p>
<p>Table 5: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \&amp; technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>LLM</th>
<th>Param.</th>
<th>CP</th>
<th>FP</th>
<th>IR</th>
<th>LR</th>
<th>ST</th>
<th>MA</th>
<th>Avg.</th>
<th>MG $\uparrow$</th>
<th>ML $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random Choice</td>
<td>-</td>
<td>-</td>
<td>23.7</td>
<td>24.5</td>
<td>25.3</td>
<td>24.3</td>
<td>24.8</td>
<td>25.1</td>
<td>24.6</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Closed-source LVLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GeminiPro-Vision[41]</td>
<td>GeminiPro[41]</td>
<td>-</td>
<td>51.6</td>
<td>28.8</td>
<td>50.8</td>
<td>46.0</td>
<td>28.4</td>
<td>50.0</td>
<td>42.6</td>
<td>27.4</td>
<td>0.0</td>
</tr>
<tr>
<td>GPT4V (low)[35]</td>
<td>GPT4-Turbo[34]</td>
<td>-</td>
<td>62.0</td>
<td>32.8</td>
<td>55.2</td>
<td>48.0</td>
<td>33.6</td>
<td>44.8</td>
<td>46.1</td>
<td>32.6</td>
<td>1.3</td>
</tr>
<tr>
<td>GPT4V (high)[35]</td>
<td>GPT4-Turbo[34]</td>
<td>-</td>
<td>76.6</td>
<td>51.4</td>
<td>66.6</td>
<td>55.8</td>
<td>42.6</td>
<td>49.8</td>
<td>57.1</td>
<td>43.6</td>
<td>1.3</td>
</tr>
<tr>
<td>Open-source LVLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TinyLLaVA[53]</td>
<td>Phi2-2.7B[32]</td>
<td>3B</td>
<td>60.4</td>
<td>31.6</td>
<td>50.8</td>
<td>30.4</td>
<td>18.0</td>
<td>24.8</td>
<td>36.0</td>
<td>16.4</td>
<td>7.6</td>
</tr>
<tr>
<td>Yi-VL[49]</td>
<td>Yi-6B[49]</td>
<td>6B</td>
<td>58.0</td>
<td>33.6</td>
<td>46.4</td>
<td>34.8</td>
<td>20.4</td>
<td>34.0</td>
<td>37.9</td>
<td>15.6</td>
<td>0.0</td>
</tr>
<tr>
<td>LLaVA-1.5[24]</td>
<td>Vicuna-v1.5-7B[8]</td>
<td>7B</td>
<td>58.8</td>
<td>24.0</td>
<td>38.8</td>
<td>24.0</td>
<td>13.6</td>
<td>22.8</td>
<td>30.3</td>
<td>10.7</td>
<td>0.0</td>
</tr>
<tr>
<td>ShareGPT4V[5]</td>
<td>Vicuna-v1.5-7B[8]</td>
<td>7B</td>
<td>58.8</td>
<td>28.0</td>
<td>45.6</td>
<td>24.4</td>
<td>17.2</td>
<td>24.0</td>
<td>33.0</td>
<td>11.9</td>
<td>0.0</td>
</tr>
<tr>
<td>InternLM-XC2[12]</td>
<td>InternLM2-7B[42]</td>
<td>7B</td>
<td>70.8</td>
<td>48.8</td>
<td>65.2</td>
<td>56.4</td>
<td>42.0</td>
<td>49.2</td>
<td>55.4</td>
<td>28.1</td>
<td>7.5</td>
</tr>
<tr>
<td>Qwen-VL-Chat[2]</td>
<td>Qwen-7B[1]</td>
<td>8B</td>
<td>59.6</td>
<td>32.0</td>
<td>50.8</td>
<td>29.2</td>
<td>22.0</td>
<td>31.6</td>
<td>37.5</td>
<td>23.9</td>
<td>0.0</td>
</tr>
<tr>
<td>Deepseek-VL[28]</td>
<td>Deepseek-7B[3]</td>
<td>8B</td>
<td>64.0</td>
<td>30.8</td>
<td>49.2</td>
<td>36.4</td>
<td>21.6</td>
<td>20.4</td>
<td>37.1</td>
<td>15.7</td>
<td>0.0</td>
</tr>
<tr>
<td>Monkey-Chat[23]</td>
<td>Qwen-7B[1]</td>
<td>10B</td>
<td>57.6</td>
<td>36.4</td>
<td>51.6</td>
<td>33.2</td>
<td>26.4</td>
<td>24.4</td>
<td>38.3</td>
<td>13.5</td>
<td>17.6</td>
</tr>
<tr>
<td>LLaVA-1.5[24]</td>
<td>Vicuna-v1.5-13B[8]</td>
<td>13B</td>
<td>58.8</td>
<td>28.0</td>
<td>41.6</td>
<td>24.4</td>
<td>18.4</td>
<td>25.6</td>
<td>32.8</td>
<td>13.9</td>
<td>0.0</td>
</tr>
<tr>
<td>CogVLM-Chat[45]</td>
<td>Vicuna-v1.5-7B[8]</td>
<td>17B</td>
<td>66.8</td>
<td>36.8</td>
<td>49.2</td>
<td>31.2</td>
<td>23.6</td>
<td>11.6</td>
<td>36.5</td>
<td>14.9</td>
<td>0.0</td>
</tr>
<tr>
<td>Yi-VL[49]</td>
<td>Yi-34B[49]</td>
<td>34B</td>
<td>53.2</td>
<td>31.2</td>
<td>52.0</td>
<td>32.4</td>
<td>12.4</td>
<td>35.2</td>
<td>36.1</td>
<td>18.8</td>
<td>0.0</td>
</tr>
<tr>
<td>LLaVA-Next[25]</td>
<td>NH2-Yi-34B[33]</td>
<td>34B</td>
<td>66.4</td>
<td>52.0</td>
<td>62.4</td>
<td>46.0</td>
<td>32.4</td>
<td>53.6</td>
<td>52.1</td>
<td>29.4</td>
<td>2.4</td>
</tr>
<tr>
<td>InternVL-Chat-V1.2[6]</td>
<td>NH2-Yi-34B[33]</td>
<td>40B</td>
<td>67.6</td>
<td>43.2</td>
<td>61.2</td>
<td>47.2</td>
<td>24.0</td>
<td>19.2</td>
<td>43.7</td>
<td>32.6</td>
<td>0.0</td>
</tr>
<tr>
<td>Sphinx-X-MOE[15]</td>
<td>Mixtral-8x7B[19]</td>
<td>57B</td>
<td>58.4</td>
<td>40.8</td>
<td>47.6</td>
<td>35.2</td>
<td>19.2</td>
<td>32.0</td>
<td>38.9</td>
<td>14.8</td>
<td>1.0</td>
</tr>
</tbody>
</table>
<p>setting can achieve the best average score of $57.1 \%$ among all LVLMs. Increasing the resolution and number of image tokens can boost the average score from $46.1 \%$ to $57.1 \%$ for GPT4V, offering a positive signal to the research community. Among the open-source LVLMs, InternLMXcomposer2 [12] achieves an impressive score of 55.4\%. LLaVA-Next [25] even surpasses GPT4V and GeminiPro-Vision [41] in the mathematics (MA) core capability. Notably, no LVLMs managed to reach a passing average score (i.e. $60 \%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \&amp; Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs. Moreover, TinyLLaVA [53], despite its modest 3B scale, outperformed some competitors of 7B and even 13B surprisingly, underscoring the potential of smaller-scale LVLMs. Additionally, even with the same architecture, ShareGPT4V7B [5] even outperforms LLaVA-1.5-13B with high-quality caption data. This result highlights the significance of high-quality caption data for LVLM performance to the community.</p>
<h1>5.3 Results Analysis of MG/ML</h1>
<p>In this section, we present the results of our proposed multi-modal gain (MG) and multi-modal leakage (ML) metrics of 16 LVLMs with varying sizes and architectures on 6 popular benchmarks and our MMStar benchmark. We then detail our observations and analyses from both the model and benchmark perspectives. Analysis from the model perspective. In Table 6, we illustrate the MG/ML (Multi-modal Gain/Multi-modal Leakage) metrics for each LVLM across each benchmark and provide an average MG/ML metric across all benchmarks in the final column. For closed-source LVLMs, GPT4V demonstrates notable performance gains attributed to its multi-modal training, while GeminiProVision shows lesser data leakage during multi-modal training. This suggests that GPT4V may have utilized a broader range of multi-modal training data compared to GeminiPro-Vision. Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks, whereas LLaVA-1.5-7B records the lowest at 14.8. This outcome is reason-</p>
<p>Table 6: Evaluation of various LVLMs on 7 Benchmarks with multi-modal gain (MG) and multi-modal leakage (ML) metrics. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. The bottom row represents the average across models for the same benchmark, while the rightmost column shows the average across benchmarks for the same LVLM. The best results are highlighted in bold and underlined. The worst results of MG and ML metrics are in italic red.</p>
<p>| Model | Param. | MMMU | | MMB | | ScienceQA | | AI2D | | SEED | | MathVista | | MMStar | | Avg. | |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  |  | $\mathrm{MG} \uparrow \mathrm{ML}<em _downarrow="\downarrow">{\downarrow}$ |  | $\mathrm{MG} \uparrow \mathrm{ML}</em>}$ |  | $\mathrm{MG} \uparrow \mathrm{ML<em _downarrow="\downarrow">{\downarrow}$ |  | $\mathrm{MG} \uparrow \mathrm{ML}</em>}$ |  | $\mathrm{MG} \uparrow \mathrm{ML<em _downarrow="\downarrow">{\downarrow}$ |  | $\mathrm{MG} \uparrow \mathrm{ML}</em>}$ |  | $\mathrm{MG} \uparrow \mathrm{ML<em _downarrow="\downarrow">{\downarrow}$ |  | $\mathrm{MG} \uparrow \mathrm{ML}</em>$ |
| Closed-source LVLMs |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| GPT4V[35] | - | 8.5 | 3.9 | 52.0 | 5.4 | 13.2 | 3.9 | 12.8 | 2.8 | 43.2 | 18.3 | 19.3 | 1.2 | 32.6 | 1.3 | 25.9 | 5.3 |
| GeminiPro-Vision[41] | - | 5.0 | 0.0 | 51.4 | 0.0 | 14.3 | 0.0 | 13.5 | 0.0 | 36.4 | 0.0 | 11.5 | 1.2 | 27.4 | 0.0 | 22.8 | 0.2 |
| Open-source LVLMs |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| TinyLLaVA[53] | 3B | 6.0 | 10.0 | 45.9 | 13.8 | 6.8 | 15.2 | 10.5 | 13.2 | 32.9 | 10.8 | 5.4 | 1.5 | 16.4 | 7.6 | 17.7 | 10.3 |
| Yi-VL[49] | 6B | 5.3 | 7.4 | 45.6 | 14.1 | 5.1 | 9.4 | 3.9 | 16.6 | 29.2 | 10.9 | 3.8 | 3.0 | 15.6 | 0.0 | 15.5 | 8.8 |
| LLaVA-1.5[24] | 7B | 4.5 | 0.0 | 45.5 | 9.2 | 4.6 | 5.2 | 6.9 | 6.2 | 28.1 | 4.9 | 3.3 | 0.0 | 10.7 | 0.0 | 14.8 | 3.6 |
| ShareGPT4V[5] | 7B | 3.5 | 1.8 | 49.1 | 10.1 | 4.2 | 6.3 | 8.5 | 6.9 | 31.7 | 5.1 | 3.0 | 0.7 | 11.9 | 0.0 | 16.0 | 4.4 |
| InternLM-XC2[12] | 7B | 7.5 | 1.4 | 53.4 | 17.3 | 24.8 | 7.9 | 18.1 | 15.0 | 36.8 | 6.2 | 28.0 | 10.5 | 28.1 | 7.5 | 28.1 | 9.4 |
| Qwen-VL-Chat[2] | 8B | 10.0 | 4.2 | 49.6 | 0.3 | 11.0 | 4.0 | 12.3 | 6.4 | 44.5 | 11.9 | 11.4 | 0.3 | 23.9 | 0.0 | 23.2 | 3.9 |
| Deepseek-VL[28] | 8B | 3.2 | 10.6 | 49.6 | 15.5 | 14.3 | 10.8 | 11.6 | 14.9 | 33.7 | 23.1 | 11.4 | 3.3 | 15.7 | 0.0 | 19.9 | 11.2 |
| Monkey-Chat[23] | 10B | 4.7 | 12.6 | 55.4 | 7.2 | 11.3 | 18.4 | 11.7 | 14.2 | 33.0 | 28.5 | 9.0 | 4.5 | 13.5 | 11.1 | 19.8 | 13.8 |
| LLaVA-1.5[24] | 13B | 9.6 | 0.0 | 47.2 | 9.8 | 5.7 | 7.0 | 8.6 | 7.2 | 31.1 | 10.7 | 5.3 | 1.5 | 13.9 | 0.0 | 17.3 | 5.2 |
| CogVLM-Chat[45] | 17B | 4.1 | 0.2 | 47.9 | 5.2 | 11.7 | 0.0 | 10.8 | 10.0 | 32.0 | 4.1 | 9.7 | 3.0 | 14.9 | 0.0 | 18.7 | 3.2 |
| Yi-VL[49] | 34B | 5.9 | 0.2 | 48.3 | 12.7 | 6.7 | 15.0 | 6.0 | 2.6 | 27.1 | 3.7 | 2.9 | 1.0 | 18.8 | 0.0 | 16.5 | 5.0 |
| LLaVA-Next[25] | 34B | 6.6 | 2.8 | 54.7 | 4.8 | 11.2 | 1.5 | 12.8 | 5.6 | 34.1 | 6.7 | 16.5 | 4.3 | 29.4 | 2.4 | 23.6 | 4.0 |
| InternVL-Chat-v1.2[6] | 40B | 7.4 | 4.1 | 58.5 | 3.8 | 12.2 | 0.9 | 13.5 | 4.8 | 34.9 | 5.5 | 23.7 | 6.1 | 32.6 | 0.0 | 26.1 | 3.6 |
| Sphinx-X-MoE[15] | 57B | 1.2 | 17.9 | 48.7 | 11.9 | 3.8 | 11.2 | 3.9 | 12.4 | 31.2 | 26.4 | 9.7 | 5.0 | 14.8 | 1.0 | 16.2 | 12.3 |
| Avg. across models | - | 5.8 | 4.9 | 50.1 | 8.9 | 10.0 | 7.4 | 10.3 | 8.7 | 33.7 | 11.1 | 10.8 | 3.0 | 20.0 | 1.9 | - | - |</p>
<p>able given that LLaVA-1.5-7B employed the least amount of multi-modal training data among these open-source LVLMs. Despite LLaVA-1.5-7B having the lowest average multi-modal gain, it exhibits minimal multi-modal leakage. Additionally, models like Monkey-Chat, Spinx-X-MoE, and Deepspeek-VL display higher degrees of multi-modal leakage, highlighting the need for the community to consider this factor for fair comparisons.</p>
<p>Analysis from the benchmark perspective. In the final row of Table 6, we list the average multimodal gain and multi-modal leakage for existing LVLMs across all benchmarks for analysis. MMBench registers the highest average multi-modal gain at 50.1, indicating a significant overlap between the domains covered by existing LVLMs' training data and MMBench. Conversely, MMMU exhibits the lowest average multi-modal gain at 5.8, suggesting a lesser degree of overlap between the domains of existing LVLMs' training corpora and those included in MMMU. Additionally, MMStar, as expected, has the lowest degree of multi-modal leakage at 1.9. This provides a comprehensive and fair arena for comparing existing LVLMs. Moreover, we believe evaluating existing LVLMs to derive average ML metrics can also be helpful to the following works in examining newly developed multi-modal benchmarks.</p>
<h1>6 Conclusion</h1>
<p>In this work, we dig into current evaluation works for large vision-language models (LVLMs) and identify two primary issues: 1) visual content is unnecessary for many samples, and 2) unintentional data leakage exists in LLM and LVLM training. To address these issues, we develop an elite vision-dependent multi-modal benchmark named MMStar and propose two metrics to measure the data leakage and actual performance gain in LVLMs' multi-modal training. MMStar undergoes the manual review of each sample, covering 6 core capabilities and 18 detailed axes for an in-depth evaluation of LVLMs' multimodal capabilities. In our evaluation of 16 diverse LVLMs on MMStar, even the best model scores under 60 on average. We also analyze the MG and ML metrics across 6 multimodal benchmarks and MMStar, providing valuable insights for the community on gathering multimodal training data and crafting new benchmarks. In the future, we plan to expand MMStar into a larger, online test set and explore dynamic evaluation methods to maintain sample visual dependency and reduce accidental data leakage into LLM's and LVLM's training corpora.</p>
<h1>A Cases of Lacking Visual Dependency</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: We highlight cases in existing benchmarks where evaluation samples lack the visual necessary.</p>
<h1>B Cases of Data Leakage in LLMs' Training Data</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: We highlight cases in existing benchmarks where evaluation samples are leaked into LLMs' training data.</p>
<h1>C Cases of Data Leakage in LVLMs' Multi-Modal Training Data</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: We highlight cases in existing benchmarks where evaluation samples are leaked into LVLMs' multimodal training data.</p>
<h1>D Detailed Evaluation Results of LVLMs on Six Multi-modal Benchmarks</h1>
<p>Table 7: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the "strategy" column, "LLM" refers to evaluating using the corresponding LLM base of the LVLM, while "LVLM-text" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Param.</th>
<th style="text-align: center;">Strategy</th>
<th style="text-align: center;">MMMU</th>
<th style="text-align: center;">MMB</th>
<th style="text-align: center;">ScienceQA</th>
<th style="text-align: center;">AI2D</th>
<th style="text-align: center;">SEED</th>
<th style="text-align: center;">MathVista</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random Choice</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">18.7</td>
</tr>
<tr>
<td style="text-align: center;">Closed-source LVLMs and corresponding LLM bases</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT4V[35] <br> (GPT4-Turbo[34])</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: center;">GeminiPro-Vision[41] <br> (GeminiPro[41])</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">41.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;">Open-source LVLMs and corresponding LLM bases</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TinyLLaVA[53] <br> (Phi2-2.7B[32])</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">26.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">55.6</td>
</tr>
<tr>
<td style="text-align: center;">Yi-VL[49] <br> (Yi-6B[49])</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">30.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">55.9</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5[24] <br> (Vicuna-v1.5-7B[8])</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: center;">ShareGPT4V[5] <br> (Vicuna-v1.5-7B[8])</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">54.5</td>
</tr>
<tr>
<td style="text-align: center;">InternLM2-XC2[12] <br> (InternLM2-7B[42])</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">43.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-VL-Chat[2] <br> (Qwen-7B[1])</td>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">25.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">52.9</td>
</tr>
<tr>
<td style="text-align: center;">Deepseek-VL[28] <br> (Deepseek-7B[3])</td>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">39.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: center;">Monkey-Chat[23] <br> (Qwen-7B[1])</td>
<td style="text-align: center;">10B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">25.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">39.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">60.4</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5[24] <br> (Vicuna-v1.5-13B[8])</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">31.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">55.3</td>
</tr>
<tr>
<td style="text-align: center;">CogVLM-Chat[45] <br> (Vicuna-v1.5-7B[8])</td>
<td style="text-align: center;">17B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: center;">Yi-VL[49] <br> (Yi-34B[49])</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">36.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">42.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-Next[25] <br> (NH2-Yi-34B[33])</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;">InternVL-Chat-v1.2[6] <br> (NH2-Yi-34B[33])</td>
<td style="text-align: center;">40B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">40.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">69.3</td>
</tr>
<tr>
<td style="text-align: center;">Sphinx-X-MoE[15] <br> (Mixtral-8x7B[19])</td>
<td style="text-align: center;">57B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">29.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM-text</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">43.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LVLM</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">60.1</td>
</tr>
</tbody>
</table>
<h1>References</h1>
<p>[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.
[2] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.
[3] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.
[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.
[6] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.
[7] S. Cheng, Z. Guo, J. Wu, K. Fang, P. Li, H. Liu, and Y. Liu. Can vision-language models think from a first-person perspective? arXiv preprint arXiv:2311.15596, 2023.
[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.
[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[10] O. Contributors. Opencompass: A universal evaluation platform for foundation models. https:// github.com/open-compass/opencompass, 2023.
[11] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
[12] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, X. Wei, S. Zhang, H. Duan, M. Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in visionlanguage large model. arXiv preprint arXiv:2401.16420, 2024.
[13] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.
[14] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.
[15] P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, et al. Sphinxx: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024.
[16] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.
[17] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904-4916. PMLR, 2021.
[18] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. 1. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[19] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. 1. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
[20] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. A diagram is worth a dozen images. ArXiv, abs/1603.07396, 2016.</p>
<p>[21] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.
[22] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.
[23] Z. Li, B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023.
[24] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.
[25] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.
[26] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.
[27] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.
[28] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.
[29] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.
[30] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.
[31] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, and R. Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. arXiv preprint arXiv:2305.15023, 2023.
[32] Microsoft. Phi2: The surprising power of small language models. https://www.microsoft.com/ en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/, 2023.
[33] NousResearch. Nous-hermes-2-yi-34b. https://huggingface.co/NousResearch/ Nous-Hermes-2-Yi-34B, 2023.
[34] OpenAI. Chatgpt. https://chat.openai.com/, 2023.
[35] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.
[36] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[37] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.
[38] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162. Springer, 2022.
[39] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.
[40] H. Taud and J.-F. Mas. Multilayer perceptron (mlp). Geomatic approaches for modeling land change scenarios, pages 451-455, 2018.
[41] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
[42] I. Team. Internlm: A multilingual language model with progressively enhanced capabilities, 2023.</p>
<p>[43] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[44] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023.
[45] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.
[46] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023.
[47] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.
[48] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.
[49] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.
[50] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.
[51] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.
[52] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023.
[53] B. Zhou, Y. Hu, X. Weng, J. Jia, J. Luo, X. Liu, J. Wu, and L. Huang. Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024.
[54] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution. This work is done during internship in Shanghai AI Laboratory.
${ }^{\dagger}$ Correspoding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-680 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-680</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-680</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-02cbb0db288af2c83b48a023f245812bd22a2408</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/02cbb0db288af2c83b48a023f245812bd22a2408" target="_blank">Handling Divergent Reference Texts when Evaluating Table-to-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A new metric is proposed, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall, and is applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.</p>
                <p><strong>Paper Abstract:</strong> Automatically constructed datasets for generating text from semi-structured data (tables), such as WikiBio, often contain reference texts that diverge from the information in the corresponding semi-structured data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed by Wiseman et al (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e680.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e680.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Divergent References</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Divergent reference texts vs. source tables in table-to-text datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reference (natural language) descriptions associated with semi-structured source tables often contain information that is not present in the source table (extra/hallucinated facts) and/or omit information that is present in the table, producing a misalignment between the NL description and the underlying data. This misalignment degrades automatic evaluation and model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WikiBio table-to-text evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation setup for table-to-text generation (WikiBio dataset): each test instance consists of a source table, an automatically-constructed reference description, and generated texts from systems; automatic metrics that compare generated text only to the reference are used to score systems.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset-provided reference text (natural language description)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated model outputs and evaluation code (automatic metric implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>divergent reference (extra information present in reference; missing information present in table)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many references in automatically-constructed datasets contain content that cannot be inferred from the corresponding table (e.g., extra facts, hallucinatory details) and fail to mention facts that are present in the table. This causes mismatch between what the NL description claims and what the data actually provides, so metrics that treat references as gold targets unfairly penalize correct system outputs (which reference the table) and reward spurious matches to the divergent reference.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation data (reference dataset) and evaluation metrics (metrics that compare only to reference)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>human annotation (paired comparisons and binary entailment labels) and manual inspection; dataset-level analysis counting occurrences of extra/missing information; lexical entailment heuristics (word-overlap) and statistical co-occurrence models to detect n‑grams not entailed by the table.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantified by: (1) human judgements aggregated via Thurstone's method over 16 models on 1100 examples; (2) binary entailment labeling by 3 annotators per (table,reference) pair (majority vote); (3) dataset statistics (percentage of references with extra info); (4) downstream effect measured as Pearson correlation between automatic metric scores and aggregated human scores, and instance-level agreement (accuracy) with pairwise human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: automatic metrics that rely solely on reference texts (BLEU, ROUGE, METEOR, CIDEr) show weak or even negative Pearson correlation with human judgments in the hyperparameter-comparison setting, meaning tuning to these metrics can select worse models. Example quantitative impacts reported: 62% of WikiBio references contain extra information; only 38% of references were labeled as fully entailed by their table. BLEU's average Pearson correlation across settings was ~0.478 vs. PARENT-W average 0.838; in the hyperparams setting some text-only metrics had negative correlations. Instance-level decision agreement: best metrics only reached ~60% accuracy vs. humans, indicating nontrivial disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High in automatically constructed datasets: in WikiBio the authors found ~62% of references mention extra information and only ~38% of references were judged as fully entailed by the associated table (majority-vote over 3 annotators).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Automatic/heuristic dataset construction and weak alignment heuristics that pair tables with nearby text (or use noisy extraction), leading to references that include externally known facts or omitted salient table facts; implicit assumptions about reference completeness; paraphrasing and lexical variability between table content and reference.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly incorporate the source table into evaluation: (a) develop an entailment-aware metric (PARENT) that weights n-grams by their probability of being entailed by the table and computes precision/recall against both table and reference; (b) use table values as additional references (BLEU-T); (c) use an information-extraction-based evaluation pipeline (train a text->table extractor and compute RG/CS/RG-F); (d) collect human entailment labels when possible; (e) tune metric trade-off parameter (lambda) per-instance using reference recall vs. table as a heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified improvements reported: PARENT-W (word-overlap entailment) achieved Pearson correlations of 0.912 (Systems) and 0.763 (Hyperparams) with human judgments (average 0.838), substantially higher than BLEU (0.548 / 0.407, avg 0.478) and ROUGE/METEOR/CIDEr variants. A tuned variant PARENT*-W reached 0.982 (Systems) and 0.844 (Hyperparams) (avg 0.913). Information-extraction RG-F achieved competitive correlations (e.g., ~0.753 / 0.763). Ablation: removing the entailment weighting w(g) from PARENT dramatically reduced average correlation (PARENT-W dropped to 0.168); removing table-recall (lambda=0) dropped correlation to 0.328, showing the mitigation's critical role. Instance-level agreement with human pairwise judgments for PARENT variants was ~59.8–60.6%, the best among tested metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural language generation evaluation / machine learning (table-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Handling Divergent Reference Texts when Evaluating Table-to-Text Generation', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e680.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e680.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PARENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PARENT (Precision And Recall of Entailed N-grams from the Table)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation metric that computes precision and recall of generated text n-grams with respect to both the reference and the source table, using an entailment probability w(g) (estimated via lexical overlap or co-occurrence models) to decide whether an n-gram is supported by the table; combines components with geometric averaging and instance-level F-score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PARENT evaluation metric for table-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Metric that: (1) estimates entailment probability w(g) for each n-gram versus the table (word-overlap or co-occurrence model); (2) computes entailed precision (reward n-grams present in reference or with high w(g)); (3) computes entailed recall as geometric average of recall to reference (weighted by w(g)) and recall to table (LCS-based per-record coverage); (4) combines into instance F-scores and averages across dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric description in paper (natural language specification of PARENT)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation metric implementation/code used to score model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>mitigation of reference-vs-table misalignment (addresses evaluation gap)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>PARENT was designed to address the discrepancy where reference texts diverge from source tables; it uses entailment probabilities to avoid penalizing correct table-derived content missing from the reference and to discount reference n-grams that are not supported by the table.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Evaluated by comparing metric scores to aggregated human judgments (Thurstone method) across multiple models and bootstrap sampling; also via ablation studies and instance-level pairwise decision agreement with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Primary measurement: Pearson correlation between per-model metric scores and aggregated human scores; secondary: instance-level accuracy in matching human pairwise judgments; bootstrap confidence intervals; ablation experiments measuring drops in correlation when key components (w(g), table recall) are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>PARENT substantially improved alignment of automatic evaluation with human judgments: e.g., PARENT-W average Pearson correlation 0.838 vs. BLEU avg 0.478; PARENT*-W (tuned lambda) average 0.913. Using PARENT reduces the risk of metric-driven selection of worse models when references are divergent, and makes hyperparameter comparisons more reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Demonstrated effective on WikiBio (highly divergent references) and applicable to WebNLG (human-elicited references) where it remains competitive; choice of entailment model matters (word-overlap worked better on WikiBio, co-occurrence better on WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Designed specifically to counter errors introduced by treating automatically-collected references as gold targets; addresses missing/extra-reference content via entailment modeling and table-aware recall.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Compute entailment-weighted precision and recall using (1) simple lexical entailment models (word-overlap or co-occurrence), (2) table-based recall using LCS per record, (3) geometric averaging with per-instance lambda (heuristic: 1 - lambda = reference recall against table), (4) smoothing for geometric means, and (5) optional tuning of lambda on human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Numerically effective: see correlations above (PARENT-W systems correlation 0.912; hyperparams 0.763; tuned PARENT*-W 0.982/0.844). Ablation confirmed critical components: removing w(g) reduced average correlation to 0.168; removing table recall reduced average correlation to 0.328. Instance-level agreement with humans ~60%.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural language generation evaluation / ML benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Handling Divergent Reference Texts when Evaluating Table-to-Text Generation', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e680.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e680.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RG-F (IE-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-Generation F-score (Information-extraction-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation approach adapted from Wiseman et al. (2017): train a text->table information extraction model to recover (attribute, value) pairs from generated text, then compare extracted pairs to ground-truth table and/or to pairs extracted from reference text; RG-F is the F-score versus ground-truth table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Information-extraction-based evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Train a pointer-generator text->table model to produce linearized (attribute,value) outputs from generated/reference text; parse outputs into tuples and compute precision/recall/F-score against ground-truth tables to evaluate content selection and faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reference text and evaluation protocol described in paper</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>trained information extraction model and evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>evaluation gap due to divergent references (indirect approach: measure fidelity by recovering table content from text)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because references can diverge, estimating fidelity by extracting table tuples from text and comparing to the true table offers an alternative that bypasses reliance on the reference; however the extractor itself is imperfect and can hallucinate or miss tuples, introducing measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation pipeline (auxiliary extraction model)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Train-and-evaluate the IE model on the dataset (pointer-generator network), measure its extraction F-score on held-out data and then use its outputs to compute evaluation metrics for generated texts; compare resulting metric correlations with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>IE system performance reported as precision/recall/F-score on extracting (attribute,value) pairs against ground-truth tables (example: 0.428 precision, 0.310 recall, F=0.351). Downstream evaluation measured by Pearson correlation between RG-F scores and aggregated human judgments across systems.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>RG-F achieved strong correlations with human judgments (e.g., RG-F average ~0.758, competitive with PARENT) and provides an alternative metric less reliant on the reference; however, the imperfect extractor (F≈0.351) limits its reliability and requires training resources and domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Effective when a reasonably accurate IE model can be trained; in open-domain WikiBio the extractor attained F≈0.351, yet RG-F correlations were competitive, showing applicability but also highlighting dependence on extractor quality.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Addresses divergence by measuring text->table fidelity, but inherits shortcomings from noisy extraction models and dataset heterogeneity (open schema, lexical variability).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Train a pointer-generator sequence-to-sequence extractor on training data; use its parsed (attribute,value) tuples to compute content selection (CS), relation generation precision (RG), and RG-F metrics; omit ordering metrics if extractor does not align records.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>RG-F attained Pearson correlations comparable to PARENT in the experiments (e.g., RG-F ~0.753 systems / 0.763 hyperparams in Table 2), demonstrating effectiveness despite extractor imperfections. However, extraction F-score (0.351) indicates measurement noise remains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Natural language generation evaluation / information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Handling Divergent Reference Texts when Evaluating Table-to-Text Generation', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Challenges in data-to-document generation <em>(Rating: 2)</em></li>
                <li>Neural text generation from structured data with application to the biography domain <em>(Rating: 2)</em></li>
                <li>Why we need new evaluation metrics for NLG <em>(Rating: 2)</em></li>
                <li>Creating training corpora for micro-planners <em>(Rating: 1)</em></li>
                <li>Detecting cross-lingual semantic divergence for neural machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-680",
    "paper_id": "paper-02cbb0db288af2c83b48a023f245812bd22a2408",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Divergent References",
            "name_full": "Divergent reference texts vs. source tables in table-to-text datasets",
            "brief_description": "Reference (natural language) descriptions associated with semi-structured source tables often contain information that is not present in the source table (extra/hallucinated facts) and/or omit information that is present in the table, producing a misalignment between the NL description and the underlying data. This misalignment degrades automatic evaluation and model selection.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "WikiBio table-to-text evaluation pipeline",
            "system_description": "Evaluation setup for table-to-text generation (WikiBio dataset): each test instance consists of a source table, an automatically-constructed reference description, and generated texts from systems; automatic metrics that compare generated text only to the reference are used to score systems.",
            "nl_description_type": "dataset-provided reference text (natural language description)",
            "code_implementation_type": "generated model outputs and evaluation code (automatic metric implementations)",
            "gap_type": "divergent reference (extra information present in reference; missing information present in table)",
            "gap_description": "Many references in automatically-constructed datasets contain content that cannot be inferred from the corresponding table (e.g., extra facts, hallucinatory details) and fail to mention facts that are present in the table. This causes mismatch between what the NL description claims and what the data actually provides, so metrics that treat references as gold targets unfairly penalize correct system outputs (which reference the table) and reward spurious matches to the divergent reference.",
            "gap_location": "evaluation data (reference dataset) and evaluation metrics (metrics that compare only to reference)",
            "detection_method": "human annotation (paired comparisons and binary entailment labels) and manual inspection; dataset-level analysis counting occurrences of extra/missing information; lexical entailment heuristics (word-overlap) and statistical co-occurrence models to detect n‑grams not entailed by the table.",
            "measurement_method": "Quantified by: (1) human judgements aggregated via Thurstone's method over 16 models on 1100 examples; (2) binary entailment labeling by 3 annotators per (table,reference) pair (majority vote); (3) dataset statistics (percentage of references with extra info); (4) downstream effect measured as Pearson correlation between automatic metric scores and aggregated human scores, and instance-level agreement (accuracy) with pairwise human preferences.",
            "impact_on_results": "Substantial: automatic metrics that rely solely on reference texts (BLEU, ROUGE, METEOR, CIDEr) show weak or even negative Pearson correlation with human judgments in the hyperparameter-comparison setting, meaning tuning to these metrics can select worse models. Example quantitative impacts reported: 62% of WikiBio references contain extra information; only 38% of references were labeled as fully entailed by their table. BLEU's average Pearson correlation across settings was ~0.478 vs. PARENT-W average 0.838; in the hyperparams setting some text-only metrics had negative correlations. Instance-level decision agreement: best metrics only reached ~60% accuracy vs. humans, indicating nontrivial disagreement.",
            "frequency_or_prevalence": "High in automatically constructed datasets: in WikiBio the authors found ~62% of references mention extra information and only ~38% of references were judged as fully entailed by the associated table (majority-vote over 3 annotators).",
            "root_cause": "Automatic/heuristic dataset construction and weak alignment heuristics that pair tables with nearby text (or use noisy extraction), leading to references that include externally known facts or omitted salient table facts; implicit assumptions about reference completeness; paraphrasing and lexical variability between table content and reference.",
            "mitigation_approach": "Explicitly incorporate the source table into evaluation: (a) develop an entailment-aware metric (PARENT) that weights n-grams by their probability of being entailed by the table and computes precision/recall against both table and reference; (b) use table values as additional references (BLEU-T); (c) use an information-extraction-based evaluation pipeline (train a text-&gt;table extractor and compute RG/CS/RG-F); (d) collect human entailment labels when possible; (e) tune metric trade-off parameter (lambda) per-instance using reference recall vs. table as a heuristic.",
            "mitigation_effectiveness": "Quantified improvements reported: PARENT-W (word-overlap entailment) achieved Pearson correlations of 0.912 (Systems) and 0.763 (Hyperparams) with human judgments (average 0.838), substantially higher than BLEU (0.548 / 0.407, avg 0.478) and ROUGE/METEOR/CIDEr variants. A tuned variant PARENT*-W reached 0.982 (Systems) and 0.844 (Hyperparams) (avg 0.913). Information-extraction RG-F achieved competitive correlations (e.g., ~0.753 / 0.763). Ablation: removing the entailment weighting w(g) from PARENT dramatically reduced average correlation (PARENT-W dropped to 0.168); removing table-recall (lambda=0) dropped correlation to 0.328, showing the mitigation's critical role. Instance-level agreement with human pairwise judgments for PARENT variants was ~59.8–60.6%, the best among tested metrics.",
            "domain_or_field": "Natural language generation evaluation / machine learning (table-to-text generation)",
            "reproducibility_impact": true,
            "uuid": "e680.0",
            "source_info": {
                "paper_title": "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "PARENT",
            "name_full": "PARENT (Precision And Recall of Entailed N-grams from the Table)",
            "brief_description": "An evaluation metric that computes precision and recall of generated text n-grams with respect to both the reference and the source table, using an entailment probability w(g) (estimated via lexical overlap or co-occurrence models) to decide whether an n-gram is supported by the table; combines components with geometric averaging and instance-level F-score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PARENT evaluation metric for table-to-text generation",
            "system_description": "Metric that: (1) estimates entailment probability w(g) for each n-gram versus the table (word-overlap or co-occurrence model); (2) computes entailed precision (reward n-grams present in reference or with high w(g)); (3) computes entailed recall as geometric average of recall to reference (weighted by w(g)) and recall to table (LCS-based per-record coverage); (4) combines into instance F-scores and averages across dataset.",
            "nl_description_type": "metric description in paper (natural language specification of PARENT)",
            "code_implementation_type": "evaluation metric implementation/code used to score model outputs",
            "gap_type": "mitigation of reference-vs-table misalignment (addresses evaluation gap)",
            "gap_description": "PARENT was designed to address the discrepancy where reference texts diverge from source tables; it uses entailment probabilities to avoid penalizing correct table-derived content missing from the reference and to discount reference n-grams that are not supported by the table.",
            "gap_location": "evaluation metrics / evaluation pipeline",
            "detection_method": "Evaluated by comparing metric scores to aggregated human judgments (Thurstone method) across multiple models and bootstrap sampling; also via ablation studies and instance-level pairwise decision agreement with humans.",
            "measurement_method": "Primary measurement: Pearson correlation between per-model metric scores and aggregated human scores; secondary: instance-level accuracy in matching human pairwise judgments; bootstrap confidence intervals; ablation experiments measuring drops in correlation when key components (w(g), table recall) are removed.",
            "impact_on_results": "PARENT substantially improved alignment of automatic evaluation with human judgments: e.g., PARENT-W average Pearson correlation 0.838 vs. BLEU avg 0.478; PARENT*-W (tuned lambda) average 0.913. Using PARENT reduces the risk of metric-driven selection of worse models when references are divergent, and makes hyperparameter comparisons more reliable.",
            "frequency_or_prevalence": "Demonstrated effective on WikiBio (highly divergent references) and applicable to WebNLG (human-elicited references) where it remains competitive; choice of entailment model matters (word-overlap worked better on WikiBio, co-occurrence better on WebNLG).",
            "root_cause": "Designed specifically to counter errors introduced by treating automatically-collected references as gold targets; addresses missing/extra-reference content via entailment modeling and table-aware recall.",
            "mitigation_approach": "Compute entailment-weighted precision and recall using (1) simple lexical entailment models (word-overlap or co-occurrence), (2) table-based recall using LCS per record, (3) geometric averaging with per-instance lambda (heuristic: 1 - lambda = reference recall against table), (4) smoothing for geometric means, and (5) optional tuning of lambda on human judgments.",
            "mitigation_effectiveness": "Numerically effective: see correlations above (PARENT-W systems correlation 0.912; hyperparams 0.763; tuned PARENT*-W 0.982/0.844). Ablation confirmed critical components: removing w(g) reduced average correlation to 0.168; removing table recall reduced average correlation to 0.328. Instance-level agreement with humans ~60%.",
            "domain_or_field": "Natural language generation evaluation / ML benchmarking",
            "reproducibility_impact": true,
            "uuid": "e680.1",
            "source_info": {
                "paper_title": "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "RG-F (IE-based)",
            "name_full": "Relation-Generation F-score (Information-extraction-based evaluation)",
            "brief_description": "An evaluation approach adapted from Wiseman et al. (2017): train a text-&gt;table information extraction model to recover (attribute, value) pairs from generated text, then compare extracted pairs to ground-truth table and/or to pairs extracted from reference text; RG-F is the F-score versus ground-truth table.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Information-extraction-based evaluation pipeline",
            "system_description": "Train a pointer-generator text-&gt;table model to produce linearized (attribute,value) outputs from generated/reference text; parse outputs into tuples and compute precision/recall/F-score against ground-truth tables to evaluate content selection and faithfulness.",
            "nl_description_type": "reference text and evaluation protocol described in paper",
            "code_implementation_type": "trained information extraction model and evaluation scripts",
            "gap_type": "evaluation gap due to divergent references (indirect approach: measure fidelity by recovering table content from text)",
            "gap_description": "Because references can diverge, estimating fidelity by extracting table tuples from text and comparing to the true table offers an alternative that bypasses reliance on the reference; however the extractor itself is imperfect and can hallucinate or miss tuples, introducing measurement noise.",
            "gap_location": "evaluation pipeline (auxiliary extraction model)",
            "detection_method": "Train-and-evaluate the IE model on the dataset (pointer-generator network), measure its extraction F-score on held-out data and then use its outputs to compute evaluation metrics for generated texts; compare resulting metric correlations with human judgments.",
            "measurement_method": "IE system performance reported as precision/recall/F-score on extracting (attribute,value) pairs against ground-truth tables (example: 0.428 precision, 0.310 recall, F=0.351). Downstream evaluation measured by Pearson correlation between RG-F scores and aggregated human judgments across systems.",
            "impact_on_results": "RG-F achieved strong correlations with human judgments (e.g., RG-F average ~0.758, competitive with PARENT) and provides an alternative metric less reliant on the reference; however, the imperfect extractor (F≈0.351) limits its reliability and requires training resources and domain adaptation.",
            "frequency_or_prevalence": "Effective when a reasonably accurate IE model can be trained; in open-domain WikiBio the extractor attained F≈0.351, yet RG-F correlations were competitive, showing applicability but also highlighting dependence on extractor quality.",
            "root_cause": "Addresses divergence by measuring text-&gt;table fidelity, but inherits shortcomings from noisy extraction models and dataset heterogeneity (open schema, lexical variability).",
            "mitigation_approach": "Train a pointer-generator sequence-to-sequence extractor on training data; use its parsed (attribute,value) tuples to compute content selection (CS), relation generation precision (RG), and RG-F metrics; omit ordering metrics if extractor does not align records.",
            "mitigation_effectiveness": "RG-F attained Pearson correlations comparable to PARENT in the experiments (e.g., RG-F ~0.753 systems / 0.763 hyperparams in Table 2), demonstrating effectiveness despite extractor imperfections. However, extraction F-score (0.351) indicates measurement noise remains.",
            "domain_or_field": "Natural language generation evaluation / information extraction",
            "reproducibility_impact": true,
            "uuid": "e680.2",
            "source_info": {
                "paper_title": "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Challenges in data-to-document generation",
            "rating": 2
        },
        {
            "paper_title": "Neural text generation from structured data with application to the biography domain",
            "rating": 2
        },
        {
            "paper_title": "Why we need new evaluation metrics for NLG",
            "rating": 2
        },
        {
            "paper_title": "Creating training corpora for micro-planners",
            "rating": 1
        },
        {
            "paper_title": "Detecting cross-lingual semantic divergence for neural machine translation",
            "rating": 1
        }
    ],
    "cost": 0.01480575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Handling Divergent Reference Texts when Evaluating Table-to-Text Generation</h1>
<p>Bhuwan Dhingra ${ }^{\dagger *}$<br>Manaal Faruqui ${ }^{\ddagger}$ Ankur Parikh ${ }^{\ddagger}$<br>Dipanjan Das ${ }^{\ddagger}$ William W. Cohen ${ }^{\dagger \ddagger}$<br>${ }^{\dagger}$ Carnegie Mellon University<br>${ }^{\ddagger}$ Google Research<br>bdhingra@cs.cmu.edu<br>{mfaruqui,aparikh,mingweichang,dipanjand,wcohen}@google.com</p>
<h4>Abstract</h4>
<p>Automatically constructed datasets for generating text from semi-structured data (tables), such as WikiBio (Lebret et al., 2016), often contain reference texts that diverge from the information in the corresponding semistructured data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017).</p>
<p>For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table.</p>
<p>We show that existing automatic metrics, including BLEU, correlate poorly with human judgments when the evaluation sets contain divergent references (§5.4). For many table-to-text generation tasks, the tables themselves are in a pseudonatural language format (e.g., WikiBio, WebNLG (Gardent et al., 2017), and E2E-NLG (Dušek et al., 2019)). In such cases we propose to compare the generated text to the underlying table as well to improve evaluation. We develop a new metric, PARENT (Precision And Recall of Entailed Ngrams from the Table) (§3). When computing precision, PARENT effectively uses a union of the reference and the table, to reward correct information missing from the reference. When computing recall, it uses an intersection of the reference and the table, to ignore extra incorrect information in the reference. The union and intersection are computed with the help of an entailment model to decide if a text n-gram is entailed by the table. ${ }^{2}$ We</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reference:</th>
<th style="text-align: center;">Michael Dahlquist (December 22, 1965 - July 14, 2005) was a drummer in the Seattle band Srikworm.</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE</th>
<th style="text-align: center;">PARENT</th>
<th style="text-align: center;">Michael Dahlquist</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Birth name</td>
<td style="text-align: center;">Michael Dahlquist</td>
</tr>
<tr>
<td style="text-align: center;">Candidate 1:</td>
<td style="text-align: center;">Michael Dahlquist (December 22, 1965 - July 14, 2005) was a drummer in the California band Grateful Dead.</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">Born</td>
<td style="text-align: center;">December 22, 1965</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seattle, Washington</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Candidate 2:</td>
<td style="text-align: center;">Michael Dahlquist (December 22, 1965 - July 14, 2005) was a drummer.</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">Died</td>
<td style="text-align: center;">July 14, 2005 (aged 39)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Geneva, Illinois, Illinois</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Candidate 3:</td>
<td style="text-align: center;">Michael Dahlquist (December 22, 1965 - July 14, 2005) was a drummer from Seattle, Washington.</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">Occupation(s)</td>
<td style="text-align: center;">Drummer</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Instruments</td>
<td style="text-align: center;">Drums</td>
</tr>
</tbody>
</table>
<p>Figure 1: A table from the WikiBio dataset (right), its reference description and three hypothetical generated texts with scores assigned to them by automatic evaluation metrics. Text which cannot be inferred from the table is in red, and text which can be inferred but isn't present in the reference is in green. PARENT is our proposed metric.
show that this method is more effective than using the table as an additional reference. Our main contributions are:</p>
<ul>
<li>We conduct a large-scale human evaluation of the outputs from 16 table-to-text models on 1100 examples from the WikiBio dataset, many of which have divergent references (§5.2).</li>
<li>We propose a new metric, PARENT (§3), and show that it improves correlation with human judgments over existing metrics, both when comparing similar systems (such as different hyperparameters of a neural network) and when comparing vastly different systems (such as template-based and neural models).</li>
<li>We also develop information extraction based metrics, inspired from Wiseman et al. (2017), by training a model to extract tables from the reference texts (§4). We find that these metrics have comparable correlation to PARENT, with the latter being easier to use out of the box.</li>
<li>We analyze the sensitivity of the metrics to divergence by collecting labels for which references contain only information also present in the tables. We show that PARENT maintains high correlation as the number of such examples is varied. (§5.5).</li>
<li>We also demonstrate the applicability of PARENT on the data released as part of the WebNLG challenge (Gardent et al., 2017), where the references are elicited from humans, and hence are of high quality (§5.4).</li>
</ul>
<h2>2 Table-to-Text Generation</h2>
<p>We briefly review the task of generating natural language descriptions of semi-structured data, which we refer to as tables henceforth (Barzilay</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and Lapata, 2005; Liang et al., 2009). Tables can be expressed as set of records $T=\left{r_{k}\right}<em M="M">{k=1}^{K}$, where each record is a tuple (entity, attribute, value). When all the records are about the same entity, we can truncate the records to (attribute, value) pairs. For example, for the table in Figure 1, the records are [(Birth Name, Michael Dahlquist), (Born, December 22 1965), ...]. The task is to generate a text $G$ which summarizes the records in a fluent and grammatical manner. ${ }^{3}$ For training and evaluation we further assume that we have a reference description $R$ available for each table. We let $\mathcal{D}</em>\right)\right}}=\left{\left(T^{i}, R^{i}, G^{i<em _mathrm_n="\mathrm{n">{i=1}^{N}$ denote an evaluation set of tables, references and texts generated from a model $M$, and $R</em>$, respectively. We use $#}}^{i}, G_{\mathrm{n}}^{i}$ denote the collection of n-grams of order $n$ in $R^{i}$ and $G^{i<em _mathrm_n="\mathrm{n">{R</em>$, and $#}}^{i}}(g)$ to denote the count of n-gram $g$ in $R_{\mathrm{n}}^{i<em _mathrm_n="\mathrm{n">{G</em>$. Our goal is to assign a score to the model, which correlates highly with human judgments of the quality of that model.}}^{i}, R_{\mathrm{n}}^{i}}(g)$ to denote the minimum of its counts in $R_{\mathrm{n}}^{i}$ and $G_{\mathrm{n}}^{i</p>
<p>Divergent References. In this paper we are interested in the case where reference texts diverge from the tables. In Figure 1, the reference, though technically correct and fluent, mentions information which cannot be gleaned from the associated table. It also fails to mention useful information which a generation system might correctly include (e.g. candidate 3 in the figure). We call such references divergent from the associated table. This phenomenon is quite common - in WikiBio we found that $62 \%$ of the references mention extra information (§5.5). Divergence is common in human-curated translation datasets as well (Carpuat et al., 2017; Vyas et al., 2018).</p>
<p>How does divergence affect automatic evalua-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>tion? As a motivating example, consider the three candidate generations shown in Figure 1. Clearly, candidate 1 is the worst since it "hallucinates" false information, and candidate 3 is the best since it is correct and mentions more information than candidate 2. However, BLEU and ROUGE, which only compare the candidates to the reference, penalize candidate 3 for both excluding the divergent information in the reference (in red) and including correct information from the table (in green). ${ }^{4}$ PARENT, which compares to both the table and reference, correctly ranks the three candidates.</p>
<h2>3 PARENT</h2>
<p>PARENT evaluates each instance $\left(T^{i}, R^{i}, G^{i}\right)$ separately, by computing the precision and recall of $G^{i}$ against both $T^{i}$ and $R^{i}$.</p>
<p>Entailment Probability. The table is in a semistructured form, and hence not directly comparable to the unstructured generated or reference texts. To bridge this gap, we introduce the notion of entailment probability, which we define as the probability that the presence of an n-gram $g$ in a text is "correct" given the associated table. We denote this probability as $w(g)=\operatorname{Pr}\left(g \Leftarrow T^{i}\right)$. Estimating this probability is in itself a challenging language understanding task, since the information in the table may be expressed in varied forms in text. Here, we describe two simple models of lexical entailment, inspired by work on the Recognizing Textual Entailment Challenge (Dagan et al., 2006). We found these simple models to be effective; while more sophisticated models may be used if there are complex inferences between the table and text, they are beyond the scope of this paper.</p>
<ol>
<li>Word Overlap Model: Let $\bar{T}^{i}$ denote all the lexical items present in the table $T^{i}$, including both attribute names and their values. Then, $w(g)=\sum_{j=1}^{n} \mathbb{1}\left(g_{j} \in \bar{T}^{i}\right) / n$, where $n$ is the length of $g$, and $g_{j}$ is the $j$ th token in $g$.</li>
<li>Co-occurrence Model: (Glickman and Dagan, 2005) Originally proposed for the RTE task, this model computes the probability of a term $g_{j}$ in the n-gram being entailed by the table as the maximum of its probabilities of being en-</li>
</ol>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tailed by each lexical item $v$ in the table:</p>
<p>$$
\operatorname{Pr}\left(g_{j} \Leftarrow T^{i}\right)=\max <em j="j">{v \in \bar{T}^{i}} \operatorname{Pr}\left(g</em> \Leftarrow v\right)
$$</p>
<p>$\operatorname{Pr}\left(g_{j} \Leftarrow v\right)$ is estimated using co-occurrence counts from a training set of table-reference pairs. Then the overall probability of the ngram being entailed is taken as the geometric average $w(g)=\left(\prod_{j=1}^{n} \operatorname{Pr}\left(g_{j} \Leftarrow T^{i}\right)\right)^{1 / n}$.5</p>
<p>We note that these models are not sensitive to paraphrases between the table and text. For tasks where this is important, embedding-based similarities may be used, but those are beyond the scope of this paper. Next we discuss how to compute the precision and recall of the generation.</p>
<p>Entailed Precision. When computing precision, we want to check what fraction of the n-grams in $G_{n}^{i}$ are correct. We consider an n-gram $g$ to be correct either if it occurs in the reference $R_{n}^{i} 6$, or if it has a high probability of being entailed by the table (i.e. $w(g)$ is high). Let $\operatorname{Pr}\left(g \in R_{n}^{i}\right)=$ $\frac{#<em n="n">{G</em>{#}^{i}, R_{n}^{i}}(g)<em n="n">{G</em>$ for n-grams of order $n$ is given by:}^{i}}(g)}$ denote the probability that an n-gram in $G_{n}^{i}$ also appears in $R_{n}^{i}$. Then, the entailed precision $E_{p}^{n</p>
<p>$$
\begin{aligned}
&amp; E_{p}^{n}= \
&amp; \frac{\sum_{g \in G_{n}^{i}}\left[\operatorname{Pr}\left(g \in R_{n}^{i}\right)+\operatorname{Pr}\left(g \notin R_{n}^{i}\right) w(g)\right] #<em n="n">{G</em> #}^{i}}(g)}{\sum_{g \in G_{n}^{i}<em n="n">{G</em> \
&amp; =\frac{\sum_{g \in G_{n}^{i}} #}^{i}}(g)<em n="n">{G</em>(g) w(g)+#}^{i}<em n="n">{G</em> #}^{i}, R_{n}^{i}}(g)[1-w(g)]}{\sum_{g \in G_{n}^{i}<em n="n">{G</em>
\end{aligned}
$$}^{i}}(g)</p>
<p>In words, an n-gram receives a reward of 1 if it appears in the reference, with probability $\operatorname{Pr}(g \in$ $R_{n}^{i}$ ), and otherwise it receives a reward of $w(g)$. Both numerator and denominator are weighted by the count of the n-gram in $G_{n}^{i} . \operatorname{Pr}\left(g \in R_{n}^{i}\right)$ rewards an n-gram for appearing as many times as it appears in the reference, not more. We combine precisions for n-gram orders 1-4 using a geometric</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>average, similar to BLEU:</p>
<p>$$
E_{p}=\exp \left(\sum_{n=1}^{4} \frac{1}{4} \log E_{p}^{n}\right)
$$</p>
<p>Entailed Recall. We compute recall against both the reference $\left(E_{r}\left(R^{i}\right)\right)$, to ensure proper sentence structure in the generated text, and the table $\left(E_{r}\left(T^{i}\right)\right)$, to ensure that texts which mention more information from the table get higher scores (e.g. candidate 3 in Figure 1). These are combined using a geometric average:</p>
<p>$$
E_{r}=E_{r}\left(R^{i}\right)^{(1-\lambda)} E_{r}\left(T^{i}\right)^{\lambda}
$$</p>
<p>The parameter $\lambda$ trades-off how much the generated text should match the reference, versus how much it should cover information from the table. The geometric average, which acts as an AND operation, ensures that the overall recall is high only when both the components are high. We found this necessary to assign low scores to bad systems which, for example, copy values from the table without phrasing them in natural language.</p>
<p>When computing $E_{r}\left(R^{i}\right)$, divergent references will have n-grams with low $w(g)$. We want to exclude these from the computation of recall, and hence their contributions are weighted by $w(g)$ :</p>
<p>$$
E_{r}^{n}\left(R^{i}\right)=\frac{\sum_{g \in R_{n}^{i}} #<em n="n">{G</em> #}^{i}, R_{n}^{i}}(g) w(g)}{\sum_{g \in R_{n}^{i}<em n="n">{R</em>
$$}^{i}}(g) w(g)</p>
<p>Similar to precision, we combine recalls for $n=$ $1-4$ using a geometric average to get $E_{r}\left(R^{i}\right)$.</p>
<p>For computing $E_{r}\left(T^{i}\right)$, note that a table is a set of records $T^{i}=\left{r_{k}\right}<em k="k">{k=1}^{K}$. For a record $r</em>$ denote its string value (such as "Michael Dahlquist" or "December 22 1965"). Then:}$, let $\bar{r}_{k</p>
<p>$$
E_{r}\left(T^{i}\right)=\frac{1}{K} \sum_{k=1}^{K} \frac{1}{\left|\bar{r}<em k="k">{k}\right|} L C S\left(\bar{r}</em>\right)
$$}, G^{i</p>
<p>where $\bar{r}<em k="k">{k}$ denotes the number of tokens in the value string, and $L C S(x, y)$ is the length of the longest common subsequence between $x$ and $y$. The $L C S$ function, borrowed from ROUGE, ensures that entity names in $\bar{r}</em>$.}$ appear in the same order in the text as the table. Higher values of $E_{r}\left(T^{i}\right)$ denote that more records are likely to be mentioned in $G^{i</p>
<p>The entailed precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a
model $M$ is the average of instance level PARENT scores across the evaluation set:</p>
<p>$$
\frac{1}{N} \sum_{i=1}^{N} \operatorname{PARENT}\left(G^{i}, R^{i}, T^{i}\right)
$$</p>
<p>Smoothing \&amp; Multiple References. The danger with geometric averages is that if any of the components being averaged become 0 , the average will also be 0 . Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value $\epsilon$ to any of $E_{p}^{n}, E_{r}^{n}\left(R^{i}\right)$ and $E_{r}\left(T^{i}\right)$ which are 0 . When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014).</p>
<p>Choosing $\lambda$ and $\epsilon$. To set the value of $\lambda$ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6 , as the value of $1-\lambda$. The intuition here is that if the recall of the reference against the table is high, it already covers most of the information, and we can assign it a high weight in Eq. 4. This leads to a separate value of $\lambda$ automatically set for each instance. ${ }^{7} \epsilon$ is set to $10^{-5}$ for all experiments.</p>
<h2>4 Evaluation via Information Extraction</h2>
<p>Wiseman et al. (2017) proposed to use an auxiliary model, trained to extract structured records from text, for evaluation. However, the extraction model presented in that work is limited to the closed-domain setting of basketball game tables and summaries. In particular, they assume that each table has exactly the same set of attributes for each entity, and that the entities can be identified in the text via string matching. These assumptions are not valid for the open-domain WikiBio dataset, and hence we train our own extraction model to replicate their evaluation scheme.</p>
<p>Our extraction system is a pointer-generator network (See et al., 2017), which learns to produce a linearized version of the table from the text. ${ }^{8}$ The network learns which attributes need to be populated in the output table, along with their values. It is trained on the training set of WikiBio. At test</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>time we parsed the output strings into a set of (attribute, value) tuples and compare it to the ground truth table. The F-score of this text-to-table system was $35.1 \%$, which is comparable to other challenging open-domain settings (Huang et al., 2017). More details are included in the Appendix A.1.</p>
<p>Given this information extraction system, we consider the following metrics for evaluation, along the lines of Wiseman et al. (2017). Content Selection (CS): F-score for the (attribute, value) pairs extracted from the generated text compared to those extracted from the reference. Relation Generation (RG): Precision for the (attribute, value) pairs extracted from the generated text compared to those in the ground truth table. RG-F: Since our task emphasizes the recall of information from the table as well, we consider another variant which computes the F-score of the extracted pairs to those in the table. We omit the content ordering metric, since our extraction system does not align records to the input text.</p>
<h2>5 Experiments \&amp; Results</h2>
<p>In this section we compare several automatic evaluation metrics by checking their correlation with the scores assigned by humans to table-to-text models. Specifically, given $l$ models $M_{1}, \ldots, M_{l}$, and their outputs on an evaluation set, we show these generated texts to humans to judge their quality, and obtain aggregated human evaluation scores for all the models, $\widehat{h}=\left(h_{1}, \ldots, h_{l}\right)(\$ 5.2)$. Next, to evaluate an automatic metric, we compute the scores it assigns to each model, $\bar{a}=$ $\left(a_{1}, \ldots, a_{l}\right)$, and check the Pearson correlation between $\widehat{h}$ and $\bar{a}$ (Graham and Baldwin, 2014).</p>
<h3>5.1 Data \&amp; Models</h3>
<p>Our main experiments are on the WikiBio dataset (Lebret et al., 2016), which is automatically constructed and contains many divergent references. In $\S 5.6$ we also present results on the data released as part of the WebNLG challenge.</p>
<p>We developed several models of varying quality for generating text from the tables in WikiBio. This gives us a diverse set of outputs to evaluate the automatic metrics on. Table 1 lists the models along with their hyperparameter settings and their scores from the human evaluation (\$5.2). Our focus is primarily on neural sequence-to-sequence methods since these are most widely used, but we</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Models used for WikiBio, with the human evaluation scores for these model outputs and the reference texts. PG-Net: Pointer-Generator network. Human scores computed using Thurstone's method (Tsukida and Gupta, 2011).
also include a template-based baseline. All neural models were trained on the WikiBio training set. Training details and sample outputs are included in Appendices A. $2 \&amp;$ A. 3.</p>
<p>We divide these models into two categories and measure correlation separately for both the categories. The first category, WikiBio-Systems, includes one model each from the four families listed in Table 1. This category tests whether a metric can be used to compare different model families with a large variation in the quality of their outputs. The second category, WikiBioHyperparams, includes 13 different hyperparameter settings of PG-Net (See et al., 2017), which was the best performing system overall. 9 of these were obtained by varying the beam size and length normalization penalty of the decoder network ( Wu et al., 2016), and the remaining 4 were obtained by re-scoring beams of size 8 with the information extraction model described in $\S 4$. All the models in this category produce high quality fluent texts, and differ primarily on the quantity and accuracy of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance. This is an important use-case as metrics are often used to tune hyperparameters of a model.</p>
<h3>5.2 Human Evaluation</h3>
<p>We collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure 2 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts</p>
<p>In this task, you will be shown a table that may have one or more rows and pairs of sentences below it. Your job is to select which sentence in each pair is a better summary of the table.</p>
<p>You should compare the two sentences in the following aspects:</p>
<ol>
<li>Naturalness - Which sentence sounds more like it would have been produced by a native speaker?</li>
<li>Faithfulness - Which sentence only contains information which is also in the table? Any additional information in the sentence that is not contained in the table should be considered negative, even if it is true/valuable information.</li>
<li>Informativeness - Which sentence captures the most important information from the table? Note that having more facts in a sentence is not always better if it hurts the naturalness of the sentence.</li>
</ol>
<p>If the two sentence are similar based on the factors above, use your best judgment to pick one which seems better.</p>
<p>Figure 2: Instructions to crowd-workers for comparing two generated texts.
(Callison-Burch et al., 2007). However, for measuring correlation the comparisons need to be aggregated into real-valued scores, $\bar{h}=\left(h_{1}, \ldots, h_{l}\right)$, for each of the $l=16$ models. For this, we use Thurstone's method (Tsukida and Gupta, 2011), which assigns a score to each model based on how many times it was preferred over an alternative.</p>
<p>The data collection was performed separately for models in the WikiBio-Systems and WikiBioHyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table 1). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.</p>
<h3>5.3 Compared Metrics</h3>
<p>Text only: We compare BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr and CIDErD (Vedantam et al., 2015) using their publicly available implementations.
Information Extraction based: We compare the CS, RG and RG-F metrics discussed in $\S 4$.
Text \&amp; Table: We compare a variant of BLEU, denoted as BLEU-T, where the values from the table are used as additional references. BLEUT draws inspiration from iBLEU (Sun and Zhou, 2012) but instead rewards n-grams which match the table rather than penalizing them. For PARENT, we compare both the word-overlap model (PARENT-W) and the co-occurrence model (PARENT-C) for determining entailment. We also compare versions where a single $\lambda$ is tuned on the entire dataset to maximize correlation with human judgments, denoted as PARENT*-W/C.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">WikiBio <br> Systems</th>
<th style="text-align: center;">WikiBio <br> Hyperparams</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE</td>
<td style="text-align: center;">$0.518 \pm 0.07^{C, W}$</td>
<td style="text-align: center;">$-0.585 \pm 0.15^{C, W}$</td>
<td style="text-align: center;">-0.034</td>
</tr>
<tr>
<td style="text-align: left;">CIDEr</td>
<td style="text-align: center;">$0.674 \pm 0.06^{C, W}$</td>
<td style="text-align: center;">$-0.516 \pm 0.15^{C, W}$</td>
<td style="text-align: center;">0.079</td>
</tr>
<tr>
<td style="text-align: left;">CIDEr-D</td>
<td style="text-align: center;">$0.646 \pm 0.06^{C, W}$</td>
<td style="text-align: center;">$-0.372 \pm 0.16^{C, W}$</td>
<td style="text-align: center;">0.137</td>
</tr>
<tr>
<td style="text-align: left;">METEOR</td>
<td style="text-align: center;">$0.697 \pm 0.06^{C, W}$</td>
<td style="text-align: center;">$-0.079 \pm 0.24^{C, W}$</td>
<td style="text-align: center;">0.309</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: center;">$0.548 \pm 0.07^{C, W}$</td>
<td style="text-align: center;">$0.407 \pm 0.15^{C, W}$</td>
<td style="text-align: center;">0.478</td>
</tr>
<tr>
<td style="text-align: left;">CS</td>
<td style="text-align: center;">$0.735 \pm 0.06^{W}$</td>
<td style="text-align: center;">$-0.604 \pm 0.16^{C, W}$</td>
<td style="text-align: center;">0.066</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-T</td>
<td style="text-align: center;">$0.688 \pm 0.11^{W}$</td>
<td style="text-align: center;">$0.587 \pm 0.14^{C, W}$</td>
<td style="text-align: center;">0.638</td>
</tr>
<tr>
<td style="text-align: left;">RG</td>
<td style="text-align: center;">$0.645 \pm 0.07^{C, W}$</td>
<td style="text-align: center;">$0.749 \pm 0.12$</td>
<td style="text-align: center;">0.697</td>
</tr>
<tr>
<td style="text-align: left;">RG-F</td>
<td style="text-align: center;">$0.753 \pm 0.06^{W}$</td>
<td style="text-align: center;">$0.763 \pm 0.12$</td>
<td style="text-align: center;">0.758</td>
</tr>
<tr>
<td style="text-align: left;">PARENT-C</td>
<td style="text-align: center;">$0.776 \pm 0.05^{W}$</td>
<td style="text-align: center;">$0.755 \pm 0.12$</td>
<td style="text-align: center;">0.766</td>
</tr>
<tr>
<td style="text-align: left;">PARENT-W</td>
<td style="text-align: center;">$0.912 \pm 0.03$</td>
<td style="text-align: center;">$0.763 \pm 0.12$</td>
<td style="text-align: center;">0.838</td>
</tr>
<tr>
<td style="text-align: left;">PARENT*-C</td>
<td style="text-align: center;">$0.976 \pm 0.01$</td>
<td style="text-align: center;">$0.793 \pm 0.11$</td>
<td style="text-align: center;">0.885</td>
</tr>
<tr>
<td style="text-align: left;">PARENT*-W</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 2} \pm \mathbf{0 . 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 4} \pm \mathbf{0 . 1 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 3}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Correlation of metrics with human judgments on WikiBio. A superscript of $C / W$ indicates that the correlation is significantly lower than that of PARENTC/W using a bootstrap confidence test for $\alpha=0.1$.</p>
<h3>5.4 Correlation Comparison</h3>
<p>We use bootstrap sampling ( 500 iterations) over the 1100 tables for which we collected human annotations to get an idea of how the correlation of each metric varies with the underlying data. In each iteration, we sample with replacement, tables along with their references and all the generated texts for that table. Then we compute aggregated human evaluation and metric scores for each of the models and compute the correlation between the two. We report the average correlation across all bootstrap samples for each metric in Table 2. The distribution of correlations for the best performing metrics are shown in Figure 3.</p>
<p>Table 2 also indicates whether PARENT is significantly better than a baseline metric. Graham and Baldwin (2014) suggest using the William's test for this purpose, but since we are computing correlations between only $4 / 13$ systems at a time, this test has very weak power in our case. Hence, we use the bootstrap samples to obtain a $1-\alpha$ confidence interval of the difference in correlation</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: Distribution of metric correlations across 500 bootstrap samples. PRT $=$ PARENT.
between PARENT and any other metric and check whether this is above 0 (Wilcox, 2016).</p>
<p>Correlations are higher for the systems category than the hyperparams category. The latter is a more difficult setting since very similar models are compared, and hence the variance of the correlations is also high. Commonly used metrics which only rely on the reference (BLEU, ROUGE, METEOR, CIDEr) have only weak correlations with human judgments. In the hyperparams category, these are often negative, implying that tuning models based on these may lead to selecting worse models. BLEU performs the best among these, and adding n-grams from the table as references improves this further (BLEU-T).</p>
<p>Among the extractive evaluation metrics, CS, which also only relies on the reference, has poor correlation in the hyperparams category. RG-F, and both variants of the PARENT metric achieve the highest correlation for both settings. There is no significant difference among these for the hyperparams category, but for systems, PARENT-W is significantly better than the other two. While RG-F needs a full information extraction pipeline in its implementation, PARENT-C only relies on co-occurrence counts, and PARENT-W can be used out-of-the-box for any dataset. To our knowledge, this is the first rigorous evaluation of using information extraction for generation evaluation.</p>
<p>On this dataset, the word-overlap model showed higher correlation than the co-occurrence model for entailment. In $\S 5.6$ we will show that for the WebNLG dataset, where more paraphrasing is involved between the table and text, the opposite is true. Lastly, we note that the heuristic for selecting $\lambda$ is sufficient to produce high correlations for PARENT, however, if human annotations are available, this can be tuned to produce significantly higher correlations (PARENT*-W/C).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Correlation of the metrics to human judgment as the percentage of entailed examples in WikiBio is varied.</p>
<h3>5.5 Analysis</h3>
<p>In this section we further analyze the performance of PARENT-W ${ }^{10}$ under different conditions, and compare to the other best metrics from Table 2.</p>
<p>Effect of Divergence. To study the correlation as we vary the number of divergent references, we also collected binary labels from workers for whether a reference is entailed by the corresponding table. We define a reference as entailed when it mentions only information which can be inferred from the table. Each table and reference pair was judged by 3 independent workers, and we used the majority vote as the label for that pair. Overall, only $38 \%$ of the references were labeled as entailed by the table. Fleiss' $\kappa$ was 0.30 , which indicates a fair agreement. We found the workers sometimes disagreed on what information can be reasonably entailed by the table.</p>
<p>Figure 4 shows the correlations as we vary the percent of entailed examples in the evaluation set of WikiBio. Each point is obtained by fixing the desired proportion of entailed examples, and sampling subsets from the full set which satisfy this proportion. PARENT and RG-F remain stable and show a high correlation across the entire range, whereas BLEU and BLEU-T vary a lot. In the hyperparams category, the latter two have the worst correlation when the evaluation set contains only entailed examples, which may seem surprising. However, on closer examination we found that this subset tends to omit a lot of information from the tables. Systems which produce more information than these references are penalized by BLEU, but not in the human evaluation. PARENT overcomes this issue by measuring recall against the table in addition to the reference.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">BLEU-T</th>
<th style="text-align: center;">RG-F</th>
<th style="text-align: center;">PARENT-W</th>
<th style="text-align: center;">PARENT-C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">$0.567^{*}$</td>
<td style="text-align: center;">$0.588^{*}$</td>
<td style="text-align: center;">$0.598^{\ddagger}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 6}^{\ddagger}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy on making the same judgments as humans between pairs of generated texts. $p&lt;$ $0.01^{*} / 0.05^{\dagger} / 0.10^{\ddagger}$ : accuracy is significantly higher than the next best accuracy to the left using a paired McNemar's test.</p>
<p>Ablation Study. We check how different components in the computation of PARENT contribute to its correlation to human judgments. Specifically, we remove the probability $w(g)$ of an ngram $g$ being entailed by the table from Eqs. 2 and $5 .{ }^{11}$ The average correlation for PARENT-W drops to 0.168 in this case. We also try a variant of PARENT with $\lambda=0$, which removes the contribution of Table Recall (Eq. 4). The average correlation is 0.328 in this case. With these components, the correlation is 0.838 , showing that they are crucial to the performance of PARENT.</p>
<p>Sentence Level Discrimination. Chaganty et al. (2018) point out that hill-climbing on an automatic metric is meaningless if that metric has a low instance-level correlation to human judgments. In Table 3 we show the average accuracy of the metrics in making the same judgments as humans between pairs of generated texts. Both variants of PARENT are significantly better than the other metrics, however the best accuracy is only $60 \%$ for the binary task. This is a challenging task, since there are typically only subtle differences between the texts. Achieving higher instance-level accuracies will require more sophisticated language understanding models for evaluation.</p>
<h3>5.6 WebNLG Dataset</h3>
<p>To check how PARENT correlates with human judgments when the references are elicited from humans (and less likely to be divergent), we check its correlation with the human ratings provided for the systems competing in the WebNLG challenge (Gardent et al., 2017). The task is to generate text describing 1-5 RDF triples (e.g. John E Blaha, birthPlace, San Antonio), and human ratings were collected for the outputs of 9 participating systems on 223 instances. These systems include a mix of pipelined, statistical and neural methods. Each instance has upto 3 reference texts associated with</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Average pearson correlation across 500 bootstrap samples of each metric to human ratings for each aspect of the generations from the WebNLG challenge.
the RDF triples, which we use for evaluation.
The human ratings were collected on 3 distinct aspects - grammaticality, fluency and semantics, where semantics corresponds to the degree to which a generated text agrees with the meaning of the underlying RDF triples. We report the correlation of several metrics with these ratings in Table 4. ${ }^{12}$ Both variants of PARENT are either competitive or better than the other metrics in terms of the average correlation to all three aspects. This shows that PARENT is applicable for high quality references as well.</p>
<p>While BLEU has the highest correlation for the grammar and fluency aspects, PARENT does best for semantics. This suggests that the inclusion of source tables into the evaluation orients the metric more towards measuring the fidelity of the content of the generation. A similar trend is seen comparing BLEU and BLEU-T. As modern neural text generation systems are typically very fluent, measuring their fidelity is of increasing importance. Between the two entailment models, PARENTC is better due to its higher correlation with the grammaticality and fluency aspects.</p>
<p>Distribution of $\lambda$. The $\lambda$ parameter in the calculation of PARENT decides whether to compute recall against the table or the reference (Eq. 4). Figure 5 shows the distribution of the values taken by $1-\lambda$ using the heuristic described in $\S 3$ for instances in both WikiBio and WebNLG. For WikiBio, the recall of the references against the table is generally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence</p>
<p><sup id="fnref4:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set $1-\lambda$. Lower values indicate that the metric relies more on the table and less on the reference.
the recall of the generated text relies more on the reference.</p>
<h2>6 Related Work</h2>
<p>Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks the quality of metrics when table-to-text references are divergent. We show that in this case even system level correlations can be unreliable.</p>
<p>Hallucination (Rohrbach et al., 2018; Lee et al., 2018) refers to when an NLG system generates text which mentions extra information than what is present in the source from which it is generated. Divergence can be viewed as hallucination in the reference text itself. PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table.</p>
<p>PARENT draws inspiration from iBLEU (Sun and Zhou, 2012), a metric for evaluating paraphrase generation, which compares the generated text to both the source text and the reference.</p>
<p>While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics.</p>
<h2>7 Conclusions</h2>
<p>We study the automatic evaluation of table-to-text systems when the references diverge from the table. We propose a new metric, PARENT, which shows the highest correlation with humans across a range of settings with divergent references in WikiBio. We also perform the first empirical evaluation of information extraction based metrics (Wiseman et al., 2017), and find RG-F to be effective. Lastly, we show that PARENT is comparable to the best existing metrics when references are elicited by humans on the WebNLG data.</p>
<h2>Acknowledgements</h2>
<p>Bhuwan Dhingra is supported by a fellowship from Siemens, and by grants from Google. We thank Maruan Al-Shedivat, Ian Tenney, Tom Kwiatkowski, Michael Collins, Slav Petrov, Jason Baldridge, David Reitter and other members of the Google AI Language team for helpful discussions and suggestions. We thank Sam Wiseman for sharing data for an earlier version of this paper. We also thank the anonymous reviewers for their feedback.</p>
<h2>References</h2>
<p>Srinivas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation. In Proc. of INLG.</p>
<p>Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proc. of EMNLP.</p>
<p>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of nlg systems. In Proc. of EACL.</p>
<p>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-) evaluation of machine translation. In Proc. of WMT.</p>
<p>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluation the role of bleu in machine translation research. In Proc. of EACL.</p>
<p>Marine Carpuat, Yogarshi Vyas, and Xing Niu. 2017. Detecting cross-lingual semantic divergence for neural machine translation. In Proc.of Workshop on NMT.</p>
<p>Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proc. of ACL.</p>
<p>Boxing Chen and Colin Cherry. 2014. A systematic comparison of smoothing techniques for sentencelevel bleu. In Proc. of WMT.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177-190, Berlin, Heidelberg. Springer Berlin Heidelberg.</p>
<p>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proc. of WMT.</p>
<p>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proc. of HLT.</p>
<p>Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2019. Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG Challenge. arXiv preprint arXiv:1901.11528.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for micro-planners. In Proc. of ACL.</p>
<p>Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial Intelligence Research, 61:65-170.</p>
<p>Oren Glickman and Ido Dagan. 2005. A probabilistic setting and lexical cooccurrence model for textual entailment. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 43-48. Association for Computational Linguistics.</p>
<p>Yvette Graham and Timothy Baldwin. 2014. Testing for significance of increased correlation with human judgment. In Proc. of EMNLP.</p>
<p>Lifu Huang, Avirup Sil, Heng Ji, and Radu Florian. 2017. Improving slot filling performance with attentive neural networks on dependency structures.</p>
<p>Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis, and Erkut Erdem. 2017. Re-evaluating automatic metrics for image captioning. In Proc. of EACL.</p>
<p>Karen Kukich. 1983. Design of a knowledge-based report generator. In Proc. of ACL.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proc. of EMNLP.</p>
<p>Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David Sussillo. 2018. Hallucinations in neural machine translation.</p>
<p>Percy Liang, Michael I Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proc. of ACL.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proc. of Workshop on Text Summarization Branches Out.</p>
<p>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. PEM: A paraphrase evaluation metric exploiting parallel texts. In Proc. of EMNLP.</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proc. of EMNLP.</p>
<p>Kathleen R. McKeown. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press, New York, NY, USA.</p>
<p>Preksha Nema and Mitesh M Khapra. 2018. Towards a better metric for evaluating question generation systems. In Proc. of EMNLP.</p>
<p>Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, and Verena Rieser. 2017a. Why we need new evaluation metrics for NLG. In Proc. of EMNLP.</p>
<p>Jekaterina Novikova, Ondřej Dušek, and Verena Rieser. 2017b. The E2E dataset: New challenges for end-to-end generation. In Proc. of SIGDIAL.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proc. of ACL.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Ehud Reiter. 2018. A structured review of the validity of bleu. Computational Linguistics, pages 1-12.</p>
<p>Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. Nat. Lang. Eng., 3(1):57-87.</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in image captioning. EMNLP.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proc. of ACL.</p>
<p>Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In Proc. of CICLing.</p>
<p>Hong Sun and Ming Zhou. 2012. Joint learning of a dual smt system for paraphrase generation. In Proc. of $A C L$.</p>
<p>Kristi Tsukida and Maya R Gupta. 2011. How to analyze paired comparison data. Technical report, Washington University Seattle Dept of Electrical Engineering.</p>
<p>Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566-4575.</p>
<p>Yogarshi Vyas, Xing Niu, and Marine Carpuat. 2018. Identifying semantic divergences in parallel text without annotations. In Proc. of NAACL.</p>
<p>Rand R Wilcox. 2016. Comparing dependent robust correlations. British Journal of Mathematical and Statistical Psychology, 69(3):215-224.</p>
<p>Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document generation. In Proc. of EMNLP.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, ĀĄukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.</p>
<p>Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Optimizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401-415.</p>
<h2>A Appendices</h2>
<h2>A. 1 Information Extraction System</h2>
<p>For evaluation via information extraction (Wiseman et al., 2017) we train a model for WikiBio which accepts text as input and generates a table as the output. Tables in WikiBio are open-domain, without any fixed schema for which attributes may be present or absent in an instance. Hence we</p>
<p>Text:
michael dahlquist ( december 22 , 1965 - july 14 , 2005 ) was a drummer in the seattle band silkworm .
Table:
name $&lt;\mathrm{C}&gt;$ michael dahlquist $&lt;\mathrm{R}&gt;$ birth date $&lt;\mathrm{C}&gt;22$ december $1965&lt;\mathrm{R}&gt;$ birth place $&lt;\mathrm{C}&gt;$ seattle, washington $&lt;\mathrm{R}&gt;$ death date $&lt;\mathrm{C}&gt;14$ july $2005&lt;\mathrm{R}&gt;$ death place $&lt;\mathrm{C}&gt;$ skokie , illinois $&lt;\mathrm{R}&gt;$ genres $&lt;\mathrm{C}&gt;$ male $&lt;\mathrm{R}&gt;$ occupation(s) $&lt;\mathrm{C}&gt;$ drummer $&lt;\mathrm{R}&gt;$ instrument $&lt;\mathrm{C}&gt;$ drums</p>
<p>Figure 6: An input-output pair for the information extraction system. $&lt;\mathrm{R}&gt;$ and $&lt;\mathrm{C}&gt;$ are special symbols used to separate (attribute, value) pairs and attributes from values, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F-score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.351</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of the Information Extraction system.
employ the Pointer-Generator Network (PG-Net) (See et al., 2017) for this purpose. Specifically, we use a sequence-to-sequence model, whose encoder and decoder are both single-layer bi-directional LSTMs. The decoder is augmented with an attention mechanism over the states of the encoder. Further, it also uses a copy mechanism to optionally copy tokens directly from the source text. We do not use the coverage mechanism of See et al. (2017) since that is specific to the task of summarization they study. The decoder is trained to produce a linearized version of the table where the rows and columns are flattened into a sequence, and separate by special tokens. Figure 6 shows an example.</p>
<p>Clearly, since the references are divergent, the model cannot be expected to produce the entire table, and we see some false information being hallucinated after training. Nevertheless, as we show in $\S 5.4$, this system can be used for evaluating generated texts. After training, we can parse the output sequence along the special tokens $&lt;\mathrm{R}&gt;$ and $&lt;\mathrm{C}&gt;$ to get a set of (attribute, value) pairs. Table 5 shows the precision, recall and F-score of these extracted pairs against the ground truth tables, where the attributes and values are compared using an exact string match.</p>
<h2>A. 2 Hyperparameters</h2>
<p>After tuning we found the same set of hyperparameters to work well for both the table-to-text PG-Net, and the inverse information extraction PG-Net. The hidden state size of the biLSTMs</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reference</th>
<th style="text-align: center;">vedran nikÁqiÁG ( born 5 may 1987 in osijek ) is a croatian football striker . [STOP]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prediction</td>
<td style="text-align: center;">vedran nikÁqiÁG ( born 5 may 1987 ) is a croatian football forward who is currently a free agent . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">adam whitehead ( born 28 march 1980 ) is a former breaststroke swimmer from coventry, england, who competed at the 2000 <br> summer olympics in sydney, australia . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Prediction</td>
<td style="text-align: center;">adam whitehead ( born 28 march 1980 ) is an english swimmer . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">chris fortier is an american dj and founder of the balance record pool as well as co-founder and owner of fade records . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Prediction</td>
<td style="text-align: center;">chris fortier ( born in melbourne, florida ) is an american disc jockey and record producer from melbourne, florida . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">pretty balanced was an american band based in columbus, ohio . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Prediction</td>
<td style="text-align: center;">pretty balanced is an american piano band from columbus, ohio . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">ben street ( born february 13, 1987 ) is a canadian professional ice hockey player who is a member within the colorado avalanche <br> organization of the national hockey league . [STOP]</td>
</tr>
<tr>
<td style="text-align: center;">Prediction</td>
<td style="text-align: center;">ben street ( born february 13, 1987 ) is a canadian professional ice hockey centre currently playing for the colorado avalanche of <br> the national hockey league ( nhl ) . [STOP]</td>
</tr>
</tbody>
</table>
<p>Table 6: Sample references and predictions from PG-Net with beam size 8. Information which is absent from the reference, but can be inferred from the table is in bold. Information which is present in the reference, but cannot be inferred from the table is in italics.
was set to 200. The input and output vocabularies were set to 50000 most common words in the corpus, with additional special symbols for table attribute names (such as "birth-date"). The embeddings of the tokens in the vocabulary were initialized with Glove (Pennington et al., 2014). Learning rate of 0.0003 was used during training, with the Adam optimizer, and a dropout of 0.2 was also applied to the outputs of the biLSTM. Models were trained till the loss on the dev set stopped dropping. Maximum length of a decoded text was set to 40 tokens, and that of the tables was set to 120 tokens. Various beam sizes and length normalization penalties were applied for the table-totext system, which are listed in the main paper. For the information extraction system, we found a beam size of 8 and no length penalty to produce the highest F-score on the dev set.</p>
<h1>A. 3 Sample Outputs</h1>
<p>Table 6 shows some sample references and the corresponding predictions from the best performing model, PG-Net for WikiBio.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ When computing precision we set $w(g)=0$, and when computing recall we set $w(g)=1$ for all $g$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{12}$ We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
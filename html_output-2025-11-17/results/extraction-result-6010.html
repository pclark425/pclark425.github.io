<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6010 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6010</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6010</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-265309034</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.12315v1.pdf" target="_blank">AcademicGPT: Empowering Academic Research</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Yet, many of these advanced LLMs are tailored for broad, general-purpose applications. In this technical report, we introduce AcademicGPT, designed specifically to empower academic research. AcademicGPT is a continual training model derived from LLaMA2-70B. Our training corpus mainly consists of academic papers, thesis, content from some academic domain, high-quality Chinese data and others. While it may not be extensive in data scale, AcademicGPT marks our initial venture into a domain-specific GPT tailored for research area. We evaluate AcademicGPT on several established public benchmarks such as MMLU and CEval, as well as on some specialized academic benchmarks like PubMedQA, SCIEval, and our newly-created ComputerScienceQA, to demonstrate its ability from general knowledge ability, to Chinese ability, and to academic ability. Building upon AcademicGPT's foundation model, we also developed several applications catered to the academic area, including General Academic Question Answering, AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract Generation.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6010.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6010.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AcademicGPT (continual-pretrained LLaMA2 for academic research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific LLM produced by continual pretraining of LLaMA2-70B on an academic-centric corpus (≈120B tokens) and additional Chinese data, aimed at improving academic understanding and downstream academic applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AcademicGPT (foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A continual-pretraining adaptation of LLaMA2-70B focused on academic text and Chinese corpora to produce a foundation model better suited for academic tasks and downstream applications.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLaMA2-70B continual-pretrained → AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Pretraining corpus of ~120 billion tokens composed primarily of academic papers, theses, ArXiv (≈2.26M papers until May 2023), 1M parsed research papers, 1M theses crawled from ~200 universities, Unpaywall (open-access articles ~48M entries filtered), filtered Falcon-RefinedWeb academic content, English Wiki, PubMed, Semantic Scholar metadata and additional cleaned Chinese corpora (CommonCrawl subsets, Baike, books).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a separate distillation pipeline; domain adaptation via continual pretraining on curated academic corpora (language-model weight updates) to internalize academic knowledge and Chinese language usage.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>A generative foundation LLM able to produce answers, summaries, reviews, titles/abstracts and act as the brain for downstream academic tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Standard LLM benchmarks and domain-specific benchmarks (5-shot MMLU, 5-shot CEval, PubMedQA 1k expert-annotated 5-shot, SCIEval 3-shot, ComputerScienceQA 3-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Improved domain performance on several academic benchmarks: MMLU averaged 0.688 (vs LLaMA2-70B 0.693, LLaMA-65B 0.634, ChatGPT 0.664); CEval 55.1% (AcademicGPT) vs 50.8% (LLaMA2); PubMedQA accuracy 0.806 (AcademicGPT) surpassing LLaMA1/2 and ChatGPT and slightly above GPT-4-Base; SCIEval average 0.688 (AcademicGPT) vs 0.636 (LLaMA2) and 0.679 (ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>MMLU, CEval, PubMedQA, SCIEval, ComputerScienceQA (newly created), training corpora described above (ArXiv, Unpaywall, OpenReview-derived corpora, Nougat-parsed theses).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality and domain coverage depend on data cleaning; continual pretraining led to gains in heavily represented topics but modest drops in underrepresented categories; large compute cost (training used 192 A100 40GB GPUs for ~37 days); Chinese data had to be heavily cleaned; model remains imperfect on certain subdomains vs expert systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Direct benchmark comparisons reported against LLaMA-65B, LLaMA2-70B, ChatGPT (gpt-3.5-turbo) and GPT-4 (where available). AcademicGPT shows domain-specific improvements (PubMedQA, SCIEval, ComputerScienceQA) while retaining similar general capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6010.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AcademicGPT-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AcademicGPT-powered General Academic Question Answering Agent (ReAct-style, tool-enabled)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent architecture using an instruction-finetuned AcademicGPT as the 'brain', augmented with planning/action, memory, and external tools (AcademicSearch KG, web search) using a modified ReAct prompting scheme that emits JSON actions to call tools and iteratively compose answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AcademicGPT-powered Agent (General Academic Q&A)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-empowered agent that integrates AcademicGPT (instruction-finetuned) with an action/planning module (inspired by ReAct), short-term dialogue memory, and external tools (elastic-search knowledge-graph and web search) to perform multi-turn academic Q&A, paper retrieval, concept explanation and multi-paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Instruction-finetuned AcademicGPT (based on LLaMA2-70B continual-pretrained AcademicGPT); instruction-finetune data includes cleaned Wizard, LIMA, Alpaca (Chinese & English) and 384 tool-usage instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>User query (multi-turn), optionally paper content or paper-selection; the backend tools query an indexed academic KG covering metadata (authors, title, abstract, publish date, venue, citations) derived from their corpus; the KG supports fuzzy queries and sorting. No explicit global count of papers queried per request is fixed—retrieval is from the full academic corpus indexed in ES.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Retrieval-augmented synthesis: the agent uses the LLM to reason and plan, issues retrieval actions (AcademicSearch, WebSearchEngine) to gather evidence from many papers, and synthesizes answers via chain-of-thought / ReAct inner monologue, with action outputs encoded as JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured multi-turn answers and action traces; final answers are natural-language responses; intermediate outputs are JSON tool-calls and retrieved metadata snippets; can also provide paper recommendations and concise summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Qualitative case studies illustrated in figures (paper recommendation, concept explanation). Benchmark-style quantitative evaluation not reported specifically for the agent as a whole; underlying AcademicGPT evaluated on standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Demonstrated capability on example cases (paper recommendation, concept explaining); no aggregate numeric agent-level evaluation reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Underlying corpora (ArXiv, Unpaywall, parsed theses, PubMed, PapersWithCode metadata) indexed into the ES-based AcademicSearch; no separate agent benchmark reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires high-quality retrieval index and cleaned metadata; depends on retrieval precision for correctness; potential for hallucinations if retrieved evidence is incomplete or noisy; engineering cost to keep tools and indexes up-to-date.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Design inspiration drawn from ReAct and other LLM-agent works. Differences: action outputs are strict JSON with single-tool-per-action constraint; no direct quantitative comparisons to other agent frameworks provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6010.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-assisted Paper Reading</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-assisted Paper Reading (long-context AcademicGPT pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline to enable long-context interactive reading: (1) continue pretraining with an NTK-aware method to extend context window (to 32K tokens), and (2) supervised finetuning on LongAlpaca-12k to produce long instruction-following behavior for reading and QA over papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-assisted Paper Reading (long-context AcademicGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A long-context reading system built by extending AcademicGPT's context window via NTK-aware continuation training and then finetuning on long-instruction data (LongAlpaca-12k) so the model can answer interactive QA about long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>AcademicGPT continual-pretrained model extended via NTK-aware technique; then supervised finetuned using LongAlpaca-12k.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Stage 1: continued pretraining on 5B tokens sampled from the training corpus to facilitate extension to 32K context (NTK-aware); Stage 2: supervised fine-tune on LongAlpaca-12k dataset (9k long QA + 3k short QA), where each sample contains an instruction and a long context (e.g., segments of books or papers).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Long-context training + supervised instruction fine-tuning so the LLM can extract and synthesize information from long documents; retrieval is not the main mechanism—model ingests large context windows directly after NTK-based extension.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Interactive QA responses, summaries and explanations about long papers; example outputs demonstrate multi-turn question answering and extracted future directions or model workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Demonstrative case studies are provided (example Q&A and reading sessions). No specific large-scale benchmark reported for the reading pipeline itself; underlying finetune uses LongAlpaca-12k.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Case examples illustrate plausible, coherent reading assistance (summaries, workflows); no numeric benchmark results for long-document QA reported in this technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>LongAlpaca-12k (used in stage 2); NTK-based extension references and techniques (cited Chen et al., Peng et al. references).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Long-context training is compute-intensive; extending context required specialized NTK-aware techniques and additional continued training (~5B tokens sampled); alternative engineering approach suggested (use a smaller model to extract context then query AcademicGPT) indicating trade-offs between end-to-end long-context training cost and multi-model pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>References to other long-context extension methods (Rozière et al., Xiong et al., YaRN) and mention of alternative practical approaches (small extractor LLM + AcademicGPT) but no head-to-head quantitative comparison in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6010.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperReview-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper Review System (supervised-finetuned AcademicGPT for reviewing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuned AcademicGPT system trained on OpenReview-derived papers and reviews (with ReviewAdvisor annotations) to generate paper reviews and final recommendation predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AcademicGPT Paper Review (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A supervised finetuned AcademicGPT trained to act as a professional reviewer: input is a paper content and the model outputs a structured review covering strengths, concerns, and an accept/reject style recommendation. Training labels derive from OpenReview reviews and automated ReviewAdvisor aspect annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SFT variant of AcademicGPT (max context length increased to 8,192 tokens compared to the base 4,096), finetuned on review data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Training corpus: scraped 29,119 papers and 79,000 reviews from OpenReview; after filtering, 22,213 papers with 67,874 review comments used for training and 500 papers with 1,513 review comments for testing. SFT format: input = 'This is the paper for your review: Paper Content', output = review comment.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Supervised finetuning on human reviews, aided by ReviewAdvisor-style aspect labeling (clarity, originality, soundness, etc.) to teach the model to produce structured review outputs; model learns to map paper content → review text and recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Full review text including strengths, weaknesses, recommendation and aspect-based comments; numeric recommendation (final accept/reject) is predicted and evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Three metrics: recommendation accuracy (alignment with meta-review recommendation), aspect accuracy (whether aspects reported by the model appear in the meta-review), and aspect recall (fraction of meta-review aspects recalled). Comparisons are made with human reviewers' consistency vs meta-reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Recommendation accuracy: 68.4% (AcademicGPT) vs 85.2% (human reviewers). Aspect accuracy: 76.4% vs 81.6% (human). Aspect recall: 24.8% vs 26.0% (human). Authors conclude the model has some reviewing capability but a noticeable gap remains relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>OpenReview scraped dataset (22,213 papers/67,874 reviews training; 500 papers/1,513 reviews test); ReviewAdvisor code used for labeling aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Significant gap vs human reviewers in recommendation accuracy and recall of evaluation aspects; noisy or inconsistent review texts required filtering; removed reviews that were too short/long or inconsistent; model cannot fully replace expert reviewers but can provide reference comments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared primarily to human reviewers (meta-review alignment). The paper references ReviewAdvisor and prior work concluding LMs lacked reviewing capabilities; here the SFT model shows some capability but inferior to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6010.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Title/Abstract Gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-assisted Title and Abstract Generation (supervised AcademicGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised generation system that produces paper titles and abstracts from the introduction section, trained on a corpus of ~1M research papers to assist authors and produce concise summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-assisted Title & Abstract Generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Supervised-finetuned AcademicGPT that maps an input 'Introduction (+ optional experiments/results)' to a generated Title and Abstract, using a special token '<begin generate>' to trigger generation and trained on ~1M papers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Supervised-finetuned AcademicGPT (foundation AcademicGPT model finetuned on paired introduction→(title,abstract) examples).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Training corpus: approximately 1 million research papers; input during training is the introduction (and optionally experiments/results) segments; prompt format: 'Introduction Experiments Results <begin generate> Title:...; Abstract:...'.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Supervised sequence-to-sequence style generation (instruction-following) where the model learns to condense an introduction into title and abstract (extractive/abstractive summarization style).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated Title and Abstract text for a given paper introduction; used to assist authors or produce concise summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Qualitative case examples comparing generated title/abstract to ground truth; no large-scale automatic metric results (e.g., ROUGE/BLEU) reported in the technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Example cases show coherent, relevant title and abstract generations that are generally close to ground truth; no aggregate numeric evaluation provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom supervised dataset derived from ~1M research papers (introduction→title+abstract pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>No reported large-scale automatic evaluation; potential for missing important experimental specifics if only introduction is provided; generation quality depends on input completeness and domain coverage of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>No direct quantitative comparison to other title/abstract generation methods in the paper; method is positioned as a practical application built on AcademicGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6010.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AcademicSearch (ES-KG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AcademicSearch — ElasticSearch-based Knowledge Graph (Academic KG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval tool (used as an agent tool) that indexes paper metadata (authors, title, abstract, publication date, venue, citation counts, references) for fuzzy search and logical sorting to support retrieval-augmented synthesis by the AcademicGPT agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AcademicSearch (ElasticSearch Knowledge Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An ES-backed knowledge store containing bibliographic and paper metadata that supports fuzzy matching on abstracts, authors, titles, fields of study, dates and venues; used by the agent to retrieve supporting evidence and recommend similar papers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Tool used by AcademicGPT agent; not itself an LLM. It is queried by the AcademicGPT / agent pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Indexed metadata from the authors' full academic corpus (ArXiv, Unpaywall, parsed theses, PubMed, PapersWithCode). Query inputs are structured JSON fields (abstracts, authors, fieldOfStudy, date ranges, title, venue) and resultParameters list specifying which metadata fields to return.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Knowledge aggregation via structured indexing and retrieval; supports evidence gathering for LLM synthesis rather than distillation per se.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured metadata results (authors, abstracts, publishDate, title, venue, citationCount) returned to the agent for synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not separately benchmarked in the report; utility demonstrated via agent example cases (paper recommendation, retrieval for QA).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Used effectively in case studies for retrieval to support AcademicGPT answers and paper recommendations; no standalone metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Built over the paper corpus described in the paper (ArXiv, Unpaywall, parsed theses, PubMed, PapersWithCode metadata).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality of retrieval depends on upstream data cleaning and indexing; fuzzy matching and ranking heuristics are critical; potential for retrieval of outdated or low-quality papers if filters are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>No head-to-head comparison to other retrieval/index systems provided; design choices (ES-based KG with fuzzy search) motivated by practicality for academic metadata retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6010.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning+action framework) — adapted variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-and-action prompting framework that interleaves chain-of-thought style reasoning with explicit actions (tool calls); in this work, the authors adopt ReAct principles but enforce JSON-formatted single-action-per-step outputs for tool calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct (adapted JSON-action variant)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The authors adapt the ReAct paradigm for their AcademicGPT agent: the model emits an inner monologue and selects discrete actions, but action outputs are constrained to a strict JSON blob (single tool action per step) specifying tool name and parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used with instruction-finetuned AcademicGPT as the reasoning engine.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>User query and multi-turn dialogue context; ReAct-style prompt templates and tool specification prompts (AcademicSearch and WebSearchEngine) guide action choices.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a distillation method—ReAct is used to structure LLM interaction with retrieval tools enabling multi-step evidence gathering and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Interleaved reasoning traces (Thought) and JSON actions (Action) followed by Observations and eventual final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Demonstrated via examples and described agent behavior; no formal evaluation of ReAct adaptation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Enables the agent to plan and call retrieval tools in a structured manner; demonstrated case studies show successful retrieval+answer flows.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Built-in prompt templates and tool APIs; underlying corpora for retrieval are the same academic corpora indexed in AcademicSearch.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Enforcing strict single-action JSON may reduce multi-query expressiveness per step; requires well-designed prompts and tool schemas to avoid malformed actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Authors cite ReAct as inspiration and note the main difference: action outputs are strict JSON for API usage; no quantitative comparison to vanilla ReAct.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6010.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComputerScienceQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ComputerScienceQA (benchmark derived from PapersWithCode)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A newly created multiple-choice QA benchmark focused on computer science methods and datasets, constructed from PapersWithCode metadata to evaluate a model's knowledge of methods and datasets across CS subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ComputerScienceQA (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A curated evaluation dataset comprising 9,686 multiple-choice questions (1,885 about methods and 7,801 about datasets) built from PapersWithCode descriptions to test model comprehension of CS methods and dataset descriptions (constructed to prevent leakage by masking method names in descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used to evaluate AcademicGPT, LLaMA-65B, LLaMA2-70B and ChatGPT (gpt-3.5); not itself an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>3-shot evaluation protocol; question format masks method names in descriptions and provides multiple-choice options derived from other methods/datasets in the same domain; total dataset size = 9,686 questions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Benchmarking of knowledge synthesis capacity (no distillation algorithm) — tests whether an LLM can map descriptions to method/dataset names and synthesize domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multiple-choice answers (method/dataset identification).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>3-shot accuracy compared across models (AcademicGPT, LLaMA variants, ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>AcademicGPT achieved overall 83.5% vs LLaMA2 79.9% and ChatGPT 81.1% (per Table 7), with improvements in several subcategories.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Derived from PapersWithCode (methods and datasets metadata).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Dataset construction required masking method names to avoid leakage; multiple-choice format simplifies evaluation but may not capture deeper synthesis or reasoning across multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared model accuracies against LLaMA-65B, LLaMA2-70B and ChatGPT in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6010.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6010.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific LLM tailored to scientific research (cited in related work) that aims to streamline inquiry and discovery across scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Galactica (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a prior effort to build a scientific-domain LLM; used as inspiration/motivation for AcademicGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Galactica (as reported in Taylor et al., 2022) — external work referenced, not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Mentioned as a domain-specific LLM (no operational details provided in this technical report beyond citation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Cited as prior art; no critique in this report beyond being an initial motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Positioned among domain-specific LLMs; AcademicGPT is described as inspired by Galactica.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AcademicGPT: Empowering Academic Research', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Review Advisor (Can we automate scientific reviewing?) <em>(Rating: 2)</em></li>
                <li>Long Alpaca / LongLLaMA (Long-context instruction-following models / LongLora references) <em>(Rating: 2)</em></li>
                <li>Extending context window of large language models via positional interpolation <em>(Rating: 1)</em></li>
                <li>YaRN: Efficient context window extension of large language models <em>(Rating: 1)</em></li>
                <li>Nougat: Neural optical understanding for academic documents <em>(Rating: 1)</em></li>
                <li>PapersWithCode <em>(Rating: 1)</em></li>
                <li>PubMedQA: A dataset for biomedical research question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6010",
    "paper_id": "paper-265309034",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "AcademicGPT",
            "name_full": "AcademicGPT (continual-pretrained LLaMA2 for academic research)",
            "brief_description": "A domain-specific LLM produced by continual pretraining of LLaMA2-70B on an academic-centric corpus (≈120B tokens) and additional Chinese data, aimed at improving academic understanding and downstream academic applications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AcademicGPT (foundation model)",
            "system_description": "A continual-pretraining adaptation of LLaMA2-70B focused on academic text and Chinese corpora to produce a foundation model better suited for academic tasks and downstream applications.",
            "llm_model_used": "LLaMA2-70B continual-pretrained → AcademicGPT",
            "input_type_and_size": "Pretraining corpus of ~120 billion tokens composed primarily of academic papers, theses, ArXiv (≈2.26M papers until May 2023), 1M parsed research papers, 1M theses crawled from ~200 universities, Unpaywall (open-access articles ~48M entries filtered), filtered Falcon-RefinedWeb academic content, English Wiki, PubMed, Semantic Scholar metadata and additional cleaned Chinese corpora (CommonCrawl subsets, Baike, books).",
            "distillation_approach": "Not a separate distillation pipeline; domain adaptation via continual pretraining on curated academic corpora (language-model weight updates) to internalize academic knowledge and Chinese language usage.",
            "output_type": "A generative foundation LLM able to produce answers, summaries, reviews, titles/abstracts and act as the brain for downstream academic tools.",
            "evaluation_methods": "Standard LLM benchmarks and domain-specific benchmarks (5-shot MMLU, 5-shot CEval, PubMedQA 1k expert-annotated 5-shot, SCIEval 3-shot, ComputerScienceQA 3-shot).",
            "results": "Improved domain performance on several academic benchmarks: MMLU averaged 0.688 (vs LLaMA2-70B 0.693, LLaMA-65B 0.634, ChatGPT 0.664); CEval 55.1% (AcademicGPT) vs 50.8% (LLaMA2); PubMedQA accuracy 0.806 (AcademicGPT) surpassing LLaMA1/2 and ChatGPT and slightly above GPT-4-Base; SCIEval average 0.688 (AcademicGPT) vs 0.636 (LLaMA2) and 0.679 (ChatGPT).",
            "datasets_or_benchmarks": "MMLU, CEval, PubMedQA, SCIEval, ComputerScienceQA (newly created), training corpora described above (ArXiv, Unpaywall, OpenReview-derived corpora, Nougat-parsed theses).",
            "challenges_or_limitations": "Quality and domain coverage depend on data cleaning; continual pretraining led to gains in heavily represented topics but modest drops in underrepresented categories; large compute cost (training used 192 A100 40GB GPUs for ~37 days); Chinese data had to be heavily cleaned; model remains imperfect on certain subdomains vs expert systems.",
            "comparisons_to_other_methods": "Direct benchmark comparisons reported against LLaMA-65B, LLaMA2-70B, ChatGPT (gpt-3.5-turbo) and GPT-4 (where available). AcademicGPT shows domain-specific improvements (PubMedQA, SCIEval, ComputerScienceQA) while retaining similar general capabilities.",
            "uuid": "e6010.0",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "AcademicGPT-Agent",
            "name_full": "AcademicGPT-powered General Academic Question Answering Agent (ReAct-style, tool-enabled)",
            "brief_description": "An agent architecture using an instruction-finetuned AcademicGPT as the 'brain', augmented with planning/action, memory, and external tools (AcademicSearch KG, web search) using a modified ReAct prompting scheme that emits JSON actions to call tools and iteratively compose answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AcademicGPT-powered Agent (General Academic Q&A)",
            "system_description": "An LLM-empowered agent that integrates AcademicGPT (instruction-finetuned) with an action/planning module (inspired by ReAct), short-term dialogue memory, and external tools (elastic-search knowledge-graph and web search) to perform multi-turn academic Q&A, paper retrieval, concept explanation and multi-paper comparisons.",
            "llm_model_used": "Instruction-finetuned AcademicGPT (based on LLaMA2-70B continual-pretrained AcademicGPT); instruction-finetune data includes cleaned Wizard, LIMA, Alpaca (Chinese & English) and 384 tool-usage instructions.",
            "input_type_and_size": "User query (multi-turn), optionally paper content or paper-selection; the backend tools query an indexed academic KG covering metadata (authors, title, abstract, publish date, venue, citations) derived from their corpus; the KG supports fuzzy queries and sorting. No explicit global count of papers queried per request is fixed—retrieval is from the full academic corpus indexed in ES.",
            "distillation_approach": "Retrieval-augmented synthesis: the agent uses the LLM to reason and plan, issues retrieval actions (AcademicSearch, WebSearchEngine) to gather evidence from many papers, and synthesizes answers via chain-of-thought / ReAct inner monologue, with action outputs encoded as JSON.",
            "output_type": "Structured multi-turn answers and action traces; final answers are natural-language responses; intermediate outputs are JSON tool-calls and retrieved metadata snippets; can also provide paper recommendations and concise summaries.",
            "evaluation_methods": "Qualitative case studies illustrated in figures (paper recommendation, concept explanation). Benchmark-style quantitative evaluation not reported specifically for the agent as a whole; underlying AcademicGPT evaluated on standard benchmarks.",
            "results": "Demonstrated capability on example cases (paper recommendation, concept explaining); no aggregate numeric agent-level evaluation reported in the paper.",
            "datasets_or_benchmarks": "Underlying corpora (ArXiv, Unpaywall, parsed theses, PubMed, PapersWithCode metadata) indexed into the ES-based AcademicSearch; no separate agent benchmark reported.",
            "challenges_or_limitations": "Requires high-quality retrieval index and cleaned metadata; depends on retrieval precision for correctness; potential for hallucinations if retrieved evidence is incomplete or noisy; engineering cost to keep tools and indexes up-to-date.",
            "comparisons_to_other_methods": "Design inspiration drawn from ReAct and other LLM-agent works. Differences: action outputs are strict JSON with single-tool-per-action constraint; no direct quantitative comparisons to other agent frameworks provided.",
            "uuid": "e6010.1",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "AI-assisted Paper Reading",
            "name_full": "AI-assisted Paper Reading (long-context AcademicGPT pipeline)",
            "brief_description": "A two-stage pipeline to enable long-context interactive reading: (1) continue pretraining with an NTK-aware method to extend context window (to 32K tokens), and (2) supervised finetuning on LongAlpaca-12k to produce long instruction-following behavior for reading and QA over papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI-assisted Paper Reading (long-context AcademicGPT)",
            "system_description": "A long-context reading system built by extending AcademicGPT's context window via NTK-aware continuation training and then finetuning on long-instruction data (LongAlpaca-12k) so the model can answer interactive QA about long documents.",
            "llm_model_used": "AcademicGPT continual-pretrained model extended via NTK-aware technique; then supervised finetuned using LongAlpaca-12k.",
            "input_type_and_size": "Stage 1: continued pretraining on 5B tokens sampled from the training corpus to facilitate extension to 32K context (NTK-aware); Stage 2: supervised fine-tune on LongAlpaca-12k dataset (9k long QA + 3k short QA), where each sample contains an instruction and a long context (e.g., segments of books or papers).",
            "distillation_approach": "Long-context training + supervised instruction fine-tuning so the LLM can extract and synthesize information from long documents; retrieval is not the main mechanism—model ingests large context windows directly after NTK-based extension.",
            "output_type": "Interactive QA responses, summaries and explanations about long papers; example outputs demonstrate multi-turn question answering and extracted future directions or model workflows.",
            "evaluation_methods": "Demonstrative case studies are provided (example Q&A and reading sessions). No specific large-scale benchmark reported for the reading pipeline itself; underlying finetune uses LongAlpaca-12k.",
            "results": "Case examples illustrate plausible, coherent reading assistance (summaries, workflows); no numeric benchmark results for long-document QA reported in this technical report.",
            "datasets_or_benchmarks": "LongAlpaca-12k (used in stage 2); NTK-based extension references and techniques (cited Chen et al., Peng et al. references).",
            "challenges_or_limitations": "Long-context training is compute-intensive; extending context required specialized NTK-aware techniques and additional continued training (~5B tokens sampled); alternative engineering approach suggested (use a smaller model to extract context then query AcademicGPT) indicating trade-offs between end-to-end long-context training cost and multi-model pipelines.",
            "comparisons_to_other_methods": "References to other long-context extension methods (Rozière et al., Xiong et al., YaRN) and mention of alternative practical approaches (small extractor LLM + AcademicGPT) but no head-to-head quantitative comparison in this report.",
            "uuid": "e6010.2",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PaperReview-SFT",
            "name_full": "Paper Review System (supervised-finetuned AcademicGPT for reviewing)",
            "brief_description": "A supervised fine-tuned AcademicGPT system trained on OpenReview-derived papers and reviews (with ReviewAdvisor annotations) to generate paper reviews and final recommendation predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AcademicGPT Paper Review (SFT)",
            "system_description": "A supervised finetuned AcademicGPT trained to act as a professional reviewer: input is a paper content and the model outputs a structured review covering strengths, concerns, and an accept/reject style recommendation. Training labels derive from OpenReview reviews and automated ReviewAdvisor aspect annotations.",
            "llm_model_used": "SFT variant of AcademicGPT (max context length increased to 8,192 tokens compared to the base 4,096), finetuned on review data.",
            "input_type_and_size": "Training corpus: scraped 29,119 papers and 79,000 reviews from OpenReview; after filtering, 22,213 papers with 67,874 review comments used for training and 500 papers with 1,513 review comments for testing. SFT format: input = 'This is the paper for your review: Paper Content', output = review comment.",
            "distillation_approach": "Supervised finetuning on human reviews, aided by ReviewAdvisor-style aspect labeling (clarity, originality, soundness, etc.) to teach the model to produce structured review outputs; model learns to map paper content → review text and recommendation.",
            "output_type": "Full review text including strengths, weaknesses, recommendation and aspect-based comments; numeric recommendation (final accept/reject) is predicted and evaluated.",
            "evaluation_methods": "Three metrics: recommendation accuracy (alignment with meta-review recommendation), aspect accuracy (whether aspects reported by the model appear in the meta-review), and aspect recall (fraction of meta-review aspects recalled). Comparisons are made with human reviewers' consistency vs meta-reviewers.",
            "results": "Recommendation accuracy: 68.4% (AcademicGPT) vs 85.2% (human reviewers). Aspect accuracy: 76.4% vs 81.6% (human). Aspect recall: 24.8% vs 26.0% (human). Authors conclude the model has some reviewing capability but a noticeable gap remains relative to humans.",
            "datasets_or_benchmarks": "OpenReview scraped dataset (22,213 papers/67,874 reviews training; 500 papers/1,513 reviews test); ReviewAdvisor code used for labeling aspects.",
            "challenges_or_limitations": "Significant gap vs human reviewers in recommendation accuracy and recall of evaluation aspects; noisy or inconsistent review texts required filtering; removed reviews that were too short/long or inconsistent; model cannot fully replace expert reviewers but can provide reference comments.",
            "comparisons_to_other_methods": "Compared primarily to human reviewers (meta-review alignment). The paper references ReviewAdvisor and prior work concluding LMs lacked reviewing capabilities; here the SFT model shows some capability but inferior to humans.",
            "uuid": "e6010.3",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Title/Abstract Gen",
            "name_full": "AI-assisted Title and Abstract Generation (supervised AcademicGPT)",
            "brief_description": "A supervised generation system that produces paper titles and abstracts from the introduction section, trained on a corpus of ~1M research papers to assist authors and produce concise summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI-assisted Title & Abstract Generation",
            "system_description": "Supervised-finetuned AcademicGPT that maps an input 'Introduction (+ optional experiments/results)' to a generated Title and Abstract, using a special token '&lt;begin generate&gt;' to trigger generation and trained on ~1M papers.",
            "llm_model_used": "Supervised-finetuned AcademicGPT (foundation AcademicGPT model finetuned on paired introduction→(title,abstract) examples).",
            "input_type_and_size": "Training corpus: approximately 1 million research papers; input during training is the introduction (and optionally experiments/results) segments; prompt format: 'Introduction Experiments Results &lt;begin generate&gt; Title:...; Abstract:...'.",
            "distillation_approach": "Supervised sequence-to-sequence style generation (instruction-following) where the model learns to condense an introduction into title and abstract (extractive/abstractive summarization style).",
            "output_type": "Generated Title and Abstract text for a given paper introduction; used to assist authors or produce concise summaries.",
            "evaluation_methods": "Qualitative case examples comparing generated title/abstract to ground truth; no large-scale automatic metric results (e.g., ROUGE/BLEU) reported in the technical report.",
            "results": "Example cases show coherent, relevant title and abstract generations that are generally close to ground truth; no aggregate numeric evaluation provided in the paper.",
            "datasets_or_benchmarks": "Custom supervised dataset derived from ~1M research papers (introduction→title+abstract pairs).",
            "challenges_or_limitations": "No reported large-scale automatic evaluation; potential for missing important experimental specifics if only introduction is provided; generation quality depends on input completeness and domain coverage of training data.",
            "comparisons_to_other_methods": "No direct quantitative comparison to other title/abstract generation methods in the paper; method is positioned as a practical application built on AcademicGPT.",
            "uuid": "e6010.4",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "AcademicSearch (ES-KG)",
            "name_full": "AcademicSearch — ElasticSearch-based Knowledge Graph (Academic KG)",
            "brief_description": "A retrieval tool (used as an agent tool) that indexes paper metadata (authors, title, abstract, publication date, venue, citation counts, references) for fuzzy search and logical sorting to support retrieval-augmented synthesis by the AcademicGPT agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AcademicSearch (ElasticSearch Knowledge Graph)",
            "system_description": "An ES-backed knowledge store containing bibliographic and paper metadata that supports fuzzy matching on abstracts, authors, titles, fields of study, dates and venues; used by the agent to retrieve supporting evidence and recommend similar papers.",
            "llm_model_used": "Tool used by AcademicGPT agent; not itself an LLM. It is queried by the AcademicGPT / agent pipeline.",
            "input_type_and_size": "Indexed metadata from the authors' full academic corpus (ArXiv, Unpaywall, parsed theses, PubMed, PapersWithCode). Query inputs are structured JSON fields (abstracts, authors, fieldOfStudy, date ranges, title, venue) and resultParameters list specifying which metadata fields to return.",
            "distillation_approach": "Knowledge aggregation via structured indexing and retrieval; supports evidence gathering for LLM synthesis rather than distillation per se.",
            "output_type": "Structured metadata results (authors, abstracts, publishDate, title, venue, citationCount) returned to the agent for synthesis.",
            "evaluation_methods": "Not separately benchmarked in the report; utility demonstrated via agent example cases (paper recommendation, retrieval for QA).",
            "results": "Used effectively in case studies for retrieval to support AcademicGPT answers and paper recommendations; no standalone metrics reported.",
            "datasets_or_benchmarks": "Built over the paper corpus described in the paper (ArXiv, Unpaywall, parsed theses, PubMed, PapersWithCode metadata).",
            "challenges_or_limitations": "Quality of retrieval depends on upstream data cleaning and indexing; fuzzy matching and ranking heuristics are critical; potential for retrieval of outdated or low-quality papers if filters are insufficient.",
            "comparisons_to_other_methods": "No head-to-head comparison to other retrieval/index systems provided; design choices (ES-based KG with fuzzy search) motivated by practicality for academic metadata retrieval.",
            "uuid": "e6010.5",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ReAct (adapted)",
            "name_full": "ReAct (reasoning+action framework) — adapted variant",
            "brief_description": "A reasoning-and-action prompting framework that interleaves chain-of-thought style reasoning with explicit actions (tool calls); in this work, the authors adopt ReAct principles but enforce JSON-formatted single-action-per-step outputs for tool calls.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ReAct (adapted JSON-action variant)",
            "system_description": "The authors adapt the ReAct paradigm for their AcademicGPT agent: the model emits an inner monologue and selects discrete actions, but action outputs are constrained to a strict JSON blob (single tool action per step) specifying tool name and parameters.",
            "llm_model_used": "Used with instruction-finetuned AcademicGPT as the reasoning engine.",
            "input_type_and_size": "User query and multi-turn dialogue context; ReAct-style prompt templates and tool specification prompts (AcademicSearch and WebSearchEngine) guide action choices.",
            "distillation_approach": "Not a distillation method—ReAct is used to structure LLM interaction with retrieval tools enabling multi-step evidence gathering and synthesis.",
            "output_type": "Interleaved reasoning traces (Thought) and JSON actions (Action) followed by Observations and eventual final answer.",
            "evaluation_methods": "Demonstrated via examples and described agent behavior; no formal evaluation of ReAct adaptation provided.",
            "results": "Enables the agent to plan and call retrieval tools in a structured manner; demonstrated case studies show successful retrieval+answer flows.",
            "datasets_or_benchmarks": "Built-in prompt templates and tool APIs; underlying corpora for retrieval are the same academic corpora indexed in AcademicSearch.",
            "challenges_or_limitations": "Enforcing strict single-action JSON may reduce multi-query expressiveness per step; requires well-designed prompts and tool schemas to avoid malformed actions.",
            "comparisons_to_other_methods": "Authors cite ReAct as inspiration and note the main difference: action outputs are strict JSON for API usage; no quantitative comparison to vanilla ReAct.",
            "uuid": "e6010.6",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ComputerScienceQA",
            "name_full": "ComputerScienceQA (benchmark derived from PapersWithCode)",
            "brief_description": "A newly created multiple-choice QA benchmark focused on computer science methods and datasets, constructed from PapersWithCode metadata to evaluate a model's knowledge of methods and datasets across CS subdomains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ComputerScienceQA (benchmark)",
            "system_description": "A curated evaluation dataset comprising 9,686 multiple-choice questions (1,885 about methods and 7,801 about datasets) built from PapersWithCode descriptions to test model comprehension of CS methods and dataset descriptions (constructed to prevent leakage by masking method names in descriptions).",
            "llm_model_used": "Used to evaluate AcademicGPT, LLaMA-65B, LLaMA2-70B and ChatGPT (gpt-3.5); not itself an LLM.",
            "input_type_and_size": "3-shot evaluation protocol; question format masks method names in descriptions and provides multiple-choice options derived from other methods/datasets in the same domain; total dataset size = 9,686 questions.",
            "distillation_approach": "Benchmarking of knowledge synthesis capacity (no distillation algorithm) — tests whether an LLM can map descriptions to method/dataset names and synthesize domain knowledge.",
            "output_type": "Multiple-choice answers (method/dataset identification).",
            "evaluation_methods": "3-shot accuracy compared across models (AcademicGPT, LLaMA variants, ChatGPT).",
            "results": "AcademicGPT achieved overall 83.5% vs LLaMA2 79.9% and ChatGPT 81.1% (per Table 7), with improvements in several subcategories.",
            "datasets_or_benchmarks": "Derived from PapersWithCode (methods and datasets metadata).",
            "challenges_or_limitations": "Dataset construction required masking method names to avoid leakage; multiple-choice format simplifies evaluation but may not capture deeper synthesis or reasoning across multiple papers.",
            "comparisons_to_other_methods": "Compared model accuracies against LLaMA-65B, LLaMA2-70B and ChatGPT in the paper.",
            "uuid": "e6010.7",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Galactica (mention)",
            "name_full": "Galactica: A large language model for science",
            "brief_description": "A domain-specific LLM tailored to scientific research (cited in related work) that aims to streamline inquiry and discovery across scientific literature.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "mention",
            "system_name": "Galactica (related work mention)",
            "system_description": "Referenced as a prior effort to build a scientific-domain LLM; used as inspiration/motivation for AcademicGPT.",
            "llm_model_used": "Galactica (as reported in Taylor et al., 2022) — external work referenced, not used in experiments.",
            "input_type_and_size": "",
            "distillation_approach": "Mentioned as a domain-specific LLM (no operational details provided in this technical report beyond citation).",
            "output_type": "",
            "evaluation_methods": "",
            "results": "",
            "datasets_or_benchmarks": "",
            "challenges_or_limitations": "Cited as prior art; no critique in this report beyond being an initial motivation.",
            "comparisons_to_other_methods": "Positioned among domain-specific LLMs; AcademicGPT is described as inspired by Galactica.",
            "uuid": "e6010.8",
            "source_info": {
                "paper_title": "AcademicGPT: Empowering Academic Research",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Review Advisor (Can we automate scientific reviewing?)",
            "rating": 2,
            "sanitized_title": "review_advisor_can_we_automate_scientific_reviewing"
        },
        {
            "paper_title": "Long Alpaca / LongLLaMA (Long-context instruction-following models / LongLora references)",
            "rating": 2,
            "sanitized_title": "long_alpaca_longllama_longcontext_instructionfollowing_models_longlora_references"
        },
        {
            "paper_title": "Extending context window of large language models via positional interpolation",
            "rating": 1,
            "sanitized_title": "extending_context_window_of_large_language_models_via_positional_interpolation"
        },
        {
            "paper_title": "YaRN: Efficient context window extension of large language models",
            "rating": 1,
            "sanitized_title": "yarn_efficient_context_window_extension_of_large_language_models"
        },
        {
            "paper_title": "Nougat: Neural optical understanding for academic documents",
            "rating": 1,
            "sanitized_title": "nougat_neural_optical_understanding_for_academic_documents"
        },
        {
            "paper_title": "PapersWithCode",
            "rating": 1,
            "sanitized_title": "paperswithcode"
        },
        {
            "paper_title": "PubMedQA: A dataset for biomedical research question answering",
            "rating": 1,
            "sanitized_title": "pubmedqa_a_dataset_for_biomedical_research_question_answering"
        }
    ],
    "cost": 0.0201005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AcademicGPT: Empowering Academic Research
21 Nov 2023</p>
<p>Shufa Wei 
ReadPaper Team International Digital Economy Academy</p>
<p>Xiaolong Xu 
ReadPaper Team International Digital Economy Academy</p>
<p>Xianbiao Qi 
ReadPaper Team International Digital Economy Academy</p>
<p>Xi Yin 
ReadPaper Team International Digital Economy Academy</p>
<p>Jun Xia 
ReadPaper Team International Digital Economy Academy</p>
<p>Jingyi Ren 
ReadPaper Team International Digital Economy Academy</p>
<p>Peijun Tang 
ReadPaper Team International Digital Economy Academy</p>
<p>Yuxiang Zhong 
ReadPaper Team International Digital Economy Academy</p>
<p>Yihao Chen 
ReadPaper Team International Digital Economy Academy</p>
<p>Xiaoqin Ren 
ReadPaper Team International Digital Economy Academy</p>
<p>Yuxin Liang 
ReadPaper Team International Digital Economy Academy</p>
<p>Liankai Huang 
ReadPaper Team International Digital Economy Academy</p>
<p>Kai Xie 
ReadPaper Team International Digital Economy Academy</p>
<p>Weikang Gui 
ReadPaper Team International Digital Economy Academy</p>
<p>Wei Tan 
ReadPaper Team International Digital Economy Academy</p>
<p>Shuanglong Sun 
ReadPaper Team International Digital Economy Academy</p>
<p>Yongquan Hu 
ReadPaper Team International Digital Economy Academy</p>
<p>Qinxian Liu 
ReadPaper Team International Digital Economy Academy</p>
<p>Nanjin Li 
ReadPaper Team International Digital Economy Academy</p>
<p>Chihao Dai 
ReadPaper Team International Digital Economy Academy</p>
<p>Lihua Wang 
ReadPaper Team International Digital Economy Academy</p>
<p>Xiaohui Liu 
ReadPaper Team International Digital Economy Academy</p>
<p>Lei Zhang 
ReadPaper Team International Digital Economy Academy</p>
<p>Yutao Xie 
ReadPaper Team International Digital Economy Academy</p>
<p>AcademicGPT: Empowering Academic Research
21 Nov 2023EB25E7EA7430F89BD3DE2F9B9D99E8D5arXiv:2311.12315v1[cs.CL]
Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks.Yet, many of these advanced LLMs are tailored for broad, general-purpose applications.In this technical report, we introduce AcademicGPT, designed specifically to empower academic research.AcademicGPT is a continual training model derived from LLaMA2-70B.Our training corpus mainly consists of academic papers, thesis, content from some academic domain, high-quality Chinese data and others.While it may not be extensive in data scale, AcademicGPT marks our initial venture into a domain-specific GPT tailored for research area.We evaluate AcademicGPT on several established public benchmarks such as MMLU and CEval, as well as on some specialized academic benchmarks like PubMedQA, SCIEval, and our newly-created ComputerScienceQA, to demonstrate its ability from general knowledge ability, to Chinese ability, and to academic ability.Building upon AcademicGPT's foundation model, we also developed several applications catered to the academic area, including General Academic Question Answering, AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract Generation.† denotes equal contributions.* Work was done when they worked at IDEA. ‡ denotes equal leadership.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Du et al., 2021;Hoffmann et al., 2022;OpenAI, 2023a;Ouyang et al., 2022;Radford et al., 2018Radford et al., , 2019;;Touvron et al., 2023a,b;Zeng et al., 2022) have transformed natural language processing (NLP) and artificial intelligence (AI).LLMs have not only redefined our capabilities in understanding and generating text content but have also branched their influence into various domains.In the sphere of writing, LLMs (OpenAI, 2023a) have breathed life into nuanced narratives and precise technical content.In the programming world, they (Rozière et al., 2023) have offered solutions to intricate coding problems, bridging the gap between human language and code.Moving to sectors like finance, these models (Wu et al., 2023b) decode complex datasets and predict market trends with precision.In the healthcare domain (OpenAI, 2023a,b;Singhal et al., 2022), they assist in diagnosis, treatment suggestions, and even complex research tasks.In the creative arts, combining with multi-modality large models, they have opened doors to AI-driven music generation, costume designing , and other forms of artistic expression.In conclusion, LLMs have revolutionized numerous industries (Bran et al., 2023;Chen and Koohy, 2024;Cui et al., 2023;Luo et al., 2022;Rozière et al., 2023;Scarlatos and Lan, 2023;Singhal et al., 2022;Wu et al., 2023b;Zheng et al., 2023).However, the journey to this revolutionary phase has not happened overnight.BERT (Devlin et al., 2018) and GPT-1 (Radford et al., 2018) ushered in the era of large models.Models like GPT-3 (Brown et al., 2020) laid the groundwork, with its billions of parameters setting new benchmarks.Subsequent innovations, including ChatGPT's (Ouyang et al., 2022) conversational prowess, PalM's (Chowdhery et al., 2022) multitasking abilities, the LLaMA series' (Touvron et al., 2023a,b) advanced linguistic capabilities, CodeGeeX and CodeL-LaMA's (Rozière et al., 2023;Zheng et al., 2023) programming ability, GPT4's (OpenAI, 2023a) improved general and professional ability, and GPT-4V (OpenAI, 2023b;Yang et al., 2023b)'s multi-model ability have continuously pushed the envelope, setting new frontiers in what AI can achieve.In conclusion, thanks to the ease of information dissemination, the pace of innovation has significantly outstripped that of the past.</p>
<p>With knowledge burgeoning and scientifical discoveries emerging at an astonishing rate, scholars and researchers are continually overwhelmed by an expanding ocean of literature.This overwhelming abundance is paradoxical, signifying both our triumphant strides in human understanding and the looming challenge that researchers face in keeping abreast of fresh insights.This issue becomes especially pronounced within specialized sectors or subdivisions.Here, the rapid growth of targeted studies, novel methodologies, and intricate findings intensifies the difficulty for scholars to rapidly understand and assimilate the particulars of these niche domains.Such information saturation hampers not only the smooth flow of knowledge but also erects barriers for interdisciplinary endeavors.Grasping the intricate details of these subdivisions demands significant time, slowing down the pace of integration and innovation.</p>
<p>It becomes increasingly crucial to provide researchers with effective tools and methodologies that allow them to distill essential insights from the vast ocean of information, ensuring that critical advancements and findings are recognized and built upon.These tools are not just limited to aiding in comprehension but span a broad spectrum of research activities, including paper reading where AI-assisted methods can highlight key findings and offer a concise summary, paper polishing where advanced tools can provide grammar checks, stylistic recommendations, and ensure the clarity and coherence of the presented ideas, paper reviewing where tool can give a critical comments about the paper, content-based paper writing where predictive and generative models can assist researchers in constructing well-structured narratives and arguments, saving them invaluable time.</p>
<p>In this technical report, our contributions are highlighted as two folders.</p>
<p>• we introduce AcademicGPT, a GPT model specifically tailored for scientific research.This model stands as a testament to the power of harnessing vast academic corpora, having been trained on a academic corpus with 120 billion tokens.The sheer volume of data processed ensures its robustness and accuracy in comprehending intricate scientific nuances.</p>
<p>• we build several applications based on AcademicGPT, as shown in Figure 1.1, including General Academic Question Answering, AI-Assisted Paper Reading, Paper Review and AI-assisted Content Generation.Our General Academic Q&amp;A system is a sophisticated agent equipped with multi-turn dialogue memory.In the agent, our strategic planning and application architecture draw inspiration from the ReAct framework, integrating its principles to achieve the desired outcomes.This ensures continuity and context-awareness in academic discussions, setting the stage for meaningful and deep interactions.Recognizing the challenges presented by lengthy academic articles, we introduced an AI-powered solution to simplify and enhance the paper reading experience, ensuring researchers grasp the core concepts efficiently.Our paper review system, underpinned by the supervised finetuning (SFT) model based on AcademicGPT, introduces a way of assessing academic content.Our AI-Powered Content Generation generate content such as abstracts and titles based solely on a given introduction.By manipulating the order of input context, our model exhibits strong adaptability in content creation.</p>
<p>In essence, our work with AcademicGPT not only introduces a powerful model for scientific research but also demonstrates its practical applications, promising a transformative impact on the academic community.</p>
<p>The structure of this technical report is depicted as follows: Section 2 discuss some related works.Section 3 describes the AcademicGPT model and report its results on several benchmarks.Section 4 describes four applications built on AcademicGPT.</p>
<p>Related Work</p>
<p>Large Language Models (LLMs).The domain of natural language processing (NLP) (Devlin et al., 2018;Radford et al., 2018) and artificial intelligence (AI) (OpenAI, 2023b;Yang et al., 2023b) has witnessed a transformative shift, primarily driven by the emergence and rapid evolution of LLMs (Black et al., 2022;Brown et al., 2020;Devlin et al., 2018;Peters et al., 2017;Radford et al., 2018Radford et al., , 2019)).These models, with their unprecedented scale and capability, have redefined the paradigms of linguistic understanding, reasoning, and generation.From a historical perspective, the journey of LLMs began with models comprising millions of parameters, like GPT-1 (Radford et al., 2018).However, as the field matured, the scale expanded drastically, moving to models boasting billions, or even trillions, of parameters, such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Switch Transformers (Fedus et al., 2022).This massive increase in model size has been a cornerstone in enhancing their capabilities, offering more human-like fluency and versatility in a plethora of natural language tasks.Interestingly, two main trajectories have dominated the LLM landscape: closed-source models and open-source models.Closed-source Models such as GPT-4 (Bubeck et al., 2023;OpenAI, 2023a;Yang et al., 2023b), ChatGPT (Ouyang et al., 2022), Claude (Bai et al., 2022), PaLM (Chowdhery et al., 2022), Chinchilla (Hoffmann et al., 2022), Gopher (Rae et al., 2021) and ERNIE (Sun et al., 2021;Zhang et al., 2019) have a dominating position in current LLM research and applications.Their introduction has reshaped the general perception about machine capabilities.For instance, ChatGPT's capacity to engage in diverse linguistic interactions, ranging from casual dialogues to elucidating intricate topics, underscores the potential of LLMs in automating tasks requiring linguistic prowess.However, a significant drawback accompanying these closed-source behemoths like GPT-4, PaLM-2, and Claude is the restricted access to their full parameters.This limitation hampers the broader research community from delving deep into these systems or optimizing them further, thereby constraining a collective progress.In contrast to their closed-source counterparts, opensource models such as OPT (Zhang et al., 2022), Bloom (Scao et al., 2022), Falcon (Almazrouei et al., 2023), Baichuan (Yang et al., 2023a), QWen (Bai et al., 2023), LLaMA1 (Touvron et al., 2023a) andLLaMA2 (Touvron et al., 2023b) champion the cause of transparency and community engagement.LLaMA1, for example, with its vast 65 billion parameters, is not just a marvel in itself but also an exemplar of openness.The full availability of such models has been a boon, as researchers and developers can probe, experiment, and build upon them without constraints.This liberal approach has acted as a catalyst, furthering research and leading to the birth of new models like Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and more.As the field forges ahead, it remains to be seen how these two pathways coalesce or diverge, but what is undeniable is their collective contribution to the magnificent world of LLMs.Continual Pretraining of LLMs.Continual pretraining refers to the process of incrementally and continuously training a model on new data.Roughly speaking, continual pretraining can be categorized into four classes that targets different goals including lifelong pretraining, longer context window, domain adaptive learning, improving training strategy of continual pretraining.In the realm of lifelong learning, Jin et al. (2021) introduced the concept of "lifelong pretraining".They explore where various continual learning algorithms were employed to incrementally pretrain language models.Through evaluations on the model's adaptability to new data, they find that distillation-based approaches effectively preserve the performance on downstream tasks from earlier domains.In another noteworthy work, Qin et al. (2022) proposed a model named ELLE.This model aspires to achieve efficient lifelong pretraining by leveraging pre-trained language model (PLM) extensions and pretraining domain prompts.Its primary aim is to adapt to continuously streaming data.In the domain of expanding context window, several key works have made noteworthy contributions.Rozière et al. (2023) employed continual pretraining to enlarge the model's window.Xiong et al. (2023) further advanced this paradigm, achieving a series of long-context LLMs that support effective context windows of up to 32,768 tokens, starting from a foundation of continual pretraining on LLaMA2.Targeting the RoPE (Su et al., 2021) positional encoding, Peng et al. (2023) introduced the YaRN approach, a novel methodology devised specifically for expanding the model's context window.In domain of adaptive learning, there have been several pivotal contributions.Rozière et al. (2023) leveraged a continual pretraining to enhance the code capabilities of LLMs.They introduced, "CodeLlama", a state-of-the-art large-scale code language model built upon LLaMA2, boasting unrivaled performance in the open-source community, exceptional code completion capabilities, support for extensive input context sizes, and the adeptness in autonomously following directives in programming tasks.Furthermore, Zhang et al. (2023) delved into the domain of continual pretraining in the context of biomedical visual language processing, shedding light on the nuances of domain-specific adaptations.In the domain of improving training strategy with a focus on stability, a series of studies have been conducted to advance our understanding.Gupta et al. (2023) explored various training approaches for continual pretraining.They examined the effects of different warmingup strategies on large language models.Their findings highlighted that restarting model warm-up can boost downstream performance, even outperforming models trained from scratch on sizable downstream datasets.In a parallel vein, Ke et al. (2022) investigated methodologies to enhance performance in domain-specific scenarios via continual pretraining.Their research proposed an innovative technique that harnesses a series of unlabeled domain-specific corpora for the continual pretraining of language models, thereby augmenting their end-task efficacy.Domain-Specific LLMs.LLMs have been applied to different domains after its success in natural language processing and AI.In scientific research, Galactica (Taylor et al., 2022) model stands out as a tool tailored for general scientific research, streamlining the process of inquiry and discovery in the vast expanse of scientific literature.In the domain of Finance, BloombergGPT (Wu et al., 2023b) is tailored for the financial sector, providing insights, analyses, and information tailored to financial professionals and stakeholders.In Medicine, Med-PaLM (Singhal et al., 2022) is engineered specifically for the medical domain, ensuring accurate and context-aware responses pertinent to medical professionals and researchers.In Programming, Code-LLaMA (Rozière et al., 2023) is geared towards programming, aiding developers by understanding and generating code, making the coding process more intuitive and efficient.In Legal, ChatLaw (Cui et al., 2023) emerges as an open-source legal LLM, providing a new way legal professionals access, interpret, and utilize legal texts.In Biomedical, with models like BioGPT (Luo et al., 2022), the biomedical domain can benefit from advanced text generation and mining, aiding in research, diagnosis, and treatment planning.In Physics, GPT-PINN (Chen and Koohy, 2024) is a confluence of physics and AI, designed as a Physics-Informed Neural Network.It is tailored for meta-learning of parametric PDEs, offering a non-intrusive approach to solving complex physics problems.In Mathematics, MathGPT (Scarlatos and Lan, 2023) targets the realm of mathematical reasoning, assisting researchers and students in understanding complex mathematical concepts and problems.</p>
<p>In summary, the rise of domain-specific LLMs underscores the potential of AI to cater to specialized needs across diverse fields.These models not only amplify the capabilities within their respective domains but also promise to transform the way professionals across sectors approach and solve challenges.</p>
<p>Remark.AcademicGPT builds upon the foundation of LLaMA2, an open-source Large Language Model (LLM) renowned for its versatility and extensive capabilities.AcademicGPT is a continual pretraining on LLaMA2.The primary domain of focus for AcademicGPT is academic research, our initial motivation is inspired by Galactica (Taylor et al., 2022).AcademicGPT marks our initial venture into a domain-specific GPT tailored for research area.In essence, AcademicGPT targets to help researchers, academicians, and students to quickly understand the fresh insights.</p>
<p>AcademicGPT</p>
<p>In this section, we delve into AcademicGPT by examining its data sources, model architecture, and experimental results.We begin by elucidating the datasets that are employed to cultivate AcademicGPT's capabilities.Then we give an overview of the model's architecture.We conclude by reported the model's performance on benchmarks such as MMLU (Hendrycks et al., 2020), CEval (Huang et al., 2023), PubMedQA (Jin et al., 2019), SCIEval (Sun et al., 2023), and our newly collected ComputerScienceQA.</p>
<p>Data</p>
<p>Our goal in AcademicGPT is to enhance LLaMA2's capcibility on academic research, and meanwhile improve its ability on Chinese language.Therefore, the way of our data collection is around this two targets.It is well known that the capcibility of LLaMA2 on understanding Chinese language is limited due to its limited usage of Chinese corpus.Meanwhile, since LLaMA2 is a general LLM, that does not use enough academic data.Some existing large-scale dataset includes the Pile (Gao et al., 2020), Roots (Laurençon et al., 2022), RedPajama-Data (TogetherAI, 2023), Falcon-Refinedweb (Penedo et al., 2023), WudaoCorpora Text (Yuan et al., 2021).These data were collected for general purpose.</p>
<p>Our training data is constructed based on the up-mentioned two goals.</p>
<p>• including more academic data.</p>
<p>• adding more Chinese data Specifically, on one side, our training data should consists of both high-quality Chinese and english data.On another side, our training data should be mainly from academic area, including academic paper, thesis, content from some academic domains, and more.Our Chinese data consists of four types: Common Crawl (CC), Wiki, Baike, and Books.However, the data collected from CC are usually very dirty, it includes a lot of advertisements, pornographic information, violence and other toxic information.We need to clean the data.Our Chinese data cleaning pipeline includes four stages.1) we crawl 200K of articles from some top academic domains; (2) we use a powerful LLM to label the data.The prompt we use to label the data is shown as in Figure 3 Please note that you only need to directly return the JSON results without providing any additional unnecessary text."""When collecting Academic English data, we focus on collecting a more high-quality data.Our academic data consists of a mix of several sources.First, we crawl more than 1 million of theses from 200 top universities in the world.We believe the thesis data, compared to traditional conference or journal paper, are self-consistent.Instead, conference and journal papers are usually inconsistent, and requires more expert experience to understand.Since the content in the thesis is usually very long.For this kind of data, we use Nougat (Blecher et al., 2023) to parse the pdf files.Second, we crawl Arxiv 1 papers (it contains around 2.26 millions of papers until to May 2023.).Third, we use the data from unpaywall 2 , it contains an open database of 48,383,164 free scholarly articles that collect Open Access content from over 50,000 publishers and repositories.For the paper PDFs that are not too long, we use our own PDF parser to structure these PDF documents.Fourth, we filter the academic data from the Falcon-Refinedweb (Penedo et al., 2023) according to some domains.Generally, we believe the quality in the Falcon-Refinedweb3 is good, what we need to do is to select out the high-quality academic data from it.Besides of the above-mentioned sources, we also use wiki pages (only English pages), bibliographic from Semantic Scholar4 , papers from PubMed.</p>
<p>In Table 1, we list the detailed information of the data we collected and used in this paper.Most of data are research papers, theses, and some other academic data.</p>
<p>Modeling</p>
<p>Neural network is essentially a function approximation problem.Given a large amount of data (x i , y i ) for i ∈ [1, N ], our target is to learn a function F (•) to minimize the following loss function:
loss = 1 N N i=1 L (F (x i ; W ) , y i ) ,(1)
where L(•, •) is the loss function.</p>
<p>After training stage, the inference is essentially an interpolation process.</p>
<p>Model Capability.To expect the model will have strong approximation ability, we need to promise that the network will have a larger Lipschitz constant that is defined as:
∥F (x 1 ; W ) − F (x 2 ; W )∥ ≤ L 0 ∥x 1 − x 2 ∥,
where L 0 is the Lipschitz constant.</p>
<p>A large Lipschitz constant means that the model has a stronger nonlinearity, and thus it has a stronger approximation ability.For instance, compared to the Convolution (LeCun et al., 1998) network, Transformer architecture (Vaswani et al., 2017) has a much larger Lipschitz constant, and thus a powerful representation ability.Via understanding the Jacobian matrix of each module and its corresponding Lipschitz constant, we can theoretically estimate the representative ability of the network.Readers can refer to Qi et al. (2023) for a detailed analysis.</p>
<p>Training Stability.However, larger Lipschitz constant may lead to training instability.Thus, to promise the network will have a stable training process, we need to keep that
x l , ∂L ∂x l &lt; R, for l ∈ [1, L].(2)
The above equation means that the activations and their gradients should be bounded to the range of numerical representation (e.g., FP16, FP32 or BF16).</p>
<p>Our Training Strategy.To train AcademicGPT, we use the AdamW optimizer (Loshchilov and Hutter, 2017), with β 1 and β 2 values set at 0.9 and 0.95 respectively and ϵ = 10 −8 .We leverages a cosine learning rate schedule, decaying the final learning rate to be a mere 1 10 of the peak learning rate 1.5e-5.Our used batch size is around 1.57M tokens, where each sample comprises sequences of 4,096 tokens.For gradient accumulation, we accumulate 64 mini-batch.To train the model stably, we conduct the following tricks:</p>
<p>• we use BF16 (Kalamkar et al., 2019) instead of FP16.</p>
<p>• we use FP32 for the LayerNorm (Ba et al., 2016) layer.</p>
<p>• we set gradient clipping to be 0.4 instead of 1.0.• for the ϵ in the LayerNorm layer, we set it to be 1e-5.</p>
<p>• we use a longer warmup (Loshchilov and Hutter, 2016).</p>
<p>These above tricks are either to extend the range R of numerical representation or to constraint rapid growth of the Lipchistz constant L 0 of the modules or the whole network.In this way, we can promise Equation 2 will always hold true in the training process.</p>
<p>To speedup the training process, we also integrated some new and advanced techniques including FlashAt-tention2 (Dao, 2023) that not only speedup the attention module but also save a large amount of memory, Apex RMSNorm that implements a fused cuda kernel.Since AcademicGPT is a continual training model of LLaMA2-70B, it uses some same technology as LLaMA2 including RMSNorm (Zhang and Sennrich, 2019) instead of LayerNorm, SwiGLU (Shazeer, 2020) instead of GeLU.For position embedding, it uses RoPE (Su et al., 2021) instead of Alibi (Press et al., 2021).For tokenizer, it uses BPE (Sennrich et al., 2015).It uses DeepSpeed (Rasley et al., 2020) with Zero (Rajbhandari et al., 2020).Our training is based gpt-neox (Black et al., 2022) framework in which we integrate many newly introduced skills.It takes around 37 days to finish the training of 120B data using 192 A100 GPUs with 40GB memory.</p>
<p>Results</p>
<p>We evaluate AcademicGPT on several benchmarks.First, we evaluate our models on some general benchmarks, including MMLU (Hendrycks et al., 2020) and CEval (Huang et al., 2023).Our goals are to evaluate whether the continual training will deteriorate the performance of the original LLaMA2 (Touvron et al., 2023b) model and to evaluate the Chinese ability of our AcademicGPT after our continual training.Second, we evaluate the capability of AcademicGPT on some academic benchmarks, including PubMedQA (Jin et al., 2019), SCIEval (Sun et al., 2023) and ComputerScienceQA.ComputerScienceQA is a newly created dataset by us to evaluate the capability of the model on computer science area.In default, when we mention LLaMA1 and LLaMA2, we mean LLaMA1-65B and LLaMA2-70B.</p>
<p>Results on MMLU.We examine AcademicGPT's ability on MMLU.The MMLU test set covers 57 tasks including elementary mathematics, US history, computer science, law, and more.To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, MMLU can be used to analyze models across many tasks and to identify important shortcomings.</p>
<p>Following some standard evaluation methods (Chowdhery et al., 2022;OpenAI, 2023a;Touvron et al., 2023a,b), we use 5-shot setting for evaluation.In Table 2, we report the averaged performance on 57 classes of MMLU test set, and compare AcademicGPT with LLaMA1 (65B), LLaMA2 (70B), ChatGPT (gpt-3.5-turbo-0613).In Table 3, we show the results of AcademicGPT and LLaMA2 on several subjects in MMLU.</p>
<p>We can find that the continual training on LLaMA2 will not deteriorate the performance averagely.Meanwhile, we observe that the results on several categories that have large amount of data used in our continual training will improve, but on some categories that is not largely covered in the data of our continual training, their performance will slightly decrease.CEval is a Chinese evaluation toolkit, aiming to swiftly assess and understand a model's capabilities from various perspectives, especially its worldly knowledge and reasoning abilities.This assessment originates from real-world Chinese human exams spanning middle school, high school, university, and professional levels, covering 52 subjects including STEM, humanities, and social sciences.We utilize the valid set of CEval for evaluations during the model development process, which comprises 1,346 questions across all 52 subjects.During our assessment, we employ a 5-shot evaluation setting.The results are shwon in Table 4.</p>
<p>We can see that from Table 4, by integrating a modest amount of Chinese common crawler content from textbooks and Baidu Baike (a Chinese version of Wikipedia), we enhanced the performance of AcademicGPT to 55.1% on CEval from 50.8% of the original LLaMA2.In our side-by-side evaluations, the Chinese-enhanced AcademicGPT significantly outperforms its original version in scenarios like academic reading assistance and translation.Results on PubMedQA.PubMedQA (Jin et al., 2019) 5 is a biomedical question answering dataset that is collected from PubMed abstracts.The task of PubMedQA is to answer research questions with three choices: yes/no/maybe, according to the corresponding abstracts.It consists of 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances.Each instance is composed of four parts: a question, a context, a long answer and an assertion.This question is either an existing research article title or content derived from the title.The context is the corresponding abstract excluding its conclusion.The long answer is the conclusion of the abstract and, presumably, answers the research question.Finally, there is a yes/no/maybe assertion which summarizes the conclusion.</p>
<p>Methods</p>
<p>For our evaluation, we only use the 1k expert-annotated instances.We use the 5-shot for our evaluation.The results of different models are shown in Table 5.</p>
<p>We can see that from Table 5 on the PubMedQA dataset, our method achieved better results than LLaMA1, LLaMA2, ChatGPT3.5, and GPT4.We reckon this may be attributed to the presence of more medical-related corpus in our continual training data.</p>
<p>Results on SCIEval.SCIEval6 (Sun et al., 2023) is a scientific evaluation system based on Bloom's Taxonomy, designed to assess a model's performance in foundational knowledge, knowledge application, scientific computation, and research capabilities.The data primarily originates from Socratic Q&amp;A7 and integrates multiple public datasets, encompassing three subjects: biology, chemistry, and physics.We conducted tests using SCIEval's validation set, focusing solely on the objective questions within the validation set-a total of 1,187 questions, with 380 in biology, 643 in chemistry, and 164 in physics.</p>
<p>Methods</p>
<p>Biology We leverage 3-shot context learning evaluation as previous methods (Sun et al., 2023).We compare Aca-demicGPT with ChatGPT3.5 and the original LLaMA2 (Touvron et al., 2023b) that have been tested on SCIEval.The results are reported in Table 6.</p>
<p>We can see that from Table 6, AcademicGPT improves the average accuracy from 63.6 obtained by LLaMA2 to 68.8, and also surpasses ChatGPT's score of 67.9.We can see that compared to ChatGPT, AcademicGPT performs better on physics but does not perform well on chemistry.PapersWithCode consists of two sections: "dataset" and "method".The method part predominantly delves into descriptions of techniques detailed in research papers, whereas the dataset part pertains to dataset descriptions.This information is curated and reviewed by an open community.From a methodological viewpoint, PapersWithCode spans across seven major areas, each comprising multiple categorical layers.For instance, under "Attention/Attention Mechanisms/Attention Patterns", one would find descriptions of varied method concepts such as Strided Attention, Fixed Factorized Attention, Sliding Window Attention, etc. From a dataset perspective, PapersWithCode covers an array of modalities like Images, Text, Video, Audio, etc., providing a holistic and real-time overview of datasets in the Computer Science domain.Until to September 2023, our Com-puterScienceQA includes 1,885 methods and 7,801 datasets.Each sub-domain consists of several topics, and each topic contains an array of methodologies.For example, under "self-attention", there exists a multitude of distinct self-attention mechanism implementations, including linear attention, sparse attention, fast attention, dot-product attention, L2 similarity attention, etc., from a dataset angle.</p>
<p>Results on</p>
<p>Below, we will describe our construction strategy.For the "method" question type:</p>
<ol>
<li>
<p>Retrieve the method description and process it: case-insensitive matching of the description against the method's name and full name is done and replaced with "()", to prevent information leakage.All HTTP(s) links are removed to avoid data breaches.</p>
</li>
<li>
<p>Craft the question prompt as: Question: Which of the following options is a description of "method.get('fullname', method['name'])"?</p>
</li>
</ol>
<p>The correct option stems from the method's description, while the distractor options are derived from the descriptions of other methods within the same domain collection.</p>
<p>Four Samples in ComputerScienceQA</p>
<p>Case 1 (about method introduction):</p>
<p>Question : Which of the following options is a description of ''Convolution''?Choices : A: () softly switches the convolutional computation between different atrous rates and gathers the results using switch functions.The switch functions are spatially dependent, i.e., each location of the feature map might have different switches to control the outputs of ().To use () in a detector, we convert all the standard 3x3 convolutional layers in the bottom-up backbone to ().B: A () is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.Intuitively, a () allows for weight sharing -reducing the number of effective parameters -and image translation (allowing for the same feature to be detected in different parts of the input space).C: A () layer is a simple extension to the standard convolutional layer.It has the same functional signature as a convolutional layer, but accomplishes the mapping by first concatenating extra channels to the incoming representation.These channels contain hard-coded coordinates, the most basic version of which is one channel for the i coordinate and one for the j coordinate.The () layer keeps the properties of few parameters and efficient computation from convolutions, but allows the network to learn to keep or to discard translation invariance as is needed for the task being learned.This is useful for coordinate transform based tasks where regular convolutions can fail.D: While performs the channelwise and spatial-wise computation in one step, () splits the computation into two steps: applies a single convolutional filter per each input channel and is used to create a linear combination of the output of the depthwise convolution.The comparison of standard convolution and () is shown to the right.Answer:</p>
<p>Case 2 (about method reference): Case 3 (about dataset introduction): Our methodology is primarily inspired by the the Galactica (Taylor et al., 2022) paper, aiming to gauge a model's proficiency in grasping methods and datasets within the computer science domain.The merits of such a construct method include</p>
<p>• a comprehensive coverage of current mainstream knowledge in the CS domain and an objective,</p>
<p>• a multiple-choice format that simplifies creation and facilitates accurate evaluation.</p>
<p>In conclusion, we have collated a total of 9,686 questions, of which 1,885 pertain to methods and 7,801 relate to datasets.Samples about method and dataset can be referred to in Figure 3.2.</p>
<p>For our evaluation, we employed a three-shot approach.We contrasted our methodology with ChatGPT and the native architecture of LLaMA2.The outcomes can be viewed in  We can see that from Table 7 AcademicGPT performs much better than the original LLaMA2 and improves the performance from 79.9% to 83.5%.Compared to ChatGPT, it also shows better performance.</p>
<p>Applications of AcademicGPT</p>
<p>Based on AcademicGPT, we built several applications, including general academic question answering, AIassisted paper reading, paper review, and AI-assisted title and abstract generation.In essence, by building upon the robust foundation of AcademicGPT, we not only enhance the capabilities of the model but also create several tools that can empower academic research.Figure 1.1 shows our overall framework.</p>
<p>General Academic Question Answering</p>
<p>Academic question answering requires more rigors compared to general question answering.Our academic question answering is a LLM-empowered agent (Karpas et al., 2022;Schick et al., 2023;Shen et al., 2023;Weng, 2023;Wu et al., 2023a;Xi et al., 2023) which consists of the following modules: a AcademicGPT-powered engine that acts a brain, a planning and action module, memory, and tools.This system can harness the power of various academic tools, tailored to diverse types of questions such as paper retrieval, conceptual clarifications, or multi-paper comparisons and to paper recommendation.A overview of the AcademicGPT-empowered agent is shown in Figure 4.1.Below, we will introduce each module in detail.</p>
<p>AcademicGPT-empowered engine.As shown in Figure 4.1, the engine is the brain of the system.Essentially, our AcademicGPT-powered engine is a instruction-finetuned AcademicGPT.The engine should have the following two abilities:</p>
<p>• understanding and execute instructions</p>
<p>• knowing when to use tools, which tool to use, and how to use the tool.</p>
<p>To endow our model with the above-mentioned two abilities, our instruction-finetune data should include two types of data: general instruction-finetune data and instruction data for tool usage.Our instructionfinetune data primarily consists of our further cleaning of open-source data, including the cleaned Wizard9 , LIMA10 , both Chinese and English versions of alpaca (Taori et al., 2023), and our constructed 384 tool usage instructions.</p>
<p>Planning and Action.Leveraging the capabilities of LLMs as the brain of our agent, the system can contemplate and strategize over diverse questions.After LLMs, there are many works (Shinn et al., 2023;Wang et al., 2022;Wei et al., 2022;Yao et al., 2022) focusing on improving models' planning and reasoning abilities including chain of thoughts (CoT) (Wei et al., 2022), self-consistency (Wang et al., 2022), reflection (Shinn et al., 2023) and ReAct (Yao et al., 2022).Our approach employed ReAct.ReAct expands the action space, combining discrete actions for specific tasks with linguistic constructs.This amalgamation seamlessly integrates reasoning and action into the LLM.The ReAct method is a synthesis of reasoning and subsequent action.This method was conceptualized based on a keen observation of human behavior: humans tend to engage in a reasoning process between steps of multi-step tasks.We adapted this by enabling the LLMs to vocalize its "inner monologue", aligning subsequent actions with this articulated reasoning, thereby emulating human cognitive processes.This approach, tested across diverse datasets, achieved state-of-the-art results, boosting the credibility of LLMs and reducing its propensity for nonsensical outputs.</p>
<p>Differing from ReAct (Yao et al., 2022), our action outputs are in the JSON format, detailing the APIs used along with their respective parameters.Further insight into these parameters can be found in the following prompts as shown in Figure 4.2.</p>
<p>Memory.All the historical multi-turn dialogues' contexts are considered as the model's short-term memory.In contrast, academic knowledge graphs retrieved via fuzzy keyword searches serve as long-term memory.</p>
<p>Tool Utilization.Many tools can be used in an agent including search engine, knowledge graph (KG), vector knowledge library, and other.In our system, we use the following tools: KG and Bing search engine.For the KG, we use a elastic search (ES) based KG that incorporates information like author, title, abstract, publication date, institution, citations, and referenced papers into an ES setup, this tool offers fuzzy search capabilities across fields and logical sorting.Based on the KG, we add some features including recommendation of similar papers.This feature recommends multiple similar papers with precision, based on references and keywords.For the Bing search engine, we also specially handled some websites, such as "PapersWithCode".that allows for the retrieval of cutting-edge academic knowledge, such as the state-of-the-art results across datasets and their associated papers.The detailed utility, application scenarios, and parameters of each API have been elaborated upon in the model's input prompts.</p>
<p>We have shown three cases in Figure 4.3, Figure 4.4, and Figure 4.5.We can see that our system can do well on paper recommendation, concept explaining and etc.</p>
<p>ReAct Prompt</p>
<p>System Prompt:</p>
<p>You are a literature reading assistant.You can rigorously answer users' academic questions.You have access to the following tools:</p>
<p>AcademicSearch:</p>
<p>{"description": "This is an tool for retrieving academic knowledge base through fuzzy matching on abstracts, authors, title, fieldOfStudy, publishDate or venue.","input parameters": {"abstracts": {"type": "str", "description": "The query of the abstract."}, "authors": {"type": "list(str)", "description": "The authors of paper."},"fieldOfStudy": {"type": "str", "description": "The field of the paper."}, "publishDate": {"type": "json", "description": "The key is gte or lte, and value is date(yyyy/MM/dd), such as {{'gte': '2020/01/01', 'lte': '2023/12/31'}}."},"title": {"type": "str", "description": "The title of paper.If there are multiple papers, use ';' to distinguish them, such as title1;title2."},"venue": {"type": "str", "description": "Published journals or conferences."},"sort by": {"type": "json", "description": "The Key is abstracts, authors, fieldOfStudy, publishDate, title or venue.The value is 'desc' (descending) or 'asc' (ascending)."},"resultParameters": {"type": "list(str)", "description": "Must required.Each item in the list should be abstracts, authors, fieldOfStudy, publishDate, title, venue or citationCount(the number of citations of the paper).Format should be like ['xxx', 'xxx']"}}, "example of INPUT": "{{'title': 'xxx', 'resultParameters': ['authors', 'publishDate', 'abstracts']}}"} WebSearchEngine:</p>
<p>{"description": "This is a web search engine.This tool will be very useful when you need to query basic academic knowledge and the latest academic knowledge.","input parameters": {"query": {"type": "str", "description": "Must required.Input is the search query related to the question."}},"example of INPUT": "{{'query': 'xxx'}}"} The way you use the tools is by specifying a Json blob.Specifically, this Json should have a 'action' key (with the name of the tool to use) and a 'action input' key (with the input to the tool going here).</p>
<p>The only values that should be in the "action" field are: AcademicSearch, WebSearchEngine The $JSON BLOB should only contain a SINGLE action, do NOT return a list of multiple actions.$JSON BLOB should start with '''.Here is an example of a valid $JSON BLOB:  AI: Based on the latest research in image generation using stable diffusion, there have been several exciting developments in the field.Stable diffusion refers to a type of generative model that uses a diffusion process to model the generation of images.This approach has shown great promise in generating high-quality images that are both diverse and realistic.</p>
<p>One of the most significant advancements in this field is the ability to generate images that are not only visually appealing but also semantically consistent.Researchers have demonstrated that by using stable diffusion, they can generate images that accurately represent the intended scene or object, while also capturing the nuances of the input data.This is particularly useful in applications such as com puter vision, where the generated images need to be both visually realistic and semantically meaningful.</p>
<p>Another area of progress is the use of stable diffusion for image-to-image translation tasks.By using stable diffusion, researchers have been able to translate images from one domain to another, such as converting a photo of a cat to a painting, while preserving the underlying structure and details of the original image.This has significant implications for applications such as image editing and enhance ment.</p>
<p>In addition, stable diffusion has also been used for image synthesis, where researchers have been able to generate completely new images that are not based on any existing data.This has been achieved by using stable diffusion to model the generation of images in a way that is both efficient and scalable.The resulting images are not only visually realistic but also diverse and coherent, opening up new possi bilities for applications such as virtual reality and computer graphics.</p>
<p>Furthermore, researchers have also explored the use of stable diffusion for image denoising and de blurring tasks.By using stable diffusion, they have been able to remove noise and blur from images, resulting in higher-quality images that are more suitable for analysis and processing.</p>
<p>Overall, the latest results in image generation using stable diffusion show great promise for a wide range of applications, from computer vision to virtual reality.As the field continues to evolve, we can expect to see even more exciting developments and advancements in the coming years.</p>
<p>AI-assisted Paper Reading</p>
<p>AI-assisted paper reading is an effective tool to help readers and explain their confusions by an interactive QA format.For AI-assisted paper reading, LLMs with longer context windows are required because the full article of paper is usually long.However, training an LLM with long context windows from scratch requires significantly larger investments.In this technical report, to achieve the ability of AI-assisted paper reading, we employ a two-stage pipeline.In the first stage, we continue the pre-training process and use a dynamic ntk-aware method to extend to larger window context.In the second stage, we use a supervised finetune mode and train the model on LongAlpaca-12k dataset.Below, we will describe these two stages in detail.</p>
<p>AI-assisted Reading (Case 1)</p>
<p>Context: the content of this paper, or the content selected by the user from the paper.</p>
<p>Paper link: https://arxiv.org/abs/1807.06521</p>
<p>Human: Based on this paper, what are the future research directions?AI: Based on the information presented in this paper, there are several areas for further research.</p>
<p>Firstly, the paper proposes a new attention mechanism for convolutional neural networks (CNNs) called Convolutional Block Attention Module (CBAM), which infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive fea ture refinement.Future research could explore the effectiveness of this attention mechanism in other tasks and datasets, as well as the potential benefits of combining CBAM with other attention mechanisms or pre-training strategies.Secondly, the paper validates the effectiveness of CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets.Future research could explore the use of CBAM in other image classification and detection tasks, as well as the potential benefits of combining CBAM with other network architectures or optimization strategies.Thirdly, the paper demonstrates that CBAM can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs.Future research could explore the potential benefits of combining CBAM with other network architectures or optimization strategies, as well as the potential applications of CBAM in low-end devices or real-time systems.Finally, the paper presents visualization results of the attention maps generated by CBAM, which in dicate that CBAM can focus on target object regions better than other methods.Future research could explore the potential benefits of using attention visualization to interpret the behavior of neural networks and to improve their performance.Human: what is the workflow of LipGAN?</p>
<p>AI:</p>
<p>The workflow of the LipGAN model is as follows:</p>
<p>1.The input to the model is a face image, an audio signal, and a desired pose for the face.</p>
<ol>
<li>
<p>The face image is encoded using a face encoder, and the audio signal is encoded using an audio en coder.</p>
</li>
<li>
<p>The encoded face image and audio signal are concatenated and fed into a face decoder to generate a talking face image that is in sync with the input audio signal.</p>
</li>
<li>
<p>The generated talking face image is fed into a discriminator network to determine whether it is syn chronized with the audio signal.</p>
</li>
<li>
<p>The discriminator network is trained using a contrastive loss, while the generator is trained using an L1 reconstruction loss.</p>
</li>
</ol>
<p>6.The model is trained in a GAN-like fashion, with the generator and discriminator competing against each other to improve the quality of the generated talking face images.Human: You are a professional reviewer.You need to review a paper and discuss the novelty and originality of ideas, correctness, clarity, the significance of results, potential impact and quality of the presentation.This is the paper for your review: <paper content> AI: Summary of the paper:</p>
<p>This paper proposes a Knowledge Graph Domain Adaptation Framework (KGDA) to construct a KG from fully unlabelled raw text data with the assistance of an existing KG in another domain.The key idea is to em ploy distant supervision to learn the mapping function from the input text to the KG space.The authors claim that their framework can discover new knowledge in the target domain.</p>
<p><strong>Strength</strong> 1.The topic is interesting and important.<strong>Weaknesses</strong> 1.My major concern is about the technical novelty.Most of the components in the proposed framework are borrowed from existing works.The most significant contribution seems to be the iterative training strategy.However, the authors do not provide sufficient evidence to support the superiority of the proposed method.2. The writing needs to be improved.Some parts are hard to follow.3. The experimental setup is not convincing.The authors did not compare with state-of-the-art base lines.</p>
<p>Summary of the review:My major concern is about the lack of technical novelty.Please refer to the Strength &amp; Weaknesses Section.</p>
<p>Rating:marginally below the acceptance threshold Human: You are a professional reviewer.You need to review a paper and discuss the novelty and originality of ideas, correctness, clarity, the significance of results, potential impact and quality of the presentation.This is the paper for your review: <paper content> AI: Summary of the paper:</p>
<p>This paper proposes a new learning framework called "assisted learning" to address the issue of limited data availability in real-world applications.The key idea is to leverage the existence of a third-party service provider with abundant data to boost the learning performance of the learner.The authors pro vide theoretical analyses showing the convergence of the proposed approach.Empirical results on image classification and reinforcement learning demonstrate the effectiveness of the proposed approach.Strengths: 1.The motivation of the paper is clear and easy to follow.2. The proposed approach is simple and intuitive.3. Theoretical analyses are provided to support the proposed approach.</p>
<p>Weaknesses: 1.My biggest concern about this paper is the lack of justification for the proposed approach.Why should we design a new learning framework instead of directly leveraging the pre-trained model from the service provider?What are the advantages of the proposed approach compared to simply fine-tuning the pre-trained weights from the service provider?2. Another question is how to choose the hyperparameters γ L and γ P .If γ L ≪ γ P , then most of the data points come from the service provider.Doesn't it mean that the learner doesn't really benefit from the presence of the service provider?How to balance the trade-off between them? 3. The assumption that the learner and service provider have the same data distribution seems too strong.Is it possible to relax this assumption?4. Since the focus of this paper is on helping the learner with limited data, why don't the authors compare the proposed approach with methods like knowledge distillation?Knowledge distillation is widely used to transfer knowledge from a teacher model to a student model with limited data.5.The writing of the paper could be improved.For example, the authors should explain what x t,i means in Eq. (1).In stage 1, we use NTK (Chen et al., 2023a;Peng et al., 2023) to extent our window size to 32K, and we continue training on 5B sampled from our data collection as shown in Tabel 1.In stage 2, we use the LongAlpaca-12k (Chen et al., 2023b,c) dataset for fully supervised finetune.The LongAlpaca-12k dataset comprises 9k long QA entries and an additional 3k short QA entries sampled from the original Alpaca dataset.This mix ensures that the model's proficiency in responding to shorter instructions remains unaffected.In line with the conventional Alpaca structure, the Long QA data adopts the following prompts for fine-tuning: 1) instruction: a string that lays out the task for the model.For instance, it might direct the model to answer a query after examining a segment of a book or a research paper.They have diversified the content and queries to ensure a wide range of instructions.2) output: a string providing the response to the given instruction.</p>
<p>In engineering, we can also use some other methods to extend the window size.One choice is to train a small model, such as LLaMA-7B, to extract context information, and then use our AcademicGPT to generate the final answer.</p>
<p>In Figure 4.6 and Figure 4.7, we have shown two cases to demonstrate the system of AI-assisted reading.</p>
<p>Paper Review</p>
<p>Data Collection and Cleaning.The data for our paper review is from OpenReview11 .We scraped 29,119 papers and 79,000 reviews from OpenReview.After that, we filtered out 7,115 papers that did not contain PDFs or review comments.Further, we removed some specific strings, such as "Under review as a conference paper at ICLR 2023" and "Anonymous authors Paper under double-blind review", and also deleted content from failed PDF parsing.For the review cleaning, we removed reviews with excessive line breaks, those shorter than 100 tokens or longer than 2,000 tokens, and those inconsistent with the decision having the lowest confidence.As Review Advisor (Yuan et al., 2022) 12 , we consider from seven aspects including "clarity", "meaningful comparison", "motivation", "originality", "replicability", "soundness", "substance", we use their opensource code to annotate the data.Finally, we obtain 22,213 papers with 67,874 review comments for training and 500 papers with 1,513 review comments for testing.</p>
<p>SFT data format for Paper Review</p>
<p>''You are a professional reviewer in the field of computer science and artificial intelligence.I will give you a paper.You need to review this paper and discuss the novelty and originality of ideas, correctness, clarity, the significance of results, potential impact, and quality of the presentation.You need to give a complete review opinion including the strengths of this paper, your main concerns regarding this paper, and specific reasons for its assessment.This is the paper for your review: Paper Content'' Output: [review comment] SFT Details.Our SFT data format is shown in Figure 4.11.We finetune the paper review model on AcademicGPT.The maximum length is 8,192 instead of the original 4,096 in LLaMA2 and AcademicGPT.We use a batch size set of 128 and finetune 3 epochs.</p>
<p>Metrics.We employed three metrics for evaluation.The first is the accuracy of the final recommendation results; a recommendation is considered correct if it aligns with the meta-review recommendation, and incorrect otherwise.The second metric involves the accuracy related to the seven aspects mentioned earlier; if the aspects reported by us are also mentioned in the meta review, they are deemed accurate, otherwise incorrect.For instance, if out of M predictions, K are correct, the accuracy stands at K M .The third metric is the recall rate.For example, if the meta mentions N items for evaluation across all evaluation papers and we recall L, then the recall is L N .The evaluation metrics used in this report are motivated by Review Advisor (Yuan et al., 2022).The results are shown in Table 8.</p>
<p>Methods</p>
<p>Final From Table 8, we can see that our final recommendation accuracy is 68%.In the paper Review Adviser (Yuan et al., 2022), the authors concluded that the language models at the time did not possess the capability for paper reviewing.Our perspective is that the paper review script has certain reviewing capabilities and can serve as a reference comments for the authors.However, there is still a noticeable gap compared to human reviewers.</p>
<p>In Figure 4.8, Figure 4.9, and Figure 4.10, we show three cases of paper review.</p>
<p>AI-assisted Title and Abstract Generation</p>
<p>The essence of a research paper is often captured in its title and abstract.They not only provide a snapshot of the study's content but also play a pivotal role in garnering readers' attention.It is, therefore, paramount that they are crafted with precision and clarity.</p>
<p>AI-assisted Title and Abstract Generation (Case 1)</p>
<p>Input: the introduction part from the paper ''DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework'', the paper url is https://arxiv.org/pdf/2310.12081.pdf.</p>
<p>AI:</p>
<p>Title:DHOT-GM: Differentiable Hierarchical Optimal Transport for Graph Matching Abstract:The task of graph matching attempts to find the node correspondence between two or more graphs, which is fundamental in graph-related applications.While most existing methods match graphs merely based on the adjacency matrices, or the node attributes, or the subgraph structures, they fail to consider the interactions between different modalities of graphs and are less robust in practice.To address these issues, we propose a multi-modal graph matching method, called DHOT-GM, based on a dif ferentiable hierarchical optimal transport (DHOT) framework.Specifically, we capture the multi-modal information of graphs by a set of relational matrices.The matching results within different modalities and across modalities are measured by the lower-level optimal transport matrices.We further learn the significance of the modalities and the weights of the modality pairs by solving an upper-level op timal transport problem.The two-level optimal transport problems are solved iteratively, leading to the proposed DHOT framework.Compared with existing methods, our DHOT-GM method explicitly considers the matching results across different modalities and demonstrates their contributions to improving the final matching performance.Additionally, by learning the weights, our DHOT-GM method can find useful modalities and rely more on the reliable matching results.Extensive experiments on both synthetic and real-world graph matching tasks demonstrate the superiority of our method and its robustness in highly noisy cases.</p>
<p>Ground Truth: Title:DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework Abstract:Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs.Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc.In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM.Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities.Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results.This method can be implemented as computing the HOT distance between the two graphs | each matching result is an optimal transport plan associated with the Gromov-Wasserstein (GW) distance between two relational matrices, and the weights of all matching results are the elements of an upper-level optimal transport plan defined on the matrix sets.We propose a bi-level optimization algorithm to compute the HOT distance in a differentiable way, making the significance of the relational matrices adjustable.Experiments on various graph matching tasks demonstrate the superiority and robustness of our method compared to state-of-the-art approaches.</p>
<p>Figure 4.12: The 1st case for AI-assisted generation.</p>
<p>In our technical report, inspired by the readers' understanding process, we designed our data formulation as: "Introduction Experiments Results <begin generate>Title:content;Abstract:content.".Where the content in Italy font is optional."<begin generate>" is a special token to trigger the generation process.</p>
<p>To ensure robustness in our approach, we drew data from an extensive corpus of 1 million research papers.Our methodology employed a supervised training mode, harnessing the power of labeled data to guide our AI-assisted Title and Abstract Generation (Case 2) Input: the introduction part from the paper ''Fast and Low-Cost Approximate Multiplier for FPGAs using Dynamic Reconfiguration'', the paper url is https://arxiv.org/pdf/2310.10053.pdf.</p>
<p>AI:</p>
<p>Title:DyRecMul: Low-Power Reconfigurable Approximate Multiplier for Machine Learning Inference on FPGAs Abstract:Approximation techniques can accelerate multiplication operations by improving efficiency in terms of delay, energy consumption, and circuit area, which is critical for applications such as machine learning that require a large number of multiplications.This letter proposes DyRecMul, a dynamically reconfigurable approximate multiplier, which is optimized for machine learning inference on AMD-Xilinx FPGAs.It utilizes approximate 3-2 and 4-2 compressors in a reconfigurable partial product accumulation tree.DyRecMul is optimized for low-cost multiplication of short bit-width operands.It also addresses the limited dynamic range of such operands by transforming fixed-point numbers to a floating-point representation.The letter presents the design details of an INT8 version of the multiplier and its eval uation results.We demonstrate that DyRecMul offers a 64% reduction in delay and 49% reduction in energy consumption, while the accuracy loss is negligible in inference tasks.</p>
<p>Ground Truth: Title:Fast and Low-Cost Approximate Multiplier for FPGAs using Dynamic Reconfiguration Abstract:Multipliers are widely-used arithmetic operators in digital signal processing and machine learning circuits.Due to their relatively high complexity, they can have high latency and be a significant source of power consumption.One strategy to alleviate these limitations is to use approximate computing.This paper thus introduces an original FPGA-based approximate multiplier specifically optimized for machine learning computations.It utilizes dynamically reconfigurable lookup table (LUT) primitives in AMD-Xilinx technology to realize the core part of the computations.The paper provides an in-depth analysis of the hardware architecture, implementation outcomes, and accuracy evaluations of the multiplier proposed in INT8 precision.Implementation results on an AMD-Xilinx Kintex Ultrascale+ FPGA demonstrate remarkable savings of 64% and 67% in LUT utilization for signed multiplication and multiply-and-accumulation configurations, respectively, when compared to the standard Xilinx multiplier core.Accuracy measurements on four popular deep learning (DL) benchmarks indicate a minimal average accuracy decrease of less than 0.29% during post-training deployment, with the maximum reduction staying less than 0.33%.The source code of this work is available on GitHub.We present two generation cases in Figure 4.12 and Figure 4.13.These figures showcase the model's ability to generate coherent and relevant titles and abstracts based on new test data, underscoring the potential of our approach in aiding the academic community.</p>
<p>Conclusion</p>
<p>In this technical report, we have illuminated two principal advancements we made in the realm of academic research.Firstly, we introduce AcademicGPT, A LLM tailored specifically for academic research.Trained on a colossal 120 billion tokens, it underscores the potential of extensive academic datasets, ensuring a high degree of precision in grasping scientific subtleties.Secondly, we have taken AcademicGPT's capabilities further by applying it in a range of applications, from a nuanced General Academic Q&amp;A system to AI-assisted reading and content creation.Our Q&amp;A tool, empowered by the ReAct framework, enriches academic dialogues by maintaining context.Furthermore, our initiatives in simplifying dense academic texts and in reviewing papers position AI as an indispensable tool for researchers.Notably, the adaptability our AI showcases in content generation, like abstracts, highlights its versatility.In conclusion, AcademicGPT and its associated applications represent a pioneering leap in bridging advanced AI technologies with the demands of academic research.Through these endeavors, we anticipate a substantial shift in how information is processed, interacted with, and generated within the academic sphere.</p>
<p>Figure 1 . 1 :
11
Figure 1.1:An overview of AcademicGPT and four applications around it.</p>
<p>Figure 3 . 1 :
31
Figure 3.1: Prompt to label data.</p>
<p>ComputerScienceQA.The ComputerScienceQA evaluation benchmark is built based on PapersWithCode 8 .Papers With Code showcases trending Machine Learning research along with the corresponding implementation code.</p>
<p>Question</p>
<p>Figure 3.2: Two QA samples from our ComputerScienceQA.</p>
<p>Figure 4 . 1 :
41
Figure 4.1: Overview of our AcademicGPT-powered academic question answering engine.</p>
<p>{ action: $TOOL NAME, action input: $INPUT } ALWAYS use the following format: Thought: you should always think about what to do Action: $JSON BLOB Observation: the result of the action... (this Thought/Action/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Input prompt: User: [User's utterence] AI: [AI's utterence] User: [User's utterence]</p>
<p>Figure 4 . 2 :
42
Figure 4.2: The ReAct prompt we used in our general academic question answering.</p>
<p>Figure 4 . 5 :
45
Figure 4.5: The 3rd case for General Academic Question Answering.</p>
<p>Figure 4
4
Figure 4.6: The 1st case for AI-assisted reading.</p>
<p>7.</p>
<p>The final objective function of the model is a combination of the GAN objective function and the reconstruction loss.8.During inference, the model generates the talking face image of the target speaker frame-by-frame using the input audio signal and the current frame as input.</p>
<p>Figure 4
4
Figure 4.7: The 2nd case for AI-assisted reading.</p>
<p>Figure 4 . 8 :
48
Figure 4.8: The 1st case for paper review.</p>
<p>Figure 4 . 9 :
49
Figure 4.9: The 2nd case for paper review.</p>
<p>Figure 4 .
4
Figure 4.10: The 3rd case for paper review.</p>
<p>Figure 4 .
4
Figure 4.11: SFT data format for Paper Review.</p>
<p>Figure 4 .
4
Figure 4.13: The 2nd case for AI-assisted generation.</p>
<p>Table 1 :
1
Data for our continual pre-training.
.1; (3) we train a LLM for classification using
SYS PROMPT = ''In terms of checking data quality, you are a helpful and precise assistant.''PROMPT= """ Please assess the provided CommonCrawl sample based on the following criteria and return the results in the specified JSON format.The evaluation criteria are as follows: 1. Quality: Evaluate grammar completeness, language coherence, information accuracy, and the presence of low-quality content such as explicit, violent, advertising, promotional, or recruitment-related information.Categorize quality as ''Excellent'', ''Average'', or ''Poor''; 2. Domain: Determine if the sample is related to fields such as computer science, natural sciences, social sciences, engineering and technology, medical and health, arts and literature, humanities, economics and management, law, education, agricultural sciences, space sciences, etc., and categorize it accordingly; 3. Depth: Assess the content as ''Beginner'', ''Intermediate'', ''Advanced'', or ''Expert''; 4. Category: Identify whether it falls under the category of ''Academic Article'', ''Academic Report'', ''Monograph'', ''Whitepaper'', ''Technical Blog'', ''Popular Science Article'',''Forum Discussion'',''News Report'', or ''Promotional Content''; 5. Suitability Rating: Determine whether the sample is suitable for training academic large models, imparting serious knowledge to the model, enhancing the model's academic, common-sense, logical, and reasoning capabilities.Suitability has three standards: ''Highly Suitable'', ''Average'', or ''Not Suitable''.The returned results should be in the following format: { Quality: Excellent/Average/Poor, Domain: Computer Science/Natural Sciences/Social Sciences/Engineering and Technology/Medical and Health/Arts and Literature/Other/Promotional Content, Depth: Beginner/Intermediate/Advanced/Expert, Category: Academic Article/Academic Report/Monograph/Whitepaper/Technical Blog/Popular Science Article/Forum Discussion/News Report/Promotional Content/Other, Suitability: Highly Suitable/Average/Not Suitable }.</p>
<p>Table 2 :
2
Comparison of different methods on MMLU.
MethodsAccuracyLLaMA-65B0.634LLaMA2-70B0.693ChatGPT0.664AcademicGPT0.688</p>
<p>Table 3 :
3
Result comparison of AcademicGPT and LLaMA2 on some subjects in MMLU.Results on CEval.To evaluate the capacity of AcademicGPT on Chinese language, we evaluate it on CEval benchmark, and compare it with several other methods.
Methodscollege computer science college biology high school geography sociologyLLaMA2-70B0.580.8130.8890.881AcademicGPT0.620.8470.8550.851MethodsAccuracyLLaMA-65B0.390LLaMA2-70B0.508ChatGPT0.471AcademicGPT0.551</p>
<p>Table 4 :
4
Result comparison of different methods on CEval.</p>
<p>Table 5 :
5
Result comparison of different methods on PubMedQA.The result of GPT4 can be found at https://pubmedqa.github.io/.
AccuracyLLaMA-65B0.772LLaMA2-70B0.776ChatGPT0.716GPT-4-Base0.804AcademicGPT0.806</p>
<p>Table 6 :
6
Result comparison of different methods on SCIEval.
Chemistry Physics Average AccuracyLLaMA1-65B0.7740.5960.4700.613LLaMA2-70B0.7970.6490.4630.636ChatGPT0.8130.7050.5180.679AcademicGPT0.8000.6800.5850.688</p>
<p>Table 7 .
7Methodsmethods-intro methods-refer datasets-intro datasets-refer OverallLLaMA-65B0.5190.8820.6080.8290.710LLaMA2-70B0.6410.9320.7620.8610.799ChatGPT0.7150.9370.7530.8390.811AcademicGPT0.7670.9130.7770.8830.835</p>
<p>Table 7 :
7
Result comparison of different methods on ComputerScienceQA.</p>
<p>Table 8 :
8
Results of AcademicGPT and Human Reviewer on paper review.The accuracy and recall rates of Human Reviewers are determined based on their consistency with meta-reviewers.
Recommendation Accuracy Aspect Recall Aspect AccuracyAcademicGPT68.4%76.4%24.8%Human85.2%81.6%26.0%
https://arxiv.org/
https://unpaywall.org/
https://huggingface.co/datasets/tiiuae/falcon-refinedweb
https://www.semanticscholar.org/
https://pubmedqa.github.io/
https://github.com/OpenDFM/BAI-SciEval
https://socratic.org/socratic/contributing-to-socratic/asking-questions
https://paperswithcode.com/
https://huggingface.co/WizardLM
https://github.com/GaloisInc/LIMA
https://openreview.net/
https://github.com/neulab/ReviewAdvisor
Human: What is the state-of-the-art performance on the CIFAR-10 dataset?Which paper does it come from?AI: According to the provided information, the current state-of-the-art performance on the CIFAR-10 dataset is achieved by the paper "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" with a percentage correct of 99.5 and 632M parameters.The paper was published on October 22, 2020.Here is the full reference for the paper: ** An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ** Published: October 22, 2020 URL: https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1The paper proposes a novel approach to image recognition using transformers, achieving state-of-the-art performance on the CIFAR-10 dataset.The author introduces a new architecture that utilizes a combi nation of local and global context to improve the recognition of images.The paper also provides a thorough evaluation of the model's performance and compares it to other state-of-the-art methods.It's worth noting that the performance of the model can be improved by using a larger dataset, pre-train ing the model on a larger dataset, or using a different architecture.Human: what are the currently best Large Language Models?AI: Currently, the best methods for large language models include: Meta AI's Llama: Released in 2023, Llama is a large language model developed by Meta, with the largest version being 65 billion parameters in size.ChatGPT: Developed by OpenAI, ChatGPT is a popular language model that has been used for a wide range of natural language processing tasks, including text generation, question answering, and language trans lation.Galactica: Developed by Meta, Galactica is a large language model that has been used for a variety of tasks, including text generation and language translation.LaMDA: Developed by Google, LaMDA is a large language model that has been used for a wide range of natu ral language processing tasks, including text generation, question answering, and language translation.GPT-3: Developed by OpenAI, GPT-3 is a large language model that has been used for a wide range of natu ral language processing tasks, including text generation, question answering, and language translation.Megatron-Turing: Megatron-Turing is a large language model that has been used for a variety of tasks, including text generation and language translation.It's worth noting that the field of large language models is rapidly evolving, and new models are being developed and improved upon regularly.These are some of the current state-of-the-art models, but there may be others that have been developed since the cutoff date of the knowledge base.
The falcon series of language models: Towards open frontier models. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Alhammadi, M Daniele, D Heslow, J Launay, Q Malartic, 2023Hugging Face repository</p>
<p>. J L Ba, J R Kiros, G E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, B Hui, L Ji, M Li, J Lin, R Lin, D Liu, G Liu, C Lu, K Lu, J Ma, R Men, X Ren, X Ren, C Tan, S Tan, J Tu, P Wang, S Wang, W Wang, S Wu, B Xu, J Xu, A Yang, H Yang, J Yang, S Yang, Y Yao, B Yu, H Yuan, Z Yuan, J Zhang, X Zhang, Y Zhang, Z Zhang, C Zhou, J Zhou, X Zhou, T Zhu, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>S Black, S Biderman, E Hallahan, Q Anthony, L Gao, L Golding, H He, C Leahy, K Mcdonell, J Phang, arXiv:2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022arXiv preprint</p>
<p>L Blecher, G Cucurull, T Scialom, R Stojnic, arXiv:2308.13418Nougat: Neural optical understanding for academic documents. 2023arXiv preprint</p>
<p>A M Bran, S Cox, A D White, P Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Extending context window of large language models via positional interpolation. S Chen, S Wong, L Chen, Y Tian, arXiv:2306.155952023aarXiv preprint</p>
<p>Gpt-pinn: Generative pre-trained physics-informed neural networks toward non-intrusive meta-learning of parametric pdes. Finite Elements in Analysis and Design. Y Chen, S Koohy, 2024228104047</p>
<p>Y Chen, S Qian, H Tang, X Lai, Z Liu, Han, J Jia, arXiv:2309.12307Longlora: Efficient fine-tuning of long-context large language models. 2023b</p>
<p>Y Chen, S Yu, S Qian, H Tang, X Lai, Z Liu, S Han, J Jia, Long alpaca: Long-context instruction-following models. 2023c</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>J Cui, Z Li, Y Yan, B Chen, L Yuan, arXiv:2306.16092Chatlaw: Open-source legal large language model with integrated external knowledge bases. 2023arXiv preprint</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. T Dao, arXiv:2307.086912023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, arXiv:2103.10360Glm: General language model pretraining with autoregressive blank infilling. 2021arXiv preprint</p>
<p>Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. W Fedus, B Zoph, N Shazeer, The Journal of Machine Learning Research. 2312022</p>
<p>L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>K Gupta, B Thérien, A Ibrahim, M L Richter, Q Anthony, E Belilovsky, I Rish, T Lesort, arXiv:2308.04014Continual pre-training of large language models: How to (re) warm your model?. 2023arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, arXiv:2305.083222023arXiv preprint</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>X Jin, D Zhang, H Zhu, W Xiao, S.-W Li, X Wei, A Arnold, X Ren, arXiv:2110.08534Lifelong pretraining: Continually adapting language models to emerging corpora. 2021arXiv preprint</p>
<p>D Kalamkar, D Mudigere, N Mellempudi, D Das, K Banerjee, S Avancha, D T Vooturi, N Jammalamadaka, J Huang, H Yuen, arXiv:1905.12322A study of bfloat16 for deep learning training. 2019arXiv preprint</p>
<p>Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. E Karpas, O Abend, Y Belinkov, B Lenz, O Lieber, N Ratner, Y Shoham, H Bata, Y Levine, K Leyton-Brown, arXiv:2205.004452022arXiv preprint</p>
<p>Continual pre-training of language models. Z Ke, Y Shao, H Lin, T Konishi, G Kim, B Liu, The Eleventh International Conference on Learning Representations. 2022</p>
<p>The bigscience roots corpus: A 1.6 tb composite multilingual dataset. H Laurençon, L Saulnier, T Wang, C Akiki, A Villanova Del Moral, T Le Scao, L Von Werra, C Mou, E González Ponferrada, H Nguyen, Advances in Neural Information Processing Systems. 352022</p>
<p>Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 86111998</p>
<p>Sgdr: Stochastic gradient descent with warm restarts. I Loshchilov, F Hutter, arXiv:1608.039832016arXiv preprint</p>
<p>I Loshchilov, F Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Gpt-4v(ision) system card. Openai, arXiv:2303.08774.OpenAIGpt-4 technical report. 2023a. 2023barXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>G Penedo, Q Malartic, D Hesslow, R Cojocaru, A Cappelli, H Alobeidli, B Pannier, E Almazrouei, J Launay, arXiv:2306.01116The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>Yarn: Efficient context window extension of large language models. B Peng, J Quesnelle, H Fan, E Shippole, arXiv:2309.000712023arXiv preprint</p>
<p>Semi-supervised sequence tagging with bidirectional language models. M E Peters, W Ammar, C Bhagavatula, R Power, arXiv:1705.001082017arXiv preprint</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. O Press, N A Smith, M Lewis, arXiv:2108.124092021arXiv preprint</p>
<p>Understanding optimization of deep learning via jacobian matrix and lipschitz constant. X Qi, J Wang, L Zhang, arXiv:2306.093382023arXiv preprint</p>
<p>Y Qin, J Zhang, Y Lin, Z Liu, P Li, M Sun, J Zhou, arXiv:2203.06311Elle: Efficient lifelong pre-training for emerging data. 2022arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>Zero: Memory optimizations toward training trillion parameter models. S Rajbhandari, J Rasley, O Ruwase, Y He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. J Rasley, S Rajbhandari, O Ruwase, Y He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>B Rozière, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Tree-based representation and generation of natural and mathematical language. A Scarlatos, A Lan, arXiv:2302.079742023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>R Sennrich, B Haddow, A Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>N Shazeer, arXiv:2002.05202Glu variants improve transformer. 2020arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023arXiv preprint</p>
<p>K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, arXiv:2212.13138Large language models encode clinical knowledge. 2022arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, Y Lu, S Pan, A Murtadha, B Wen, Y Liu, arXiv:2104.098642021arXiv preprint</p>
<p>L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, arXiv:2308.13149Scieval: A multi-level large language model evaluation benchmark for scientific research. 2023arXiv preprint</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Y Sun, S Wang, S Feng, S Ding, C Pang, J Shang, J Liu, X Chen, Y Zhao, Y Lu, arXiv:2107.021372021arXiv preprint</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Redpajama: An open source recipe to reproduce llama training dataset. Togetherai, 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Selfconsistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chainof-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Llm-powered autonomous agents. L Weng, 2023</p>
<p>C Wu, S Yin, W Qi, X Wang, Z Tang, N Duan, arXiv:2303.04671Visual chatgpt: Talking, drawing and editing with visual foundation models. 2023aarXiv preprint</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023barXiv preprint</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>W Xiong, J Liu, I Molybog, H Zhang, P Bhargava, R Hou, L Martin, R Rungta, K A Sankararaman, B Oguz, arXiv:2309.16039Effective long-context scaling of foundation models. 2023arXiv preprint</p>
<p>A Yang, B Xiao, B Wang, B Zhang, C Yin, C Lv, D Pan, D Wang, D Yan, F Yang, arXiv:2309.10305Baichuan 2: Open large-scale language models. 2023aarXiv preprint</p>
<p>Z Yang, L Li, K Lin, J Wang, C.-C Lin, Z Liu, L Wang, arXiv:2309.17421The dawn of lmms: Preliminary explorations with gpt-4v (ision). 2023barXiv preprint</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Wudaocorpora: A super large-scale chinese corpora for pre-training language models. S Yuan, H Zhao, Z Du, M Ding, X Liu, Y Cen, X Zou, Z Yang, J Tang, AI Open. 22021</p>
<p>Can we automate scientific reviewing. W Yuan, P Liu, G Neubig, Journal of Artificial Intelligence Research. 752022</p>
<p>A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprint</p>
<p>Root mean square layer normalization. B Zhang, R Sennrich, Advances in Neural Information Processing Systems. 201932</p>
<p>S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.01068Opt: Open pre-trained transformer language models. 2022arXiv preprint</p>
<p>Large-scale domain-specific pretraining for biomedical vision-language processing. S Zhang, Y Xu, N Usuyama, J Bagga, R Tinn, S Preston, R Rao, M Wei, N Valluri, C Wong, arXiv:2303.009152023arXiv preprint</p>
<p>Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, arXiv:1905.07129Ernie: Enhanced language representation with informative entities. 2019arXiv preprint</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Q Zheng, X Xia, X Zou, Y Dong, S Wang, Y Xue, Z Wang, L Shen, A Wang, Y Li, arXiv:2303.175682023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
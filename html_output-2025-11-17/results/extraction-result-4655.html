<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4655 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4655</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4655</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-261582775</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.03736v1.pdf" target="_blank">TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs), prominently highlighted by the recent evolution in the Generative Pre-trained Transformers (GPT) series, have displayed significant prowess across various domains, such as aiding in healthcare diagnostics and curating analytical business reports. The efficacy of GPTs lies in their ability to decode human instructions, achieved through comprehensively processing historical inputs as an entirety within their memory system. Yet, the memory processing of GPTs does not precisely emulate the hierarchical nature of human memory. This can result in LLMs struggling to prioritize immediate and critical tasks efficiently. To bridge this gap, we introduce an innovative LLM multi-agent framework endowed with layered memories. We assert that this framework is well-suited for stock and fund trading, where the extraction of highly relevant insights from hierarchical financial data is imperative to inform trading decisions. Within this framework, one agent organizes memory into three distinct layers, each governed by a custom decay mechanism, aligning more closely with human cognitive processes. Agents can also engage in inter-agent debate. In financial trading contexts, LLMs serve as the decision core for trading agents, leveraging their layered memory system to integrate multi-source historical actions and market insights. This equips them to navigate financial changes, formulate strategies, and debate with peer agents about investment decisions. Another standout feature of our approach is to equip agents with individualized trading traits, enhancing memory diversity and decision robustness. These sophisticated designs boost the system's responsiveness to historical trades and real-time market signals, ensuring superior automated trading accuracy.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4655.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4655.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TradingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TRADINGGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered multi-agent trading framework that assigns each agent layered memories (short-, middle-, long-term) with custom decay, uses embeddings + FAISS for storage/retrieval, and enables inter-agent debates and character-driven decision diversity to support automated financial trading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TradingGPT agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-agent system where each agent uses an LLM core (prompted LLM) to generate trading evaluations and reflections based on retrieved items from a three-layer memory (short/middle/long). Agents can perform single-agent reflections and participate in multi-agent debates; agents also have distinct character profiles (risk preferences, sectors) affecting memory salience.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5-turbo (used for prompt design; other LLMs planned for ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automated financial trading: generate trading recommendations (five discrete actions from significantly increase to significantly decrease position), execute trades, and reflect/evaluate performance using multi-modal financial data and memory history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Layered external memory with three layers (short-, middle-, long-term) using decay-based recency scoring, relevancy, and importance; retrieval-augmented via embedding similarity and ranking; includes reflection and debate classes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual memory events (market facts, agent reflections, debates, trading records) stored as embeddings in a FAISS vector store; reflection indices and debate-tagged records also stored.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Memories are placed into one of three lists by predefined rules; recency decays via layer-specific stability Q values (Q_long=365, Q_middle=90, Q_short=3); an add-counter boosts scores for events triggered by significant P/L to promote longevity; immediate reflections stored daily and extended reflections periodically (e.g., weekly). Low-scoring events (score < 20) are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Per-layer top-K retrieval using a linear combination ranking score γ = α·Recency + β·Relevancy + λ·Importance; relevancy computed as cosine similarity between memory and prompt embeddings (FAISS used for semantic search); layer-specific thresholds govern inclusion (thresholds reported: long=80, middle=60, short=40).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No completed ablation results reported yet — authors state planned ablation studies to compare backbone LLMs (e.g., GPT-3.5 turbo vs. CodeLlama 34B) and to evaluate components (memory strategies, debate). Current work is at prompt design and planned evaluation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>No empirical trading results reported yet. Authors note potential issues when LLMs process memory as a whole (difficulty prioritizing immediate/critical events), the need to tune many hyperparameters (α, β, λ, Q values, thresholds, add-counter), and differences between training (with fund trading records) and testing (without those records) that complicate evaluation. Scalability/latency of frequent retrievals and debate phases are not empirically analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Design memory in layered form aligned with human short/mid/long-term distinctions; compute retrieval ranking from recency (exponential decay with layer-specific stability), embedding-based relevancy (cosine similarity), and an importance constant per layer; use add-count promotion for frequently triggered events; store embeddings in FAISS for fast semantic retrieval; combine immediate (daily) and extended (weekly) reflections; use character profiles and multi-agent debate to diversify salient memories and improve robustness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4655.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4655.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative agent framework (Park et al.) where each agent maintains its own memory stream and character profile; agents archive experiences and retrieve key memories by ranking them on recency, significance, and relevance to support behavior and collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agent (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents maintain individual memory streams and character seeds, record actions and observations, and retrieve ranked memories (by recency, significance, relevance) to condition LLM responses for simulated human-like behavior and collaborative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Interactive simulation of human behavior / collaborative task completion (not a text game benchmark); agents respond to tasks and interact using retrieved memories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Per-agent memory streams with weighted retrieval scores combining recency, significance, and relevance (retrieval-augmented generation style).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Archived agent experiences and actions (textual), weighted memories; likely stored as textual entries and embeddings (as described by Park et al. in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Agents archive experiences as they occur; memories accumulate in per-agent streams and are re-ranked when queried according to recency/significance/relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Ranked retrieval using a computed retrieval score per memory (linear combination of recency, significance, relevance) to select top memories for prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>This paper cites Park et al.'s weighted retrieval approach as an inspiration and redesigns the mathematical representations for layered memories, but does not report Park et al.'s ablations here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper argues the prior approach of retrieving from all memories as an entirety can make LLMs inefficient at prioritizing immediate/critical events; Park et al.'s approach motivates introducing explicit layered memories and refined scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Leverage per-agent memory streams and rank memories by recency, significance, and relevance; tailor retrieval selection to task context; incorporate character profiles to shape memory salience.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4655.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4655.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate (Du et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improving Factuality and Reasoning in Language Models through Multiagent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A debate mechanism where multiple language model agents propose and contest answers, improving factuality and reasoning through multi-agent exchange; cited as a mechanism to enhance cooperative decision-making among trading agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Factuality and Reasoning in Language Models through Multiagent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Debate-enabled LLM agents (Du et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A framework where multiple instances of language models engage in structured debates to converge on more accurate or better-reasoned outputs; included here as the inter-agent debate mechanism inspiration for TradingGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve factuality and reasoning of LLM outputs via multi-agent debate; in this paper, cited for improving cooperative decision-making (applied to finance debates among trading agents).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Referenced as prior work showing benefits of debate mechanisms for reasoning and factuality; no ablation details presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper references debate as beneficial but does not discuss its computational cost or interaction with layered memory in detail; integration complexity and overhead are implied but not empirically analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use debate phases among agents to enhance cooperative decision-making and to surface diverse memories/reflections for better consensus; store debate feedback for later retrieval.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Improving Factuality and Reasoning in Language Models through Multiagent Debate <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4655",
    "paper_id": "paper-261582775",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "TradingGPT",
            "name_full": "TRADINGGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance",
            "brief_description": "An LLM-powered multi-agent trading framework that assigns each agent layered memories (short-, middle-, long-term) with custom decay, uses embeddings + FAISS for storage/retrieval, and enables inter-agent debates and character-driven decision diversity to support automated financial trading.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TradingGPT agent",
            "agent_description": "A multi-agent system where each agent uses an LLM core (prompted LLM) to generate trading evaluations and reflections based on retrieved items from a three-layer memory (short/middle/long). Agents can perform single-agent reflections and participate in multi-agent debates; agents also have distinct character profiles (risk preferences, sectors) affecting memory salience.",
            "llm_model_name": "GPT-3.5-turbo (used for prompt design; other LLMs planned for ablation)",
            "game_or_benchmark_name": null,
            "task_description": "Automated financial trading: generate trading recommendations (five discrete actions from significantly increase to significantly decrease position), execute trades, and reflect/evaluate performance using multi-modal financial data and memory history.",
            "memory_used": true,
            "memory_type": "Layered external memory with three layers (short-, middle-, long-term) using decay-based recency scoring, relevancy, and importance; retrieval-augmented via embedding similarity and ranking; includes reflection and debate classes.",
            "memory_representation": "Textual memory events (market facts, agent reflections, debates, trading records) stored as embeddings in a FAISS vector store; reflection indices and debate-tagged records also stored.",
            "memory_update_mechanism": "Memories are placed into one of three lists by predefined rules; recency decays via layer-specific stability Q values (Q_long=365, Q_middle=90, Q_short=3); an add-counter boosts scores for events triggered by significant P/L to promote longevity; immediate reflections stored daily and extended reflections periodically (e.g., weekly). Low-scoring events (score &lt; 20) are removed.",
            "memory_retrieval_mechanism": "Per-layer top-K retrieval using a linear combination ranking score γ = α·Recency + β·Relevancy + λ·Importance; relevancy computed as cosine similarity between memory and prompt embeddings (FAISS used for semantic search); layer-specific thresholds govern inclusion (thresholds reported: long=80, middle=60, short=40).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No completed ablation results reported yet — authors state planned ablation studies to compare backbone LLMs (e.g., GPT-3.5 turbo vs. CodeLlama 34B) and to evaluate components (memory strategies, debate). Current work is at prompt design and planned evaluation stage.",
            "challenges_or_limitations": "No empirical trading results reported yet. Authors note potential issues when LLMs process memory as a whole (difficulty prioritizing immediate/critical events), the need to tune many hyperparameters (α, β, λ, Q values, thresholds, add-counter), and differences between training (with fund trading records) and testing (without those records) that complicate evaluation. Scalability/latency of frequent retrievals and debate phases are not empirically analyzed in this paper.",
            "best_practices_or_recommendations": "Design memory in layered form aligned with human short/mid/long-term distinctions; compute retrieval ranking from recency (exponential decay with layer-specific stability), embedding-based relevancy (cosine similarity), and an importance constant per layer; use add-count promotion for frequently triggered events; store embeddings in FAISS for fast semantic retrieval; combine immediate (daily) and extended (weekly) reflections; use character profiles and multi-agent debate to diversify salient memories and improve robustness.",
            "uuid": "e4655.0"
        },
        {
            "name_short": "Generative Agents (Park et al.)",
            "name_full": "Generative agents: Interactive simulacra of human behavior",
            "brief_description": "A generative agent framework (Park et al.) where each agent maintains its own memory stream and character profile; agents archive experiences and retrieve key memories by ranking them on recency, significance, and relevance to support behavior and collaboration.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agent (Park et al.)",
            "agent_description": "Agents maintain individual memory streams and character seeds, record actions and observations, and retrieve ranked memories (by recency, significance, relevance) to condition LLM responses for simulated human-like behavior and collaborative tasks.",
            "llm_model_name": null,
            "game_or_benchmark_name": null,
            "task_description": "Interactive simulation of human behavior / collaborative task completion (not a text game benchmark); agents respond to tasks and interact using retrieved memories.",
            "memory_used": true,
            "memory_type": "Per-agent memory streams with weighted retrieval scores combining recency, significance, and relevance (retrieval-augmented generation style).",
            "memory_representation": "Archived agent experiences and actions (textual), weighted memories; likely stored as textual entries and embeddings (as described by Park et al. in the referenced work).",
            "memory_update_mechanism": "Agents archive experiences as they occur; memories accumulate in per-agent streams and are re-ranked when queried according to recency/significance/relevance.",
            "memory_retrieval_mechanism": "Ranked retrieval using a computed retrieval score per memory (linear combination of recency, significance, relevance) to select top memories for prompts.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "This paper cites Park et al.'s weighted retrieval approach as an inspiration and redesigns the mathematical representations for layered memories, but does not report Park et al.'s ablations here.",
            "challenges_or_limitations": "Paper argues the prior approach of retrieving from all memories as an entirety can make LLMs inefficient at prioritizing immediate/critical events; Park et al.'s approach motivates introducing explicit layered memories and refined scoring.",
            "best_practices_or_recommendations": "Leverage per-agent memory streams and rank memories by recency, significance, and relevance; tailor retrieval selection to task context; incorporate character profiles to shape memory salience.",
            "uuid": "e4655.1"
        },
        {
            "name_short": "Multi-Agent Debate (Du et al.)",
            "name_full": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "brief_description": "A debate mechanism where multiple language model agents propose and contest answers, improving factuality and reasoning through multi-agent exchange; cited as a mechanism to enhance cooperative decision-making among trading agents.",
            "citation_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "mention_or_use": "mention",
            "agent_name": "Debate-enabled LLM agents (Du et al.)",
            "agent_description": "A framework where multiple instances of language models engage in structured debates to converge on more accurate or better-reasoned outputs; included here as the inter-agent debate mechanism inspiration for TradingGPT.",
            "llm_model_name": null,
            "game_or_benchmark_name": null,
            "task_description": "Improve factuality and reasoning of LLM outputs via multi-agent debate; in this paper, cited for improving cooperative decision-making (applied to finance debates among trading agents).",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Referenced as prior work showing benefits of debate mechanisms for reasoning and factuality; no ablation details presented in this paper.",
            "challenges_or_limitations": "Paper references debate as beneficial but does not discuss its computational cost or interaction with layered memory in detail; integration complexity and overhead are implied but not empirically analyzed here.",
            "best_practices_or_recommendations": "Use debate phases among agents to enhance cooperative decision-making and to surface diverse memories/reflections for better consensus; store debate feedback for later retrieval.",
            "uuid": "e4655.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        }
    ],
    "cost": 0.008225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRADINGGPT: MULTI-AGENT SYSTEM WITH LAYERED MEMORY AND DISTINCT CHARACTERS FOR ENHANCED FINANCIAL TRADING PERFORMANCE</p>
<p>Yang Li 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Yangyang Yu 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Haohang Li 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Zhi Chen 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Khaldoun Khashanah kkhashan@stevens.edu 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>TRADINGGPT: MULTI-AGENT SYSTEM WITH LAYERED MEMORY AND DISTINCT CHARACTERS FOR ENHANCED FINANCIAL TRADING PERFORMANCE
Financial AIMulti-Modal LearningTrading AlgorithmsDeep LearningFinancial Technology
Large Language Models (LLMs), prominently highlighted by the recent evolution in the Generative Pre-trained Transformers (GPT) series, have displayed significant prowess across various domains, such as aiding in healthcare diagnostics and curating analytical business reports. The efficacy of GPTs lies in their ability to decode human instructions, achieved through comprehensively processing historical inputs as an entirety within their memory system. Yet, the memory processing of GPTs does not precisely emulate the hierarchical nature of human memory, which is categorized into long, medium, and short-term layers. This can result in LLMs struggling to prioritize immediate and critical tasks efficiently. To bridge this gap, we introduce an innovative LLM multi-agent framework endowed with layered memories. We assert that this framework is well-suited for stock and fund trading, where the extraction of highly relevant insights from hierarchical financial data is imperative to inform trading decisions. Within this framework, one agent organizes memory into three distinct layers, each governed by a custom decay mechanism, aligning more closely with human cognitive processes. Agents can also engage in inter-agent communication and debate. In financial trading contexts, LLMs serve as the decision core for trading agents, leveraging their layered memory system to integrate multi-source historical actions and market insights. This equips them to navigate financial changes, formulate strategies, and debate with peer agents about investment decisions. Another standout feature of our approach is to enable agents with individualized trading characters, which enrich the diversity of their highlighted essential memories and improve decision-making robustness. By leveraging agents' layered memory processing and consistent information interchange, the entire trading system demonstrates augmented adaptability to historical trades and real-time market cues. This synergistic approach guarantees premier automated trading with heightened execution accuracy.</p>
<p>Introduction</p>
<p>As the influx of diverse data streams continues to rise, there is a growing need for individuals to effectively harness information. This trend is particularly pronounced in the realm of finance, where traders must consider multiple sources to inform their investment decisions. In light of this demand, researchers design intelligent trading robot-agents that can synthesize and interpret data objectively [14,5]. These robot-agents harness diverse machine algorithms, assimilate a broader spectrum of data, autonomously refine trading strategies via methodical planning, and even potentially collaborate [7]. Here, we introduce an advanced LLM-powered multi-agent trading agent framework, supported by layered memories and customized characters. By employing a collaborative multi-agent system and capturing the intricate market dynamics from varied perspectives, this approach significantly enhances automated trading outcomes. This approach substantially elevates the performance of automated trading by fostering collaborative interactions among agents and capturing the intricate dynamics of the market from diverse perspectives.</p>
<p>Previous studies have introduced multi-agent trading algorithms that employ machine learning techniques, such as reinforcement learning and have reported significant performance outcomes [5]. Yet, these methods exhibit limitations in precisely identifying, representing, and emulating crucial components of trading systems. This includes aspects like agents' memory archives and the evolving social interplay among agents.</p>
<p>LLMs, with a particular focus on their recent advancements, such as the Generative Pre-trained Transformer (GPT), have demonstrated remarkable effectiveness in enhancing human decision-making across various domains [9]. Notably, a growing body of research has focused on harnessing this technology to make informed trading decisions for stocks and funds by continuously interacting with financial environment information [17,16]. While current financial LLM applications predominantly operate within single-agent systems based on textual uni-modality, their immense potential to elevate trading performance is becoming increasingly evident. Moreover, these financial agent systems make trading decisions relying solely on pre-trained LLMs or a memory system processing received information streams as an entirety. This can lead to a challenge for LLMs in efficiently prioritizing immediate and critical memory events for optimized trading.</p>
<p>Park et al. [10] recently introduced a generative agent framework aiming to enhance the efficient retrieval of critical events from agents empowered by LLMs. This structure comprises several agents, each distinguished by separate memory streams and unique character profiles configured by LLMs. Each agent, owning its seed memories, not only tracks its actions but also monitors other agents and environmental behaviors. Faced with a task, agents sift through memory segments to input into the language model, ranking them by recency, significance, and relevance. By archiving an agent's experiences, the system integrates individual weighted memories and the nuances of group dynamics. As a result, agents can collaboratively strategize, leveraging their collective knowledge. Moreover, Du et al. [3] presented a debate mechanism for LLM agents, emphasizing enhanced cooperative decision-making through debate phases in inter-agent memory interactions. These advancements align the LLM-driven multi-agent system more with human memory structures, paving the way for a more adept financial automated trading system.</p>
<p>Leveraging the capabilities of LLMs, we propose a novel trading agent framework, "TradingGPT". It offers a realistic scenario simulation through the integration of the trader's layered memory streams and character analysis. This framework is characterized by remarkable self-enhancement ability and performance to conduct automated trading and optimal execution. The primary contributions of our work include:</p>
<p>This represents a pioneering multi-agent trading system that integrates memory streams and debate mechanisms, anchored on LLMs. Building on Park et al.'s weighted memory mechanisms, our system innovatively categorizes the agent's memories into short-term, middle-term, and long-term layers, which are closely aligned with the structure of the human cognitive system. We adapt this layered memory framework to the financial trading system, equipping agents to reflect on past and present events, derive insights from trading performance, and leverage collective wisdom for future decisions. This approach improves the system's robustness.</p>
<p>This marks the debut of the LLM agent trading system that incorporates the character design. The design assigns agents with different varying risk preferences, such as risk-seeking, risk-neutral, and risk-averse, and various investment subscopes across industries. This design enables these collaborative agents to resonate more with human intuition and possess the potential to uncover latent market opportunities.</p>
<p>Our trading system also integrates real-time multi-modal data from diverse information sources, offering a comprehensive view of the financial landscape by encompassing both macro and micro perspectives, as well as historical trading records. With updates available on both daily and minute-by-minute frequencies, our system ensures prompt reactions to daily trades and offers the capability for high-frequency trading.</p>
<p>In this paper, we commence with an in-depth exposition of TradingGPT. We then present multi-modal datasets for the effective training of TradingGPT. We methodically evaluate the pivotal components of the system, illustrating their ability to yield notable results. We prospect that, when deployed on representative fund firms like ARK, TradingGPT will markedly outperform other automated trading strategies.</p>
<p>Related Work</p>
<p>Large language models (LLMs)</p>
<p>The evolution of LLMs has reshaped artificial intelligence and natural language processing. From foundational embeddings like Word2Vec [4] and GloVe [11], the field advanced with the introduction of BERT [2]. Today, the new-generation LLMs, like Generative Pre-trained Transformer series (GPTs) [12,9] and Large Language Model Meta AI (Llamas) [15], demonstrate expressive proficiency across diverse applications.</p>
<p>Generative agent system with memory streams and customized character design</p>
<p>Park et al. [10] introduced generative agents' memory streams and innovatively employed character design concepts from gaming, expanding LLM capabilities for the multi-agent system [13]. In their design, agents display human-like behaviors while retaining individual characters. They dynamically interact with peers and their environment, forging memories and relationships. Moreover, these agents coordinate collaborative tasks through natural language, creating a captivating fusion of artificial intelligence and interactive design.</p>
<p>Multi-agent debate mechanism</p>
<p>Du et al. [3] introduced a debate mechanism leveraging multiple language models in a multi-agent system. Within this framework, various model instances propose debate and collaboratively converge to a unified answer. This approach bolsters mathematical and strategic reasoning while enhancing the factual accuracy of the generated content. </p>
<p>Dataset and Database Structure</p>
<p>For TradingGPT's development, we systematically integrated an extensive array of multi-modal financial data from August 15, 2020, to August 15, 2023. These datasets were sourced from financial databases and APIs, exemplified by the Databento Stock Price Database, Alpaca News API, publicly available daily holdings history records from ARK, etc. This data serves two purposes: (a) to formulate multi-layer memories for agents, and (b) to train, guide, and back-test the agents using ARK funds' historical trading records, refining their trading decisions and actions. In our study, we employed FAISS [6], an open-source vector database, due to its capacity to store data as high-dimensional vectors, enabling semantic searches based on exact matches. Two primary reasons informed our decision: (a) The majority of our data, including audio transcriptions from ARK Invest videos (translated to texts via the Whisper API), benefits from FAISS's unique underlying structure to fast query data. (b) FAISS's compatibility incorporating OpenAI and efficient computation of cosine similarities for specific tickers. the Raw Input schema. This data is then channeled into the Agents' Cognition Schema, guided by both the system's foundational logic and LLM-agent processing. A comprehensive schema structure is in Figure. 1.</p>
<p>Proposed Method</p>
<p>Our methodology integrates LLM across multiple facets of the trading agent workflow. Details and associated notation are provided in the subsequent sections.</p>
<p>Trading Agents Layered Generative Memory Formulation</p>
<p>In our LLM-based trading system, agents autonomously manage their actions and memory trajectories, engaging in communication and deliberation as needed.</p>
<p>Layered-memory structure</p>
<p>Each agent within TradingGPT discerns and categorizes perceived information into three distinct memory layers: long-term, middle-term, and short-term. Compared to the approach of extracting key insights through the computation of ranked retrieval scores from all memories in the generative agent system [10], this layered memory approach introduces a more nuanced ranking mechanism for retrieving crucial events from individual layers. This closely aligns with the human cognition proposed by Atkinson et al. [1]. Our framework initially categorizes memories into separate lists for each layer, guided by predefined rules tailored to specific situations and the nature of events. Subsequently, within each memory layer, we leverage three crucial metrics, inspired by the work of Park et al. -recency, relevancy, and importance -to establish the hierarchical arrangement of events within an agent's memory. However, we have reconstructed their mathematical representations to attain a more logical and advanced formulation.</p>
<p>For a memory event E within the memory layer i ∈ {short, middle, long}, upon the arrival of a prompt P from the LLM, the agent computes the recency score S E Recency as per Equation.</p>
<ol>
<li>This score inversely correlates with the time difference between the prompt's arrival and the event's memory timestamp, aligning with Ebbinghaus's forgetting curve on memory decay [8]. Q i Equation.1 represents the stability term, employed to control the memory decay rates across layers. A higher stability value in the long-term memory layer compared to the short-term layer suggests that memories persist longer in the former. The relevancy score S E relevancy represents the cosine similarity between the embedding vectors for the textual content of the memory event m E and the prompt query m P . The importance score S E Importance is determined using a uniform piecewise function as described in Equation.3, adhering to the relationship c short &lt; c middle &lt; c long . After normalizing their values to the [0,1] range using min-max scaling, these scores, S E Recency , S E Relevancy and S E Importance are linearly combined to produce the final ranking score γ E i for each memory layer in the Equation. 4 (equivalent to retrieval score in the study of Park et al.). In our setup, the ranking score thresholds, γ E i , are 80 for long-term, 60 for middle-term, and 40 for short-term memory. Events scoring below 20 are removed.
S E Recency = e − δ E Q i δ E = t P − t E(1)
, where Q long = 365 for long-term, Q middle = 90 for middle-term, and Q short = 3 for short-term events.
S E Relevancy = m E · m P ∥m E ∥ 2 × ∥m P ∥ 2 (2) S E Importance =    c short if short-term memory c middle if middle-term memory c long if long-term memory(3)
, where c short , c middle and c long are all constants.
γ E i = α E i × S E Recency i + β E i × S E Relevancy i + λ E i × S E Importance i (4)
where each memory event is only associated with one score, as it can only belong to one of the memory layers.</li>
</ol>
<p>To ensure dynamic interactions across memory layers, we define upper and lower thresholds for memory event ranking scores in each layer. We also utilize an add-counter function to boost the scores of events that are triggered by trading executions resulting from significant trading profits and losses. This promotes frequent events to transition from short-term to potentially longer-term memory, enhancing their retention and recall by agents. The hyperparameters α E i , β E i , and λ E i exhibit variations across different layers. The transferable layered memory system allows the agents to capture and prioritize crucial memory events by considering both their types and frequencies when conducting queries.</p>
<p>Memory formulated by individual experience</p>
<p>In the trading paradigm, macro-level market indicators are stored in the long-term memory, quarterly investment strategies are allocated to the mid-term memory, and daily investment messages are channeled into the short-term  memory. These three memory classes constitute the initial structure within the Agents' Cognition Schema of our data warehouse in Figure. 1. In our trading system, agents make informed trading decisions relying on the outcomes of two distinct workflows: the single-agent workflow and the multi-agent workflow, as depicted on the left side of Figure 2.</p>
<p>In the single-agent workflow, when presented with a specific stock ticker, agents' LLM core generates evaluations and reflections, which encompass trading recommendations and the reasons behind them, based on the essential events retrieved from their layered memory. Subsequently, the agent can proceed to execute trading actions in accordance with these generated insights. The key features that empower our system are (a) Immediate reflection: Conducted daily, this mechanism allows agents to consolidate top-ranked events of each memory layer and market facts, such as daily stock prices and ARK fund trading records. Using LLM and specific prompts, agents generate five trading recommendations: "significantly increase position", "slightly increase position", "hold", "slightly decrease position", and "significantly decrease position", with its justification. Each option is associated with a predetermined trade value. which can be adjusted to suit the business scale represented by the agents. Additionally, this reflection captures the agent's trade volumes and returns. (b)Extended reflection: This provides a broader performance overview over a designated period, like a week. It includes stock prices, the agent's trading trends, and self-evaluation. The immediate reflection guides trade execution directly, while the extended reflection acts as a supplementary reference for recalling recent investment transactions. Both types of reflections are stored in the Agents' Cognition Schema's reflection index, as shown in Figure 1, distinguished by a specific flag.</p>
<p>Memory gained by interacting with other agents</p>
<p>For stocks that appear in multiple agents' trading portfolios, TradingGPT enables inter-agent dialogue via a debate mechanism. This mechanism encourages collaboration between agents typically specializing in distinct sectors, with the goal of optimizing trading outcomes. Within these debates, agents present their top-K layered memories as well as immediate reflections, encompassing recommendations, trade values, volumes, and returns, inviting feedback from their peers. All feedback is subsequently stored in the debate class of the Agents' Cognition Schema, tagged with the receiver's index, as shown in Figure. 1.</p>
<p>Design of Training and Testing Workflows</p>
<p>The distinct design of our training and testing workflows is crucial for curating valuable past memory events and strategizing optimal future trading actions.</p>
<p>Training</p>
<p>The training process is twofold: a single-agent workflow followed by a multi-agent phase, as detailed in the left section of Fig. 2. In the single-agent phase, the LLM-driven agent is prompted with key data like stock ticker, date, and trader characters. Using this context, it evaluates top-K-ranked memories across each layer to derive preliminary investment signals, where K is a predefined hyperparameter. The LLM then synchronizes and analyzes these signals with market data, such as daily records from fund firms like ARK and stock closing prices, leading the agent to formulate an immediate reflection and trade accordingly. Subsequently, the agent collaborates in the multi-agent phase, joining debates with agents trading the same stock from varied sectors on that day (refer to 4.1.3).</p>
<p>Test</p>
<p>The testing process, illustrated in the right section of Figure. 2, blends single-agent and multi-agent operations. Both individually processed memories and insights from inter-agent exchanges are concurrently inputted into the LLM to inform trading decisions. Key differences from the training phase include: (a) During testing, agents operate without the guidance of trading records from the representative fund firm, relying solely on daily stock prices as market facts. (b) Time series patterns of prior training reflections and debates, covering a week in our setup, act as auxiliary references in the absence of substantial market ground truths, as noted in (a). Other aspects of the test workflow align with the training phase. </p>
<p>Current Stage And Future work</p>
<p>Our research consists of two phases: prompt design and ablation studies. We've crafted efficient LLM prompts using GPT3.5 turbo as the backbone. Examples of prompts that encapsulate the necessary insights for each phase of the TradingGPT training and testing workflow. The specific design of these prompts is illustrated by examples in Figure. 3.</p>
<p>With our established prompt template, we're poised to undertake ablation studies to assess the trading efficacy of agent systems based on various backbone models. This will involve comparisons within LLMs, such as GPT3.5 turbo versus CodeLlama 34B, and against models like multi-agent reinforcement learning. The training phase will utilize data spanning from August 15, 2020, to February 15, 2023, while the testing phase will extend until August 15, 2023. We'll assess performance using financial metrics like cumulative trade returns, volatility, and the Sharpe Ratio (see 4.1.2).</p>
<p>Harnessing an innovative multi-layer memory system and character design, our main goal is to establish a state-of-the-art LLM-based multi-agent automated trading system adaptable to various LLMs as its core. This system aspires to achieve superior trading performance over other leading trading agent systems by emulating human traders' cognitive behaviors and ensuring responsiveness in the constantly changing market scenario. We also posit that this LLM-based multi-agent design can improve working efficiency and collaborative performance in artificial systems across diverse sectors. Potential applications range from character development in video games to the creation of robo-consultants in business, healthcare, and technology domains.</p>
<p>Figure 1 :
1TradingGPT Data Warehouse.</p>
<p>Figure 2 :
2TradingGPT training and test workflow.</p>
<p>Figure 3 :
3Prompt template for key steps of TradingGPT workflow.
Data entities without specific timestamps are extracted as per the date displayed at the top of the plots.</p>
<p>Human memory: A proposed system and its control processes. C Richard, Richard M Atkinson, Shiffrin, Psychology of learning and motivation. Elsevier2Richard C Atkinson and Richard M Shiffrin. "Human memory: A proposed system and its control processes". In: Psychology of learning and motivation. Vol. 2. Elsevier, 1968, pp. 89-195.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, arXiv:1810.04805arXiv preprintJacob Devlin et al. "Bert: Pre-training of deep bidirectional transformers for language understanding". In: arXiv preprint arXiv:1810.04805 (2018).</p>
<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate. Yilun Du, arXiv:2305.14325arXiv preprintYilun Du et al. "Improving Factuality and Reasoning in Language Models through Multiagent Debate". In: arXiv preprint arXiv:2305.14325 (2023).</p>
<p>word2vec Explained: deriving Mikolov et al.'s negative-sampling wordembedding method. Yoav Goldberg, Omer Levy, arXiv:1402.3722arXiv preprintYoav Goldberg and Omer Levy. "word2vec Explained: deriving Mikolov et al.'s negative-sampling word- embedding method". In: arXiv preprint arXiv:1402.3722 (2014).</p>
<p>MSPM: A modularized and scalable multi-agent reinforcement learningbased system for financial portfolio management. Zhenhan Huang, Fumihide Tanaka, Plos one. 17263689Zhenhan Huang and Fumihide Tanaka. "MSPM: A modularized and scalable multi-agent reinforcement learning- based system for financial portfolio management". In: Plos one 17.2 (2022), e0263689.</p>
<p>Billion-scale similarity search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 7Jeff Johnson, Matthijs Douze, and Hervé Jégou. "Billion-scale similarity search with GPUs". In: IEEE Transac- tions on Big Data 7.3 (2019), pp. 535-547.</p>
<p>Adaptive quantitative trading: An imitative deep reinforcement learning approach. Yang Liu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Yang Liu et al. "Adaptive quantitative trading: An imitative deep reinforcement learning approach". In: Proceed- ings of the AAAI conference on artificial intelligence. Vol. 34. 02. 2020, pp. 2128-2135.</p>
<p>Replication and analysis of Ebbinghaus' forgetting curve. M J Jaap, Joeri Murre, Dros, PloS one 10. 7120644Jaap MJ Murre and Joeri Dros. "Replication and analysis of Ebbinghaus' forgetting curve". In: PloS one 10.7 (2015), e0120644.</p>
<p>. Openai, arXiv:2303.08774GPT-4 Technical Report. 2023.cs.CLOpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].</p>
<p>Generative agents: Interactive simulacra of human behavior. Joon Sung Park, arXiv:2304.03442arXiv preprintJoon Sung Park et al. "Generative agents: Interactive simulacra of human behavior". In: arXiv preprint arXiv:2304.03442 (2023).</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing. the 2014 conference on empirical methods in natural language processingJeffrey Pennington, Richard Socher, and Christopher D Manning. "Glove: Global vectors for word representation". In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014, pp. 1532-1543.</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Alec Radford et al. "Improving language understanding by generative pre-training". In: (2018).</p>
<p>Interactive narrative: A novel application of artificial intelligence for computer games. Mark Riedl, Vadim Bulitko, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence26Mark Riedl and Vadim Bulitko. "Interactive narrative: A novel application of artificial intelligence for computer games". In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 26. 1. 2012, pp. 2160-2165.</p>
<p>Automatic financial trading agent for low-risk portfolio management using deep reinforcement learning. Wonsup Shin, Seok-Jun, Sung-Bae Bu, Cho, arXiv:1909.03278arXiv preprintWonsup Shin, Seok-Jun Bu, and Sung-Bae Cho. "Automatic financial trading agent for low-risk portfolio management using deep reinforcement learning". In: arXiv preprint arXiv:1909.03278 (2019).</p>
<p>Llama: Open and efficient foundation language models. Hugo Touvron, arXiv:2302.13971arXiv preprintHugo Touvron et al. "Llama: Open and efficient foundation language models". In: arXiv preprint arXiv:2302.13971 (2023).</p>
<p>Bloomberggpt: A large language model for finance. Shijie Wu, arXiv:2303.17564arXiv preprintShijie Wu et al. "Bloomberggpt: A large language model for finance". In: arXiv preprint arXiv:2303.17564 (2023).</p>
<p>FinGPT: Open-Source Financial Large Language Models. Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang, arXiv:2306.06031arXiv preprintHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. "FinGPT: Open-Source Financial Large Language Models". In: arXiv preprint arXiv:2306.06031 (2023).</p>            </div>
        </div>

    </div>
</body>
</html>
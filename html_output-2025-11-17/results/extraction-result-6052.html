<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6052 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6052</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6052</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-25738c43c0c4788d803981eaf5d397691aba0958</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/25738c43c0c4788d803981eaf5d397691aba0958" target="_blank">MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information and extensively benchmarked it on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines.</p>
                <p><strong>Paper Abstract:</strong> Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines. Our codes and checkpoints can be found at https://github.com/acharkq/MolCA.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6052",
    "paper_id": "paper-25738c43c0c4788d803981eaf5d397691aba0958",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0083575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter</h1>
<p>Zhiyuan Liu ${ }^{\dagger}$ Sihang $\mathbf{L i}^{\ddagger}$ Yanchen Luo ${ }^{\ddagger}$ Hao Fei ${ }^{\dagger}$<br>Yixin Cao ${ }^{\ddagger}$ Kenji Kawaguchi ${ }^{\dagger}$ Xiang Wang ${ }^{\ddagger *}$ Tat-Seng Chua ${ }^{\dagger}$<br>${ }^{\dagger}$ National University of Singapore, ${ }^{\ddagger}$ University of Science and Technology of China<br>${ }^{\S}$ Singapore Management University<br>{acharkq, sihang0520,luoyc0830, caoyixin2011, xiangwang1223}@gmail.com<br>haofei37@nus.edu.sg, {kenji, chuats}@comp.nus.edu.sg</p>
<h4>Abstract</h4>
<p>Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (i.e., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a QFormer to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines. Our codes and checkpoints can be found at https: //github.com/acharkq/MolCA.</p>
<h2>1 Introduction</h2>
<p>Language Models (LMs) have demonstrated significant achievements across various domains (Devlin et al., 2019; Zhao et al., 2023). Notably, the wealth of biochemical literature in LMs' pretraining data has enabled LMs to obtain a high-level understanding of biochemical concepts and molecule properties. This can be reflected by their promising performances in biochemical and medical questionanswering benchmarks (Taylor et al., 2022; Ope-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>nAI, 2023). Therefore, it becomes increasingly urgent to incorporate these LMs to augment research in chemistry and biology.</p>
<p>For this purpose, we aim to utilize LMs for molecule understanding. As shown in Figure 1a, most existing LMs (Touvron et al., 2023; Zhang et al., 2022; Zeng et al., 2022) represent molecules by their 1D Simplified Molecular Input Line Entry System (SMILES) strings (Weininger, 1988) and process them in a manner similar to texts. While convenient, treating molecules as strings overlooks the molecules' 2D graph representations, which are crucial to human professionals in comprehending the molecule structures (Wells, 2012). To combat that, recent works (Su et al., 2022; Liu et al., 2022b) represent molecules as graphs and use a Graph Neural Network (GNN; Xu et al., 2019) as the molecular graph encoder. The graph encoder is trained jointly with an LM through cross-modal contrastive learning (Radford et al., 2021; Li et al., 2022), as illustrated in Figure 1b. However, the application scope of cross-modal contrastive learning is limited (Alayrac et al., 2022): it is suitable for retrieval tasks, but is insufficient for open-ended molecule-to-text generation tasks, such as molecule captioning (Edwards et al., 2022) and molecule's IUPAC name prediction (Taylor et al., 2022). This is because molecule-to-text generation is a conditional generation task (Keskar et al., 2019; Raffel et al., 2020). It requires the LM to understand 2D graphs as the generation conditions, which contrastive learning cannot achieve. Su et al. (2022) attempt to directly input 2D graphs' representations into LMs, however showing limited improvement.</p>
<p>To bridge this gap, we devise MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables the LM to understand 2D graphs as inputs, therefore effectively conditioning the molecule-to-text gener-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of molecular language modeling methods.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: MolCA's three-stage training pipeline.</p>
<p>ation process. To enable the LM to understand 2D graphs, we identify that the key challenge is <strong>cross-modal alignment</strong> (Li et al., 2023; Merullo et al., 2023; Alayrac et al., 2022): translating the representations of 2D graphs into 1D soft prompts (Li and Liang, 2021) in the text space so that the LM can understand. This translation is facilitated by the cross-modal projector, bridging the gap between the graph encoder's representation space and the LM's input space, as illustrated in Figure 1. Specifically, we implement the cross-modal projector as a Q-Former (Li et al., 2023) due to its effectiveness in vision-language tasks. With an effective cross-modal projector, we can harness the power of existing large LMs (Taylor et al., 2022; Touvron et al., 2023) for molecule-to-text generation. However, given a large LM with billion scale parameters, its efficiency of downstream fine-tuning arises as a new problem. Therefore, we integrate the LM with a uni-modal adapter, <em>i.e.</em>, LoRA (Hu et al., 2022), to enable its efficient adaptation.</p>
<p>As Figure 2 illustrates, MolCA uses a three-stage training pipeline to integrate its components. The two pretrain stages aim to develop the cross-modal alignment ability of the cross-modal projector. In pretrain stage 1, the projector and the encoder are trained to extract the molecule features that are the most relevant to the text. This stage endows the resulting model with powerful molecule-text retrieval ability. In pretrain stage 2, the cross-modal projector is connected to a frozen LM and trained for molecule captioning. This task forces the cross-modal projector to produce soft prompts that the LM can understand. In the final stage, MolCA is fine-tuned for downstream generation tasks. Our contributions can be summarized as follows:</p>
<ul>
<li>We propose MolCA, a pioneering method for molecular language modeling. MolCA enables an LM to perceive 2D molecular graphs, thereby facilitating molecule-to-text generation tasks.</li>
<li>MolCA sets new state-of-the-arts in a variety of benchmarks. It surpasses the baselines by 2.1 and 7.6 BLEU-2 for molecule captioning on CheBI-20 (Edwards et al., 2022) and our curated PubChem324k dataset, respectively. Moreover, in predicting IUPAC names, MolCA shows a significant advantage of 10.0 BLEU-2 over the baselines. For molecule-text retrieval, MolCA outperforms the baselines by 20% retrieval accuracy in PubChem324k and achieves the best performances in PCDes (Zeng et al., 2022) and MoMu datasets (Su et al., 2022).</li>
<li>We conduct ablation studies to show MolCA's effectiveness of incorporating 2D graphs into LMs for molecule-related tasks. Additionally, our quantitative analysis shows that incorporating 2D graphs helps improve the LM's ability to count functional groups inside molecules.</li>
</ul>
<h2>2 Model Architecture</h2>
<p>Here we introduce three key components of MolCA's architecture: 1) a graph encoder for 2D structure understanding, 2) an LM for text generation, and 3) a cross-modal projector to connect the graph encoder and the LM. We describe the uni-modal adapter in Section 3.3.</p>
<p><strong>Graph Encoder.</strong> Given the rich structural patterns in molecules, we leverage a GNN-based encoder to encode molecular graphs. Specifically, we employ a five-layer GINE (Hu et al., 2020) that is pretrained on 2 million molecules from the ZINC15 (Sterling and Irwin, 2015) dataset by contrastive learning (You et al., 2020). Given a molecular graph <em>g</em>, the graph encoder <em>f</em> can generate</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: MolCA's pretrain stage 1. The graph encoder and the cross-modal projector (<em>i.e.,</em> Q-Former) are jointly optimized using three cross-modal tasks. Modules of the same color share weights.</p>
<p>structure-aware features for every node of <em>g</em>:</p>
<p>$$f(g) = \mathbf{Z} \in \mathbb{R}^{|g| \times d},\tag{1}$$</p>
<p>where |<em>g</em>| denotes the number of nodes in <em>g</em>.</p>
<p><strong>Language Model.</strong> To achieve effective text generation performance, we employ Galactica <em>Taylor et al. (2022)</em> as the base LM. Galactica is pretrained on a large collection of scientific literature, which encompasses fields like chemistry, biology, and medicine. Its promising performance in text-based science question-answering benchmarks <em>Hendrycks et al. (2021); Jin et al. (2019)</em> underscores its understanding of high-level biochemical concepts. Notably, Galactica can process 1D SMILES of molecules, which can potentially benefit our downstream tasks. Galactica is a decoder-only transformer LM based on the OPT <em>Zhang et al. (2022)</em> architecture.</p>
<p><strong>Cross-Modal Projector.</strong> We implement the cross-modal projector as a Querying-Transformer (Q-Former) <em>Li et al. (2023)</em> to map the graph encoder's outputs to the LM's input text space. As shown in Figure 3, Q-former has different procedures for processing 2D molecular graphs and 1D texts. Given text inputs, Q-Former inserts [CLS] tokens at the beginning and processes the texts by N layers of self-attention modules and feed-forward networks. The self-attention modules adopt causal masks <em>Raffel et al. (2020)</em> when the pretraining task is text generation. On the other hand, given a molecular graph <em>g</em>, Q-Former works as a molecule feature extractor. Specifically, it maintains a set of learnable query tokens {q<sub>k</sub>}<sub>k=1</sub> as inputs. These query tokens can interact with the graph encoder's output <strong>Z</strong> through the cross-attention modules <em>Vaswani et al. (2017)</em> and extract molecule features. The cross-attention modules are added every two layers. Additionally, the query tokens can interact with the text inputs through the same self-attention modules. Note that, the query tokens and text inputs are processed by different feed-forward networks, in order to maintain capacities for processing molecules and texts.</p>
<p>We initialize Q-Former from Sci-BERT <em>Beltagy et al. (2019)</em>, an encoder-only transformer pretrained on scientific publications. Q-Former's cross-attention modules are randomly initialized.</p>
<h1>3 Training Pipeline</h1>
<p>This section delves into the details of MolCA's three-stage training pipeline (<em>cf.</em> Figure 2). The two pretrain stages leverage a dataset of molecule-text pairs D = {(<em>g</em><sub>1</sub>, <em>y</em><sub>1</sub>), (<em>g</em><sub>2</sub>, <em>y</em><sub>2</sub>), ...} to train the cross-modal projector and the graph encoder. The goal of pretraining is to translate 2D molecular graphs into soft prompts that a frozen LM can understand. The fine-tune stage focuses on efficient adaptation to downstream generation tasks.</p>
<h3>3.1 Pretrain Stage 1: Learning to Extract Text Relevant Molecule Representations</h3>
<p>In this stage, we aim to optimize the cross-modal projector (<em>i.e.,</em> Q-Former) to extract the molecule features most relevant to the text input. This stage serves as a "warmup" training for the cross-modal projector before connecting to the LM. Inspired by BLIP2 <em>Li et al. (2023)</em>, we simultaneously apply three cross-modal pretraining tasks that are tailored for Q-Former's architecture: molecule-text contrasting, molecule-text matching, and molecule captioning. These pretraining tasks endow the Q-Former with a strong molecule-text retrieval ability. Therefore, we save the resulting model from this stage for downstream retrieval tasks. We now elaborate on the three pretraining tasks.</p>
<p><strong>Molecule-Text Contrasting (MTC).</strong> We apply</p>
<p>cross-modal contrastive learning <em>Radford et al. (2021)</em> to train the Q-Former to extract text-revelant molecule features. In this task, query tokens and text inputs are fed into the Q-Former separately (left of Figure 3) to obtain Q-Former’s molecule representations and text representations.</p>
<p>Formally, let $\left{\left(g_{1}, \boldsymbol{y}<em B="B">{1}\right), \ldots,\left(g</em>}, \boldsymbol{y<em i="i">{B}\right)\right}$ be a batch of molecule-text pairs. We denote $g</em>}$ 's Q-Former representations as $\left{\boldsymbol{m<em k="1">{i k}\right}</em>}^{N_{q}}$ (each element for one query token), and denote $\boldsymbol{y<em i="i">{i}$ 's Q-Former representation as $\boldsymbol{t}</em>}$ (representation of the [CLS] token). For arbitrary $i, j \in[1, B]$, we measure the similarity between $\boldsymbol{t<em j="j" k="k">{i}$ and $\left{\boldsymbol{m}</em>\right}<em q="q">{k=1}^{N</em>}}$ by computing the maximum similarity between $\boldsymbol{t<em j="j" k="k">{i}$ and every element in $\left{\boldsymbol{m}</em>\right}<em q="q">{k=1}^{N</em>$ can be written as:}}$. The MTC loss $\ell_{\mathrm{MTC}</p>
<p>$$
\begin{aligned}
&amp; \ell_{\mathrm{g} 2 \mathrm{t}}=\sum_{i=1}^{B} \log \frac{\exp \left(\max <em i="i" k="k">{k} \cos \left(\boldsymbol{m}</em>}, \boldsymbol{t<em j="1">{i}\right) / \tau\right)}{\sum</em> \exp \left(\max }^{B<em i="i" k="k">{k} \cos \left(\boldsymbol{m}</em>}, \boldsymbol{t<em _mathrm_t="\mathrm{t">{j}\right) / \tau\right)} \
&amp; \ell</em> \log \frac{\exp \left(\max } 2 \mathrm{~g}}=\sum_{i=1}^{B<em i="i">{k} \cos \left(\boldsymbol{t}</em>}, \boldsymbol{m<em j="1">{i k}\right) / \tau\right)}{\sum</em> \exp \left(\max }^{B<em i="i">{k} \cos \left(\boldsymbol{t}</em>}, \boldsymbol{m<em _mathrm_MTC="\mathrm{MTC">{j k}\right) / \tau\right)} \
&amp; \quad \ell</em>
\end{aligned}
$$}}=-\frac{1}{B} \ell_{\mathrm{g} 2 \mathrm{t}}-\frac{1}{B} \ell_{\mathrm{t} 2 \mathrm{~g}</p>
<p>where $\cos (\cdot, \cdot) / \tau$ is the temperature-scaled cosine similarity. Temperature $\tau$ is empirically set to 0.1 .</p>
<p>Molecule-Text Matching (MTM). MTM is a binary classification task, aiming to predict whether a molecule-text pair is matched (positive) or unmatched (negative). As Figure 3 illustrates, MTM allows the queries and the texts to interact through the same self-attention module. In this way, the queries can extract multi-modal information from both molecules and texts. For MTM prediction, we attach a linear classifier after the mean pooling of all queries' Q-Former representations. Let $\rho(g, \boldsymbol{y})$ denotes MTM's predicted probability that $(g, \boldsymbol{y})$ is matched. MTM loss $\ell_{\text {MTM }}$ can be written as:</p>
<p>$$
\begin{aligned}
\ell_{\mathrm{MTM}} &amp; =\frac{1}{B} \mathbb{E}<em i="1">{j, k \sim \mathrm{U}(1, B)}[\sum</em>}^{B}-\log \rho\left(g_{i}, \boldsymbol{y<em i="i">{i}\right)+ \
&amp; \log \rho\left(g</em>}, \boldsymbol{y<em k="k">{j}\right)+\log \rho\left(g</em>\right)]
\end{aligned}
$$}, \boldsymbol{y}_{i</p>
<p>where $\mathrm{U}(1, B)$ is a uniform distribution; $\boldsymbol{y}<em k="k">{j}$ and $g</em>$ are random negative samples in batch.</p>
<p>Similar to MTC, MTM also computes the similarity between molecule-text pairs. The difference is that MTM can capture more fine-grained similarity between a molecule and a text through the self-attention and cross-attention modules, compared to the simple cosine similarity used by MTC.</p>
<p>Therefore, in retrieval experiments, we use MTC to first retrieve the top k samples and use MTM for re-ranking, thereby improving the performance.</p>
<p>Molecule Captioning (MCap). MCap aims to generate the molecule's text description based on the molecule representations. For this task, we adopt a special masking strategy in self-attention modules to ensure that the queries learn to extract molecule features that correspond to the text descriptions. Specifically, we employ the bidirectional self-attention masks for queries, allowing them to see each other but not the text tokens. Further, we apply causal masks for texts on the same self-attention module to perform autoregressive decoding of text descriptions. Each text token can see the queries and the preceding text, but not the subsequent text tokens. Since the text tokens cannot directly interact with the graph encoder, they must obtain molecule information from the queries, forcing the queries to extract molecule information through the cross-attention modules. Let $p_{1}(\boldsymbol{y} \mid g)$ be the probability of Q-Former generating text $\boldsymbol{y}$ for a graph $g$. We use the following loss function:</p>
<p>$$
\ell_{\mathrm{MCap}}=-\frac{1}{B} \sum_{i=1}^{B} \log p_{1}\left(\boldsymbol{y}<em i="i">{i} \mid g</em>\right)
$$</p>
<h3>3.2 Pretrain Stage 2: Aligning 2D Molecular Graphs to Texts via Language Modeling</h3>
<p>In this stage, we aim to align the cross-modal projector's outputs to the text space of a frozen LM. As Figure 4 illustrates, we feed the cross-modal projector's representations of 2D molecular graphs to the frozen LM as inputs, and train the model to generate molecules' text descriptions. This process encourages the cross-modal projector to provide representations that the LM can understand, so as to prompt the text generation. Additionally, we also use a molecule's 1D SMILES to guide the generation (cf. Figure 4). This is because most LMs <em>Taylor et al. (2022); Touvron et al. (2023); Zhang et al. (2022)</em> use SMILES during pretraining. Therefore, these LMs have established some correlations between SMILES and their text contexts. Thus, including SMILES can potentially prompt the corresponding biochemical knowledge. On the other hand, incorporating 2D graphs can help capture structural patterns that are hard to learn from 1D SMILES. We will show later in experiments that combining 2D graphs and 1D SMILES can boost performance.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: MolCA's pretrain stage 2 by molecule captioning.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: MolCA's fine-tune stage for molecule-to-text generation. The example shows the prediction of a molecule's IUPAC name.</p>
<p>Formally, consider a molecule-text pair (<em>g</em>, <em>y</em>) and <em>g</em>'s SMILES representation <em>s</em>. The cross-modal projector representations of <em>g</em> are denoted as {<em>m</em><sub><em>k</em></sub>}<em>N</em><sub><em>q</em></sub><em>. We define </em>p*<sub>2</sub>(·) as the text distribution parameterized by the frozen LM. We optimize the cross-modal projector and the graph encoder by minimizing the following loss function:</p>
<p>$$
\begin{aligned}
&amp; -\log p_2(\mathbf{y}|{m_k}<em l="1">{k=1}^N_q, \mathbf{s}) \
= &amp; -\sum</em>).
\end{aligned}
$$}^L \log p_2(y_l|y_1, \dots, y_l-1, {m_k}_{k=1}^N_q, \mathbf{s</p>
<h3>3.3 Fine-tune Stage: Uni-Modal Adapter for Efficient Downstream Adaptation</h3>
<p>In this stage, we fine-tune MolCA for downstream generation tasks. As Figure 5 illustrates, we append a text prompt of the task description after the molecule representations. Then, we apply language modeling loss to fine-tune MolCA for generation tasks, such as molecule's IUPAC name prediction.</p>
<p><strong>Uni-Modal Adapter.</strong> In MolCA, the LM is accounted for a large portion of computation overhead: it can have ~1B parameters, while the cross-modal projector and graph encoder only have a total of ~0.1B parameters. Therefore, we employ a uni-modal adapter for the LM's efficient adaptation to downstream tasks. Specifically, we employ the LoRA (Hu et al., 2022) adapter due to its simple implementation and promising performances (Liu et al., 2022a). As shown in Figure 5, for selected weight matrices (<em>e.g.,</em> <strong>W</strong> ∈ <strong>R</strong><sup><em>d</em><sub>1</sub>×<em>d</em><sub>2</sub>) in the LM, LoRA adds pairs of rank decomposition matrices (<em>e.g.,</em> <strong>BA</strong>, <strong>B</strong> ∈ <strong>R</strong><sup><em>d</em><sub>1</sub>×<em>r</em></sup>, <strong>A</strong> ∈ <strong>R</strong><sup><em>r</em>×<em>d</em><sub>2</sub>) in parallel to them. The original <em>h</em> = <strong>W</strong><em>x</em> layer is changed to:</p>
<p>$$
h = Wx + BAx, \tag{6}
$$</p>
<p>where <strong>W</strong> is kept frozen and the newly added <strong>BA</strong> is trained during adaptation. Given a small <em>r</em> ≪</p>
<table>
<thead>
<tr>
<th>Subset</th>
<th>Size</th>
<th>Avg mol len</th>
<th>Min text len</th>
<th>Avg text len</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pretrain</td>
<td>298083</td>
<td>35</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>Train</td>
<td>12000</td>
<td>32</td>
<td>20</td>
<td>60</td>
</tr>
<tr>
<td>Valid</td>
<td>1000</td>
<td>32</td>
<td>20</td>
<td>61</td>
</tr>
<tr>
<td>Test</td>
<td>2000</td>
<td>31</td>
<td>20</td>
<td>60</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the PubChem324k dataset. We count the text length by splitting the text at spaces.</p>
<p>min(<em>d</em><sub>1</sub>, <em>d</em><sub>2</sub>), LoRA can effectively adapt the LM to downstream tasks while requiring little memory overhead for storing gradients.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setting</h3>
<p>Here we briefly present the experimental settings. More details can be found in Appendix B.</p>
<p><strong>PubChem324k Dataset.</strong> We collect PubChem-324k – a dataset containing 324k molecule-text pairs from the PubChem website<sup>1</sup>. Table 1 presents the dataset statistics. Notice that, the dataset includes many uninformative texts, such as "The molecule is a peptide". Therefore, we sample a high-quality subset of 15k pairs with text longer than 19 words for downstream tasks. This high-quality subset is further randomly divided into the train/valid/test sets. The remaining dataset, which is more noisy, is used for pretraining. Additionally, we filter our pretrain subset to exclude molecules from the valid/test sets of other downstream datasets, including CheBI-20 (Edwards et al., 2022), PCDes (Zeng et al., 2022), and MoMu (Su et al., 2022) datasets. The dataset after filtering includes totally 313k molecule-text pairs.</p>
<p><strong>Baselines.</strong> For generation tasks, we compare MolCA with the following baselines: T5 (Raffel et al., 2020), MolT5 (Edwards et al., 2022), and MoMu (Su et al., 2022). For molecule-text retrieval,</p>
<p>^{1}https://pubchem.ncbi.nlm.nih.gov</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">#Trainable params</th>
<th style="text-align: left;">BLEU-2</th>
<th style="text-align: left;">BLEU-4</th>
<th style="text-align: left;">ROUGE-1</th>
<th style="text-align: left;">ROUGE-2</th>
<th style="text-align: left;">ROUGE-L</th>
<th style="text-align: left;">METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1D SMILES</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MolT5-Small</td>
<td style="text-align: left;">80M, full ft</td>
<td style="text-align: left;">14.8</td>
<td style="text-align: left;">8.5</td>
<td style="text-align: left;">26.5</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">23.6</td>
<td style="text-align: left;">18.5</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-Base</td>
<td style="text-align: left;">250M, full ft</td>
<td style="text-align: left;">30.1</td>
<td style="text-align: left;">20.9</td>
<td style="text-align: left;">40.3</td>
<td style="text-align: left;">25.1</td>
<td style="text-align: left;">33.8</td>
<td style="text-align: left;">35.6</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-Large</td>
<td style="text-align: left;">780M, full ft</td>
<td style="text-align: left;">30.2</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">41.5</td>
<td style="text-align: left;">25.9</td>
<td style="text-align: left;">34.8</td>
<td style="text-align: left;">36.6</td>
</tr>
<tr>
<td style="text-align: left;">1D SMILES + 2D Graph</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MoMu-Small</td>
<td style="text-align: left;">82M, full ft</td>
<td style="text-align: left;">19.1</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">29.7</td>
<td style="text-align: left;">16.3</td>
<td style="text-align: left;">26.7</td>
<td style="text-align: left;">21.8</td>
</tr>
<tr>
<td style="text-align: left;">MoMu-Base</td>
<td style="text-align: left;">252M, full ft</td>
<td style="text-align: left;">30.2</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">40.5</td>
<td style="text-align: left;">25.1</td>
<td style="text-align: left;">34.4</td>
<td style="text-align: left;">34.2</td>
</tr>
<tr>
<td style="text-align: left;">MoMu-Large</td>
<td style="text-align: left;">782M, full ft</td>
<td style="text-align: left;">31.1</td>
<td style="text-align: left;">22.8</td>
<td style="text-align: left;">41.8</td>
<td style="text-align: left;">25.7</td>
<td style="text-align: left;">36.7</td>
<td style="text-align: left;">36.2</td>
</tr>
<tr>
<td style="text-align: left;">MolCA, MolT5-Large</td>
<td style="text-align: left;">877M, full ft</td>
<td style="text-align: left;">$\underline{32.9}$</td>
<td style="text-align: left;">$\underline{26.3}$</td>
<td style="text-align: left;">$\underline{49.8}$</td>
<td style="text-align: left;">$\underline{35.7}$</td>
<td style="text-align: left;">$\underline{44.2}$</td>
<td style="text-align: left;">$\underline{42.4}$</td>
</tr>
<tr>
<td style="text-align: left;">MolCA, Galac $_{125 \mathrm{M}}$</td>
<td style="text-align: left;">222M, full ft</td>
<td style="text-align: left;">31.9</td>
<td style="text-align: left;">24.3</td>
<td style="text-align: left;">47.3</td>
<td style="text-align: left;">33.9</td>
<td style="text-align: left;">43.2</td>
<td style="text-align: left;">41.6</td>
</tr>
<tr>
<td style="text-align: left;">MolCA, Galac ${ }_{1.3 \mathrm{~B}}$</td>
<td style="text-align: left;">100M, LoRA ft*</td>
<td style="text-align: left;">$\mathbf{3 8 . 7}$</td>
<td style="text-align: left;">$\mathbf{3 0 . 3}$</td>
<td style="text-align: left;">$\mathbf{5 0 . 2}$</td>
<td style="text-align: left;">$\mathbf{3 5 . 9}$</td>
<td style="text-align: left;">$\mathbf{4 4 . 5}$</td>
<td style="text-align: left;">$\mathbf{4 5 . 6}$</td>
</tr>
</tbody>
</table>
<p>(a) PubChem324k dataset. Baseline performances are reproduced using their source codes (Edwards et al., 2022; Su et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">#Trainable params</th>
<th style="text-align: left;">BLEU-2</th>
<th style="text-align: left;">BLEU-4</th>
<th style="text-align: left;">ROUGE-1</th>
<th style="text-align: left;">ROUGE-2</th>
<th style="text-align: left;">ROUGE-L</th>
<th style="text-align: left;">METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1D SMILES</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-Small</td>
<td style="text-align: left;">80M, full ft</td>
<td style="text-align: left;">50.1</td>
<td style="text-align: left;">41.5</td>
<td style="text-align: left;">60.2</td>
<td style="text-align: left;">44.6</td>
<td style="text-align: left;">54.5</td>
<td style="text-align: left;">53.2</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base</td>
<td style="text-align: left;">250M, full ft</td>
<td style="text-align: left;">51.1</td>
<td style="text-align: left;">42.3</td>
<td style="text-align: left;">60.7</td>
<td style="text-align: left;">45.1</td>
<td style="text-align: left;">55.0</td>
<td style="text-align: left;">53.9</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: left;">780M, full ft</td>
<td style="text-align: left;">55.8</td>
<td style="text-align: left;">46.7</td>
<td style="text-align: left;">63.0</td>
<td style="text-align: left;">47.8</td>
<td style="text-align: left;">56.9</td>
<td style="text-align: left;">58.6</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-Small</td>
<td style="text-align: left;">80M, full ft</td>
<td style="text-align: left;">51.9</td>
<td style="text-align: left;">43.6</td>
<td style="text-align: left;">62.0</td>
<td style="text-align: left;">46.9</td>
<td style="text-align: left;">56.3</td>
<td style="text-align: left;">55.1</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-Base</td>
<td style="text-align: left;">250M, full ft</td>
<td style="text-align: left;">54.0</td>
<td style="text-align: left;">45.7</td>
<td style="text-align: left;">63.4</td>
<td style="text-align: left;">48.5</td>
<td style="text-align: left;">57.8</td>
<td style="text-align: left;">56.9</td>
</tr>
<tr>
<td style="text-align: left;">MolT5-Large</td>
<td style="text-align: left;">780M, full ft</td>
<td style="text-align: left;">59.4</td>
<td style="text-align: left;">50.8</td>
<td style="text-align: left;">65.4</td>
<td style="text-align: left;">51.0</td>
<td style="text-align: left;">59.4</td>
<td style="text-align: left;">61.4</td>
</tr>
<tr>
<td style="text-align: left;">1D SMILES + 2D Graph</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MoMu-Small</td>
<td style="text-align: left;">82M, full ft</td>
<td style="text-align: left;">53.2</td>
<td style="text-align: left;">44.5</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">56.4</td>
<td style="text-align: left;">55.7</td>
</tr>
<tr>
<td style="text-align: left;">MoMu-Base</td>
<td style="text-align: left;">252M, full ft</td>
<td style="text-align: left;">54.9</td>
<td style="text-align: left;">46.2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">57.5</td>
<td style="text-align: left;">57.6</td>
</tr>
<tr>
<td style="text-align: left;">MoMu-Large</td>
<td style="text-align: left;">782M, full ft</td>
<td style="text-align: left;">59.9</td>
<td style="text-align: left;">51.5</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">59.3</td>
<td style="text-align: left;">59.7</td>
</tr>
<tr>
<td style="text-align: left;">MolCA, Galac $_{125 \mathrm{M}}$</td>
<td style="text-align: left;">222M, full ft</td>
<td style="text-align: left;">$\underline{61.2}$</td>
<td style="text-align: left;">$\underline{52.6}$</td>
<td style="text-align: left;">$\underline{67.4}$</td>
<td style="text-align: left;">$\underline{52.1}$</td>
<td style="text-align: left;">$\underline{60.6}$</td>
<td style="text-align: left;">$\underline{63.6}$</td>
</tr>
<tr>
<td style="text-align: left;">MolCA, Galac $_{1.3 \mathrm{~B}}$</td>
<td style="text-align: left;">110M, LoRA ft*</td>
<td style="text-align: left;">$\mathbf{6 2 . 0}$</td>
<td style="text-align: left;">$\mathbf{5 3 . 1}$</td>
<td style="text-align: left;">$\mathbf{6 8 . 1}$</td>
<td style="text-align: left;">$\mathbf{5 3 . 7}$</td>
<td style="text-align: left;">$\mathbf{6 1 . 8}$</td>
<td style="text-align: left;">$\mathbf{6 5 . 1}$</td>
</tr>
</tbody>
</table>
<p>(b) CheBI-20 dataset. Baseline performances are borrowed from their original papers (Edwards et al., 2022; Su et al., 2022).</p>
<p>Table 2: Performances (\%) of molecule captioning on the PubChem324k and CheBI-20 datasets. Bold indicates the best performance and underline indicates the second best performance. Full ft denotes full parameter fine-tuning. *The LoRA configurations for PubChem324k and CheBI-20 datasets are different. Details are in Appendix B.
we also include these methods: MoleculeSTM (Liu et al., 2022b), KV-PLM (Zeng et al., 2022), and Sci-BERT (Beltagy et al., 2019).</p>
<h3>4.2 Molecule Captioning</h3>
<p>We evaluate MolCA for molecule captioning on the datasets of PubChem324k and CheBI-20 (Edwards et al., 2022). Specifically, we implement MolCA with the base LMs of Galactica $<em 125="125" _mathrm_M="\mathrm{M">{1.3 \mathrm{~B}}$, Galactica $</em>$ and MolT5-Large due to their smaller scales. We fine-tune MolCA and baselines on the dataset's training set and report the test set performance selected by the valid set. Following (Edwards et al., 2022), we adopt BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) as the evaluation metrics. As shown in Table 2, we observe that:}}$, and MolT5-Large. We employ full parameter finetuning for Galactica $_{125 \mathrm{M}</p>
<ol>
<li>MolCA consistently outperforms the baselines by a large margin. Specifcally, MolCA, Galac $_{1.3 \mathrm{~B}}$ achieves the highest performance on all metrics. It outperforms the baselines by 7.6 BLEU2 on PubChem324k and 2.1 BLEU-2 on CheBI-20.</li>
<li>MolCA, Galac $_{125 \mathrm{M}}$ outperforms baselines of larger sizes across all metrics, showing that MolCA's advantage is not limited to model scale.</li>
</ol>
<h3>4.3 IUPAC Name Prediction</h3>
<p>The International Union of Pure and Applied Chemistry (IUPAC) has established a standardized naming system for chemical compounds, known as IUPAC names (Favre and Powell, 2013). Notably, this naming system relies on identifying specific molecule structures, including hydrocarbon chains and double/triple bonds. Therefore, correctly predicting IUPAC names indicates a model's proficiency to understand molecule structures. We fine-tune MolCA and baselines using the PubChem324k's training set to generate a molecule's IUPAC name. As shown in Table 3, MolCA consistently outperforms the baselines by a large margin of 10.0 BLEU-2, highlighting MolCA's advantage in comprehending molecule structures.</p>
<h3>4.4 Molecule-Text Retrieval</h3>
<p>We evaluate MolCA for molecule-text retrieval on the datasets of PubChem324k, PCDes (Zeng et al.,</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>#Trainable params</th>
<th>BLEU-2</th>
<th>BLEU-4</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-L</th>
<th>METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td>1D SMILES</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MolT5-Small</td>
<td>80M, full ft</td>
<td>48.6</td>
<td>35.2</td>
<td>40.0</td>
<td>16.1</td>
<td>34.3</td>
<td>42.5</td>
</tr>
<tr>
<td>MolT5-Base</td>
<td>250M, full ft</td>
<td>52.7</td>
<td>41.5</td>
<td>50.7</td>
<td>26.0</td>
<td>44.3</td>
<td>53.2</td>
</tr>
<tr>
<td>MolT5-Large</td>
<td>780M, full ft</td>
<td>59.4</td>
<td>49.7</td>
<td>55.9</td>
<td>33.3</td>
<td>49.1</td>
<td>58.5</td>
</tr>
<tr>
<td>1D SMILES + 2D Graph</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MolCA, Galac ${ }_{125 \mathrm{M}}$</td>
<td>222M, full ft</td>
<td>73.9</td>
<td>66.3</td>
<td>69.0</td>
<td>47.8</td>
<td>63.2</td>
<td>71.8</td>
</tr>
<tr>
<td>MolCA, Galac ${ }_{1.3 \mathrm{~B}}$</td>
<td>100M, LoRA ft</td>
<td>75.0</td>
<td>66.6</td>
<td>69.6</td>
<td>48.2</td>
<td>63.4</td>
<td>72.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Performances (\%) of predicting molecule's IUPAC names on the PubChem324k dataset. Baseline performances are obtained by running their source codes (Edwards et al., 2022).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>M2T</th>
<th></th>
<th>T2M</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1D SMILES</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Sci-BERT</td>
<td>39.7</td>
<td>85.8</td>
<td>37.5</td>
<td>85.2</td>
</tr>
<tr>
<td>KV-PLM</td>
<td>38.8</td>
<td>86.0</td>
<td>37.7</td>
<td>85.5</td>
</tr>
<tr>
<td>2D Graph</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MoMu-S*</td>
<td>11.5</td>
<td>41.2</td>
<td>12.6</td>
<td>43.6</td>
</tr>
<tr>
<td>MoMu-K*</td>
<td>11.3</td>
<td>41.0</td>
<td>12.4</td>
<td>39.9</td>
</tr>
<tr>
<td>MoMu-S</td>
<td>40.9</td>
<td>86.2</td>
<td>40.8</td>
<td>86.1</td>
</tr>
<tr>
<td>MoMu-K</td>
<td>41.8</td>
<td>87.5</td>
<td>41.6</td>
<td>87.8</td>
</tr>
<tr>
<td>MoleculeSTM</td>
<td>45.8</td>
<td>88.4</td>
<td>44.3</td>
<td>90.3</td>
</tr>
<tr>
<td>MolCA w/o MTM</td>
<td>58.3</td>
<td>92.3</td>
<td>56.0</td>
<td>90.6</td>
</tr>
<tr>
<td>MolCA</td>
<td>66.6</td>
<td>94.6</td>
<td>66.0</td>
<td>93.5</td>
</tr>
</tbody>
</table>
<p>(a) Performances (\%) in the PubChem324k dataset.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>PCDes dataset</th>
<th></th>
<th>MoMu dataset</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1D SMILES</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Sci-BERT ${ }^{\dagger}$</td>
<td>60.7</td>
<td>60.8</td>
<td>0.3</td>
<td>0.3</td>
</tr>
<tr>
<td>KV-PLM ${ }^{\dagger}$</td>
<td>75.9</td>
<td>64.3</td>
<td>0.5</td>
<td>0.3</td>
</tr>
<tr>
<td>2D Graph</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MoMu-S ${ }^{\dagger}$</td>
<td>79.1</td>
<td>75.5</td>
<td>43.3</td>
<td>43.4</td>
</tr>
<tr>
<td>MoMu-K ${ }^{\dagger}$</td>
<td>80.2</td>
<td>79.0</td>
<td>43.7</td>
<td>43.5</td>
</tr>
<tr>
<td>MoleculeSTM</td>
<td>80.4</td>
<td>77.0</td>
<td>70.5</td>
<td>66.9</td>
</tr>
<tr>
<td>MolCA w/o MTM</td>
<td>80.6</td>
<td>76.5</td>
<td>68.5</td>
<td>64.8</td>
</tr>
<tr>
<td>MolCA</td>
<td>$\mathbf{8 5 . 6}$</td>
<td>$\mathbf{8 2 . 3}$</td>
<td>$\mathbf{7 6 . 8}$</td>
<td>$\mathbf{7 3 . 3}$</td>
</tr>
</tbody>
</table>
<p>(b) Recall@20 (\%) in the PCDes and MoMu datasets.</p>
<p>Table 4: Molecule-text retrieval performances. We report performances of using molecule to retrieve text (M2T) and using text to retrieve molecule (T2M). * denotes performance evaluated on the baseline's released checkpoint. † denotes result borrowed from (Su et al., 2022). Other models are trained on PubChem324k's pretrain subset. The complete results are in Appendix C
2022) and MoMu (Su et al., 2022). Specifically, we evaluate MolCA's checkpoint from pretrain stage 1 without further fine-tuning. For all experiments, MolCA first retrieves the top 128 candidates using MTC, then employs the MTM module for reranking. We select Accuracy (Acc) and Recall@20 (R@20) as the evaluation metrics, and report the performances of retrieval in the entire test set. As shown in Table 4, we observe that:</p>
<ol>
<li>MolCA demonstrates superior performance over baselines. Specifically, in PubChem324k, MolCA improves the accuracy by more than $20 \%$ over the baselines. In PCDes and MoMu, MolCA also consistently outperforms the baselines, demonstrating its effectiveness for molecule-text retrieval.</li>
<li>Incorporating MTM significantly improves MolCA's performance. This can be attributed to MTM's ability to model long-range interactions between molecule features and texts, achieved by the cross-attention and self-attention modules.</li>
<li>MolCA's good performances can be partially attributed to our larger pretrain dataset - PubChem324k. As shown in Table 4a, we compare the performances of MoMu's original checkpoint (pretrained on 15k molecule-text pairs) with our reproduced MoMu using PubChem324k. The latter improves the retrieval accuracy by over $25 \%$.</li>
</ol>
<h3>4.5 Ablation Study on Representation Types</h3>
<p>Here we ablate the two representations types of molecules: 1D SMILES and 2D graphs. We compare MolCA with its two variants: 1) 1D SMILES: an LM that uses only 1D SMILES for pretraining and fine-tuning. For a fair comparison, we pretrain this variant on PubChem324k's pretrain subset for molecule captioning before its downstream adaptation; 2) 2D Graph: this variant follows the original MolCA's training pipeline, except not using 1D SMILES in pretrain stage 2 and fine-tune stage.</p>
<p>End Task Ablation. Table 5 presents the results for molecule-to-text generation and molecule property prediction (Hu et al., 2020) tasks. We can observe that combing 2D graphs and 1D SMILES leads to improved performance in all the compared tasks. This demonstrates MolCA's effectiveness in incorporating molecules' 2D graph representations.</p>
<p>Counting Functional Groups (FGs). We ablate MolCA's capability of counting 85 types of FGs inside molecules. An FG is a molecule's subgraph that exhibits consistent chemical behaviors across different molecules (Rong et al., 2020). Correctly</p>
<table>
<thead>
<tr>
<th>Representation type</th>
<th>BLEU-2</th>
<th>BLEU-4</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-L</th>
<th>METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td>Molecule Captioning, PubChem324k</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1D SMILES</td>
<td>34.6</td>
<td>26.9</td>
<td>46.3</td>
<td>32.3</td>
<td>41.5</td>
<td>41.1</td>
</tr>
<tr>
<td>2D Graph</td>
<td>34.5</td>
<td>26.2</td>
<td>46.4</td>
<td>31.6</td>
<td>41.2</td>
<td>40.9</td>
</tr>
<tr>
<td>1D SMILES + 2D Graph</td>
<td>38.7</td>
<td>30.3</td>
<td>50.2</td>
<td>35.9</td>
<td>44.5</td>
<td>45.6</td>
</tr>
<tr>
<td>Molecule Captioning, CheBI-20</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1D SMILES</td>
<td>55.3</td>
<td>45.8</td>
<td>64.3</td>
<td>48.8</td>
<td>58.0</td>
<td>60.3</td>
</tr>
<tr>
<td>1D SMILES + 2D Graph</td>
<td>62.0</td>
<td>53.1</td>
<td>68.1</td>
<td>53.7</td>
<td>61.8</td>
<td>65.1</td>
</tr>
<tr>
<td>IUPAC Name Prediction, PubChem324k</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1D SMILES</td>
<td>71.0</td>
<td>60.6</td>
<td>68.4</td>
<td>45.8</td>
<td>61.5</td>
<td>71.5</td>
</tr>
<tr>
<td>1D SMILES + 2D Graph</td>
<td>75.0</td>
<td>66.6</td>
<td>69.6</td>
<td>48.2</td>
<td>63.4</td>
<td>72.1</td>
</tr>
<tr>
<td>(a) Ablating the representation type on tasks of molecule captioning and IUPAC name prediction.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Representation type</td>
<td>Bace</td>
<td>BBBP</td>
<td>ClinTox</td>
<td>ToxCast</td>
<td>Sider</td>
<td>Tox21</td>
</tr>
<tr>
<td>1D SMILES</td>
<td>79.3±0.8</td>
<td>70.8±0.6</td>
<td>89.0±1.7</td>
<td>56.2±0.7</td>
<td>61.1±1.2</td>
<td>76.0±0.5</td>
</tr>
<tr>
<td>1D SMILES + 2D Graph</td>
<td>79.8±0.5</td>
<td>70.0±0.5</td>
<td>89.5±0.7</td>
<td>64.5±0.8</td>
<td>63.0±1.7</td>
<td>77.2±0.5</td>
</tr>
</tbody>
</table>
<p>(b) ROC-AUC (\%) scores on six molecule property prediction datasets from MoleculeNet (Wu et al., 2018). We use scaffold split following (Hu et al., 2020). We report the performance’s mean values and standard deviations across three random seeds.</p>
<p>Table 5: Ablating molecule’s representation types. All compared models fine-tune the base LM of Galactica_{1.3B}.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Ablating MolCA for counting FGs inside molecules from PubChem324k. We plot average values across three random seeds. Blue shades indicate the range of $\pm$ one standard deviation. Evaluation metric is Root Mean Square Error (RMSE). Lower value indicates better performance.
counting FGs can help understand a molecule's properties. As shown in Figure 6, incorporating 2D graphs significantly improves MolCA's performance in counting FGs, thereby enhancing its ability in understanding molecule structures.</p>
<h2>5 Related Works</h2>
<p>Here we briefly review the molecule-related literature. We discuss MolCA's relations to visionlanguage pretraining methods in Appendix A.</p>
<p>Molecule Understanding via 1D Language Modeling. Due to the extensive biochemical literature in their training corpus, some open-domain LMs (Zhang et al., 2022; Touvron et al., 2023; Chowdhery et al., 2022) have obtained a high-level understanding of molecular and chemical concepts. This is demonstrated through their promising performances in text-related biochemical and medical question-answering benchmarks (Hendrycks et al., 2021; Jin et al., 2019). Among these LMs, Galactica (Taylor et al., 2022) shows competitive performances for using a corpus that is primarily composed of scientific literature. Focusing on the chemistry domain, KV-PLM (Zeng et al., 2022) models molecules by applying masked language modeling loss on 1D SMILES. Vaucher et al. (2021) propose to predict the chemistry experiment actions by reading chemical reaction equations. MolT5 (Edwards et al., 2022) presents several T5-based (Raffel et al., 2020) LMs for SMILES-to-text and text-to-SMILES translations. Further, Christofidellis et al. (2023) propose to fine-tune T5 for chemical reaction prediction and retrosynthesis tasks. MolCA is different from these methods that exclusively utilize 1D SMILES to represent molecules. Instead, MolCA aims to enable LMs to perceive molecules' 2D graph representations.</p>
<p>Molecule-Text Contrastive Learning. Driven by the demand of a molecule-text retrieval system, Text2Mol (Edwards et al., 2021) employs crossmodal contrastive learning to train a molecular graph encoder of GCNs (Kipf and Welling, 2017) and a text encoder of Sci-BERT (Beltagy et al., 2019). Subsequent works (Su et al., 2022; Liu et al., 2022b; Seidl et al., 2023) have proposed enhancements, including the addition of inter-modal contrastive learning loss (Su et al., 2022) and applying the model for text-based molecule editing (Liu et al., 2022b). However, cross-modal contrastive learning is unsuitable for open-ended conditional generation task (Alayrac et al., 2022), because of its</p>
<p>focus on learning a similarity function. To resolve the problem, we propose MolCA to enable the LM's understanding of 2D molecular graphs, facilitating MolCA's capability of open-ended molecule-to-text generation.</p>
<h2>6 Conclusion and Future Works</h2>
<p>In this work, we propose MolCA, a novel molecular language modeling method. MolCA aims to enable LMs to perceive 2D graphs for molecule-totext generation. For this purpose, MolCA features a cross-modal projector to map representations of 2D graphs into the text space of LMs. It also employs a uni-modal adapter for efficient downstream adaptation. MolCA achieves state-of-the-art performances on molecule captioning and molecule-text retrieval benchmarks. Looking forward, we are interested in exploring LMs for 3D molecular modeling and drug discovery tasks.</p>
<h2>Limitations</h2>
<p>This work focuses on utilizing LMs' generation ability for molecule-text tasks. Other interesting abilities of LMs, like in-context learning and chain-of-thought reasoning, are beyond the scope of this research. We leave that to future exploration.</p>
<p>While MolCA offers improvements over baselines, we observe that the current performance in molecule captioning is not yet sufficient for practical application. This can be attributed to the scale of pretraining data. To our knowledge, our PubChem324k dataset is the largest dataset of molecule-text pairs. However, compared to the $\sim 10 \mathrm{M}$ scale dataset (Changpinyo et al., 2021) for vision-language pretraining, our dataset, consists of 324 k data points, is comparatively smaller and limits the model's performance. Remedy solutions may include mining weakly supervised data from biochemical literature.</p>
<h2>Broader Impacts</h2>
<p>Our work has established new state-of-the-art performances in molecule captioning and moleculetext retrieval. It has broader impacts in two aspects: 1) for chemistry professionals, our method of molecule captioning and molecule-text retrieval could be useful tools, potentially speeding up their research process; 2) for individuals without specialized chemistry knowledge, our method could provide a more affordable way to access the basic chemical information of molecules.</p>
<p>Our model shares the risks of most LMs. It can generate inaccurate information and can potentially be abused to produce biased content. Further, considering the limited scale of our training data, we strongly advise strictly testing our model before applying it in real applications.</p>
<h2>Acknowledgement</h2>
<p>This research is supported by the National Natural Science Foundation of China (92270114) and the University Synergy Innovation Program of Anhui Province (GXXT-2022-040). This material is based upon work supported by the Google Cloud Research Credit program with the award (6NW8-CF7K-3AG4-1WH1). This research is supported by NExT Research Center.</p>
<h2>References</h2>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022. Flamingo: a visual language model for few-shot learning. In NeurIPS.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In IEEvaluation@ACL, pages 65-72. Association for Computational Linguistics.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientific text. In EMNLP/IJCNLP (1), pages 3613-3618. Association for Computational Linguistics.</p>
<p>Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts. In CVPR, pages 3558-3568. Computer Vision Foundation / IEEE.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. 2023. Unifying molecular and textual representations via multi-task language modelling. In ICML.</p>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. In NeurIPS.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171-4186. Association for Computational Linguistics.</p>
<p>Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904.</p>
<p>Carl Edwards, Tuan Manh Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. Translation between molecules and natural language. In EMNLP, pages 375-413. Association for Computational Linguistics.</p>
<p>Carl Edwards, ChengXiang Zhai, and Heng Ji. 2021. Text2mol: Cross-modal molecule retrieval with natural language queries. In EMNLP (1), pages 595-607. Association for Computational Linguistics.</p>
<p>Henri A Favre and Warren H Powell. 2013. Nomenclature of organic chemistry: IUPAC recommendations and preferred names 2013. Royal Society of Chemistry.</p>
<p>Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In ICLR. OpenReview.net.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In ICLR. OpenReview.net.</p>
<p>Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. 2020. Strategies for pre-training graph neural networks. In ICLR.</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering. In EMNLP/IJCNLP (1), pages 2567-2577. Association for Computational Linguistics.</p>
<p>Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858.</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A. Shoemaker, Paul A. Thiessen, Bo Yu, Leonid Zaslavsky, Jian Zhang, and Evan Bolton. 2021. Pubchem in 2021: new data content and improved web interfaces. Nucleic Acids Res., 49(DatabaseIssue):D1388-D1395.</p>
<p>Thomas N Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In ICLR.</p>
<p>Greg Landrum. 2013. Rdkit documentation. Release, 1(1-79):4.</p>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. CoRR, abs/2301.12597.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP (1), pages 4582-4597. Association for Computational Linguistics.</p>
<p>Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. 2022. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In ICLR. OpenReview.net.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022a. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. arXiv preprint arXiv:2304.08485.</p>
<p>Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. 2022b. Multi-modal molecule structure-text model for text-based retrieval and editing. CoRR, abs/2212.10789.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In ICLR (Poster). OpenReview.net.</p>
<p>Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. 2022. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft.</p>
<p>Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. 2023. Linearly mapping from image to text space. In ICLR.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL, pages 311-318. ACL.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 8748-8763. PMLR.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. 2020. Self-supervised graph transformer on largescale molecular data. In NeurIPS.</p>
<p>Philipp Seidl, Andreu Vall, Sepp Hochreiter, and Günter Klambauer. 2023. Enhancing activity prediction models in drug discovery with the ability to understand human language. arXiv preprint arXiv:2303.03363.</p>
<p>Teague Sterling and John J. Irwin. 2015. ZINC 15 ligand discovery for everyone. J. Chem. Inf. Model., 55(11):2324-2337.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and JiRong Wen. 2022. A molecular multimodal foundation model associating molecule graphs with natural language. CoRR, abs/2209.05481.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. CoRR, abs/2211.09085.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. In NeurIPS, pages 200-212.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998-6008.</p>
<p>Alain C Vaucher, Philippe Schwaller, Joppe Geluykens, Vishnu H Nair, Anna Iuliano, and Teodoro Laino. 2021. Inferring experimental procedures from textbased representations of chemical reactions. Nature communications, 12(1):2573.</p>
<p>David Weininger. 1988. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31-36.</p>
<p>Alexander Frank Wells. 2012. Structural inorganic chemistry. Oxford university press.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530.</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural networks? In ICLR.</p>
<p>Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2022. FILIP: finegrained interactive language-image pre-training. In ICLR. OpenReview.net.</p>
<p>Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph contrastive learning with augmentations. In NeurIPS.</p>
<p>Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(1):862.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR, abs/2303.18223.</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.</p>
<p>A Complete Related Works</p>
<p>We present the complete literature review. In addition to the molecule-related literature, as addressed in the main body of the paper, we also discuss MolCA's relation to vision-language pretraining.</p>
<p>Molecule Understanding via 1D Language Modeling. Due to the extensive biochemical literature in their training corpus, some open-domain LMs (Zhang et al., 2022; Touvron et al., 2023; Chowdhery et al., 2022) have obtained a high-level understanding of molecular and chemical concepts. This is demonstrated through their promising performances in text-related biochemical and medical question-answering benchmarks (Hendrycks et al., 2021; Jin et al., 2019). Among these LMs, Galactica (Taylor et al., 2022) shows competitive performances for using a corpus that is primarily composed of scientific literature. Focusing on the chemistry domain, KV-PLM (Zeng et al., 2022) models molecules by applying masked language modeling loss on 1D SMILES. Vaucher et al. (2021) propose to predict the chemistry experiment actions by reading chemical reaction equations. MolT5 (Edwards et al., 2022) presents several T5-based (Raffel et al., 2020) LMs for SMILES-to-text and text-to-SMILES translations. Further, Christofidellis et al. (2023) propose to fine-tune T5 for chemical reaction prediction and retrosynthesis tasks. MolCA is different from these methods that exclusively utilize 1D SMILES to represent molecules. Instead, MolCA aims to enable LMs to perceive molecules' 2D graph representations.</p>
<p>Molecule-Text Contrastive Learning. Driven by the demand of a molecule-text retrieval system, Text2Mol (Edwards et al., 2021) employs crossmodal contrastive learning to train a molecular graph encoder of GCNs (Kipf and Welling, 2017) and a text encoder of Sci-BERT (Beltagy et al., 2019). Subsequent works (Su et al., 2022; Liu et al., 2022b; Seidl et al., 2023) have proposed improvements, including the addition of inter-modal contrastive learning loss (Su et al., 2022) and applying the model for text-based molecule editing (Liu et al., 2022b). However, cross-modal contrastive learning is unsuitable for open-ended conditional generation task (Alayrac et al., 2022), because of its focus on learning a similarity function. To resolve the problem, we propose MolCA to enable the LM's understanding of 2D molecular graphs, facilitating MolCA's capability of open-ended molecule-to-text generation.</p>
<p>Vision-Language Pretraining (VLP). Both VLP and Molecular Language Modeling aim to bridge the gap between text and another modality. Notably, VLP methods of CLIP (Radford et al., 2021) and others (Li et al., 2022; Yao et al., 2022) use contrastive learning to connect a visual encoder and a text encoder. These methods can be applied for tasks like image-text retrieval and zero-shot image classification. Recently, a series of VLP works (Tsimpoukelli et al., 2021; Merullo et al., 2023; Li et al., 2023; Alayrac et al., 2022) show that visual features can be aligned to the text space of LMs. This cross-modal alignment allows LMs to utilize their language generation and few-shot learning abilities for multi-modal tasks. MolCA draws inspiration from these findings. To the best of our knowledge, we are the first to align 2D molecular graphs to the text space of LMs. Furthermore, we incorporate a uni-modal adapter to improve the adaptation efficiency on downstream tasks.</p>
<h2>B Experimental Settings</h2>
<p>Pretrain Settings. MolCA's pretrain stage 1 has 50 epochs and pretrain stage 2 has 10 epochs. QFormer has 8 query tokens ( $N_{q}=8$ ). Our optimizer's configuration follows (Li et al., 2023). We use the AdamW optimizer (Loshchilov and Hutter, 2019) with a weight-decay of 0.05 . The learning rate is scheduled by a combination of linear warmup and cosine decay. The peak learning rate is 1e-4 and the warmup has 1000 steps.</p>
<p>Molecule Captioning. MolCA is fine-tuned for 100 epochs using the same configuration of optimizer and learning rate scheduler. LoRA is implemented using the OpenDelta library (Ding et al., 2022) and the PEFT library (Mangrulkar et al., 2022). For the PubChem324k dataset, we set LoRA's rank $r$ to 8 and apply LoRA to Galactica's modules of [q_proj, v_proj]. This configuration yields a LoRA adapter with 2M parameters, which constitutes $0.12 \%$ of the parameters in the Galactica ${ }<em 1.3="1.3" B="B">{1.3 B}$. For the CheBI-20 dataset, we set LoRA's rank $r$ to 16 and apply LoRA to Galactica's modules of [q_proj, v_proj, out_proj, fc1, fc2]. This configuration yields a LoRA adapter with 12M parameters, which constitutes $0.94 \%$ of the parameters in the Galactica ${ }</em>$.</p>
<p>IUPAC Name Prediction. We collect IUPAC names for molecules in the train/valid/test sets of PubChem324k using the PubChemPy library ${ }^{2}$. The</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subset</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Usage</th>
<th style="text-align: center;">Avg mol len</th>
<th style="text-align: center;">Avg text len</th>
<th style="text-align: center;">Min text len</th>
<th style="text-align: center;">Max text len</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pretrain</td>
<td style="text-align: center;">298083</td>
<td style="text-align: center;">Pretrain stage 1 \&amp; 2</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1305</td>
</tr>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: center;">12000</td>
<td style="text-align: center;">Downstream fine-tune</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">937</td>
</tr>
<tr>
<td style="text-align: left;">Valid</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Downstream validation</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">1197</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">Downstream test</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">879</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of the PubChem324k dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Retrieval in batch</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Retrieval in test set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M2T (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2M (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">M2T (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2M (\%)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
</tr>
<tr>
<td style="text-align: center;">1D SMILES</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sci-BERT</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: center;">KV-PLM</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">85.5</td>
</tr>
<tr>
<td style="text-align: center;">2D Graph</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MoMu-S*</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">43.6</td>
</tr>
<tr>
<td style="text-align: center;">MoMu-K*</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">39.9</td>
</tr>
<tr>
<td style="text-align: center;">MoMu-S</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: center;">MoMu-K</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">87.8</td>
</tr>
<tr>
<td style="text-align: center;">MoleculeSTM</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">90.3</td>
</tr>
<tr>
<td style="text-align: center;">MolCA w/o MTM</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">90.6</td>
</tr>
<tr>
<td style="text-align: center;">MolCA</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">93.5</td>
</tr>
</tbody>
</table>
<p>(a) Molcule-text retrieval performances in the PubChem324k dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Retrieval in batch</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Retrieval in test set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M2T (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2M (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">M2T (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2M (\%)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
</tr>
<tr>
<td style="text-align: center;">1D SMILES</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sci-BERT ${ }^{\dagger}$</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60.8</td>
</tr>
<tr>
<td style="text-align: center;">KV-PLM ${ }^{\dagger}$</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64.3</td>
</tr>
<tr>
<td style="text-align: center;">2D Graph</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MoMu-S ${ }^{\dagger}$</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: center;">MoMu-K ${ }^{\dagger}$</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: center;">MoleculeSTM</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;">MolCA w/o MTM</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">76.5</td>
</tr>
<tr>
<td style="text-align: center;">MolCA</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">82.3</td>
</tr>
</tbody>
</table>
<p>(b) Molecule-text retrieval performances in the PCDes dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Retrieval in batch</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Retrieval in test set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M2T (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2M (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">M2T (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2M (\%)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">R@20</td>
</tr>
<tr>
<td style="text-align: center;">1D SMILES</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sci-BERT ${ }^{\dagger}$</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">KV-PLM ${ }^{\dagger}$</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">2D Graph</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MoMu-S ${ }^{\dagger}$</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: center;">MoMu-K ${ }^{\dagger}$</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: center;">MoleculeSTM*</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">66.9</td>
</tr>
<tr>
<td style="text-align: center;">MolCA w/o MTM</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">64.8</td>
</tr>
<tr>
<td style="text-align: center;">MolCA</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">73.3</td>
</tr>
</tbody>
</table>
<p>(c) Molecule-text retrieval performances in the MoMu dataset.</p>
<p>Table 7: Complete molecule-text retrieval performances on the datasets of PubChem324k, PCDes and MoMu. * denotes performance evaluated on the baseline's released checkpoint. $\dagger$ denotes result borrowed from (Su et al., 2022). Other models are trained on PubChem324k's pretrain subset.
experiment uses the same hyperparameters as the molecule captioning experiment. We append a text prompt "The molecule's IUPAC name is" after the molecule representations as the task description (cf.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Pretrain Stage 1</th>
<th>Pretrain Stage 2</th>
<th>BLEU-2</th>
<th>BLEU-4</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-L</th>
<th>METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td>MolCA, Galac_{1.3B}</td>
<td>✗</td>
<td>✗</td>
<td>35.8</td>
<td>27.6</td>
<td>47.4</td>
<td>33.0</td>
<td>42.1</td>
<td>42.2</td>
</tr>
<tr>
<td>MolCA, Galac_{1.3B}</td>
<td>✓</td>
<td>✗</td>
<td>36.7</td>
<td>28.3</td>
<td>48.6</td>
<td>34.1</td>
<td>43.3</td>
<td>43.5</td>
</tr>
<tr>
<td>MolCA, Galac_{1.3B}</td>
<td>✓</td>
<td>✓</td>
<td>38.7</td>
<td>30.3</td>
<td>50.2</td>
<td>35.9</td>
<td>44.5</td>
<td>45.6</td>
</tr>
</tbody>
</table>
<p>Table 8: Ablating MolCA’s two pretrain stages by the task of molecule captioning in the PubChem324k dataset.</p>
<table>
<thead>
<tr>
<th>Cross-Modal Projector</th>
<th>Representation Type</th>
<th>BLEU-2</th>
<th>BLEU-4</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-L</th>
<th>METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>1D SMILES</td>
<td>33.7</td>
<td>26.0</td>
<td>45.4</td>
<td>31.6</td>
<td>40.7</td>
<td>40.3</td>
</tr>
<tr>
<td>Linear</td>
<td>1D SMILES + 2D Graph</td>
<td>35.2</td>
<td>28.1</td>
<td>48.2</td>
<td>33.0</td>
<td>42.1</td>
<td>43.5</td>
</tr>
<tr>
<td>Q-Former</td>
<td>1D SMILES + 2D Graph</td>
<td>39.8</td>
<td>31.7</td>
<td>51.7</td>
<td>37.3</td>
<td>46.2</td>
<td>46.8</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparing different cross-modal projectors for molecule captioning on the PubChem324k dataset. All the compared methods apply LoRA fine-tuning on Galactica_{1.3B}.</p>
<p>(a) Samples of molecule captioning.</p>
<p>(b) Samples of IUPAC name prediction.</p>
<p>Figure 7: Examples of MolCA’s molecule-to-text generation results. We highlight text snippets in blue that correctly describe the molecule structures in the predicted texts. To save space, some parts of texts are replaced by (…).</p>
<p>Figure 5).</p>
<p>Molecule-Text Retrieval. We use MolCA’s checkpoint from pretrain stage 1 for retrieval without fine-tuning on any other datasets. This is similar to the setting of zero-shot retrieval in <em>Su et al. (2022); Liu et al. (2022b)</em>.</p>
<p>Molecule Property Prediction. Following <em>Hu et al. (2020)</em>, we fine-tune the models for 100 epochs and report the test performance selected by the valid set. For molecule classification, we attach a linear classifier after the mean pooling of the LM’s hidden states of the last layer. We use the AdamW optimizer with a constant learning rate of 1e-4 and weight decay of 0.05. This experiment uses the same LoRA configuration as the molecule captioning experiment in the PubChem324k dataset.</p>
<p>Counting Functional Groups (FGs). We use the molecules in PubChem324k’s train set for finetuning and use the molecules in the valid set for evaluation. Following <em>Rong et al. (2020)</em>, we use RDkit <em>Landrum (2013)</em> to obtain the ground truth counts of FGs in every molecule. For each FG type, we employ a separate linear classifier to regress its numbers. Our model is trained using the Mean Square Error (MSE) loss function. Other settings, including optimizer and LoRA, are the same as the Molecule Property Prediction experiment.</p>
<p>Galactica. Following the instructions in <em>Taylor et al. (2022)</em>, we wrap SMILES sequences with special tokens of [START_I_SMILES] and [END_I_SMILES] before feeding them into Galactica.</p>
<p>PubChem324k Dataset. Our dataset collection process follows the procedures described in <em>Liu et al. (2022b)</em>. The resulting dataset is larger due to the frequent updates made to the PubChem database <em>Kim et al. (2021)</em>. For each molecule in this website, we use the “description” field in its webpage as the corresponding text description. To avoid information leakage, we replace any common name or IUPAC name of the molecule at the beginning of texts with a text template (i.e., “The molecule”). Detailed statistics of PubChem324k are presented in Table 6.</p>
<h2>Appendix C More Experimental Results</h2>
<p>Molecule-Text Retrieval. Here we present MolCA’s complete molecule-text retrieval performance on the PubChem324k, PCDes, and MoMu datasets. Following <em>Su et al. (2022)</em>, we report the performance of retrieval in a batch of 64 random</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sample 1</th>
<th style="text-align: left;">SMILES: $\quad \mathrm{C}([\mathrm{C} @ @ \mathrm{H}] 1<a href="[\mathrm{C} @ @ \mathrm{H}]([\mathrm{C} @ \mathrm{H}](\mathrm{C}(\mathrm{O} 1) \mathrm{O}) \mathrm{NS}(=\mathrm{O})(=\mathrm{O}) \mathrm{O}) \mathrm{O}">\mathrm{C} @ \mathrm{H}</a> \mathrm{O}[\mathrm{C} @ \mathrm{H}] 2$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ground truth</td>
<td style="text-align: left;">$<a href="[\mathrm{C} @ \mathrm{H}](\mathrm{C}(=\mathrm{C}(\mathrm{O} 2) \mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{O}) \mathrm{O}">\mathrm{C} @ @ \mathrm{H}</a> \mathrm{OS}(=\mathrm{O})(=\mathrm{O}) \mathrm{O}$</td>
</tr>
<tr>
<td style="text-align: left;">1D SMILES</td>
<td style="text-align: left;">The molecule is an amino disaccharide consisting of alpha-(...) joined in sequence by a (1-&gt;4) <br> glycosidic bond. It is a disaccharide derivative, an oligosaccharide sulfate, a member of sulfamic <br> acids, a monocarboxylic acid (...)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">The molecule is a disaccharide sulfate consisting of 2-acetamido-(...) joined in sequence by a <br> (1-&gt;4) glycosidic bond. It is functionally related to a N-acetyl-D-glucosamine and a N-acetyl-D- <br> galactosamine.</td>
</tr>
<tr>
<td style="text-align: left;">1D SMILES + 2D Graph</td>
<td style="text-align: left;">The molecule is a disaccharide that consists of 2-O-(..) residues joined in sequence by a (1-&gt;4) <br> glycosidic bond. It is a disaccharide, an amino disaccharide, and a member of sulfamic acids.</td>
</tr>
<tr>
<td style="text-align: left;">Sample 2</td>
<td style="text-align: left;">SMILES: CCCCCCCCCCCCCCCCCCCCCC(C(=O)O)O</td>
</tr>
<tr>
<td style="text-align: left;">Ground truth</td>
<td style="text-align: left;">The molecule is a long-chain fatty acid that is behenic acid substituted at position 2 by a hydroxy <br> group. It is a 2-hydroxy fatty acid. It is functionally related to a docosanoic acid. It is a conjugate <br> acid of a 2-hydroxybehenate.</td>
</tr>
<tr>
<td style="text-align: left;">1D SMILES</td>
<td style="text-align: left;">The molecule is a 2-hydroxy fatty acid that is the 2-hydroxy derivative of tetracosanoic acid. It is <br> functionally related to a tetracosanoic acid. It is a conjugate acid of a 2-hydroxytetracosanoate.</td>
</tr>
<tr>
<td style="text-align: left;">1D SMILES + 2D Graph</td>
<td style="text-align: left;">The molecule is a 2-hydroxy fatty acid that is hexacosanoic acid substituted at position 2 by a <br> hydroxy group. It is a long-chain fatty acid. It is functionally related to an hexacosanoic acid. It <br> is a conjugate acid of a 2-hydroxyhexacosanoate.</td>
</tr>
</tbody>
</table>
<p>Table 10: Molecule captioning samples of MolCA (i.e., 1D SMILES + 2D Graph) and its variant of using only 1D SMILES. We highlight text snippets in blue that correctly describe the molecule structures in the predicted texts. To save space, some parts of texts are replaced by (...).
samples and the performance of retrieval in the entire test set. As shown in Table 7, our conclusions align with those from Section 4.4: 1) MolCA consistently outperforms the baselines for moleculetext retrieval; 2) applying the MTM module for re-ranking is crucial for MolCA's molecule-text retrieval performances.</p>
<p>Ablating the Pretrain Stages. We conduct ablation studies on MolCA's two pretrain stages. As shown in Table 8, both the two pretrain stages have significant contributions to MolCA's molecule captioning performances.</p>
<p>Ablating the Cross-Modal Projector. We compare the performances of our selected cross-modal projector Q-Former and a linear cross-modal projector. For the linear cross-modal projector, we feed the node representations from the graph encoder to the base LM after the linear projector layer. We tune the weights of the graph encoder, linear projector, and the base LM's LoRA adapter. The experimental setting and hyperparameters are the same as those of MolCA. Table 9 shows the results. We can observe that: 1) Linear cross-modal projector underperforms Q-Former. We conjecture that a linear layer is suboptimal to bridge the modality gap between 2D molecules and 1D texts. This aligns with findings in the MME benchmark (Fu et al., 2023), where Q-Former-based methods (e.g., BLIP-2, InstructBLIP (Dai et al., 2023), MiniGPT-</p>
<p>4 (Zhu et al., 2023)) outperform linear cross-modal projector based method (e.g., LLaVA (Liu et al., 2023)). 2) Linear cross-modal projector slightly outperforms the SMILES-only baseline. We attribute this improvement to the usage of 2D molecular graphs, but the gains are limited because the linear projector is less effective.</p>
<p>MolCA's Generation Results. Figure 7 shows MolCA's molecule-to-text generation results. The two samples of molecule captioning is also presented in Table 10. Specifically, we compare MolCA (i.e., 1D SMILES + 2D Graph) and its variant that is pretrained and fine-tuned using only 1D SMILES. We can observe that using both 1D SMILES and 2D graph leads to more accurate descriptions of molecule structures.</p>
<p>Computational Cost. We present the real-world training time of MolCA's three training stages in Table 11. All experiments are conducted on two NVIDIA A100 40 GB GPUs. Notably, we observe that the fine-tuning stage is affordable in terms of computational resources.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Stage</th>
<th style="text-align: left;">Base LM</th>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Epochs</th>
<th style="text-align: center;">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pretrain stage 1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">PubChem324k pretrain subset</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">18.0h</td>
</tr>
<tr>
<td style="text-align: left;">Pretrain stage 2</td>
<td style="text-align: left;">Galac $_{1.7 \mathrm{~B}}$, freeze</td>
<td style="text-align: left;">PubChem324k pretrain subset</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9.0h</td>
</tr>
<tr>
<td style="text-align: left;">Pretrain stage 2</td>
<td style="text-align: left;">Galac $_{125 \mathrm{M}}$, freeze</td>
<td style="text-align: left;">PubChem324k pretrain subset</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3.0h</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune stage</td>
<td style="text-align: left;">Galac $_{1.7 \mathrm{~B}}$, LoRA ft</td>
<td style="text-align: left;">PubChem324k train subset</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">6.0h</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune stage</td>
<td style="text-align: left;">Galac $_{125 \mathrm{M}}$, full ft</td>
<td style="text-align: left;">PubChem324k train subset</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1.5h</td>
</tr>
</tbody>
</table>
<p>Table 11: Compuational cost for MolCA's three stages.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/mcs07/PubChemPy&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
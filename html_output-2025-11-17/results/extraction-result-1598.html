<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1598 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1598</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1598</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-4833725</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/1602.04648v1.pdf" target="_blank">Prediction of Dynamical Systems by Symbolic Regression</a></p>
                <p><strong>Paper Abstract:</strong> We study the modeling and prediction of dynamical systems based on conventional models derived from measurements. Such algorithms are highly desirable in situations where the underlying dynamics are hard to model from physical principles or simplified models need to be found. We focus on symbolic regression methods as a part of machine learning. These algorithms are capable of learning an analytically tractable model from data, a highly valuable property. Symbolic regression methods can be considered as generalized regression methods. We investigate two particular algorithms, the so-called fast function extraction which is a generalized linear regression algorithm, and genetic programming which is a very general method. Both are able to combine functions in a certain way such that a good model for the prediction of the temporal evolution of a dynamical system can be identified. We illustrate the algorithms by finding a prediction for the evolution of a harmonic oscillator based on measurements, by detecting an arriving front in an excitable system, and as a real-world application, the prediction of solar power production based on energy production observations at a given site together with the weather forecast.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1598.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1598.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming for Symbolic Regression (implemented in this study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary algorithm that evolves expression-tree programs (mathematical formulae) by fitness‑proportionate selection, subtree crossover and subtree mutation to discover analytic predictive models; multiobjective selection (NRMSE vs tree size) and parameter optimization for numeric constants are integrated to improve accuracy and control bloat.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming (symbolic regression implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Searches a space of mathematical expressions represented as expression trees. A population of candidate trees is evolved by repeated cycles of breeding (pairing parents, applying crossover with probability 0.5, then applying mutation), evaluation (NRMSE fitness computed after inner numerical optimization of symbolic constants), and selection using a multiobjective NSGA-II scheme that optimizes two objectives: normalized root mean squared error (NRMSE) and expression-tree size (number of nodes). The algorithm enforces uniqueness by prohibiting identical trees in the population and uses multiple independent runs with Pareto-front aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (expression trees / mathematical formulae)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Subtree crossover: pick two parent expression trees, select a random subtree in each parent, and swap those subtrees to create two offspring; implemented on tree-encoded mathematical expressions. Crossover is attempted with probability p = 0.5 during breeding.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Subtree mutation: select a random subtree in an individual and replace it with a newly generated random subtree. New subtree generation uses a 'half-and-half' (ramped/half-and-half) method with minimum size 0 and maximum size 2 for the mutation operation in this setup. After crossover (if performed) mutation is applied always to offspring in the described breeding step.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>No explicit novelty-search metric is used. Diversity/novelty is encouraged indirectly by multiobjective selection (Pareto ranking and NSGA-II crowding distance/sparsity) and by a hard rule forbidding identical expression trees in the population; these are the only stated novelty/diversity mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Normalized root mean squared error (NRMSE) between observed target values y and predictions ŷ = f(x) after inner numerical optimization of symbolic constants; defined as sum((y_i-ŷ_i)^2) normalized by Ny(max(y)-min(y))). Threshold heuristics reported: a model considered 'accurate' if Γ1 <= 0.05 for numerical data and Γ1 <= 0.20 for measured data; in examples they aimed for NRMSE = 0.01 (1%) for the harmonic oscillator.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Reported application-dependent NRMSE outcomes: harmonic oscillator — analytical solution recovered to ~1% NRMSE in low-noise limit; for coupled oscillators and solar power they report Pareto fronts of NRMSE vs complexity and example model errors (e.g. many models achieving errors below ~0.05–0.2 depending on dataset). No single aggregated executability statistic for the GP method beyond example NRMSE values and Pareto-front plots.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Genotypic uniqueness prohibition (explicitly disallow identical expression trees) and NSGA-II crowding distance (sparsity in objective (error, complexity) fitness space) used as diversity heuristics; no explicit behavioral-diversity metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>No quantitative diversity statistics reported (e.g., no genotypic/phenotypic entropy or pairwise distances). Implementation details that affect diversity are given: population size µ = 500, offspring per generation λ = 500, initial individual depth randomized between 1 and 4, prohibition of identical solutions (reduces effective offspring), and use of crowding distance during selection to spread Pareto-front solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Indirectly characterized: multiobjective optimization explicitly trades off accuracy (NRMSE, executability/predictive performance) against complexity (size) to avoid overfitting and bloat; no explicit discussion of 'novelty vs executability' tradeoff, but Pareto fronts demonstrate tradeoffs between error and model complexity (simpler models generally less overfitted and more interpretable while more complex models can achieve lower training error but risk overfitting).</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td>Yes — Pareto frontier between NRMSE (Γ1) and model complexity (Γ2 = node count). Fronts are reported qualitatively and plotted: non-convex fronts occur in GP runs (averaged over seeds) for different feature sets; no closed-form quantitative characterization is provided beyond plotted fronts and examples. For FFX (comparison), elastic-net parameter ρ changes front shape (convex vs non-convex).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression for dynamical systems prediction: harmonic oscillator (synthetic), coupled FitzHugh–Nagumo oscillators for front detection (synthetic spatio-temporal data), and one-day-ahead solar power forecasting (real-world time series).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared primarily to FFX (Fast Function Extraction, an elastic-net–based deterministic symbolic regression) and to simple persistence baselines in the solar power application; also uses multi-run averaging and references to standard model-selection heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Standard subtree crossover and subtree mutation on expression trees are effective: with sufficiently informative features GP finds simple, accurate linear predictors (e.g., for traveling-front detection GP discovered compact linear combinations that capture front velocity). 2) Noisy data cause GP to produce more complex (bloated / overfitted) models — complexity increases with noise unless complexity is constrained via multiobjective selection. 3) Multiobjective optimization (NRMSE vs size) and NSGA-II crowding distance help maintain diversity on the Pareto front and combat bloat, enabling selection of interpretable models. 4) Prohibiting identical trees in population reduces premature convergence (maintains diversity) but lowers the effective offspring rate. 5) Replacing random constant mutation by inner numerical optimization of constants (c* = argmin_c NRMSE) yields a two-layer optimization that improves parameter estimation and separates structural search (GP) from numeric parameter fitting. 6) GP can overfit with high complexity; testing-set deviations indicate overfitting for very complex models. Quantitative diversity/novelty measures (novelty-search metrics, counts of unique genotypes, behavioral diversity) are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prediction of Dynamical Systems by Symbolic Regression', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1598.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1598.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSGA-II (selection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-dominated Sorting Genetic Algorithm II (selection and diversity promotion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multiobjective evolutionary selection algorithm used to rank candidate solutions by Pareto dominance and to promote spread on the Pareto front via crowding distance (sparsity) for tie-breaking among same-rank individuals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NSGA-II (used as GP selection operator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applied to choose the next generation members from parents and offspring by non-dominated sorting (Pareto ranks) and using crowding-distance (scaled Euclidean distance in objective fitness space) as a diversity-preserving tie-breaker among individuals with equal rank; ensures elitism by carrying forward non-dominated solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (expression trees / mathematical formulae) represented by their multiobjective fitness values</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Crowding distance (sparsity): scaled Euclidean distance in objective (error, complexity) fitness space to neighboring solutions used as a heuristic uniqueness/diversity measure for tie-breaking among same-rank solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Not an executability metric per se; it operates on the specified objectives, here NRMSE (Γ1) and tree size (Γ2), so executability/performance is represented by the first objective (NRMSE) used in sorting.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Crowding distance (fitness-space sparsity) to promote a spread of solutions on the Pareto front; the algorithm therefore promotes phenotypic diversity in objective space rather than explicit genotypic/behavioral diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>No numeric statistics reported; paper notes that NSGA-II's sparsity metric is used to prefer unique/less crowded Pareto solutions and that this helps avoid premature convergence and maintain a spread of solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>NSGA-II explicitly embodies tradeoff management between competing objectives (here: error vs complexity); the Pareto front produced demonstrates the trade-off between lower NRMSE (better executability/prediction) and higher complexity (less interpretable / potentially overfit). No explicit 'novelty vs executability' curve is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td>Used to compute Pareto fronts (error vs complexity) for candidate models. Paper reports convex and non-convex Pareto front shapes depending on algorithm and regularization (FFX's elastic-net ρ parameter can change front convexity). No numeric parameterization beyond plotted fronts and example points.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Used within GP to select symbolic regression models for dynamical-system prediction tasks (harmonic oscillator, coupled oscillators, solar power forecasting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared as an algorithm itself; used as the selection mechanism for GP and contrasted qualitatively with FFX multiobjective results (elastic-net Pareto filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NSGA-II's ranking and crowding-distance tie-breaker are effective components in maintaining a set of Pareto-optimal models with diversity in error/complexity tradeoffs. Use of this selection mechanism, together with prohibiting identical genotypes, helps avoid premature convergence and supports discovery of interpretable solutions on the Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prediction of Dynamical Systems by Symbolic Regression', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic programming: on the programming of computers by means of natural selection <em>(Rating: 2)</em></li>
                <li>Ffx: Fast, scalable, deterministic symbolic regression technology <em>(Rating: 2)</em></li>
                <li>A fast and elitist multiobjective genetic algorithm: Nsga-ii <em>(Rating: 2)</em></li>
                <li>Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming <em>(Rating: 1)</em></li>
                <li>Pareto-front exploitation in symbolic regression <em>(Rating: 1)</em></li>
                <li>Controlling code growth by dynamically shaping the genotype size distribution <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1598",
    "paper_id": "paper-4833725",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "GP (this paper)",
            "name_full": "Genetic Programming for Symbolic Regression (implemented in this study)",
            "brief_description": "An evolutionary algorithm that evolves expression-tree programs (mathematical formulae) by fitness‑proportionate selection, subtree crossover and subtree mutation to discover analytic predictive models; multiobjective selection (NRMSE vs tree size) and parameter optimization for numeric constants are integrated to improve accuracy and control bloat.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Genetic Programming (symbolic regression implementation)",
            "system_description": "Searches a space of mathematical expressions represented as expression trees. A population of candidate trees is evolved by repeated cycles of breeding (pairing parents, applying crossover with probability 0.5, then applying mutation), evaluation (NRMSE fitness computed after inner numerical optimization of symbolic constants), and selection using a multiobjective NSGA-II scheme that optimizes two objectives: normalized root mean squared error (NRMSE) and expression-tree size (number of nodes). The algorithm enforces uniqueness by prohibiting identical trees in the population and uses multiple independent runs with Pareto-front aggregation.",
            "input_type": "programs (expression trees / mathematical formulae)",
            "crossover_operation": "Subtree crossover: pick two parent expression trees, select a random subtree in each parent, and swap those subtrees to create two offspring; implemented on tree-encoded mathematical expressions. Crossover is attempted with probability p = 0.5 during breeding.",
            "mutation_operation": "Subtree mutation: select a random subtree in an individual and replace it with a newly generated random subtree. New subtree generation uses a 'half-and-half' (ramped/half-and-half) method with minimum size 0 and maximum size 2 for the mutation operation in this setup. After crossover (if performed) mutation is applied always to offspring in the described breeding step.",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": "No explicit novelty-search metric is used. Diversity/novelty is encouraged indirectly by multiobjective selection (Pareto ranking and NSGA-II crowding distance/sparsity) and by a hard rule forbidding identical expression trees in the population; these are the only stated novelty/diversity mechanisms.",
            "novelty_results": null,
            "executability_metric": "Normalized root mean squared error (NRMSE) between observed target values y and predictions ŷ = f(x) after inner numerical optimization of symbolic constants; defined as sum((y_i-ŷ_i)^2) normalized by Ny(max(y)-min(y))). Threshold heuristics reported: a model considered 'accurate' if Γ1 &lt;= 0.05 for numerical data and Γ1 &lt;= 0.20 for measured data; in examples they aimed for NRMSE = 0.01 (1%) for the harmonic oscillator.",
            "executability_results": "Reported application-dependent NRMSE outcomes: harmonic oscillator — analytical solution recovered to ~1% NRMSE in low-noise limit; for coupled oscillators and solar power they report Pareto fronts of NRMSE vs complexity and example model errors (e.g. many models achieving errors below ~0.05–0.2 depending on dataset). No single aggregated executability statistic for the GP method beyond example NRMSE values and Pareto-front plots.",
            "diversity_metric": "Genotypic uniqueness prohibition (explicitly disallow identical expression trees) and NSGA-II crowding distance (sparsity in objective (error, complexity) fitness space) used as diversity heuristics; no explicit behavioral-diversity metric reported.",
            "diversity_results": "No quantitative diversity statistics reported (e.g., no genotypic/phenotypic entropy or pairwise distances). Implementation details that affect diversity are given: population size µ = 500, offspring per generation λ = 500, initial individual depth randomized between 1 and 4, prohibition of identical solutions (reduces effective offspring), and use of crowding distance during selection to spread Pareto-front solutions.",
            "novelty_executability_tradeoff": "Indirectly characterized: multiobjective optimization explicitly trades off accuracy (NRMSE, executability/predictive performance) against complexity (size) to avoid overfitting and bloat; no explicit discussion of 'novelty vs executability' tradeoff, but Pareto fronts demonstrate tradeoffs between error and model complexity (simpler models generally less overfitted and more interpretable while more complex models can achieve lower training error but risk overfitting).",
            "frontier_characterization": "Yes — Pareto frontier between NRMSE (Γ1) and model complexity (Γ2 = node count). Fronts are reported qualitatively and plotted: non-convex fronts occur in GP runs (averaged over seeds) for different feature sets; no closed-form quantitative characterization is provided beyond plotted fronts and examples. For FFX (comparison), elastic-net parameter ρ changes front shape (convex vs non-convex).",
            "benchmark_or_domain": "Symbolic regression for dynamical systems prediction: harmonic oscillator (synthetic), coupled FitzHugh–Nagumo oscillators for front detection (synthetic spatio-temporal data), and one-day-ahead solar power forecasting (real-world time series).",
            "comparison_baseline": "Compared primarily to FFX (Fast Function Extraction, an elastic-net–based deterministic symbolic regression) and to simple persistence baselines in the solar power application; also uses multi-run averaging and references to standard model-selection heuristics.",
            "key_findings": "1) Standard subtree crossover and subtree mutation on expression trees are effective: with sufficiently informative features GP finds simple, accurate linear predictors (e.g., for traveling-front detection GP discovered compact linear combinations that capture front velocity). 2) Noisy data cause GP to produce more complex (bloated / overfitted) models — complexity increases with noise unless complexity is constrained via multiobjective selection. 3) Multiobjective optimization (NRMSE vs size) and NSGA-II crowding distance help maintain diversity on the Pareto front and combat bloat, enabling selection of interpretable models. 4) Prohibiting identical trees in population reduces premature convergence (maintains diversity) but lowers the effective offspring rate. 5) Replacing random constant mutation by inner numerical optimization of constants (c* = argmin_c NRMSE) yields a two-layer optimization that improves parameter estimation and separates structural search (GP) from numeric parameter fitting. 6) GP can overfit with high complexity; testing-set deviations indicate overfitting for very complex models. Quantitative diversity/novelty measures (novelty-search metrics, counts of unique genotypes, behavioral diversity) are not reported.",
            "uuid": "e1598.0",
            "source_info": {
                "paper_title": "Prediction of Dynamical Systems by Symbolic Regression",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "NSGA-II (selection)",
            "name_full": "Non-dominated Sorting Genetic Algorithm II (selection and diversity promotion)",
            "brief_description": "A multiobjective evolutionary selection algorithm used to rank candidate solutions by Pareto dominance and to promote spread on the Pareto front via crowding distance (sparsity) for tie-breaking among same-rank individuals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NSGA-II (used as GP selection operator)",
            "system_description": "Applied to choose the next generation members from parents and offspring by non-dominated sorting (Pareto ranks) and using crowding-distance (scaled Euclidean distance in objective fitness space) as a diversity-preserving tie-breaker among individuals with equal rank; ensures elitism by carrying forward non-dominated solutions.",
            "input_type": "programs (expression trees / mathematical formulae) represented by their multiobjective fitness values",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": null,
            "novelty_metric": "Crowding distance (sparsity): scaled Euclidean distance in objective (error, complexity) fitness space to neighboring solutions used as a heuristic uniqueness/diversity measure for tie-breaking among same-rank solutions.",
            "novelty_results": null,
            "executability_metric": "Not an executability metric per se; it operates on the specified objectives, here NRMSE (Γ1) and tree size (Γ2), so executability/performance is represented by the first objective (NRMSE) used in sorting.",
            "executability_results": null,
            "diversity_metric": "Crowding distance (fitness-space sparsity) to promote a spread of solutions on the Pareto front; the algorithm therefore promotes phenotypic diversity in objective space rather than explicit genotypic/behavioral diversity.",
            "diversity_results": "No numeric statistics reported; paper notes that NSGA-II's sparsity metric is used to prefer unique/less crowded Pareto solutions and that this helps avoid premature convergence and maintain a spread of solutions.",
            "novelty_executability_tradeoff": "NSGA-II explicitly embodies tradeoff management between competing objectives (here: error vs complexity); the Pareto front produced demonstrates the trade-off between lower NRMSE (better executability/prediction) and higher complexity (less interpretable / potentially overfit). No explicit 'novelty vs executability' curve is reported.",
            "frontier_characterization": "Used to compute Pareto fronts (error vs complexity) for candidate models. Paper reports convex and non-convex Pareto front shapes depending on algorithm and regularization (FFX's elastic-net ρ parameter can change front convexity). No numeric parameterization beyond plotted fronts and example points.",
            "benchmark_or_domain": "Used within GP to select symbolic regression models for dynamical-system prediction tasks (harmonic oscillator, coupled oscillators, solar power forecasting).",
            "comparison_baseline": "Not compared as an algorithm itself; used as the selection mechanism for GP and contrasted qualitatively with FFX multiobjective results (elastic-net Pareto filtering).",
            "key_findings": "NSGA-II's ranking and crowding-distance tie-breaker are effective components in maintaining a set of Pareto-optimal models with diversity in error/complexity tradeoffs. Use of this selection mechanism, together with prohibiting identical genotypes, helps avoid premature convergence and supports discovery of interpretable solutions on the Pareto front.",
            "uuid": "e1598.1",
            "source_info": {
                "paper_title": "Prediction of Dynamical Systems by Symbolic Regression",
                "publication_date_yy_mm": "2016-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic programming: on the programming of computers by means of natural selection",
            "rating": 2,
            "sanitized_title": "genetic_programming_on_the_programming_of_computers_by_means_of_natural_selection"
        },
        {
            "paper_title": "Ffx: Fast, scalable, deterministic symbolic regression technology",
            "rating": 2,
            "sanitized_title": "ffx_fast_scalable_deterministic_symbolic_regression_technology"
        },
        {
            "paper_title": "A fast and elitist multiobjective genetic algorithm: Nsga-ii",
            "rating": 2,
            "sanitized_title": "a_fast_and_elitist_multiobjective_genetic_algorithm_nsgaii"
        },
        {
            "paper_title": "Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming",
            "rating": 1,
            "sanitized_title": "order_of_nonlinearity_as_a_complexity_measure_for_models_generated_by_symbolic_regression_via_pareto_genetic_programming"
        },
        {
            "paper_title": "Pareto-front exploitation in symbolic regression",
            "rating": 1,
            "sanitized_title": "paretofront_exploitation_in_symbolic_regression"
        },
        {
            "paper_title": "Controlling code growth by dynamically shaping the genotype size distribution",
            "rating": 2,
            "sanitized_title": "controlling_code_growth_by_dynamically_shaping_the_genotype_size_distribution"
        }
    ],
    "cost": 0.011970749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prediction of Dynamical Systems by Symbolic Regression
March 7, 2022</p>
<p>Markus Quade 
Markus Abel 
Kamran Shafi 
Robert K Niven 
Bernd R Noack </p>
<p>Universität Potsdam
Institut für Physik und Astronomie
Karl-Liebknecht-Straße 24/2514476PotsdamGermany</p>
<p>and Ambrosys GmbH
David-Gilly Straße 114469PotsdamGermany</p>
<p>School of Engineering and Information Technology
The University of New South Wales
2600CanberraACTAustralia</p>
<p>Institut für Strömungsmechanik
Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur LIMSI-CNRS
Technische Universität Braunschweig
Hermann-Blenk-Straße 37BP 13391403, 38108Orsay cedex, BraunschweigFrance and, Germany</p>
<p>Prediction of Dynamical Systems by Symbolic Regression
March 7, 2022AD2F6DEE833D9F885657870DF7A3E72CarXiv:1602.04648v1[physics.data-an]
We study the modeling and prediction of dynamical systems based on conventional models derived from measurements.Such algorithms are highly desirable in situations where the underlying dynamics are hard to model from physical principles or simplified models need to be found.We focus on symbolic regression methods as a part of machine learning.These algorithms are capable of learning an analytically tractable model from data, a highly valuable property.Symbolic regression methods can be considered as generalized regression methods.We investigate two particular algorithms, the so-called fast function extraction which is a generalized linear regression algorithm, and genetic programming which is a very general method.Both are able to combine functions in a certain way such that a good model for the prediction of the temporal evolution of a dynamical system can be identified.We illustrate the algorithms by finding a prediction for the evolution of a harmonic oscillator based on measurements, by detecting an arriving front in an excitable system, and as a real-world application, the prediction of solar power production based on energy production observations at a given site together with the weather forecast.</p>
<p>I. INTRODUCTION</p>
<p>The prediction of the behavior of dynamical systems is of fundamental importance in all scientific disciplines.Since ancient times, philosophers and scientists have tried to formulate observational models and infer future states of such systems.Applications include topics as diverse as weather forecasting [1], the prediction of the motion of the planets [2], or the estimation of quantum evolution [3].The common ingredient of such systems -at least in natural sciences -is the existence of an underlying mathematical model which can be applied as the predictor.In recent years, the use of artificial intelligence (AI) or machine learning (ML) methods have complemented the formulation of such mathematical models through the application of advanced data analysis algorithms that allow accurate estimation of observed dynamics by learning automatically from the given observations and building models in terms of their own modelling languages.Artificial Neural Networks (ANNs) are one example of such techniques that are popularly applied to model dynamic phenomena.ANNs are structured as networks of soft weights organized in layers or so-called neurons or hidden units.One problem of ANN type approaches is the difficult-to-interpret black-box nature of the learnt models.Symbolic regression-based approaches, such as Genetic Programming (GP), pro-vide alternative ML methods that are recently gaining increasing popularity.These methods, similar to other ML counterparts, learn models from observed data and act as good predictors of the future states of dynamical systems.Their added advantages over other methods include the interpretable nature of their learnt models and a flexible and weakly-typed [4] modelling language that allows them to be applied to a variety of domains and problems.</p>
<p>Undoubtedly, the methods used most often in ML are neural networks.These involve deep learning, in the sense that several layers are used and interpreted as the organization of patterns, as one imagines the human brain to work.In the present study, involving deterministic systems, we want to use a certain branch of ML, namely symbolic regression.This technique joins the classical, equation-oriented approach with its computerscientific upstart.In this publication we do not present any major improvements in the algorithms; rather we demonstrate how one can apply symbolic regression to identify and predict the future state of dynamical systems.</p>
<p>Symbolic regression algorithms work by exploring a function space, which is generally bounded by a preselected set of mathematical operators and operands (variables, constants, etc.), using a population of randomly generated candidate solutions.Each candidate solution encoded as a tree essentially works as a function and is evaluated based on its fitness or in other words its ability to match the observed output.These candidate solutions are evolved using a fitness-weighted selection mechanism and different recombination and variation operators.One common problem in symbolic regression is the bloating effect which is caused by excessive lengthening of individual solutions or filling of the population by large number of solutions with low fitness.In this work we use a multi-objective function evaluation mechanism to avoid this problem by including minimizing the solution length as an explicit objective in the fitness function.</p>
<p>Symbolic regression subsumes linear regression, generalized linear regression, and generalized additive models into a larger class of methods.Such methods have been used with success to infer equations of dynamical systems directly from data [5][6][7][8][9].One problem with deterministic chaotic systems is the sampling of phase space using embedding.For a high-dimensional system, this leads to prohibitively long sampling times.Typical reconstruction methods use delay coordinates and the associated differences, this results in mapping models for the observed systems.Mathematically, differential coordinates are better suited for modelingbut they are not always accessible from data.Both approaches, difference and differential embedding, are discussed in [10] with numerical methods to obtain suitable differential variables from data.Modern methods like diffusion maps [11,12] or local linear embedding [13], including the analysis of stochastic systems, circumvent the curse of dimensionality by working directly on the manifold of the dynamical system.</p>
<p>Apart from prediction and identification of dynamical systems [14,15], the symbolic regression approach has been used recently for the control of turbulent flow systems [16,17].In that application, we demonstrate how to find the symbolic equations in a very general form combined with subsequent automatic simplification and multiobjective optimization.This yields interpretable equations of a complexity that we can select.We use open-source Python packages for the analysis.Symbolic regression is conducted using an elastic net method provided by the fast function extraction package (FFX) for quick tests, and the more general, but usually slower method implemented as a genetic programming algorithm (GP) based on the deap package.Subsequent simplification is obtained using sympy.Of course, any other programming framework with similar functionality will do.</p>
<p>For a systematic study we examine numericallygenerated data from a harmonic oscillator as the simplest system to be predicted, and a more involved system of coupled FitzHugh-Nagumo oscillators, which are known to produce complex behaviour and may serve as a very simple model for neurons.We investigate the capacity of the ML approach to detect an incoming front of activity, and give exact equations for the regression.We compare different sampling and spatio-temporal embedding methods, and discuss the results: it is shown that a space-time embedding has advantages over time-only and space-only embedding.</p>
<p>Our final example concerns a real-world application, the short-term and medium-term forecasting of solar power production.In principle, this could be achieved trivially by a high-resolution weather forecast and knowledge of the transfer of solar energy to solar cells, a very well-understood process [18].However, such a highly resolved weather forecast does not exist, because it is prohibitively expensive: even the largest meteorological computers are still unable to compute the weather on small spatial scales, let alone with a long time horizon at high accuracy.As the dynamical systems community identified a long time ago, this is mainly due to uncertainties in the initial conditions, as demonstrated by the celebrated Lorenz equations [19].Consequently, we follow a data-based approach and improve upon weather predictions using local energy production data as a time series.We are aware that use of the full set of weather data will improve the reported forecast, but increasing the resolution is not our interest here, rather the proof of concept of the ML method and its applicability to realworld problems.</p>
<p>The rest of this paper is organized as follows.In Sec.III we discuss the methods and explain our approach.This section is followed by a longer section IV where results are presented for the above-mentioned example systems.We end the paper with a summary and conclusions, Sec.V.</p>
<p>II. METHODS</p>
<p>In the field of dynamical systems (DS), and in particular nonlinear dynamical systems, reconstruction of the characteristics of an observed system from data has been and is a fundamental scientific topic.</p>
<p>In this regard, one can distinguish parameter and structure identification.We first discuss the existing literature on parameter identification which is easier in that there is an established mathematical framework to fit coefficients to known curves representing experimental data, which in turn result from known dynamics.This can be conducted for linear or non-linear functions.For deterministic systems, with the advent of modern computers, quantities like fractal dimensions, Lyapunov exponents and entropies can also be computed to make systems comparable in dynamics [20,21].These analyses further allow the rough characterization of the type and number of orbits of a DS [22].On the other hand, embedding techniques have been developed to reconstruct the dynamics of a high-dimensional system from lowerdimensional time series [23][24][25].</p>
<p>These techniques have a number of limitations with respect to accuracy and the amount of data needed for making good predictive models.A chaotic system with positive Lyapunov exponents has a prediction horizon which depends heavily on accuracy and precision of the data, since chaos "destroys" information.This can be seen very clearly by the shift map example [21].However a system on a regular orbit, even marked with complicated equations, might be predicted accurately.For high-dimensional systems, one needs a large amount of data to address the "curse of dimensionality" [20].In fact it can be shown that for each dimension, the number of data needed increases on a power-law basis [20,26].Eventually, the direct inference of the underlying equations of motion from data can be approached using regression methods, like Kalman filtering, general linear models (GLM), generalized additive models (GAM), or more general schemes, see [27] and references therein.Apart from the equations themselves, partial derivatives often have to be estimated [10], which is an additional problem for low-precision data</p>
<p>We also consider structure identification, which as mentioned above is a more complicated task.In the last 10-15 years, powerful new methods from computer science have been applied to this purpose.This includes numerous studies on diffusion maps, local linear embedding, manifold learning, support vector machines, artificial neural networks, and symbolic regression [11,13,14,28].Here, we focus on symbolic regression.It must be emphasized that most methods are not unique and their success can only be tested based on their predictive power.</p>
<p>A. Symbolic Regression</p>
<p>One drawback of many computational-oriented methods is the lack of equations that can be analyzed mathematically in the neighborhood of analyzed trajectories.Symbolic regression is a way to produce such equations.It includes methods that identify the structure or parameters of the searched equation or both of them simultaneously with respect to objective functions Γ i .</p>
<p>This means that methods like GLM, or GAM are contained in such a description.A recent implementation of GLMs is Fast Function Extraction (FFX) [29], which is explained briefly below.Genetic programming, explained in detail below, is another intuitive method and often used for symbolic regression.Here, the algorithm searches the function space through random combinations and mutations of functions, chosen from a basic set of equations.</p>
<p>Symbolic regression is supposed to be form free and thus unbiased towards human perception.However, human knowledge enters in the meta-rules imposed on the model through the basic building blocks and rules on how they can be combined.Thus, the optimal model is always conditioned on the underlying meta-rules.</p>
<p>Genetic Programming</p>
<p>Genetic programming is an evolutionary algorithm to find an optimal algorithm or program.The term "pro-gramming" in optimization is used synonymously with "plan" or algorithm.It was used first by Dantzig, the inventor of linear programming, at a time when computer programs did not exist as we know them today [30].The algorithm seeks an optimal algorithm, in our case a function, using evolutionary, or "genetic" strategies, as explained below.The pioneering work was established by [31].We can briefly describe it as follows: in GP we can represent formulae as expression trees, such as that shown in Fig. 1.Non-terminal nodes are filled with elements from a basic function set defined by the meta-rules.Terminal nodes consist of variables or parameters.Given the optimization problem
f * = argopt f Γ (1)
we seek the optimal solution f * through optimizing (minimizing or maximizing, or for some cost functionals, finding the supremum or infimum) the fitness (or cost) functional Γ.To find the optimal solution, GP uses a whole population of candidate solutions in parallel which are evolved iteratively through fitness proportionate selection, recombination and mutation operations.The initial generation is created randomly.Afterwards, the algorithm cycles through the following loop until it reaches its convergence or stopping criteria:</p>
<p>• breed: Based on the current generation G t , a new set of size λ of alternative candidate solutions, the offspring O t , are selected.Several problemdependent operators are used for this tweaking step, e.g.changing parts of a candidate solution (mutation) or combining two solutions into two new ones (crossover).These tweaking operations may include selection pressure, so that the "fitter" solutions are more likely to produce offspring.</p>
<p>• evaluate: The offspring O t are evaluated, i.e. their fitness is calculated.</p>
<p>• select: Based on the fitness value, members of the next generation are selected.</p>
<p>This scheme fits the requirements of symbolic regression.Mutation is typically conducted by replacing a random subtree by a new tree.Crossover takes two trees and swaps random subtrees between them.This procedure is illustrated in Fig. 1.The fitness function uses a typical error metric, e.g.least squares or normalized root mean squared error.The random mutations sample the vicinity of their parent solution in function space.As a random mutation could likely lead to less optimal solution, it does not ensure a bias towards optimality.However, this is achieved by the selection, because it ensures that favourable mutations are kept in the set while others are not considered in further iterations.</p>
<p>By design and when based on similar meta-rules, GP includes other algorithms like GLMs or linear programming [28].Algorithm 1 Top level description of a GP algorithm
procedure main G0 ← random(λ) evaluate(G0) t ← 1 repeat Ot ← breed(Gt−1, λ) evaluate(Ot) Gt ← select(Ot, Gt−1, µ) t ← t + 1 until t &gt; T or Gt = good() end procedure</p>
<p>FFX and the Elastic Net</p>
<p>Here we briefly summarize the FFX algorithm of Mc-Conaghy et al. [29].This is a symbolic regression algorithm based on a combined generalized linear model and elastic net approach:
f ( x) = a 0 + N B i=1 a i φ i ( x)(2)
where {a i } are a set of coefficients to be determined, and {φ i } are an overdetermined set of basis functions described by an heuristic, simplicity-driven set of rules (e.g.highest allowed polynomial exponent, products, non-linear functions, . ..).</p>
<p>In the elastic method, a least squares criterion is used to solve the fitting problem.To avoid overfitting, i.e. high model sensitivity on training data, two regulating terms are added: The 1 , and 2 norms of the coefficient vector.The 1 norm favors a sparse model (few coefficients) and simultaneously avoids large coefficients.The 2 norm ensures a more stable convergence as it allows for several, possibly correlated variables instead of a single one.The resulting objective function written in its explicit form reads [32]:
a * = argmin a ||y − f ( x, a)|| 2 + λρ|| a|| 1 + (1 − ρ)λ|| a|| 2 (3)
where y are the data, λ ≥ 0 the regularization weight and ρ ∈ [0, 1] is the mixing between 1 and 2 norms.A benefit of the regularized objective function is that it implicitly gives rise to models with different complexity, i.e. different number of bases N B .</p>
<p>For large values of λ, the predicted coefficients will all be zero.Reducing λ will result in more complicated combinations of non-zero coefficients.For every point on the (λ, ρ)-grid, the "elastic net", one can obtain a single optimal model using a standard solver like coordinate descent to determine the optimal coefficients a * .</p>
<p>A small change in the elastic net parameters leads to a small change in a * such that one can use the already obtained solution of a neighboring grid point to restart coordinate descent with the new parameters.</p>
<p>For the obtained models we can calculate the normalized root mean-squared error and model complexity (number of used basis functions).The FFX algorithm is based purely on deterministic calculations.Hence its runtime compared to a similar GP algorithm is significantly shorter.However, the meta-rules are more stringent.</p>
<p>B. Multiobjective Fitness</p>
<p>As mentioned above, the solution of the regression problem is not unique in general.A major factor which motivates symbolic regression is its comprehensible white-box nature opposed to the black-box nature of, for example neural networks.Invoking Ockhams razor (lex parsimoniae), a simple solution is considered superior to a complicated one [33,34] as it is more easy to comprehend.In addition, more complicated functions are prone to overfitting.This means that complexity should be a criterion in the function search, such that more complex functions are considered less optimal.We therefore seek a solution which satisfies two objectives.</p>
<p>Comparing solutions by more than one metric Γ i is not straightforward.One possible approach is to weight these metrics into one objective Γ:
Γ = N i w i Γ i(4)
making different candidate solutions easily comparable.</p>
<p>The elastic net Eq. 3 uses such a composite metric.However, a priori it is assumed that there is a linear trade-off between the individual objectives.This has three major flaws:</p>
<p>• One needs to determine suitable (problem dependent) w i .</p>
<p>• One does not account for non-linear trade-offs (e.g.all-or-nothing in one objective).</p>
<p>• Instead of single optimal solution there may be a set of optimal solutions defining the compromise between conflicting objectives (here error vs complexity).</p>
<p>The optimal set is also called the Pareto-front.This is the set of non-dominated candidate solutions, i.e. candidate solutions that are not worse than any other solution in the population when compared on all objectives.For the FFX algorithm, explained above, one can obtain the (Pareto-) optimal set of candidate solutions by sorting the models.The mapping from parameter space to the Pareto-optimal set is called Pareto-filtering.Interestingly, the concept of non-domination already partly solves the sorting problem in higher dimensions as it maps from R N to M ordered one-dimensional manifolds: Candidate solutions in the Pareto-front are of rank 0. Similarly, one can find models of rank 1, i.e. all models that are dominated only once (or in other words the nondominated models of all models taken out of the original Pareto-front).</p>
<p>Model 1 f 1 can be said to be better than Model 2 f 2 if its rank is lower:
f 1 f 2 ⇐= rank(f 1 ) &lt; rank(f 2 )
To compare models of the same rank, one has to introduce an additional heuristic criterion, for which there are several choices [35][36][37].Usually the criterion promotes uniqueness of a candidate solution to ensure diversity of the population to avoid becoming trapped in a local minimum.As the uniqueness of a solution may depend on its representation and is usually costly to compute, often its projection to fitness space is used.This is conducted to ensure an effective spread of candidate solutions on the Pareto-front.</p>
<p>For example, the non-dominated sorting algorithm II (NSGAII) [35] uses a heuristic metric called crowding distance or sparsity to compare two models of the same rank.The scaled Euclidean distance in fitness space to the neighboring models is used to describe the uniqueness of a model.For NSGAII we have:
f 1 f 2 ⇐=    rank(f 1 ) &lt; rank(f 2 ) rank(f 1 ) = rank(f 2 ) and sparsity(f 1 ) &gt; sparsity(f 2 )(5)
Out of the current generation and their offspring G t ∩ O t the µ best, in terms of , solutions are chosen for the next generation G t+1 .This selection method ensures elitism, i.e. the best solutions found so far are carried forward in next generations.Looking at the high-level description in Algorithm 1, G t can be seen as an archive which keeps old members as long as they are not dominated by a new solution from the current offspring O t .</p>
<p>The different selection strategies were first studied in the context of genetic algorithms, but more recently they have been successfully applied to symbolic regression [38,39].</p>
<p>III. OUR GP SETUP</p>
<p>For all applications below, our function set is {+, * , −, /, sin, cos, exp, log, √ , 2 }.All discontinuities are defined as zero.Our terminal set consists of the input data x i as well as symbolic constants c i which are determined during evaluation.We set up our multiple objectives as follows: the algorithm runs until the error of the most accurate model is below 0.1%, or for 100 generations.The population size µ as well as the number of offspring per generation λ is set to 500.The depth of individuals of the initial populations varies randomly between 1 and 4. With equal probability we generate the corresponding expression trees where each leaf might have a different depth or each leaf is forced to have the same depth.For mutation we randomly pick a subtree and replace it with a new tree, again using the half and half method, with minimum size 0 and maximum size 2. Crossover is conducted by randomly picking a subtree each and exchanging them.Our breeding step is composed of randomly choosing two individuals from the current population, performing crossover on them with probability p = 0.5 and afterwards always mutating them.</p>
<p>Our multiobjective cost functional has the following components
Γ 1 = NRMSE (y, ŷ) = N i=1 (y i − ŷi ) 2 N y max − y min (6)
where NRMSE is the normalized root mean-squared error of the observed data y and its predictor ŷ = f ( x), and
Γ 2 = size(f )(7)
is simply the total number of nodes in the expression tree f .Selection is conducted according to NSGAII.In this paper, a model is called accurate if its error metric Γ 1 is small, where "small" depends on the context.For example, numerical data might be modeled accurately if Γ 1 ≤ 0.05 and measured data might be modeled accurately if Γ 1 ≤ 0.20.Similarly a model is complicated if its complexity Γ 2 is relatively large."Good" and its comparatives are to be understood in the sense of .</p>
<p>During the generation of the initial population and selection, we force diversity by prohibiting identical solutions.It is very unlikely to randomly create identical solutions.However, offspring may be nearly identical in structure as well as fitness and consequently a crossover between parent and child solution may produce an identical grandchild solution.The probability of such an event grows exponentially with the number of identical solutions in a population and therefore it reduces the diversity of the population in the long-term risking a premature convergence of the algorithm.Thus, by prohibiting identical solutions, the population will have a transient period until it reaches its maximum capacity.This will also reduce the effective number of offspring per generation.This change reduces the probability of becoming trapped in a local minimum because of a steady state in the evolutionary loop.</p>
<p>Our main emphasis is the treatment of the model parameters c i .In standard implementations, e.g. the already mentioned [38,39], the parameters are mutated randomly, like all other nodes.Here, using modern computational power we are able to use traditional parameter optimization algorithms.Thus, the calculation of Γ 1 becomes another optimization task given the current model f j :
Γ 1 = NRMSE (y, f ( x, c * ))(8)
with
c * = argmin c NRMSE (y, f ( x, c))(9)
The initial guess for c i is either inherited or set to one.Thus, we effectively have two combined optimization layers.Each run is conducted using 10 restarts of the algorithm.The Pareto front is the joined front of the individual runs.Finally, we can use algebraic frameworks to simplify the obtained formulae.This is useful, since a formula (phenotype, macrostate) may be represented by many different expression trees (genotypes, microstates).</p>
<p>IV. CASE STUDIES</p>
<p>We present here results for three systems with increasing difficulty: first, we demonstrate the principles using a very simple system, the harmonic oscillator; second, we infer a predictive model for a set of coupled oscillators; and finally we show how we can predict a very applied system, namely the power production from a solar panel.For the first two examples we use numerically produced data, where we have full control over the system, while for the demonstration of applicability we use data from a small solar power station [40].</p>
<p>A. Harmonic Oscillator</p>
<p>In this subsection we describe the first test of our methodology: an oscillator should be identified correctly and a accurate prediction must be possible.Consequently, we investigate the identification of a prediction model, not necessarily using a differential formalism.This might be interpreted as finding an approximation to the solution of the underlying equation by data analysis.A deep investigation of the validity of the solution for certain classes of systems is rather mathematical and is beyond the scope of this investigation.</p>
<p>Our system reads
ẋ = y (10) ẏ = −ω 2 x (11)
where x and y are the state variables and ω is a constant.We use the particular analytical solution x(t) = x 0 sin(ωt), y(t) = x 0 ω cos(ωt).The prediction target is x(t + τ ), where τ is a time increment.</p>
<p>Since the analytical solution is a linear combination of the feature inputs, just N = 2 data points are needed to train the model.This holds for infinite accuracy of the data and serves as a trivial test for the method.In general, a learning algorithm is "trained" on some data and the validity of the result is tested on another set, that is as independent as possible.That way, overfitting is avoided.For the same reason one needs to define a stop criterion for the algorithm, e.g. the data accuracy is 10 −5 , it is useless and even counterproductive to run an algorithm until a root mean square error of 10 −10 (the cost function used here) is achieved.For the example under consideration, we stop the training once the training error is smaller than 1</p>
<p>Typically, a realistic scenario should include the effect of noise, e.g. in the form of measurement uncertainties.We consequently add "measurement" Gaussian noise with mean zero and variance proportional to the signal amplitude: ξ 1 ∼ N (0, (σx 0 ) 2 ), ξ 2 ∼ N (0, (σx 0 ω) 2 ), hence x = x + ξ 1 , ỹ = y + ξ 2 .The training and testing data sets were created as follows: the data are generated between [0, t max ].Out of the first half, we chose N values at random for training.For testing purposes we use the second half.We study the parameter space (N, τ, σ) and average the testing errors over 10 realizations for each parameter set.In Fig. 2 we display the normalized root mean squared error of the prediction using FFX (measured against the noisy data) as a function of the noise amplitude.Given x(t) and y(t) the analytical solution for the non-noisy system is just a linear combination, i.e. x(t + τ ) = cos(ωτ )x(t) + sin(ωτ ) ω y(t), and has a complexity of two.During training we aim for a NRMSE of 1%.Thus, we find the analytical solution in the limit of small noise amplitude σ, see Fig.The length of the analyzed data is another important parameter: typically one expects convergence of the error ∼ 1 √ N for more data.A "vertical" cut through the data in Fig. 2 is shown in Fig. 3.The training set length N has a much lower impact than the classical scaling suggests.Crucial for this scaling is the form free structure as well as the heuristic which is used to select the final model.For demonstration purposes, we chose the most accurate model on the testing set, which is of course vulnerable to overfitting.The average complexity, calculated by Eq. ( 7) of the final model as a function of the noise amplitude, is shown in Fig 4 .As evident we can recover the three regimes of Fig. 2. For small noise, the analytical and numerical solution agree.In the intermediate regime we find on average more complex models (in comparison to the analytical solution).Very strong noise hides the signal and a good prediction is impossible.The optimal solution tends to be single constant , i.e. for high σ the complexity tends to smaller values as seen in Fig 4 .The prediction error has two components: 1) given a structure, noisy data will lead to uncertain parameters and 2) due to the form-free nature of symbolic regression, noisy data will also lead to an uncertain structure, increasing the uncertainty in the parameters.Thus, final model selection has to be performed carefully, especially when dealing with noisy data.A detailed study is presented for the example of coupled oscillators.</p>
<p>B. Coupled Oscillators</p>
<p>The harmonic oscillator is an easy case to treat with our methods.Now, we extend the analysis to add a spatial dimension.We study a model of FitzHugh-Nagumo oscillators [41] on a ring.The oscillators are coupled and generate traveling pulse solutions.The model was originally derived as a simplification of the Hodgkin-Huxley model to describe spikes in axons [42], and serves nowadays as a paradigm for excitable dynamics.Here, its spiky behavior is used as an abstraction of a front, observed in real world applications like the human brain, modeled by connected neurons, or a wind power plant network where fronts of different pressure pass through the locations of the wind power plants.The aim is to  The form-free structure allows for overfitting.For small noise, the true solution is found with complexity 2, for higher noise levels, the algorithm starts to fit the noise and more terms are added, reflected by a higher complexity.</p>
<p>show that temporal and/or spatial information on the state of some network sites enables an increase in predictability of a chosen site or eventually (if there are waves in the network) to the front detection.The model for the ith oscillator is:
vi = v i − v 3 i 3 − w i + I i + D i,j A ij (v j − v i ) (12) ẇi = ε(v i + a − bw i ) . (13)
where v i and w i , i, j = 1, . . ., N , denote the fast and slower state variables, I i is an external driving force, D is the coupling strength parameter, and A ij ∈ {0, 1} describes the coupling structure between nodes i and j.The constant parameters ε, a and b determine the dynamics of the system as ε −1 is the time scale of the slower "recovery variable", and a, and b set the position of the fixed point(s).For A ij we choose diffusive coupling on a ring, i.e. periodic boundary conditions.With the external current I i we can locally pump energy into the system to create two pulses which will travel with the same speed but in opposite directions, annihilating when they meet.</p>
<p>Using different spatio-temporal sampling strategies, the aim is to detect and predict the arrival of a spike train at a location far enough away from the excitation center (i.e.farther than the wave train diameter).We mark this special location with the index zero.</p>
<p>Note that we do not aim to find a model for a spatiotemporal differential equation, since this would involve the estimation of spatial derivatives, which in turn require a fine sampling.This is definitely not the scope here.Rather we focus on the more application-relevant question to make a prediction based on an equation.</p>
<p>The construction of the data set was similar to the single oscillator case: sensors were restricted to the v i variables.We can record the time series of v 0 and use time delayed features for the prediction.Another option is to use information from non-local sensors.</p>
<p>We prepare and integrate the system as follows: we consider a ring of N = 200 oscillators.The constants are chosen as a = 0.7, b = 0.8, τ = 12.5 and D = 1.The system is initialized with v i (0) = 0 and w i (0) = −1.5.With the characteristic function χ T (x) = 1 if x ∈ T else 0 we can write the space and time dependent perturbation as I i (t) = 5χ t− t ≤0.4 (t)χ t≤40 (t)χ i∈{−50,−49} (i).This periodic perturbation leads to a pair of traveling waves.The data were sampled at times t n = n∆t with ∆t = 0.1.The system has multiple time scales: two are associated with the on-site FitzHugh-Nagumo oscillator (τ f ast = 1, τ slow = 1 ε ), while two more are due to diffusive coupling (τ Dif f = D) and perturbation (τ P ert behaves as I i (t) described above).The temporal width of the pulse traveling through a particular site, τ P = 8.4, corresponds to the full width half maximum of the pulse.In Fig. 5 we show the evolution of the oscillator network.The state of v i is color-coded.The horizontal width of the yellow stripe corresponds to the spatial pulse width ξ 10.75.The speed of the spike or front is consequently v f ront ∼ ξ/τ P = 1.28.An animation of this can be found in the supplemental material.The training data, denoted as well feature set, were recorded in three different ways:</p>
<p>• site-only: Only v 0 is recorded, and time-delayed features v 0,∆n = v 0 (t = (n − ∆n)∆t) are also included with ∆n∆t = −1, −2, −3, −4.</p>
<p>• spatially extended: We record v 0 and additionally v i with i = −2, −4, . . ., −10, −20 (upstream direction).</p>
<p>• mixed: This combines the two approaches above.</p>
<p>For each site we also include the time delayed features.</p>
<p>To avoid introducing additional symbols we use state variables with double subscripts for discrete times, where the second index refers to time, and one subscript for continuous time.The respective useage is evident from the context.We choose to predict the state at time t = 2 given the data described above.In other words, the prediction target is v 0 (t n + τ ) with τ = 20 2.5τ P , corresponding to the requirement to be far engouh from the excitation point.Of course, this implies a distance of ∆x ∼ 2.5ξ.The testing and training sets were selected by using every second point of the recorded time series.</p>
<p>FFX Results</p>
<p>We first discuss the results obtained by FFX (Sec.II A 2).In Fig. 6 we display the Pareto fronts using the three different approaches for the training set.All curves have one point in common which represents the best fitting constant (complexity 0).As one would expect, the site only data do not contain enough information to detect a front.Thus, even high complexity models cannot reach an error below 4% and the required error of 1% is never met.In the two other datasets the algorithm has the possibility to find a combination of spatial amd temporal inputs to account for the front velocity.Note that the shape of the front strongly depends on the internal ρ parameter of the elastic net Eq. 3.More information should not lead to a decrease in predictability.Thus, the Pareto front of a data set richer in features dominate the corresponding Pareto front of a data set with less features.Counter-intuitively, using ρ = 0.95 [43] the front for the mixed dataset becomes non-convex as some good fitting models are hidden by the regularizer.Thus, we can use ρ to influence the shape of the front.Despite that, the most accurate model of the mixed data set is still the most accurate model overall.</p>
<p>In the following we discuss the results for the best models for each feature set.</p>
<p>If we take the perspective of an observer sitting at i = 0, we see the spike passing: first the state is zero, then a slow increase is observed followed by a rapid increase and decrease around the spike maximum.Eventually the state returns slowly to zero.Statistically, the algorithm is trained by long quiet times and a short, complicated spike form which is hard to model by a reduced set of state variables.This is illustrated in Fig. 7a where for any feature set the biggest differences occur in the spike region.Apparently, the model with site-only variables shows worse results than the spatial one, and the spatiotemporal set models best the passing spike.We note that in a direct confrontation, the true and modeled signal would hard to be distinguished.In Fig. 7b we confront the time derivative for the model from mixed variables.The true and modeled spike are indistinguishable by eye.The formulae of the most accurate models are shown in Table I.For site-only features, quadratic combinations of points at different times occur.This reflects the approximation of the incoming front by a quadratic term.If, however only spatial points are used, the dynamics far away are used to predict the incoming front.If the small terms are neglected, the model consists of the signal at the target site itself, and the previous site (-2) which carries the largest weight.Physically, it means that despite being far away the front is already felt at 2 sites away.Since the front is stationary in a co-moving frame, spatio-temporal embedding is best, namely sampling the spike train in space and moving in time with the train velocity.Then we have a simple and compact linear dependence as seen in the last row of Table I.Let us inspect the possible physics in the model approximating the constants a 0 , a 1 , a 2 , a 3 roughly as 0, 0.45, 0.35, 0.175 such that a 2 = 2a 3 .We first notice that τ p = 8. 4 10.The last terms can then be recombined to a 3 v −2,−10 + a 3 v −2,−10 + v −2,0 as a mean value of the state with time distance of approximately one typical time scale.The state at −30 is at the backside of the front and together the most important information, namely the increase and decrease of the incoming signal is selected by the model.Alternatively, since v(0, t) = v(−v f τ P , t − τ P ) the best model in Table I can be interpreted as the weighted average of the clos-  est combination (∆i, ∆t) to represent the front velocity ( ∆i ∆t = 4 3 ≈ v f ).This demonstrates how powerful the algorithm works in selecting important features.</p>
<p>GP Results</p>
<p>We again examine the Pareto-optimal models illustrated in Fig. 8.For each feature set we obtain a nonconvex Pareto front.The shape and the values of the fronts are broadly similar to the results obtained by FFX.Because GP is an evolutionary method and relies on random breeding rules, we display averaged results: we initialize the algorithm with different seeds of the random number generator, calculate the Pareto fronts and average the errors for the non dominated models of the same complexity.Note that not all complexities occur on each particular front.This way, we obtain a generic Pareto front and avoid atypical models which may occur by chance.The specific model given below in the tables   For the spatially extended and mixed data sets the errors are smaller than the circle size.The models are re-evaluated on the testing set.</p>
<p>is not averaged, but the best result for one specific seed (42).The errors of the models reachable by the different sets are again decreasing from site only over spatially extended to mixed.However, the mixed model reaches almost zero error which is quite remarkable!The difference plots for the method are given in Fig. 9.While the site only set is not able to give a convincing model for an incoming front, the spatially extended set gives a reasonable model with little error.The mixed model is very good with perfect coincidence of model and true dynamics.This model cannot be distinguished by eye from the observed signal.</p>
<p>The models provided by the GP algorithm with seed 42 are given in Table II.Due to the very general character of GP these can be overwhelming at first glance.However, we can simplify them down by using computer algebra systems like sympy or mathematica (here we use sympy).</p>
<p>The interpretation of the GP results requires a bit more thinking.In essence, they follow a logic similar to the FFX results.The site-only model is complicated, and instead of a square operator a trigonometric function is used to mimic the incoming pulse.Since the data do not include directly all information needed, the algorithm tries to fit unphysical functions.This is clearly a non-deterministic and overfitting result, mirrored by the high complexity of the functions involved.For spatially extended models, we obtain a linear and sinusoidal components, and the model uses only three features, namely the on-site values and the ones at two and four units left on our site under consideration.Remarkably, a sinusoidal behavior detected with an exponential decrease, which is our intuition.Eventually, the spatio-temporal embedding yields a very simple model which approximates the front velocity v f to be between 4  3 and 1.The accuracy of this model is very high.</p>
<p>Summarizing, when given enough input information, both methods find a linear model for the predictor v0 (t + τ ) by finding the most suitable combination of temporal and spatial shift to mimic the constant front velocity.If this information is not available in the input data, nonlinear functions are used.</p>
<p>C. Solar Power Data</p>
<p>In this section, we describe the results obtained for oneday-ahead forecasting of solar power production.The input data used for training are taken from the unisolar solar panel installation at Potsdam University with about 30 kW installed.Details are found at [40].We join the solar power data with meteorological forecast data from the freely available European Centre for Medium-Range Weather Forecasts (ECMWF) portal [44] as well as the actual observed weather data.These public data are of limited quality and serve for our proof of concept with real data and all their deficiencies.</p>
<p>The solar panel data P (t) were recorded every five minutes, at geoposition 52.41 latitude, 12.98 longitude.The information about the weather can be split into two categories: weather observations of a station near the power source W (t) and the weather forecast Ŵ (t + τ ), where τ is the time difference to the prediction target.We do not have weather data from the station directly, but can use data from a weather station nearby (ID: 10379).The weather forecast data are obtained every six hours at the closest location publicly accessible, 52.5 latitude and 13 longitude.Typical meteorological data contain, but are not limited to, the wind speed and direction, pressure at different levels, the irradiation, cloud coverage, temperature and humidity.However, in this example, we only use temperature and cloudiness as well as their forecasts as features for our model.The latter is obtained by minimizing Γ 1 = NRMSE P (t + τ ), P P (t), W (t), Ŵ (t + τ ) Γ 2 = size(f ) ( 14) with f the model under consideration.Our prediction target is P (t + τ ) with τ = 24, the one-day-ahead power production.We create our datasets with a sampling of 1h.While additional information from the solar power data remains unused, the prediction variables have to be interpolated.The quality of the forecast depends on quality of the weather measurement and weather forecast.As we use publicly available data, we can only demonstrate the procedure and cannot attain errors as low as those used in commercial products, which will be discussed elsewhere.The features of the the data set are listed in Table IV C Furthermore, we scale each feature to have its minimum equal zero and maximum equal to one.The models are trained with data from June and July of 2014.Testing is conducted for August 2014.To obtain first impression (assuming no prior knowledge), we calculate the mutual correlation of the data.The power produced the next day is heavily correlated with the predicted solar irradiation.This is a confirmation that the physics involved is mirrored in the model and that weather prediction is good in average.Quantitative statements on the quality of weather prediction is not easy and can be found in the literature [44].</p>
<p>GP Results</p>
<p>Let us consider the results of our forecasting with GP shown in Fig. 10.The Pareto fronts are shown for both the training and testing set.As above, for the coupled oscillators, we have conducted 10 runs with different seeds and display the averaged result.Of course, for the training set (filled diamonds), increasing complexity means decreasing error.We see a strong deviation for very complicated models of the testing data (filled circles).This may be an indication of a small testing sample, or indicate overfitting.The outlier at Γ = 18 is a result of the particular realization of the evolutionary optimization.With a different setting, e.g. more iterations, or multiple runs such outliers are eliminated.To clarify this question, we show the functions found as solution of our procedure with increasing complexity and one specific seed (42) in Table IV.</p>
<p>From Table IV we see that GP follows a very reasonable strategy: First, it recognizes that the persistence method is a very reasonable thing, with production tomorrow being the same as today (x 1 = P (t)).Veto a complexity of 5, the identified models only depend on the solar power x 1 and describe with increasing accuracy the conditioned average daily profile.The more complex models include the weather data and forecast.The geometric mean of current power and predicted temperature is present.However, due to the low quality weather forecast as well as the seasonal weather difference between training and testing data, there is no net gain in prediction quality.</p>
<p>Without any further analysis, the model with the lowest testing error is chosen.In Fig. 11 (a) we confront the real time series with the prediction from GP for the model of complexity 4. One clearly finds the already mentioned conditioned average profile.This predicts the production onset a bit too early.The error distribution is shown in Fig. 11 (b), where we recognize an asymmetric error distribution with more probable under-than overprediction.</p>
<p>FFX Results</p>
<p>The results of the FFX method are shown in Fig. 12-13 and the models in Table V.As shown, the FFX method is less capable of predicting longer inactive periods, such as at night, where no solar power is produced.This is clearly visible in Fig. 13.</p>
<p>Analyzing the equations of Table V, we notice that the best FFX function is a quadratic form with maxima to limit the signal above zero.This amounts to recover the mean shape of the signal as a quadratic function.Unfortunately this seems almost trivial since one could obtain this mean shape by purely geometrical considerations with a factor for the cloud coverage.</p>
<p>Summarizing the results for the solar power curves, both methods are able to reproduce the true curve to approximately 20% which is reasonable for a nonoptimized method.The detection of changes when clear sky switches to partially or fully clouded one is not entirely satisfactory and one needs to investigate the improvement of weather predictions for a single location.As said in the introduction, a perfect weather prediction with high resolution would render this work useless for power production forecast (although not for other questions).</p>
<p>Nevertheless, we note that the results in the form of analytic models are highly valuable, because interpretations and further mathematical analysis are possible.This demonstrates not only consistency of the models, but less variability of the models found.</p>
<p>V. CONCLUSION</p>
<p>We have demonstrated the use of symbolic regression combined with complexity analysis of the resulting models for the future prediction of dynamical systems.More precisely, we identify a system of equations yielding optimal forecasts in terms of a minimized normalized root mean squared error of the difference between model forecast and observation of the system state.We did not investigate theoretical aspects such as the underlying state space, nor what implications of the functions on the model.These will be subject of future investigations.Such work is to be carried out carefully to find the limitations of the approach, in particular of genetic programming, which is rather uncontrolled in the way the search space is explored.On the other hand, the methods stand in line with a large collection of methods from regression and classification and one can use much of this previous knowledge.In our opinion, the multiobjective analysis is crucial to identify models to a degree such that they can be used in practice.Probably, this approach will prove very helpful if used in combination with scale analysis, e.g. by prefiltering the data on a selected spatio-temporal scale and then identify equations for this level.</p>
<p>We have tried to show the possible power by three examples of increasing complexity: a trivial one -the harmonic oscillator with an almost perfect predictive power, a collection of excitable oscillators where we demonstrated that the methods can perform a kind of multiscale analysis based on the data.Thirdly, examine the one-day-ahead forecasting of solar power production we have shown that even for messy data we can improve the classical methods by a few percent (in NRMSE).For theoretical considerations, this might be negligible, for real world applications, a few percent might translate into a considerable advantage, since the usage of rare resources can be optimized.</p>
<p>A question for further research is how we can use simplification during the GP iteration to alter the complexity.It may be even a viable choice to control the complexity growth over time, the so-called bloat, in single objective genetic programming -a topic of ongoing interest [45].Additionally, we introduced an intermediate step to only allow for one of many identical solutions for further evolution.One could consider to expand the idea of identical expression trees to include symmetries.</p>
<p>We conclude that symbolic regression is very useful for the prediction of dynamical systems, based on observations only.Our future research will focus on the use of equations couple the systems to other macroscopic ones (e.g.finance, in the case of wind power), and on the analysis of system stability and other fundamental properties using the found equations, which is scientifically a very crucial point.</p>
<p>Figure 1 .
1
Figure1.Illustration of the genetic programming mutation and crossover.The upper left expression tree describes the function f (x, y) = 0.981 + sin(x).Mutation is conducted by picking a random subtree, here the single terminal node 0.981 and replacing it with a new random expression tree.Similarly, the crossover operator (right) takes two expression trees and swaps two random subtrees.</p>
<p>2 and Fig 4. Strong noise covers the signal and thus the error saturates.</p>
<p>1 Figure 3 .
13
Figure 3. Harmonic oscillator study: In solid blue: normalized root mean squared error vs training set length N for σ = 0.17.Dashed green: e −2 / √ N .The error decreases slightly with N , but the scaling is much less rapid than 1/ √ N .</p>
<p>Figure 4 .
4
Figure 4. Harmonic oscillator study: Average complexity of the chosen model vs. noise amplitude σ.The form-free structure allows for overfitting.For small noise, the true solution is found with complexity 2, for higher noise levels, the algorithm starts to fit the noise and more terms are added, reflected by a higher complexity.</p>
<p>Figure 5 .
5
Figure 5. Space-time plot of the pulse evolution.vi is color coded.The front velocity is v f = 1.28.Pulse width (full width half maximum) τP = 8.4</p>
<p>Figure 6 .
6
Figure 6.Coupled spiking oscillators, method FFX: Pareto fronts for the different spatio-temporal samplings of the network data.For this plot we use ρ = 0.95.This leads to the non-convex shape of the front based on the most information.The models are re-evaluated on the testing set.</p>
<p>Figure 7 .
7
Figure 7. Coupled spiking oscillators, method FFX.For each feature set, the most accurate model is used as the predictor v0.In (a) we show the difference δv0 = v0 − v0.The upper two curves are shifted by 0.25 and 0.5 respectively.In (b) we compare the time derivative (approximated by the finite difference quotient) of the most accurate model overall and the real data.For details see text.</p>
<p>temporal site-only −0.0273 + 3.34v0,0 − 2.41v0,0v0,−10 − 2.09v0,−40v0,−10 + 1.64v 2 0,−20 − 1.53v0,−20 − 1.16v0,−10 + 0.991v 2 0,−30 + 0.684v 2 0,0 + 0.463v0,−30 + 0.433v0,−20v0,0 + 0.373v0,−20v0,−10 − 0.359v 2 0,−40 + 0.216v0,−40 + 0.00286v 2 0,−10 spatially extended −0.00247 + 0.897v−2,0 + 0.178v0,0 − 0.0650v−4,0 + 0.00280v−10,0 − 0.00210v−8,0 temporal spatial 0.00894 + 0.442v−4,−30 + 0.346v−2,−10 + 0.175v−2,0</p>
<p>Figure 8 .
8
Figure 8. Coupled spiking oscillators, method GP.Averaged Pareto fronts, for each spatio-temporal sampling option, 10 runs are conducted and the resulting complexities and errors are averaged.Errorbars represent the standard deviation.For the spatially extended and mixed data sets the errors are smaller than the circle size.The models are re-evaluated on the testing set.</p>
<p>Figure 9 .
9
Figure 9. Coupled spiking oscillators, method GP.For each feature set, the most accurate model is used as the predictor v0.In (a) we show the difference δv0 = v0 − v0.The upper two curves are shifted by 0.25 and 0.5 respectively.In (b) we compare the time derivative (approximated by the finite difference quotient) of the most accurate model overall and the real data.Prediction and true data cannot be distinguished by eye.</p>
<p>temporal site-only v 2 0,0 /(v0,−10 + ( − v0,−10(v0,0 − v0,−30)(v0,−30/ sin(v0,−10 + v0,−20) + exp(v0,−30) − ( sin(v0,−30)) + cos( (v0,−30)v0,−40)))) spatially extended 0.208v0,0 + 0.792v−2,0 + 0.0274362547430272 exp(−v−4,0) sin(v−2,0) temporal spatial 0.878v−4,−30 + 0.124496v−4,−40 TableII.Coupled spiking oscillators, method GP.Formulas of the most accurate models for seed 42.</p>
<p>Figure 10 .
10
Figure10.Solar power study, Average Pareto front obtained using GP: with increasing complexity, training and testing data first behave similarly, then testing deviates strongly indicating overfitting or too small testing data set, respectively.The peaks around complexity 20 are due to two reasons: there are only few models on the fronts (1-3), and one of them is an extreme outlier.</p>
<p>Figure 11 .
11
Figure 11.Solar power study, method GP: a) Real and predicted time series.We display the results of the first week of August 2015.Prediction used model of complexity 4 which had lowest error on the test set.b) Histogram of the residuals ε = P − P .The distribution is asymmetric around zero.The model tends to underpredict.</p>
<p>Figure 12 .
12
Figure 12.Solar power study, Pareto front obtained using FFX.The results for FFX are as accurate as the ones obtained with GP.Test and training set are, however, nicely aligned.This demonstrates not only consistency of the models, but less variability of the models found.</p>
<p>Figure 13 .
13
Figure13.Solar power study, method FFX: a) Timeseries of the predicted ( P ), and observed (P ) data.We display the results of the first week of August 2015.Similar to the GP prediction extrema are not particularly well predicted.For the linear model, even the zero values are not well hit.The reason for this is the regression to mean values and the inability of powers to stay at zero for a sufficient time.b) Histogram of the residuals ε = P − P .Despite different formulas, the histogram of the residuals is asymmetric around zero with a trend to underpredict as well.</p>
<p>Figure 2. Harmonic oscillator study: NRMSE (6) versus noise level σ for different training set lengths N and fixed τ = 10.Sufficiently small noise does not worsen the predictability, i.e. the prediction algorithm stops at the target training NRMSE of 1%.After 0.3 the error does not increase further, since the noise covers the signal completely.
Average Error Γ 10.04 0.06 0.08 0.10 0.12 0.14 0.1610 20 50 100200 500 10000.020.0010 -410 -310 -2 σ10 -110 0</p>
<p>Table I .
I
Coupled spiking oscillators, method FFX.Formulae of the most accurate models.The spatio-temporal embedding reproduces the data very well, i.e. an early detection is possible.
0.08 0.10 0.12site only spatially extended mixed1Error Γ0.04 0.060.020.00051015 Complexity Γ 2 20253035</p>
<p>Table III .
III
Solar power study: description of the data set.We use a set of 5 features drawn from different sources with different sampling.
NameSymbolSourceSampling VariableSolar powerP (t)direct access10 minx1Total cloud coveragetcc(t)Synop1hx42 meter temperatureT (t)Synop1hx3Total cloud coverage prediction tcc pred (t, τ ) ECMWF-TIGGE 6hx22 meter temperature prediction T pred (t, τ ) ECMWF-TIGGE 6hx0</p>
<p>Table IV .
IV
Solar power study, method GP: formulae of the Pareto front models for seed 42.
Error Γ1 Complexity Γ2 Formula0.21171.0x10.19972.0sin(x1)0.19383.0sin(sin(x1))0.18274.00.662 (x1)0.19935.0(x0 sin(x1))0.19316.0( sin(x0) sin(x1))0.1897.0(x0x1 cos(x3))0.19438.0(x1)(x0 − x3 + 0.649)0.23489.0(x1) cos(x2x3/x0)0.19210.0(x1)(x0 − x4(x3 − 0.699))0.205712.0(x1)(x0 − x2(x2x3 − 0.592))0.268416.0(x1)(x0 + (−x2x3 + 0.597) sin(x4 + sin(x1)))0.199518.0−x0 (x1)((sin(x3) − 0.641) exp(x4) cos(x3) − 1)0.190425.0x0( (x1) + (−x3 + 0.715)(sin(x1) + sin(x2x4)) cos(x1))Error Γ1Complexity Γ2 Formula0.269419547644 00.2210.199597415208 10.108 + 0.511x10.194122751788 20.0223 + 0.606x1 + 0.139x00.193361252172 30.0470 + 0.436x1 + 0.328x0x1 + 0.138x4x00.189889358594 40.459 − 0.
ACKNOWLEDGEMENTSWe acknowledge discussion with C. Cornaro and M. Pierro on solar power forecast, helpful words on machine learning by M. Segond.We thank the unisolar initiative for their provision of the data.M. Quade and M. Abel acknowledge support by the German ministry of economy, ZIM project "green energy forecast", grant ID KF2768302ED4.
The El Niño-Southern Oscillation Phenomenon. Edward S Sarachik, Mark A Cane, 2010Cambridge Books Online</p>
<p>Theoria motus corporum coelestium in sectionibus conicis solem ambientium auctore Carolo Friderico Gauss. sumtibus Frid. Carl Friedrich, Gauss , 1809Perthes et IH Besser</p>
<p>An undulatory theory of the mechanics of atoms and molecules. Erwin Schrödinger, Physical Review. 2861926</p>
<p>On understanding types, data abstraction, and polymorphism. Luca Cardelli, Peter Wegner, ACM Computing Surveys (CSUR). 1741985</p>
<p>Amplitude equations from spatiotemporal binary-fluid convection data. H U Voss, P Kolodner, M Abel, J Kurths, Phys. Rev. Lett. 83171999</p>
<p>Additive nonparametric reconstruction of dynamical systems from time series. Markus Abel, Karsten Ahnert, Jürgen Kurths, Simon Mandelj, Physical Review E. 711152032005</p>
<p>Nonparametric modeling and spatiotemporal dynamical systems. M Abel, Int. J. Bif. Chaos. 1462004</p>
<p>Accurate state and parameter estimation in nonlinear systems with sparse observations. Daniel Rey, Michael Eldridge, Mark Kostuk, D I Henry, Jan Abarbanel, Ulrich Schumann-Bischoff, Parlitz, Physics Letters A. 37811-122014</p>
<p>Discovering governing equations from data: Sparse identification of nonlinear dynamical systems. Joshua L Steven L Brunton, Nathan Proctor, Kutz, arXiv:1509.035802015arXiv preprint</p>
<p>Numerical differentiation: global versus local methods. K Ahnert, M Abel, 10.1016/j.cpc.2007.03.009Comput. Phys. Commun. 177102007</p>
<p>A global geometric framework for nonlinear dimensionality reduction. Joshua B Tenenbaum, Vin De Silva, John C Langford, Science. 29055002000</p>
<p>Detecting intrinsic slow variables in stochastic dynamical systems by anisotropic diffusion maps. Amit Singer, Radek Erban, Ioannis G Kevrekidis, Ronald R Coifman, Proceedings of the National Academy of Sciences. 106382009</p>
<p>Nonlinear dimensionality reduction by locally linear embedding. T Sam, Lawrence K Roweis, Saul, Science. 29055002000</p>
<p>Distilling freeform natural laws from experimental data. science. Michael Schmidt, Hod Lipson, 2009324</p>
<p>Multiobjective genetic programming for nonlinear system identification. - Rodriguez, Peter J Vazquez, Fleming, Electronics Letters. 3491998</p>
<p>Closed-loop separation control using machine learning. N Gautier, J.-L Aider, B R T Duriez, M Noack, M W Segond, Abel, journal of fluid mechanics. 7702015Journal of Fluid Mechanics</p>
<p>Closed-loop turbulence control: Progress and challenges. S Brunton, B R Noack, Appl. Mech. Rev. 675508012015</p>
<p>Solar Electricity: Engineering of Photovoltaic Systems. E Lorenzo, PROGENSA. 1994</p>
<p>Deterministic nonperiodic flow. E N Lorenz, J. Athmos. Sci. 201301963</p>
<p>Nonlinear time series analysis. H Kantz, T Schreiber, 1997Cambridge University Press</p>
<p>Coping with Chaos. Series in Nonlinear Science. E Ott, T Sauer, J A Yorke, 1994WileyNew York</p>
<p>H Steven, Strogatz, Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering. Westview press2014</p>
<p>Differentiable manifolds. H Whitney, Annals of Math. 376451936</p>
<p>Detecting strange attractors in turbulence. F Takens, Lecture Notes in Mathematics No.. 8981981Springer</p>
<p>. T Sauer, J Yorke, M Casdagli, Embeddology, J. Stat. Phys. 653/41991</p>
<p>Analysis Of Observed Chaotic Data. H D I Abarbanel, M E Gilpin, M Rotenberg, 1997SpringerNew York</p>
<p>N A Gershenfeld, The Nature of Mathematical Modeling. Cambridge University Press1999</p>
<p>Pattern recognition and machine learning. M Christopher, Bishop, 2006springer</p>
<p>Ffx: Fast, scalable, deterministic symbolic regression technology. Trent Mcconaghy, Genetic Programming Theory and Practice IX. Springer2011</p>
<p>Mathematical programming: essays in honor of George B. Dantzig. Number Bd. 1 in Mathematical Programming. G B Dantzig, R Cottle, Y P Aneja, 1985Essays in Honor of George B. Dantzig. North-Holland</p>
<p>Genetic programming: on the programming of computers by means of natural selection. John R Koza, 1992MIT press1</p>
<p>Regularization and variable selection via the elastic net. Hui Zou, Trevor Hastie, Journal of the Royal Statistical Society, Series B. 672005</p>
<p>Occam's razor. Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, Manfred K Warmuth, Information processing letters. 2461987</p>
<p>Ockham's razor and bayesian analysis. H William, James O Jefferys, Berger, American Scientist. 1992</p>
<p>A fast and elitist multiobjective genetic algorithm: Nsga-ii. Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, Meyarivan, Evolutionary Computation. 622002IEEE Transactions on</p>
<p>Spea2: Improving the strength pareto evolutionary algorithm. Eckart Zitzler, Marco Laumanns, Lothar Thiele, Eckart Zitzler, Eckart Zitzler, Lothar Thiele, Lothar Thiele, 2001</p>
<p>The pareto archived evolution strategy: A new baseline algorithm for pareto multiobjective optimisation. Joshua Knowles, David Corne, CEC 99. Proceedings of the 1999 Congress on. IEEE1999. 19991Evolutionary Computation</p>
<p>Pareto-front exploitation in symbolic regression. F Guido, Mark Smits, Kotanchek, Genetic programming theory and practice II. Springer2005</p>
<p>Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming. Ekaterina J Vladislavleva, Guido F Smits, Dick Den Hertog, Evolutionary Computation. 1322009IEEE Transactions on</p>
<p>An active pulse transmission line simulating nerve axon. Jinichi Nagumo, Suguru Arimoto, Shuji Yoshizawa, Proceedings of the IRE. the IRE196250</p>
<p>A quantitative description of membrane current and its application to conduction and excitation in nerve. Alan L Hodgkin, Andrew F Huxley, The Journal of physiology. 11745001952</p>
<p>The default value of the package which works well in most scenarios. </p>
<p>European Centre for Medium-Range Weather Forecasts. Ecmwf -european centre for medium-range weather forecasts. </p>
<p>Controlling code growth by dynamically shaping the genotype size distribution. Genetic Programming and Evolvable Machines. Marc-André Gardner, Christian Gagné, Marc Parizeau, 2014</p>            </div>
        </div>

    </div>
</body>
</html>
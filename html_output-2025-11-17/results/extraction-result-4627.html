<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4627 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4627</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4627</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-267782525</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14660v2.pdf" target="_blank">ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4627.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4627.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source large multimodal autoregressive transformer from OpenAI that achieves strong average performance on many mathematical benchmarks and is evaluated in this paper as a baseline closed-source model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI models (gpt-4 and gpt-3.5-turbo) used via API; described as the most powerful closed models evaluated in ConceptMath (no architecture/training details given beyond that they are closed-source OpenAI models).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>General math word problems spanning arithmetic (basic four ops), multi-step calculations, fractions, percentages, geometry, basic probability and measurement (ConceptMath covers ~214 concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Performance improvements with chain-of-thought (CoT) and few-shot prompting suggest GPT models can execute internal multi-step reasoning traces when elicited; performance may also partly reflect exposure to large pretraining corpora (possible memorization) but paper's contamination checks suggest low direct memorization of ConceptMath items.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper reports that closed-source models benefit from CoT and few-shot prompting in their evaluations (qualitative statement that CoT / few-shot boost closed-source models); contamination analyses show low Rouge-L and MIN-K% probabilities on ConceptMath, arguing against simple memorization of benchmark items.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct mechanistic probes (e.g., neuron-level analyses) presented; paper notes some models may already have CoT data in pretraining (Longpre et al.) which complicates causal interpretation of CoT effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: closed-source models (GPT-3.5/GPT-4) are included among the top performers on many benchmarks (not all ConceptMath per-concept numbers reproduced in the text), and the paper states closed-source models often receive boosts from CoT and few-shot prompting. Exact ConceptMath per-concept accuracies for GPT models are not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No internal probing for GPT models reported; contamination checks (Rouge-based and probability-based MIN-K%) applied to dataset to test whether items appear in pretraining; results indicated low contamination rates for ConceptMath.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Despite strong average performance on prior benchmarks, closed-source models still show substantial variation across fine-grained concepts (ConceptMath observation). Performance can depend on prompt style (CoT/few-shot helps) and may still fail on specific basic concepts when evaluated concept-wise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Paper contrasts closed-source GPT models with many open-source and math-specialized models; closed-source models typically show higher average performance and respond well to CoT, whereas some open-source models do not benefit from CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4627.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4627.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source transformer-based foundation model by Meta (13B parameter variant) evaluated extensively on ConceptMath and used as a base for math-specialized fine-tuning experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source model from Meta (LLaMA2 family); evaluated in zero-shot, CoT, and few-shot settings and also used as the base model for efficient fine-tuning experiments described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Wide range of elementary and middle-school math concepts (arithmetic operations, fractions, percentages, ratios/proportions, geometry primitives, volumes/areas, probability, measurement, multi-step word problems).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Arithmetic capability appears strongly data-dependent: model learns concept-specific procedural patterns through fine-tuning / continued pretraining rather than possessing a single robust algorithmic arithmetic module; domain-specific fine-tuning and large-scale math pretraining can both change performance substantially (either improving or degrading generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Efficient fine-tuning intervention (train a concept classifier, crawl concept-specific samples, combine with general math data MMQA) substantially improves performance on weakest concepts (Table 4 & 5). Continued math pretraining or concept-specific fine-tuning yields large gains on targeted concepts, supporting a data-driven learning mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Domain-specific fine-tuning can degrade generalization—paper notes domain-specific fine-tuning may reduce broader problem-solving ability for some models (qualitative observation). Contamination checks indicate ConceptMath items unlikely to be memorized from pretraining, so gains are not explained by memorized answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Quantitative fine-tuning results (Table 4): For 10 originally worst concepts (Elementary-EN) LLaMA2-13B baseline accuracies (percent) by concept: Cones 0.00, Spheres 5.88, Polygons 11.11, Rational Number 11.76, Cylinders 11.76, Angles 11.76, Probability 18.75, Perimeter 26.32, Volume 27.78, Proportional 27.78. Avg acc over these 10 concepts: 15.29% (baseline). After fine-tuning with MMQA: avg 36.88%. After MMQA & Concept-Specific (CS) data: avg 53.36%. Avg over 33 concepts: baseline 51.94%, after MMQA 58.14%, after MMQA&CS 60.67%. Overall acc: baseline 44.02%, after MMQA 53.94%, after MMQA&CS 59.29%. (All reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Efficient fine-tuning experiment: built a concept classifier (backbone Baichuan2-13B) to crawl ~495 CS samples for the 10 worst concepts and fine-tuned LLaMA2-13B with MMQA and CS combined; this intervention improved bottom-concept performance substantially while preserving other concepts. Contamination detection methods (Rouge-based and MIN-K% probability) applied to rule out memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Large variations across concepts and catastrophic failures on some basic concepts (e.g., cones 0%, rational numbers ~12% baseline). Domain-specific fine-tuning alone may harm generalization if not combined with general math data. Some prompting styles (CoT) may not help open-source variants as much as closed-source ones.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared directly to math-specialized variants and other open-source models: fine-tuned LLaMA2-13B with MMQA & CS narrows the gap to specialized math models; math-specialized models that underwent large-scale math pretraining (InternLM2-Math, DeepSeekMath) show stronger performance than same-size general models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4627.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4627.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM2-Math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM2-Math (7B / 20B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Math-specialized variants of the InternLM2 family created by continuing pretraining / fine-tuning on large math-related corpora, reported to perform substantially better on mathematical reasoning tasks in ConceptMath.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Math-7B / InternLM2-Math-20B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Math-specialized LLMs derived from InternLM2 via continued pretraining/fine-tuning on large-scale math-related datasets (paper cites them as evaluated models; specific pretraining scale for math-specialized models is reported as large-scale math-related pretraining for other models >100B tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Comprehensive ConceptMath coverage including arithmetic, fractions, geometry, probability, and multi-step problems across elementary and middle school concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Large-scale math-centric pretraining provides exposure to many math problem patterns and rationales (including CoT-style reasoning), enabling better pattern induction for arithmetic and multi-step reasoning, rather than a distinct algorithmic arithmetic module.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper states InternLM2-Math and DeepSeekMath, which continued pretraining on large-scale math-related datasets (>=100B tokens), show sufficient improvements compared to models of similar size, indicating effectiveness of large-scale math pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No internal probing or ablation isolating whether improvements come from dataset size, style (CoT), or curriculum; potential risk that heavy domain-specific pretraining could degrade some out-of-domain generalization (paper warns about domain-specific fine-tuning degrading generalization in other contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports these math-specialized models achieve notable improvements relative to same-size general models; exact per-concept numeric values for InternLM2-Math are not quoted in the provided excerpt, but figures/tables in the paper contain model comparisons where math-specialized models rank higher.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No targeted neuron-level probing; the intervention-like evidence is the training recipe (continued math pretraining) correlating with improved ConceptMath scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Paper warns that domain-specific fine-tuning or pretraining can sometimes harm generalization and that different models have unique concept-wise blind spots despite overall gains; no detailed failure modes for InternLM2-Math alone given in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>InternLM2-Math and DeepSeekMath outperform general-purpose open-source models of similar size on ConceptMath, supporting the value of math-targeted pretraining/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4627.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4627.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeekMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeekMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A math-focused open model (7B) reported in the paper as a strong mathematical language model, produced by continued pretraining or fine-tuning on math-specific corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeekMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open math-specialized LLM (7B) that the authors evaluated; described as a powerful mathematical language model resulting from math-targeted training.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>ConceptMath tasks across arithmetic operations, fractions, geometry, probability, and multi-step word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Continued large-scale math pretraining supplies abundant examples of procedural and chain-of-thought style solutions enabling improved pattern-based multi-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper explicitly states DeepSeekMath (trained on large-scale math data) shows sufficient improvements versus similar-size general models, suggesting pretraining on math corpora improves arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No fine-grained mechanistic analyses; paper also notes that models still show concept-wise variability, so pretraining doesn't yield uniform mastery across all arithmetic concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Described qualitatively as showing substantial improvements relative to similarly sized general-purpose models; exact ConceptMath numeric scores for DeepSeekMath are referenced in figures/tables but not excerpted in text.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No neuron-level probes; the paper uses performance comparisons before/after math-focused pretraining for inference about mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Even math-specialized models show differences across concepts and may still fail on specific basic concepts; no detailed breakdown for DeepSeekMath alone in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared favorably to same-size general models; combined evidence in paper places DeepSeekMath among the stronger open-source performers on ConceptMath.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4627.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4627.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaMath-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaMath-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-specific mathematical language model fine-tuned from LLaMA2 on the MetaMathQA dataset; evaluated in ConceptMath as one of several math-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaMath-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A math-specialized 13B model fine-tuned from LLaMA2 using the MetaMathQA dataset (cited in paper); used as an evaluated model in ConceptMath comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems including arithmetic, algebra, geometry, and multi-step reasoning covered by ConceptMath.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Fine-tuning on a curated math QA dataset (MetaMathQA) imparts concept-specific solution patterns and likely exposes the model to chain-of-thought style solutions, improving arithmetic performance on related items.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper lists MetaMath among math-specialized models that show improvements, and references MetaMathQA as a large curated math dataset used for fine-tuning math models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper notes that domain-specific fine-tuning may hurt generalization if not balanced with broader math data; no intrinsic probing to pinpoint the mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included in the model comparisons; specific numeric ConceptMath scores for MetaMath-13B are not reproduced in the excerpt but it is reported as one of the evaluated math-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No additional probing; MetaMath is cited as a prior method/dataset used to build math-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>As with other specialized models, MetaMath may have concept-wise blind spots and could overfit to styles present in its fine-tuning data unless balanced with more general math examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Placed among other math-specialized LLaMA derivatives (WizardMath, MAmmoTH) that generally outperform base LLaMA2 models on math benchmarks when fine-tuned with math QA datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4627.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4627.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAmmoTH / WizardMath</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAmmoTH-13B / WizardMath-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two LLaMA-derived math-specialized models (MAmmoTH uses a hybrid instruction tuning dataset with CoT and PoT; WizardMath uses reinforced evol-instruct) evaluated in ConceptMath for improved arithmetic/math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAmmoTH-13B, WizardMath-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Math-focused LLaMA-based 13B models: MAmmoTH is fine-tuned on MathInstruct containing chain-of-thought and program-of-thought rationales; WizardMath is fine-tuned using reinforcement learning from evolutionary instruct signals.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems including multi-step arithmetic, fraction operations, programmatic solution patterns (PoT), geometry and probability as represented in ConceptMath.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Exposure to CoT and PoT style rationales during fine-tuning teaches models to produce intermediate reasoning steps and simple program-like solution sequences, improving arithmetic procedure execution on many problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper references the datasets and training styles (CoT, PoT, RL fine-tuning) used to construct these models and notes that such math-specialized training yields better performance in ConceptMath comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct ablation isolating PoT vs CoT contributions; the authors also caution that different fine-tuning regimens produce distinct concept-wise strengths and weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported among evaluated models; excerpt does not include per-model numeric values for these models specifically, but table/figures in paper show relative rankings where math-specialized models often outperform base models.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No internal probing presented; training recipes (use of PoT/CoT and RL) are the main evidence linking training style to improved arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Even with CoT/PoT fine-tuning, models display large per-concept variability and can still fail catastrophically on some basic concepts; domain-specific training can alter generalization patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared alongside MetaMath and other specialized models, generally performing better than base LLaMA variants and many general-purpose open-source models in ConceptMath evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MetaMath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
                <li>Openwebmath: An open dataset of high-quality mathematical web text <em>(Rating: 2)</em></li>
                <li>Mammoth: Building math generalist models through hybrid instruction tuning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Detecting pretraining data from large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4627",
    "paper_id": "paper-267782525",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Closed-source large multimodal autoregressive transformer from OpenAI that achieves strong average performance on many mathematical benchmarks and is evaluated in this paper as a baseline closed-source model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-3.5",
            "model_description": "Closed-source OpenAI models (gpt-4 and gpt-3.5-turbo) used via API; described as the most powerful closed models evaluated in ConceptMath (no architecture/training details given beyond that they are closed-source OpenAI models).",
            "arithmetic_task_type": "General math word problems spanning arithmetic (basic four ops), multi-step calculations, fractions, percentages, geometry, basic probability and measurement (ConceptMath covers ~214 concepts).",
            "mechanism_hypothesis": "Performance improvements with chain-of-thought (CoT) and few-shot prompting suggest GPT models can execute internal multi-step reasoning traces when elicited; performance may also partly reflect exposure to large pretraining corpora (possible memorization) but paper's contamination checks suggest low direct memorization of ConceptMath items.",
            "evidence_for_mechanism": "Paper reports that closed-source models benefit from CoT and few-shot prompting in their evaluations (qualitative statement that CoT / few-shot boost closed-source models); contamination analyses show low Rouge-L and MIN-K% probabilities on ConceptMath, arguing against simple memorization of benchmark items.",
            "evidence_against_mechanism": "No direct mechanistic probes (e.g., neuron-level analyses) presented; paper notes some models may already have CoT data in pretraining (Longpre et al.) which complicates causal interpretation of CoT effectiveness.",
            "performance_metrics": "Qualitative: closed-source models (GPT-3.5/GPT-4) are included among the top performers on many benchmarks (not all ConceptMath per-concept numbers reproduced in the text), and the paper states closed-source models often receive boosts from CoT and few-shot prompting. Exact ConceptMath per-concept accuracies for GPT models are not provided in the excerpt.",
            "probing_or_intervention_results": "No internal probing for GPT models reported; contamination checks (Rouge-based and probability-based MIN-K%) applied to dataset to test whether items appear in pretraining; results indicated low contamination rates for ConceptMath.",
            "limitations_and_failure_modes": "Despite strong average performance on prior benchmarks, closed-source models still show substantial variation across fine-grained concepts (ConceptMath observation). Performance can depend on prompt style (CoT/few-shot helps) and may still fail on specific basic concepts when evaluated concept-wise.",
            "comparison_to_other_models": "Paper contrasts closed-source GPT models with many open-source and math-specialized models; closed-source models typically show higher average performance and respond well to CoT, whereas some open-source models do not benefit from CoT prompting.",
            "uuid": "e4627.0",
            "source_info": {
                "paper_title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA2-13B",
            "name_full": "LLaMA2 13B",
            "brief_description": "Open-source transformer-based foundation model by Meta (13B parameter variant) evaluated extensively on ConceptMath and used as a base for math-specialized fine-tuning experiments in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B",
            "model_description": "Open-source model from Meta (LLaMA2 family); evaluated in zero-shot, CoT, and few-shot settings and also used as the base model for efficient fine-tuning experiments described in the paper.",
            "arithmetic_task_type": "Wide range of elementary and middle-school math concepts (arithmetic operations, fractions, percentages, ratios/proportions, geometry primitives, volumes/areas, probability, measurement, multi-step word problems).",
            "mechanism_hypothesis": "Arithmetic capability appears strongly data-dependent: model learns concept-specific procedural patterns through fine-tuning / continued pretraining rather than possessing a single robust algorithmic arithmetic module; domain-specific fine-tuning and large-scale math pretraining can both change performance substantially (either improving or degrading generalization).",
            "evidence_for_mechanism": "Efficient fine-tuning intervention (train a concept classifier, crawl concept-specific samples, combine with general math data MMQA) substantially improves performance on weakest concepts (Table 4 & 5). Continued math pretraining or concept-specific fine-tuning yields large gains on targeted concepts, supporting a data-driven learning mechanism.",
            "evidence_against_mechanism": "Domain-specific fine-tuning can degrade generalization—paper notes domain-specific fine-tuning may reduce broader problem-solving ability for some models (qualitative observation). Contamination checks indicate ConceptMath items unlikely to be memorized from pretraining, so gains are not explained by memorized answers.",
            "performance_metrics": "Quantitative fine-tuning results (Table 4): For 10 originally worst concepts (Elementary-EN) LLaMA2-13B baseline accuracies (percent) by concept: Cones 0.00, Spheres 5.88, Polygons 11.11, Rational Number 11.76, Cylinders 11.76, Angles 11.76, Probability 18.75, Perimeter 26.32, Volume 27.78, Proportional 27.78. Avg acc over these 10 concepts: 15.29% (baseline). After fine-tuning with MMQA: avg 36.88%. After MMQA & Concept-Specific (CS) data: avg 53.36%. Avg over 33 concepts: baseline 51.94%, after MMQA 58.14%, after MMQA&CS 60.67%. Overall acc: baseline 44.02%, after MMQA 53.94%, after MMQA&CS 59.29%. (All reported in paper tables.)",
            "probing_or_intervention_results": "Efficient fine-tuning experiment: built a concept classifier (backbone Baichuan2-13B) to crawl ~495 CS samples for the 10 worst concepts and fine-tuned LLaMA2-13B with MMQA and CS combined; this intervention improved bottom-concept performance substantially while preserving other concepts. Contamination detection methods (Rouge-based and MIN-K% probability) applied to rule out memorization.",
            "limitations_and_failure_modes": "Large variations across concepts and catastrophic failures on some basic concepts (e.g., cones 0%, rational numbers ~12% baseline). Domain-specific fine-tuning alone may harm generalization if not combined with general math data. Some prompting styles (CoT) may not help open-source variants as much as closed-source ones.",
            "comparison_to_other_models": "Compared directly to math-specialized variants and other open-source models: fine-tuned LLaMA2-13B with MMQA & CS narrows the gap to specialized math models; math-specialized models that underwent large-scale math pretraining (InternLM2-Math, DeepSeekMath) show stronger performance than same-size general models.",
            "uuid": "e4627.1",
            "source_info": {
                "paper_title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InternLM2-Math",
            "name_full": "InternLM2-Math (7B / 20B)",
            "brief_description": "Math-specialized variants of the InternLM2 family created by continuing pretraining / fine-tuning on large math-related corpora, reported to perform substantially better on mathematical reasoning tasks in ConceptMath.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternLM2-Math-7B / InternLM2-Math-20B",
            "model_description": "Math-specialized LLMs derived from InternLM2 via continued pretraining/fine-tuning on large-scale math-related datasets (paper cites them as evaluated models; specific pretraining scale for math-specialized models is reported as large-scale math-related pretraining for other models &gt;100B tokens).",
            "arithmetic_task_type": "Comprehensive ConceptMath coverage including arithmetic, fractions, geometry, probability, and multi-step problems across elementary and middle school concepts.",
            "mechanism_hypothesis": "Large-scale math-centric pretraining provides exposure to many math problem patterns and rationales (including CoT-style reasoning), enabling better pattern induction for arithmetic and multi-step reasoning, rather than a distinct algorithmic arithmetic module.",
            "evidence_for_mechanism": "Paper states InternLM2-Math and DeepSeekMath, which continued pretraining on large-scale math-related datasets (&gt;=100B tokens), show sufficient improvements compared to models of similar size, indicating effectiveness of large-scale math pretraining.",
            "evidence_against_mechanism": "No internal probing or ablation isolating whether improvements come from dataset size, style (CoT), or curriculum; potential risk that heavy domain-specific pretraining could degrade some out-of-domain generalization (paper warns about domain-specific fine-tuning degrading generalization in other contexts).",
            "performance_metrics": "Paper reports these math-specialized models achieve notable improvements relative to same-size general models; exact per-concept numeric values for InternLM2-Math are not quoted in the provided excerpt, but figures/tables in the paper contain model comparisons where math-specialized models rank higher.",
            "probing_or_intervention_results": "No targeted neuron-level probing; the intervention-like evidence is the training recipe (continued math pretraining) correlating with improved ConceptMath scores.",
            "limitations_and_failure_modes": "Paper warns that domain-specific fine-tuning or pretraining can sometimes harm generalization and that different models have unique concept-wise blind spots despite overall gains; no detailed failure modes for InternLM2-Math alone given in excerpt.",
            "comparison_to_other_models": "InternLM2-Math and DeepSeekMath outperform general-purpose open-source models of similar size on ConceptMath, supporting the value of math-targeted pretraining/fine-tuning.",
            "uuid": "e4627.2",
            "source_info": {
                "paper_title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DeepSeekMath-7B",
            "name_full": "DeepSeekMath-7B",
            "brief_description": "A math-focused open model (7B) reported in the paper as a strong mathematical language model, produced by continued pretraining or fine-tuning on math-specific corpora.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeekMath-7B",
            "model_description": "Open math-specialized LLM (7B) that the authors evaluated; described as a powerful mathematical language model resulting from math-targeted training.",
            "arithmetic_task_type": "ConceptMath tasks across arithmetic operations, fractions, geometry, probability, and multi-step word problems.",
            "mechanism_hypothesis": "Continued large-scale math pretraining supplies abundant examples of procedural and chain-of-thought style solutions enabling improved pattern-based multi-step arithmetic reasoning.",
            "evidence_for_mechanism": "Paper explicitly states DeepSeekMath (trained on large-scale math data) shows sufficient improvements versus similar-size general models, suggesting pretraining on math corpora improves arithmetic performance.",
            "evidence_against_mechanism": "No fine-grained mechanistic analyses; paper also notes that models still show concept-wise variability, so pretraining doesn't yield uniform mastery across all arithmetic concepts.",
            "performance_metrics": "Described qualitatively as showing substantial improvements relative to similarly sized general-purpose models; exact ConceptMath numeric scores for DeepSeekMath are referenced in figures/tables but not excerpted in text.",
            "probing_or_intervention_results": "No neuron-level probes; the paper uses performance comparisons before/after math-focused pretraining for inference about mechanism.",
            "limitations_and_failure_modes": "Even math-specialized models show differences across concepts and may still fail on specific basic concepts; no detailed breakdown for DeepSeekMath alone in the excerpt.",
            "comparison_to_other_models": "Compared favorably to same-size general models; combined evidence in paper places DeepSeekMath among the stronger open-source performers on ConceptMath.",
            "uuid": "e4627.3",
            "source_info": {
                "paper_title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MetaMath-13B",
            "name_full": "MetaMath-13B",
            "brief_description": "Domain-specific mathematical language model fine-tuned from LLaMA2 on the MetaMathQA dataset; evaluated in ConceptMath as one of several math-specialized models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MetaMath-13B",
            "model_description": "A math-specialized 13B model fine-tuned from LLaMA2 using the MetaMathQA dataset (cited in paper); used as an evaluated model in ConceptMath comparisons.",
            "arithmetic_task_type": "Math word problems including arithmetic, algebra, geometry, and multi-step reasoning covered by ConceptMath.",
            "mechanism_hypothesis": "Fine-tuning on a curated math QA dataset (MetaMathQA) imparts concept-specific solution patterns and likely exposes the model to chain-of-thought style solutions, improving arithmetic performance on related items.",
            "evidence_for_mechanism": "Paper lists MetaMath among math-specialized models that show improvements, and references MetaMathQA as a large curated math dataset used for fine-tuning math models.",
            "evidence_against_mechanism": "Paper notes that domain-specific fine-tuning may hurt generalization if not balanced with broader math data; no intrinsic probing to pinpoint the mechanism.",
            "performance_metrics": "Included in the model comparisons; specific numeric ConceptMath scores for MetaMath-13B are not reproduced in the excerpt but it is reported as one of the evaluated math-specialized models.",
            "probing_or_intervention_results": "No additional probing; MetaMath is cited as a prior method/dataset used to build math-specialized models.",
            "limitations_and_failure_modes": "As with other specialized models, MetaMath may have concept-wise blind spots and could overfit to styles present in its fine-tuning data unless balanced with more general math examples.",
            "comparison_to_other_models": "Placed among other math-specialized LLaMA derivatives (WizardMath, MAmmoTH) that generally outperform base LLaMA2 models on math benchmarks when fine-tuned with math QA datasets.",
            "uuid": "e4627.4",
            "source_info": {
                "paper_title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MAmmoTH / WizardMath",
            "name_full": "MAmmoTH-13B / WizardMath-13B",
            "brief_description": "Two LLaMA-derived math-specialized models (MAmmoTH uses a hybrid instruction tuning dataset with CoT and PoT; WizardMath uses reinforced evol-instruct) evaluated in ConceptMath for improved arithmetic/math reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MAmmoTH-13B, WizardMath-13B",
            "model_description": "Math-focused LLaMA-based 13B models: MAmmoTH is fine-tuned on MathInstruct containing chain-of-thought and program-of-thought rationales; WizardMath is fine-tuned using reinforcement learning from evolutionary instruct signals.",
            "arithmetic_task_type": "Math word problems including multi-step arithmetic, fraction operations, programmatic solution patterns (PoT), geometry and probability as represented in ConceptMath.",
            "mechanism_hypothesis": "Exposure to CoT and PoT style rationales during fine-tuning teaches models to produce intermediate reasoning steps and simple program-like solution sequences, improving arithmetic procedure execution on many problems.",
            "evidence_for_mechanism": "Paper references the datasets and training styles (CoT, PoT, RL fine-tuning) used to construct these models and notes that such math-specialized training yields better performance in ConceptMath comparisons.",
            "evidence_against_mechanism": "No direct ablation isolating PoT vs CoT contributions; the authors also caution that different fine-tuning regimens produce distinct concept-wise strengths and weaknesses.",
            "performance_metrics": "Reported among evaluated models; excerpt does not include per-model numeric values for these models specifically, but table/figures in paper show relative rankings where math-specialized models often outperform base models.",
            "probing_or_intervention_results": "No internal probing presented; training recipes (use of PoT/CoT and RL) are the main evidence linking training style to improved arithmetic performance.",
            "limitations_and_failure_modes": "Even with CoT/PoT fine-tuning, models display large per-concept variability and can still fail catastrophically on some basic concepts; domain-specific training can alter generalization patterns.",
            "comparison_to_other_models": "Compared alongside MetaMath and other specialized models, generally performing better than base LLaMA variants and many general-purpose open-source models in ConceptMath evaluations.",
            "uuid": "e4627.5",
            "source_info": {
                "paper_title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MetaMath: Bootstrap your own mathematical questions for large language models",
            "rating": 2,
            "sanitized_title": "metamath_bootstrap_your_own_mathematical_questions_for_large_language_models"
        },
        {
            "paper_title": "Openwebmath: An open dataset of high-quality mathematical web text",
            "rating": 2,
            "sanitized_title": "openwebmath_an_open_dataset_of_highquality_mathematical_web_text"
        },
        {
            "paper_title": "Mammoth: Building math generalist models through hybrid instruction tuning",
            "rating": 2,
            "sanitized_title": "mammoth_building_math_generalist_models_through_hybrid_instruction_tuning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Detecting pretraining data from large language models",
            "rating": 1,
            "sanitized_title": "detecting_pretraining_data_from_large_language_models"
        }
    ],
    "cost": 0.0166585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models</p>
<p>Yanan Wu 
Alibaba Group</p>
<p>Jie Liu 
The Chinese University of Hong Kong</p>
<p>Xingyuan Bu 
Alibaba Group</p>
<p>Jiaheng Liu 
Alibaba Group</p>
<p>Zhanhui Zhou 
Alibaba Group</p>
<p>Yuanxing Zhang 
Alibaba Group</p>
<p>Chenchen Zhang 
Zhiqi Bai 
Alibaba Group</p>
<p>Haibin Chen 
Alibaba Group</p>
<p>Tiezheng Ge 
Alibaba Group</p>
<p>Wanli Ouyang 
Alibaba Group</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Wenbo Su 
Alibaba Group</p>
<p>Bo Zheng 
Jiaheng Liu 
Alibaba Group</p>
<p>ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models
EFFB72163FDDA014575827E7C8E66D9C
This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs).Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with conceptwise accuracies.Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones.Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs.Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models.</p>
<p>Introduction</p>
<p>Mathematical reasoning is a crucial capability for Large Language Models (LLMs).Recent advancements in LLMs, including Anthropic (Anthropic, 2023), GPT-4 (OpenAI, 2023), and LLaMA (Touvron et al., 2023a), have demonstrated impressive mathematical reasoning on existing benchmarks with high average accuracies on datasets like GSM8K (Cobbe et al., 2021).Although these benchmarks are able to measure the overall mathematical reasoning capabilities of LLMs on average, they fail to probe the fine-grained failure modes of mathematical reasoning on specific mathematical concepts.For example, Fig. 1 shows that the performance of LLaMA2-13B varies significantly across
3 R Z H U V 1 X P H U L F D O H [ S U V ( V W L P D W L R Q U R X Q G L Q J ' H F L P D O V / L J K W K H D Y \ 7 H P S H U D W X U H 5 D W L R 3 D W W H U Q V &amp; \ O L Q G H U V 3 H U L P H W H U 5 D W L R Q D O Q X P E H U 3 R O \ J R Q V 3 U R E D E L O L W \
$FFXUDF\ :HDNQHVVHV (QKDQFLQJ:HDNQHVVHV //D0$ //D0$)7</p>
<p>Figure 1: The concept-wise accuracies of LLaMA2-13B and the fine-tuned version based on our efficient finetuning method (i.e., LLaMA2-FT).</p>
<p>different concepts and fails on simple concepts like Rational number and Cylinders.It is crucial to know these specific failure modes of the language model, especially in some practical applications where we need to focus on specific mathematical abilities.For example, for financial analysts, calculation and statistics are the concepts of most interest while others like geometry are not as important.Moreover, the mathematics system, by its nature, is more fine-grained than holistic.It is typically organized into distinct math concepts , and humans develop comprehensive mathematical capabilities through a concept-by-concept, curriculum-based learning process (Simon, 2011;Fritz et al., 2013).These issues underscore the core motivation of this paper: the need for a fine-grained benchmark that evaluates concept-wise mathematical reasoning capabilities of LLMs.</p>
<p>Therefore, first, we introduce ConceptMath, the first bilingual (English and Chinese), concept-wise benchmark for measuring mathematical reasoning.ConceptMath gathers math concepts from four edhttps://en.wikipedia.org/wiki/Lists_of_mathematics_topics arXiv:2402.14660v2[cs.CL] 23 Feb 2024 ucational systems, resulting in four distinct mathematical concept systems: English Elementary, English Middle, Chinese Elementary, and Chinese Middle .Each of these concept systems organizes around 50 atomic math concepts under a three-level hierarchy and each concept includes approximately 20 mathematical problems.Overall, ConceptMath comprises a total of 4011 math word problems across 214 math concepts, and Fig. 2 shows the diagram overview of ConceptMath.</p>
<p>Second, based on our ConceptMath, we perform extensive experiments to assess the mathematical reasoning of existing LLMs, including 2 closesourced LLMs and 17 open-sourced LLMs.These evaluations were performed in zero-shot, chainof-thought (CoT), and few-shot settings.To our surprise, even though most of the evaluated LLMs claim to achieve high average accuracies on traditional mathematical benchmarks (e.g., GSM8K), they fail catastrophically across a wide spectrum of mathematical concepts.</p>
<p>Third, to make targeted improvements on underperformed math concepts, we propose an efficient fine-tuning strategy by first training a concept classifier and then crawling a set of samples from a large open-sourced math dataset (Paster et al., 2023;Wang et al., 2023b) for further LLMs finetuning.In Fig. 1, for LLaMA2-FT, we observe that the results of these weaknesses improved a lot after using the efficient fine-tuning method.</p>
<p>In summary, our contributions are as follows: • We introduce ConceptMath, the first bilingual, concept-wise benchmark for measuring mathematical reasoning.ConceptMath encompasses 4 systems, approximately 214 math concepts, and 4011 math word problems, which can guide further improvements on the mathematical reasoning of existing models.</p>
<p>• Based on ConceptMath, we evaluate many LLMs and perform a comprehensive analysis of their results.For example, we observe that most of these LLMs (including open-sourced, closedsourced, general-purpose, or math-specialized models) show significant variations in their performance results across math concepts.</p>
<p>• We also evaluate the contamination rate of our ConceptMath and introduce a simple and efficient fine-tuning method to improve the weaknesses of existing LLMs.</p>
<p>The four concept systems are abbreviated as Elementary-EN, Middle-EN, Elementary-ZH, and Middle-ZH.</p>
<p>ConceptMath</p>
<p>ConceptMath is the first bilingual, concept-wise benchmark for measuring mathematical reasoning.In this section, we describe the design principle, dataset collection process, dataset statistics and an efficient fine-tuning strategy to enhance the weaknesses identified by our ConceptMath.</p>
<p>Design Principle</p>
<p>We created ConceptMath based on the following two high-level design principles:</p>
<p>Concept-wised Hierarchical System.The primary goal of ConceptMath is to evaluate the mathematical reasoning capacities of language models at different granularity.Therefore, ConceptMath organizes math problems within a three-level hierarchy of mathematical concepts in Fig. 2.This approach provides concept-wise evaluation for mathematical reasoning of language models and makes targeted and effective improvements possible.</p>
<p>Bilingualism.Most of the current mathematical benchmark focuses solely on English, leaving multi-lingual mathematical reasoning unexplored.As an early effort to explore multi-lingual mathematical reasoning, we evaluate mathematical reasoning in two languages: English and Chinese.Besides, since cultures and educational systems vary across different languages, common math concepts can differ a lot.Therefore, we carefully collect concepts in both languages, instead of merely translating from one language to another.For example, measurement metrics (e.g., money, size) are different for English and Chinese.</p>
<p>Data Collection</p>
<p>Subsequently, for data collection, we take a twostep approach to operationalize the aforementioned design principles: First, we recruit experts to delineate a hierarchy of math concepts based on different education systems.Secondly, we collect problems for each concept from various sources or design problems manually, which is succeeded by quality assessment and data cleaning.</p>
<p>Math Concept System Construction.Since the education systems provide a natural hierarchy of math concepts, we recruited four teachers from elementary and middle schools, specializing in either English or Chinese, to organize a hierarchy of math concepts for different education systems.This leads to four concept systems: Elementary-EN, Middle-EN, Elementary-ZH, and Middle-ZH, with each system consisting of a three-level hierarchy of around 50 atomic math concepts (Fig. 2).
几何 圆 三角形 平行 四边 形 梯形 立 体 图 形 应 用 度 量 与 统 计 度量 面积问题 浓度 问题 人民 币问 题 时 间 问 题 温 度 问 题 统 计 规 律 排 列 组 合 统 计 指 标 圆 柱 长 方 体 正 方 体 立 体 图 形 综 合 平面 图形 角 长 方 形 正 方 形 平 面 图 形 综 合 数与代数 基 础 运 算 乘 法 问 题 除 法 问 题 倒 数 问 题 定 义 新 运 算 方 程 问 题 四 则 运 算 比与比例 百分 率问 题 倍数问 题 比例问题 概率问 题 分 数 运 算 分数 应用 分 数 与 小 数 分 数 运 算 最 简 分 数 因 数 与 倍 数 公 倍 数 问 题 公 约 数 问 题 因 数 问 题 质 数 问 题 因 数 与 倍 数 综 合 经典问题 对 折 问 题 工 程 问 题 还 原 问 题 鸡兔 同笼 问题 利息问题 年龄问题 税率 问题 页 码 问 题 折 扣 问 题 植 树 问 题 周 期 问 题 基 础 问 题 差 倍 问 题 归 一 问 题 归 总 问 题 和 差 倍 问 题 基 础 问 题 行程 问题 相 遇 问 题 行程 问题 追击问题 (c) Chinese Elementary (Elementary-ZH) 方 程 与 不 等 式 不等 式与 不等 式组 解一元一次 不等式 解一 元一 次 不等 式组 一 元 一 次 不 等 式 的 应 用 一 元 一 次 不 等 式 组 的 应 用 分 式 方 程 分 式 方 程 的 应 用 解 分 式 方 程 一 元 二 次 方 程 解 一 元 二 次 方 程 一 元 二 次 方 程 的 应 用 一 元 一 次 方 程 解 一 元 一 次 方 程 一 元 一 次 方 程 的 应 用 函 数 平 面 直 角 坐 标 系 点 所 在 象 限 有 序 数 对 二次 函数 二次 函数 的应 用 抛物线的 性质 反比 例函 数 反比例函数的定义 反比 例函 数的 性质 反比 例函 数的 应用 一 次 函 数 求 一 次 函 数 解 析 式 函 数 与 二 元 一 次 方 程 组 函 数 与 一 元 一 次 不 等 式 函 数 与 一 元 一 次 方 程 正 比 例 函 数 几何 圆 垂 径 定 理 点 线 圆 位 置 关 系 弧 长 和 扇 形 面 积 圆 心 角 圆周 角 正多边形 和圆 三角 形 等边三角形 等腰 三角 形 勾 股 定 理 全 等 三 角 形 四 边 形 平 行 四 边 形 梯 形 立 体 图 形 圆 锥 数与 式 根 式 二 次 根 式 的 运 算 立 方 根 平 方 根 与 算 术 平 方 根 同 类 二 次 根 式 整式 乘 法 公 式 整式 的乘 除及 混合 整式的加减 代数式 代数式求值 同类 项 分 式 约 分 与 通 分 指 数 幂 无 理 数 判 断 无 理 数 因 式 十 字 相 乘 法 提 公 因 式 应 用 鸽 巢 问 题 流 水 问 题 统 计 与 概 率 概 率 概 率 的 应 用 求 概 率 随 机 事 件 与 概 率 数据分 析 数据 的波 动趋 势 数据的集中 趋势(
Math Problem Construction.Then we conducted a thorough data acquisition from various sources (including educational websites, textbooks, and search engines with specific concepts) to collect math word problems (including both questions and answers) for each math concept.To guarantee a balance across all concepts, approximately 20 problems were gathered for each math concept.Following this, both GPT-4 (OpenAI, 2023) and human experts were employed to verify and rectify the categorization and the solution of each problem.However, we observed that for some concepts, the problem count was significantly below 20.To address this issue, manual efforts were undertaken to augment these categories, ensuring a consistent collection of 20 problems for each concept.Furthermore, to broaden the diversity of the dataset and minimize the risk of data contamination, all gathered problems were paraphrased using GPT-4.</p>
<p>It is important to note that the collection and annotation processes were carried out by a team of six members, each possessing a university degree in an engineering discipline, to maintain a high level of technical expertise in executing these tasks.</p>
<p>Dataset Statistics</p>
<p>Comparison to existing datasets.As shown in Table 1, our ConceptMath differs from related datasets in various aspects: (1) ConceptMath is the first dataset to study fine-grained mathematical concepts and encompasses 4 systems, 214 math concepts, and 4011 math word problems.</p>
<p>(2) Problems in ConcepthMath are carefully annotated based on the mainstream education systems for English (EN) and Chinese (ZH).Details on the hierarchical system.Apart from Fig. 2, we also provide the details on the hierarchical system more clearly in Appendix A.</p>
<p>Length distribution.Fig. 3 shows the length distribution of our ConcepthMath, where number of tokens is reported .The minimum, average and maximum of the tokens for these questions are 4, 41 and 309, respectively, which shows that they have lexical richness.</p>
<p>Efficient Fine-Tuning</p>
<p>Based on our ConceptMath, we are able to identify the weaknesses in the mathematical reasoning capability of LLMs through concept-wise evaluation.In</p>
<p>We use the "cl100k base" tokenizer from https:// github.com/openai/tiktokenthis section, we explore a straightforward approach to enhance mathematical abilities towards specific concepts by first training a concept classifier and then curating a set of samples from a large opensourced math dataset.Specifically, first, by additionally collecting extra 10 problems per concept, we construct a classifier capable of identifying the concept class of a given question.The backbone of this classifier is a pretrained bilingual LLM, where the classification head is operated on its last hidden output feature.Then, we proceed to fine-tune LLMs using this specific dataset combined with the existing general math dataset, which aims to avoid overfitting on a relatively small dataset.More details have been provided in the Appendix B.</p>
<p>Experiments</p>
<p>In this section, we perform extensive experiments to demonstrate the effect of our ConceptMath.</p>
<p>Experimental Setup</p>
<p>Evaluated Models.We assess the mathematical reasoning of existing advanced LLMs on ConceptMath, including 2 close-sourced LLMs (i.e., GPT-3.5/GPT-4 (OpenAI, 2023)) and 17 open-sourced LLMs (i.e., WizardMath-13B (Luo et al., 2023), MetaMath-13B (Yu et al., 2023), MAmmoTH-13B (Yue et al., 2023), Qwen-14B/72B (Bai et al., 2023b), Baichuan2-13B (Baichuan, 2023), ChatGLM3-6B (Du et al., 2022), InternLM2-7B/20B (Team, 2023a), InternLM2-Math-7B/20B (Ying et al., 2024), LLaMA2-7B/13B/70B (Touvron et al., 2023b), Yi-6B/34B (Team, 2023b) and DeepSeekMath-7B (Shao et al., 2024)).Note that WizardMath-13B, MetaMath-13B, and MAmmoTH-13B are specialized math language models fine-tuned from LLaMA2.InternLM2-Math and DeepSeekMath-7B are specialized math language models finetuned from corresponding language models.More details of these evaluated models can be seen in Appendix C. Evaluation Settings.We employ three distinct evaluation settings: zero-shot, zero-shot with chainof-thought (CoT), and few-shot promptings.The zero-shot prompting assesses the models' intrinsic problem-solving abilities without any prior examples.The zero-shot with CoT prompting evaluates the models' ability to employ a logical chain of thought.In the few-shot prompting setting, the model is provided with fixed 5-shot prompts for dif- Table 2: Results of different models on our constructed ConceptMath benchmark dataset.Note that "ZS", "ZS-COT", "FS" represents "zero-shot", "zero-shot w/ chain-of-thought" and "few-shot", repsectively.Models are grouped roughly according to their model sizes.</p>
<p>ferent systems (See Appendix E), which includes five newly created examples with concise ground truth targets.This approach is designed to measure the in-context learning abilities.Besides, following MATH (Hendrycks et al., 2021b), all questions and answers in ConceptMath have been carefully curated, and each problem is evaluated based on exact matches.Moreover, greedy decoding with a temperature of 0 is used.</p>
<p>Results</p>
<p>Overall Accuracy We present the overall accuracies of different LLMs on our ConceptMath benchmark under various prompt settings in Table 2. Subsequently, we analyzed the mathematical abilities of these LLMs in both English and Chinese in Fig. 4. Our analysis led to the following key findings: (1) GPT-3.systems a lot, which indicates that domain-specific fine-tuning may degrade the generalization abilities of LLMs.(4).The mathematical models (i.e., InternLM2-Math-7B/20B and DeepSeekMath-7B) by continuing pretraining on the large-scale mathrelated dataset (¿=100B tokens) show sufficient improvements when compared to models with similar size, which indicates that large-scale pertaining is effective to improve the mathematical reasoning abilities.
L ] D U G 0 D W K % / / D 0 $ % 0 H W D 0 D W K % / / D 0 $ % / / D 0 $ % 0 $ P P R 7 + % &lt; L % &amp; K D W * / 0 % % D L F K X D Q % , Q W H U Q / 0 % 4 Z H Q % , Q W H U Q / 0 % &lt; L % , Q W H U Q / 0 0 D W K % ' H H S 6 H H N 0 D W K % , Q W H U Q / 0 0 D W K % * 3 7 4 Z H Q % * 3 7 0HDQ$FFXUDF\ (QJOLVK &amp;KLQHVH 2YHUDOO
Average Concept-wised Accuracy.In Fig. 5 and Fig. 6, to better analyze the effectiveness of our ConceptMath, we further provide the conceptwised accuracies average on evaluated models for different mathematical concepts by zero-shot prompting on Middle-EN and Middle-ZH.(See Appendix D for more results on Elementary-EN and Elementary-ZH).In Fig. 5 and Fig. 6, we observe that the accuracies across concepts vary a lot for existing LLMs.For example, for Middle-ZH in Fig. 6, around 18% of concepts exhibit an accuracy lower than 30%.Thus, to improve the mathematical abilities of LLMs, these concepts with large room for improvement should be given the highest priority, which further shows the advantage of ConceptMath.
&amp; LU FO H 5 DG LF DO H [ S UV ( [ S R Q HQ WV V FL HQ WL IL F Q R WD WL R Q 4 X DG UD Q WV * HR P HW UL F VH T X HQ FH V 3 UR E DE LO LW \ R I FR P S R X Q G H Y HQ WV ,Q G HS HQ G HQ W G HS HQ G HQ W HY HQ WV &amp; R X Q WL Q J S UL Q FL S OH 7 UD Q VI R UP DW LR Q V 5 DW LR Q DO L UU DW LR Q DO Q X P E HU V 6 \ VW HP V R I HT X DW LR Q V 3 UR E DE LO LW \ R I VL P S OH R S S R VL WH H Y HQ WV 6 FD OH G UD Z LQ J V $ E VR OX WH Y DO X H 3 UR E DE LO LW \ R I R Q H HY HQ W 2 Q H Y DU LD E OH V WD WL VW LF V 0 DN H S UH G LF WL R Q V ' R P DL Q U DQ J H R I IX Q FW LR Q V / LQ HD U IX Q FW LR Q V 7 Z R Y DU LD E OH V WD WL VW LF V ) LQ DQ FL DO O LW HU DF \ 0 HD Q P HG LD Q P R G H U DQ J H / LQ HD U HT X DW LR Q V $ UL WK P HW LF V HT X HQ FH V &amp; HQ WH U Y DU LD E LO LW \ 3 UL P H ID FW R UL ]D WL R Q 3 HU FH Q WV ' LY LG H ) UD FW LR Q V G HF LP DO V 6 X UI DF H DU HD Y R OX P H ' LV WD Q FH E HW Z HH Q W Z R S R LQ WV 9 DU LD E OH H [ S UV 6 T X DU H UR R WV F X E H UR R WV &amp; R Q J UX HQ FH V LP LO DU LW \ 1 R Q OL Q HD U IX Q FW LR Q V ' HF LP DO V 3 HU LP HW HU D UH D 7 UL DQ J OH 0 X OW LS O\ $ G G V X E WU DF W $ [ HV 3 R O\ J R Q V ) DF WR UV 7 UD S H] R LG V ,Q WH US UH W IX Q FW LR Q V ( T X LY DO HQ W H[ S UV / LQ HV D Q J OH V 3 UR S R UW LR Q DO U HO DW LR Q VK LS V 6 OR S H 2 S S R VL WH L Q WH J HU V ,Q HT X DO LW LH V &amp; R
Concept-wised Accuracy.Fig. 7 and Fig. 8 show that most existing LLMs, whether opensourced, closed-sourced, general-purpose, or mathspecialized, exhibit notable differences in their concept accuracies in the zero-shot prompt setting.These disparities may stem from variations in training datasets, strategies, and model sizes, which suggests that apart from common weaknesses, each model possesses its unique areas of deficiency or shortcomings.For the sake of brevity in the presentation, we only show a subset of models on
$ G G V X E WU DF W $ UL WK P HW LF V HT X HQ FH V &amp; R Q J UX HQ FH V LP LO DU LW \ &amp; R Q VX P HU P DW K &amp; R X Q WL Q J S UL Q FL S OH ' HF LP DO V ' LV WD Q FH E HW Z HH Q W Z R S R LQ WV ' LY LG H ' R P DL Q U DQ J H R I IX Q FW LR Q V ( T X[ S R Q HQ WV V FL HQ WL IL F Q R WD WL R Q ) LQ DQ FL DO O LW HU DF \ ) UD FW LR Q V G HF LP DO V * HR P HW UL F VH T X HQ FH V ,Q WH US UH W IX Q FW LR Q V / LQ HD U HT X DW LR Q V / LQ HD U IX Q FW LR Q V / LQ HV D Q J OH V 0 DN H S UH G LF WL R Q V 0 X OW LS O\ 1 R Q OL Q HD U IX Q FW LR Q V 2 Q H Y DU LD E OH V WD WL VW LF V 3 HU FH Q WV 3 HU LP HW HU D UH D 3 UL P H ID FW R UL ]D WL R Q 3 UL P H R U FR P S R VL WH 3 UR E DE LO LW \ R I FR P S R X Q G H Y HQ WV 3 UR E DE LO LW \ R I R Q H HY HQ W 3 UR E DE LO LW \ R I VL P S OH R S S R VL WH H Y HQ WV 3 UR S R UW LR Q DO U HO DW LR Q VK LS V 4 X DG UD Q WV 5 DW LR Q DO L UU DW LR Q DO Q X P E HU V 6 FD OH G UD Z LQ J V 6 OR S H 6 T X DU H UR R WV F X E H UR R WV 6 X UI DF H DU HD Y R OX P H 6 \ VW HP V R I HT X DW LR Q V 7 UL DQ J OH 7 Z R Y DU LD E OH V WD WL VW LF V $ E VR OX WH Y DO X H $ [ HV &amp; HQ WH U Y DU LD E LO LW \ &amp; LU FO H ) DF WR UV ,Q G HS HQ G HQ W G HS HQ G HQ W HY HQ WV ,Q HT X DO LW LH V 0 HD Q P HG LD Q P R G H U DQ J H 2 S S R VL WH L Q WH J HU V 2 X WO LH U 3 R O\ J R Q V 3 R O\ K HG UD 5 DG LF DO H [ S UV 7 UD Q VI R UP DW LR Q V 6 T X DU H 7 UD S H] R LG V 9 DU LD E OH H [ S UV
$FFXUDF\ 0HWD0DWK% //D0$% *37 Middle-EN and Middle-ZH.The concept accuracies of Elementary-EN and Elementary-ZH systems and all results of all models can be found in Appendix D.</p>
<p>Analysis</p>
<p>Contamination.To determine whether a text is in the pretraining data of a LLM, we provide two different contamination detection methods (i.e., Rouge-based and Prob-based methods) to analyze our ConceptMath in Table 3. Specifically, for the Rouge-based method, we just input the first 50% of the question as the input and compute the Rouge-L score between the generation results and the ground-truth label of the last 50% of the text, where a lower Rouge-L score means a lower contamination rate.For the Prob-based method, we follow (Shi et al., 2023) to use the MIN-K% probability metric, which first gets the probability for each token in the test, and selects the K% tokens with minimum probabilities and calculates their average log-likelihood.If the average log-likelihood is high, the text is likely in the pretraining data.Note that we choose K as 10 in our setting.In Table 3, we observe that the contaminate rates on our ConceptMath are very low, which means that our  ConceptMath can provide a reasonable evaluation for existing LLMs.</p>
<p>Unmastered Concepts.We also highlight the several unmastered concepts of the LLaMA2-13B in Table 4, which shows ConceptMath is effective in guiding further refinement of existing LLMs.</p>
<p>Evaluation Prompting.Different from the fewshot or cot prompting evaluation that can boost closed-source models, we find that zero-shot prompting is more effective for certain open-source LLMs in Table 2.This disparity may arise either because the models are not sufficiently powerful to own mathematical CoT capabilities (Yu et al., 2023;Wei et al., 2022) or because these models have already incorporated CoT data during training (Longpre et al., 2023).Consequently, to ensure a comprehensive analysis, we have employed all three prompting methods for evaluation.</p>
<p>Efficient Fine-tuning.To show the effect of efficient fine-tuning, we take the LLaMA2-13B as an example in Table 5.Specifically, for LLaMA2-13B, we first select 10 concepts with the lowest accuracies in Elementary-EN.Then, we crawl 495 samples (about 50 samples per concept) using the trained classifier as the Concept-Specific (CS) training data (See Appendix B for more details).Meanwhile, to avoid overfitting, we introduce the Meta-MathQA (MMQA (Yu et al., 2023) ) data to preserve general mathematical abilities.After that, we can fine-tune LLaMA2-13B by only using MMQA (i.e., LLaMA2 (w/ MMQA)), or using both MMQA and CS data (i.e., LLaMA2 (w/ MMQA &amp; CS)).In Table 5, we observe that LLaMA2 (w/ MMQA &amp; CS) archives significant improvements on the lowest 10 concepts and preserves well on the other 33 concepts, which shows the effect of efficient finetuning and the advantages of our ConceptMath.</p>
<p>Related Work</p>
<p>Large Language Models for Mathematics.</p>
<p>Large Language Models (LLMs) such as GPT-3.5 and GPT-4 have exhibited promising capabilities in complex mathematical tasks.However, the proficiency of open-source alternatives like LLaMA (Touvron et al., 2023a) and LLaMA2 (Touvron et al., 2023b) remains notably inferior on these datasets, particularly in handling non-English problems.In contrast, models like Baichuan2 (Baichuan, 2023) and Qwen (Bai et al., 2023b) pretrained on multilingual datasets (i.e., Chinese and English) have achieved remarkable performance.Recently, many domain-specialized math language models have been proposed.For example, MetaMath (Yu et al., 2023) leverages the LLaMA2 models and finetunes on the constructed MetaMathQA dataset.MAmmoTH (Yue et al., 2023) synergizes Chain-of-Thought (CoT) and Program-of-Thought (PoT) rationales.</p>
<p>Mathmatical Reasoning Benchmarks.Recently, many mathematical datasets (Roy and Roth, 2015;Koncel-Kedziorski et al., 2015;Lu et al., 2023;Huang et al., 2016;Miao et al., 2020;Patel et al., 2021) have been proposed.For example, SingleOp (Roy et al., 2015), expands the scope to include more complex operations like multiplication and division.Math23k (Wang  Fine-Grained Benchmarks.Traditional benchmarks focus on assessing certain abilities of models on one task (Guo et al., 2023b;Wang et al., 2023a;Liu et al., 2020;Guo et al., 2022;Chai et al., 2024;Liu et al., 2024;Guo et al., 2024Guo et al., , 2023c;;Bai et al., 2023a;Liu et al., 2022;Guo et al., 2023a;Bai et al., 2024;Liu et al., 2021) (e.g., reading comprehension (Rajpurkar et al., 2018), machine translation (Bojar et al., 2014), and summarization (Narayan et al., 2018)).For example, the GLUE benchmark (Wang et al., 2019) combines a collection of tasks, and has witnessed superhuman model performance for pretraining models (Kenton and Toutanova, 2019;Radford et al., 2019) (Hendrycks et al., 2021a) introduced MMLU, a benchmark with multiple-choice questions across 57 subjects including STEM, humanities, and social sciences, for assessing performance and identifying weaknesses.(et al., 2022) proposed BIGbench with over 200 tasks.To enhance the mathematical capabilities of LLMs, we introduce a comprehensive mathematical reasoning Concept-Math dataset designed to assess model performance across over 200 diverse mathematical concepts in both Chinese and English.</p>
<p>Conclusion</p>
<p>We introduce a new bilingual concept-wise math reasoning dataset called ConceptMath to assess models across a diverse set of concepts.First, Con-ceptMath covers more than 200 concepts across elementary and middle schools for mainstream English and Chinese systems.Second, we extensively evaluate existing LLMs by three prompting methods, which can guide further improvements for these LLMs on mathematical abilities.Third, we analyze the contamination rates, error cases and provide a simple and efficient fine-tuning strategy to enhance the weaknesses.</p>
<p>Limitations.Human efforts are required to carefully design the hierarchical systems of mathematical concepts.In the future, we have three plans as follows: (1) Extend the input modality to multi-modalities.</p>
<p>(2) Extend the education systems to high school and college levels.</p>
<p>(3) Extend the reasoning abilities to more STEM fields.</p>
<p>A Details on the ConceptMath</p>
<p>As shown in Table 7, Table 8, Table 17 and Table 9, we have provided the details on the three-level hierarchical system of our ConceptMath for better illustration.</p>
<p>B Details on the Efficient Fine-Tuning</p>
<p>In this section, we provide the details on the efficient fine-tuning to enhance mathematical reasoning abilities towards specific concepts by first training a concept classifier and then curating a set of samples from a large open-sourced math dataset.Specifically, first, by additionally collecting extra 10 problems per concept, we construct a classifier capable of identifying the concept class of a given question.The backbone of this classifier is a pretrained bilingual LLM (i.e., Baichuan2-13B), where the classification head is operated on its last hidden output feature.Note that the concept classification accuracies in English and Chinese are 92.5 and 86.9, respectively, which indicates that it is reasonable to use an additional classifier for curating an extra concept-related dataset from large-scale math-related data.Note that in our work, we crawl from the OpenWebMath (Paster et al., 2023) to produce the concept-related training dataset.</p>
<p>C Details on the Evaluated Models</p>
<p>In this section, we offer a detailed overview of the Large Language Models (LLMs) and present the corresponding model links in Table 6.</p>
<p>• GPT-3.5/GPT-4(OpenAI, 2023): The most powerful closed-model from OpenAI.We utilize its API: gpt-3.5-turboand gpt-4.</p>
<p>• LLaMa2-7B/13B/70B (Touvron et al., 2023b):</p>
<p>A set of open-source models developed by Meta.</p>
<p>• Qwen-14B/72B (Bai et al., 2023b): This model pre-trained on multilingual data, concentrates on Chinese and English languages.We employ both the Qwen-Base-14B, and the Qwen-Base-72B.</p>
<p>• Baichuan2-13B (Baichuan, 2023): This model demonstrates impressive performance in both Chinese and English benchmarks.</p>
<p>• MetaMath-13B (Megill and Wheeler, 2019): A domain-specific language model for mathematical reasoning, fine-tuned from the LLaMA-2 model using the MetaMathQA dataset.</p>
<p>• WizardMath-13B (Luo et al., 2023): Another domain-specific language model for mathematical reasoning, fine-tuned from the LLaMA-2 model using reinforcement learning.</p>
<p>• MAmmoTH-13B (Yue et al., 2023): This model is specifically designed for general math problem-solving and has been fine-tuned from the LLaMA model using the MathInstruct dataset.This dataset features training data that includes both chain-of-thought (CoT) and program-of-thought (PoT) rationales.</p>
<p>• Yi-6B/34B (Team, 2023b): This model released by 01 shows promising performance results in both Chinese and English.</p>
<p>• ChatGLM3-6B (Zeng et al., 2022): a lightweight and high-performance pre-trained dialogue model released by Zhipu AI in both Chinese and English.</p>
<p>• InternLM-7B/20B (Team, 2023a): A Multilingual Language Model with Progressively Enhanced Capabilities released by InternLM team.</p>
<p>• InternLM-Math-7B/20B (Ying et al., 2024): Well-performed math reasoning language models.</p>
<p>• DeepSeekMath-7B (Shao et al., 2024): One powerful mathematical language model released by DeepSeek.</p>
<p>D More Results</p>
<p>As shown in Fig. 9 and Fig. 10, we provide the mean concept accuracies of Elementary-EN and Elementary-ZH of the evaluated models across different concepts.</p>
<p>As shown in Fig. 11 and Fig. 12, we illustrate the concept accuracies on Elementary-EN and Elementary-ZH for different selected models.For the results of all models, please refer to Fig. 13,Fig. 14,Fig. 15 and Fig. 16.</p>
<p>E Details on 5-shot Prompts</p>
<p>We provide the 5-shot prompts for our Concept-Math in Pages 17-20.
$ G G V X E WU DF W $ UL WK P HW LF V HT X HQ FH V &amp; R Q J UX HQ FH V LP LO DU LW \ &amp; R Q VX P HU P DW K &amp; R X Q WL Q J S UL Q FL S OH ' HF LP DO V ' LV WD Q FH E HW Z HH Q W Z R S R LQ WV ' LY LG H ' R P DL Q U DQ J H R I IX Q FW LR Q V ( T X[ S R Q HQ WV V FL HQ WL IL F Q R WD WL R Q ) LQ DQ FL DO O LW HU DF \ ) UD FW LR Q V G HF LP DO V * HR P HW UL F VH T X HQ FH V ,Q WH US UH W IX Q FW LR Q V / LQ HD U HT X DW LR Q V / LQ HD U IX Q FW LR Q V / LQ HV D Q J OH V 0 DN H S UH G LF WL R Q V 0 X OW LS O\ 1 R Q OL Q HD U IX Q FW LR Q V 2 Q H Y DU LD E OH V WD WL VW LF V 3 HU FH Q WV 3 HU LP HW HU D UH D 3 UL P H ID FW R UL ]D WL R Q 3 UL P H R U FR P S R VL WH 3 UR E DE LO LW \ R I FR P S R X Q G H Y HQ WV 3 UR E DE LO LW \ R I R Q H HY HQ W 3 UR E DE LO LW \ R I VL P S OH R S S R VL WH H Y HQ WV 3 UR S R UW LR Q DO U HO DW LR Q VK LS V 4 X DG UD Q WV 5 DW LR Q DO L UU DW LR Q DO Q X P E HU V 6 FD OH G UD Z LQ J V 6 OR S H 6 T X DU H UR R WV F X E H UR R WV 6 X UI DF H DU HD Y R OX P H 6 \ VW HP V R I HT X DW LR Q V 7 UL DQ J OH 7 Z R Y DU LD E OH V WD WL VW LF V $ E VR OX WH Y DO X H $ [ HV &amp; HQ WH U Y DU LD E LO LW \ &amp; LU FO H ) DF WR UV ,Q G HS HQ G HQ W G HS HQ G HQ W HY HQ WV ,Q HT X DO LW LH V 0 HD Q P HG LD Q P R G H U DQ J H 2 S S R VL WH L Q WH J HU V 2 X WO LH U 3 R O\ J R Q V 3 R O\ K HG UD 5 DG LF DO H [ S UV 7 UD Q VI R UP DW LR Q V 6 T X DU H 7 UD S H] R LG V 9 DU LD E OH H [ S UV $FFXUDF\ &lt;L% &amp;KDW*/0% //D0$% 'HHS6HHN0DWK% $ G G V X E WU DF W $ UL WK P HW LF V HT X HQ FH V &amp; R Q J UX HQ FH V LP LO DU LW \ &amp; R Q VX P HU P DW K &amp; R X Q WL Q J S UL Q FL S OH ' HF LP DO V ' LV WD Q FH E HW Z HH Q W Z R S R LQ WV ' LY LG H ' R P DL Q U DQ J H R I IX Q FW LR Q V ( T X[ S R Q HQ WV V FL HQ WL IL F Q R WD WL R Q ) LQ DQ FL DO O LW HU DF \ ) UD FW LR Q V G HF LP DO V * HR P HW UL F VH T X HQ FH V ,Q WH US UH W IX Q FW LR Q V / LQ HD U HT X DW LR Q V / LQ HD U IX Q FW LR Q V / LQ HV D Q J OH V 0 DN H S UH G LF WL R Q V 0 X OW LS O\ 1 R Q OL Q HD U IX Q FW LR Q V 2 Q H Y DU LD E OH V WD WL VW LF V 3 HU FH Q WV 3 HU LP HW HU D UH D 3 UL P H ID FW R UL ]D WL R Q 3 UL P H R U FR P S R VL WH 3 UR E DE LO LW \ R I FR P S R X Q G H Y HQ WV 3 UR E DE LO LW \ R I R Q H HY HQ W 3 UR E DE LO LW \ R I VL P S OH R S S R VL WH H Y HQ WV 3 UR S R UW LR Q DO U HO DW LR Q VK LS V 4 X DG UD Q WV 5 DW LR Q DO L UU DW LR Q DO Q X P E HU V 6 FD OH G UD Z LQ J V 6 OR S H 6 T X DU H UR R WV F X E H UR R WV 6 X UI DF H DU HD Y R OX P H 6 \ VW HP V R I HT X DW LR Q V 7 UL DQ J OH 7 Z R Y DU LD E OH V WD WL VW LF V $ E VR OX WH Y DO X H $ [ HV &amp; HQ WH U Y DU LD E LO LW \ &amp; LU FO H ) DF WR UV ,Q G HS HQ G HQ W G HS HQ G HQ W HY HQ WV ,Q HT X DO LW LH V 0 HD Q P HG LD Q P R G H U DQ J H 2 S S R VL WH L Q WH J HU V 2 X WO LH U 3 R O\ J R Q V 3 R O\ K HG UD 5 DG LF DO H [ S UV 7 UD Q VI R UP DW LR Q V 6 T X DU H 7 UD S H] R LG V 9 DU LD E OH H [ S UV
Figure 2 :
2
Figure 2: Diagram overview of four concept systems in ConceptMath.We have provided translated Chinese concept names in English (See Appendix A).</p>
<p>Figure 4 :
4
Figure 4: Mean accuracies for English, Chinese, and overall educational systems.</p>
<p>Figure 6 :
6
Figure 5: Mean concept accuracies on Middle-EN.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: Concept accuracies on Middle-EN.</p>
<p>Figure 13 :
13
Figure 13: Concept accuracies on Elementary-EN of more models.</p>
<p>Table 1 :
1
A comparison of our ConceptMath with some notable mathematical datasets.Note that the size is the number of samples of the test split.
BenchmarkLanguageFine-grainedSizeGSM8KEN✗1319MATHEN✗5000TabMWPEN✗7686Dolphin18KEN✗1504Math23KZH✗1000ASDivEN✗2305SVAMPEN✗300SingleOpEN✗159MMLU-MathEN✗906ConceptMathEN&amp;ZH✓4011
! 4XHVWLRQ/HQJWK7RNHQV 1XPEHURI4XHVWLRQV Figure 3: Length distributions of our ConceptMath.</p>
<p>Table 3 :
3
Data contamination rate of LLMs.We provide two different contamination detection methods.The values in the table represent "Rouge / Prob".Note that the second method based on output probability distributions can only be applied to the open-source models.</p>
<p>Table 4 :
4
Error reasons of several unmastered concepts of LLaMA2-13B.
ModelsLLaMA2LLaMA2 (w/ MMQA)LLaMA2 (w/ MMQA &amp;CS)Cones0.0017.6523.53Spheres5.8829.4135.29Polygons11.1161.1166.67Rational Number11.7623.5352.94Cylinders11.7635.2947.06Angles11.7647.0658.82Probability18.7525.0075.00Perimeter26.3242.1163.16Volume27.7838.8966.67Proportional27.7833.3344.44Avg Acc. (over 10 concepts)15.2936.8853.36Avg Acc. (over 33 concepts)51.9458.1460.67Overall Acc.44.0253.9459.29</p>
<p>Table 5
5: Results of fine-tuning models. "MMQA" and "CS" denote MetaMathQA and our constructed Concept-Specific training datasets, respectively. Introducing CS data specifically for the bottom 10 concepts significantly enhances these concepts' performance, while slightly improving the performance across the remaining 33 concepts.et al., 2017) gathers 23,161 problems labeled withstructured equations and corresponding answers.GSM8K (Cobbe et al., 2021) is a widely useddataset, which requires a sequence of elementarycalculations with basic arithmetic operations.</p>
<p>Table 8 :
8
Details of the hierarchical concepts in Middle-EN.
,QWHUQ/00DWK%,QWHUQ/0%0$PPR7+%:L]DUG0DWK%$FFXUDF\</p>
<p>Table 9 :
9
Details of the hierarchical concepts in Elementary-ZH.一元一次不等式的应用 (Applications of Unary First Order Inequality) 20 一元一次不等式组的应用(Applications of Unary First Order Groups of Inequalities) 20 解一元一次不等式(Solve the First Inequality of One Variable) 20 解一元一次不等式组(Solve Unary First Order Groups of Inequalities) 20
LEVEL1LEVEL2LEVEL3# Samples三角形(Triangle)全等三角形(Congruent Triangle) 勾股定理(Pythagorean Theorem) 等腰三角形(Isosceles Triangle) 等边三角形(Equilateral Triangle)20 20 20 20几何 (Geometry)四边形(Quadrilateral) 圆(Circle)平行四边形(Parallelogram) 梯形(Trapezium) 圆周角(Angle of Circumference) 圆心角(Angle of Center) 垂径定理(Vertical Path Theorem) 弧长和扇形面积(Arc length &amp; Sector Area) 正多边形和圆(Regular Polygons &amp; Circles) 点线圆位置关系(Relations of Point, Line &amp; Circle)20 20 20 20 20 20 20 20立体图形 (Three-dimensional Shapes)圆锥(Cone)20函数与一元一次方程 (Univariate Function &amp; Equation)20一次函数(Linear Function)函数与一元一次不等式 (Linear Functions &amp; Univariate Linear Inequalities)20函数 (Function)一次函数与二元一次方程组 (Linear Functions &amp; System of Binary Linear Equations) 正比例函数(Proportional Function) 一次函数解析式 (Analytical Formula of Linear Functions )20 20 20二次函数(Quadratic Function)二次函数的应用 (Applications of Quadratic Functions)20抛物线的性质 (Properties of Parabolas)18反比例函数 (Inverse Proportional Function)定义(Definition) 应用(Applications) 性质(Properties)20 20 19平面直角坐标系 (Rectangular Coordinate System)有序数对(Ordered Pair) 象限中的点(Points of Quadrant)20 14代数式(Algebra Expression)代数式求值(Algebraic Expression Evaluation) 同类项(Similar Items)20 20分式(Fraction)指数幂(Exponential Power) 约分(Fraction Reduction)20 19数与式 (Number and Expression)因式(Factor) 应用(Application)十字相乘法(Cross Multiplication) 公因式提取(Common Factor Extraction) 流水问题(Flow Problem) 鸽巢问题(Pigeon Nest Problem)20 18 20 20整式(Integral Expression)乘法公式(Multiplication) 整式的乘除及混合(Multiplication, Division &amp; Mixing) 整式的加减(Addition &amp; Subtraction)20 20 20无理数(Irrational Number)无理数识别(Irrational Number Recognition)20根式(Radical Expression)二次根式的运算(Operation of Quadratic Radicals) 同类二次根式(Similar Quadratic Radicals) 平方根与算术平方根(Square Root &amp; Arithmetic Square Root) 立方根(Cube Root)20 20 20 20一元一次方程 (Linear Equation in One Variable)一元一次方程的应用(Applications) 解一元一次方程(Solutions)20 20方程与不等式 (Equations &amp; Inequalities)一元二次方程 (Quadratic Equation in One Variable) 不等式与不等式组 (Inequalities &amp; Groups of Inequalities)一元二次方程的应用(Applications) 解一元二次方程(Solutions)20 20分式方程(Fractional Equation)分式方程的应用(Application of Fractional Equation) 解分式方程(Solve Fractional Equation)20 20统计与概率 (Statistics and Probability)数据分析(Data Analysis) 概率(Probability)数据的波动趋势(Fluctuating Trend of Data) 数据的集中趋势(Central Tendency of Data) 概率的应用(Applications of Probability) 求概率(Find Probability) 随机事件与概率(Random Events &amp; Probabilities)20 20 20 20 20
24Figure17: Details of the hierarchical concepts in Elementary-ZH.</p>
<p>Table 9 :
9
Details of the hierarchical concepts in Middle-ZH.</p>
<p>https://huggingface.co/datasets/ meta-math/MetaMathQA https://huggingface.co/datasets/ TIGER-Lab/MathInstruct
0HDQ$FFXUDF\Figure10: Mean concept accuracies of Elementary-ZH.$FFXUDF\ 0HWD0DWK% //D0$% *37Figure12: Concept accuracies on Elementary-ZH.Elementary-EN (5-shot prompts)Q: For a party, Oscar gets 4 different kinds of soda.He buys a 6-pack of each kind.At home, he divides the sodas evenly among 3 coolers.How many sodas are in each cooler?A: Oscar buys 4 different kinds of soda, and he gets a 6-pack of each kind.This means that he has 4 * 6 = 24.sodas in total.He divides these 24 sodas evenly among 3 coolers.So number of sodas per cooler is 24 / 3 = 8.Therefore, the answer is 8. Q: Amazing Animals Zoo is building a new home for its 8 zebras.Each zebra will get 2 acres of grassy land to roam around in.The zebras will have a pond that covers 1 acre.How big will the zebras' new home be?A: The total required grassland area for the zebras is: 8 zebras * 2 acres per zebra = 16 acres.Then, we need to add the extra 1 acre for the pond: 16 acres (for the zebras) + 1 acre (for the pond) = 17 acres.Therefore, the answer is 17.Q: The state taxes personal income exceeding 3500 yuan (this portion not to exceed 3500 yuan) at a rate of 3%.If Xiaoming's father's monthly salary is 4700 yuan, how much personal income tax should Xiaoming's father pay each month?A: To calculate the amount of personal income tax Xiaoming's father should pay, we can follow this simple process:Subtract the tax-free threshold from the monthly salary.Taxable income = Monthly salary -Tax-free threshold = 4700 -3500 = 1200 yuan.Apply the tax rate to the amount that exceeds the threshold.Tax owed = Taxable income * Tax rate = 1200 * 3% = 36 yuan.Therefore, the answer is 36.Q: Find the 10th term in the arithmetic sequence where the first term is 3 and the common difference is 4. A: To find the nth term in an arithmetic sequence, we can use the formula:where: a n is the nth term, a 1 is the first term, d is the common difference, n is the term number.Given that the first term (a 1 ) is 3 and the common difference (d) is 4, we can find the 10th term (a 10 ) by plugging these values into the formula: a 10 = a 1 + (10 − 1) * d = 3 + 9 * 4 = 3 + 36 = 39.Therefore, the answer is 39.Q: A regular polygon has an interior angle of 156 degrees.How many sides does the polygon have?A: To find the number of sides of a regular polygon with a given interior angle, we can use the formula: Interior Angle = [(n -2) * 180] / n, where n is the number of sides.Given that interior angle is 156: 156 = [(n -2) * 180] / n.Solve for n, so we get n = 15.Therefore, the answer is 15.Middle-EN (5-shot prompts) Therefore, the answer is -794.Q: What is the slope of the line segment between the two points: (3,1), (5,2)?A: The slope of a line between two points (x 1 , y 1 ) and (x 2 , y 2 ) can be found using the formula: slope(m) = (y 2 − y 1 )/(x 2 − x 1 ).Given the two points (3, 1) and (5, 2), we can substitute these values into the formula: slope(m) = (2 − 1)/(5 − 3) = 1/2 = 0.5.Therefore, the answer is 1/2.Q: In a party, there are 5 cupcakes remaining and 10 children.A mother inserts 10 pieces of paper into a hat, 5 with a cupcake image and 5 without.The children who draw the cupcake paper will receive a cupcake.If the first child draws a paper with a cupcake image, what is the probability that the second child will also draw a cupcake paper?A: Initially, there are 5 cupcake papers and 5 non-cupcake papers in the hat, making a total of 10 papers.After the first child draws a cupcake paper, there are 4 cupcake papers and 5 non-cupcake papers left in the hat, with a total of 9 papers.The probability that the second child will draw a cupcake paper is then the number of cupcake papers left divided by the total number of papers left: Probability = Number of cupcake papers left / Total number of papers left = 4/9.Therefore, the answer is 4/9.Q: What is the total area of an irregular polygon that consists of two squares and two rectangles where the sides of the squares are 12 cm and 16 cm respectively, and the dimensions of the rectangles are 13 cm by 20 cm and 10 cm by 7 cm respectively?A: To find the total area of an irregular polygon consisting of two squares and two rectangles, you would sum up the individual areas of each shape.The area of a square is given by the formula A = s 2 , where s is the length of a side of the square.For the first square with a side of 12 cm: A 1 = 12 2 = 144cm 2 .For the second square with a side of 16 cm: A 2 = 16 2 = 256cm 2 .The area of a rectangle is given by the formula ( A = l * w ), where l is the length and w is the width of the rectangle.For the first rectangle with dimensions 13 cm by 20 cm: A 3 = 13 * 20 = 260cm 2 For the second rectangle with dimensions 10 cm by 7 cm:Therefore, the answer is 730.Q: Susie is making invitations to her party this coming weekend.It takes her about 5 minutes to make each invitation.It took her 2 hours to finish all the invitations.How many people is Susie inviting?A: First, convert the 2 hours into minutes to match the unit of time used for the invitation-making process. 2 hours = 120 minutes.Then, divide the total time spent by the time it takes to make one invitation to find out how many invitations she made: 120 minutes total / 5 minutes per invitation = 24 invitations Therefore, the answer is 24.Elementary-ZH (5-shot prompts)
. Wizardmath, 13B 41.73 44.78 34.99 36.85 37.72 45.11 10.51 11.26 18.70 12.36 15.52 22.92 27.70</p>
<p>. -13b Metamath, </p>
<p>References Anthropic. 2023. Model card and evaluations for claude models. </p>
<p>Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang, arXiv2024</p>
<p>Griprank: Bridging the gap between retrieval and generation via the generative knowledge improved passage ranking. Jiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang, Xinnian Liang, Zhao Yan, Zhoujun Li, 2023aCIKM</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv:2309.16609Qwen technical report. Xiaohuan Zhou, and Tianhang Zhu2023barXiv preprint</p>
<p>Baichuan, arXiv:2309.10305Open large-scale language models. 20232arXiv preprint</p>
<p>Findings of the 2014 workshop on statistical machine translation. Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Aleš Tamchyna, 10.3115/v1/W14-3302Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAAssociation for Computational Linguistics2014</p>
<p>Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, arXiv:2401.07037xcot: Cross-lingual instruction tuning for crosslingual chain-of-thought reasoning. 2024arXiv preprint</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, arXiv: Arxiv-2206.046152022arXiv preprint</p>
<p>Development of mathematical concepts as basis for an elaborated mathematical understanding. Annemarie Fritz, Antje Ehlert, Lars Balzer, South African Journal of Childhood Education. 20133</p>
<p>Lvp-m3: language-aware visual prompt for multilingual multimodal machine translation. Hongcheng Guo, Jiaheng Liu, Haoyang Huang, Jian Yang, Zhoujun Li, Dongdong Zhang, Zheng Cui, Furu Wei, 2022EMNLP</p>
<p>M2c: Towards automatic multimodal manga complement. Hongcheng Guo, Boyang Wang, Jiaqi Bai, Jiaheng Liu, Jian Yang, Zhoujun Li, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023a</p>
<p>Logformer: A pretrain and tuning pipeline for log anomaly detection. Hongcheng Guo, Jian Yang, Jiaheng Liu, Jiaqi Bai, Boyang Wang, Zhoujun Li, Tieqiao Zheng, Bo Zhang, Qi Tian, 2024AAAI</p>
<p>Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, arXiv:2309.09298Owl: A large language model for it operations. 2023barXiv preprint</p>
<p>Adaptive contrastive knowledge distillation for bert compression. Jinyang Guo, Jiaheng Liu, Zining Wang, Yuqing Ma, Ruihao Gong, Ke Xu, Xianglong Liu, Findings of the Association for Computational Linguistics: ACL 2023. 2023c</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021b</p>
<p>How well do computers solve math word problems? large-scale dataset construction and evaluation. Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, Wei-Ying Ma, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics20161</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , Transactions of the Association for Computational Linguistics. 32015</p>
<p>Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, arXiv:2401.06951E2-llm: Efficient and extreme length extension of large language models. 2024arXiv preprint</p>
<p>Dam: discrepancy alignment metric for face recognition. Jiaheng Liu, Yudong Wu, Yichao Wu, Chuming Li, Xiaolin Hu, Ding Liang, Mengyu Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Cross-lingual cross-modal consolidation for effective multilingual video corpus moment retrieval. Jiaheng Liu, Tan Yu, Hanyu Peng, Mingming Sun, Ping Li, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>Block proposal neural architecture search. Jiaheng Liu, Shunfeng Zhou, Yichao Wu, Ken Chen, Wanli Ouyang, Dong Xu, IEEE Transactions on Image Processing. 302020</p>
<p>The flan collection: designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, Norman Megill, David A Wheeler, arXiv:2308.09583Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Lulu Com Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, the 58th Annual Meeting of the Association for Computational Linguistics2023. 2019. 2020arXiv preprintA diverse corpus for evaluating and developing english math word problem solvers</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, 10.18653/v1/D18-1206Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Gpt-4 technical report. PREPRINT. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2023. Openwebmath: An open dataset of high-quality mathematical web text. 2023OpenAI</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Know what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsShort Papers20182</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Reasoning about quantities in natural language. Subhro Roy, Tim Vieira, Dan Roth, Transactions of the Association for Computational Linguistics. 32015</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024</p>
<p>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, arXiv:2310.16789Detecting pretraining data from large language models. 2023arXiv preprint</p>
<p>Studying mathematics conceptual learning: Student learning through their mathematical activity. Simon Martin, 2011North American Chapter of the International Group for the Psychology of Mathematics Education</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. Internlm Team, 2023a</p>
<p>Building the next generation of open-source and bilingual llms. Yi 2023b Team, Yi, </p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, International Conference on Learning Representations. 2019</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, Proceedings of the 2017 conference on empirical methods in natural language processing. the 2017 conference on empirical methods in natural language processing2017</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, arXiv:2310.00746Jie Fu, and Junran Peng. 2023a. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint</p>
<p>Zengzhi Wang, Rui Xia, Liu Pengfei, arXiv:2312.17120Generative ai for math: Part i -mathpile: A billion-tokenscale pretraining corpus for math. 2023barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Internlmmath: Open math large language models toward verifiable reasoning. Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin, 2024</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprint</p>
<p>Models HuggingFace Link / OpenAI Model ChatGLM3 ChatGLM3-6B. </p>
<p>DeepSeekMath DeepSeekMath-7B. </p>
<p>WizardMath WizardMath-13B. </p>
<p>InternLM InternLM-7B. </p>
<p>. Yi Yi, -6b , </p>
<p>Qwen Qwen, -14b , Chat GPT GPT-3.5 gpt-3.5-turbo GPT-4 gpt-4 Table 6: Model links. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2470 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2470</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2470</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-265698550</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.03497v1.pdf" target="_blank">Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research</a></p>
                <p><strong>Paper Abstract:</strong> This paper engages in a speculative exploration of the concept of an artificial agent capable of conducting research. Initially, it examines how the act of research can be conceptually characterized, aiming to provide a starting point for discussions about what it means to create such agents. The focus then shifts to the core components of research: question formulation, hypothesis generation, and hypothesis verification. This discussion includes a consideration of the potential and challenges associated with enabling machines to autonomously perform these tasks. Subsequently, this paper briefly considers the overlapping themes and interconnections that underlie them. Finally, the paper presents preliminary thoughts on prototyping as an initial step towards uncovering the challenges involved in developing these research-capable agents.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2470.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2470.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DENDRAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dendral (first expert system for scientific hypothesis formation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early expert-system AI for hypothesis formation in scientific domains (notably chemistry), historically cited as one of the first automated hypothesis-generation systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dendral: a case study of the first expert system for scientific hypothesis formation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Dendral</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Rule-based expert system that encodes domain knowledge and heuristics to generate candidate scientific hypotheses (e.g., molecular structure hypotheses) from observed data; operates by symbolic inference and search guided by domain rules.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Hypothesis Generation System / Expert System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry (molecular structure inference) and general scientific hypothesis formation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate plausible candidate hypotheses (e.g., molecular structures) consistent with observed experimental data such as mass spectra; reduce search space via domain heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Combinatorial search over candidate structures; high branching factor but constrained by encoded domain rules; complexity dominated by combinatorial hypothesis space and symbolic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on pre-existing domain knowledge and observed experimental data (spectra); data availability moderate (requires domain-specific observations and encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Relatively low by modern ML standards; symbolic search and rule evaluation; compute requirements depend on size of hypothesis space and pruning heuristics (not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined, discrete, symbolic, constrained search problem with clear evaluation criteria (matching predicted vs observed spectra); deterministic in inference given rules.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Quality/consistency of generated hypotheses with experimental observations; ability to reduce candidate set to correct structures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited by completeness and correctness of encoded domain rules; brittle to out-of-domain phenomena; struggles when domain rules are incomplete or when noisy data violate assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong domain knowledge encoding, effective heuristics/pruning to reduce combinatorial search, structured symbolic representation of hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper notes Dendral historically as an early example among many automated discovery attempts but does not provide quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human experts historically outperformed in open-ended reasoning but DENDRAL provided systematic acceleration and consistent rule-based exploration; no quantitative baseline provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2470.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientist (King et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a Robot Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated robotic platform that automatically generated hypotheses in functional genomics, planned and executed wet-lab experiments, and iteratively revised hypotheses (closed-loop automation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot Scientist (King et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A closed-loop automated discovery system that generates hypotheses from data, designs experimental protocols, executes experiments using laboratory automation hardware, records and analyzes results, and generates new hypotheses — effectively automating several steps of the scientific cycle.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Discovery System / Self-driving Lab (closed-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Functional genomics (biology), automated experimental wet-lab science</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automatically propose gene-function hypotheses, plan and execute experiments (wet-lab protocols) to test them, collect and analyze experimental results, and iterate to discover gene functions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: involves combinatorial hypothesis space (many candidate gene-function relationships), experimental design constraints, wet-lab procedural complexity, and multi-step iterative cycles; both discrete (protocol choices) and stochastic (biological variability).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires experimental data generated by the system; pre-existing literature and databases can be used but much data must be generated via lab runs; wet-lab experiments are costly and time-consuming relative to computational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Moderate to high due to experimental planning, analysis, and control of lab hardware; the paper reports the system demonstrated closed-loop operation but does not provide compute-hours or cost figures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended scientific discovery with stochastic experimental outcomes, clear verification criteria (experimental results) but many auxiliary hypotheses (protocol validity, instrument performance) affect interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Demonstrated ability to generate testable hypotheses, execute experiments, and produce novel validated findings (new hypotheses supported by experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited by laboratory automation capabilities, preparation overhead, experimental noise/biological variability, and the challenge of diagnosing negative results (many potential causes).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Tight integration of hypothesis generation, automated experimental protocols, and analysis; constrained domain (genomics) where experiments are automatable; strong domain-specific encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper cites King et al. as a canonical example of successful closed-loop automation; contrasts such closed-loop systems with more numerous hypothesis-generation-only systems.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human experimentalists perform complex nuanced lab operations and interpretation; Robot Scientist demonstrated targeted automated discovery but direct quantitative comparison to human researchers is not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2470.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (Highly accurate protein structure prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system that predicts three-dimensional protein structures from amino-acid sequences with high accuracy, representing a major automated hypothesis/prediction tool in structural biology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Deep neural network that maps protein sequences to predicted 3D structures using learned sequence-structure relationships and attention-based architectures; used as an automated hypothesis generator for protein structure.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Hypothesis Generation System / Predictive Model</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Structural biology / protein structure prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict the 3D folded structure of proteins from amino-acid sequences, providing structural hypotheses that inform downstream experiments and design.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: maps high-dimensional sequence space to continuous 3D coordinate space; involves complex non-linear relationships and high-dimensional outputs; search/optimization in continuous space.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained on large curated structural databases (PDB) and multiple sequence alignments; data-rich domain though experimental structures are expensive to obtain for new proteins.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>High training compute (GPU/TPU clusters) and moderate inference compute depending on model variant; exact compute figures not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined prediction problem with continuous outputs and well-established evaluation benchmarks (e.g., CASP); deterministic inference given model and inputs but biological uncertainty remains.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Structure prediction accuracy (e.g., RMSD, TM-score) relative to experimentally solved structures; success often judged by CASP benchmarks and community standards.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limitations for proteins with few homologs, multi-protein complexes, dynamics and conformational ensembles; predictions may lack mechanistic explanatory insight despite accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large training datasets, architectural scaling, incorporation of evolutionary information (MSAs), and engineering for structural constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper uses AlphaFold as an example of successful ML-driven hypothesis generation; notes wide adoption but does not provide direct comparative statistics here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Experimental structure determination (X-ray, NMR, cryo-EM) is slower and costlier; AlphaFold provides rapid computational predictions but experimental validation remains the gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2470.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-driving labs / Closed-loop automation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-driving laboratories / closed-loop automation of scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integrated systems that close the loop between hypothesis generation, experimental planning, automated execution, and analysis to accelerate discovery (applied widely in chemistry, materials, and biology).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-driving lab / Closed-loop automated discovery systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Frameworks combining AI for experiment planning and hypothesis selection with laboratory automation hardware (robotics, liquid handlers, instruments) to run experiments autonomously and feed results back into algorithmic decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Experimentation Platform / Closed-loop Discovery System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry, materials science, biology (experimental sciences where protocols can be automated)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Optimize experimental conditions, discover novel materials/compounds, validate hypotheses via iterative automated experiments; often framed as experimental optimization or active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: large continuous or discrete experimental parameter spaces, noisy and stochastic experimental outcomes, multi-objective tradeoffs (e.g., yield vs. purity), high-cost evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often requires on-demand data generation; pre-existing datasets may be limited; experiments can be costly and time-consuming, limiting sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Varies: active learning/modeling components moderate; physical experiments dominate time and monetary cost; exact compute/time costs not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended but often formulated as optimization or Bayesian experimental design with clear objective metrics; stochastic and costly evaluations with partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Discovery speed, optimization objective value (e.g., improved yield, new compound properties), number of validated novel findings, reduction in number of experiments required.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hardware limitations (robot dexterity), difficulty transferring methods across experimental modalities, high cost of wet-lab experiments, integration complexity, brittle handling of unexpected experimental conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Domains where experiments are automatable, strong simulation or surrogate models, clear objective functions, and effective active-learning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper notes steady progress and cites multiple demonstrations (King et al., mobile robotic chemist, etc.) but emphasizes remaining challenges in generality and low-level manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human-led experimentation is flexible and creative but slower; closed-loop systems can accelerate exploration but lack broad generality compared to human experts in many contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2470.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mobile Robotic Chemist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A mobile robotic chemist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A laboratory robotic system demonstrating autonomous experimental execution for chemistry tasks (example of physical automation enabling experimental discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mobile robotic chemist.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mobile Robotic Chemist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Laboratory robotics platform capable of planning and executing chemical experiments with autonomous control and integration of instruments, enabling some degree of autonomous experimental discovery in chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Experimentation Platform / Robotic Lab</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry (experimental synthesis and characterization)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Execute chemical synthesis and characterization protocols autonomously, enabling large-scale or high-throughput exploration of chemical parameter spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High physical complexity: many low-level manipulations (liquid handling, heating, mixing), multi-step protocols, safety constraints, and variability in reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires generation of experimental data via the robotic platform; prior datasets may exist but platform primarily generates new data at experimental cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Moderate computational control for planning and analysis; largest burden is experimental run-time and hardware integration rather than compute alone.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Physical experimental optimization with stochastic outcomes; discrete and continuous protocol parameters; well-defined evaluation metrics (reaction yield, selectivity).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Ability to autonomously run experiments, reproduce protocols, and discover/optimize chemical outcomes; specific quantitative success metrics not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Challenges in robust low-level manipulation, generalization across diverse chemistries, and handling unexpected hardware or reaction failures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong hardware-software integration, domain-specific automation protocols, constrained problem settings suited to current robotic capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Used as an example of progress in experimental automation; no quantitative comparison to other platforms provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human chemists retain flexibility and troubleshooting ability; robotic chemist excels at throughput and reproducibility in constrained tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2470.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maholo Labdroids</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic crowd biology with Maholo Labdroids</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Robotic lab assistants (Labdroids) designed to conduct wet-lab biology experiments automatically, enabling reproducible high-throughput experimental workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robotic crowd biology with maholo labdroids.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Maholo Labdroids</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Physical robotic systems that perform hands-on laboratory operations (pipetting, mixing, incubating, etc.) to run biological experiments with high repeatability and automation, often coordinated for scaled experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Experimentation Platform / Robotic Lab</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Biology / biotechnology (wet-lab experimental execution)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate standard wet-lab procedures to enable reproducible high-throughput experimentation, data generation, and iterative testing for biological discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to delicate manipulation, biological variability, and need for sterile and safety-compliant operations; many low-level actions must be reliable and flexible.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Primarily generates new experimental data; data quality influenced by robot precision and protocol robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Control and scheduling compute moderate; hardware and throughput dominate time and cost requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Stochastic experimental processes with clear success/failure metrics per protocol; often discrete protocol steps with continuous experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Successful autonomous execution of complex biological protocols and resulting reproducible data; quantitative success rates not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fragility in handling novel manipulations, requirement for extensive preparation and protocol formalization, and difficulty in diagnosing negative results.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Precise robotic manipulation, rigorous protocol formalization, and integration with experiment planning systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Cited as evidence of progress in automating wet-lab execution; no numerical comparisons included here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human technicians can adapt and troubleshoot; Labdroids provide scale and reproducibility but with reduced adaptability to unstructured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2470.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A.I. Cooper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A.I. Cooper (autonomous robot equipment operator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned system that facilitates autonomous robots' use of laboratory equipment similar to human researchers, enabling robotic execution of experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>A.I. Cooper</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Platform or approach (as cited) that enables autonomous robots to interact with and operate laboratory equipment, lowering the barrier for robotic execution of diverse experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Robotic Experimentation Support System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Laboratory automation across experimental sciences</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Enable robots to handle and operate laboratory instruments and equipment in ways comparable to humans, increasing scope of automatable experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — requires capability in perception, manipulation, procedural understanding, and error recovery for diverse equipment and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on experimental tasks; primarily supports generation of new experimental data via robotic operation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Non-trivial for perception and control; hardware and real-world interaction dominate complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Physical, open-ended, and often unstructured; success depends on flexible manipulation and robust perception.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Operational capability to manipulate and operate lab equipment reliably; no quantitative metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Robot manipulation limits, perception failures, fragile assumptions about equipment state, and difficulty with unanticipated instrument behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Advances in robotics, dexterous manipulation, robust perception, and standardized equipment interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Cited as an example of progress toward broader experimental automation but the paper provides no comparative performance data.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Humans currently superior in flexible manual instrument use and troubleshooting; A.I. Cooper aims to narrow this gap but no quantitative comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2470.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boiko et al. multi-LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous agent comprising multiple LLMs that designed and executed complex scientific experiments (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM system reported to autonomously design and execute complex scientific experiments in silico or via automation, demonstrating emergent autonomous scientific capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent autonomous scientific research capabilities of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Boiko et al. multi-LLM autonomous agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A composite autonomous agent architecture using multiple large language models to plan experiments, generate protocols, control execution (possibly through integration with lab automation or simulation), and interpret results, reported to have successfully designed and executed complex experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Multi-LLM Autonomous Research Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General scientific experimentation (examples discussed in the literature include natural sciences and computational experiments); paper cites their success in designing and executing complex experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Autonomously design experimen­tal plans, translate plans into executable protocols, orchestrate execution (via simulated or real lab interfaces), and analyze outcomes to iterate on research questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: coordination across planning, low-level execution, and analysis; must handle open-ended research questions and translate them into concrete experimental actions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Variable — may use published literature and web resources for knowledge; experiments may be executed in simulation or via automated lab resources; specifics not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Substantial due to multiple LLMs, orchestration logic, and possible control loops to hardware/simulators; exact compute not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, multi-stage, and partially observable; mixes discrete planning with continuous experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Reported successful design and execution of complex experiments; success judged by ability to carry out experimental cycles end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Noted general challenges include hallucination/factuality issues of LLMs, difficulties in low-level execution, and diagnosing failed experiments; paper references these limitations in the broader literature.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Integration of multiple LLMs for modular roles (planning, protocol writing, analysis), availability of automation interfaces, constrained experimental tasks amenable to automation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper mentions these systems as early examples of LLM-driven experimental automation and contrasts them with more mature closed-loop lab robotics and hypothesis-generation systems; no quantitative cross-system comparison included.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human researchers retain strengths in nuanced protocol design, manual troubleshooting, and complex experimental judgement; multi-LLM agents demonstrated end-to-end capability in limited scenarios but not general parity with humans as reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2470.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gpt-lab: Next generation of optimal chemistry discovery by gpt driven robotic lab</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that integrates GPT-family LLMs with laboratory robotics to plan and autonomously run chemistry experiments, an example of LLM-driven self-driving lab concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-lab: Next generation of optimal chemistry discovery by gpt driven robotic lab.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-Lab</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline leveraging GPT-style LLMs to generate experimental plans, translate plans into robot-executable instructions, and orchestrate a robotic chemistry lab to perform and analyze experiments for discovery/optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven Automated Experimentation Platform / Robotic Lab Integration</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry (autonomous experimental design and execution for discovery/optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate design and execution of chemistry experiments guided by LLM-generated protocols, aiming to accelerate discovery or optimization of chemical reactions/compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: includes natural-language-to-protocol translation, robotic instruction generation, and handling real-world experimental variability; parameter spaces can be large and multi-objective.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires experimental runs to generate data; may use literature and prior data for initial guidance; experimental data cost/time is a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Moderate-to-high (LLM inference and orchestration); total cost dominated by physical experiment runtime and equipment.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Semi-structured: natural language tasks mapped to structured robotic actions; experiments are stochastic with well-defined evaluation metrics (yields, properties).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Ability to design and carry out experiments that lead to improved outcomes or validated discoveries; paper references such systems but does not supply detailed quantitative metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>LLM hallucination or incorrect protocol generation, mismatch between natural-language instructions and safe robot actions, hardware limitations, and experimental noise.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Constrained chemistry tasks, robust instruction-to-action translation, standardized robotic interfaces, and effective safety checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Cited as part of emerging LLM+robot lab efforts; paper positions GPT-Lab among several contemporary attempts rather than as a definitive comparative leader.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human chemists provide nuanced judgement and troubleshooting; GPT-driven labs can increase throughput in constrained tasks though no direct quantitative comparison reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2470.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoML-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automl-gpt: Automatic machine learning with gpt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach using GPT-family LLMs to automate machine learning tasks such as model design, hyperparameter selection, and pipeline construction (bridging natural-language instructions to ML pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automl-gpt: Automatic machine learning with gpt.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses LLMs to generate ML workflows from natural language goals, including proposing model architectures, hyperparameters, data preprocessing, and orchestrating training/evaluation; integrates with AutoML tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AutoML / LLM-assisted AutoML</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning research and engineering (model selection, hyperparameter tuning, pipeline generation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate the process of designing and producing deployable machine learning models from high-level specifications, reducing developer effort and accelerating experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: involves combinatorial architecture and hyperparameter search, model evaluation under computational constraints, and potentially multi-objective tradeoffs (accuracy vs. latency).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on dataset availability for the ML task; AutoML typically assumes pre-existing labeled datasets; data quality and size crucial to success.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Potentially high due to model training/evaluation cycles; AutoML-GPT may reduce wasted experiments but overall compute depends on search strategy (not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined ML optimization problem (typically supervised learning) with clear evaluation metrics (accuracy, AUC, latency); often amenable to automation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Model performance on evaluation metrics, time-to-deploy, developer-time saved; paper references AutoML-GPT as an example in the space without providing numeric performance here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>LLM-generated suggestions may be suboptimal or not executable; compute cost of training many candidate models; overfitting to validation data without rigorous experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of labeled data, clear evaluation metrics, and integration with efficient model search/evaluation backends.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper highlights AutoML/AutoML-GPT as mature relative to question generation and verification automation, noting better progress in hypothesis-generation-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human ML engineers can craft tailored solutions; AutoML systems can approach or exceed human baselines on many standardized tasks but exact comparisons depend on task and compute budget (not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2470.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperRobot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paperrobot: Incremental draft generation of scientific ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system designed to help generate drafts of scientific ideas and incrementally build research concepts and text, supporting idea generation and writing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperrobot: Incremental draft generation of scientific ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperRobot</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A generative system that incrementally drafts scientific ideas, outlines, or text by synthesizing concepts and literature cues to assist researchers in ideation and writing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Idea Generation / Writing Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Scientific ideation and academic writing across disciplines</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Assist in generating novel research ideas, drafts, and structured outlines from input prompts or literature, facilitating early-stage research conception and documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Conceptual and linguistic complexity rather than heavy experimental or numerical complexity; success depends on quality of language generation and literature grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on textual corpora and scientific literature; performance tied to training/in-context data quality and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Primarily LLM inference costs; moderate depending on model size and usage patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended creative task with subjective evaluation metrics (usefulness, novelty, coherence); discrete outputs in text form.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Human judgments of idea novelty and usefulness, writing quality, and time saved; this paper cites PaperRobot as an example but does not report metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hallucinated or ungrounded suggestions, low novelty, or suggestions that are impractical; requires human curation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong language models, relevant literature context, human-in-the-loop validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Positioned among early systems for automated idea generation; the paper notes such systems exist but offers no quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human creativity and domain expertise remain central; PaperRobot can augment but not replace human judgement in ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2470.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that combines LLMs with domain-specific chemistry tools to improve chemistry-related tasks such as protocol generation, reaction planning, and information retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemcrow: Augmenting large-language models with chemistry tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Augments LLMs with chemical domain tools and interfaces (e.g., reaction databases, cheminformatics toolkits) to improve the factuality and utility of chemistry-related LLM outputs and support experimental planning.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + Domain Tools Integration / Chemistry-assist System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry (protocol suggestion, reaction planning, information extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide chemistry-aware assistance by grounding LLM outputs in domain tools to produce safer, more accurate, and actionable chemistry information and protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high given chemical domain constraints, safety concerns, and need for correct mechanistic/stoichiometric details.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on curated chemical databases and tools; quality and coverage of these resources influence performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference plus calls to domain toolkits; computational costs moderate but dominated by LLM usage and interfacing overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Mixed natural-language and structured-domain reasoning tasks; clear correctness constraints (chemical validity) and safety implications.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Factuality, chemical validity of outputs, usefulness for experimental planning; the paper references ChemCrow as an example but does not provide numerical metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>LLM hallucinations, unsafe or chemically invalid protocol suggestions, and incomplete grounding if tool coverage is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Rich domain tool integration, curated databases, safety filters, and strong LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Listed among systems that aim to make LLMs practical for experimental sciences by grounding them; no direct comparative statistics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human chemists' domain expertise required for final validation; ChemCrow intended to augment human workflows rather than fully replace humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2470.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2470.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reviewergpt / Peer-review automation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReviewerGPT (LLM-based peer review automation) and related peer-review automation efforts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of approaches using LLMs to assist or automate aspects of academic peer review, such as generating reviews, screening submissions, and assessing papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reviewergpt? an exploratory study on using large language models for paper reviewing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReviewerGPT / Peer-review automation systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-driven tools that generate draft peer reviews, screen submissions for suitability, assess methodological soundness, and assist editors/reviewers; some integrate retrieval and factual-checking components.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Assessment / Review Assistance System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Scholarly peer review across disciplines (textual analysis tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate or assist in evaluation of manuscripts by assessing clarity, novelty, methodological soundness, and reproducibility, generating review text and screening recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: tasks are primarily textual and evaluative but involve nuanced judgement, domain expertise, and ethical considerations; subjective criteria are prominent.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large corpora of scientific articles exist for pretraining; peer-review labels and high-quality training data are scarcer; system performance depends on availability of domain-specific review examples.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Primarily LLM inference and retrieval; moderate compute requirements compared to experimental automation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-structured textual evaluation tasks but with high subjectivity and requirements for domain knowledge and alignment with human value judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Agreement with human reviewers, usefulness as an assistant, quality of generated critiques; the paper cites multiple works assessing utility but does not report a single metric.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Bias amplification, hallucinations, failing to catch methodological flaws or ethical issues, and potential to amplify reviewer fatigue or bias.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Integration with domain-specific knowledge, retrieval for evidence grounding, human-in-the-loop workflows, and careful prompt/instruction design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper notes peer-review automation is an active area with many LLM-based studies showing assistance potential but also risks; no unified quantitative comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human peer reviewers provide nuanced, expert judgements; LLMs can assist but currently do not uniformly match expert review quality across complex manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Functional genomic hypothesis generation and experimentation by a robot scientist. <em>(Rating: 2)</em></li>
                <li>Dendral: a case study of the first expert system for scientific hypothesis formation. <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with alphafold. <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models. <em>(Rating: 2)</em></li>
                <li>Gpt-lab: Next generation of optimal chemistry discovery by gpt driven robotic lab. <em>(Rating: 2)</em></li>
                <li>A mobile robotic chemist. <em>(Rating: 2)</em></li>
                <li>Robotic crowd biology with maholo labdroids. <em>(Rating: 2)</em></li>
                <li>Automl-gpt: Automatic machine learning with gpt. <em>(Rating: 2)</em></li>
                <li>Paperrobot: Incremental draft generation of scientific ideas. <em>(Rating: 2)</em></li>
                <li>Chemcrow: Augmenting large-language models with chemistry tools. <em>(Rating: 2)</em></li>
                <li>Reviewergpt? an exploratory study on using large language models for paper reviewing. <em>(Rating: 2)</em></li>
                <li>Autonomous discovery in the chemical sciences part i: Progress. <em>(Rating: 1)</em></li>
                <li>Autonomous discovery in the chemical sciences part ii: outlook. <em>(Rating: 1)</em></li>
                <li>Automated scientific discovery: From equation discovery to autonomous discovery systems. <em>(Rating: 1)</em></li>
                <li>The rise of self-driving labs in chemical and materials sciences. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2470",
    "paper_id": "paper-265698550",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "DENDRAL",
            "name_full": "Dendral (first expert system for scientific hypothesis formation)",
            "brief_description": "An early expert-system AI for hypothesis formation in scientific domains (notably chemistry), historically cited as one of the first automated hypothesis-generation systems.",
            "citation_title": "Dendral: a case study of the first expert system for scientific hypothesis formation.",
            "mention_or_use": "mention",
            "system_name": "Dendral",
            "system_description": "Rule-based expert system that encodes domain knowledge and heuristics to generate candidate scientific hypotheses (e.g., molecular structure hypotheses) from observed data; operates by symbolic inference and search guided by domain rules.",
            "system_type": "Automated Hypothesis Generation System / Expert System",
            "problem_domain": "Chemistry (molecular structure inference) and general scientific hypothesis formation",
            "problem_description": "Generate plausible candidate hypotheses (e.g., molecular structures) consistent with observed experimental data such as mass spectra; reduce search space via domain heuristics.",
            "problem_complexity": "Combinatorial search over candidate structures; high branching factor but constrained by encoded domain rules; complexity dominated by combinatorial hypothesis space and symbolic constraints.",
            "data_availability": "Relies on pre-existing domain knowledge and observed experimental data (spectra); data availability moderate (requires domain-specific observations and encodings).",
            "computational_requirements": "Relatively low by modern ML standards; symbolic search and rule evaluation; compute requirements depend on size of hypothesis space and pruning heuristics (not quantified in this paper).",
            "problem_structure": "Well-defined, discrete, symbolic, constrained search problem with clear evaluation criteria (matching predicted vs observed spectra); deterministic in inference given rules.",
            "success_metric": "Quality/consistency of generated hypotheses with experimental observations; ability to reduce candidate set to correct structures.",
            "success_rate": null,
            "failure_modes": "Limited by completeness and correctness of encoded domain rules; brittle to out-of-domain phenomena; struggles when domain rules are incomplete or when noisy data violate assumptions.",
            "success_factors": "Strong domain knowledge encoding, effective heuristics/pruning to reduce combinatorial search, structured symbolic representation of hypotheses.",
            "comparative_results": "Paper notes Dendral historically as an early example among many automated discovery attempts but does not provide quantitative comparison.",
            "human_baseline": "Human experts historically outperformed in open-ended reasoning but DENDRAL provided systematic acceleration and consistent rule-based exploration; no quantitative baseline provided in this paper.",
            "uuid": "e2470.0",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Robot Scientist (King et al.)",
            "name_full": "Functional genomic hypothesis generation and experimentation by a Robot Scientist",
            "brief_description": "An integrated robotic platform that automatically generated hypotheses in functional genomics, planned and executed wet-lab experiments, and iteratively revised hypotheses (closed-loop automation).",
            "citation_title": "Functional genomic hypothesis generation and experimentation by a robot scientist.",
            "mention_or_use": "mention",
            "system_name": "Robot Scientist (King et al.)",
            "system_description": "A closed-loop automated discovery system that generates hypotheses from data, designs experimental protocols, executes experiments using laboratory automation hardware, records and analyzes results, and generates new hypotheses — effectively automating several steps of the scientific cycle.",
            "system_type": "AI Scientist / Automated Discovery System / Self-driving Lab (closed-loop)",
            "problem_domain": "Functional genomics (biology), automated experimental wet-lab science",
            "problem_description": "Automatically propose gene-function hypotheses, plan and execute experiments (wet-lab protocols) to test them, collect and analyze experimental results, and iterate to discover gene functions.",
            "problem_complexity": "High: involves combinatorial hypothesis space (many candidate gene-function relationships), experimental design constraints, wet-lab procedural complexity, and multi-step iterative cycles; both discrete (protocol choices) and stochastic (biological variability).",
            "data_availability": "Requires experimental data generated by the system; pre-existing literature and databases can be used but much data must be generated via lab runs; wet-lab experiments are costly and time-consuming relative to computational tasks.",
            "computational_requirements": "Moderate to high due to experimental planning, analysis, and control of lab hardware; the paper reports the system demonstrated closed-loop operation but does not provide compute-hours or cost figures.",
            "problem_structure": "Open-ended scientific discovery with stochastic experimental outcomes, clear verification criteria (experimental results) but many auxiliary hypotheses (protocol validity, instrument performance) affect interpretation.",
            "success_metric": "Demonstrated ability to generate testable hypotheses, execute experiments, and produce novel validated findings (new hypotheses supported by experiments).",
            "success_rate": null,
            "failure_modes": "Limited by laboratory automation capabilities, preparation overhead, experimental noise/biological variability, and the challenge of diagnosing negative results (many potential causes).",
            "success_factors": "Tight integration of hypothesis generation, automated experimental protocols, and analysis; constrained domain (genomics) where experiments are automatable; strong domain-specific encoding.",
            "comparative_results": "Paper cites King et al. as a canonical example of successful closed-loop automation; contrasts such closed-loop systems with more numerous hypothesis-generation-only systems.",
            "human_baseline": "Human experimentalists perform complex nuanced lab operations and interpretation; Robot Scientist demonstrated targeted automated discovery but direct quantitative comparison to human researchers is not provided in this paper.",
            "uuid": "e2470.1",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (Highly accurate protein structure prediction)",
            "brief_description": "A deep learning system that predicts three-dimensional protein structures from amino-acid sequences with high accuracy, representing a major automated hypothesis/prediction tool in structural biology.",
            "citation_title": "Highly accurate protein structure prediction with alphafold.",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_description": "Deep neural network that maps protein sequences to predicted 3D structures using learned sequence-structure relationships and attention-based architectures; used as an automated hypothesis generator for protein structure.",
            "system_type": "Automated Hypothesis Generation System / Predictive Model",
            "problem_domain": "Structural biology / protein structure prediction",
            "problem_description": "Predict the 3D folded structure of proteins from amino-acid sequences, providing structural hypotheses that inform downstream experiments and design.",
            "problem_complexity": "Very high: maps high-dimensional sequence space to continuous 3D coordinate space; involves complex non-linear relationships and high-dimensional outputs; search/optimization in continuous space.",
            "data_availability": "Trained on large curated structural databases (PDB) and multiple sequence alignments; data-rich domain though experimental structures are expensive to obtain for new proteins.",
            "computational_requirements": "High training compute (GPU/TPU clusters) and moderate inference compute depending on model variant; exact compute figures not reported in this paper.",
            "problem_structure": "Well-defined prediction problem with continuous outputs and well-established evaluation benchmarks (e.g., CASP); deterministic inference given model and inputs but biological uncertainty remains.",
            "success_metric": "Structure prediction accuracy (e.g., RMSD, TM-score) relative to experimentally solved structures; success often judged by CASP benchmarks and community standards.",
            "success_rate": null,
            "failure_modes": "Limitations for proteins with few homologs, multi-protein complexes, dynamics and conformational ensembles; predictions may lack mechanistic explanatory insight despite accuracy.",
            "success_factors": "Large training datasets, architectural scaling, incorporation of evolutionary information (MSAs), and engineering for structural constraints.",
            "comparative_results": "Paper uses AlphaFold as an example of successful ML-driven hypothesis generation; notes wide adoption but does not provide direct comparative statistics here.",
            "human_baseline": "Experimental structure determination (X-ray, NMR, cryo-EM) is slower and costlier; AlphaFold provides rapid computational predictions but experimental validation remains the gold standard.",
            "uuid": "e2470.2",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-driving labs / Closed-loop automation",
            "name_full": "Self-driving laboratories / closed-loop automation of scientific discovery",
            "brief_description": "Integrated systems that close the loop between hypothesis generation, experimental planning, automated execution, and analysis to accelerate discovery (applied widely in chemistry, materials, and biology).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Self-driving lab / Closed-loop automated discovery systems",
            "system_description": "Frameworks combining AI for experiment planning and hypothesis selection with laboratory automation hardware (robotics, liquid handlers, instruments) to run experiments autonomously and feed results back into algorithmic decision-making.",
            "system_type": "Automated Experimentation Platform / Closed-loop Discovery System",
            "problem_domain": "Chemistry, materials science, biology (experimental sciences where protocols can be automated)",
            "problem_description": "Optimize experimental conditions, discover novel materials/compounds, validate hypotheses via iterative automated experiments; often framed as experimental optimization or active learning.",
            "problem_complexity": "High: large continuous or discrete experimental parameter spaces, noisy and stochastic experimental outcomes, multi-objective tradeoffs (e.g., yield vs. purity), high-cost evaluations.",
            "data_availability": "Often requires on-demand data generation; pre-existing datasets may be limited; experiments can be costly and time-consuming, limiting sample sizes.",
            "computational_requirements": "Varies: active learning/modeling components moderate; physical experiments dominate time and monetary cost; exact compute/time costs not provided in paper.",
            "problem_structure": "Open-ended but often formulated as optimization or Bayesian experimental design with clear objective metrics; stochastic and costly evaluations with partial observability.",
            "success_metric": "Discovery speed, optimization objective value (e.g., improved yield, new compound properties), number of validated novel findings, reduction in number of experiments required.",
            "success_rate": null,
            "failure_modes": "Hardware limitations (robot dexterity), difficulty transferring methods across experimental modalities, high cost of wet-lab experiments, integration complexity, brittle handling of unexpected experimental conditions.",
            "success_factors": "Domains where experiments are automatable, strong simulation or surrogate models, clear objective functions, and effective active-learning strategies.",
            "comparative_results": "Paper notes steady progress and cites multiple demonstrations (King et al., mobile robotic chemist, etc.) but emphasizes remaining challenges in generality and low-level manipulation.",
            "human_baseline": "Human-led experimentation is flexible and creative but slower; closed-loop systems can accelerate exploration but lack broad generality compared to human experts in many contexts.",
            "uuid": "e2470.3",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Mobile Robotic Chemist",
            "name_full": "A mobile robotic chemist",
            "brief_description": "A laboratory robotic system demonstrating autonomous experimental execution for chemistry tasks (example of physical automation enabling experimental discovery).",
            "citation_title": "A mobile robotic chemist.",
            "mention_or_use": "mention",
            "system_name": "Mobile Robotic Chemist",
            "system_description": "Laboratory robotics platform capable of planning and executing chemical experiments with autonomous control and integration of instruments, enabling some degree of autonomous experimental discovery in chemistry.",
            "system_type": "Automated Experimentation Platform / Robotic Lab",
            "problem_domain": "Chemistry (experimental synthesis and characterization)",
            "problem_description": "Execute chemical synthesis and characterization protocols autonomously, enabling large-scale or high-throughput exploration of chemical parameter spaces.",
            "problem_complexity": "High physical complexity: many low-level manipulations (liquid handling, heating, mixing), multi-step protocols, safety constraints, and variability in reactions.",
            "data_availability": "Requires generation of experimental data via the robotic platform; prior datasets may exist but platform primarily generates new data at experimental cost.",
            "computational_requirements": "Moderate computational control for planning and analysis; largest burden is experimental run-time and hardware integration rather than compute alone.",
            "problem_structure": "Physical experimental optimization with stochastic outcomes; discrete and continuous protocol parameters; well-defined evaluation metrics (reaction yield, selectivity).",
            "success_metric": "Ability to autonomously run experiments, reproduce protocols, and discover/optimize chemical outcomes; specific quantitative success metrics not reported in this paper.",
            "success_rate": null,
            "failure_modes": "Challenges in robust low-level manipulation, generalization across diverse chemistries, and handling unexpected hardware or reaction failures.",
            "success_factors": "Strong hardware-software integration, domain-specific automation protocols, constrained problem settings suited to current robotic capabilities.",
            "comparative_results": "Used as an example of progress in experimental automation; no quantitative comparison to other platforms provided in this paper.",
            "human_baseline": "Human chemists retain flexibility and troubleshooting ability; robotic chemist excels at throughput and reproducibility in constrained tasks.",
            "uuid": "e2470.4",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Maholo Labdroids",
            "name_full": "Robotic crowd biology with Maholo Labdroids",
            "brief_description": "Robotic lab assistants (Labdroids) designed to conduct wet-lab biology experiments automatically, enabling reproducible high-throughput experimental workflows.",
            "citation_title": "Robotic crowd biology with maholo labdroids.",
            "mention_or_use": "mention",
            "system_name": "Maholo Labdroids",
            "system_description": "Physical robotic systems that perform hands-on laboratory operations (pipetting, mixing, incubating, etc.) to run biological experiments with high repeatability and automation, often coordinated for scaled experiments.",
            "system_type": "Automated Experimentation Platform / Robotic Lab",
            "problem_domain": "Biology / biotechnology (wet-lab experimental execution)",
            "problem_description": "Automate standard wet-lab procedures to enable reproducible high-throughput experimentation, data generation, and iterative testing for biological discovery.",
            "problem_complexity": "High due to delicate manipulation, biological variability, and need for sterile and safety-compliant operations; many low-level actions must be reliable and flexible.",
            "data_availability": "Primarily generates new experimental data; data quality influenced by robot precision and protocol robustness.",
            "computational_requirements": "Control and scheduling compute moderate; hardware and throughput dominate time and cost requirements.",
            "problem_structure": "Stochastic experimental processes with clear success/failure metrics per protocol; often discrete protocol steps with continuous experimental outcomes.",
            "success_metric": "Successful autonomous execution of complex biological protocols and resulting reproducible data; quantitative success rates not provided in this paper.",
            "success_rate": null,
            "failure_modes": "Fragility in handling novel manipulations, requirement for extensive preparation and protocol formalization, and difficulty in diagnosing negative results.",
            "success_factors": "Precise robotic manipulation, rigorous protocol formalization, and integration with experiment planning systems.",
            "comparative_results": "Cited as evidence of progress in automating wet-lab execution; no numerical comparisons included here.",
            "human_baseline": "Human technicians can adapt and troubleshoot; Labdroids provide scale and reproducibility but with reduced adaptability to unstructured tasks.",
            "uuid": "e2470.5",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "A.I. Cooper",
            "name_full": "A.I. Cooper (autonomous robot equipment operator)",
            "brief_description": "Mentioned system that facilitates autonomous robots' use of laboratory equipment similar to human researchers, enabling robotic execution of experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "A.I. Cooper",
            "system_description": "Platform or approach (as cited) that enables autonomous robots to interact with and operate laboratory equipment, lowering the barrier for robotic execution of diverse experiments.",
            "system_type": "Robotic Experimentation Support System",
            "problem_domain": "Laboratory automation across experimental sciences",
            "problem_description": "Enable robots to handle and operate laboratory instruments and equipment in ways comparable to humans, increasing scope of automatable experiments.",
            "problem_complexity": "High — requires capability in perception, manipulation, procedural understanding, and error recovery for diverse equipment and tasks.",
            "data_availability": "Depends on experimental tasks; primarily supports generation of new experimental data via robotic operation.",
            "computational_requirements": "Non-trivial for perception and control; hardware and real-world interaction dominate complexity.",
            "problem_structure": "Physical, open-ended, and often unstructured; success depends on flexible manipulation and robust perception.",
            "success_metric": "Operational capability to manipulate and operate lab equipment reliably; no quantitative metrics reported in this paper.",
            "success_rate": null,
            "failure_modes": "Robot manipulation limits, perception failures, fragile assumptions about equipment state, and difficulty with unanticipated instrument behaviors.",
            "success_factors": "Advances in robotics, dexterous manipulation, robust perception, and standardized equipment interfaces.",
            "comparative_results": "Cited as an example of progress toward broader experimental automation but the paper provides no comparative performance data.",
            "human_baseline": "Humans currently superior in flexible manual instrument use and troubleshooting; A.I. Cooper aims to narrow this gap but no quantitative comparison provided here.",
            "uuid": "e2470.6",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Boiko et al. multi-LLM agent",
            "name_full": "Autonomous agent comprising multiple LLMs that designed and executed complex scientific experiments (Boiko et al.)",
            "brief_description": "A multi-agent LLM system reported to autonomously design and execute complex scientific experiments in silico or via automation, demonstrating emergent autonomous scientific capabilities.",
            "citation_title": "Emergent autonomous scientific research capabilities of large language models.",
            "mention_or_use": "mention",
            "system_name": "Boiko et al. multi-LLM autonomous agent",
            "system_description": "A composite autonomous agent architecture using multiple large language models to plan experiments, generate protocols, control execution (possibly through integration with lab automation or simulation), and interpret results, reported to have successfully designed and executed complex experiments.",
            "system_type": "AI Scientist / Multi-LLM Autonomous Research Agent",
            "problem_domain": "General scientific experimentation (examples discussed in the literature include natural sciences and computational experiments); paper cites their success in designing and executing complex experiments.",
            "problem_description": "Autonomously design experimen­tal plans, translate plans into executable protocols, orchestrate execution (via simulated or real lab interfaces), and analyze outcomes to iterate on research questions.",
            "problem_complexity": "High: coordination across planning, low-level execution, and analysis; must handle open-ended research questions and translate them into concrete experimental actions.",
            "data_availability": "Variable — may use published literature and web resources for knowledge; experiments may be executed in simulation or via automated lab resources; specifics not quantified in this paper.",
            "computational_requirements": "Substantial due to multiple LLMs, orchestration logic, and possible control loops to hardware/simulators; exact compute not reported here.",
            "problem_structure": "Open-ended, multi-stage, and partially observable; mixes discrete planning with continuous experimental outcomes.",
            "success_metric": "Reported successful design and execution of complex experiments; success judged by ability to carry out experimental cycles end-to-end.",
            "success_rate": null,
            "failure_modes": "Noted general challenges include hallucination/factuality issues of LLMs, difficulties in low-level execution, and diagnosing failed experiments; paper references these limitations in the broader literature.",
            "success_factors": "Integration of multiple LLMs for modular roles (planning, protocol writing, analysis), availability of automation interfaces, constrained experimental tasks amenable to automation.",
            "comparative_results": "Paper mentions these systems as early examples of LLM-driven experimental automation and contrasts them with more mature closed-loop lab robotics and hypothesis-generation systems; no quantitative cross-system comparison included.",
            "human_baseline": "Human researchers retain strengths in nuanced protocol design, manual troubleshooting, and complex experimental judgement; multi-LLM agents demonstrated end-to-end capability in limited scenarios but not general parity with humans as reported here.",
            "uuid": "e2470.7",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT-Lab",
            "name_full": "Gpt-lab: Next generation of optimal chemistry discovery by gpt driven robotic lab",
            "brief_description": "A system that integrates GPT-family LLMs with laboratory robotics to plan and autonomously run chemistry experiments, an example of LLM-driven self-driving lab concepts.",
            "citation_title": "Gpt-lab: Next generation of optimal chemistry discovery by gpt driven robotic lab.",
            "mention_or_use": "mention",
            "system_name": "GPT-Lab",
            "system_description": "Pipeline leveraging GPT-style LLMs to generate experimental plans, translate plans into robot-executable instructions, and orchestrate a robotic chemistry lab to perform and analyze experiments for discovery/optimization tasks.",
            "system_type": "LLM-driven Automated Experimentation Platform / Robotic Lab Integration",
            "problem_domain": "Chemistry (autonomous experimental design and execution for discovery/optimization)",
            "problem_description": "Automate design and execution of chemistry experiments guided by LLM-generated protocols, aiming to accelerate discovery or optimization of chemical reactions/compounds.",
            "problem_complexity": "High: includes natural-language-to-protocol translation, robotic instruction generation, and handling real-world experimental variability; parameter spaces can be large and multi-objective.",
            "data_availability": "Requires experimental runs to generate data; may use literature and prior data for initial guidance; experimental data cost/time is a limiting factor.",
            "computational_requirements": "Moderate-to-high (LLM inference and orchestration); total cost dominated by physical experiment runtime and equipment.",
            "problem_structure": "Semi-structured: natural language tasks mapped to structured robotic actions; experiments are stochastic with well-defined evaluation metrics (yields, properties).",
            "success_metric": "Ability to design and carry out experiments that lead to improved outcomes or validated discoveries; paper references such systems but does not supply detailed quantitative metrics here.",
            "success_rate": null,
            "failure_modes": "LLM hallucination or incorrect protocol generation, mismatch between natural-language instructions and safe robot actions, hardware limitations, and experimental noise.",
            "success_factors": "Constrained chemistry tasks, robust instruction-to-action translation, standardized robotic interfaces, and effective safety checks.",
            "comparative_results": "Cited as part of emerging LLM+robot lab efforts; paper positions GPT-Lab among several contemporary attempts rather than as a definitive comparative leader.",
            "human_baseline": "Human chemists provide nuanced judgement and troubleshooting; GPT-driven labs can increase throughput in constrained tasks though no direct quantitative comparison reported here.",
            "uuid": "e2470.8",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AutoML-GPT",
            "name_full": "Automl-gpt: Automatic machine learning with gpt",
            "brief_description": "An approach using GPT-family LLMs to automate machine learning tasks such as model design, hyperparameter selection, and pipeline construction (bridging natural-language instructions to ML pipelines).",
            "citation_title": "Automl-gpt: Automatic machine learning with gpt.",
            "mention_or_use": "mention",
            "system_name": "AutoML-GPT",
            "system_description": "Uses LLMs to generate ML workflows from natural language goals, including proposing model architectures, hyperparameters, data preprocessing, and orchestrating training/evaluation; integrates with AutoML tooling.",
            "system_type": "AutoML / LLM-assisted AutoML",
            "problem_domain": "Machine learning research and engineering (model selection, hyperparameter tuning, pipeline generation)",
            "problem_description": "Automate the process of designing and producing deployable machine learning models from high-level specifications, reducing developer effort and accelerating experimentation.",
            "problem_complexity": "Moderate-to-high: involves combinatorial architecture and hyperparameter search, model evaluation under computational constraints, and potentially multi-objective tradeoffs (accuracy vs. latency).",
            "data_availability": "Depends on dataset availability for the ML task; AutoML typically assumes pre-existing labeled datasets; data quality and size crucial to success.",
            "computational_requirements": "Potentially high due to model training/evaluation cycles; AutoML-GPT may reduce wasted experiments but overall compute depends on search strategy (not quantified in this paper).",
            "problem_structure": "Well-defined ML optimization problem (typically supervised learning) with clear evaluation metrics (accuracy, AUC, latency); often amenable to automation.",
            "success_metric": "Model performance on evaluation metrics, time-to-deploy, developer-time saved; paper references AutoML-GPT as an example in the space without providing numeric performance here.",
            "success_rate": null,
            "failure_modes": "LLM-generated suggestions may be suboptimal or not executable; compute cost of training many candidate models; overfitting to validation data without rigorous experimental design.",
            "success_factors": "Availability of labeled data, clear evaluation metrics, and integration with efficient model search/evaluation backends.",
            "comparative_results": "Paper highlights AutoML/AutoML-GPT as mature relative to question generation and verification automation, noting better progress in hypothesis-generation-like tasks.",
            "human_baseline": "Human ML engineers can craft tailored solutions; AutoML systems can approach or exceed human baselines on many standardized tasks but exact comparisons depend on task and compute budget (not provided here).",
            "uuid": "e2470.9",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PaperRobot",
            "name_full": "Paperrobot: Incremental draft generation of scientific ideas",
            "brief_description": "A system designed to help generate drafts of scientific ideas and incrementally build research concepts and text, supporting idea generation and writing.",
            "citation_title": "Paperrobot: Incremental draft generation of scientific ideas.",
            "mention_or_use": "mention",
            "system_name": "PaperRobot",
            "system_description": "A generative system that incrementally drafts scientific ideas, outlines, or text by synthesizing concepts and literature cues to assist researchers in ideation and writing tasks.",
            "system_type": "Automated Idea Generation / Writing Assistant",
            "problem_domain": "Scientific ideation and academic writing across disciplines",
            "problem_description": "Assist in generating novel research ideas, drafts, and structured outlines from input prompts or literature, facilitating early-stage research conception and documentation.",
            "problem_complexity": "Conceptual and linguistic complexity rather than heavy experimental or numerical complexity; success depends on quality of language generation and literature grounding.",
            "data_availability": "Relies on textual corpora and scientific literature; performance tied to training/in-context data quality and coverage.",
            "computational_requirements": "Primarily LLM inference costs; moderate depending on model size and usage patterns.",
            "problem_structure": "Open-ended creative task with subjective evaluation metrics (usefulness, novelty, coherence); discrete outputs in text form.",
            "success_metric": "Human judgments of idea novelty and usefulness, writing quality, and time saved; this paper cites PaperRobot as an example but does not report metrics.",
            "success_rate": null,
            "failure_modes": "Hallucinated or ungrounded suggestions, low novelty, or suggestions that are impractical; requires human curation.",
            "success_factors": "Strong language models, relevant literature context, human-in-the-loop validation.",
            "comparative_results": "Positioned among early systems for automated idea generation; the paper notes such systems exist but offers no quantitative comparison.",
            "human_baseline": "Human creativity and domain expertise remain central; PaperRobot can augment but not replace human judgement in ideation.",
            "uuid": "e2470.10",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large-language models with chemistry tools",
            "brief_description": "A system that combines LLMs with domain-specific chemistry tools to improve chemistry-related tasks such as protocol generation, reaction planning, and information retrieval.",
            "citation_title": "Chemcrow: Augmenting large-language models with chemistry tools.",
            "mention_or_use": "mention",
            "system_name": "ChemCrow",
            "system_description": "Augments LLMs with chemical domain tools and interfaces (e.g., reaction databases, cheminformatics toolkits) to improve the factuality and utility of chemistry-related LLM outputs and support experimental planning.",
            "system_type": "LLM + Domain Tools Integration / Chemistry-assist System",
            "problem_domain": "Chemistry (protocol suggestion, reaction planning, information extraction)",
            "problem_description": "Provide chemistry-aware assistance by grounding LLM outputs in domain tools to produce safer, more accurate, and actionable chemistry information and protocols.",
            "problem_complexity": "Moderate-to-high given chemical domain constraints, safety concerns, and need for correct mechanistic/stoichiometric details.",
            "data_availability": "Relies on curated chemical databases and tools; quality and coverage of these resources influence performance.",
            "computational_requirements": "LLM inference plus calls to domain toolkits; computational costs moderate but dominated by LLM usage and interfacing overhead.",
            "problem_structure": "Mixed natural-language and structured-domain reasoning tasks; clear correctness constraints (chemical validity) and safety implications.",
            "success_metric": "Factuality, chemical validity of outputs, usefulness for experimental planning; the paper references ChemCrow as an example but does not provide numerical metrics here.",
            "success_rate": null,
            "failure_modes": "LLM hallucinations, unsafe or chemically invalid protocol suggestions, and incomplete grounding if tool coverage is insufficient.",
            "success_factors": "Rich domain tool integration, curated databases, safety filters, and strong LLM prompts.",
            "comparative_results": "Listed among systems that aim to make LLMs practical for experimental sciences by grounding them; no direct comparative statistics provided in this paper.",
            "human_baseline": "Human chemists' domain expertise required for final validation; ChemCrow intended to augment human workflows rather than fully replace humans.",
            "uuid": "e2470.11",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Reviewergpt / Peer-review automation",
            "name_full": "ReviewerGPT (LLM-based peer review automation) and related peer-review automation efforts",
            "brief_description": "A suite of approaches using LLMs to assist or automate aspects of academic peer review, such as generating reviews, screening submissions, and assessing papers.",
            "citation_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing.",
            "mention_or_use": "mention",
            "system_name": "ReviewerGPT / Peer-review automation systems",
            "system_description": "LLM-driven tools that generate draft peer reviews, screen submissions for suitability, assess methodological soundness, and assist editors/reviewers; some integrate retrieval and factual-checking components.",
            "system_type": "Automated Assessment / Review Assistance System",
            "problem_domain": "Scholarly peer review across disciplines (textual analysis tasks)",
            "problem_description": "Automate or assist in evaluation of manuscripts by assessing clarity, novelty, methodological soundness, and reproducibility, generating review text and screening recommendations.",
            "problem_complexity": "Moderate: tasks are primarily textual and evaluative but involve nuanced judgement, domain expertise, and ethical considerations; subjective criteria are prominent.",
            "data_availability": "Large corpora of scientific articles exist for pretraining; peer-review labels and high-quality training data are scarcer; system performance depends on availability of domain-specific review examples.",
            "computational_requirements": "Primarily LLM inference and retrieval; moderate compute requirements compared to experimental automation.",
            "problem_structure": "Well-structured textual evaluation tasks but with high subjectivity and requirements for domain knowledge and alignment with human value judgements.",
            "success_metric": "Agreement with human reviewers, usefulness as an assistant, quality of generated critiques; the paper cites multiple works assessing utility but does not report a single metric.",
            "success_rate": null,
            "failure_modes": "Bias amplification, hallucinations, failing to catch methodological flaws or ethical issues, and potential to amplify reviewer fatigue or bias.",
            "success_factors": "Integration with domain-specific knowledge, retrieval for evidence grounding, human-in-the-loop workflows, and careful prompt/instruction design.",
            "comparative_results": "Paper notes peer-review automation is an active area with many LLM-based studies showing assistance potential but also risks; no unified quantitative comparison is provided.",
            "human_baseline": "Human peer reviewers provide nuanced, expert judgements; LLMs can assist but currently do not uniformly match expert review quality across complex manuscripts.",
            "uuid": "e2470.12",
            "source_info": {
                "paper_title": "Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Functional genomic hypothesis generation and experimentation by a robot scientist.",
            "rating": 2,
            "sanitized_title": "functional_genomic_hypothesis_generation_and_experimentation_by_a_robot_scientist"
        },
        {
            "paper_title": "Dendral: a case study of the first expert system for scientific hypothesis formation.",
            "rating": 2,
            "sanitized_title": "dendral_a_case_study_of_the_first_expert_system_for_scientific_hypothesis_formation"
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold.",
            "rating": 2,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models.",
            "rating": 2,
            "sanitized_title": "emergent_autonomous_scientific_research_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Gpt-lab: Next generation of optimal chemistry discovery by gpt driven robotic lab.",
            "rating": 2,
            "sanitized_title": "gptlab_next_generation_of_optimal_chemistry_discovery_by_gpt_driven_robotic_lab"
        },
        {
            "paper_title": "A mobile robotic chemist.",
            "rating": 2,
            "sanitized_title": "a_mobile_robotic_chemist"
        },
        {
            "paper_title": "Robotic crowd biology with maholo labdroids.",
            "rating": 2,
            "sanitized_title": "robotic_crowd_biology_with_maholo_labdroids"
        },
        {
            "paper_title": "Automl-gpt: Automatic machine learning with gpt.",
            "rating": 2,
            "sanitized_title": "automlgpt_automatic_machine_learning_with_gpt"
        },
        {
            "paper_title": "Paperrobot: Incremental draft generation of scientific ideas.",
            "rating": 2,
            "sanitized_title": "paperrobot_incremental_draft_generation_of_scientific_ideas"
        },
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools.",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing.",
            "rating": 2,
            "sanitized_title": "reviewergpt_an_exploratory_study_on_using_large_language_models_for_paper_reviewing"
        },
        {
            "paper_title": "Autonomous discovery in the chemical sciences part i: Progress.",
            "rating": 1,
            "sanitized_title": "autonomous_discovery_in_the_chemical_sciences_part_i_progress"
        },
        {
            "paper_title": "Autonomous discovery in the chemical sciences part ii: outlook.",
            "rating": 1,
            "sanitized_title": "autonomous_discovery_in_the_chemical_sciences_part_ii_outlook"
        },
        {
            "paper_title": "Automated scientific discovery: From equation discovery to autonomous discovery systems.",
            "rating": 1,
            "sanitized_title": "automated_scientific_discovery_from_equation_discovery_to_autonomous_discovery_systems"
        },
        {
            "paper_title": "The rise of self-driving labs in chemical and materials sciences.",
            "rating": 1,
            "sanitized_title": "the_rise_of_selfdriving_labs_in_chemical_and_materials_sciences"
        }
    ],
    "cost": 0.02527175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research
6 Dec 2023</p>
<p>Shiro Takagi 
Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research
6 Dec 20236B49DACBF9EE7784386406D33BB6EB03arXiv:2312.03497v1[cs.AI]
This paper engages in a speculative exploration of the concept of an artificial agent capable of conducting research.Initially, it examines how the act of research can be conceptually characterized, aiming to provide a starting point for discussions about what it means to create such agents.The focus then shifts to the core components of research: question formulation, hypothesis generation, and hypothesis verification.This discussion includes a consideration of the potential and challenges associated with enabling machines to autonomously perform these tasks.Subsequently, this paper briefly considers the overlapping themes and interconnections that underlie them.Finally, the paper presents preliminary thoughts on prototyping as an initial step towards uncovering the challenges involved in developing these research-capable agents.</p>
<p>Introduction</p>
<p>Research has been the foundation of human progress.Through research, humans have deepened their understanding of the world and created unprecedented innovations, leading to groundbreaking advancements.It would be not an exaggeration to say that the future development of humanity heavily depends on the evolution and progress of research endeavors.</p>
<p>Since the inception of artificial intelligence (AI) research, a key goal has been to develop AI capable of conducting research [1].AI-led research not only accelerates existing research and development but also offers significant potential for improving research practice and methodologies themselves, unencumbered by human cognitive limitations, research conventions, or unnecessary social constraints [1,2].</p>
<p>Researchers have developed systems that automate scientific decision makings [3], infer natural laws [4], autonomously cycle through hypothesis and experimentation [5], and many more [1,6].With advancements in machine learning, there have been remarkable successes in using it for scientific discovery [7,8,9,10].These efforts by humanity have significantly advanced the potential of machines as valuable assistants in research.</p>
<p>On the other hand, the journey toward fully autonomous intelligent agents1 capable of conducting research is still ongoing [1,11]."Autonomous" here implies functioning without human intervention, prior design, or preparation.An agent is considered more autonomous if it can conduct research with less human involvement.By "agents that conduct research," I refer to a single agent capable of performing research activities in various fields, like history, mathematics, or physics.The wider the range of research a single agent can handle, the more general it is considered.In this paper, when referring to an agent capable of conducting research, it denotes a general and autonomous artificial researcher.The realization of such agents has been a deeply held aspiration in the quest for advancing human research capabilities.</p>
<p>While there are excellent papers presenting perspectives on AI capable of conducting research [1,12,11,2,7,6,9,13,14], there is still much to discuss about the nature of such agents and what it means to create an agent capable of doing research.Therefore, this paper presents a speculative thought around the concept of artificial intelligent agents capable of conducting research.This discussion aims to provide an opportunity to consider what future discussions are needed as we work towards realizing agents capable of conducting research First, I will explore conceptually how to characterize the research activities.This preliminary discussion is intended to help us consider what it would mean to create an agent capable of conducting research and what discussions would be necessary for its development.Next, I will discuss elements widely considered essential in research, specifically exploring the nature of question construction, hypothesis generation, and hypothesis verification, along with the potential challenges in autonomously performing these tasks.Subsequently, I will consider topics that combine these elements, topics common to them, and points that could not be discussed previously.Finally, as a reference, I will share some simple, preliminary ideas for prototyping aimed at identifying challenges in developing an agent capable of conducting research.</p>
<p>The speculative discussion in this paper is still in its early stages and is provisional.Given the breadth of the subject matter and my limited capabilities, each point of discussion may be somewhat superficial or not entirely accurate.I plan to update these discussions continuously.Therefore, if anyone notices any points that should be improved, errors, or topics that would be beneficial to discuss, I would greatly appreciate your feedback.</p>
<p>Conceptual Characterization of Research</p>
<p>Understanding the fundamental nature of research is crucial for creating an agent capable of autonomous research.This section will therefore speculatively consider how the act of research can be characterized.</p>
<p>The aim of this section is not to establish a universal and singular definition of research, a task that exceeds the scope of this paper.Rather, it explores some characteristics of research to provide a provisional basis for discussions on the development of an artificial researcher.</p>
<p>As such, the definition of research presented here should be regarded as tentative and operational, and the ensuing discussion is just one example of an endeavor to characterize research.Refining our understanding of research through in-depth discussions in the future is essential for the development of agents capable of conducting research.</p>
<p>Research as Knowledge Production</p>
<p>While finding a unified, all-encompassing definition of research or science remains infeasible [15,16], various interpretations exist.For instance, one view posits that research occurs "whenever we gather information to answer a question that solves a problem" [17], while another describes research (and development) as comprising "creative and systematic work undertaken in order to increase the stock of knowledge" [18].Additionally, some perceive the science as "processes that maximize the evidence for a generative model of the sensed and measured world" [19].These descriptions highlight different crucial aspects of research, with none being entirely incorrect or absolutely definitive.</p>
<p>Among these, a broadly recognized definition can be that research is an endeavor to generate new knowledge.Since this characterization seems to align with our research practices, regardless of the field, this definition serves as a suitable starting point for our discussion.In this paper, I adopt this interpretation as a provisional working definition.Specifically, I will regard research as the attempts to produce new knowledge for certain society.The inclusion of "for certain society" acknowledges the societal relativity of knowledge, a point I will elaborate on in subsequent sections.</p>
<p>Knowledge Production as Belief Revision</p>
<p>Having defined research as the endeavor to generate new knowledge, it becomes important to consider what "knowledge" itself entails, and what constitutes its production.This section aims to explore these concepts.</p>
<p>Defining knowledge and the process of knowledge production rigorously remains an unsettled philosophical debate [20].Given that providing a precise definition of knowledge is beyond the scope of this paper, an in-depth exploration of these debates will not be undertaken here.Instead, this section aims to present a basic and preliminary conception of what knowledge might entail so that it serves as a starting point for further discussion.</p>
<p>Knowledge as Belief</p>
<p>The concept of knowledge has long been a subject of debate within epistemology, a branch of philosophy.This paper will reference some basic and introductory concepts in this field as an exemplar to explore how research might be characterized.</p>
<p>Within epistemology, knowledge has traditionally been viewed as justified true belief (JTB) [20].The term "true" is challenging to define rigorously; however, for the purposes of this discussion, it can be understood as something that corresponds with fact."Belief" is tentatively defined as an individual's thought or conviction about a subject."Justified" implies that it is reasonable to hold such a belief.The nature of justification has been a focal point of debate in epistemology, particularly following criticisms that JTB may not adequately define knowledge [21].Consequently, the refinement or expansion of the JTB has become a significant topic in epistemological discourse [20].</p>
<p>Although many philosophers contend that the JTB properties alone are insufficient for defining knowledge, there is some consensus that they might be necessary components [20].In epistemological debates, rather than discarding JTB entirely, many theorists use it as a foundational concept.Hence, this paper will tentatively adopt JTB as a preliminary basis for discussion.</p>
<p>The subsequent sections will explore how research is conceptualized under this definition.Analyzing the congruences and discrepancies between the implications drawn from this characterization and our expectations of a research-capable agent will generate insights for refining the definition of research and hypothesizing the capabilities such agents should possess.</p>
<p>Knowledge Production as Belief Updating</p>
<p>In the current framework, knowledge is equated with belief.Therefore, knowledge production can be reinterpreted as the process of adopting the belief that a particular proposition is true and subsequently revising this belief based on justification.Essentially, in this framework, knowledge production equates to the updating of beliefs.</p>
<p>While it might initially seem counterintuitive to view research as a process of updating beliefs, this perspective gains plausibility when considering several factors.Research involves the continual revision of hypotheses and theories; inductive reasoning, unlike deduction, does not conclusively prove propositions; and, as will be discussed, knowledge is essentially subjective.The characterization of research as belief aligns well with these aspects of research.Therefore, this characterization seems reasonably valid.</p>
<p>The primary aim of research can be conceptualized as uncovering the unknown truths of the world.Consequently, the justification employed in research must be capable of accurately discerning the truth or falsity of propositions.This form of justification, known for its capacity to lead to truth, is termed "truth-conducive."While there are varied debates on the nature of justification, it is widely accepted that justification in research should indeed be truth-conducive.Thus, knowledge production can be conceptualized as the construction of belief in new propositions about the world and ascertaining their veracity through truth-conducive justification.</p>
<p>To Know Depends on Knowing Subjects</p>
<p>Fundamentally, the concept of knowing presupposes not only the existence of the object being known but also of the subject doing the knowing.This interplay explains why the definition of knowledge incorporates the subjective element of belief.Consequently, while the notion that knowledge is a form of belief might initially seem counterintuitive, it holds validity in this context.</p>
<p>Moreover, conceptualizing research as an updating of beliefs aligns closely with actual research practices.For instance, the experimental validation of a hypothesis reinforces our belief in its truth or falsity.Our confidence in a hypothesis increases as it withstands various rounds of verification.This process of iterative validation and belief reinforcement mirrors the concept of research as a continual renewal of beliefs.</p>
<p>Finally, since the justification in research is expected to be truth-conducive, the knowledge thus produced would have an objective quality.Therefore, in conjunction with the discussion in the previous section, it seems that the use of the subjective concept of belief is not that problematic.</p>
<p>Knowledge for Humans</p>
<p>As previously discussed, research is a pursuit dedicated to uncovering the unknown truths of the world, necessitating that the knowledge it generates be novel.This raises the question: what constitutes new or unknown knowledge?</p>
<p>Under the current definition, knowledge is a justified belief regarding the truth or falsity of a certain hypothesis.Thus, unknown knowledge could be a state where such a belief is either non-existent or, if it exists, lacks justification.In simpler terms, within this framework, the state of certain knowledge being unknown is essentially a state of belief.</p>
<p>Given that the concept of knowing is contingent on the knowing subject, the notion of the unknown is also inherently subject-dependent.In the realm of research, the term "subject" has seemingly encompassed humanity at large.Researchers do not deem knowledge as unknown simply because it eludes an individual; it is regarded as truly unknown only when it is beyond the collective understanding of humanity.Consequently, the knowledge produced through research is expected to contribute to the collective understanding of human society.This is the reason why the term "for society" is included in the definition of research.</p>
<p>Knowledge for Non-Humans</p>
<p>Since the act of knowing is subject-dependent, it is theoretically feasible to conceive of non-human knowledge and research by considering non-human agents as knowing subjects.Naturally, there is also the unknown for these non-human agents.Such knowledge and unknowns for non-humans can naturally differ from those for humans.Therefore, when referring to "knowledge," "unknown," or "novel," it is necessary to specify for whom these concepts apply.</p>
<p>While this discussion might seem like mere speculation, the idea that machines might have a different scope of the unknown compared to humans has implications for realizing an artificial researcher.This is because it suggests that merely replicating current research methodologies in AI might not necessarily yield new knowledge for humans.</p>
<p>Research methods essentially have been developed to uncover truths unknown to knowing subjects.Therefore, an AI mimicking these methods might only reveal truths unknown to itself, which may not align with human knowledge gaps.If we want AI to conduct research autonomously, we must find a way to ensure it understands what is unknown to humans, not just to itself, and guide it to discover knowledge that is truly unknown in the human context.This is just a preliminary discussion.It is hoped that further discussion will continue on how to realize them for developing research-capable AI</p>
<p>Can Truth-Conducive Justification Be Developed by Non-Human Agents?</p>
<p>I acknowledge that we cannot definitively "prove" our empirical verification methods to be entirely truth-conducive.Yet, given the myriad discoveries achieved through these methods, questioning their legitimacy seems to have little merit.If agents can thoroughly grasp and effectively apply these humandeveloped justifications, it is poised to uncover numerous unknown truths, a prospect few researchers would dispute.</p>
<p>Then, what about when the agent constructs its own methods of justification?Is it possible for an agent to construct new truth-conducive justification methods on its own, instead of just mastering human-developed methods?Humans have devised tools like statistical hypothesis testing to evaluate hypotheses; can AI similarly innovate unique methodologies?Even if it were possible, how significant would those be?</p>
<p>Our justification methods are founded on various premises, implying diverse interpretations of "what constitutes a truth-conducive justification" or "what justification entails."These interpretations lead to multiple methods of justification even among humans [22].Consequently, when allowing artificial agents to autonomously develop justification methods, these agents must consider what can serve as justification, navigate value judgments regarding the nature and effectiveness of justification, and thereby conceive and select optimal methodologies.This inquiry goes beyond mere philosophical speculation.Since the current justification methodologies have not been proved to be the optimal, the potential for superior methodologies exists in theory.Machines, unfettered by human cognitive limitations, theoretically possess the possibility to discover such methods.Importantly, truth-conducive verification methods does not essentially require human value judgments to evaluate their quality, thus, there is ample potential for machines to "autonomously" devise them.The feasibility, realization, and significance of these possibilities remain open for exploration and debate.</p>
<p>Conclusion</p>
<p>In this section, I have discussed a provisional working definition of research.My initial premise was the intuitive belief that research is an endeavor aimed at generating new knowledge for a particular society.The discussion subsequently delved into a speculative inquiry into the idea that that knowledge is fundamentally a form of belief and that the production of knowledge is the updating of beliefs.Building on these ideas, I presented some conjectural insights of non-human agents doing knowledge production.</p>
<p>It is important to note that the definition proposed here serves merely as a starting point.A more comprehensive and nuanced understanding of research can be cultivated through the collective insights of philosophers, scientists, and practitioners across various fields.This collaborative approach will enable us to delve deeper into the definition of research and develop more robust and effective guidelines to realize an artificial researcher.</p>
<p>3 Question, Hypothesis, and Verification</p>
<p>In the preceding section, I briefly examined a preliminary conceptual definition of research and its implications.This section shifts focus to the widely acknowledged fundamental components of research: question construction, hypothesis generation, and hypothesis verification.</p>
<p>The objective here is to advance the discussion beyond the somewhat too abstract considerations in the previous section.By dissecting these core elements, this section seeks to offer a more concrete exploration of what constitutes research and the prospects of machines engaging in research activities.</p>
<p>Question Construction</p>
<p>The first essential element in research is question construction.To produce new knowledge, it is imperative to recognize what is unknown and strive to generate that elusive knowledge.This act of identifying the unknown for investigation can be considered as the process of questioning.Subsequently, formulating potential answers for these questions constitutes hypothesis generation.Essentially, research can be reinterpreted as the act of posing and answering to questions.Furthermore, since research inherently involves lots of uncertainty, the generation of multiple questions, beyond the initial research question, is a natural part of the process.That is, in the pursuit of confronting the unknown, question construction is an inevitable aspect of research.</p>
<p>There have been the studies to find research questions and challenges from academic literature [23,24,25], generate ideas for future work [26], and identify research trends [27,28].However, enabling machines to autonomously generate research questions is less common.In the domain of question answering from natural language processing (NLP) research, tasks exist for generating questions [29,30], but these are motivated differently from generating research questions.Research on artificial curiosity for generating non-textual questions has been conducted [31], yet it doesn't generate research questions akin to a human researcher, as far as I know.Recent advancements in large language models (LLMs) have led to initial attempts at generating research questions [32,33], but this area remains nascent.</p>
<p>While there are efforts towards automation as shown above, the number of these attempts are relatively limited compared to those for hypothesis generation and verification.The automation of question construction, or determining the underlying goals of such automation, is recognized as a key challenge in the field of research automation [11,6,2].</p>
<p>In this section, I start with a speculative exploration of the nature of questioning.This will be followed by a discussion of the open challenges in enabling an artificial agent to effectively pose research questions.</p>
<p>What is Questioning?</p>
<p>Asking questions is often characterized as an information-seeking behavior [34,35].This behavior typically involves two distinct steps: firstly, recognizing an information need, and secondly, undertaking actions to acquire the desired information [36,37].While not all information-seeking behaviors necessitate linguistic expressions [34], in the context of research, queries are typically formulated in text.This textual formulation occurs between the stages of information need recognition and the initiation of information-seeking behavior.Specifically, in research, the process of question construction is generally understood as the journey leading to the formulation of such queries.Thus, for the purposes of this paper, question construction is regarded as the process culminating in the formulation of a query.The subsequent steps of information seeking are considered part of hypothesis generation and verification.</p>
<p>Recognizing an information need seems to involve at least two sub-processes: identifying the knowledge gap and deciding to address it (judging that the missing information is a "need"). 2 Therefore, to enable an artificial agent to autonomously construct questions, it is necessary to consider how to imbue it with these capabilities.The following sections will delve into a speculative consideration and exploration of these steps. 3</p>
<p>Recognizing Unknowns</p>
<p>Recognizing that certain knowledge is unknown typically involves an initial attempt to access that knowledge.This process usually entails referring to our personal knowledge base and, upon not finding the information, deeming it as unknown.For an individual, this knowledge base is essentially the memory stored within the brain.However, in the realm of research, the unknowns that researchers aim to elucidate are those unknown to a specific society, not just to an individual researcher.That is, a research-capable agent does not need to judge whether it is unknown to itself, but rather it can directly determine whether it is unknown to certain society.In this context, the knowledge base extends beyond the agent's memory to encompass societal knowledge sources, such as a collection of research papers. 4s outlined in Section 2, for machines to generate new knowledge beneficial to humans, they must be capable of identifying what is unknown to humans, not to themselves.While this task might initially seem as straightforward as conducting a literature survey as humans do, the reality may necessitate more complex approaches.The specific requirements for achieving this goal merit further discussion.</p>
<p>An additional consideration arises regarding the reliance on a machine's judgment to determine what is unknown to us.If an AI has been pre-trained on an extensive corpus of scholarly papers, its judgment might appear credible.However, as previously mentioned, an AI's determination of unknowns does not necessarily coincide with human unknowns.Therefore, it is not certain whether it is appropriate to unconditionally believe that what AI has determined to be unknown is indeed unknown.This issue becomes increasingly pertinent and significant as AI amass more knowledge and enhance their capabilities.</p>
<p>Deciding What Knowledge to Seek</p>
<p>While we encounter numerous unknowns, we do not formulate questions for each one, as not all unknowns hold equal "importance" or "interest."Instead, we construct questions for matters we are eager to understand.This process involves assessing the "value" of questions based on certain criteria to determine their worthiness of pursuit.For individuals, this can be a largely subconscious process.However, in research, this need not be an internal process, as long as that is the value judgments of questions.</p>
<p>Knowledge, in itself, is value-neutral.The "value," "significance," or "goodness" of knowledge is ascribed by its users.A noteworthy aspect here is that the criteria for determining "value" are subjective and arbitrary.Hence, if we aim for artificial agents to autonomously pose questions meaningful to humanity, it is crucial to identify what constitutes "good" or "significant" questions for us and instill these values in the agents.</p>
<p>On the other hand, it is also vital to recognize that certain questions deemed "unimportant" by us may actually hold importance under different criteria.Fundamental research, for example, often yields knowledge initially perceived as "useless" but later proves pivotal for innovations.Human cognitive limitations may sometimes hinder our ability to fully appreciate the potential utility of such knowledge.Moreover, social factors unrelated to the initial purpose of knowledge production can influence our value judgments, implying that these human judgments are not always optimal.</p>
<p>Given that machines are not inherently limited by such constraints, they could theoretically make more effective value judgments.Therefore, while providing some guidance to ensure that the generated question is relevant to humans is crucial, developing agents capable of autonomously constructing these value criteria themselves may also be fruitful.How to achieve this forms a significant open challenge in developing research-capable agents. 5</p>
<p>Origin of Information Need</p>
<p>I previously outlined that question formation begins with the recognition of an information need.This leads to the question: what triggers the recognition of an information need?</p>
<p>The initiation of this process can be attributed to various factors.Some researchers may generate questions through logical contemplation aimed at achieving a specific goal.Others might identify questions upon noticing anomalies in experimental data or inconsistencies between theoretical assumptions and actual observations.Researcher also sometimes search questions that can be answered by techniques that you have.Furthermore, humans typically do not rely on a singular criterion for value judgment.Instead, multiple criteria are often intricately combined and weighted according to the context, culminating in a complex value assessment process.To develop an agent capable of autonomously constructing research questions as humans do, therefore, it appears necessary to create a system with a general methodology for questioning applicable across these diverse scenarios.</p>
<p>The process in humans that connects these various triggers to an information need, and the development of an agent capable of emulating this process, remains an open question.Researchers in the field of curiosity, which is broadly conceptualized as a "drive state for information" [38], have been investigating this challenge.Curiosity is often characterized as a precursor to information need in information-seeking processes [37].In reinforcement learning, efforts to instill curiosity or knowledgebased intrinsic motivation in AI have been explored.Here, curiosity is defined in terms of novelty, information gain, or prediction error, and is considered a catalyst for exploration [39].</p>
<p>These efforts provide insights into implementing mechanisms that drive AI towards question formation.However, we are still distant from realizing a system that autonomously constructs research questions under complex value judgments, as humans do.A significant challenge lies in identifying the minimal input required for question generation; namely, while the minimal input is clear for hypothesis generation and hypothesis verification, it remains unclear for the construction of questions.Designing a complex, contextually adaptive internal driving force for questioning remains a significant hurdle.Identifying the prerequisites for an AI system with such a mechanism is an ongoing challenge.</p>
<p>Examples of Criteria for Evaluating Research Questions</p>
<p>Thus far, I have discussed abstract concepts related to questioning in general.Now, I will move on to focusing specifically on the characteristics pertinent to research questions.</p>
<p>The "quality" of a research question can be evaluated against various criteria.Here, I will briefly explore some examples to illustrate how humans seemingly appraise the value of a research question.It is important to note that these examples are only a few of the many criteria utilized and do not represent a comprehensive list.</p>
<p>One widely accepted criterion within the research community is that a question is important if it offers new perspectives, understandings, or conceptual advances, particularly those that challenge our common assumptions.For example, Alvesson and Sandberg emphasize the significance of such questions and discuss strategies for their construction [40].This criterion rests on the idea that a valuable question is one that significantly impacts our current knowledge.This seems to be a value that aligns with the highest-level objectives of the endeavor of research.</p>
<p>No matter how significant a question may be, if it is nearly impossible to address with current technology, deriving meaningful research outcomes from it may be unfeasible.Consequently, the feasibility of answering a question is considered a vital aspect of its quality [41,42,43].Assessing feasibility involves complex decision-making, taking into account factors like available resources, researcher capabilities, deadlines, and technological constraints.An agent engaged in research would need the capacity for such multifaceted evaluations.</p>
<p>Another prevalent view is that research questions should stem from individual intellectual curiosity.Given that curiosity drives exploration [44], curiosity-driven research can foster exploration in the knowledge space.Research can be seen as an exploration of the world's truths, making this value standard important.However, curiosity is not the sole criterion for exploration; there may be better criteria for uncovering unknown truths.If agents can adopt such criteria, it might surpass human efficiency in uncovering truths.</p>
<p>In contrast to bottom-up curiosity-driven research, questions that contribute to achieving specific top-down goals are also considered valuable.For example, in corporate or government-led research, questions aligned with predetermined objectives are prioritized.Since we expect that agents capable of autonomous research will contribute to human-set goals, ability to make such value judgments deemed important.</p>
<p>In practice, the value of a research question is determined by integrating multiple criteria.Hulley et al. suggest that questions which are feasible, interesting, novel, ethical, and relevant (FINER) are considered valuable [41].Huntington-Klein argues that a good research question is one that is answerable and whose answer enhances our understanding of the world [43].As mentioned above, autonomous agents are also expected to determine the questions they should pursue based on such complex value judgments.</p>
<p>As emphasized, these criteria represent only a portion of the value judgments humans make in question formulation.Future discussions should further investigate the nature of these judgments, their role in scientific discovery, and how they can be replicated in artificial researcher.</p>
<p>Hypothesis Generation</p>
<p>The second integral element of research is hypothesis generation.Research inherently involves posing questions and endeavoring to answer them.Typically, researchers bifurcate the answering process into two phases: generating hypotheses and verifying them.Hypothesis generation entails predicting the answer to a posed question, while hypothesis verification involves examining the plausibility of that prediction.This two-stage approach is adopted because researchers address questions to which no one in this world knows the answer, making it challenging to immediately ascertain definitive answers.Therefore, the separation of hypothesis generation and verification represents a humandeveloped methodology for uncovering truths in a context of high uncertainty.</p>
<p>Hypothesis generation is often seen as a showcase of human creativity in research.The longstanding belief that human creativity defies analysis has led to the assumption that both question construction and hypothesis generation are inherently unanalyzable [45].However, efforts to characterize this creative process began to emerge in the mid-20th century.Notable concepts include the role of abduction in generating hypotheses for "why" questions [46,47], the significance of analogical reasoning [48], the interpretation of scientific discovery as a form of search problem [4], and the conceptualization of hypothesis generation as probabilistic sampling [49].</p>
<p>The potential for machines to generate hypotheses has been a focal point in artificial intelligence research.Pioneering attempts to develop machines capable of hypothesis generation date back to the early 20th century [4,3].By the mid-2000s, advancements led to the creation of machines capable of making autonomous scientific discoveries [5].</p>
<p>Hypothesis Generation and Machine Learning</p>
<p>Generating hypotheses is predicting answers to questions from existing knowledge.This process essentially aligns closely with machine learning, particularly question-answering.As it involves predicting answers that even nobody knows, it can also be viewed as a prediction under significant distribution shifts.</p>
<p>Indeed, machine learning have become increasingly prominent in scientific hypothesis generation [8,9,7].Attempts such as predicting protein structures [50] and new materials [51] are all examples of hypothesis generation. 6he sources for hypotheses, the nature of the hypothesis space, and the representation of hypotheses differ across research fields.For instance, hypotheses can be represented combinatorially, and machines can be employed to explore these spaces to find hypotheses [12].Some studies represent hypotheses as symbolic equations and try to discover them from scientific data [52], while others endeavor to generate or extract textual hypotheses from academic papers [53,54,55,56,57].The advent of LLMs has spurred efforts to generate hypotheses from the models' internal knowledge, without relying on direct information from academic papers [58,10].</p>
<p>While the specifics of hypothesis generation vary, a unified description can be drawn from certain perspectives.Viewing the hypothesis space as a human-defined and fixed entity, scientific discoveries can often be framed as search problems [12].Since the hypothesis space is often combinatorially vast, strategies for efficient exploration in the space deemed necessary [11,1], and efforts have been made to optimize exploration with techniques such as active learning.As another perspective, Wang et al. provide a categorization of how AI is utilized in scientific hypothesis generation, highlighting its applications in black box prediction, aiding hypothesis space exploration, and finding solutions within a differentiable hypothesis space [7].</p>
<p>As such, integration of machine learning to hypothesis generation has progressed significantly compared to question formulation and hypothesis testing.The applications in this field are vast and diverse, and detailed evaluation of individual cases goes beyond the author's capacity.Therefore, this paper does not provide introductions to specific cases.Those interested are encouraged to refer to survey papers in their respective fields.</p>
<p>While machine learning's application in hypothesis generation is notable, the development of AI capable of generating complex hypotheses in response to varying questions remains a challenge.Achieving such capability may require abilities to generate hypothesis in versatile and flexible manner and to construct hypothesis spaces themselves.Future discussions are expected to further explore how to realize these capabilities in AI.</p>
<p>Speculation on Key Aspects of Hypothesis Generation by Machines</p>
<p>It can be said that autonomous hypothesis generation in general manner by AI has already gained considerable attention, compared to question generation and hypothesis verification.That's largely because, as previously mentioned, predicting answers to questions is a problem already central to many machine learning researchers.Therefore, many challenges in aiming for AI that generates hypotheses as flexibly as humans overlap with the challenges of pursuing an artificial general intelligence (AGI).These include systematic thinking such as deduction, out-of-distribution generalization, causal inference, efficient exploration, and problem decomposition, all crucial for autonomous flexible hypothesis generation and considered fundamental in the pursuit of AGI as well.</p>
<p>In this section, I will preliminary explore elements deemed important for AI's ability to generate hypotheses.However, due to the circumstances mentioned in the previous paragraph, this discussion might intersect with existing debates in the realm of AGI, potentially lacking novelty.Nonetheless, I will explore two aspects that appear vital for the development of an artificial hypothesis generator.</p>
<p>Firstly, it's important to note that even AI might not know the answers to the research questions.It's not always true that a question unknown to humans is also unknown to machines, as has been repeatedly emphasized.However, once the answer is unknown to both humans and the machine, hypothesis generation can be a challenging task even for AGI.I acknowledge that this essentially reduces to a problem of out-of-distribution generalization, but it's particularly challenging because no agents in this world know the answer.To solve such problems, machines, like humans, may need to recognize their ignorance of the answer, reduce uncertainty step-by-step, and gradually approach the answer.Current AI still does not even understand what it doesn't know [59,60].How AI capable of reasoning under such high uncertainty can be realized remains an open question.</p>
<p>Secondly, the role of mathematics in hypothesis generation cannot be overstated.The first point to note is that the power of mathematics in hypothesis generation lies significantly in its deductive nature.Deduction ensures that if the premises are true, the resulting conclusions are also true, even if they may seem counterintuitive.This aspect gives AI, which largely depends on experiential inferences, a substantial advantage.Furthermore, as humans do by the hypothetico-deductive method, deduction enables the evaluation of hypotheses that are not directly testable.If deductive results are rejected, the hypothesis is deemed false; acceptance, conversely, strengthens its plausibility.This plays a crucial role in expanding the empirical knowledge boundaries in research.</p>
<p>The abstract nature of mathematics is also important.Since ancient times, even before the formalization of deductive methods, mathematics engaged with concepts such as numbers, which are fundamentally abstract and have long captivated human interest [61].The introduction of symbolic representation and manipulation has further amplified its abstract nature.Significantly, mathematics not only abstracts real-world objects but also engages in a cycle of further abstraction.By abstracting already abstracted concepts, it has developed highly sophisticated systems [62].This level of abstraction allows for the reference to subjects not directly experienced, facilitating the progress in science [63].</p>
<p>These characteristics render mathematics an indispensable tool in the process of hypothesis generation.AI capable of doing mathematics has not yet been realized, but related research attempts have made steady progress [64,65].</p>
<p>While these two elements are discussed separately, systematic thinking seems necessary for both, reinforcing the widely acknowledged importance of systematic or high-level thought in AI development.However, due to the extensive existing discourse on this topic [66], I will not delve deeper into it here.</p>
<p>Due to my limitations, this paper only scratches the surface of this topic.I would appreciate any feedback from those with insights into elements not widely recognized in the machine learning community but deemed essential for autonomous hypothesis generation.</p>
<p>Hypothesis Verification</p>
<p>The final critical element in the research process is the verification of hypotheses.We justify our belief in the truth or falsehood of a hypothesis by confirming the plausibility of our prediction in response to a question through verification.Thus, verification is essential for generating knowledge.</p>
<p>Verification hinges on the nature of the question and hypothesis posed.For instance, a "why" question demands verification methods that elucidate causal relationships.Questions about the physical world require interaction with physical world for verification.In cases where hypotheses are amenable to mathematical proof, such proof constitutes verification.This necessitates an agent capable of verification to possess an understanding of what constitutes verification and to develop suitable verification methods tailored to the specific question and hypothesis.</p>
<p>While there has been extensive discussion on AI in hypothesis generation, its involvement in verification is less explored.Certainly, some studies have utilized AI in aspects of verification, such as experimental design [67] and scientific simulations [68].However, initiatives enabling AI to fully comprehend and independently execute verification processes akin to human scientific research are still limited.</p>
<p>In machine learning, research focusing on the validation of scientific claims [69], factual accuracy of predictions [70], evidence search to support hypotheses [71], and self-verification of machine responses [72] aligns with aspects of verification.Nevertheless, none of them aim to construct and execute verification as humans do in a scientific research.The automation of peer review [73,74] is also related to verification in the sense that it demands judgment on the validity of the verification, but it does not generate verification.</p>
<p>In this section, I aim to delve into the concept of verification to stimulate further contemplation.Having already addressed the nature of verification, or justification, in Section 2, I will omit that discussion here.Instead, I will focus on experimentation, an essential aspect of human-like verification in scientific inquiry.</p>
<p>Experimentation</p>
<p>No researcher would deny the importance of experiments.An experiment involves the planning and execution of a series of procedures to empirically test a hypothesis, essentially constituting the process of verification in empirical science.Therefore, any agent capable of verification must necessarily possess the ability to conduct experiments.</p>
<p>In experiments, phenomena that are difficult to observe, or the effects of various conditions, are precisely investigated.This is achieved by artificially generating phenomena in a controlled manner and actively intervening in them [75].Such interventions create the different observations of interest.These observations are recorded as experimental data, and subsequent analysis of the data determine the validity of the hypothesis. 7o conduct an experiment, one must first design it, document the procedures, and plan its execution.Planning requires an understanding of what constitutes a successful hypothesis test and the ability to devise methods to realize this using existing technology.For example, if the hypothesis pertains to the causes of a particular phenomenon, one must understand what causality is in the first place and how it can be identified in order to plan the verification process.</p>
<p>Preparations for the experiment are also essential.These preparations can include purchasing chemicals, preparing flasks, training animals, applying to ethics committees, constructing necessary equipment, and sometimes even building large apparatus like accelerators from scratch.Unfortunately, since research aims to uncover the unknown, constructing equipment from scratch for experiments is not uncommon in research.The autonomous execution of these preparations by a non-human agent from scratch seems almost infeasible.</p>
<p>After the preparation for the experiment is complete, the experiment is conducted according to the experimental protocol.This task also presents considerable challenges for autonomous machines.The reason is that even a single experiment requires a myriad of low-level operations such as grasping, cutting, carrying, mixing, moving, pouring, dispensing, washing, and opening lids.These operations need to be flexibly combined and executed according to the self-generated experimental protocol.An autonomous machine capable of conducting experiments must possess the ability to generate these operations flexibly in response to the experimental protocol.</p>
<p>Automating Experimentation</p>
<p>As we observe, the challenge of making a machine fully autonomous in planning, preparing, and executing experiments is considerable.Particularly, since the specific experiments to be conducted cannot be determined until questions and hypotheses are formulated, enabling a machine to autonomously conduct research from question construction demands the capability to accommodate many possible experimental scenarios.This, I believe, represents one of the greatest barriers to creating machines capable of autonomously conducting research.</p>
<p>Automating experiments is a daunting task, yet humanity has made steady progress in this area.In relation to the planning stage of experiments, the automation of exploring experimental conditions have a long-standing history, for example.Wang et al. have summarized these studies, which utilize AI to assist in experiment planning, research guidance, and generating observational data through numerical simulations [7].</p>
<p>Furthermore, there is an initiative known as laboratory automation or self-driving lab that aims to automate experiments, including their execution -an aspect previously mentioned as challenging [76,77].A notable example is the research in genetics by King et al., who fully automated the cycle of hypothesis generation, verification, and the discovery of new hypotheses [5].Another example is the work of A.I. Cooper, which facilitated the use of experimental equipment by autonomous robots, similar to human researchers [78].These are just a few examples, and there is a vast number of studies in this field.</p>
<p>These examples illustrate efforts to autonomously drive the research cycle, encompassing hypothesis generation, planning and execution of experiments, and generation of new hypotheses based on exper-imental results.Such endeavors are referred to as the closed-loop automation of scientific discovery [1], representing a significant milestone in achieving high autonomy in research automation.Additionally, there are efforts to develop humanoid robots capable of conducting multiple different experiments with a single robot, considered a foundational step towards more generalized research automation [79].</p>
<p>In recent years, there have been efforts to explore possibilities of autonomous experiment using LLMs [80,81,82].For instance, Boiko et al. developed an autonomous agent comprising multiple LLMs that successfully designed and executed complex scientific experiments [80].</p>
<p>While we have primarily discussed experimental data generation process, validation also requires interpretation of the data.Observation inherently involves theoretical underpinnings [46].Hence, interpreting experimental data necessitates adequate prior knowledge.Some studies are focused on enabling machine learning models to interpret scientific data by embedding physical prior knowledge, like symmetries, differential equations, and intuitive physics, into them [83,84]. 8arious research efforts have significantly advanced the automation of experiments.However, it is also true that numerous challenges remain in realizing machines capable of autonomously conducting experiments.Coley et al. delve into these challenges in the automation of experimental and computational validations and the selection of experiments, while referring to studies on automated verification [11].They also point out the significance of removing hardware constraints to increase the automatability of research projects and reducing the costs associated with research automation [11].Zenil et al. also discuss the challenges of automating experiments and propose specific action plans [6].</p>
<p>Among these challenges, the development of robots capable of manipulating low-level actions as humans do, which is necessary to achieve a versatile automated experimental machine adaptable to diverse research tasks, seems to be exceedingly challenging.How to address these challenges will require further discussion.</p>
<p>Additional Topics</p>
<p>Combining Question Formulation, Hypothesis Generation, and Hypothesis Verification</p>
<p>Reflecting retrospectively on completed studies, it becomes evident that each study possesses its unique question, an accompanying hypothesis, and a process for verifying that hypothesis.Viewed from this perspective, research can be considered an endeavor that involves a sequence of constructing questions, generating hypotheses, and verifying these hypotheses, as classically described.</p>
<p>However, as we know, actual research is a highly complex, cyclical process of trial and error.Rarely do these tasks unfold as initially planned or occur just once in a single study.In practice, for instance, numerous questions and hypotheses might be generated even when formulating a single hypothesis, and not all of these lead to the final research outcome. 9You will notice that even some major scientific discoveries throughout history were also made through these trials and errors [46,86,87].</p>
<p>Therefore, it is more accurate to view the construction of questions, the generation of hypotheses, and the verification of hypotheses as fundamental units for reducing uncertainty.In the research process, they are combined to gradually reduce the vast uncertainty inherent in the research endeavor.AI capable of conducting research is expected to master these flexible and complex operations.In this section, I will speculatively explore these characteristics of real research practice that have not been previously discussed</p>
<p>Countless Questions, Hypotheses, and Verifications in a Single Research Process</p>
<p>In the course of developing a single question or hypothesis, or in planning and preparing for a single verification, we generate countless questions and hypotheses, including implicit ones.Whether it's searching for problems, contemplating why a problem hasn't been solved, considering possible hypothesis candidates, planning verification, or doing anything else, we always pose questions and formulate hypotheses whenever dealing with unknowns or uncertainties.</p>
<p>We also conduct a form of verification, whether implicit or explicit, and with varying degrees of simplicity, to generate plausible hypotheses.Generating plausible hypotheses requires having sufficient grounds to believe in their validity.These grounds could include knowledge from our memory, insights from recently researched literature, opinions from other researchers, or a belief in the simplicity of natural laws.Furthermore, we might conduct simple tests or even preliminary experiments to assess their plausibility.All these function as verification for researchers to be convinced.</p>
<p>Agents capable of conducting research should autonomously generate numerous questions and hypotheses as needed, and select the more plausible hypotheses through simple verifications during the knowledge production process.These are inevitable as long as uncertainties exist.How to realize such flexible agents remains an open question.</p>
<p>Operations Apparently Unrelated to Knowledge Production</p>
<p>Research comprises numerous operations that may initially seem unrelated to knowledge production. 10In Section 3.3.1,I argue that such tasks are essential in the context of experimentation.Most researchers would concur that daily academic activities are primarily characterized by these operations.</p>
<p>The construction of questions, generation of hypotheses, and verification of these hypotheses represent the core aims and functions in the knowledge production process.To implement these functions, performing operations like those mentioned above and combining them effectively to achieve desired objectives is crucial.Appropriately integrating these varied operations poses a significant challenge, even when tailored to a specific research question [11].To develop an agent capable of autonomously executing the entire research process, starting from question generation, replicating the flexibility of human action is indispensable.</p>
<p>Discovering New Questions</p>
<p>Researchers often begin with a specific question, only to discover an entirely unrelated question during their investigation.This new question, divergent from the original and its underlying purpose, can lead to a shift in research focus and potentially significant scientific breakthroughs.Given the inherent unpredictability in research, such discovery and redirection of focus are not rare phenomena.</p>
<p>If an agent designed for conducting research were tasked with a singular objective, such serendipitous discoveries might be overlooked.This is because the agent, focused on its predefined goal, may disregard new questions not aligned directly with its initial objective, regardless of their scientific value.To facilitate the agent's identification of such unrelated questions, it may be necessary to assign multiple objectives or a broader, overarching goal that accommodates both the original and emergent questions.</p>
<p>On the other hand, having a common high-level goal alone is not sufficient.For an agent to transition from its current question to a newly discovered one, it must be capable of evaluating which question is more valuable.The decision-making process regarding the value of a question, as discussed in Section 3.1.3,should encompass comparing multiple questions that share higher-order objectives.The development of such evaluative capabilities remains a challenging and open question.</p>
<p>Incorporating Feedback from Verification Result</p>
<p>In research, it is uncommon that the initial hypothesis is the answer of the posed question.Typically, research entails revising the hypothesis based on verification results and conducting subsequent rounds of testing.This iterative cycle of hypothesis revision and retesting is critical for scientific discovery.Therefore, an agent designed for conducting research should possess the ability to revise its hypotheses based on these outcomes of verification.</p>
<p>Efforts to automate incorporating feedback from verification results include studies in closed-cycle laboratory automation, as discussed in Section 3.3 and automation with scientific workflow [89].These have significantly contributed to automating the hypothesis revision cycle.</p>
<p>Despite these advancements, challenges persist in developing machines that autonomously analyze and respond to verification results like humans.When verification results are negative, pinpointing the exact cause is complex.This complexity arises because verification relies on a web of implicit and explicit hypotheses, any of which could contribute to the result [90].The cause might be the primary hypothesis, underlying premises, auxiliary hypotheses, observations, experimental instruments, or a combination of these.A research-conducting agent must be capable of discerning the likely cause among these numerous candidates.Although it seems that humans do this well [91], it is a challenging task for machines to do this autonomously.</p>
<p>Moreover, appropriate interpretation of experimental data is essential.Interpretations can vary based on the researcher's beliefs, prior knowledge, theoretical framework, and expectations [46].Thus, verification results may undergo multiple reinterpretations, each potentially altering the hypothesis in need of revision.Additionally, as previously mentioned, researchers sometimes derive entirely different questions from these results and may temporarily halt their research.Current machines have yet to match this level of complex interpretation and adaptability in handling verification results that humans exhibit.An ideal autonomous research agent would be expected to possess these capabilities.</p>
<p>Common Topics</p>
<p>In the preceding sections, I have speculatively examined various key elements integral to research and their interplay.In this section, I aim to delve into topics that are universally relevant to these elements or that have not yet been explored in this paper.</p>
<p>Language Models</p>
<p>Recent years have witnessed rapid advancements in language models [92], opening new frontiers in research.The future of research agents is undoubtedly intertwined with the insights provided by these models.This section explores various initiatives investigating the potential of language models in research.</p>
<p>Beginning with the transformative impact of the Transformer [93] and BERT [94], the concept of "scaling law" [95] and "foundation model" [96] has catalyzed the development of large scale models pretrained on extensive corpora.This has led to the creation of scientific language models like SciBERT [97] and others [98,99,100,101,102,103,104,105,106,107].Given that scientific data is multimodal, attempts are emerging to construct general-purpose models using multimodal data as well [108,109,110,111].</p>
<p>The development of GPTs, including GPT-3 [112], InstructGPT [113], and GPT-4 [114] and the advent of the web application called ChatGPT [115] marked significant milestones.Their ability to perform various intellectual tasks has spurred research into their scientific applications.Emerging research examines the potential of LLMs, especially these GPTs, in various research fields, including the natural sciences [10,80,81,116,117,118,119,120,121,122,123,124,125,126], mathematics and engineering [127,128,129,130,131,132], and social sciences [71,133,134,135,136,137,138,139].</p>
<p>Additionally, there are efforts applicable to all research fields, which involve using GPTs for processing academic documents [140].Some of them include paper search and reading [141, 142], paper writing [143], abstract generation [144], literature review generation [145], and peer review [146,147,148]. 11 The scope of applications of LLMs ranges from generating hypotheses to generating research questions finding research challenges [32,24,33].Some studies even attempted to automate experimentation [80,81].</p>
<p>It is important to recognize that research and development in the field of language models are advancing rapidly.The potential applications for automating research are vast and constantly evolving, warranting further exploration and critical assessment of their impact on the scientific method.</p>
<p>Incorporating Scientific Knowledge</p>
<p>While discussing the challenges of enabling machines to conduct research, it's pertinent to acknowledge that even humans do not embark on research from a completely zero starting point.Firstly, humans are inherently equipped with brains and bodies evolved and developed for interpreting the world.Moreover, before engaging in research, we study the fundamental knowledge of the research fields.Thus, it would be reasonable to assume that agents should also possess basic knowledge before they begin conducting research.The advancement of language models has brought a wealth of knowledge to machines, but it is still considered necessary for them to acquire the knowledge required for research. 11Hosseini and Horbach discuss the influence of LLMs on the role of peer review [149].This is embodied in the concept of physics-informed machine learning [84], where biases and scientific knowledge are integrated into AI to process scientific data.Karniadakis et al. [84] and Hao et al. [83] provide a systematic overview of research in this domain.As previously discussed, imparting scientific knowledge through training on textual and multi-modal data is also a prevalent strategy.</p>
<p>Furthermore, just as humans continuously update their scientific knowledge, it's imperative for machines to not only embed knowledge through pre-training and inductive biases or retrieval during inference but also to continually update this knowledge.Specifically, it is crucial to acknowledge that knowledge produced by research is perpetually evolving.Thus, an agent must assimilate new knowledge, retain and update existing knowledge, and adapt to revisions in previously learned concepts.While the methodology for achieving this remains an open question, it is a subject of increasing debate [2,1].</p>
<p>Autonomy, Generality, and Open-Endedness</p>
<p>As previously emphasized, ongoing efforts are being made to enable machines to autonomously generate research questions, formulate hypotheses, and validate these hypotheses.However, the significant challenge remains in achieving this autonomy with minimal human intervention.Even in the realm of closed-loop research automation, which represents a substantial stride towards autonomy, full automation of all research processes is still an unrealized goal [6,12,11].</p>
<p>This problem becomes particularly serious when attempting to enable machines to independently formulate research objectives, problems, and questions.If machines are to autonomously generate goals and questions, they must also be capable of independently generating and validating corresponding hypotheses.This necessitates a versatile approach to hypothesis generation and validation, adaptable to a wide range of questions.</p>
<p>In such scenarios, humans cannot provide predefined methods, potential hypotheses, or necessary information.Consequently, machines must be equipped to extract pertinent information from an open-ended environment, mirroring the human approach.Research, in essence, is a process of seeking information from the vast outer world of scientific data and processing it within the agent's cognitive framework, as highlighted in [13].Assuming an open-ended environment means minimizing humanimposed constraints on this outer world and allowing the agent maximum freedom in selecting and processing information from the outer world.</p>
<p>Given these considerations, the extent to which autonomy should be expected from machines and the level of constraints that can be imposed without stifling their potential for autonomous hypothesis generation and validation, remains a critical and open question.</p>
<p>Scientific Understanding</p>
<p>While the primary focus of this paper has been on knowledge discovery, it's crucial to also consider another important goal of research: understanding.As Krenn et al. highlight, scientific discoveries can be made without understanding [150], suggesting that facilitating scientific understanding in humans by automated machines demands more than the things discussed so far.</p>
<p>Scientific understanding in humans involves comprehending theories or hypotheses -both their nature and their underlying rationale.Therefore, an additional requirement seems necessary in the context of hypothesis generation.It remains unclear whether this pertains to the representation of generated hypotheses, the description of their generation process, or anything else.Identifying what is needed to be added in this process to foster scientific understanding, and how to implement it, is a significant issue.</p>
<p>Krenn et al. propose two conditions for an AI to achieve scientific understanding: 1. the ability to "recognize qualitatively characteristic consequences of a theory without performing exact computations and use them in a new context", and 2. the capacity to "transfer its understanding to a human expert" [150].As previously mentioned, the belief systems of humans and AI may differ.Thus whether AI having scientific understanding is necessary for bringing new scientific understanding to humans is unclear.Nonetheless, the capability to communicate understanding to humans is essential for bringing scientific understanding to humans.The explanation of machine prediction results has already been extensively researched and discussed as explainable AI [151], and its importance has already been widely pointed out in AI for Science research, so I will not delve further into it here. 12</p>
<p>Alignment</p>
<p>Alignment is a critical concern in the development of autonomous AI researchers, akin to other areas of AI research.The primary concern is ensuring that these autonomous agents do not harm humans, a priority that becomes increasingly significant as we seek greater autonomy in machines.Given that knowledge is inherently value-neutral and can be used for benevolent or malevolent purposes, addressing this challenge is complex and necessitates ongoing discourse.</p>
<p>As previously discussed in Section 3.1, alignment with human values and worldviews is crucial not only for safety but also for the relevance and effectiveness of AI-generated knowledge for humans.AI agents need to make value judgments aligned with human assessments of question quality and discern what is unknown and comprehensible from a human perspective, not just from their own standpoint.</p>
<p>These value judgments are often not explicitly stated in human-generated texts, indicating a need for proactive teaching of these values to AI.The methodology for implementing such teaching and ensuring alignment in a broader sense remains a complex issue that warrants further discussion and exploration.This topic's complexity is amplified by the varying and evolving nature of human values and worldviews.As AI continues to advance, the challenge of continuously adapting these systems to align with human ethics and understanding becomes more pronounced.Future discussions should focus on developing robust frameworks and methodologies to achieve and maintain this alignment, ensuring that autonomous AI researchers contribute positively and safely to the scientific community.</p>
<p>Ideas for Prototyping</p>
<p>Realizing an autonomous intelligent agent capable of conducting research is an exceptionally challenging goal, one that will likely require a significant amount of time to achieve.The challenges discussed so far represent merely the tip of the iceberg found in speculative discussions; undoubtedly, many more critical issues remain unidentified.Therefore, it is crucial to begin by identifying these unknown challenges.A practical starting point might be the development of a simplified prototype of a researchcapable agent.Such a prototype would allow us to explore and understand the challenges inherent to our goal during the prototyping process.In this section, I aim to discuss, in a speculative and brief manner, what might constitute such prototyping.</p>
<p>Prototyping Agents that Conduct Research</p>
<p>Requirements for Prototype</p>
<p>As discussed in Section 3, Research seems to involve constructing questions, generating hypotheses, and verifying these hypotheses.Therefore, it appears appropriate for this prototype to incorporate these functions as distinct modules.The "question construction" module should take any input and formulate a question.The "hypothesis generation" module would then take this question as input and generate a hypothesis.Subsequently, the "hypothesis verification" module would take the hypothesis and provide verification results.By flexibly combining these modules at various levels, the prototype could mimic the research process.</p>
<p>For the prototype agent to function autonomously, human involvement in its design, implementation, and intervention should be minimized.Consequently, each module should autonomously gather information from the open-ended world, similar to how humans acquire information for research, while requiring minimal inputs.This means the agent should interact with the physical or digital realms to gather necessary information for research.</p>
<p>Moreover, to ensure the system's generality, the internal workings of each module should not overly rely on specific research topics.For instance, a verification method like experimentation tailored for specific physics research would not be applicable to psychological research.The human-designed elements of each module should be minimal, confined to what is essential for the module's function.</p>
<p>Creating a system that simultaneously meets the criteria of autonomy and generality, while effectively constructing questions, generating hypotheses, and verifying them within this abstract frameexploring not only the enhancement of machine capabilities but also the expansion of human cognitive boundaries.While delving into this might border on speculative, it's a discussion that could yield valuable insights in the scientific community.</p>
<p>work, is impractical, even in simpler scenarios.Thus, it may be necessary to introduce some constraints to this abstract framework.Discussing the nature, necessity, and potential relaxation of these constraints could shed light on the challenges in realizing an autonomous research agent.To initiate this discussion, I will present some candidates for potential constraints.</p>
<p>Candidate Constraints in Prototyping</p>
<p>In Section 2, I discussed the perspective of research as a process of updating beliefs and the potential for autonomously constructing verification from foundational concepts, as well as autonomously assessing the value of questions.However, these concepts are visionary and present significant challenges, making it unrealistic to expect immediate, meaningful outcomes for humans through prototyping.Therefore, it would be beneficial to start by prototyping agents that can master existing human values and research methods.</p>
<p>As mentioned in Section 3.1, formulating questions from open-ended situations is an exceptionally challenging task, often with even no clear starting point.A pragmatic approach would be to predetermine the inputs for question construction, rather than relying on unrestricted information sources.A viable input could be a high-level goal, commonly assumed in many studies.Specifically, it would be beneficial to start with high-level goals recognized as research objectives in specific research fields.</p>
<p>A significant obstacle in developing a fully autonomous research agent, as discussed in Section 4.1.2,is the need for expertise in complex low-level actions.Developing a robot capable of free physical world interaction like humans remains a formidable challenge.Hence, for prototyping, focusing initially on research confined to computational environments appears more manageable.While creating an agent capable of operating freely within a computer environment is also challenging, it is arguably more feasible than one operating in the physical world.Indeed, there have been efforts to enable language models to perform various computer operations [152,153], and to operate web browsers [154,155].</p>
<p>The primary objective of prototyping is to concretize a concept, however rudimentary, and to identify challenges.Thus, it seems prudent to initially limit the prototype's environment to the digital realm, while waiting for advancements in foundational research that could enable free activity in the physical world.</p>
<p>The ideas presented here are merely initial suggestions and are neither definitive nor exhaustive.In the prototyping phase, it is crucial to discuss the extent and nature of the constraints to be applied.More suitable constraints are likely to emerge as these discussions evolve.</p>
<p>Implementing Each Module with Large Language Models</p>
<p>Given the need for generality and considering the remarkable capabilities of LLMs, it seems inevitable that each module in prototypes would be instantiated as an LLM.As highlighted in previous sections, there are emerging studies focused on constructing automated research pipelines using LLMs.I propose that our initial prototyping efforts should focus on creating autonomous research agents modeled on these LLM pipelines, in alignment with current endeavors to develop autonomous agents utilizing language models [133,156].</p>
<p>Here is a provisional concept, modeled after a typical autonomous agent.The research agent begins by formulating a question based on a high-level goal provided by a human.Following the posing of this question, the agent autonomously generates hypotheses to address it, and then proceeds to verify these hypotheses.After obtaining the final verification results, they are analyzed in relation to the initial objective and research question, prompting the generation of subsequent questions.This process -comprising question formulation, hypothesis generation, and hypothesis verification -is iteratively and hierarchically repeated to conduct research.</p>
<p>The agent is envisioned to perform four fundamental actions: 1) Formulating questions, 2) Determining task completion, 3) Verifying hypotheses, and 4) Executing low-level computer operations.The processes of question formulation, hypothesis generation, and verification primarily involve executing these low-level computer operations.</p>
<p>When the agent opts to generate a question, it temporarily pauses its current task, such as hypothesis verification, and initiates hypothesis generation for the new question.Upon completing hypothesis generation, the agent decides whether to proceed with verification.Following verification, it updates the hypotheses based on the results.Whether or not the hypotheses are verified, the agent then resumes the higher-level process that was previously paused, incorporating the results of the low-level process.In this way, the agent continuously cycles through lower-level tasks of question construction, hypothesis generation, and verification until the highest-level hypothesis is formulated.When the highest-level hypothesis -the response to the original question -is ready, the agent always proceeds to its verification.</p>
<p>To ensure the system's adaptability to a wide range of research questions, the prompts given to the LLMs should be composed of only general instructions.For example, an instruction like "generate a hypothesis for the following question" is sufficiently generic to apply to any research question.However, providing such instructions alone is unlikely to spontaneously yield research outcomes, so there may be a need for additional auxiliary instructions that are as general as possible; identifying these is one of the main goals of prototyping.</p>
<p>For open-ended operations within a computer environment, ideally, the LLMs should have access to nearly all operations on the computer.As previously mentioned, initiatives to develop language models capable of executing any action in such environments are underway [153,152].Minimal access to web browsers, search engines, or shells may be permissible, but reliance on custom corpora or predefined hypothesis spaces should be avoided.Successful autonomous research under these conditions would indeed demonstrate the system's capacity for independent research.</p>
<p>Agents that Conduct Machine Learning Research</p>
<p>To effectively provide a high-level goal for the prototype, it is necessary to select objectives from a specific research field that align with the constraints previously outlined and are conducive to prototyping.</p>
<p>I propose that machine learning research is an ideal candidate for such prototyping.First, many aspects of machine learning research, including verification, can be conducted entirely on a computer, thus meeting the constraints we have established.Second, the field typically features shorter research cycles compared to other disciplines, which allows for more rapid feedback for the prototype.Third, machine learning not only forms a foundational technology across various research fields but is crucial for developing a research-capable agent itself.Automating machine learning research would thus not only contribute to the automation of research processes in numerous other areas but also advance our primary objective.Finally, there already have been significant efforts towards automation in machine learning, such as AutoML [157,158,159,160] and MLOps [161].Particularly in recent years, there have been attempts to utilize language models for these tasks [130,131,132].These existing efforts are likely to provide valuable support in developing the prototype.</p>
<p>In conclusion, initiating the prototyping of autonomous agents, composed of language models and given the most general instructions possible, and focusing on specific types of machine learning research appears to be a strategic choice.While such efforts are already underway, I anticipate that increased participation in this area will significantly accelerate this movement.</p>
<p>Prototyping Agents that Conduct Peer Review</p>
<p>To identify challenges associated with creating agents capable of conducting research, another promising initial step could be to target the automation of the academic peer review process.This approach presents several advantages, which I will detail in the following section.</p>
<p>Why Aim for Agents Capable of Conducting Peer Review?</p>
<p>Firstly, the competencies necessary for peer review closely align with those required by a researchcapable agent.This similarity arises because peer review fundamentally involves evaluating critical aspects of research, such as the soundness of the verification.</p>
<p>Secondly, automating peer review might present fewer challenges compared to developing a fully autonomous research agent.The distinction lies in the scope of tasks: peer review primarily entails assessing whether research incorporates the necessary elements, whereas a research-capable agent must not only evaluate but also synthesize these elements.As a preliminary step in prototyping, addressing simpler problems like peer review could effectively highlight key challenges.</p>
<p>Thirdly, peer review is a universal practice across various research fields.Insights gained from automating this process can thus contribute significantly to the development of a general research agent, applicable in multiple disciplines.</p>
<p>Fourthly, peer review predominantly involves textual analysis and does not require physical or extensive digital interactions, unlike conducting research autonomously.Although it may involve searches to review existing literature, tasks such as performing experiments are typically not necessary.Given the advancements in LLMs, we are now better equipped to handle complex textual tasks.This focus on text-based evaluation is beneficial for pinpointing specific challenges in achieving our broader objective.</p>
<p>Finally, peer review encompasses the evaluation of subjective aspects like the "significance" of a research question.As discussed in Section 3.1, understanding how humans assess such value in research is crucial, especially considering that alignment with human values is a significant challenge.Peer review offers a unique opportunity to observe and analyze these value judgments explicitly.Therefore, beginning with the automation of peer reviews could provide valuable insights into human evaluative processes in research.</p>
<p>Peer Review Automation</p>
<p>A considerable body of research has been devoted to automating various aspects of the peer review process.Efforts have included automating the generation of reviews [162,163,164], screening papers [165], assessing research papers [73], and assigning reviewers [166], among other tasks.In line with trends across other fields, recent years have witnessed a surge in studies exploring the use of LLMs for automating peer review [146,147,148,149].For a more comprehensive understanding of traditional research in this area, Kousha et al. [73] and Lin et al. [74] have conducted extensive literature reviews.</p>
<p>Considering the goals of prototyping, it is desirable that such efforts already exist.The insights and findings from these prior attempts would help further discussions on developing AI capable of conducting peer review.</p>
<p>Conclusion</p>
<p>In this paper, I have undertaken a speculative exploration of the concept of an artificial agent capable of conducting research.The initial discussion centered on characterizing what constitutes research, tentatively framing it as the process of updating beliefs in hypotheses.Subsequently, I delved into the critical elements of research: the construction of questions, generation of hypotheses, and their verification.This paper then briefly looks at the common themes to these elements.Following the discussions, I highlighted the significance of identifying challenges in realizing such agents and proposed preliminary ideas for prototyping.</p>
<p>It is important to acknowledge that the discussions in this paper are purely speculative.The definition of research provided is provisional, the challenges and implications discussed represent only a fraction of the myriad possibilities, and the ideas for prototyping are rudimentary, akin to early-stage experiments.Furthermore, the literature referenced is not exhaustive, omitting many pivotal works.My ability to evaluate each reference thoroughly may have been limited, potentially leading to partial perspectives or inaccuracies.I plan to update this paper in the future to address these limitations.I greatly value feedback and corrections from readers, as they will be crucial in improving this work.</p>
<p>The primary motivation for publishing this paper in its current nascent form, despite its numerous limitations, is to serve as an initial step towards future exploration into the concept of a researchcapable artificial agent.In order to realize agents capable of conducting research, there must still be many issues that need to be discussed.I hope that the discussion surrounding this concept will become more active to accelerate the development of research-capable agents.
 In this paper, the terms AI, machine, and agent are used interchangeably.
In this discussion, the process of recognizing an information need is described as initially identifying something as unknown and then deciding whether to formulate a question about it. However, the sequence of these steps is not fixed. For instance, one might first have a desire to know something and only afterward ascertain that it is indeed unknown. The critical aspect is that the process encompasses these two elements.
It is important to note that questions in research are not personal but societal in nature. This societal aspect may introduce slight variations in the question construction process. This point will be revisited later in the paper.
As mentioned earlier, something being unknown implies either the absence of a proposition or the presence of an unverified belief. Therefore, accurately determining the unknown from academic papers requires an assessment of whether each paper has been appropriately justified, meaning whether its verification processes are sound.
Kitano has described the approach where humans apply their value judgment criteria to determine questions and hypotheses as value-driven science[2]. He advocates for the advancement of exploration-driven science, which prioritizes extensive and comprehensive exploration. While a completely value-neutral exploration is unattainable, the notion of employing diverse and extensive criteria is indeed significant for the future of research. By embracing a wider range of criteria, we can expand the exploration space of knowledge.
Since there are a vast number of studies, I will skip the introduction of individual studies here.
Experiments are not conducted solely during the verification phase but also when generating hypotheses. Furthermore, new questions and hypotheses are often formulated based on the results obtained from experiments. In these instances, the process leading up to data generation, or conducting experiments not solely for verification but for data generation and some form of data analysis, seems to be what is referred to as an experiment. This paper defines an experiment as the planning, preparation, data generation, analysis, and determination of verification results. However, be aware that this definition may not always reflect actual practice.
Interpretation of scientific data is not solely for validation purposes. Thus, these technologies extend beyond just automating validation processes.
The trial-and-error nature of activities is particularly significant in the context of discovery[85].
Latour's anthropological study of daily practices in laboratories aptly illustrates these realities[88].
In considering how machine-driven scientific discoveries can facilitate human understanding, it might be worth</p>
<p>Additional Topics 4.1 Combining Question Formulation, Hypothesis Generation, and Hypothesis Verification . 4.1.1 Countless Questions, Hypotheses, and Verifications in a Single Research Process 4.1.2 Operations Apparently Unrelated to. </p>
<p>Autonomy, Generality, and. </p>
<p>5.1.3 Implementing Each Module with Large Language Models. </p>
<p>Agents that. </p>
<p>5.2.1 Why Aim for Agents Capable of Conducting Peer Review?. </p>
<p>The future of fundamental science led by generative closed-loop artificial intelligence. Hector Zenil, Jesper Tegnér, Felipe S Abrahão, Alexander Lavin, Vipin Kumar, Jeremy G Frey, Adrian Weller, Larisa Soldatova, Alan R Bundy, Nicholas R Jennings, arXiv:2307.075222023arXiv preprint</p>
<p>Nobel turing challenge: creating the engine for scientific discovery. Hiroaki Kitano, Systems Biology and Applications. 71292021</p>
<p>Dendral: a case study of the first expert system for scientific hypothesis formation. Bruce G Robert K Lindsay, Edward A Buchanan, Joshua Feigenbaum, Lederberg, Artificial intelligence. 6121993</p>
<p>Scientific discovery: Computational explorations of the creative processes. Pat Langley, 1987MIT press</p>
<p>Functional genomic hypothesis generation and experimentation by a robot scientist. Ross D King, Ffion M Kenneth E Whelan, Philip Gk Jones, Christopher H Reiser, Stephen H Bryant, Douglas B Muggleton, Stephen G Kell, Oliver, Nature. 42769712004</p>
<p>Hector Zenil, Ross D King, The Automated AI-driven Future of Scientific Discovery. World Scientific2023</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023</p>
<p>Artificial intelligence: A powerful paradigm for scientific research. Yongjun Xu, Xin Liu, Xin Cao, Changping Huang, Enke Liu, Sen Qian, Xingchen Liu, Yanjun Wu, Fengliang Dong, Cheng-Wei Qiu, The Innovation. 241001792021</p>
<p>Artificial intelligence for science in quantum, atomistic, and continuum systems. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, arXiv:2307.084232023arXiv preprint</p>
<p>arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023arXiv preprint</p>
<p>Autonomous discovery in the chemical sciences part ii: outlook. Natalie S Connor W Coley, Klavs F Eyke, Jensen, Angewandte Chemie International Edition. 59522020</p>
<p>Autonomous discovery in the chemical sciences part i: Progress. Natalie S Connor W Coley, Klavs F Eyke, Jensen, Angewandte Chemie International Edition. 59512020</p>
<p>A computational inflection for scientific discovery. Tom Hope, Doug Downey, Oren Etzioni, Eric Daniel S Weld, Horvitz, arXiv:2205.020072022arXiv preprint</p>
<p>Automated Research Workflows for Accelerated Discovery: Closing the Knowledge Discovery Loop. Medicine, 2022The National Academies PressNational Academies of Sciences Engineering,</p>
<p>What is this thing called science?. Alan F Chalmers, 2013Hackett Publishing</p>
<p>Scientific Method. Brian Hepburn, Hanne Andersen, The Stanford Encyclopedia of Philosophy. Edward N Zalta, 2021Metaphysics Research Lab, Stanford UniversitySummer 2021 edition</p>
<p>The craft of research. C Wayne, Gregory G Booth, Joseph M Colomb, Williams, 2003University of Chicago press</p>
<p>Guidelines for collecting and reporting data on research and experimental development. Frascati Manual, 2015</p>
<p>Distributed science-the scientific process as multi-scale active inference. Francesco Balzan-Francesco, John Campbell, Karl Friston, Maxwell James Ramstead, Daniel Friedman, Axel Constant, 2023OSF Preprints</p>
<p>Matthias Steup, Ram Neta, Epistemology, The Stanford Encyclopedia of Philosophy. Edward N Zalta, 2020Metaphysics Research Lab, Stanford UniversityFall 2020 edition</p>
<p>Is justified true belief knowledge? analysis. Edmund L Gettier, 196323</p>
<p>Thinking About Statistics: The Philosophical Foundations. Jun Otsuka, 2022Taylor &amp; Francis</p>
<p>A search engine for discovery of scientific challenges and directions. Dan Lahav, Jon Saad Falcon, Bailey Kuehl, Sophie Johnson, Sravanthi Parasa, Noam Shomron, Horng Duen, Diyi Chau, Eric Yang, Horvitz, Daniel S Weld, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Mapping the challenges of hci: An application and evaluation of chatgpt and gpt-4 for cost-efficient question answering. Jonas Oppenlaender, Joonas Hämäläinen, arXiv:2306.050362023arXiv preprint</p>
<p>Can questions summarize a corpus? using question generation for characterizing covid-19 research. Gabriela Surita, Rodrigo Nogueira, Roberto Lotufo, arXiv:2009.092902020arXiv preprint</p>
<p>Paperrobot: Incremental draft generation of scientific ideas. Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan, arXiv:1905.078702019arXiv preprint</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. Mario Krenn, Anton Zeilinger, Proceedings of the National Academy of Sciences. 11742020</p>
<p>Predicting the future of ai with ai: High-quality link prediction in an exponentially growing knowledge network. Mario Krenn, Lorenzo Buffoni, Bruno Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, Joao P Moutinho, Nima Sanjabi, arXiv:2210.008812022arXiv preprint</p>
<p>Liangming Pan, Wenqiang Lei, Tat-Seng Chua, Min-Yen Kan, arXiv:1905.08949Recent advances in neural question generation. 2019arXiv preprint</p>
<p>A review on question generation from natural language text. Ruqing Zhang, Jiafeng Guo, Lu Chen, Yixing Fan, Xueqi Cheng, ACM Transactions on Information Systems (TOIS). 4012021</p>
<p>A possibility for implementing curiosity and boredom in model-building neural controllers. Jürgen Schmidhuber, Proc. of the international conference on simulation of adaptive behavior: From animals to animats. of the international conference on simulation of adaptive behavior: From animals to animats1991</p>
<p>Creative research question generation for human-computer interaction research. Yiren Liu, Mengxia Yu, Meng Jiang, Yun Huang, Joint Proceedings of the ACM IUI Workshop. 2023</p>
<p>Adi Lahat, Eyal Shachar, Benjamin Avidan, Zina Shatz, Benjamin S Glicksberg, Eyal Klang, Evaluating the use of large language model in identifying top research questions in gastroenterology. Scientific reports. 2023134164</p>
<p>What is a question. Royal Institute of Philosophy Supplements. Lani Watson, 202189</p>
<p>The process of asking questions. Taylor Robert, American documentation. 1341962</p>
<p>Information behaviour: an interdisciplinary perspective. Information processing &amp; management. Tom D Wilson, 199733</p>
<p>Looking for information: A survey of research on information seeking, needs, and behavior. O Donald, Lisa M Case, Given, 2016Emerald Group Publishing</p>
<p>The psychology and neuroscience of curiosity. Celeste Kidd, Benjamin Y Hayden, Neuron. 8832015</p>
<p>A survey on intrinsic motivation in reinforcement learning. Arthur Aubret, Laetitia Matignon, Salima Hassas, arXiv:1908.069762019arXiv preprint</p>
<p>Constructing research questions: Doing interesting research. Mats Alvesson, Jorgen Sandberg, Sage. 2013</p>
<p>Designing clinical research. Stephen B Hulley, 2007Lippincott Williams &amp; Wilkins</p>
<p>How to choose a good scientific problem. Uri Alon, Molecular cell. 3562009</p>
<p>The effect: An introduction to research design and causality. Nick Huntington-Klein, 2021CRC Press</p>
<p>Pierre-Yves Oudeyer, arXiv:1802.10546Computational theories of curiosity-driven learning. 2018arXiv preprint</p>
<p>Scientific Discovery. Jutta Schickore, The Stanford Encyclopedia of Philosophy. Edward N Zalta, Uri Nodelman, 2022Metaphysics Research Lab, Stanford UniversityWinter 2022 edition</p>
<p>Patterns of discovery: An inquiry into the conceptual foundations of science. Norwood Russell, Hanson , CUP Archive. 1965</p>
<p>Abduction, reason and science: Processes of discovery and explanation. Lorenzo Magnani, 2011Springer Science &amp; Business Media</p>
<p>Analogy in scientific discovery: The case of johannes kepler. Dedre Gentner, Model-based reasoning. Springer2002</p>
<p>Where do hypotheses come from? Cognitive psychology. Ishita Dasgupta, Eric Schulz, Samuel J Gershman, 201796</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Nature. 59678732021</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Nature. 2023</p>
<p>Automated scientific discovery: From equation discovery to autonomous discovery systems. Stefan Kramer, Mattia Cerrato, Sašo Džeroski, Ross King, arXiv:2305.022512023arXiv preprint</p>
<p>Augmenting scientific creativity with an analogical search engine. Xin Hyeonsu B Kang, Tom Qian, Dafna Hope, Joel Shahaf, Aniket Chan, Kittur, ACM Transactions on Computer-Human Interaction. 2022</p>
<p>Solvent: A mixed initiative system for finding analogies between research papers. Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, Aniket Kittur, Proceedings of the ACM on Human-Computer Interaction. 22018</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592023arXiv preprint</p>
<p>Exploring and verbalizing academic ideas by concept co-occurrence. Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou, arXiv:2306.022822023arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Can chatgpt be used to generate scientific hypotheses. Yang Jeong, Park , Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, Ju Li, arXiv:2304.122082023arXiv preprint</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, arXiv:2005.006612020arXiv preprint</p>
<p>The history of mathematics an introduction. David Burton, 2010McGraw-Hill Professional</p>
<p>The role of mathematics in the rise of science. Salomon Bochner, Banesh Hoffmann, American Journal of Physics. 3661968</p>
<p>Abstraction in modern science. Werner Heisenberg, Nishina Memorial Lectures. 2008</p>
<p>Towards the automatic mathematician. N Markus, Christian Rabe, Szegedy, Automated Deduction-CADE 28: 28th International Conference on Automated Deduction, Virtual Event. Springer International PublishingJuly 12-15, 2021. 202128</p>
<p>Shima Imani, Liang Du, Harsh Shrivastava, Mathprompter, arXiv:2303.05398Mathematical reasoning using large language models. 2023arXiv preprint</p>
<p>Inductive biases for deep learning of higher-level cognition. Anirudh Goyal, Yoshua Bengio, Proceedings of the Royal Society A. 478202100682266. 2022</p>
<p>Bayesian experimental design: A review. Kathryn Chaloner, Isabella Verdinelli, Statistical science. 1995</p>
<p>Basic research needs workshop for scientific machine learning: Core technologies for artificial intelligence. Baker, Document prepared for Department of Energy Advanced Scientific Computing Research. 102019</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, arXiv:2004.149742020arXiv preprint</p>
<p>A survey on automated fact-checking. Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Can large language models discern evidence for scientific hypotheses? case studies in the social sciences. Sai Koneru, Jian Wu, Sarah Rajtmajer, arXiv:2309.065782023arXiv preprint</p>
<p>Chain-of-verification reduces hallucination in large language models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston, arXiv:2309.114952023arXiv preprint</p>
<p>Kayvan Kousha, Mike Thelwall, arXiv:2212.06574Artificial intelligence technologies to support research assessment: A review. 2022arXiv preprint</p>
<p>Automated scholarly paper review: possibility and challenges. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Xiaodong Shi, arXiv:2111.075332021arXiv preprint</p>
<p>The philosophy of scientific experimentation: a review. Hans Radder, 20091Automated experimentation</p>
<p>Automation in the life science research laboratory. Ian Holland, Jamie A Davies, Frontiers in Bioengineering and Biotechnology. 85717772020</p>
<p>The rise of self-driving labs in chemical and materials sciences. Milad Abolhasani, Eugenia Kumacheva, Nature Synthesis. 2023</p>
<p>Rob Clowes, et al. A mobile robotic chemist. Benjamin Burger, Vladimir V Phillip M Maffettone, Catherine M Gusev, Yang Aitchison, Xiaoyan Bai, Xiaobo Wang, Li, Buyi Ben M Alston, Li, Nature. 58378152020</p>
<p>Robotic crowd biology with maholo labdroids. Nozomu Yachie, Tohru Natsume, Nature biotechnology. 3542017</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Gpt-lab: Next generation of optimal chemistry discovery by gpt driven robotic lab. Mingda Xiaokai Qin, Yangguan Song, Zhehong Chen, Jing Ai, Jiang, arXiv:2309.167212023arXiv preprint</p>
<p>Generation next: Experimentation with ai. Gary Charness, Brian Jabarian, John A List, 2023National Bureau of Economic ResearchTechnical report</p>
<p>Zhongkai Hao, Songming Liu, Yichi Zhang, Chengyang Ying, Yao Feng, Hang Su, Jun Zhu, arXiv:2211.08064Physics-informed machine learning: A survey on problems, methods and applications. 2022arXiv preprint</p>
<p>Physics-informed machine learning. George Em Karniadakis, G Ioannis, Lu Kevrekidis, Paris Lu, Sifan Perdikaris, Liu Wang, Yang, Nature Reviews Physics. 362021</p>
<p>A hypothesis is a liability. Itai Yanai, Martin Lercher, 2020</p>
<p>On The Origin of Evolution: Tracing 'Darwin's Dangerous Idea'from Aristotle to DNA. John Gribbin, Mary Gribbin, 2022Rowman &amp; Littlefield</p>
<p>Before the principia: The maturing of newton's thoughts on dynamical astronomy. Derek Thomas, Whiteside , Journal for the History of Astronomy. 111970</p>
<p>Science in action: How to follow scientists and engineers through society. Bruno Latour, 1987Harvard university press</p>
<p>Will ai write scientific papers in the future? AI Magazine. Yolanda Gil, 202242</p>
<p>Underdetermination of Scientific Theory. Kyle Stanford, The Stanford Encyclopedia of Philosophy. Edward N Zalta, Uri Nodelman, 2023Metaphysics Research Lab, Stanford UniversitySummer 2023 edition</p>
<p>Autonomous experiments using active learning and ai. Zekun Zhichu Ren, Zhen Ren, Tonio Zhang, Ju Buonassisi, Li, Nature Reviews Materials. 892023</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>Specter: Document-level representation learning using citation-informed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S Weld, arXiv:2004.071802020arXiv preprint</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, arXiv:2211.133082022arXiv preprint</p>
<p>Scientific language models for biomedical knowledge base completion: an empirical study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, arXiv:2106.097002021arXiv preprint</p>
<p>Matscibert: A materials domain language model for text mining and information extraction. Tanishq Gupta, Mohd Zaki, Krishnan, Computational Materials. 811022022</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, arXiv:2310.06786Llemma: An open language model for mathematics. 2023arXiv preprint</p>
<p>Darwin series: Domain specific large language models for natural science. Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, arXiv:2308.135652023arXiv preprint</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Gatortron: A large clinical language model to unlock patient information from unstructured electronic health records. Xi Yang, Aokun Chen, Nima Pournejatian, Chang Hoo, Kaleb E Shin, Christopher Smith, Colin Parisien, Cheryl Compas, Mona G Martin, Ying Flores, Zhang, arXiv:2203.035402022arXiv preprint</p>
<p>Learning a foundation language model for geoscience knowledge understanding and utilization. Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Le Zhou, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, arXiv:2306.050642023arXiv preprint</p>
<p>Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao, arXiv:2306.00890Llava-med: Training a large language-and-vision assistant for biomedicine in one day. 2023arXiv preprint</p>
<p>Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, arXiv:2307.14334Ira Ktena, et al. Towards generalist biomedical ai. 2023arXiv preprint</p>
<p>Foundation model for material science. Seiji Takeda, Akihiro Kishimoto, Lisa Hamada, Daiju Nakano, John R Smith, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, Aditya Grover, arXiv:2301.10343Climax: A foundation model for weather and climate. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Version of the Generative Pretrained Transformer. 2023OpenAIGpt-4</p>
<p>. Openai, Chatgpt, 2023</p>
<p>Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Do large language models know chemistry?. Glen M Andrew D White, Heta A Hocky, Mehrad Gandhi, Sam Ansari, Cox, Subarna Geemi P Wellawatte, Ziyue Sasmal, Kangxin Yang, Yuvraj Liu, Singh, 2022ChemRxiv</p>
<p>Prompt engineering of gpt-4 for chemical research: what can/cannot be done?. Kan Hatakeyama-Sato, Naoki Yamane, Yasuhiko Igarashi, Yuta Nabae, Teruaki Hayakawa, 2023ChemRxiv</p>
<p>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Kevin Maik, Jablonka , Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bocarsly, Stefan Andres M Bran, Catherine Bringuier, Kamal Brinson, Defne Choudhary, Circi, Digital Discovery. 252023</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Large language models for scientific synthesis, inference and explanation. Yizhen Zheng, Yee Huan, Jiaxin Koh, Anh Tn Ju, Lauren T Nguyen, Geoffrey I May, Shirui Webb, Pan, arXiv:2310.079842023arXiv preprint</p>
<p>Can large language models empower molecular property prediction?. Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu, arXiv:2307.074432023arXiv preprint</p>
<p>Large language models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery. Magdalena Wysocka, Oskar Wysocki, Maxime Delmas, Vincent Mutel, Andre Freitas, arXiv:2305.178192023arXiv preprint</p>
<p>Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>
<p>Are large language models ready for healthcare? a comparative study on clinical language understanding. Yuqing Wang, Yun Zhao, Linda Petzold, arXiv:2304.053682023arXiv preprint</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Sebastian Bordt, Ulrike Von, Luxburg , arXiv:2303.09461Chatgpt participates in a computer science exam. 2023arXiv preprint</p>
<p>Yiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Chi Wang, arXiv:2306.01337An empirical study on challenging math problem solving with gpt-4. 2023arXiv preprint</p>
<p>Performance of chatgpt on the us fundamentals of engineering exam: Comprehensive assessment of proficiency and potential implications for professional environmental engineering practice. Vinay Pursnani, Yusuf Sermet, Kurt Musa, Ibrahim Demir, Computers and Education: Artificial Intelligence. 1001832023</p>
<p>Can gpt-4 perform neural architecture search?. Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, Samuel Albanie, arXiv:2304.109702023arXiv preprint</p>
<p>Automl-gpt: Automatic machine learning with gpt. Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mingyuan Zhou, arXiv:2305.024992023arXiv preprint</p>
<p>Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, arXiv:2308.12261Tongshuang Wu, and Graham Neubig. Prompt2model: Generating deployable models from natural language instructions. 2023arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023arXiv preprint</p>
<p>Can generative ai improve social science? SocArXiv. A Christopher, Bail, 2023</p>
<p>Can large language models transform computational social science?. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, arXiv:2305.035142023arXiv preprint</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Large language models as simulated economic agents: What can we learn from homo silicus?. John J Horton, 2023National Bureau of Economic ResearchTechnical report</p>
<p>Generative ai for economic research: Use cases and implications for economists. Anton Korinek, Journal of Economic Literature. 6142023</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. Rosa I Gati V Aher, Adam Arriaga, Kalai Tauman, International Conference on Machine Learning. PMLR2023</p>
<p>Adhari Alzaabi, Amira Alamri, Halima Albalushi, Ruqaya Aljabri, and Abdulrahman Aal-Abdulsallam. Chatgpt applications in academic research: A review of benefits, concerns, and recommendations. 2023</p>
<p>SCISPACE. </p>
<p>Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, and Steinn Steingrimsson. Can gpt-3 write an academic paper on itself, with minimal human input? HAL Open Science. 2022</p>
<p>Comparing scientific abstracts generated by chatgpt to real abstracts with detectors and blinded human reviewers. Catherine A Gao, Frederick M Howard, Nikolay S Markov, Emma C Dyer, Siddhi Ramesh, Yuan Luo, Alexander T Pearson, NPJ Digital Medicine. 61752023</p>
<p>Openai chatgpt generated literature review: Digital twin in healthcare. Ömer Aydın, Enis Karaarslan, SSRN 4308687. 2022</p>
<p>Can large language models provide useful feedback on research papers? A large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, Daniel Mcfarland, James Zou, arXivpreprintarXiv:2310.017832023</p>
<p>Ryan Liu, Nihar B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023arXiv preprint</p>
<p>Gpt4 is slightly helpful for peer-review assistance: A pilot study. Zachary Robertson, arXiv:2307.054922023arXiv preprint</p>
<p>Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. Mohammad Hosseini, Serge Pjm Horbach, Research Integrity and Peer Review. 8142023</p>
<p>On scientific understanding with artificial intelligence. Mario Krenn, Robert Pollice, Si Yue Guo, Matteo Aldeghi, Alba Cervera-Lierta, Pascal Friederich, Gabriel Dos Passos, Florian Gomes, Adrian Häse, Akshatkumar Jinich, Nigam, Nature Reviews Physics. 4122022</p>
<p>Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion. Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, 202058</p>
<p>. KillianLucas. Open interpreter. 2023MIT</p>
<p>. Openai, 2023Chatgpt plugins -code interpreter</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Act-1: Transformer for actions. Adept. 2022</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Automated machine learning: methods, systems, challenges. Frank Hutter, Lars Kotthoff, Joaquin Vanschoren, 2019Springer Nature</p>
<p>Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, Theresa Ullmann, Marc Becker, Anne-Laure Boulesteix, Data Mining and Knowledge Discovery. 132e14842023Wiley Interdisciplinary Reviews</p>
<p>Best practices for scientific research on neural architecture search. Marius Lindauer, Frank Hutter, The Journal of Machine Learning Research. 2112020</p>
<p>Colin White, Mahmoud Safari, Rhea Sukthanker, Binxin Ru, Thomas Elsken, Arber Zela, Debadeepta Dey, Frank Hutter, arXiv:2301.08727Neural architecture search: Insights from 1000 papers. 2023arXiv preprint</p>
<p>Machine learning operations (mlops): Overview, definition, and architecture. Dominik Kreuzberger, Niklas Kühl, Sebastian Hirschl, 2023IEEE Access</p>
<p>Can we automate scientific reviewing. Weizhe Yuan, Pengfei Liu, Graham Neubig, Journal of Artificial Intelligence Research. 752022</p>
<p>Kid-review: Knowledge-guided scientific review generation with oracle pre-training. Weizhe Yuan, Pengfei Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Ji Heng, Nazneen Fatema, Rajani , arXiv:2010.06119Reviewrobot: Explainable paper review generation based on knowledge synthesis. 2020arXiv preprint</p>
<p>Is the future of peer review automated?. Robert Schulz, Adrian Barnett, René Bernard, Nicholas Jl Brown, Jennifer A Byrne, Peter Eckmann, Ma Gazda, Halil Kilicoglu, Eric M Prager, Maia Salholz-Hillel, 2022BMC Research Notes15</p>
<p>Reviewer assignment algorithms for peer review automation: A survey. Xiquan Zhao, Yangsen Zhang, Information Processing &amp; Management. 5951030282022</p>            </div>
        </div>

    </div>
</body>
</html>
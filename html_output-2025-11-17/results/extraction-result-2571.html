<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2571 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2571</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2571</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-28e66d188efbd0bbb64242b611d96769be910c15</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28e66d188efbd0bbb64242b611d96769be910c15" target="_blank">Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Cybernetics</p>
                <p><strong>Paper TL;DR:</strong> A survey of different approaches to problems related to multiagent deep RL (MADRL) is presented, including nonstationarity, partial observability, continuous state and action spaces, multiagent training schemes, and multiagent transfer learning.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms, however, have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This article addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multiagent deep RL (MADRL) is presented, including nonstationarity, partial observability, continuous state and action spaces, multiagent training schemes, and multiagent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to the future development of more robust and highly useful multiagent learning methods for solving real-world problems.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2571.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2571.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RIAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforced Inter-Agent Learning (RIAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent communication method where each agent uses a recurrent deep Q-network to learn policies under partial observability; agents learn independently with local recurrent architectures and learn communication behaviors implicitly via their networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to communicate with deep multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RIAL (Reinforced Inter-Agent Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Decentralized multi-agent system in which each agent is implemented with a recurrent deep Q-network (RNN/DQN). Agents learn their own parameters independently; communication is supported through message channels learned by the networks in training, but training itself remains largely decentralized per-agent in RIAL.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified in the survey (agents are homogeneous in architecture; each agent learns its own policy via recurrent DQN).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>execution (policy learning and action selection under partial observability); implicit communication learning during training</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Decentralized independent learning with learned inter-agent messaging (agents do not rely on a centralized controller during execution).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Learned messages passed between agents via differentiable channels during training (paper describes end-to-end learned communication but not an explicit message format).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Agents receive environment rewards and update their own recurrent DQN parameters; no central critic is described for RIAL in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step / during episodes (communication is learned as part of stepwise interactions under partial observability).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cooperative partially observable multi-agent tasks (communication learning tasks / riddles / cooperative games).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports that RIAL is one of the proposed methods for communication learning; no numeric performance metrics are provided in this review for RIAL specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared in the literature (per survey) against DIAL and other communication-learning methods; the review does not list numeric baselines here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Allows agents to learn to communicate to solve cooperative tasks under partial observability; improves coordinated behavior relative to independent agents in the original studies (qualitative in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Independent learning can be unstable under non-stationarity; implicit communication channels may be hard to interpret and require careful training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not reported in this survey for RIAL specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey does not specify optimal hyperparameters or configurations for RIAL beyond noting the use of recurrent DQN architectures and learned communication channels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2571.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Inter-Agent Learning (DIAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent learning approach that enables end-to-end gradient-based learning of communication protocols by passing differentiable messages (and gradients) between agents during centralized training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to communicate with deep multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DIAL (Differentiable Inter-Agent Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agents are realized as neural networks that communicate through differentiable channels; during training gradients are propagated between agents via the communication channel to enable end-to-end learning of both action policies and communication protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not explicitly specialized in the survey; agents are neural-policy learners that both produce actions and messages.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Policy and communication learning (training) and decentralized execution (agents act using learned message protocols).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>End-to-end gradient-based coordination during training; decentralized execution using learned messaging rules.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Differentiable real-valued messages exchanged between agents; training uses message channels that allow gradient flow between agent networks (no fixed explicit message schema provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Environment rewards and gradient signals propagated across agents during training; message-generation networks updated via backpropagation across agent boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step during episodes (communication learned as part of each interaction).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cooperative partially-observable multi-agent tasks requiring learned communication (e.g., riddles and communication emergent tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states DIAL enables end-to-end backprop across agents and improves learning of communication protocols but does not provide numeric metrics in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Described as an advance over RIAL (which learns without cross-agent backprop) in the original work; the survey does not reproduce numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Enables direct gradient flow between agents so that message formats and action policies co-adapt, improving emergence of effective communication protocols for coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Requires differentiable communication channel during training (not always applicable when messages must be discrete at execution), and may not be applicable where centralized gradient exchange is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this review for DIAL specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey notes DIAL's key design is gradient passing for communication; no specific optimal hyperparameters are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2571.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CommNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Communication Neural Network (CommNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture that learns continuous communication among cooperating agents by aggregating and broadcasting continuous message vectors to enable coordinated policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CommNet</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A centralized-during-training architecture in which agents produce continuous vector messages that are aggregated (e.g., summed/averaged) and redistributed to agents as additional inputs; the communication channel and policy networks are learned jointly to produce coordinated behavior for fully cooperative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (designed to scale to multiple agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>In reported uses agents are typically homogeneous policy learners (manager/participant/mediator roles were described as application-level types in one task allocation use-case).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Policy and communication learning; execution uses learned decentralized policies with message-processing layers.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Implicit centralized aggregation of messages during training; decentralized policies conditioned on aggregated comms at execution.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Continuous (real-valued) vector messages passed through learned neural-message layers (format is learned; not an explicit structured format in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Agents receive environment rewards and update policy and communication weights via centralized gradient-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step (communication is learned as a stepwise process during episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cooperative tasks requiring coordination and information sharing such as distributed task allocation and small-scale coordination benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports CommNet facilitates learned communication for cooperative tasks; specific numeric metrics are not reproduced in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used in the survey as an approach contrasted with other communication-learning methods; direct numeric baselines not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Learns continuous communication channels that improve task allocation and coordination relative to non-communicating baselines (qualitative in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>May struggle with heterogeneous agents and has computational/communication overheads in decentralized reallocation scenarios per the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not reported in the survey for CommNet specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey does not specify optimal architectures; emphasizes joint learning of comms and policies and that CommNet favors continuous-vector messaging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2571.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MADDPG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Deep Deterministic Policy Gradient (MADDPG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic multi-agent algorithm that uses centralized critics with access to other agents' observations/actions during training and decentralized actors for execution, enabling learning in mixed cooperative-competitive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MADDPG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each agent has a decentralized actor (policy) conditioned on local observations; during training each agent's critic is centralized and takes as input global/augmented state including other agents' observations and actions to stabilize learning and mitigate non-stationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (scales to multiple agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents are policy learners (actors); critics are centralized per-agent learners used only during training. No task-role specialization is mandated by the algorithm description in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training (centralized critics) and execution (decentralized actors for deployment).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized learning of critics with decentralized execution of actors; coordination emerges via centralized critic guidance during training.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>During execution, no inter-agent communication is required; during training critics use augmented joint observations/actions (no explicit message-passing protocol described in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Centralized critic provides value/TD learning signals to update actor policies; environment rewards are used with centralized critics to compute gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Centralized critic uses full-episode or per-step joint information during training; agents do not communicate during decentralized execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cooperative and mixed cooperative-competitive multi-agent tasks with continuous action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states MADDPG can learn continuous policies effectively and is suitable for continuous-action multi-agent problems; no numeric metrics are reproduced in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Contrasted to COMA (which is discrete-action focused) and independent learners; the survey reports MADDPG's ability to learn continuous policies but does not give specific numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Centralized critics reduce non-stationarity during training, improving learning stability and enabling continuous-action coordination where independent learning would fail.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Requires access to other agents' observations/actions during training (centralized information), which may be infeasible in some settings; scalability and privacy concerns for many agents.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not reported in this survey for MADDPG specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey highlights centralized-critic + decentralized-actor as a recommended configuration for continuous-action multi-agent problems; no numeric hyperparameter prescriptions provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2571.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual Multi-Agent (COMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic multi-agent algorithm that addresses multi-agent credit assignment by using a centralized critic and a counterfactual baseline to compute each agent's advantage contribution to a global reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counterfactual multiagent policy gradients</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>COMA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Decentralized actor policies for each agent; centralized critic computes a counterfactual baseline by marginalizing a single agent's action to estimate its contribution, enabling more precise credit assignment in cooperative tasks with global rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (designed for cooperative multi-agent teams)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents are action-policies (actors) that receive local observations; no further specialization described in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training (centralized critic and counterfactual advantage estimation) and execution (decentralized actors).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized critic computes counterfactual advantages for individual agents to shape decentralized policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Centralized critic requires access to joint action/state information during training; execution is decentralized without communication per the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Counterfactual advantage signals from the centralized critic are used as feedback to each agent's policy gradient updates to provide attribution of global rewards to individual actions.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Counterfactual computations occur during training at each learning update step; execution requires no ongoing communication.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cooperative multi-agent tasks with global/team reward where credit assignment is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey indicates COMA addresses credit assignment effectively; no numerical performance figures are provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Presented as an advance over naive centralized critics and independent learners for credit assignment; specific numeric baselines not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Improves multi-agent credit assignment, enabling better cooperative policy learning from global rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Focused on discrete action spaces per survey; computational/estimation complexity may grow with many agents.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not described in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey notes COMA's central insight is the counterfactual baseline for advantage estimation; no further configuration recommendations are given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2571.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DDRQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Distributed Recurrent Q-Network (DDRQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of DRQN for multi-agent partially observable settings that uses last-action inputs, inter-agent weight sharing, and disables experience replay to enable coordination under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to communicate to solve riddles with deep distributed recurrent Q-networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DDRQN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent recurrent Q-network architecture where agents are implemented with recurrent networks; key features include feeding last action as input, sharing network weights among agents (when action sets equal), and disabling experience replay to avoid non-stationarity introduced by replay.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (suitable for multiple cooperating agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents can be homogeneous when weight sharing is used; heterogeneous action sets preclude weight sharing per the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training in partially observable multi-agent tasks and decentralized execution using learned recurrent policies.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Coordination via shared recurrent network parameters (weight sharing) and inclusion of last-action inputs to condition policy on personal history and implicitly on other agents' behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Implicitâ€”agents condition on histories and last actions rather than explicit inter-agent message channels in the DDRQN formulation described in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Standard Q-learning style TD updates on recurrent networks using environment rewards; experience replay is disabled to reduce non-stationarity artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step via recurrent state and last-action inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-agent partially observable coordination problems and cooperative puzzles/riddles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes DDRQN's empirical success in multi-agent PO problems and cites architectural features as important; no numerical metrics are included in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Described as an extension of DRQN for multi-agent settings and compared in original studies to naive DQN/DRQN baselines; the survey does not reproduce numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Weight sharing and last-action conditioning reduce parameter counts and help agents coordinate in PO settings; disabling replay mitigates replay-induced non-stationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Assumes equal action sets for weight sharing; heterogeneous agent action spaces prevent weight sharing and complicate scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Survey highlights three features (last-action input, weight sharing, disabling replay) as crucial, implying ablation of these reduces performance, but specific ablation numbers are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey recommends using last-action inputs and inter-agent weight sharing when agents share action spaces, and disabling experience replay in concurrent multi-agent recurrent training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2571.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPIQN / DRPIQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Policy Inference Q-Network (DPIQN) and Deep Recurrent Policy Inference Q-Network (DRPIQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectures that adapt network attention to inferred policy features of other agents and their own Q-values to cope with partial observability and multi-agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DPIQN / DRPIQN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Networks that augment Q-value estimation with policy-inference modules that attend to features characterizing other agents' policies; DRPIQN adds recurrence for partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Each agent includes an inference submodule to model other agents' policies and a Q-network for action selection; no further role specialization is specified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training and execution for decision-making under partial observability with explicit policy inference of peers.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Implicit coordination by modeling and attending to other agents' policy features within each agent's network (no centralized controller).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit inter-agent communication protocol; coordination arises from learned inference of others' behaviors from observations.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Standard RL reward signals guide joint learning; inferred policy features are adapted via attention mechanisms trained end-to-end with Q-learning objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step via observation and recurrent inference modules (for DRPIQN).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-agent partially observable control tasks where modeling other agents' policies improves action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports DPIQN and DRPIQN outperform baseline DQN and DRQN in the cited experiments (no numeric values provided in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported qualitative superiority over DQN and DRQN in the survey; numeric results not included here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Explicitly inferring other agents' policy features improves coordination and decision-making under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Relies on accurate inference of other agents from observations; scalability to many heterogeneous agents not discussed in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey emphasizes attention to policy features and recurrence for PO problems as effective design choices; no hyperparameter guidance provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2571.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS-MARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Master-Slave Multi-Agent Reinforcement Learning (MS-MARL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical architecture combining decentralized slave agents and a master agent that aggregates slave messages and issues instructive messages to each slave to reduce communication burden and coordinate behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MS-MARL (Master-Slave Multi-Agent RL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Hierarchical system where slave agents send messages to a master agent; the master processes aggregated information and sends individualized instructive messages back to slaves, enabling coordinated multi-agent control with reduced peer-to-peer communication.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (suitable for systems with many slaves and a single master)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Master agent: aggregator/strategist (processes slave messages and issues instructions); Slave agents: local executors that act using local info plus master instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Coordination and execution phases (master coordinates slave behavior during runtime); training likely involves both master and slave policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Hierarchical centralized coordination (master-slave) where the master mediates communication and issues per-slave directives.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Master receives and processes messages from slaves and sends instructive messages back; survey does not specify message format (learned neural messages implied).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Master uses aggregated observations (and presumably reward signals) to guide slave instructions; slaves adapt via their own policy learningâ€”survey does not detail explicit feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step or at scheduling intervals depending on task (survey describes master processing messages and returning instructions as part of runtime control).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Coordination-heavy multi-agent control tasks where communication bandwidth or scaling is a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states MS-MARL reduces communication burden and aids coordination especially with many agents; no numeric metrics are provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually against peer-to-peer architectures; survey claims reduced communication overhead versus peer-peer when number of agents is large but provides no quantitative comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Reduces communication burden, scales better than peer-to-peer communication, and enables hierarchical guidance to slaves.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Master's policy becomes complex as agent count increases; reliance on a single master creates potential bottleneck and single point of failure.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey suggests the master-slave hierarchy is beneficial when communication overhead is a limiting factor; does not specify optimal numbers of slaves per master.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2571.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PS-TRPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parameter-Sharing Trust Region Policy Optimization (PS-TRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-sharing adaptation of TRPO for multi-agent systems where agents share policy parameters and are trained jointly under trust-region constraints to scale multi-agent policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PS-TRPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agents share a single policy network's parameters during training (parameter sharing) while each agent conditions on its own observation and index; TRPO-style trust-region constrained optimization is used to update shared parameters using trajectories from all agents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>many / variable (designed to scale via parameter sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents are policy instances of a shared network; no explicit role specialization notedâ€”agents distinguished by their local observations and an agent index input.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training (centralized shared-parameter optimization) and decentralized execution using the shared policy.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Parameter sharing induces coordination by exposing the policy to experiences from many agents; centralized update via TRPO ensures stable joint policy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No runtime inter-agent communication required by PS-TRPO; training uses trajectories aggregated centrally to compute updates.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Shared-policy gradients derived from advantage estimates computed across agent experiences; trust-region constraint (KL) stabilizes updates.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>N/A at execution (no communication); training updates use batches of trajectories collected across agents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Large-scale multi-agent tasks, including partial observability and continuous action spaces (e.g., fleet management in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports PS-TRPO demonstrates good performance when combined with parameter sharing for scaling but does not reproduce numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Presented as an effective training scheme relative to independent learning and naive centralized policies for scaling; numeric comparisons not included in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Parameter sharing reduces number of parameters, accelerates learning across many agents, and enables scaling of policy gradient methods to larger agent counts.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Assumes some homogeneity among agents (shared policy beneficial only when agents' roles are similar); can underperform in highly heterogeneous agent populations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not detailed in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey recommends parameter sharing combined with TRPO (PS-TRPO) for scaling to many similar agents; no specific architecture hyperparameters provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2571.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LDQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lenient Deep Q-Network (LDQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DQN variant applying leniency (temperature-decayed acceptance of negative updates) combined with scheduled replay to stabilize learning in multi-agent non-stationary environments, improving coordination in stochastic reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LDQN (Lenient-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Augments DQN with leniency mechanisms (decaying temperature values that soften policy updates sampled from replay) and scheduled replay strategies to reduce the negative impact of early noisy updates in non-stationary multi-agent settings.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (applied to coordinated multi-agent object transportation experiments in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents are DQN learners augmented with leniency schedules; no further role specialization is described in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training stability and policy learning in stochastic multi-agent tasks; execution uses learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Implicit coordination via stabilization of per-agent Q-updates using leniency, enabling convergence to coordinated policies in stochastic reward environments.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit inter-agent communications required by LDQN; coordination emerges from stabilized independent learners and experience replay design.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Lenient update scheme modulates how strongly negative TD errors affect updates (decaying temperature reduces penalization early in training); experience replay scheduling further shapes learning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>N/A (no explicit message passing; learning uses replay buffer sampling schedules).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Coordinated multi-agent object transportation problems with stochastic rewards (as reported in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports LDQN outperforms hysteretic-DQN (HDQN) in terms of convergence to optimal policies in a stochastic reward environment; no numeric magnitudes are provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively against HDQN in the survey (LDQN demonstrated superior convergence behavior in cited experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Leniency plus scheduled replay improves stability and convergence of independent learners in non-stationary multi-agent tasks, enabling better coordinated solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Requires careful scheduling/temperature decay tuning; may be sensitive to replay strategy and leniency hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Survey does not provide ablation numbers but notes leniency and scheduled replay are key components behind improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey suggests decaying temperature (leniency) combined with scheduled replay is effective; no precise parameter settings provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2571.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2571.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Action Decoder (BAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that constructs an approximate public belief MDP and uses Bayesian updates over public observations to enable agents to discover implicit conventions and coordinate under cooperative partial-observability (demonstrated on Hanabi and matrix games).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian action decoder for deep multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BAD (Bayesian Action Decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agents maintain and update an approximate public belief state (public belief MDP) using Bayesian updates over publicly observable features, then act to maximize expected team return under this belief; supports learning conventions and coordinated policies in PO cooperative games.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (demonstrated in small-team cooperative games like Hanabi in cited experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents are cooperative policy learners that condition on public-belief approximations; no further specialization detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training of belief-aware policies and execution using learned belief-conditioned strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Coordination via shared public-belief state and learned conventions derived from Bayesian updates over public observations.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit inter-agent message passing; coordination leverages public observations and inferred beliefs (implicit communication via actions and public signals).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Environment/team rewards combined with belief-updating provide training signals; policies learn to produce signals (actions) that shape common belief and coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per time-step via public observation updates and belief maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cooperative partially-observable games where public signals and conventions are critical (e.g., Hanabi, cooperative matrix games).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports BAD demonstrates efficiency and superiority against traditional policy gradient baselines in cited testbeds (two-step matrix game and Hanabi) but does not list numeric scores in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared favorably to standard policy gradient algorithms in the original studies (as summarized in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Enables discovery of conventions and improved coordinated play in cooperative PO tasks by leveraging public-belief approximations and Bayesian updates.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Requires tractable approximate belief updates and suitable factorization of belief representation; scaling to large state spaces may be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not described in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Survey emphasizes using approximate Bayesian public-belief updates and leveraging publicly observable features; no specific hyperparameters provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to communicate with deep multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning to communicate to solve riddles with deep distributed recurrent Q-networks <em>(Rating: 2)</em></li>
                <li>Counterfactual multiagent policy gradients <em>(Rating: 2)</em></li>
                <li>Bayesian action decoder for deep multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning Multiagent Communication with Backpropagation <em>(Rating: 2)</em></li>
                <li>Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments <em>(Rating: 2)</em></li>
                <li>Hierarchical Deep Reinforcement Learning for Multi-task Dialogue (hierarchical DQN / federated control) <em>(Rating: 1)</em></li>
                <li>Master-Slave Multi-Agent Reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2571",
    "paper_id": "paper-28e66d188efbd0bbb64242b611d96769be910c15",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "RIAL",
            "name_full": "Reinforced Inter-Agent Learning (RIAL)",
            "brief_description": "A multi-agent communication method where each agent uses a recurrent deep Q-network to learn policies under partial observability; agents learn independently with local recurrent architectures and learn communication behaviors implicitly via their networks.",
            "citation_title": "Learning to communicate with deep multi-agent reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "RIAL (Reinforced Inter-Agent Learning)",
            "system_description": "Decentralized multi-agent system in which each agent is implemented with a recurrent deep Q-network (RNN/DQN). Agents learn their own parameters independently; communication is supported through message channels learned by the networks in training, but training itself remains largely decentralized per-agent in RIAL.",
            "number_of_agents": "variable",
            "agent_specializations": "Not specified in the survey (agents are homogeneous in architecture; each agent learns its own policy via recurrent DQN).",
            "research_phases_covered": "execution (policy learning and action selection under partial observability); implicit communication learning during training",
            "coordination_mechanism": "Decentralized independent learning with learned inter-agent messaging (agents do not rely on a centralized controller during execution).",
            "communication_protocol": "Learned messages passed between agents via differentiable channels during training (paper describes end-to-end learned communication but not an explicit message format).",
            "feedback_mechanism": "Agents receive environment rewards and update their own recurrent DQN parameters; no central critic is described for RIAL in the survey.",
            "communication_frequency": "Per time-step / during episodes (communication is learned as part of stepwise interactions under partial observability).",
            "task_domain": "Cooperative partially observable multi-agent tasks (communication learning tasks / riddles / cooperative games).",
            "performance_metrics": "Survey reports that RIAL is one of the proposed methods for communication learning; no numeric performance metrics are provided in this review for RIAL specifically.",
            "baseline_comparison": "Compared in the literature (per survey) against DIAL and other communication-learning methods; the review does not list numeric baselines here.",
            "coordination_benefits": "Allows agents to learn to communicate to solve cooperative tasks under partial observability; improves coordinated behavior relative to independent agents in the original studies (qualitative in survey).",
            "coordination_challenges": "Independent learning can be unstable under non-stationarity; implicit communication channels may be hard to interpret and require careful training.",
            "ablation_studies": "Not reported in this survey for RIAL specifically.",
            "optimal_configurations": "Survey does not specify optimal hyperparameters or configurations for RIAL beyond noting the use of recurrent DQN architectures and learned communication channels.",
            "uuid": "e2571.0",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "DIAL",
            "name_full": "Differentiable Inter-Agent Learning (DIAL)",
            "brief_description": "A multi-agent learning approach that enables end-to-end gradient-based learning of communication protocols by passing differentiable messages (and gradients) between agents during centralized training.",
            "citation_title": "Learning to communicate with deep multi-agent reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "DIAL (Differentiable Inter-Agent Learning)",
            "system_description": "Agents are realized as neural networks that communicate through differentiable channels; during training gradients are propagated between agents via the communication channel to enable end-to-end learning of both action policies and communication protocols.",
            "number_of_agents": "variable",
            "agent_specializations": "Not explicitly specialized in the survey; agents are neural-policy learners that both produce actions and messages.",
            "research_phases_covered": "Policy and communication learning (training) and decentralized execution (agents act using learned message protocols).",
            "coordination_mechanism": "End-to-end gradient-based coordination during training; decentralized execution using learned messaging rules.",
            "communication_protocol": "Differentiable real-valued messages exchanged between agents; training uses message channels that allow gradient flow between agent networks (no fixed explicit message schema provided in survey).",
            "feedback_mechanism": "Environment rewards and gradient signals propagated across agents during training; message-generation networks updated via backpropagation across agent boundaries.",
            "communication_frequency": "Per time-step during episodes (communication learned as part of each interaction).",
            "task_domain": "Cooperative partially-observable multi-agent tasks requiring learned communication (e.g., riddles and communication emergent tasks).",
            "performance_metrics": "Survey states DIAL enables end-to-end backprop across agents and improves learning of communication protocols but does not provide numeric metrics in this review.",
            "baseline_comparison": "Described as an advance over RIAL (which learns without cross-agent backprop) in the original work; the survey does not reproduce numeric comparisons.",
            "coordination_benefits": "Enables direct gradient flow between agents so that message formats and action policies co-adapt, improving emergence of effective communication protocols for coordination.",
            "coordination_challenges": "Requires differentiable communication channel during training (not always applicable when messages must be discrete at execution), and may not be applicable where centralized gradient exchange is infeasible.",
            "ablation_studies": "Not provided in this review for DIAL specifically.",
            "optimal_configurations": "Survey notes DIAL's key design is gradient passing for communication; no specific optimal hyperparameters are reported here.",
            "uuid": "e2571.1",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "CommNet",
            "name_full": "Communication Neural Network (CommNet)",
            "brief_description": "A neural architecture that learns continuous communication among cooperating agents by aggregating and broadcasting continuous message vectors to enable coordinated policies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CommNet",
            "system_description": "A centralized-during-training architecture in which agents produce continuous vector messages that are aggregated (e.g., summed/averaged) and redistributed to agents as additional inputs; the communication channel and policy networks are learned jointly to produce coordinated behavior for fully cooperative tasks.",
            "number_of_agents": "variable (designed to scale to multiple agents)",
            "agent_specializations": "In reported uses agents are typically homogeneous policy learners (manager/participant/mediator roles were described as application-level types in one task allocation use-case).",
            "research_phases_covered": "Policy and communication learning; execution uses learned decentralized policies with message-processing layers.",
            "coordination_mechanism": "Implicit centralized aggregation of messages during training; decentralized policies conditioned on aggregated comms at execution.",
            "communication_protocol": "Continuous (real-valued) vector messages passed through learned neural-message layers (format is learned; not an explicit structured format in the survey).",
            "feedback_mechanism": "Agents receive environment rewards and update policy and communication weights via centralized gradient-based training.",
            "communication_frequency": "Per time-step (communication is learned as a stepwise process during episodes).",
            "task_domain": "Cooperative tasks requiring coordination and information sharing such as distributed task allocation and small-scale coordination benchmarks.",
            "performance_metrics": "Survey reports CommNet facilitates learned communication for cooperative tasks; specific numeric metrics are not reproduced in this review.",
            "baseline_comparison": "Used in the survey as an approach contrasted with other communication-learning methods; direct numeric baselines not provided here.",
            "coordination_benefits": "Learns continuous communication channels that improve task allocation and coordination relative to non-communicating baselines (qualitative in survey).",
            "coordination_challenges": "May struggle with heterogeneous agents and has computational/communication overheads in decentralized reallocation scenarios per the survey.",
            "ablation_studies": "Not reported in the survey for CommNet specifically.",
            "optimal_configurations": "Survey does not specify optimal architectures; emphasizes joint learning of comms and policies and that CommNet favors continuous-vector messaging.",
            "uuid": "e2571.2",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "MADDPG",
            "name_full": "Multi-Agent Deep Deterministic Policy Gradient (MADDPG)",
            "brief_description": "An actor-critic multi-agent algorithm that uses centralized critics with access to other agents' observations/actions during training and decentralized actors for execution, enabling learning in mixed cooperative-competitive environments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MADDPG",
            "system_description": "Each agent has a decentralized actor (policy) conditioned on local observations; during training each agent's critic is centralized and takes as input global/augmented state including other agents' observations and actions to stabilize learning and mitigate non-stationarity.",
            "number_of_agents": "variable (scales to multiple agents)",
            "agent_specializations": "Agents are policy learners (actors); critics are centralized per-agent learners used only during training. No task-role specialization is mandated by the algorithm description in the survey.",
            "research_phases_covered": "Training (centralized critics) and execution (decentralized actors for deployment).",
            "coordination_mechanism": "Centralized learning of critics with decentralized execution of actors; coordination emerges via centralized critic guidance during training.",
            "communication_protocol": "During execution, no inter-agent communication is required; during training critics use augmented joint observations/actions (no explicit message-passing protocol described in survey).",
            "feedback_mechanism": "Centralized critic provides value/TD learning signals to update actor policies; environment rewards are used with centralized critics to compute gradients.",
            "communication_frequency": "Centralized critic uses full-episode or per-step joint information during training; agents do not communicate during decentralized execution.",
            "task_domain": "Cooperative and mixed cooperative-competitive multi-agent tasks with continuous action spaces.",
            "performance_metrics": "Survey states MADDPG can learn continuous policies effectively and is suitable for continuous-action multi-agent problems; no numeric metrics are reproduced in this review.",
            "baseline_comparison": "Contrasted to COMA (which is discrete-action focused) and independent learners; the survey reports MADDPG's ability to learn continuous policies but does not give specific numeric comparisons.",
            "coordination_benefits": "Centralized critics reduce non-stationarity during training, improving learning stability and enabling continuous-action coordination where independent learning would fail.",
            "coordination_challenges": "Requires access to other agents' observations/actions during training (centralized information), which may be infeasible in some settings; scalability and privacy concerns for many agents.",
            "ablation_studies": "Not reported in this survey for MADDPG specifically.",
            "optimal_configurations": "Survey highlights centralized-critic + decentralized-actor as a recommended configuration for continuous-action multi-agent problems; no numeric hyperparameter prescriptions provided.",
            "uuid": "e2571.3",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "COMA",
            "name_full": "Counterfactual Multi-Agent (COMA)",
            "brief_description": "An actor-critic multi-agent algorithm that addresses multi-agent credit assignment by using a centralized critic and a counterfactual baseline to compute each agent's advantage contribution to a global reward.",
            "citation_title": "Counterfactual multiagent policy gradients",
            "mention_or_use": "mention",
            "system_name": "COMA",
            "system_description": "Decentralized actor policies for each agent; centralized critic computes a counterfactual baseline by marginalizing a single agent's action to estimate its contribution, enabling more precise credit assignment in cooperative tasks with global rewards.",
            "number_of_agents": "variable (designed for cooperative multi-agent teams)",
            "agent_specializations": "Agents are action-policies (actors) that receive local observations; no further specialization described in the survey.",
            "research_phases_covered": "Training (centralized critic and counterfactual advantage estimation) and execution (decentralized actors).",
            "coordination_mechanism": "Centralized critic computes counterfactual advantages for individual agents to shape decentralized policy updates.",
            "communication_protocol": "Centralized critic requires access to joint action/state information during training; execution is decentralized without communication per the survey.",
            "feedback_mechanism": "Counterfactual advantage signals from the centralized critic are used as feedback to each agent's policy gradient updates to provide attribution of global rewards to individual actions.",
            "communication_frequency": "Counterfactual computations occur during training at each learning update step; execution requires no ongoing communication.",
            "task_domain": "Cooperative multi-agent tasks with global/team reward where credit assignment is challenging.",
            "performance_metrics": "Survey indicates COMA addresses credit assignment effectively; no numerical performance figures are provided in this review.",
            "baseline_comparison": "Presented as an advance over naive centralized critics and independent learners for credit assignment; specific numeric baselines not reproduced here.",
            "coordination_benefits": "Improves multi-agent credit assignment, enabling better cooperative policy learning from global rewards.",
            "coordination_challenges": "Focused on discrete action spaces per survey; computational/estimation complexity may grow with many agents.",
            "ablation_studies": "Not described in this survey.",
            "optimal_configurations": "Survey notes COMA's central insight is the counterfactual baseline for advantage estimation; no further configuration recommendations are given.",
            "uuid": "e2571.4",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "DDRQN",
            "name_full": "Deep Distributed Recurrent Q-Network (DDRQN)",
            "brief_description": "An extension of DRQN for multi-agent partially observable settings that uses last-action inputs, inter-agent weight sharing, and disables experience replay to enable coordination under partial observability.",
            "citation_title": "Learning to communicate to solve riddles with deep distributed recurrent Q-networks",
            "mention_or_use": "mention",
            "system_name": "DDRQN",
            "system_description": "Multi-agent recurrent Q-network architecture where agents are implemented with recurrent networks; key features include feeding last action as input, sharing network weights among agents (when action sets equal), and disabling experience replay to avoid non-stationarity introduced by replay.",
            "number_of_agents": "variable (suitable for multiple cooperating agents)",
            "agent_specializations": "Agents can be homogeneous when weight sharing is used; heterogeneous action sets preclude weight sharing per the survey.",
            "research_phases_covered": "Training in partially observable multi-agent tasks and decentralized execution using learned recurrent policies.",
            "coordination_mechanism": "Coordination via shared recurrent network parameters (weight sharing) and inclusion of last-action inputs to condition policy on personal history and implicitly on other agents' behavior.",
            "communication_protocol": "Implicitâ€”agents condition on histories and last actions rather than explicit inter-agent message channels in the DDRQN formulation described in the survey.",
            "feedback_mechanism": "Standard Q-learning style TD updates on recurrent networks using environment rewards; experience replay is disabled to reduce non-stationarity artifacts.",
            "communication_frequency": "Per time-step via recurrent state and last-action inputs.",
            "task_domain": "Multi-agent partially observable coordination problems and cooperative puzzles/riddles.",
            "performance_metrics": "Survey notes DDRQN's empirical success in multi-agent PO problems and cites architectural features as important; no numerical metrics are included in this review.",
            "baseline_comparison": "Described as an extension of DRQN for multi-agent settings and compared in original studies to naive DQN/DRQN baselines; the survey does not reproduce numeric comparisons.",
            "coordination_benefits": "Weight sharing and last-action conditioning reduce parameter counts and help agents coordinate in PO settings; disabling replay mitigates replay-induced non-stationarity.",
            "coordination_challenges": "Assumes equal action sets for weight sharing; heterogeneous agent action spaces prevent weight sharing and complicate scaling.",
            "ablation_studies": "Survey highlights three features (last-action input, weight sharing, disabling replay) as crucial, implying ablation of these reduces performance, but specific ablation numbers are not provided here.",
            "optimal_configurations": "Survey recommends using last-action inputs and inter-agent weight sharing when agents share action spaces, and disabling experience replay in concurrent multi-agent recurrent training.",
            "uuid": "e2571.5",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "DPIQN / DRPIQN",
            "name_full": "Deep Policy Inference Q-Network (DPIQN) and Deep Recurrent Policy Inference Q-Network (DRPIQN)",
            "brief_description": "Architectures that adapt network attention to inferred policy features of other agents and their own Q-values to cope with partial observability and multi-agent interactions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "DPIQN / DRPIQN",
            "system_description": "Networks that augment Q-value estimation with policy-inference modules that attend to features characterizing other agents' policies; DRPIQN adds recurrence for partial observability.",
            "number_of_agents": "variable",
            "agent_specializations": "Each agent includes an inference submodule to model other agents' policies and a Q-network for action selection; no further role specialization is specified in the survey.",
            "research_phases_covered": "Training and execution for decision-making under partial observability with explicit policy inference of peers.",
            "coordination_mechanism": "Implicit coordination by modeling and attending to other agents' policy features within each agent's network (no centralized controller).",
            "communication_protocol": "No explicit inter-agent communication protocol; coordination arises from learned inference of others' behaviors from observations.",
            "feedback_mechanism": "Standard RL reward signals guide joint learning; inferred policy features are adapted via attention mechanisms trained end-to-end with Q-learning objectives.",
            "communication_frequency": "Per time-step via observation and recurrent inference modules (for DRPIQN).",
            "task_domain": "Multi-agent partially observable control tasks where modeling other agents' policies improves action selection.",
            "performance_metrics": "Survey reports DPIQN and DRPIQN outperform baseline DQN and DRQN in the cited experiments (no numeric values provided in the review).",
            "baseline_comparison": "Reported qualitative superiority over DQN and DRQN in the survey; numeric results not included here.",
            "coordination_benefits": "Explicitly inferring other agents' policy features improves coordination and decision-making under partial observability.",
            "coordination_challenges": "Relies on accurate inference of other agents from observations; scalability to many heterogeneous agents not discussed in detail.",
            "ablation_studies": "Not detailed in the survey.",
            "optimal_configurations": "Survey emphasizes attention to policy features and recurrence for PO problems as effective design choices; no hyperparameter guidance provided.",
            "uuid": "e2571.6",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "MS-MARL",
            "name_full": "Master-Slave Multi-Agent Reinforcement Learning (MS-MARL)",
            "brief_description": "A hierarchical architecture combining decentralized slave agents and a master agent that aggregates slave messages and issues instructive messages to each slave to reduce communication burden and coordinate behavior.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MS-MARL (Master-Slave Multi-Agent RL)",
            "system_description": "Hierarchical system where slave agents send messages to a master agent; the master processes aggregated information and sends individualized instructive messages back to slaves, enabling coordinated multi-agent control with reduced peer-to-peer communication.",
            "number_of_agents": "variable (suitable for systems with many slaves and a single master)",
            "agent_specializations": "Master agent: aggregator/strategist (processes slave messages and issues instructions); Slave agents: local executors that act using local info plus master instructions.",
            "research_phases_covered": "Coordination and execution phases (master coordinates slave behavior during runtime); training likely involves both master and slave policy learning.",
            "coordination_mechanism": "Hierarchical centralized coordination (master-slave) where the master mediates communication and issues per-slave directives.",
            "communication_protocol": "Master receives and processes messages from slaves and sends instructive messages back; survey does not specify message format (learned neural messages implied).",
            "feedback_mechanism": "Master uses aggregated observations (and presumably reward signals) to guide slave instructions; slaves adapt via their own policy learningâ€”survey does not detail explicit feedback loops.",
            "communication_frequency": "Per time-step or at scheduling intervals depending on task (survey describes master processing messages and returning instructions as part of runtime control).",
            "task_domain": "Coordination-heavy multi-agent control tasks where communication bandwidth or scaling is a concern.",
            "performance_metrics": "Survey states MS-MARL reduces communication burden and aids coordination especially with many agents; no numeric metrics are provided in the review.",
            "baseline_comparison": "Compared conceptually against peer-to-peer architectures; survey claims reduced communication overhead versus peer-peer when number of agents is large but provides no quantitative comparisons here.",
            "coordination_benefits": "Reduces communication burden, scales better than peer-to-peer communication, and enables hierarchical guidance to slaves.",
            "coordination_challenges": "Master's policy becomes complex as agent count increases; reliance on a single master creates potential bottleneck and single point of failure.",
            "ablation_studies": "Not reported in the survey.",
            "optimal_configurations": "Survey suggests the master-slave hierarchy is beneficial when communication overhead is a limiting factor; does not specify optimal numbers of slaves per master.",
            "uuid": "e2571.7",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "PS-TRPO",
            "name_full": "Parameter-Sharing Trust Region Policy Optimization (PS-TRPO)",
            "brief_description": "A parameter-sharing adaptation of TRPO for multi-agent systems where agents share policy parameters and are trained jointly under trust-region constraints to scale multi-agent policy optimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "PS-TRPO",
            "system_description": "Agents share a single policy network's parameters during training (parameter sharing) while each agent conditions on its own observation and index; TRPO-style trust-region constrained optimization is used to update shared parameters using trajectories from all agents.",
            "number_of_agents": "many / variable (designed to scale via parameter sharing)",
            "agent_specializations": "Agents are policy instances of a shared network; no explicit role specialization notedâ€”agents distinguished by their local observations and an agent index input.",
            "research_phases_covered": "Training (centralized shared-parameter optimization) and decentralized execution using the shared policy.",
            "coordination_mechanism": "Parameter sharing induces coordination by exposing the policy to experiences from many agents; centralized update via TRPO ensures stable joint policy improvement.",
            "communication_protocol": "No runtime inter-agent communication required by PS-TRPO; training uses trajectories aggregated centrally to compute updates.",
            "feedback_mechanism": "Shared-policy gradients derived from advantage estimates computed across agent experiences; trust-region constraint (KL) stabilizes updates.",
            "communication_frequency": "N/A at execution (no communication); training updates use batches of trajectories collected across agents.",
            "task_domain": "Large-scale multi-agent tasks, including partial observability and continuous action spaces (e.g., fleet management in the survey).",
            "performance_metrics": "Survey reports PS-TRPO demonstrates good performance when combined with parameter sharing for scaling but does not reproduce numeric metrics.",
            "baseline_comparison": "Presented as an effective training scheme relative to independent learning and naive centralized policies for scaling; numeric comparisons not included in survey.",
            "coordination_benefits": "Parameter sharing reduces number of parameters, accelerates learning across many agents, and enables scaling of policy gradient methods to larger agent counts.",
            "coordination_challenges": "Assumes some homogeneity among agents (shared policy beneficial only when agents' roles are similar); can underperform in highly heterogeneous agent populations.",
            "ablation_studies": "Not detailed in this survey.",
            "optimal_configurations": "Survey recommends parameter sharing combined with TRPO (PS-TRPO) for scaling to many similar agents; no specific architecture hyperparameters provided.",
            "uuid": "e2571.8",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "LDQN",
            "name_full": "Lenient Deep Q-Network (LDQN)",
            "brief_description": "A DQN variant applying leniency (temperature-decayed acceptance of negative updates) combined with scheduled replay to stabilize learning in multi-agent non-stationary environments, improving coordination in stochastic reward tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LDQN (Lenient-DQN)",
            "system_description": "Augments DQN with leniency mechanisms (decaying temperature values that soften policy updates sampled from replay) and scheduled replay strategies to reduce the negative impact of early noisy updates in non-stationary multi-agent settings.",
            "number_of_agents": "variable (applied to coordinated multi-agent object transportation experiments in the survey)",
            "agent_specializations": "Agents are DQN learners augmented with leniency schedules; no further role specialization is described in the survey.",
            "research_phases_covered": "Training stability and policy learning in stochastic multi-agent tasks; execution uses learned policies.",
            "coordination_mechanism": "Implicit coordination via stabilization of per-agent Q-updates using leniency, enabling convergence to coordinated policies in stochastic reward environments.",
            "communication_protocol": "No explicit inter-agent communications required by LDQN; coordination emerges from stabilized independent learners and experience replay design.",
            "feedback_mechanism": "Lenient update scheme modulates how strongly negative TD errors affect updates (decaying temperature reduces penalization early in training); experience replay scheduling further shapes learning signals.",
            "communication_frequency": "N/A (no explicit message passing; learning uses replay buffer sampling schedules).",
            "task_domain": "Coordinated multi-agent object transportation problems with stochastic rewards (as reported in the survey).",
            "performance_metrics": "Survey reports LDQN outperforms hysteretic-DQN (HDQN) in terms of convergence to optimal policies in a stochastic reward environment; no numeric magnitudes are provided in the review.",
            "baseline_comparison": "Compared qualitatively against HDQN in the survey (LDQN demonstrated superior convergence behavior in cited experiments).",
            "coordination_benefits": "Leniency plus scheduled replay improves stability and convergence of independent learners in non-stationary multi-agent tasks, enabling better coordinated solutions.",
            "coordination_challenges": "Requires careful scheduling/temperature decay tuning; may be sensitive to replay strategy and leniency hyperparameters.",
            "ablation_studies": "Survey does not provide ablation numbers but notes leniency and scheduled replay are key components behind improved performance.",
            "optimal_configurations": "Survey suggests decaying temperature (leniency) combined with scheduled replay is effective; no precise parameter settings provided.",
            "uuid": "e2571.9",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "BAD",
            "name_full": "Bayesian Action Decoder (BAD)",
            "brief_description": "An algorithm that constructs an approximate public belief MDP and uses Bayesian updates over public observations to enable agents to discover implicit conventions and coordinate under cooperative partial-observability (demonstrated on Hanabi and matrix games).",
            "citation_title": "Bayesian action decoder for deep multi-agent reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "BAD (Bayesian Action Decoder)",
            "system_description": "Agents maintain and update an approximate public belief state (public belief MDP) using Bayesian updates over publicly observable features, then act to maximize expected team return under this belief; supports learning conventions and coordinated policies in PO cooperative games.",
            "number_of_agents": "variable (demonstrated in small-team cooperative games like Hanabi in cited experiments)",
            "agent_specializations": "Agents are cooperative policy learners that condition on public-belief approximations; no further specialization detailed in the survey.",
            "research_phases_covered": "Training of belief-aware policies and execution using learned belief-conditioned strategies.",
            "coordination_mechanism": "Coordination via shared public-belief state and learned conventions derived from Bayesian updates over public observations.",
            "communication_protocol": "No explicit inter-agent message passing; coordination leverages public observations and inferred beliefs (implicit communication via actions and public signals).",
            "feedback_mechanism": "Environment/team rewards combined with belief-updating provide training signals; policies learn to produce signals (actions) that shape common belief and coordination.",
            "communication_frequency": "Per time-step via public observation updates and belief maintenance.",
            "task_domain": "Cooperative partially-observable games where public signals and conventions are critical (e.g., Hanabi, cooperative matrix games).",
            "performance_metrics": "Survey reports BAD demonstrates efficiency and superiority against traditional policy gradient baselines in cited testbeds (two-step matrix game and Hanabi) but does not list numeric scores in this review.",
            "baseline_comparison": "Compared favorably to standard policy gradient algorithms in the original studies (as summarized in the survey).",
            "coordination_benefits": "Enables discovery of conventions and improved coordinated play in cooperative PO tasks by leveraging public-belief approximations and Bayesian updates.",
            "coordination_challenges": "Requires tractable approximate belief updates and suitable factorization of belief representation; scaling to large state spaces may be challenging.",
            "ablation_studies": "Not described in this survey.",
            "optimal_configurations": "Survey emphasizes using approximate Bayesian public-belief updates and leveraging publicly observable features; no specific hyperparameters provided.",
            "uuid": "e2571.10",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to communicate with deep multi-agent reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning to communicate to solve riddles with deep distributed recurrent Q-networks",
            "rating": 2
        },
        {
            "paper_title": "Counterfactual multiagent policy gradients",
            "rating": 2
        },
        {
            "paper_title": "Bayesian action decoder for deep multi-agent reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning Multiagent Communication with Backpropagation",
            "rating": 2
        },
        {
            "paper_title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical Deep Reinforcement Learning for Multi-task Dialogue (hierarchical DQN / federated control)",
            "rating": 1
        },
        {
            "paper_title": "Master-Slave Multi-Agent Reinforcement Learning",
            "rating": 1
        }
    ],
    "cost": 0.023009750000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Reinforcement Learning for Multi-Agent Systems: A Review of Challenges, Solutions and Applications</h1>
<p>Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi<br>Institute for Intelligent Systems Research and Innovation<br>Deakin University, Waurn Ponds, Victoria, Australia<br>E-mail: thanh.nguyen@deakin.edu.au.<br>Tel: +61352278281 . Fax: +61352271046.</p>
<h4>Abstract</h4>
<p>Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms however have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This paper addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multi-agent deep RL (MADRL) is presented, including non-stationarity, partial observability, continuous state and action spaces, multi-agent training schemes, multi-agent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed, with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to future development of more robust and highly useful multi-agent learning methods for solving real-world problems.</p>
<p>Keywords: review, survey, deep learning, deep reinforcement learning, robotics, multi-agent, partial observability, non-stationary, continuous action space.</p>
<h2>1 Introduction</h2>
<p>Reinforcement learning was instigated by a trial and error (TE) procedure, conducted by Thorndike in an experiment on cat's behaviors in 1898 [113]. In 1954, Minsky [72] designed the first neural computer named Stochastic Neural-Analog Reinforcement Calculators (SNARCs), which simulated the rat's brain to solve the maze puzzle. SNARCs remarked the uplift of TE learning to a computational period. Almost two decades later, Klopf [51] integrated the mechanism of temporal-difference (TD) learning from psychology into the computational model of TE learning. That integration succeeded in making TE learning a feasible approach to large systems. In 1989, Watkins and Dayan [116] brought the theory of optimal control [6] including Bellman equation and Markov decision process together with temporal-difference learning to form a well-known $Q$-learning. Since then, Q-learning has been applied to solve various real-world problems, but it is unable to solve high-dimensional problems where the number of calculations increases drastically with number of inputs. This problem, known as the curse of dimensionality, exceeds the computational constraint of conventional computers. In 2015, Mnih et al. [73] made an important breakthrough by combining deep learning with RL to partially overcome the curse of dimensionality. Deep RL has become a normative approach in artificial intelligence, attracting significant attention from the research community since then. Milestones of the development of RL are presented in Fig. 1, which span from the trial and error method to deep RL.</p>
<p>RL originates from animal learning in psychology and thus it can mimic human learning ability to select actions that maximize long-term profit in their interactions with the environment. Therefore, RL can be used to develop an agent that is comparable to the human performance. For instance, RL has been widely used in robotics and autonomous systems, e.g. Mahadevan and Connell [70] designed a robot that can push cubes (1992); Schaal [98] created a humanoid robot that can effectively solve the pole-balancing task (1997); Benbrahim and Franklin [7] made a biped robot that can learn to walk without any knowledge of the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Emergence of deep RL through different essential milestones.</p>
<p>environment (1997); Riedmiller et al. [95] built a soccer robot team (2009); and Muelling et al. [76] trained a robot to play table tennis (2013).</p>
<p>Modern RL is truly marked by the success of deep RL in 2015 when Mnih et al. [73] made use of a structure named <em>deep Q-network</em> (DQN) in creating an agent that outperformed a professional player in a series of 49 classic Atari games [5]. In 2016, Google's DeepMind created a self-taught AlphaGo program that could beat the best professional Go players, including China's Ke Jie and Korea's Lee Sadol [107]. Deep RL has also been used to solve MuJuCo physic problems [19] and 3D maze games [4]. In 2017, OpenAI announced a bot that could beat the best professional gamer on the online game Dota 2, which is supposed to be more complicated than the Go game. These fates provide the necessary impetus to enterprise corporations such as Google, Tesla, and Uber in their race to make self-driving cars. More importantly, deep RL has become a promising approach to solving complex real-world problems and has also been a huge contribution to the field of artificial intelligence.</p>
<p>RL is a research theme that distincts from other related concepts in artificial intelligence. Historically, there had been a confusion between RL and <em>supervised learning</em> (SL) since the 1960s. It was not until 1981 that Sutton and Barto [110] shed a light on the discrepancy between the two learning methods. Concisely, SL is learning from data that define input and corresponding output (often called â€œlabelledâ€ data) by an external supervisor, whereas RL is learning by interacting with the unknown environment. In the former case, it may be infeasible to collect all possible behaviors in the real world to feed the algorithm. However, in the latter case, an RL agent conducts a TE procedure to gain experiences and improve itself over time. Furthermore, RL is not an <em>unsupervised learning</em> (UL) method. UL is learning to explore the hidden structure of data where output information is unknown (â€œunlabelledâ€ data). In contrast, RL is a goal-directed learning, i.e., it constructs a learning model that clearly specifies output to maximize the long-term profit. Finally, deep RL distinguishes with deep learning method. Deep learning uses multi-layer neural networks to learn a problem in different levels of abstraction [17]. Deep RL leverages deep learning as an approximator to deal with high-dimensional data. This fact makes deep RL a promising approach to solving complex real-world problems.</p>
<p>As real-world problems have become increasingly complicated, there are many situations where a single deep RL agent is not able to cope with. In such situations, the applications of a <em>multi-agent system</em> (MAS) are indispensable. In an MAS, agents must compete or cooperate to obtain the best overall results. Examples of such systems include multi-player online games, cooperative robots in the production factories, traffic</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: A single agent interacting with its environment.
control systems, and autonomous military systems like unmanned aerial vehicles, surveillance, and spacecraft. Among many applications of deep RL in the literature, there is a large number of studies using deep RL in MAS, henceforth multi-agent deep RL (MADRL). Extending from a single agent domain to a multi-agent environment creates several challenges. Previous surveys considered different perspectives, for example, Busoniu et al. [10] examined the stability and adaptation aspects of agents, Bloembergen et al. [8] analysed the evolutionary dynamics, Hernandez-Leal et al. [41] considered emergent behaviors, communication and cooperation learning perspectives, and Silva et al. [105] reviewed methods for knowledge reuse autonomy in multi-agent $R L$ (MARL). This paper presents an overview of technical challenges in multi-agent learning as well as deep RL approaches to these challenges. We cover numerous MADRL perspectives, including nonstationarity, partial observability, multi-agent training schemes, transfer learning in MAS, and continuous state and action spaces in multi-agent learning. Applications of MADRL in various fields are also reviewed and analysed in the current study. In the last section, we present extensive discussions and interesting future research directions of MADRL.</p>
<h1>2 Background: Reinforcement Learning</h1>
<h3>2.1 Preliminary</h3>
<p>RL is a TE learning 1) by interacting directly with the environment 2) to self-teach over time and 3) eventually achieve designating goal. Specifically, RL defines any decision maker (learner) as an agent and anything outside the agent as an environment. The interactions between agent and environment are described via three essential elements: state $s$, action $a$, and reward $r$, as illustrated in Fig. 2 [110]. The state of the environment at time-step $t$ is denoted as $s_{t}$. Thereby, the agent examines $s_{t}$ and performs a corresponding action $a_{t}$. The environment then alters its state $s_{t}$ to $s_{t+1}$ and provides a feedback reward $r_{t+1}$ to the agent. For instance, Fig. 3 illustrates one of the earliest problems in RL literature, a 2D pole-balancing task. In this problem, a state of the environment at time-step $t$ can be presented by a 4-tuple $s_{t}=\left[x_{c}, v_{c}, \alpha_{p}, \omega_{p}\right]<em c="c">{t}$, where $x</em>=0$ otherwise. Therefore, the agent's goal is to keep the pole upright as long as possible and ultimately maximize the accumulated feedback reward.}$ denotes $x$-coordinate of the cart in Cartesian coordinate system $O_{x y}, v_{c}$ presents velocity of the cart along the track, $\alpha_{p}$ is the angle created by the pole and axis $O_{y}$, and $\omega_{p}$ indicates the angular velocity of the pole around center $I$. The agent can produce two possible actions at each time-step $t$ : exert a unit force $(|\vec{F}|=1)$ to the cart along axis $O_{x}$ from left to right $a_{t}=\vec{F}$ or from right to left $a_{t}=-\vec{F}$. The agent is given a feedback reward $r_{t+1}=+1$ for every action that can keep the pole upright and $r_{t+1</p>
<p>Typically, the interactions between agent and environment can be presented by a series of states, actions, and rewards: $s_{0}, a_{0}, r_{1}, s_{1}, a_{1}, \ldots, r_{n}, s_{n}$. Although $n$ can approach to infinity, we often limit $n$ in practice by defining a terminal state $s_{n}=s_{T}$. In this case, a series of states, actions, and rewards from initial state to terminal state is called an episode. For example, in pole-balancing task, we can define a terminal state as if $\left|\alpha_{p}\right|&gt;10^{\circ}$ or $\left|x_{c}\right|&gt;X_{\max }$.</p>
<p>The next step is to formalize the agent's decision by defining a concept of policy. A policy $\pi$ is a mapping</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: A 2D pole-balancing task.
function from any perceived state $s$ to action $a$ taken from that state. A policy is deterministic if the probability of choosing an action $a$ from $s: p(a \mid s)=1$ for all state $s$. In contrast, the policy is stochastic if there exists a state $s$ so that $p(a \mid s)&lt;1$. In either case, we can define the policy $\pi$ as a probability distribution of candidate actions that will be selected from a certain state as below:</p>
<p>$$
\pi=\Psi(s)=\left{\left.p\left(a_{i} \mid s\right)\right| \forall a_{i} \in \Delta_{\pi} \wedge \sum_{i} p\left(a_{i} \mid s\right)=1\right}
$$</p>
<p>where $\Delta_{\pi}$ represents all candidate actions (action space) of policy $\pi$. For clarity, we assume that the action space is discrete because the continuous case can be straightforwardly inferred by using integral notation. Furthermore, we presume that the next state $s_{t+1}$ and feedback reward $r_{t+1}$ are entirely determined by the current state-action pair $\left(s_{t}, a_{t}\right)$ regardless of the history. Any RL problem satisfies this "memoryless" condition is known as Markov decision process (MDP). Therefore, the dynamics (model) of an RL problem is completely specified by giving all transition probabilities $p\left(a_{i} \mid s\right)$. Based on (1), we can define a deterministic policy $\pi_{d}$ :</p>
<p>$$
\pi_{d}=\Psi_{d}(s)= \begin{cases}1, &amp; a_{i}=a(s) \wedge a(s) \in \Delta_{\pi_{d}} \ 0, &amp; \forall a_{i} \in \Delta_{\pi_{d}} \wedge a_{i} \neq a(s)\end{cases}
$$</p>
<p>where $a(s)$ denotes the designating action taken at state $s$. Deterministic policy is desirable in practical application because it has a predictable behavior, which is a crucial factor for designing an effective RL algorithm. In practice, we can derive a deterministic policy $\pi_{d}$ from a stochastic policy $\pi$ using the following rule:</p>
<p>$$
\boldsymbol{R}<em d="d">{\mathbf{1}}: \pi \mapsto \pi</em>
$$}=\Psi_{d}(s)= \begin{cases}1, &amp; a_{i}=a_{j} \wedge j=\underset{k}{\arg \max } \pi\left(s, a_{k}\right) \ 0, &amp; \forall a_{i} \in \Delta_{\pi} \wedge a_{i} \neq a_{j}\end{cases</p>
<p>where $\pi\left(s, a_{k}\right)$ denotes the probability of taking action $a_{k} \in \Delta_{\pi}$ in state $s$ using policy $\pi$ and $\Delta_{\pi_{d}}=\Delta_{\pi}$.
Initially, the agent is assigned a random policy $\pi_{0}$. It adjusts the policy $\pi_{0}$ to improve itself by interacting with the environment in a TE learning manner. In this respect, we call that policy $\pi_{t+1}$ is better than policy $\pi_{t}$ and denoted as $\pi_{t+1}&gt;\pi_{t}$. Therefore, we have a series of policies improved over time as follows:</p>
<p>$$
\pi_{0}&lt;\pi_{1}&lt;\ldots&lt;\pi_{t}&lt;\pi_{t+1}&lt;\ldots&lt;\pi^{*}
$$</p>
<p>This process, named policy improvement, is repeated until the agent cannot find any policy better than the optimal policy $\pi^{*}$. By this definition, however, we still do not know exactly how to compare two policies and decide which one is better. In the next subsection, we will review other metrics that can be used to evaluate a policy and then we can use these metrics to compare how "good" between different policies.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: A naive approach to finding a better policy $\pi^{\prime}$ from $\pi$.</p>
<h1>2.2 Bellman equation</h1>
<p>Remind that the agent receives a feedback reward $r_{t+1}$ for every time-step $t$ until it reaches the terminal state $s_{T}$. However, the immediate reward $r_{t+1}$ does not represent the long-term profit, we instead leverage a generalized return value $R_{t}$ at time-step $t$ :</p>
<p>$$
R_{t}=r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots+\gamma^{T-t-1} r_{T}=\sum_{i=0}^{T-t-1} \gamma^{i} r_{t+i+1}
$$</p>
<p>where $\gamma$ is a discounted factor so that $0 \leq \gamma&lt;1$. The agent becomes farsighted when $\gamma$ approaches to 1 and vice versa the agent becomes shortsighted when $\gamma$ is close to 0 . Apparently, we often select $\gamma$ closing to 1 in practice.</p>
<p>The next step is to define a value function that is used to evaluate how "good" of a certain state $s$ or a certain state-action pair $(s, a)$. Specifically, the value function of state $s$ under policy $\pi$ is calculated by obtaining expected return value from $s[110]: V_{\pi}(s)=\mathbb{E}\left[R_{t} \mid s_{t}=s, \pi\right]$. Likewise, the value function of state-action pair $(s, a)$ is $Q_{\pi}(s, a)=\mathbb{E}\left[R_{t} \mid s_{t}=s, a_{t}=a, \pi\right]$. We can leverage value functions to compare how "good" between two policies $\pi$ and $\pi^{\prime}$ using the following rule [110]:</p>
<p>$$
\pi \leq \pi^{\prime} \Longleftrightarrow\left[V_{\pi}(s) \leq V_{\pi^{\prime}}(s) \forall s\right] \vee\left[Q_{\pi}(s, a) \leq Q_{\pi^{\prime}}(s, a) \forall(s, a)\right]
$$</p>
<p>Based on (3), we can expand $V_{\pi}(s)$ and $Q_{\pi}(s, a)$ to present the relationship between two consecutive states $s=s_{t}$ and $s^{\prime}=s_{t+1}$ as below [110]:</p>
<p>$$
V_{\pi}(s)=\sum_{a} \pi(s, a) \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right)\left(\mathbb{W}<em _pi="\pi">{s \rightarrow s^{\prime} \mid a}+\gamma V</em>\right)\right)
$$}\left(s^{\prime</p>
<p>and</p>
<p>$$
Q_{\pi}(s, a)=\sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right)\left(\mathbb{W}<em _pi="\pi">{s \rightarrow s^{\prime} \mid a}+\gamma V</em>\right)\right)
$$}\left(s^{\prime</p>
<p>where $\mathbb{W}<em t_1="t+1">{s \rightarrow s^{\prime} \mid a}=\mathbb{E}\left[r</em>$ from $\pi$ using the following rule:} \mid s_{t}=s, a_{t}=a, s_{t+1}=s^{\prime}\right]$. Solving (5) or (6), we can find value function $V(s)$ or $Q(s, a)$, respectively. Equations (5) and (6) are called Bellman equations and widely used in policy improvement. For example, Fig. 4 describes a naive approach to improve a deterministic policy $\pi$. By using a TE approach, we can "explore" a different action $a_{j} \neq a_{i}$ taken at state $s_{i}$ so that $Q_{\pi}\left(s_{i}, a_{j}\right)&gt;Q_{\pi}\left(s_{i}, a_{i}\right)$. We include $a_{j}$ as a new action taken at $s_{i}$ in a derived policy $\pi^{\prime}$ while keeping other pairs of state-action unchanged. Based on (4) and (6), we can infer that $\pi&lt;\pi^{\prime}$. Therefore, it is straightforward to select a "greedy" action $a_{j}$ so that $Q_{\pi}\left(s_{i}, a_{j}\right)$ attains maximum values. Finally, we can derive an improved policy $\pi^{\prime</p>
<p>$$
\boldsymbol{R}<em i="i">{\mathbf{2}}: \pi \mapsto \pi^{\prime}=\Psi^{\prime}(s)= \begin{cases}1, &amp; a</em>
$$}=a_{j} \wedge j=\underset{k}{\arg \max } Q_{\pi}\left(s, a_{k}\right) \ 0, &amp; \forall a_{i} \in \Delta_{\pi} \wedge a_{i} \neq a_{j}\end{cases</p>
<p>This process is iterated for all pairs of $\left(s_{i}, a_{i}\right)$ until we find an optimal solution $\pi^{*}$. The idea indeed can be generalized to any stochastic policy $\pi$.</p>
<p>Instead of repeating policy improvement process, we can estimate directly the value function of optimal policy $\pi^{*}$ using the following optimality Bellman equation [110]:</p>
<p>$$
V_{\pi^{<em>}}(s)=\max <em s_prime="s^{\prime">{a} \sum</em>}} p\left(s^{\prime} \mid s, a\right)\left(\mathbb{W<em>{s \rightarrow s^{\prime} \mid a}+\gamma V</em>{\pi^{</em>}}\left(s^{\prime}\right)\right)
$$</p>
<p>and</p>
<p>$$
Q_{\pi^{<em>}}(s, a)=\sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right)\left(\mathbb{W}<em a_prime="a^{\prime">{s \rightarrow s^{\prime} \mid a}+\gamma \max </em> Q_{\pi^{}</em>}}\left(s^{\prime}, a^{\prime}\right)\right)
$$</p>
<p>After solving optimality Bellman equations (8) or (9), we can derive an optimal deterministic policy $\pi^{*}$ using (7).</p>
<p>Although we can use dynamic programming to approximate the solutions of Bellman equations, it requires the complete dynamics information of the problem. Therefore, it is infeasible due to the lack of memory and computational power of conventional computer when the number of states is large. In the next subsection, we will review two model-free RL methods (require no knowledge of transition probabilities $p\left(a_{i} \mid s\right)$ ) to approximate the value functions.</p>
<h1>2.3 RL methods</h1>
<p>In this section, we review two well-known learning schemes in RL: Monte-Carlo and temporal-difference learning. These methods do not require the dynamics information of the environment, i.e., they can deal with larger state-space problems than trivial dynamic programming approaches.</p>
<h3>2.3.1 Monte-Carlo method</h3>
<p>Monte-Carlo (MC) method estimates value function by repeatedly generating episodes and recording average return at each state or each state-action pair. Therefore, the state-value function is calculated as follows:</p>
<p>$$
V_{\pi}^{M C}(s)=\lim <em t="t">{\epsilon \rightarrow+\infty} \mathbb{E}\left[r^{i}\left(s</em>=s, \pi\right]
$$}\right) \mid s_{t</p>
<p>where $r^{i}\left(s_{t}\right)$ denotes observed return at state $s_{t}$ in episode $i$ th. Similarly, we have value function of stateaction pair:</p>
<p>$$
Q_{\pi}^{M C}(s, a)=\lim <em t="t">{i \rightarrow+\infty} \mathbb{E}\left[r^{i}\left(s</em>=a, \pi\right]
$$}, a_{t}\right) \mid s_{t}=s, a_{t</p>
<p>MC method does not require any knowledge of transition probabilities, i.e., MC method is model-free. However, this approach has made two essential assumptions to ensure the convergence happens: 1) the number of episodes is large and 2) every state and every action must be visited with a significant number of times. To make this "exploration" possible, we often use $\epsilon$-greedy strategy in policy improvement instead of (7):</p>
<p>$$
\boldsymbol{R}<em _pi="\pi">{\mathbf{3}}: \pi \mapsto \pi^{\prime}=\Psi^{\prime}(s)= \begin{cases}1-\epsilon+\frac{\epsilon}{\left|\Delta</em>
$$}(s)\right|}, &amp; a_{i}=a_{j} \wedge j=\underset{a}{\arg \max } Q_{\pi}\left(s, a_{k}\right) \ \frac{\epsilon}{\left|\Delta_{\pi}(s)\right|}, &amp; \forall a_{i} \in \Delta_{\pi} \wedge a_{i} \neq a_{j}\end{cases</p>
<p>where $\left|\Delta_{\pi}(s)\right|$ denotes number of candidate actions taken in state $s$ and $0&lt;\epsilon&lt;1$. Generally, MC algorithms are divided into two groups: on-policy and off-policy. In on-policy methods, we use policy $\pi$ for both evaluation and exploration purpose. Therefore, the policy $\pi$ must be stochastic or soft. In contrast, offpolicy uses different policy $\pi^{\prime} \neq \pi$ to generate the episodes and hence $\pi$ can be deterministic. Although off-policy is desirable due to its simplicity, on-policy method is more stable when working with continuous state-space problems and when using together with a function approximator (such as neural networks) [114].</p>
<p>Table 1: Characteristics of RL methods</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model-free</td>
<td>- Dynamics of environment is unknown <br> - Deal with larger state-space environments</td>
<td>- Requires "exploration" condition</td>
</tr>
<tr>
<td>On-policy</td>
<td>- Stable when using with function approximator <br> - Suitable with continuous state-space problems</td>
<td>- Policy must be stochastic</td>
</tr>
<tr>
<td>Off-policy</td>
<td>- Simplify algorithm design <br> - Can tackle with different kinds of problems <br> - Policy can be deterministic</td>
<td>- Unstable when using with function approximator</td>
</tr>
<tr>
<td>Bootstrapping</td>
<td>- Learn faster in most cases</td>
<td>- Not as good as nonbootstrapping methods on mean square error</td>
</tr>
</tbody>
</table>
<h1>2.3.2 Temporal-difference method</h1>
<p>Similar to MC, temporal-difference (TD) learning is also learning from experiences (model-free method). However, unlike MC, TD learning does not wait until the end of episode to make an update. It makes an update on every step within the episode by leveraging 1-step Bellman equation (5) and hence possibly providing faster convergence:</p>
<p>$$
\boldsymbol{U}<em t="t">{\mathbf{1}}: V^{i}\left(s</em>\right)\right)
$$}\right) \longleftarrow \alpha V^{i-1}\left(s_{t}\right)+(1-\alpha)\left(r_{t+1}+\gamma V^{i-1}\left(s_{t+1</p>
<p>where $\alpha$ is step-size parameter and $0&lt;\alpha&lt;1$. TD learning uses previous estimated values $V^{i-1}$ to update the current ones $V^{i}$, which is known as bootstrapping method. Basically, bootstrapping method learns faster than non-bootstrapping ones in most of the cases [110]. TD learning is also divided into two categories: on-policy TD control (Sarsa) and off-policy TD control (Q-learning). In Sarsa, the algorithm estimates value function of state-action pair based on (6):</p>
<p>$$
\boldsymbol{U}<em t="t">{\mathbf{2}}: Q^{i}\left(s</em>\right)\right)
$$}, a_{t}\right) \longleftarrow \alpha Q^{i-1}\left(s_{t}, a_{t}\right)+(1-\alpha)\left(r_{t+1}+\gamma Q^{i-1}\left(s_{t+1}, a_{t+1</p>
<p>On the other hand, Q-learning uses 1-step optimality Bellman equation (9) to perform the update, i.e., Q-learning directly approximates value function of optimal policy:</p>
<p>$$
\boldsymbol{U}<em t="t">{\mathbf{3}}: Q^{i}\left(s</em>+\gamma \frac{\max }, a_{t}\right) \longleftarrow \alpha Q^{i-1}\left(s_{t}, a_{t}\right)+(1-\alpha)\left(r_{t+1<em t_1="t+1">{a</em>\right)
$$}^{j}} Q^{i-1}\left(s_{t+1}, a_{t+1}^{j}\right)}{</p>
<p>We notice that the operator max in update rule (11) substitutes for a deterministic policy. This strongly explains why Q-learning is off-policy.</p>
<p>In practice, MC and TD learning often use table memory structure (tabular method) to save value function of each state or each state-action pair. This makes them inefficient due to lack of memory in solving complicated problems where number of states is large. Therefore, actor-critic (AC) architecture is designed to subdue this limitation. Specifically, AC includes two separate memory structures for an agent: actor and critic. Actor structure is used to select a suitable action according to the observed state and transfer to critic structure for evaluation. Critic structure uses the following TD error to decide future tendency of the selected action:</p>
<p>$$
\delta\left(a_{t}\right)=\beta\left(r_{t+1}+\gamma V\left(s_{t+1}\right)\right)-(1-\beta) V\left(s_{t}\right)
$$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Deep Q-network structure.
where $0&lt;\beta&lt;1$; and if $\delta\left(a_{t}\right)&gt;0$, the tendency to select the action $a_{t}$ in the future is high and vice versa. Furthermore, AC can be on-policy or off-policy depending on the implementation details. Table 1 and Table 2 summarize characteristics of RL methods and the comparisons between different RL methods, respectively.</p>
<p>Table 2: Comparisons between RL methods</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Dynamic <br> programming</th>
<th style="text-align: center;">On-policy <br> MC</th>
<th style="text-align: center;">Off-policy <br> MC</th>
<th style="text-align: center;">Sarsa</th>
<th style="text-align: center;">Q-learning</th>
<th style="text-align: center;">AC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">On-policy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Off-policy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Bootstrapping</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<h1>3 Deep RL: Single Agent</h1>
<h3>3.1 Deep Q-Network</h3>
<p>Deep RL is a broad term that indicates the combination between deep learning and RL to deal with highdimensional environments [3, 65, 79]. In 2015, Mnih et al. [73] at the first time announced the success of this combination by creating an autonomous agent that can play competently a series of 49 Atari games. Concisely, the authors proposed a novel structure named deep $Q$-network (DQN) that leverages the convolutional neural network (CNN) [56] to directly interpret graphical representation of input state $s$ from the environment. The output of DQN produces Q-values of all possible actions $a \in \Delta_{\tau}$ taken at state $s$, where $\Delta_{\tau}$ denotes action space [80]. Therefore, DQN can be seen as a policy network $\tau$, parameterized by $\beta$, which is continually trained so as to approximate optimal policy. Mathematically, DQN uses Bellman equation to minimize the loss function $\mathcal{L}(\beta)$ as below:</p>
<p>$$
\mathcal{L}(\beta)=\mathbb{E}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} \mid \beta\right)-Q(s, a \mid \beta)\right)^{2}\right]
$$</p>
<p>However, using neural network to approximate value function is proved to be unstable and may result</p>
<p>in divergence due to the bias originated from correlative samples [114]. To make the samples uncorrelated, Mnih et al. [73] created a target network $\tau^{\prime}$, parameterized by $\beta^{\prime}$, which is updated in every $N$ steps from estimation network $\tau$. Moreover, generated samples are stored in an experience replay memory. Samples are then retrieved randomly from the experience replay and fed into the training process, as described in Fig. 5. Therefore, equation (12) can be rewritten as:</p>
<p>$$
\left{\begin{array}{l}
\mathcal{L}<em a_prime="a^{\prime">{\mathrm{DQN}}(\beta)=\mathbb{E}\left[\left(r+\gamma \max </em>\right] \
\beta^{\prime} \longleftarrow \beta \text { for every } \mathrm{N} \text { steps }
\end{array}\right.
$$}} Q\left(s^{\prime}, a^{\prime} \mid \beta^{\prime}\right)-Q(s, a \mid \beta)\right)^{2</p>
<p>Although DQN basically solved a challenging problem in RL, the curse of dimensionality, this is just a rudimental step in solving completely real-world applications. DQN has numerous drawbacks, which can be remedied by different schemes, from a simple form to complicated modifications that we will discuss in the next subsection.</p>
<h1>3.2 DQN variants</h1>
<p>The first and simplest form of DQN's variant is double deep $Q$-network (DDQN) proposed by Hasselt [34, 35]. The idea of DDQN is to separate the selection of "greedy" action from action evaluation. In this way, DDQN expects to reduce the overestimation of Q-values in the training process. In other words, the max operator in equation (13) is decoupled into two different operators, as represented by the following loss function:</p>
<p>$$
\mathcal{L}<em a_prime="a^{\prime">{\mathrm{DDQN}}(\beta)=\mathbb{E}\left[\left(r+\gamma Q\left(s^{\prime}, \arg \max </em>\right]
$$}} Q\left(s^{\prime}, a^{\prime} \mid \beta\right) \mid \beta^{\prime}\right)-Q(s, a \mid \beta)\right)^{2</p>
<p>Empirical experiment results on 57 Atari games show that the normalized performance of DDQN without tuning is two times greater than DQN and three times greater than DQN when tuning.</p>
<p>Secondly, experience replay in DQN plays an important role to break the correlations between samples, and at the same time, remind "rare" samples that the policy network may rapidly forget. However, the fact that selecting randomly samples from experience replay does not completely separate sample data. Specifically, we prefer rare and goal-related samples to appear more frequent than redundancy ones. Therefore, Schaul et al. [99] proposed a prioritized experience replay that gives priority to a sample $i$ based on its absolute value of TD error:</p>
<p>$$
p_{i}=\left|\delta_{i}\right|=\left|r_{i}+\gamma \max <em i="i">{a} Q\left(s</em> \mid \beta\right)\right|
$$}, a \mid \beta^{\prime}\right)-Q\left(s_{i-1}, a_{i-1</p>
<p>Prioritized experience replay when combining with DDQN provides stable convergence of policy network and achieves a performance up to five times greater than DQN in terms of normalized mean score on 57 Atari games.</p>
<p>DQN's policy evaluation process is struggle to work in "redundant" situations, i.e., there are more than two candidate actions that can be selected without getting any negative results. For instance, when driving a car and there are no obstables ahead, we can follow either the left lane or the right lane. If there is an obstacle ahead in the left lane, we must be in the right lane to avoid crashing. Therefore, it is more efficient if we focus only on the road and obstacles ahead. To resolve such situations, Wang et al. [115] proposed a novel network architecture named dueling network. In dueling architecture, there are two collateral networks that coexist: one network, parameterized by $\theta$, estimates state-value function $V(s \mid \theta)$ and the other one, parameterized by $\theta^{\prime}$, estimates advantage action function $A\left(s, a \mid \theta^{\prime}\right)$. The two networks are then aggregated using the following equation to approximate Q-value function:</p>
<p>$$
Q\left(s, a \mid \theta, \theta^{\prime}\right)=V(s \mid \theta)+\left(A\left(s, a \mid \theta^{\prime}\right)-\frac{1}{\left|\Delta_{\pi}\right|} \sum_{a^{\prime}} A\left(s, a^{\prime} \mid \theta^{\prime}\right)\right)
$$</p>
<p>Because dueling network represents action-value function, it was combined with DDQN and prioritized experience replay to boost the performance of the agent up to six times more than standard DQN on the Atari domain [115].</p>
<p>Another drawback of DQN is that it uses a history of four frames as an input to policy network. DQN is therefore inefficient to solve problems where the current state depends on a significant amount of history</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Multiple agents interacting with the same environment.
information such as Double Dunk or Frostbite. These games are often called partially observable MDP problems. The straightforward solution is to replace the fully-connected layer right after the last convolutional layer of the policy network with a recurrent long short term memory, as described in [36]. This DQN's variant named deep recurrent $Q$-network (DRQN) outperforms standard DQN up to 700 percent in games Double Dunk and Frostbite. Furthermore, Lample and Chaplot [60] successfully created an agent that beats an average player on Doom, a 3D FPS (first-person shooter) environment by adding a game feature layer in DRQN. Another interesting variant of DRQN is deep attention recurrent $Q$-network (DARQN) [108]. In that paper, Sorokin et al. add attention mechanism into DRQN so that the network can focus only on important regions in the game, allowing smaller network's parameters and hence speeding the training process. As a result, DARQN achieves a score of 7263 compared with 1284 and 1421 of DQN and DRQN on game Seaquest, respectively.</p>
<h1>4 Deep RL: Multi-Agent</h1>
<p>MASs have attracted great attention because they are able to solve complex tasks through the cooperation of individual agents. Within a MAS, agents communicate with each other and interact with the environment (Fig. 6). In a multi-agent learning domain, the MDP is generalized to a stochastic game, or a Markov game. Let denote $n$ as the number of agents, $S$ as a discrete set of environmental states, and $A_{i}, i=1,2, \ldots, n$ as a set of actions for each agent. The joint action set for all agents is defined by $A=A_{1} \times A_{2} \times \ldots . \times A_{n}$. The state transition probability function is represented by $p: S \times A \times S \rightarrow[0,1]$ and the reward function is specified as $r: S \times A \times S \rightarrow \mathbb{R}^{n}$. The value function of each agent is dependent on the joint action and joint policy, which is characterized by $V^{*}: S \times A \rightarrow \mathbb{R}^{n}$. The following subsections describe challenges and MADRL solutions as well as their applications to solve real-world problems.</p>
<h3>4.1 MADRL: Challenges and Solutions</h3>
<h3>4.1.1 Non-stationarity</h3>
<p>Controlling multiple agents poses several additional challenges as compared to single agent setting such as the heterogeneity of agents, how to define suitable collective goals or the scalability to large number of agents that requires design of compact representations, and more importantly the non-stationarity problem. In a single agent environment, an agent is concerning only the outcome of its own actions. In a multi-agent domain, an agent observes not only the outcomes of its own action but also the behavior of other agents. Learning among the agents is complex because all agents potentially interact with each other and learn concurrently. The interactions among multiple agents constantly reshape the environment and lead to nonstationarity. In this case, learning among the agents sometimes causes changes in the policy of an agent, and can affect the optimal policy of other agents. The estimated potential rewards of an action would be</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: Architecture of LDQN.
inaccurate and therefore, good policies at a given point in the multi-agent setting could not remain so in the future. The convergence theory of Q-learning applied in single agent setting is not guaranteed to most multi-agent problems as the Markov property does not hold anymore in the non-stationary environment [40]. Therefore, collecting and processing information must be performed with certain recurrence while ensuring that it does not affect the agents' stability. The exploration-exploitation dilemma could be more involved under multi-agent settings.</p>
<p>The popular independent $Q$-learning [112] or experience replay based DQN [73] was not designed for the non-stationary environments. Castaneda [12] proposed two variants of DQN, namely deep repeated update Q-network (DRUQN) and deep loosely coupled Q-network (DLCQN), to deal with the non-stationarity problem in MAS. The DRUQN is developed based on the repeated update Q-learning (RUQL) model introduced in $[1,2]$. It aims to avoid policy bias by updating the action value inversely proportional to the likelihood of selecting an action. On the other hand, DLCQN relies on the loosely coupled Q-learning proposed in [119], which specifies and adjusts an independence degree for each agent using its negative rewards and observations. Through this independence degree, the agent learns to decide whether it needs to act independently or cooperate with other agents in different circumstances. Likewise, Diallo et al. [18] extended DQN to a multi-agent concurrent $D Q N$ and demonstrated that this method can converge in a non-stationary environment. Foerster et al. [25] alternatively introduced two methods for stabilising experience replay of DQN in MADRL. The first method uses the importance sampling approach to naturally decay obsolete data whilst the second method disambiguates the age of the samples retrieved from the replay memory using a fingerprint.</p>
<p>Recently, to deal with non-stationarity due to concurrent learning of multiple agents in MAS, Palmer et al. [86] presented a method namely lenient-DQN (LDQN) that applies leniency with decaying temperature values to adjust policy updates sampled from the experience replay memory (Fig. 7). That method is applied to the coordinated multi-agent object transportation problems and its performance is compared with the hysteretic-DQN (HDQN) [85]. The experimental results demonstrate the superiority of LDQN against HDQN in terms of convergence to optimal policies in a stochastic reward environment. The notion of leniency along with a scheduled replay strategy were also incorporated into the weighted double deep $Q$ network (WDDQN) in [120] to deal with non-stationarity in MAS. Experiments show the better performance of WDDQN against double DQN in two multi-agent environments with stochastic rewards and large state space.</p>
<h1>4.1.2 Partial observability</h1>
<p>In real-world applications, there are many circumstances where agents only have partial observability of the environment. In other words, complete information of states pertaining to the environment is not known</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8: Architecture of DPIQN and DRPIQN.
to the agents when they interact with the environment. In such situations, the agents observe partial information about the environment, and need to make the best decision during each time step. This type of problem can be modelled using the partially observable Markov decision process (POMDP).</p>
<p>In the current literature, a number of deep RL models have been proposed to handle POMDP. Hausknecht and Stone [36] proposed deep recurrent Q-network (DRQN) based on a long short term memory network. With the recurrent structure, the DRQN-based agents are able to learn the improved policy in a robust sense in the partially observable environment. Unlike DQN, DRQN approximates $Q(o, a)$, which is a Q-function with observation $o$ and action $a$, by a recurrent neural network. DRQN treats a hidden state of the network $h_{t-1}$ as an internal state. The DRQN therefore is characterized by the Q-function $\left(o_{t}, h_{t-1}, a ; \theta_{i}\right)$ where $\theta_{i}$ is the parameters of the network at the $i$ th training step. In [24], DRQN is extended to deep distributed recurrent $Q$-network (DDRQN) to handle multi-agent POMDP problems. The success of DDRQN is relied on three notable features, i.e. last-action inputs, inter-agent weight sharing, and disabling experience replay. The first feature, i.e. last-action inputs, requires the provision of previous action of each agent as input to its next step. The inter-agent weight sharing means that all agents use weights of only one network, which is learned during the training process. The disabling experience replay simply excludes the experience replay feature of DQN. DDRQN therefore learns a Q-function of the form $Q\left(o_{t}^{m}, h_{t-1}^{m}, m, a_{t-1}^{m}, a_{t}^{m} ; \theta_{i}\right)$ where each agent receives its own index $m$ as input. Weight sharing decreases learning time because it reduces the number of parameters to be learned. Although each agent has different observation and hidden state, this approach however assumes that agents have the same set of actions. To address complicated problems, autonomous agents often have different sets of actions. For example, UAVs manoeuvre in the air whilst robots operate on the ground. Therefore, action spaces of UAVs and robots are different and thus the inter-agent weight sharing feature cannot be applied.</p>
<p>Scaling to a system of many agents in partial observable domains is a challenging problem. Gupta et al. [30] extended the curriculum learning technique to an MAS, which integrates with three classes of deep RL, including policy gradient, temporal-difference error, and actor-critic methods. The curriculum principle is to start learning to complete simple tasks first to accumulate knowledge before proceeding to perform complicated tasks. This is suitable with an MAS environment where fewer agents initially collaborate before extending to accommodate more and more agents to complete increasingly difficult tasks. Experimental results show the vitality of the curriculum learning method in scaling deep RL algorithms to complex multiagent problems.</p>
<p>Hong et al. [42] introduced a deep policy inference $Q$-network (DPIQN) to model multi-agent systems and its enhanced version deep recurrent policy inference $Q$-network (DRPIQN) to cope with partial observability. Both DPIQN and DRPIQN are learned by adapting network's attention to policy features and their own Q-values at various stages of the training process (Fig. 8). Experiments show the better overall performance of both DPIQN and DRPIQN over the baseline DQN and DRQN [36]. Also in the context of partial observability, but extended to multi-task, multi-agent problems, Omidshafiei et al. [85] proposed a</p>
<p>method called multi-task multi-agent RL (MT-MARL) that integrates hysteretic learners [71], DRQNs [36], distillation [96], and concurrent experience replay trajectories (CERTs), which are a decentralized extension of experience replay strategy proposed in [73]. The agents are not explicitly provided with task identity (thus partial observability) whilst they cooperatively learn to complete a set of decentralized $P O M D P$ tasks with sparse rewards. This method however has a disadvantage that cannot perform in an environment with heterogeneous agents.</p>
<p>Apart from partial observability, there are circumstances that agents must deal with extremely noisy observations, which are weakly correlated with the true state of the environment. Kilinc and Montana [48] introduced a method denoted as MADDPG-M that combines DDPG and a communication medium to address these circumstances. Agents need to decide whether their observations are informative to share with other agents and the communication policies are learned concurrently with the main policies through experience. Recently, Foerster et al. [26] proposed Bayesian action decoder (BAD) algorithm for learning multiple agents with cooperative partial observable settings. A new concept, namely public belief MDP, is introduced based on BAD that employs an approximate Bayesian update to attain a public belief with publicly observable features in the environment. BAD relies on a factorised and approximate belief state to discover conventions to enable agents to learn optimal policies efficiently. This is closely relevant to theory of mind that humans normally use to interpret others' actions. Experimental results on a proof-of-principle two-step matrix game and the cooperative partial-information card game Hanabi demonstrate the efficiency and superiority of the proposed method against traditional policy gradient algorithms.</p>
<h1>4.1.3 MAS training schemes</h1>
<p>The direct extension of single agent deep RL to multi-agent environment is to learn each agent independently by considering other agents as part of the environment as the independent Q-learning algorithm proposed in [111]. This method is vulnerable to overfitting [61] and computationally expensive, and therefore the number of agents involved is limited. An alternative and popular approach is the centralized learning and decentralized execution where a group of agents can be trained simultaneously by applying a centralized method via an open communication channel [55]. Decentralized policies where each agent can take actions based on its local observations have an advantage under partial observability and in limited communications during execution. Centralized learning of decentralized policies has become a standard paradigm in multiagent settings because the learning process may happen in a simulator and a laboratory where there are no communication constraints, and extra state information is available [55, 27].</p>
<p>Gupta et al. [30] examined three different training schemes for a MAS, which consists of centralized learning, concurrent learning and parameter sharing. Centralized policy attempts to obtain a joint action from joint observations of all agents whilst the concurrent learning trains agents simultaneously using the joint reward signal. In the latter, each agent learns its own policy independently based on private observation. Alternatively, the parameter sharing scheme allows agents to be trained simultaneously using the experiences of all agents although each agent can obtain unique observations. With the ability to execute decentralized policies, parameter sharing can be used to extend a single agent deep RL algorithm to accommodate a system of many agents. Particularly, the combination of parameter sharing and TRPO, namely PS-TRPO, has been proposed in [30], and briefly summarized in Algorithm 1. The PS-TRPO has demonstrated great performance when dealing with high-dimensional observations and continuous action spaces under partial observability.</p>
<p>Foerster et al. [23] introduced reinforced inter-agent learning (RIAL) and differentiable inter-agent learning (DIAL) methods based on the centralized learning approach to improve agent learning communication. In RIAL, deep Q-learning has a recurrent structure to address the partial observability issue, in which independent Q-learning offers individual agents to learn their own network parameters. DIAL pushes gradients from one agent to another through a channel, allowing end-to-end backpropagation across agents. Likewise, Sukhbaatar et al. [109] developed communication neural net (CommNet) allowing dynamic agents to learn continuous communication alongside their policy for fully cooperative tasks. Unlike CommNet, He et al. [38] proposed a method namely deep reinforcement opponent network (DRON) that encodes observation of the opponent agent into DQN to jointly learn a policy and behaviors of opponents without domain knowledge.</p>
<p>Kong et al. [53, 54] incorporated both decentralized and centralized perspectives into the hierarchical master-slave architecture to form a model named master-slave multi-agent $R L$ (MS-MARL) to solve the</p>
<h1>Algorithm 1 PS-TRPO</h1>
<p>1: Initialize parameters of policy network $\Theta_{0}$, and trust region size $\Delta$
2: for $i \longleftarrow 0,1, \ldots$ do
3: $\quad$ Generate trajectories for all agents as $\tau \sim \pi_{\theta_{i}}$ using the policy with shared parameters.
4: $\quad$ For each agent $m$, compute the advantage values $A_{\pi_{\theta_{i}}}\left(o^{m}, m, a^{m}\right)$ with $m$ is the agent index.
5: $\quad$ Search $\pi_{\theta_{i+1}}$ that maximizes $L(\theta)=E_{o \sim p_{\theta_{k}}, a \sim \pi_{\theta_{k}}}\left[\frac{\pi_{\theta}(a \mid o, m)}{\pi_{\theta_{k}}(a \mid o, m)} A_{\theta_{k}}(o, m, a)\right]$ subject to $\bar{D}<em _theta__i="\theta_{i">{K L}\left(\pi</em>$.
6: end for
communication problem in MAS. The master agent receives and collectively processes messages from slave agents and then generates unique instructive messages to each slave agent. Slave agents use their own information and instructive messages from the master agent to take actions. This model significantly reduces the communication burden within a MAS compared to the peer-peer architecture, especially when the system has many agents.
}} | \pi_{\theta_{i+1}}\right) \leq \Delta$ where $D_{K L}$ is the KL divergence between distributions of two policies, and $p_{\theta}$ are the discounted frequencies of state visitation caused by $\pi_{\theta<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9: Centralized learning and decentralized execution based MADDPG where policies of agents are learned by the centralized critic with augmented information from other agents' observations and actions.</p>
<p>Lowe et al. [68] proposed multi-agent deep deterministic policy gradient (MADDPG) method based on the actor-critic policy gradient algorithms. MADDPG features the centralized learning and decentralized execution paradigm in which the critic uses extra information to ease training process whilst actors take actions based on their own local observations. Fig. 9 illustrates the multi-agent decentralized actor and centralized critic components of MADDPG where only actors are used during the execution phase.</p>
<p>Recently, Foerster et al. [27] introduced another multi-agent actor-critic method, namely counterfactual multi-agent (COMA), which was also relied on the centralized learning and decentralized execution scheme. Unlike MADDPG [68], COMA can handle the multi-agent credit assignment problem [33] where agents are difficult to work out their contribution to the team's success from global rewards generated by joint actions in cooperative settings. COMA however has a disadvantage that focuses only on discrete action space whilst MADDPG is able to learn continuous policies effectively.</p>
<h3>4.1.4 Continuous action spaces</h3>
<p>Most deep RL models can only be applied to discrete spaces [66]. For example, DQN [73] is limited only to problems with discrete and low-dimensional action spaces, although it can handle high-dimensional obser-</p>
<p>vation spaces. DQN aims to find action that has maximum action-value, and therefore requires an iterative optimization process at every step in the continuous action (state) spaces. Discretising the action space is a possible solution to adapt deep RL methods to continuous domains. However, this creates many problems, notably is the curse of dimensionality: the exponential increase of action numbers against the number of degrees of freedom.</p>
<p>Schulman et al. [101] proposed trust region policy optimization (TRPO) method, which can be extended to continuous states and actions, for optimizing stochastic control policies in the domain of robotic locomotion and image-based game playing. Lillicrap et al. [66] introduced an off-policy algorithm, namely deep deterministic policy gradient (DDPG), which utilizes the actor-critic architecture [52, 74] to handle the continuous action spaces. Based on the deterministic policy gradient (DPG) [106], DDPG deterministically maps states to specific actions using a parameterized actor function while keeping DQN learning on the critic side. This approach however requires a large number of training episodes to find solutions, as found common in model-free reinforcement methods. Heess et al. [39] extended DDPG to recurrent $D P G$ (RDPG) to handle problems with continuous action spaces under partial observability, where the true state is not available to the agents when making decisions. Recently, Gupta et al. [30] introduced the PS-TRPO method for multi-agent learning (see Algorithm 1). This method is based on the foundation of TRPO so that it can deal with continuous action spaces effectively.</p>
<h1>4.1.5 Transfer Learning for MADRL</h1>
<p>Training the Q-network or generally a deep RL model of a single agent is often very computationally expensive. This problem is significantly severe for a system of multiple agents. To improve the performance and reduce computational costs during training process of multiple deep RL models, several studies have promoted transfer learning for deep RL.</p>
<p>Rusu et al. [96, 97] proposed policy distillation method and progressive neural networks to promote transfer learning in the context of deep RL. These methods however are computationally complex and expensive [49]. Yin and Pan [117] likewise introduced another policy distillation architecture to apply knowledge transfer for deep RL. That method reduces training time and outperforms DQNs but its exploration strategy is still not efficient. Parisotto et al. [87] proposed the actor-mimic method for multi-task and transfer learning that improves learning speed of a deep policy network. The network can obtain an expert performance on many games simultaneously, although its model is not so complex. That method however requires a sufficient level of similarity between source and target tasks and is vulnerable to negative transfer.</p>
<p>Egorov [20] reformulated a multi-agent environment into an image like representation and utilized CNNs to estimate Q-values for each agent in question. That approach can address the scalability problem in MAS when the transfer learning method can be used to speed up the training process. A policy network trained on a different but related environment is used for learning process of other agents to reduce computational expenses. Experiments carried out on the pursuit-evasion problem [14] show the effectiveness of the transfer learning approach in the multi-agent domain.</p>
<p>Table 3 presents a summary of reviewed papers that address different multi-agent learning challenges. It can be seen that many extensions of DQN have been proposed in the literature whilst policy-based or actor-critic methods have not adequately been explored in multi-agent environments.</p>
<p>Table 3: Multi-agent learning challenges and their solving methods</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Challenges</th>
<th style="text-align: center;">Value-based</th>
<th style="text-align: center;">Actor-critic</th>
<th style="text-align: center;">Policy-based</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Partial observability</td>
<td style="text-align: center;">DRQN [36]; DDRQN [24]; RIAL and DIAL [23]; Action-specific DRQN [121]; MT-MARL [85]; PSDQN [30]; RL as a Rehearsal (RLaR) [55]</td>
<td style="text-align: center;">PS-DDPG and PS-A3C [30]; MADDPG-M [48]</td>
<td style="text-align: center;">DPIQN and DRPIQN [42]; PS-TRPO [30]; Bayesian action decoder (BAD) [26]</td>
</tr>
<tr>
<td style="text-align: center;">Nonstationarity</td>
<td style="text-align: center;">DRUQN and DLCQN [12]; Multi-agent concurrent DQN [18]; Recurrent DQN-based multi-agent importance sampling and fingerprints [25]; Hysteretic-DQN [85]; Lenient-DQN [86]; WDDQN [120]</td>
<td style="text-align: center;">MADDPG [68]; PS-A3C [30]</td>
<td style="text-align: center;">PS-TRPO [30]</td>
</tr>
<tr>
<td style="text-align: center;">Continuous action spaces</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Recurrent DPG [39]; DDPG [66]</td>
<td style="text-align: center;">TRPO [101]; PS-TRPO [30]</td>
</tr>
<tr>
<td style="text-align: center;">Multi-agent training schemes</td>
<td style="text-align: center;">Multi-agent extension of DQN [111]; RIAL and DIAL [23]; CommNet [109]; DRON [38]; MS-MARL [53, 54]; Linearly fuzzified joint Q-function for MAS [69]</td>
<td style="text-align: center;">MADDPG [68]; COMA [27]</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Transfer learning in MAS</td>
<td style="text-align: center;">Policy distillation [96]; Multi-task policy distillation [117]; Multi-agent DQN [20]</td>
<td style="text-align: center;">Progressive networks [97]</td>
<td style="text-align: center;">Actor-Mimic [87]</td>
</tr>
</tbody>
</table>
<h1>4.2 MADRL Applications</h1>
<p>Since the success of deep RL marked by the DQN proposed in [73], many algorithms have been proposed to integrate deep learning to multi-agent learning. These algorithms can solve complex problems in various fields. This section provides a survey of these applications with a focus on the integration of deep learning and MARL. Table 4 summarizes the features and limitations of approaches to these applications.</p>
<p>Prasad and Dusparic [92] introduced a MADRL model to deal with energy sharing problem in a zeroenergy community that comprises a collection of zero energy buildings, which have the total energy use over a year smaller than or equal to the renewables generation within each building. A deep RL agent is used to characterize each building to learn appropriate actions in sharing energy with other buildings. The global reward is modelled by the negative of the community energy status as reward $=-\left(\sum_{i=1}^{n} c\left(h_{i}\right)-g\left(h_{i}\right)\right)$, where $c\left(h_{i}\right)$ and $g\left(h_{i}\right)$ are the energy consumed and energy generated by the $i$ th building. The community monitoring service is introduced to manage the group membership activities such as joining and leaving the group or maintaining a list of active agents. Experiments show the superiority of the proposed model compared to the random action selection strategy in terms of net zero energy balance as a community. A limitation of the proposed approach lies in the episodic learning manner so that agent's behaviors cannot be observed in an online fashion. Further drawbacks include that the current experiments were not performed on a large scale, i.e. ten houses at maximum, and energy price scheme is not considered.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10: Federated control with hierarchical MADRL method.
A combination of hierarchical RL and MADRL methods was developed to coordinate and control multiple agents in problems where agents' privacy is prioritized [58]. Such distributed scheduling problems could be a multi-task dialogue where an automated assistant needs to help a user to plan for several independent tasks, for example, purchase a train ticket to the city, book a movie ticket, and make a dinner reservation in a restaurant. Each of these tasks is handled by a decentralized controller whilst the assistant is a metacontroller which benefits from temporal abstractions to lessen the communication complexity, and thus able to find a globally consistent solution for the user (Fig. 10). On the other hand, Leibo et al. [62] introduced a sequential social dilemma (SSD) model based on general-sum Markov games under partial observability to address the evolution of cooperation in MAS. Being able to capture sequential structure of real-world social dilemmas, SSD is an extension of matrix game social dilemma (MGSD) that has been applied to various phenomena in social science and biology $[16,8,119]$. The general-sum modelling requires solving algorithms to either track different potential equilibria for each agent or be able to find cyclic strategy consisting of multiple policies learned by using different state space sweeps [122, 89]. DQN is utilized to characterize selfinterested independent learning agents to find equilibria of the SSD, which cannot be solved by the standard evolution and learning methods used for MGSD [50]. Perolat et al. [90] also demonstrated the application of MADRL in social science phenomenon, i.e. the common-pool resource (CPR) appropriation. The proposed method comprises a spatially and temporally dynamic CPR environment [46] and an MAS of independent self-interested DQNs. Through the RL trial and error process, the CPR appropriation problem is solved by self-organization that adjusts the incentives felt by independent individual agents over time.</p>
<p>Huttenrauch et al. [45] formulated swarm systems as a special case of the decentralized POMDP [84] and used an actor-critic deep RL approach to control a group of cooperative agents. The Q-function is learned using a global state information, which can be the view of a camera capturing the scene in swarm robotics</p>
<p>Table 4: A summary of typical MADRL applications in different fields</p>
<table>
<thead>
<tr>
<th>Applications</th>
<th>Basic DLR</th>
<th>Features</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr>
<td>Federated control [58]</td>
<td>Hierarchical- <br> DQN (h- <br> DQN) [57]</td>
<td>-Divide the control problem into disjoint subtasks and leverage temporal abstractions. <br> -Use meta-controller to guide decentralized controllers. <br> -Able to solve distributed scheduling problems such as multi-task dialogue, urban traffic control.</td>
<td>-Does not address the non-stationarity problem. <br> -Number of agents is currently limited at six. <br> -Meta-controllerâ€™s optimal policy becomes complicated and inefficient when the number of agents increases.</td>
</tr>
<tr>
<td>Large-scale fleet management [67]</td>
<td>Actor-critic and DQN</td>
<td>-Reallocate vehicles ahead of time to balance the transport demands and supplies. <br> -Geographic context and collaborative context are integrated to coordinate agents. <br> -Two proposed algorithms, contextual multi-agent actor-critic and contextual deep $Q$-learning, can achieve explicit coordination among thousands of agents.</td>
<td>-Can only deal with discrete actions and each agent has a small (simplified) action space. <br> -Assume that agents in the same region at the same time interval (i.e. same spatialtemporal state) are homogeneous.</td>
</tr>
<tr>
<td>Swarm systems [45]</td>
<td>DQN and DDPG</td>
<td>-Agents can only observe local environment (partial observability) but not the global state. <br> -Use guided approach for multi-agent learning where actors make decisions based on locally sensed information whilst critic has central access to global state.</td>
<td>-Can only work with homogeneous agents. <br> -Unable to converge to meaningful policies in huge dimensionality and partial observed problem.</td>
</tr>
<tr>
<td>Traffic lights control [11]</td>
<td>DDDQN and IDQN</td>
<td>-Learning multiple agents is performed using IDQN where each agent is modelled by DDDQN. <br> -First approach to address heterogeneous multi-agent learning in urban traffic control. <br> -Fingerprint technique is used to stabilize the experience replay memory to handle non-stationarity.</td>
<td>-The proposed deep RL approach learns ineffectively in high traffic conditions. <br> -The fingerprint does not improve the performance of experience replay although the latter is required for efficient learning.</td>
</tr>
<tr>
<td>Task and resources allocation [83]</td>
<td>CommNet [109]</td>
<td>-Propose distributed task allocation where agents can request help from cooperating neighbors. <br> -Three types of agents are defined: manager, participant and mediator. <br> -Communication protocol is learned simultaneously with agentsâ€™ policies through CommNet.</td>
<td>-May not be able to deal with heterogeneous agents. <br> -Computational deficiencies regarding the decentralization and reallocation characteristics. <br> -Experiments only on small state action spaces.</td>
</tr>
<tr>
<td>Energy sharing optimization [92]</td>
<td>DQN</td>
<td>-Each building is characterized by a DRL agent to learn appropriate actions independently. <br> -Agentsâ€™ actions include: consume and store excess energy, request neighbor or supply grid for additional energy, grant or deny requests. <br> -Agents collaborate via shared or global rewards to achieve a common goal, i.e. zero-energy status.</td>
<td>-Agentsâ€™ behaviors cannot be observed in an online fashion. <br> -Limited number of houses, currently ten houses at maximum were experimented. <br> -Energy price is not considered.</td>
</tr>
<tr>
<td>Keepaway soccer [59]</td>
<td>DQN</td>
<td>-Low-dimensional state space, described by only 13 variables. <br> -Heterogeneous MAS, each agent has different experience replay memory and different network policy.</td>
<td>-Number of agents is limited, currently setting with 3 keepers vs. 2 takers. <br> -Heterogeneous learning speed is significantly lower than homogeneous case.</td>
</tr>
<tr>
<td>Action markets [100]</td>
<td>DQN</td>
<td>-Agents can trade their atomic actions in exchange for environmental reward. <br> -Reduce greedy behavior and thus negative effects of individual reward maximization. <br> -The proposed approach significantly increases the overall reward compared to methods without action trading.</td>
<td>-Agents cannot find prices for actions by themselves because they are given at design time. <br> -Strongly assume that agents cannot cheat on each other by making offers which they do not hold afterwards.</td>
</tr>
<tr>
<td>Sequential social dilemma (SSD) [62]</td>
<td>DQN</td>
<td>-Introduce an SSD model to extend MGSD to capture sequential structure of real-world social dilemmas. <br> -Describe SSDs as general-sum Markov gasses with partial observations. <br> -Multi-agent DQN is used to find equilibria of SSD problems.</td>
<td>-Assume agentâ€™s learning is independent and regard the others as part of the environment. <br> -Agents do not recursively reason about one anotherâ€™s learning.</td>
</tr>
<tr>
<td>Commonpool resource (CPR) appropriation [90]</td>
<td>DQN</td>
<td>-Introduce a new CPR appropriation model using an MAS containing spatially and temporally environment dynamics. <br> -Use the descriptive agenda [104] to describe the behaviors emerging when agents learn in the presence of other learning agents. <br> -Simulate multiple independent agents with each learned by DQN.</td>
<td>-Single agent DQN is extended to multiagent environment where the Markov assumption is no longer hold. <br> -Agents do not do anything of rational negotiation, e.g. bargaining, building consensus, or making appeals.</td>
</tr>
</tbody>
</table>
<p>examples. The group can perform complicated tasks such as search and rescue or distributed assembly although individual agent has limited sensory capability. That model has a drawback as it assumes agents to be homogenous. Calvo and Dusparic [11] proposed the use of IDQN to address the heterogeneity problem in multi-agent environment of urban traffic light control. Each agent is learned by the dueling double deep $Q$-network (DDDQN), which integrates dueling networks, double DQN and prioritized experience replay. Heterogenous agents are trained independently and simultaneously, considering other agents as part of the environment. The non-stationarity of the multi-agent environment is dealt with by a technique of</p>
<p>fingerprinting that disambiguates the age of training samples and stabilizes the replay memory.
A special application of DQN to the heterogeneous MAS where the state space is low-dimensional was presented in [59]. Experiments are performed on a multi-agent Keepaway soccer problem whose state comprises only 13 variables. To handle heterogeneity, each DQN agent is set up with different experience replay memory and different neural network. Agents cannot communicate with each other but only can observe others' behaviors. While DQNs can enhance results in terms of game score in the heterogeneous team learning settings in low-dimensional environments, their learning process is significantly slower than those in the homogeneous cases.</p>
<p>Establishing communication channels among agents during learning is an important step in designing and constructing MADRL algorithms. Nguyen et al. [82] characterized the communication channel via human knowledge represented by images and allow deep RL agents to communicate using these shared images. The asynchronous advantage actor-critic (A3C) algorithm [74] is used to learn optimal policy for each agent, which can be extended to multiple heterogeneous agents. On the other hand, Noureddine et al. [83] introduced a method, namely task allocation process using cooperative deep RL, to allow multiple agents to interact with each other and allocate resources and tasks effectively. Agents can request help from their cooperative neighbors in a loosely coupled distributed multi-agent environment. The CommNet model [109] is used to facilitate communications among agents, which are characterized by DRQN [36]. Experimental results demonstrate the great capability of that method in handling complicated task allocation problem. One of the drawbacks of that algorithm however is its limited ability in managing heterogeneous agents. In addition, its decentralization and reallocation characteristics also pose disadvantages in terms of computational time and communication overhead.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11: The contextual multi-agent actor-critic architecture proposed in [67] where decentralized execution is coordinated using the centralized value network's outputs as illustrated in the left part whilst the right part shows how context is embedded to the policy network.</p>
<p>Lin et al. [67] addressed the large-scale fleet management using MADRL by proposing two algorithms, namely contextual deep $Q$-learning and contextual multi-agent actor-critic. These algorithms aim to balance the difference between demand and supply by reallocating transportation resources that help to reduce traffic congestion and increase transportation efficiency. The contextual multi-agent actor-critic model is illustrated in Fig. 11 where a parameter-shared policy network is used to coordinate the agents, which represent available vehicles or equivalently the idle drivers.</p>
<p>Recently, Schmid et al. [100] introduced an interesting approach to an MAS where agents can trade their actions in exchange for other resources, e.g. environmental rewards. The action trading was inspired by the fundamental theorem of welfare economics that competitive markets adjust towards the Pareto efficiency. Specifically, agents need to extend their action spaces and learn two policies simultaneously: one for the</p>
<p>original stochastic reward and another for trading environmental reward. The behavior market realized from the action trading helps mitigate greedy behavior (like the tit-for-tat game theoretic strategy proposed in [63]), enable agents to incentivize other agents and reduce the negative effects of individual reward maximization. Simulation results on the iterated matrix game and the Coin game show the effectiveness of the action trading method as it increases the social welfare, measured in terms of overall rewards of all agents.</p>
<h1>5 Conclusions and Research Directions</h1>
<p>This paper presents an overview of different challenges in multi-agent learning and solutions to these challenges using deep RL methods. We group the surveyed papers into five categories, including non-stationarity, partial observability, multi-agent training schemes, multi-agent transfer learning, and continuous state and action spaces. We have highlighted advantages and disadvantages of the approaches to address the challenges. Applications of MADRL methods in different fields are also reviewed thoroughly. We have found that the integration of deep learning into traditional MARL methods has been able to solve many complicated problems, such as urban traffic light control, energy sharing problem in a zero-energy community, large-scale fleet management, task and resources allocation, swarm robotics, and social science phenomena. The results indicate that deep RL-based methods provide a viable approach to handling complicated tasks in the MAS domain.</p>
<p>Learning from demonstration including imitation learning and inverse $R L$ has demonstrated effectiveness in single agent deep RL [91]. On one hand, imitation learning tries to map between states to actions as a supervised approach. It directly generalizes the expert strategy to unvisited states so that it is close to a multi-class classification problem in cases of finite action set. On the other hand, inverse RL agent needs to infer a reward function from the expert demonstrations. Inverse RL assumes that the expert policy is optimal regarding the unknown reward function [31, 32]. These methods however have not yet been explored fully in multi-agent environments. Both imitation learning and inverse RL have great potential for applications in MAS. It is expected that they can reduce the learning time and improve the effectiveness of MAS. A very straightforward challenge arose from these applications is the requirement of multiple experts who are able to demonstrate the tasks collaboratively. Furthermore, the communication and reasoning capabilities of experts are difficult to be characterized and modelled by autonomous agents in the MAS domain. These pose important research questions towards extensions of imitation learning and inverse RL to MADRL methods. In addition, for complicated tasks or behaviors which are difficult for humans to demonstrate, there is a need of alternative methods that allow human preferences to be integrated into deep RL [13, 81, 82].</p>
<p>Deep RL has considerably facilitated autonomy, which allows to deploy many applications in robotics or autonomous vehicles. The most common drawback of deep RL models however is the ability to interact with human through human-machine teaming technologies. In complex and adversarial environments, there is a critical need for human intellect teamed with technology because humans alone cannot sustain the volume, and machines alone cannot issue creative responses when new situations are introduced. Recent advances of human-on-the-loop architecture [78] can be fused with MADRL to integrate humans and autonomous agents to deal with complex problems. In the conventional human-in-the-loop setting, agents perform their assigned tasks autonomously for a period, then stop and wait for human commands before continuing in this rate-limited fashion. In human-on-the-loop, agents execute their tasks autonomously until completion, with a human in a monitoring or supervisory role reserving the ability to intervene in operations carried out by agents. A human-on-the-loop-based architecture can be fully autonomous if human supervisors allow task completion by agents entirely on their own [78].</p>
<p>Model-free deep RL has been able to solve many complicated problems both in single agent and multiagent domains. This category of methods however requires a huge number of samples and long learning time to achieve a good performance. Model-based methods have demonstrated effectiveness in terms of sample efficiency, transferability and generality in various problems using single as well as multi-agent models. Although the deep learning extensions of the model-based methods have been studied recently in single agent domain, e.g. [64, 29, 22, 77, 103, 15], these extensions have not been investigated widely in the multi-agent domain. This creates a research gap that could be developed to a research direction in model-based MADRL. In addition, dealing with high-dimensional observations using model-based approaches or combining elements of model-based planning and model-free policy is another active, exciting but under-explored research area.</p>
<p>Scaling to large systems, especially dealing with many heterogeneous agents, has been a primary challenge in RL research domain since its first days. As the world dynamics become more and more complex, this challenge has always been required to resolve. Since agents have common behaviors such as actions, domain knowledge, and goals (homogeneous agents), the scalability can be achievable by (partially) centralized training and decentralized execution [94, 28]. In the heterogeneous setting with many agents, the key challenge is how to provide the most optimal solution and maximize the task completion success based on selflearning with effective coordinative and cooperative strategies among the agents. This is more problematic in hostile environments where communications among agents are limited and in scenarios that involve more heterogeneous agents as the credit assignment problem become increasingly difficult. A research direction to address these difficulties is well worth an investigation.</p>
<p>Regarding applications of multi-agent learning, there have been many studies using traditional MARL methods to solve various problems such as controlling a group of autonomous vehicles or drones [43], robot soccer [102], controlling traffic signals [75], coordinating collaborative bots in factories and warehouse [47], controlling electrical power networks [93] or optimizing distributed sensor networks [37], automated trading [88], machine bidding in competitive e-commerce and financial markets [9], resource management [44], transportation [21], and phenomena of social sciences [62]. Since the emergence of DQN [73], efforts to extend traditional RL to deep RL in the multi-agent domain have been found in the literature but they are still very limited (see Table 4 for applications available in the current literature). Many applications of MARL can now be solved effectively by MADRL based on its high-dimension handling capability. Therefore, there is a need of further empirical research to apply MADRL methods to effectively solve complex real-world problems such as the aforementioned applications.</p>
<h1>References</h1>
<p>[1] Abdallah, S., and Kaisers, M. (2013, May). Addressing the policy-bias of Q-learning by repeating updates. In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems (pp. 1045-1052).
[2] Abdallah, S., and Kaisers, M. (2016). Addressing environment non-stationarity by repeating Q-learning updates. The Journal of Machine Learning Research, 17(1), 1582-1612.
[3] Arulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A. (2017). Deep reinforcement learning: a brief survey. IEEE Signal Processing Magazine, 34(6), 26-38.
[4] Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Kttler, H., ... and Schrittwieser, J. (2016). DeepMind lab. arXiv preprint arXiv:1612.03801.
[5] Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning environment: an evaluation platform for general agents. Journal of Artificial Intelligence Research, 47, 253-279.
[6] Bellman, R. (1952). On the theory of dynamic programming. Proceedings of the National Academy of Sciences, 38(8), 716-719.
[7] Benbrahim, H., and Franklin, J. A. (1997). Biped dynamic walking using reinforcement learning. Robotics and Autonomous Systems, 22(3-4), 283-302.
[8] Bloembergen, D., Tuyls, K., Hennes, D., and Kaisers, M. (2015). Evolutionary dynamics of multi-agent learning: a survey. Journal of Artificial Intelligence Research, 53, 659-697.
[9] Brandouy, O., Mathieu, P., and Veryzhenko, I. (2011, January). On the design of agent-based artificial stock markets. In International Conference on Agents and Artificial Intelligence (pp. 350-364).
[10] Busoniu, L., Babuska, R., and De Schutter, B. (2008). A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics-Part C: Applications and Reviews, 38 (2), 156-172.</p>
<p>[11] Calvo, J. J. A., and Dusparic, I. (2018). Heterogeneous multi-agent deep reinforcement learning for traffic lights control. The 26th Irish Conference on Artificial Intelligence and Cognitive Science (pp. 1-12), Dublin, Ireland.
[12] Castaneda, A. O. (2016). Deep Reinforcement Learning Variants of Multi-Agent Learning Algorithms (Master's Thesis, School of Informatics, University of Edinburgh).
[13] Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems (pp. 42994307).
[14] Chung, T. H., Hollinger, G. A., and Isler, V. (2011). Search and pursuit-evasion in mobile robotics. Autonomous Robots, 31(4), 299.
[15] Corneil, D., Gerstner, W., and Brea, J. (2018). Efficient model-based deep reinforcement learning with variational state tabulation. arXiv preprint arXiv:1802.04325.
[16] de Cote, E. M., Lazaric, A., and Restelli, M. (2006, May). Learning to cooperate in multi-agent social dilemmas. In Proceedings of the 5th International Joint Conference on Autonomous Agents and Multiagent Systems (pp. 783-785). ACM.
[17] Deng, L., and Yu, D. (2014). Deep learning: methods and applications. Foundations and Trends in Signal Processing, 7(34), 197-387.
[18] Diallo, E. A. O., Sugiyama, A., and Sugawara, T. (2017, December). Learning to coordinate with deep reinforcement learning in doubles Pong game. In Machine Learning and Applications (ICMLA), 2017 16th IEEE International Conference on (pp. 14-19). IEEE.
[19] Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016, June). Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning (pp. 1329-1338).
[20] Egorov, M. (2016). Multi-agent deep reinforcement learning. Stanford University.
[21] Fernandez-Gauna, B., Etxeberria-Agiriano, I., and Grana, M. (2015). Learning multirobot hose transportation and deployment by distributed round-robin Q-learning. PloS One, 10(7), e0127129.
[22] Finn, C., and Levine, S. (2017, May). Deep visual foresight for planning robot motion. In Robotics and Automation (ICRA), 2017 IEEE International Conference on (pp. 2786-2793). IEEE.
[23] Foerster, J., Assael, Y. M., de Freitas, N., and Whiteson, S. (2016). Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems (pp. 2137-2145).
[24] Foerster, J. N., Assael, Y. M., de Freitas, N., and Whiteson, S. (2016). Learning to communicate to solve riddles with deep distributed recurrent Q-networks. arXiv preprint arXiv:1602.02672.
[25] Foerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P. H., Kohli, P., and Whiteson, S. (2017, July). Stabilising experience replay for deep multi-agent reinforcement learning. In International Conference on Machine Learning (pp. 1146-1155).
[26] Foerster, J. N., Song, F., Hughes, E., Burch, N., Dunning, I., Whiteson, S., ... and Bowling, M. (2018). Bayesian action decoder for deep multi-agent reinforcement learning. arXiv preprint arXiv:1811.01458.
[27] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. (2018). Counterfactual multiagent policy gradients. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (pp. 2974-2982).
[28] Foerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and Mordatch, I. (2018, July). Learning with opponent-learning awareness. In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems (pp. 122-130).</p>            </div>
        </div>

    </div>
</body>
</html>
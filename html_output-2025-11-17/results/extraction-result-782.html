<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-782 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-782</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-782</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-222209017</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.03790v1.pdf" target="_blank">Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games have emerged as an important test-bed for Reinforcement Learning (RL) research, requiring RL agents to combine grounded language understanding with sequential decision making. In this paper, we examine the problem of infusing RL agents with commonsense knowledge. Such knowledge would allow agents to efficiently act in the world by pruning out implausible actions, and to perform look-ahead planning to determine how current actions might affect future world states. We design a new text-based gaming environment called TextWorld Commonsense (TWC) for training and evaluating RL agents with a specific kind of commonsense knowledge about objects, their attributes, and affordances. We also introduce several baseline RL agents which track the sequential context and dynamically retrieve the relevant commonsense knowledge from ConceptNet. We show that agents which incorporate commonsense knowledge in TWC perform better, while acting more efficiently. We conduct user-studies to estimate human performance on TWC and show that there is ample room for future improvement.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e782.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e782.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text+Commonsense (CDC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based Reinforcement Learning Agent with Dynamic Commonsense Subgraph (Contextual Direct Connections)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based RL agent that combines recurrent encoders over observations/actions with a dynamically extracted ConceptNet subgraph (Contextual Direct Connections), encodes the graph with GATs and Numberbatch embeddings, fuses graph and text via co-attention, and selects actions with an A2C policy using concatenated context/graph/action features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Text+Commonsense (Contextual Direct Connections, CDC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular architecture: (1) Observation encoder: bidirectional GRU over token embeddings (GloVe/Numberbatch). (2) Action encoder: GRU for admissible-text actions. (3) Context encoder: GRU over past observations produces state vector s_t. (4) Dynamic commonsense subgraph G_t^C: map noun-chunks (spaCy + substring match) to ConceptNet nodes; retrieve subgraph using CDC (retain only object-to-container edges), DC (direct links), or NG (neighborhood expansion). (5) Graph encoder: initialize node vectors with pretrained Numberbatch embeddings (and sentinel); update via multi-head Graph Attention Networks (GAT). (6) Knowledge integration: bidirectional co-attention (trilinear similarity) between token-level context encodings and graph node encodings to produce re-contextualized graph vectors; compute action-specific graph encoding g_t_i via attention driven by [s_t; a_t_i]. (7) Action selection: concatenate [s_t; g_t_i; a_t_i] -> MLP -> softmax; trained with Advantage Actor-Critic (A2C).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) / TextWorld engine</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text-based house cleanup benchmark built on TextWorld. The agent receives textual observations (room descriptions, inventory, objects) and must place objects into commonsense locations (object, room, container triples). Instances vary by difficulty (easy/medium/hard), include multi-room layouts, and require aggregation of commonsense knowledge and multi-step interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet (ConceptNet 5.x) as an external commonsense knowledge base; Numberbatch pretrained KG embeddings (from ConceptNet); spaCy noun-chunk extractor (for mapping text to KB concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured knowledge graph (nodes and labeled edges/triples), textual concept matches (strings mapped from observations), dense node embeddings (Numberbatch vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Hybrid belief composed of: (a) sequential context encoding s_t (GRU over observation history) representing an implicit belief/state summary; (b) an evolving commonsense subgraph G_t^C (set of observed concepts E_t and retrieved neighbors/edges) encoded with GATs. The two modalities are fused via co-attention and an action-conditioned attention to yield per-action graph encodings g_t_i.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep t the agent: (1) extracts noun-chunks from the current textual observation via spaCy and maps them to ConceptNet concepts (e_t); (2) merges e_t with prior observed-concepts E_{t-1} to form E_t and updates/expands the commonsense subgraph G_t^C via the chosen retrieval strategy (DC/CDC/NG); (3) initializes/updates node features from Numberbatch and runs message-passing with multi-head GATs to update node embeddings; (4) recomputes co-attention between updated graph node embeddings and the current context tokens to produce re-contextualized graph vectors; (5) computes action-specific graph summary g_t_i by attending the graph using [s_t; a_t_i]. The context GRU s_t is updated from the new textual observation o_t in the standard recurrent way.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free RL policy (A2C) augmented with external knowledge: the agent performs planning implicitly via learned policy conditioned on fused context+graph representations and attention (no explicit search planner). The commonsense graph provides structured priors that support look-ahead reasoning within the learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>No explicit pathfinding algorithm; navigation actions (e.g., 'go north', 'go to room') are selected by the learned policy based on the context encoding and graph-augmented action scoring. Spatial movement is handled as atomic text actions rather than by computing explicit routes (no A*/graph search reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Commonsense-augmented variants (CDC retrieval + Numberbatch) outperform text-only baselines: qualitative improvements in normalized score and reduced number of steps. Reported examples: replacing GloVe with Numberbatch reduced average steps by ~3 in easy games and ~7 in medium games; CDC retrieval outperformed naive DC retrieval. Manually curated subgraphs (see other variant) further reduced steps by ~2-5 on easy/medium. (Exact normalized scores per agent and condition are reported in Table 4 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Text-only baselines (LSTM-A2C, DRRN, LM-NSP) achieved lower normalized scores and required more steps on average, particularly in medium and hard levels; pretrained LMs (GPT2-NSP, BERT-NSP) performed poorly when not tuned to commonsense structure. (Exact values in Table 4.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamically retrieving and integrating a focused commonsense subgraph from ConceptNet (CDC) improves sample efficiency and reduces exploration compared to text-only agents; fusing graph and textual context via co-attention and GAT message-passing yields useful action-specific knowledge encodings; evolving the KB subgraph incrementally (per timestep) helps avoid overwhelming the learner and focuses exploration; manually curated subgraphs can further improve performance but are not perfect; despite gains, a substantial gap remains to human performance, especially in multi-room/hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e782.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e782.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG_Evolve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolving Commonsense Subgraph Knowledge-Aware Agent (KG_Evolve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the Text+Commonsense agent where the ConceptNet subgraph is built and exposed to the agent incrementally based on entities observed so far ('evolve' setting), shown to yield better learning and fewer noisy explorations than exposing the full graph upfront.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG_Evolve (Text+Commonsense with evolving subgraph)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same base architecture as Text+Commonsense (GRU encoders, GAT graph encoder, co-attention, A2C policy), but the commonsense subgraph G_t^C is constructed sequentially: at each timestep only relations centered on entities observed so far are fetched/attached, limiting the KB scope exposed to the agent and allowing the graph to 'evolve' with exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) / Kitchen Cleanup variant (TextWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-based cleanup tasks where the agent sees only textual descriptions of its current location and inventory; Kitchen Cleanup is a TextWorld instance with 10 relevant objects and distractors where the goal is to place objects in appropriate locations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet (subgraph extracts) accessed incrementally; spaCy for entity extraction; Numberbatch embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph fragments (nodes/edges) focused on observed concepts; dense node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Context GRU s_t plus an incrementally constructed commonsense graph G_t^C representing only observed entities and their retrieved local neighborhoods; node embeddings updated via GATs and fused with context via co-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep, newly observed concepts are mapped to KB nodes and added to E_t; only edges and neighbor nodes relevant to the newly added entities (and previously observed ones) are fetched, producing a growing G_t^C; node features are re-embedded/updated with GAT message-passing and re-fused with context as in the main agent.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (A2C) using a progressively revealed knowledge graph to bias exploration; implicit planning via policy conditioned on evolving belief.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation actions chosen by policy using context+graph features; no explicit path-search algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>KG_Evolve outperforms both the simple text-only agent and the KG_Full agent in the Kitchen Cleanup experiment (higher average score and fewer interactions). Qualitatively the paper reports KG_Evolve yields faster convergence and better efficiency (plots in Figure 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Text-only baseline scored lower and required more interactions; KG_Evolve strictly improves over that baseline in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Feeding the KB incrementally (evolve) helps the RL agent focus exploration and avoid being overwhelmed by noisy/irrelevant knowledge present in a full graph; KG_Evolve achieves better sample efficiency and higher scores than presenting the full KB upfront (KG_Full).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e782.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e782.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG_Full</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Commonsense Subgraph Knowledge-Aware Agent (KG_Full)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where the agent is given the full commonsense subgraph (extracted from ConceptNet using the entity list) at the start of the episode; found to be less effective than the evolving subgraph because the agent is overwhelmed by excessive knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG_Full (Text+Commonsense with full pre-extracted subgraph)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same base model as Text+Commonsense, except that prior to starting the episode the full commonsense subgraph (all relations among the known entities of the instance) is constructed and provided to the agent for its entire trajectory rather than constructed incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) / Kitchen Cleanup</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-based cleanup tasks (see KG_Evolve).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet full subgraph extraction; Numberbatch; spaCy for initial mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured knowledge graph (potentially large) and dense embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Context GRU s_t plus a static pre-extracted commonsense graph G^C provided at episode start; node embeddings updated via GATs during training but the graph topology does not evolve with observations.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Context s_t updates per observation as usual; the pre-extracted graph is available immediately and its node embeddings are updated by GATs but no incremental expansion based on new observations occurs (graph topology static).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (A2C) that can use the full KB; no explicit symbolic planner. In practice, exposure to the full KB led to noisier exploration and worse learning than KG_Evolve.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>No explicit path planning; navigation is action selection by the learned policy using the full graph as additional input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>KG_Full improves over text-only baselines in some settings but is outperformed by KG_Evolve; when presented with the full pre-extracted KB the agent tended to explore noisily and learned less efficiently (see Figure 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Compared to the no-KB baseline (Text-only), KG_Full shows improvements but smaller and less consistent than KG_Evolve.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing the complete KB at once can overwhelm the learner and induce noisy exploration; a focused, incremental KB (KG_Evolve) is empirically preferable for RL in partially observable text environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e782.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e782.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text+ManualGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based Agent Augmented with Manually-Curated ConceptNet Subgraph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Text+Commonsense variant where relevant commonsense paths between objects and their goal locations are manually extracted (shortest paths within a 2-hop neighborhood and pruned) and provided to the agent; yields stronger improvements but requires manual effort and can be imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Text+ManualGraph (manual ConceptNet subgraph augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same underlying model (GRU context, action encoders, GAT graph encoder, co-attention, A2C) but instead of automatic retrieval the commonsense subgraph is manually curated: the authors extract commonsensical paths between entities and goal locations from ConceptNet (shortest paths within a 2-hop neighborhood) and prune noisy nodes/edges before feeding the subgraph to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-based cleanup games as above.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet (manual inspection and path extraction), Numberbatch embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured curated graph fragments (nodes/edges) and dense node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Context GRU s_t plus a manually constructed graph provided as external knowledge; node embeddings processed via GAT and fused with context as in the main model.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The manual subgraph is provided (and pruned) before/while the agent trains; belief updates follow the standard procedure: context update via GRU and re-embedding of graph nodes via GATs; no automatic expansion of the manual graph is described.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (A2C) with curated KB input; manual graphs provide stronger priors that accelerate learning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Policy-driven navigation; no explicit path search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Agents augmented with manual graphs perform better than other automated retrieval mechanisms, showing average reductions of ~2â€“5 steps in easy and medium games (paper reports these average reductions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Compared to the same architecture using automated retrieval (DC/CDC/NG) or text-only variants, manual-graph variants show improved step-efficiency and sometimes higher scores, but manual curation is not a gold standard and can be imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curated, high-precision subgraphs can further reduce exploration and improve performance, indicating the importance of retrieval precision; however, manual curation is labor-intensive and may still omit useful connections, so automated, focused retrieval methods remain preferable for scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e782.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e782.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (Graph-based A2C Baseline from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior state-of-the-art baseline (Ammanabrolu & Hausknecht 2020) that uses a knowledge-graph representation of the game environment (built from observations) to constrain/exploit the action space and guide RL (A2C) exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Constrained Reinforcement Learning for Natural Language Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C (baseline from Ammanabrolu & Hausknecht 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in prior work and used as a baseline here: constructs a knowledge graph of the environment from textual observations and uses graph-structured information to guide an A2C policy over text actions. The paper references and evaluates this model as a competitor but does not re-implement its internal details beyond referencing the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld / TWC (used as a baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based, partially observable environments; KG-A2C constructs a game knowledge graph from observations to inform action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Internal knowledge graph constructed from game observations (not necessarily external KB like ConceptNet in its original form); in prior work the agent uses a constructed KG of the game's entities/relations.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured in-environment knowledge graph (nodes/edges) built from observations; vector embeddings derived from that graph in the original method.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Maintains an explicit knowledge graph built from the agent's observations to represent state (as described in the cited baseline paper); this functions as the agent's belief about the world.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The agent incrementally updates its internal game-knowledge graph as new observations arrive; exact update mechanics follow the baseline (Ammanabrolu & Hausknecht 2020) and are not re-specified in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Graph-augmented A2C policy that uses the constructed knowledge graph to bias action selection; planning is implicit in the learned policy rather than via explicit symbolic search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Not detailed in this paper; the baseline guides exploration via the environment-constructed knowledge graph rather than explicit pathfinding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported as a competitive baseline in the experiments; KG-A2C performed relatively well compared to text-only baselines but the paper's commonsense-augmented agents (Text+Commonsense variants) also compare favorably. Exact numbers are included in Table 4 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as a competitive baseline to show that structured knowledge of the environment (built from observations) helps exploration; the TWC paper shows that externally-sourced commonsense (ConceptNet) integrated dynamically also yields improvements and that focused retrieval (CDC/evolve) is important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph Constrained Reinforcement Learning for Natural Language Action Spaces <em>(Rating: 2)</em></li>
                <li>Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games <em>(Rating: 2)</em></li>
                <li>Integrated common sense learning and planning in POMDPs <em>(Rating: 2)</em></li>
                <li>Dungeons and DQNs: Toward Reinforcement Learning Agents that Play Tabletop Roleplaying Games <em>(Rating: 1)</em></li>
                <li>Towards deep symbolic reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-782",
    "paper_id": "paper-222209017",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "Text+Commonsense (CDC)",
            "name_full": "Text-based Reinforcement Learning Agent with Dynamic Commonsense Subgraph (Contextual Direct Connections)",
            "brief_description": "A text-based RL agent that combines recurrent encoders over observations/actions with a dynamically extracted ConceptNet subgraph (Contextual Direct Connections), encodes the graph with GATs and Numberbatch embeddings, fuses graph and text via co-attention, and selects actions with an A2C policy using concatenated context/graph/action features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Text+Commonsense (Contextual Direct Connections, CDC)",
            "agent_description": "Modular architecture: (1) Observation encoder: bidirectional GRU over token embeddings (GloVe/Numberbatch). (2) Action encoder: GRU for admissible-text actions. (3) Context encoder: GRU over past observations produces state vector s_t. (4) Dynamic commonsense subgraph G_t^C: map noun-chunks (spaCy + substring match) to ConceptNet nodes; retrieve subgraph using CDC (retain only object-to-container edges), DC (direct links), or NG (neighborhood expansion). (5) Graph encoder: initialize node vectors with pretrained Numberbatch embeddings (and sentinel); update via multi-head Graph Attention Networks (GAT). (6) Knowledge integration: bidirectional co-attention (trilinear similarity) between token-level context encodings and graph node encodings to produce re-contextualized graph vectors; compute action-specific graph encoding g_t_i via attention driven by [s_t; a_t_i]. (7) Action selection: concatenate [s_t; g_t_i; a_t_i] -&gt; MLP -&gt; softmax; trained with Advantage Actor-Critic (A2C).",
            "environment_name": "TextWorld Commonsense (TWC) / TextWorld engine",
            "environment_description": "A text-based house cleanup benchmark built on TextWorld. The agent receives textual observations (room descriptions, inventory, objects) and must place objects into commonsense locations (object, room, container triples). Instances vary by difficulty (easy/medium/hard), include multi-room layouts, and require aggregation of commonsense knowledge and multi-step interaction.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet (ConceptNet 5.x) as an external commonsense knowledge base; Numberbatch pretrained KG embeddings (from ConceptNet); spaCy noun-chunk extractor (for mapping text to KB concepts).",
            "tool_output_types": "Structured knowledge graph (nodes and labeled edges/triples), textual concept matches (strings mapped from observations), dense node embeddings (Numberbatch vectors).",
            "belief_state_mechanism": "Hybrid belief composed of: (a) sequential context encoding s_t (GRU over observation history) representing an implicit belief/state summary; (b) an evolving commonsense subgraph G_t^C (set of observed concepts E_t and retrieved neighbors/edges) encoded with GATs. The two modalities are fused via co-attention and an action-conditioned attention to yield per-action graph encodings g_t_i.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep t the agent: (1) extracts noun-chunks from the current textual observation via spaCy and maps them to ConceptNet concepts (e_t); (2) merges e_t with prior observed-concepts E_{t-1} to form E_t and updates/expands the commonsense subgraph G_t^C via the chosen retrieval strategy (DC/CDC/NG); (3) initializes/updates node features from Numberbatch and runs message-passing with multi-head GATs to update node embeddings; (4) recomputes co-attention between updated graph node embeddings and the current context tokens to produce re-contextualized graph vectors; (5) computes action-specific graph summary g_t_i by attending the graph using [s_t; a_t_i]. The context GRU s_t is updated from the new textual observation o_t in the standard recurrent way.",
            "planning_approach": "Model-free RL policy (A2C) augmented with external knowledge: the agent performs planning implicitly via learned policy conditioned on fused context+graph representations and attention (no explicit search planner). The commonsense graph provides structured priors that support look-ahead reasoning within the learned policy.",
            "uses_shortest_path_planning": false,
            "navigation_method": "No explicit pathfinding algorithm; navigation actions (e.g., 'go north', 'go to room') are selected by the learned policy based on the context encoding and graph-augmented action scoring. Spatial movement is handled as atomic text actions rather than by computing explicit routes (no A*/graph search reported).",
            "performance_with_tools": "Commonsense-augmented variants (CDC retrieval + Numberbatch) outperform text-only baselines: qualitative improvements in normalized score and reduced number of steps. Reported examples: replacing GloVe with Numberbatch reduced average steps by ~3 in easy games and ~7 in medium games; CDC retrieval outperformed naive DC retrieval. Manually curated subgraphs (see other variant) further reduced steps by ~2-5 on easy/medium. (Exact normalized scores per agent and condition are reported in Table 4 of the paper.)",
            "performance_without_tools": "Text-only baselines (LSTM-A2C, DRRN, LM-NSP) achieved lower normalized scores and required more steps on average, particularly in medium and hard levels; pretrained LMs (GPT2-NSP, BERT-NSP) performed poorly when not tuned to commonsense structure. (Exact values in Table 4.)",
            "has_tool_ablation": true,
            "key_findings": "Dynamically retrieving and integrating a focused commonsense subgraph from ConceptNet (CDC) improves sample efficiency and reduces exploration compared to text-only agents; fusing graph and textual context via co-attention and GAT message-passing yields useful action-specific knowledge encodings; evolving the KB subgraph incrementally (per timestep) helps avoid overwhelming the learner and focuses exploration; manually curated subgraphs can further improve performance but are not perfect; despite gains, a substantial gap remains to human performance, especially in multi-room/hard tasks.",
            "uuid": "e782.0",
            "source_info": {
                "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG_Evolve",
            "name_full": "Evolving Commonsense Subgraph Knowledge-Aware Agent (KG_Evolve)",
            "brief_description": "A variant of the Text+Commonsense agent where the ConceptNet subgraph is built and exposed to the agent incrementally based on entities observed so far ('evolve' setting), shown to yield better learning and fewer noisy explorations than exposing the full graph upfront.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG_Evolve (Text+Commonsense with evolving subgraph)",
            "agent_description": "Same base architecture as Text+Commonsense (GRU encoders, GAT graph encoder, co-attention, A2C policy), but the commonsense subgraph G_t^C is constructed sequentially: at each timestep only relations centered on entities observed so far are fetched/attached, limiting the KB scope exposed to the agent and allowing the graph to 'evolve' with exploration.",
            "environment_name": "TextWorld Commonsense (TWC) / Kitchen Cleanup variant (TextWorld)",
            "environment_description": "Partially observable text-based cleanup tasks where the agent sees only textual descriptions of its current location and inventory; Kitchen Cleanup is a TextWorld instance with 10 relevant objects and distractors where the goal is to place objects in appropriate locations.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet (subgraph extracts) accessed incrementally; spaCy for entity extraction; Numberbatch embeddings.",
            "tool_output_types": "Structured graph fragments (nodes/edges) focused on observed concepts; dense node embeddings.",
            "belief_state_mechanism": "Context GRU s_t plus an incrementally constructed commonsense graph G_t^C representing only observed entities and their retrieved local neighborhoods; node embeddings updated via GATs and fused with context via co-attention.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep, newly observed concepts are mapped to KB nodes and added to E_t; only edges and neighbor nodes relevant to the newly added entities (and previously observed ones) are fetched, producing a growing G_t^C; node features are re-embedded/updated with GAT message-passing and re-fused with context as in the main agent.",
            "planning_approach": "Learned policy (A2C) using a progressively revealed knowledge graph to bias exploration; implicit planning via policy conditioned on evolving belief.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation actions chosen by policy using context+graph features; no explicit path-search algorithm.",
            "performance_with_tools": "KG_Evolve outperforms both the simple text-only agent and the KG_Full agent in the Kitchen Cleanup experiment (higher average score and fewer interactions). Qualitatively the paper reports KG_Evolve yields faster convergence and better efficiency (plots in Figure 9).",
            "performance_without_tools": "Text-only baseline scored lower and required more interactions; KG_Evolve strictly improves over that baseline in reported experiments.",
            "has_tool_ablation": true,
            "key_findings": "Feeding the KB incrementally (evolve) helps the RL agent focus exploration and avoid being overwhelmed by noisy/irrelevant knowledge present in a full graph; KG_Evolve achieves better sample efficiency and higher scores than presenting the full KB upfront (KG_Full).",
            "uuid": "e782.1",
            "source_info": {
                "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG_Full",
            "name_full": "Full Commonsense Subgraph Knowledge-Aware Agent (KG_Full)",
            "brief_description": "A variant where the agent is given the full commonsense subgraph (extracted from ConceptNet using the entity list) at the start of the episode; found to be less effective than the evolving subgraph because the agent is overwhelmed by excessive knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG_Full (Text+Commonsense with full pre-extracted subgraph)",
            "agent_description": "Same base model as Text+Commonsense, except that prior to starting the episode the full commonsense subgraph (all relations among the known entities of the instance) is constructed and provided to the agent for its entire trajectory rather than constructed incrementally.",
            "environment_name": "TextWorld Commonsense (TWC) / Kitchen Cleanup",
            "environment_description": "Partially observable text-based cleanup tasks (see KG_Evolve).",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet full subgraph extraction; Numberbatch; spaCy for initial mapping.",
            "tool_output_types": "Structured knowledge graph (potentially large) and dense embeddings.",
            "belief_state_mechanism": "Context GRU s_t plus a static pre-extracted commonsense graph G^C provided at episode start; node embeddings updated via GATs during training but the graph topology does not evolve with observations.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Context s_t updates per observation as usual; the pre-extracted graph is available immediately and its node embeddings are updated by GATs but no incremental expansion based on new observations occurs (graph topology static).",
            "planning_approach": "Learned policy (A2C) that can use the full KB; no explicit symbolic planner. In practice, exposure to the full KB led to noisier exploration and worse learning than KG_Evolve.",
            "uses_shortest_path_planning": false,
            "navigation_method": "No explicit path planning; navigation is action selection by the learned policy using the full graph as additional input.",
            "performance_with_tools": "KG_Full improves over text-only baselines in some settings but is outperformed by KG_Evolve; when presented with the full pre-extracted KB the agent tended to explore noisily and learned less efficiently (see Figure 9).",
            "performance_without_tools": "Compared to the no-KB baseline (Text-only), KG_Full shows improvements but smaller and less consistent than KG_Evolve.",
            "has_tool_ablation": true,
            "key_findings": "Providing the complete KB at once can overwhelm the learner and induce noisy exploration; a focused, incremental KB (KG_Evolve) is empirically preferable for RL in partially observable text environments.",
            "uuid": "e782.2",
            "source_info": {
                "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Text+ManualGraph",
            "name_full": "Text-based Agent Augmented with Manually-Curated ConceptNet Subgraph",
            "brief_description": "A Text+Commonsense variant where relevant commonsense paths between objects and their goal locations are manually extracted (shortest paths within a 2-hop neighborhood and pruned) and provided to the agent; yields stronger improvements but requires manual effort and can be imperfect.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Text+ManualGraph (manual ConceptNet subgraph augmentation)",
            "agent_description": "Same underlying model (GRU context, action encoders, GAT graph encoder, co-attention, A2C) but instead of automatic retrieval the commonsense subgraph is manually curated: the authors extract commonsensical paths between entities and goal locations from ConceptNet (shortest paths within a 2-hop neighborhood) and prune noisy nodes/edges before feeding the subgraph to the model.",
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "Partially observable text-based cleanup games as above.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet (manual inspection and path extraction), Numberbatch embeddings.",
            "tool_output_types": "Structured curated graph fragments (nodes/edges) and dense node embeddings.",
            "belief_state_mechanism": "Context GRU s_t plus a manually constructed graph provided as external knowledge; node embeddings processed via GAT and fused with context as in the main model.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "The manual subgraph is provided (and pruned) before/while the agent trains; belief updates follow the standard procedure: context update via GRU and re-embedding of graph nodes via GATs; no automatic expansion of the manual graph is described.",
            "planning_approach": "Learned policy (A2C) with curated KB input; manual graphs provide stronger priors that accelerate learning.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Policy-driven navigation; no explicit path search.",
            "performance_with_tools": "Agents augmented with manual graphs perform better than other automated retrieval mechanisms, showing average reductions of ~2â€“5 steps in easy and medium games (paper reports these average reductions).",
            "performance_without_tools": "Compared to the same architecture using automated retrieval (DC/CDC/NG) or text-only variants, manual-graph variants show improved step-efficiency and sometimes higher scores, but manual curation is not a gold standard and can be imperfect.",
            "has_tool_ablation": true,
            "key_findings": "Curated, high-precision subgraphs can further reduce exploration and improve performance, indicating the importance of retrieval precision; however, manual curation is labor-intensive and may still omit useful connections, so automated, focused retrieval methods remain preferable for scalability.",
            "uuid": "e782.3",
            "source_info": {
                "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-A2C (baseline)",
            "name_full": "KG-A2C (Graph-based A2C Baseline from prior work)",
            "brief_description": "A prior state-of-the-art baseline (Ammanabrolu & Hausknecht 2020) that uses a knowledge-graph representation of the game environment (built from observations) to constrain/exploit the action space and guide RL (A2C) exploration.",
            "citation_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
            "mention_or_use": "use",
            "agent_name": "KG-A2C (baseline from Ammanabrolu & Hausknecht 2020)",
            "agent_description": "Described in prior work and used as a baseline here: constructs a knowledge graph of the environment from textual observations and uses graph-structured information to guide an A2C policy over text actions. The paper references and evaluates this model as a competitor but does not re-implement its internal details beyond referencing the baseline.",
            "environment_name": "TextWorld / TWC (used as a baseline comparison)",
            "environment_description": "Text-based, partially observable environments; KG-A2C constructs a game knowledge graph from observations to inform action selection.",
            "is_partially_observable": true,
            "external_tools_used": "Internal knowledge graph constructed from game observations (not necessarily external KB like ConceptNet in its original form); in prior work the agent uses a constructed KG of the game's entities/relations.",
            "tool_output_types": "Structured in-environment knowledge graph (nodes/edges) built from observations; vector embeddings derived from that graph in the original method.",
            "belief_state_mechanism": "Maintains an explicit knowledge graph built from the agent's observations to represent state (as described in the cited baseline paper); this functions as the agent's belief about the world.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "The agent incrementally updates its internal game-knowledge graph as new observations arrive; exact update mechanics follow the baseline (Ammanabrolu & Hausknecht 2020) and are not re-specified in detail in this paper.",
            "planning_approach": "Graph-augmented A2C policy that uses the constructed knowledge graph to bias action selection; planning is implicit in the learned policy rather than via explicit symbolic search.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Not detailed in this paper; the baseline guides exploration via the environment-constructed knowledge graph rather than explicit pathfinding.",
            "performance_with_tools": "Reported as a competitive baseline in the experiments; KG-A2C performed relatively well compared to text-only baselines but the paper's commonsense-augmented agents (Text+Commonsense variants) also compare favorably. Exact numbers are included in Table 4 of the paper.",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Used as a competitive baseline to show that structured knowledge of the environment (built from observations) helps exploration; the TWC paper shows that externally-sourced commonsense (ConceptNet) integrated dynamically also yields improvements and that focused retrieval (CDC/evolve) is important.",
            "uuid": "e782.4",
            "source_info": {
                "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_knowledge_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Integrated common sense learning and planning in POMDPs",
            "rating": 2,
            "sanitized_title": "integrated_common_sense_learning_and_planning_in_pomdps"
        },
        {
            "paper_title": "Dungeons and DQNs: Toward Reinforcement Learning Agents that Play Tabletop Roleplaying Games",
            "rating": 1,
            "sanitized_title": "dungeons_and_dqns_toward_reinforcement_learning_agents_that_play_tabletop_roleplaying_games"
        },
        {
            "paper_title": "Towards deep symbolic reinforcement learning",
            "rating": 1,
            "sanitized_title": "towards_deep_symbolic_reinforcement_learning"
        }
    ],
    "cost": 0.019106249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</p>
<p>Keerthiram Murugesan keerthiram.murugesan@ibm.com 
IBM Research
Yorktown Heights</p>
<p>Mattia Atzeni 
IBM Research
Zurich</p>
<p>Pavan Kapanipathi kapanipa@us.ibm.com 
IBM Research
Yorktown Heights</p>
<p>Pushkar Shukla pushkarshukla@ttic.edu 
TTI
Chicago 4 ETH Zurich</p>
<p>Sadhana Kumaravel sadhana.kumaravel1@ibm.com 
IBM Research
Yorktown Heights</p>
<p>Gerald Tesauro gtesauro@us.ibm.com 
IBM Research
Yorktown Heights</p>
<p>Kartik Talamadupula 
IBM Research
Yorktown Heights</p>
<p>Mrinmaya Sachan mrinmaya.sachan@inf.ethz.ch 
Murray Campbell 
IBM Research
Yorktown Heights</p>
<p>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</p>
<p>Text-based games have emerged as an important test-bed for Reinforcement Learning (RL) research, requiring RL agents to combine grounded language understanding with sequential decision making. In this paper, we examine the problem of infusing RL agents with commonsense knowledge. Such knowledge would allow agents to efficiently act in the world by pruning out implausible actions, and to perform lookahead planning to determine how current actions might affect future world states. We design a new text-based gaming environment called TextWorld Commonsense (TWC) for training and evaluating RL agents with a specific kind of commonsense knowledge about objects, their attributes, and affordances. We also introduce several baseline RL agents which track the sequential context and dynamically retrieve the relevant commonsense knowledge from ConceptNet. We show that agents which incorporate commonsense knowledge in TWC perform better, while acting more efficiently. We conduct user-studies to estimate human performance on TWC and show that there is ample room for future improvement.</p>
<p>Introduction</p>
<p>Over the years, simulation environments have been used extensively to drive advances in reinforcement learning (RL). A recent framework that has received much attention is TextWorld (TW) (CÃ´tÃ© et al. 2018), where an agent must interact with an external environment to achieve a given goal using only the modality of text. TextWorld and similar textbased environments seek to bring advances in grounded language understanding to a sequential decision making setup.</p>
<p>While existing text-based games are valuable for RL research, they fail to test a key aspect of human intelligence: common sense. Humans capitalize on commonsense (background) knowledge about entities -properties, spatial relations, events, causes and effects, and other social conventions -while interacting with the world (Mccarthy to clean up a house. Achieving goals in this environment requires commonsense knowledge about objects, their properties, locations, and affordances. Efficient use of commonsense knowledge would allow the agent to select correct and applicable actions at each step: i.e., improve sample efficiency by reducing exploration. Moreover, commonsense knowledge would help the agent to perform look-ahead planning and determine how current actions might affect future world states (Juba 2016). Fig 1 presents a running example from TWC that illustrates how the agent can leverage a commonsense knowledge base (KB).</p>
<p>Validating such environments is challenging, and requires: (1) verifying the information used in the games; (2) evaluating baseline agents that are capable of utilizing external commonsense knowledge against counterparts that do not; and (3) providing empirical evidence to show that the environment can drive future research. In this work, we address each of these by first performing human annotations to validate the correctness and completeness of the TWC environment. Next, we design a framework of agents that combine text-based agents with commonsense knowledge. The agents can dynamically retrieve relevant knowledge from a commonsense KB. Finally, based on human performance on the generated games and manual selection of commonsense knowledge, we discuss and justify the importance of such an environment in driving future research. Contributions: The main contributions of this paper are the following: (1) we propose an novel environment called TWC to evaluate the use of commonsense knowledge by RL agents; (2) we introduce baselines that use commonsense knowledge from ConceptNet and show that common sense indeed helps in decision making; (3) whereas our model with common sense performs well, we show a pronounced gap in performance between automated agents and humans in the TWC environment. This substantiates our claim that TWC provides a challenging test-bed for RL agents and can act as a spur to further research in this area.</p>
<p>TextWorld Commonsense (TWC)</p>
<p>Existing text-based games (Adhikari et al. 2020;CÃ´tÃ© et al. 2018) severely restrict the amount and variety of commonsense knowledge that an agent needs to know and exploit. Thus, in this paper, we create and present a new domain -TextWorld Commonsense (TWC) -by reusing the TextWorld (CÃ´tÃ© et al. 2018) engine in order to generate textbased environments where RL agents need to effectively retrieve and use commonsense knowledge. Commonsense can be defined very broadly and in various ways (Fulda et al. 2017). In this paper, we mainly focus on commonsense knowledge that pertains to objects, their attributes, and affordances 1 .</p>
<p>Constructing TWC</p>
<p>We built the TWC domain as a house clean-up environment where the agent is required to obtain knowledge about typical objects in the house, their properties, and expected location from a commonsense knowledge base. The environment is initialized with random placement of objects in various locations. The agent's high level goal is to tidy up the house by putting objects in their commonsense locations. This high level goal may consist of multiple sub-goals requiring commonsense knowledge. For example, for the sub-goal: put the apple inside the refrigerator, commonsense knowledge from ConceptNet such as (Apple â†’ AtLocation â†’ Refrigerator) can assist the agent. Goal Sources: While our main objective was to create environments that require commonsense, we did not want to bias TWC towards any of the existing knowledge bases. We additionally wanted to rule out the possibility of data leaks in situations where both the environment as well as the external knowledge came from the same part of a specific commonsense knowledge base (KB) like ConceptNet. For the construction of the TWC goal instances, we picked sources of information that were orthogonal to existing commonsense KBs. Specifically, we used: (1) the picture dictionary from 7ESL 2 ; (2) the British Council's vocabulary learning page 3 ; (3) the English At Home vocabulary learning page 4 ; and (4) ESOL courses 5 . We collected vocabulary terms from 1 Gibson in his seminal work (Gibson 1978)    these sources and manually aggregated this content in order to build a dataset that lists several kinds of objects that are typically found in a house environment. For each object, the dataset specifies a list of plausible and coherent locations. Instance Construction: A TWC instance is sampled from this dataset, which includes a configuration of 8 room types and a total of more than 900 entities (Table 1). The environment includes three main kinds of entities: objects, supporters, and containers. Objects are entities that can be carried by the agent, whereas supporters and containers are furniture where those objects can be placed. Let o represent the object or entity in the house; r represent the room that the entity is typically found in; and l represent the location inside that room where the entity is typically placed. In our running example, o:apple is an entity, l:refrigerator is the container, and r:kitchen is the room. Via a manual verification process (which we elucidate next in Section 2.2) we ensure that the associations between entities, supporters/containers, and rooms reflect commonsense. As shown in Table 1, we collected a total of 190 objects from the aforementioned resources. We further expanded this list by manually annotating the objects with qualifying properties, which are usually adjectives from a predefined set (e.g., a shirt may have a color and a specific texture). This allows increasing the cardinality of the total pool of objects for generating TWC environments to more than 800.</p>
<p>Verifying TWC</p>
<p>In order to ensure that TWC reflects commonsense knowledge, we set up two annotation tasks to verify the environment goals (i.e., goal triples of the form o, r, l , where o denotes the object, r denotes a room, and l a location within that room, as defined in Section 2.1). The first task is meant to verify the correctness of the goals and evaluate whether the goal o, r, l triples make sense to humans. The second task is aimed at verifying completeness, i.e. that other triples in the environment do not make sense to humans.</p>
<p>Verifying Correctness: To test the correctness of our environments, we asked our human annotators to determine whether they would consider a given room-location combination in the goal o, r, l to be a reasonable place for the object o. If so, the instance was labeled as positive, and as negative otherwise. We collected annotations from 10 annotators, across a total of 205 unique o, r, l triples. Each annotator labeled 70 of these triples, and each triple was as--= Corridor =-You're now in the corridor.</p>
<p>You see a shoe cabinet. What a letdown! The shoe cabinet is empty! You see an umbrella stand. The umbrella stand is standard. Unfortunately, there isn't a thing on it. You see a coat hanger. The coat hanger is usual. Looks like someone's already been here and taken everything off it, though. You see a hat rack. But the thing is empty. Oh! Why couldn't there just be stuff on it? Oh, great. Here's a key holder. But there isn't a thing on it.</p>
<p>There is a pair of climbing shoes, a brown cap and a white cap on the floor.</p>
<p>You are carrying nothing.</p>
<blockquote>
<p>take the climbing shoes</p>
</blockquote>
<p>You pick up the climbing shoes from the ground.</p>
<blockquote>
<p>insert climbing shoes into shoe cabinet</p>
</blockquote>
<p>You put the climbing shoes into the shoe cabinet.</p>
<p>Your score has just gone up by one point.</p>
<blockquote>
<p>take the brown cap</p>
</blockquote>
<p>You pick up the brown cap from the ground.</p>
<blockquote>
<p>put the brown cap on the hat rack</p>
</blockquote>
<p>You put the brown cap on the hat rack.</p>
<p>Your score has just gone up by one point.</p>
<blockquote>
<p>take the white cap</p>
</blockquote>
<p>You pick up the white cap from the ground.</p>
<blockquote>
<p>put the white cap on the hat rack</p>
</blockquote>
<p>You put the white cap on the hat rack.</p>
<p>Your score has just gone up by one point. signed to at least 3 distinct annotators. The annotators were not given any other biasing information, and all annotators worked independently. We show the overall agreement of the annotators with TWC's goals in Table 2. The high agreement from the annotators demonstrates that the goal o, r, l triples reflect human commonsense knowledge.</p>
<h1>objects #Objects to find #Rooms Easy 1 1 1 Medium 2, 3 1, 2, 3 1 Hard 6, 7 5, 6, 7 1, 2 Verifying Completeness: Similar to the above annotation exercise, we also asked human annotators to determine if a non-goal o, r, l triple made sense to them. In addition to the 70 triples mentioned above, each of the M = 10 annotators were asked to label as either positive or negative a set of 30 non-goal triples. In order to provide annotators with an informative set of non-goal o, r, l triples, we used GloVe (Pennington, Socher, and Manning 2014) to compute embeddings for each location in TWC. For a given object o, a non-goal location l' was then selected among those most similar to the goal location l, according to the cosine similarity between the embeddings of l and l'. As before, each non-goal triple was assigned to at least 3 annotators from a set that comprises a total of 97 triples. As we see in Table  2, the annotators seldom find a hypothesized non-goal o, r, l triple as commonsensical.</h1>
<p>Annotator Reliability: For our overall annotation exercise, we can report inter-annotator agreement statistics, as the overall annotation is no longer imbalanced in terms of label marginals. We report a Krippendorff's alpha (Krippendorff 2018) Î± Îº = 0.74. This number is over the accepted range for agreement and shows that our annotators have strong agreement when rating the triples.</p>
<p>Generating TWC Games</p>
<p>We used the TextWorld engine to build a set of text-based games where the goal is to tidy up a house by putting objects in the goal locations specified in the aforementioned TWC dataset. The games are grouped into three difficulty levels (easy, medium, and hard) depending on the total number of objects in the game, the number of objects that the agent needs to find (the remaining ones are already carried by the agent at the beginning of the game) and the number of rooms to explore. The values of these properties are randomly sampled from the ones listed in Table 3. For each difficulty level, we provide a training set and two test sets. The training sets were built out of 2 3 of the unique objects reported in Table 1. For the first test set, we used the same set of objects as the training games. We call this set the in distribution test set. For the second test set, we employed the remaining 1 3 objects to create the evaluation games. We call this set the out of distribution test set. This allows us to investigate not only the capability of the agents to generalize within the same distribution of the training data, but also their ability to achieve generalization to unseen entities. Figure 2 shows a game walkthrough for a specific game in the medium difficulty level.</p>
<p>Benchmarking Human Performance</p>
<p>To complete our benchmarking of the TWC domain, we conducted yet another human annotation task, focusing on the performance of human game-players. Such an experiment is essential to establishing the performance of human players, who are generally regarded as proficient at exploiting commonsense knowledge. We set up an interactive interface to TWC via a Jupyter notebook, which was then used by players to interact with the same games that we evaluated all the other RL agents on. We recorded all moves (steps) made by players, as well as the reward collected. At each step, the players were shown the current context of the game in text format, and given a drop-down box with the full list of pos-sible actions. Once the player picked an action, it was executed; and this process repeated until all possible goals in the game had been accomplished. A total of 16 annotators played 104 instances of TWC games, spread across the easy, medium, and hard levels. Each difficulty level had 5 games, each from the train and test distributions, for a total of 30 unique games. Each unique game was annotated by a minimum of 3 annotators. The results are presented in Table 4, along with the experimental results in Section 4, to allow for direct comparison with the TWC agents.</p>
<p>TWC Agents</p>
<p>Text-based games can be seen as partially observable Markov decision processes (POMDP) (Kaelbling, Littman, and Cassandra 1998) where the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. The agent receives a reward at every time step and its goal is to maximize the expected discounted sum of rewards. The TWC games allow the agent to perceive and interact with the environment via text. Thus, the observation at time step t, o t , is presented as a sequence of
tokens (o t = {o 1 t , . . . o N t }).
Similarly, each action a is also denoted as a sequence of tokens {a 1 , . . . , a M }. The goal of this project is to test RL agents with commonsense. Hence, the agents also have access to a commonsense knowledge base; and are allowed to use it while selecting actions. To model TWC, we design a framework that can: (a) learn representations of various actions; (b) learn from sequential context; (c) dynamically retrieve the relevant commonsense knowledge; (d) integrate the retrieved commonsense knowledge with the context; and (e) predict next action. A block diagram of the framework is shown in Fig 3. We describe the various components of our framework below.</p>
<p>Action and Observation Encoder</p>
<p>We learn representations of observations and actions by feeding them to a recurrent network. Given the current observation o t , we use pre-trained word embeddings to represent o t as a sequence of d-dimensional vectors x 1</p>
<p>Context Encoder</p>
<p>A key challenge for our RL agent is in modeling context, i.e. the history of observations. We model the context using another recurrent encoder over the observation representations o t . We use a GRU network to encode the sequence of previous observations up to o t into a vector s t = GRU(s tâˆ’1 , o t ).</p>
<p>We refer to s t as the state vector, or the context encoding. The context encoding will be used in addition to the commonsense knowledge in the final action prediction.  Figure 3: Overview of our framework's decision making at any given time step. The framework comprises of the following components (visually shown in color): (a) action encoder which encodes all admissible actions a âˆˆ A , (b) observation encoder which encodes the observation o t , (c) context encoder, which encodes the dynamic context C t , (d) a dynamic common sense subgraph of ConceptNet G t C extracted by the agent, (e) a knowledge integration component, which combines the information from textual observations and the extracted common sense subgraph, and (f) an action selection module. âŠ• denotes the concatenation operator.</p>
<p>Dynamic Commonsense Subgraph</p>
<p>Our model retrieves commonsense knowledge from Con-ceptNet in the form of a graph. The graph G t C is updated dynamically at each time step t. G t C is constructed by mapping the textual observation o t at time t to ConceptNet and combining it with the graph at previous time step G tâˆ’1 C . We used spaCy (https://spacy.io) to extract noun chunks, and then performed a max sub-string match with all the concepts in ConceptNet. This results in a set of entities e t for the observation o t at time t. We then combine the concepts from G tâˆ’1 C and e t to get E t . E t consists of all the concepts observed by the agent until time step t, including the description of the room, the current observation, and the objects in the inventory. Given E t , we describe three different techniques to automatically extract the commonsense graph G t from external knowledge.</p>
<p>(1) Direct Connections: This is the baseline approach to construct G t C . We fetch direct links between each of the concepts in E t from ConceptNet.</p>
<p>(2) Contextual Direct Connections: Since the goal of the agent is to clean up the house by putting objects into its appropriate containers such as apple â‡’ refrigerator, , we hypothesize that adding links only between objects and containers may benefit the agent instead of links between all concepts as done by Direct Connections, as we might overwhelm the agent with noise. To accomplish this goal, we split the entities E t into objects and containers. Since we know the entities from the inventory in E t constitutes objects, no explicit labelling is needed as we consider the remaining entities as containers. We retain only the edges between objects and containers from ConceptNet.</p>
<p>(3) Neighborhood: Previous techniques only focus on connecting links between observed concepts E t from external knowledge. In addition to the direct relations, it may be beneficial to include concepts from external knowledge that is related to E t but has not been directly observed from the game. Therefore, for each concept in E t , we include all its neighboring concepts and associated links.</p>
<p>Knowledge Integration</p>
<p>We enhance the text-based RL agent by allowing it to jointly contextualize information from both the commonsense subgraph and the observation representation. We call this step knowledge integration. We encode the commonsense graph using a graph encoder followed by a co-attention layer. Graph encoder: The graph G t C is encoded as follows: First, we use pretrained KG embeddings (Numberbatch) to map the set of nodes V t to a feature matrix [e 1 t , . . . , e
|V t | t ] âˆˆ R f Ã—|V t * | . Here, e i t âˆˆ R f is the (averaged) embedding of words in node i âˆˆ V t * .
Following (Lu et al. 2017), we also add a sentinel vector to allow the attention modules to not attend to any specific nodes in the subgraph. These node embeddings are updated at each time step by message passing between the nodes of G t c with Graph Attention Networks (GATs) (VeliÄkoviÄ‡ et al. 2018
) to get {z 1 t , z 2 t Â· Â· Â· z |V t |
t } using multi-head graph attention, resulting in a final graph representation that better captures the conceptual relations between the nodes in the subgraph. Co-Attention: In order to combine the observational context and the retrieved commonsense graph, we consider a bidirectional attention flow layer between these representations to re-contextualize the graph for the current state of the game (Seo et al. 2016;Yu et al. 2018). Similar to (Yu et al. 2018), we compute a similarity matrix S âˆˆ R NÃ—|V t C | between the context and entities in the extracted common sense subgraph using a trilinear function. In particular, the similarity between j th token's context encoding h j t and i th node encoding z i t in the commonsense subgraph is computed as:
S i j = W T 0 [z i t ; h j t ; z i t â€¢ h j t ]
where â€¢ denotes elementwise product, ; denotes concatenation and W 0 is a learnable parameter. We use the softmax function to normalize the rows (columns) of S and get the similarity function for the common-sense knowledge graphS G (context representation S O ). The commonsense-to-context attention is calculated as
A =S T G Â· O and the context-to-common sense attention is calculated as B =S GS T O Â· G, where G = [z 1 t , z 2 t , Â· Â· Â· z |V t C | t ] and O = [h 1 t , h 2 t Â· Â· Â· h N t ]
are the commonsense graph and observation encodings. The attention vectors are then combined together and the final graph encoding vectors G are calculated as W [G; A; G â€¢ A; G â€¢ B] where W is the learnable parameter. Finally, we get the commonsense graph encoding g t i for each action a i âˆˆ A t by applying a general attention over the nodes using the state vector and the action encoding [s t ; a t i ] (Luong, Pham, and Manning 2015). The attention score for each node is computed as Î± i = [s t ; a t i ]W g G, and the commonsense graph encoding for action a t i is given as g t i = Î± i G.</p>
<p>Action Selection</p>
<p>The action score for each actionÃ¢ t i is computed based on the context encoding s t , the commonsense graph encoding g t i and the action encoding a t i . We concatenate these encoding vectors into a single vector r t i = [s t ; g t i ; a t i ]. Then, we compute probability score for each action a i âˆˆ A t as p t = so f tmax(W 1 Â· ReLU(W 2 Â· r t + b 2 ) + b 1 ); where W 1 ,W 2 , b 1 , and b 2 are learnable parameters of the model. The final action chosen by the agent is then given by the one with the maximum probability score, namelyÃ¢ t = arg max i p t,i .</p>
<p>Experiments</p>
<p>In this section, we report the results of our experiments on the TWC games. Given that the quality (correctness and completeness) of TWC has already been evaluated (c.f. Section 2.2), these experiments primarily focus on showing that:</p>
<p>(1) agents that utilize commonsense knowledge can achieve better performance on TWC than their text-based counterparts; (2) TWC can aid research in the use of commonsense knowledge because of the gap between human performance and the commonsense knowledge agents. Experimental Setup: We measure the performance of the various agents using: (1) the normalized score (score achieved Ã· maximum achievable score); and (2) the number of steps taken. Each agent is trained for 100 episodes and the results are averaged over 10 runs. Following the winning strategy in the FirstTextWorld Competition (Adolphs and Hofmann 2019), we use the Advantage Actor-Critic framework ) to train the agents using reward signals from the training games.</p>
<p>RL Agents in TWC</p>
<p>We evaluate our framework on the TWC cleanup games (as described in Section 2.3). For comparison, we consider a random agent that randomly picks an action at each time step. We consider two types of experiment settings based on the type of information available to the RL agents: (1) Text-based RL agents have access to the textual description (observation) of the current state of the game provided by the TWC environment; and (2) Commonsense-based RL agents have access to both the observation and Concept-Net. Text-only Baseline Agents: As baselines, we picked various SOTA text-based agents that utilize observation only: (1) LM-NSP uses language models such as BERT (Devlin et al. 2019) and GPT2 (Radford et al. 2019) with the observation and the action pair as a Next Sentence Prediction (NSP) task; (2) LSTM-A2C (Narasimhan, Kulkarni, and Barzilay 2015) uses the observed text to select the next action; baselines, we use GloVe (Pennington, Socher, and Manning 2014) embeddings for text. The results on these baselines are reported in Table 4. For each difficulty level, we report: the agents' performance; the optimal number of steps to solve the game 6 ; and the human performance. The performance of GPT2-NSP and BERT-NSP shows that even powerful pretrained models if not tuned to this task have difficulty in these commonsense RL games, as they do not capture commonsense relationships between entities. Baselines such as LSTM-A2C, DRRN, and KG-A2C have a competitive advantage over the LM-NSP baselines, as they effectively adapt to the sequential interaction with the environment to improve performance. Among these baselines, DRRN and KG-A2C perform better than LSTM-A2C as they utilize the structure of the state and action spaces for efficient exploration of the environment. Commonsense-based agents: We introduce commonsense knowledge in two ways. The first is (Text + Numberbatch) by replacing GloVE embeddings in the LSTM-A2C agent with Numberbatch (Nb) embeddings (Speer, Chin, and Havasi 2017) which were trained on text and ConceptNet. This is the naive approach to augment text information with commonsense knowledge. The results in Table 4 show that introducing Nb embeddings allows achieving a noticeable gain (an average of 3 steps in easy and 7 steps in medium level games) over GloVe embeddings.</p>
<p>In order to explicitly use commonsense knowledge, we experiment with the three different mechanisms outlined in Section 3.2 for retrieving relevant information from Con-ceptNet: (DC, CDC and NG). These methods retrieve both the concepts and structure in the relevant sub-graphs from Con-ceptNet, which are leveraged by our co-attention mechanism (Section 3.4). The comparison of the agents' performance with different retrieving mechanisms is shown in Fig 5. The results show that CDC performs the best among other mechanisms, particularly compared to DC. Unlike DC that includes all the links between observed concepts from ConceptNet, CDC restricts links to those between observed objects and containers. This selection of relevant links from Concept-Net improves the performance of the agent.</p>
<p>Given that CDC performs best, we compare results on text-based models with CDC-augmented commonsense knowledge to other baselines. Table 4 shows results for textbased agents initialized with GloVe or Nb embeddings, and augmented with commonsense knowledge. We see that the commonsense-based RL agents perform better than textbased RL agents in the easy and medium level games. This is not surprising, as these instances mostly involve picking an object and placing it in a container in the same room. Both the text-based and commonsense RL agents struggle in the hard level, as these games have more than one room and multiple objects and containers. We also notice that the average number of steps taken by the commonsense-based RL agents are noticeably lower than the other agents as it efficiently uses commonsense knowledge to rule out implausible actions. This proves that TWC is a promising test-bed where commonsense knowledge helps.</p>
<p>Our results show that TWC still has much room for improvement in terms of retrieving and combining knowledge with observations and feedback from the environment in a sample-efficient manner. As a starting point for showing that there is headroom, we switched the retrieval mechanism to manually selected information from ConceptNet. We manually retrieved the relevant commonsense knowledge by extracting the commonsensical paths between entities in ConceptNet, corresponding to objects in the TWC games and their goal locations. The manual subgraph includes all the relevant shortest paths between an object and its location, within a 2-hop neighborhood expansion of both nodes. Since the extracted subgraph can be very large even for the easy games, further pruning was performed to remove noise. We emphasize that the manual annotation can be error-prone or result in manual subgraphs that lack potentially useful information. Thus, the manual graphs should not be taken as a gold standard. However, we are exploring other manual retrieval process to understand if better commonsense retrieval approaches can bring improvements in the future. In Table 4, agents that are augmented with the manual graph perform better than the other automated retrieval mechanisms (average reduction of 2 âˆ’ 5 steps on easy and medium). Fig 4 shows training curves for the Textonly, Text+Commonsense and Text+Manual agents on the three difficulty levels. We notice that infusing commonsense knowledge allows achieving faster convergence both in terms of the number of steps taken by the agents and the final score. We found that the extracted manual subgraphs is not perfect as can be seen in the training curves for medium and hard levels. Human Performance on TWC: We also present the results of human performance in TWC (outlined in Section 2.4). The O and H columns in Table 4 (two per condition) present these results. A quick comparison of these numbers reveals two major results: (1) human performance H is very close to the optimal number of steps O in all 3 conditions; and (2) there is significant headroom between H and all of the other agents in the table, include the ones with the manual graph. This confirms that there is still much progress to be made in retrieving and encoding the commonsense knowledge effectively to solve such problems; and that TWC can spur further research.  . O represents optimal # steps needed to accomplish the goals. H represents human level performance. All agents were restricted to a maximum of 50 steps.</p>
<p>Generalization</p>
<p>(2016) proposed Deep Symbolic RL, which combines aspects of symbolic AI with neural networks and RL as a way to introduce commonsense priors. There has also been work on policy transfer (Bianchi et al. 2015), which studies how knowledge acquired in one environment can be re-used in another one; and experience replay (Wang et al. 2016;Lin 1992Lin , 1993 which studies how an agent's previous experiences can be stored and then later reused. In this paper, we use commonsense knowledge as a way to improve sample efficiency in text-based RL agents. To the best of our knowledge, there is no prior work that practically explores how commonsense can be used to make RL agents more efficient. The most relevant prior work is by Martin, Sood, and Riedl (2018), who use commonsense rules to build agents that can play tabletop role-playing games. However, unlike our work, the commonsense rules in this work are manually engineered.</p>
<p>Leveraging Commonsense: Recently, there has been a lot of work in NLP to utilize commonsense for QA, NLI, etc. (Sap et al. 2019;Talmor et al. 2018). Many of these approaches seek to effectively utilize ConceptNet by reducing the noise retrieved from it (Lin et al. 2019;Kapanipathi et al. 2020). This is also a key challenge in TWC.</p>
<p>Conclusion</p>
<p>We created a novel environment (TWC) to evaluate the performance on RL agents on text-based games requiring commonsense knowledge. We introduced a framework of agents which tracks the state of the world; uses the sequential context to dynamically retrieve relevant commonsense knowledge from a knowledge graph; and learns to combine the two different modalities. Our agents equipped with common sense achieve their goals with greater efficiency and less exploration when compared to a text-only model, thus showing the value of our new environments and models. There-fore, we believe that our TWC environment provides interesting challenges and can be effectively used to fuel further research in this area.</p>
<p>Reproducibility</p>
<p>To ensure the wide and unrestricted usage of the TWC environment, we release the TWC environment (with anonymized human annotations), code to generate text-based games and the sample agents used in this paper here: https:</p>
<p>//github.com/IBM/commonsense-rl.</p>
<p>A Overlap between TWC and ConceptNet</p>
<p>There is definitely some overlap between the resources used to build TWC and ConceptNet. However, as we discuss below, the overlap is limited and it is non-trivial for the agents to explore the knowledge graph and retrieve relevant commonsense knowledge. Indeed, only 12.2% of the goal entitylocation pairs defined in the TWC dataset can be directly matched to a single triplet in ConceptNet. Hence, we can state that it is fair and challenging to use ConceptNet as an external source of information. At the same time, we claim that external commonsense knowledge sources can be actually useful in solving text-based games. We submit that 85.9% of the unique entities in TWC match exactly one node in ConceptNet. Moreover, 66.1% of the time, the goal location of a given entity is in its 3-hop neighborhood in ConceptNet (42.7% for a 2-hop neighborhood). This shows that an external source of commonsense like ConceptNet can help to reduce exploration while solving the games, but needs to be explored effectively. As an example, with reference to Figure 8, the relation between the entity cap and the goal location hat_rack can be derived from ConceptNet by following the path: cap â†’ relatedTo â†’ head â†’ relatedTo â†’ hat â†’ atLocation â†’ hat_rack.</p>
<p>-= Laundry Room =-You find yourself in a laundry room. An usual one. Okay, just remember what you're here to do, and everything will go great.</p>
<p>You make out a washing machine. But the thing is empty. What a horrible day! You make out a clothes drier. The clothes drier is empty! This is the worst thing that could possibly happen, ever! You scan the room, seeing a suspended shelf. Unfortunately, there isn't a thing on it. You see a work table. On the work table you can see a pair of dirty gray underpants. You make out a bench. Looks like someone's already been here and taken everything off it, though. Aw, here you were, all excited for there to be things on it! You are carrying nothing.</p>
<blockquote>
<p>take the dirty gray underpants from the work table</p>
</blockquote>
<p>You take the dirty gray underpants from the work table.</p>
<blockquote>
<p>insert the dirty gray underpants into the washing machine</p>
</blockquote>
<p>You put the dirty gray underpants into the washing machine.</p>
<p>Your score has just gone up by one point. </p>
<p>B Sample TWC games</p>
<p>In this section, we show and analyze an example of a TWC game instance from each difficulty level. Figures 6, 2, and 7 provide such examples together with the optimal solution to each of the analyzed games. In all figures, we highlight all objects (in red), their candidate locations (in green) and the actions taken by the agent (in blue). Note that this information is not available to the agent and is only used for illustrative purposes. Figure 6 shows a walk-through of an easy game. This game has only 1 room and 1 object. We recall that this holds for all the games in the easy difficulty level. The game takes place in the Laundry Room, and the goal of the agent is to identify the correct location for the only object, in this case the dirty gray underpants. As all the easy games, the agent can reach the goal with a sequence of steps consisting of only two actions. The first action is used by the agent to take the object, and then the second action is aimed at putting the object in its goal location. In general, the goal location is not unique, but in the example shown in Figure  6, there is only one correct location, namely the washing machine. Commonsense knowledge is required in the second step in order to detect the correct location among all the possible candidates.</p>
<p>The relatively large number of possible locations makes the easy games more challenging than they might look like. In our example, the locations that are not considered commonsensical for the dirty gray underpants are the following: clothes drier, shelf, work table and bench. Note that the clothes drier could have been a commonsensical location for the entity gray underpants, but the attribute dirty plays a key role. This shows that incorporating only knowledge in the form of single facts extracted from the knowledge graph is not sufficient to solve the games. On the contrary, the agent needs to aggregate commonsense knowledge from multiple triples in the knowledge graph, as previously discussed in Section A. Figure 2 shows an example of a more complex game belonging to the medium difficulty level. This game is the same that the graphs in Figure 8 refer to. All medium-level games have only 1 room and either 2 or 3 objects. The game shown in Figure 2 has 3 objects, namely a pair of climbing shoes, a brown cap and a white cap. The goal locations for these objects are shown in Figure 8a and need to be selected from a pool of 5 candidate locations. However, please notice that candidate locations and objects are not provided explicitly to the agent and need to be extracted from the natural language observations. A total of 6 steps is required to solve the game in the optimal case. These actions are reported in Figure 2. We can see that, similarly to what we have seen for the easy games, 2 steps for each object are needed. Every time that an object is placed in its goal location, the agent receives a reward, but no reward is given for the action of taking an object. Hence, the maximum final score that the agent can achieve is always equal to the number of objects, in this case 3.</p>
<p>Finally, Figure 7 shows an example of the most complex games in TWC, namely the hard games. The game includes two rooms (Kitchen and Backyard) and the agent needs to place a total of 7 objects in the corresponding goal location. At the beginning of the game, the agent is already carrying an object (some milk), so it only needs to find 6 of the remaining objects. Since the game has more than 1 room, reaching the final goal may require more than 2 steps for each object. This happens because some objects may need to be carried across rooms and the in this case the agent has to visit back the initial room. In this examples, the wet azure skirt in the Kitchen has to be carried back to the the Backyard and placed in the clothesline. The provided optimal solution to the analysed game consists of the 15 actions reported in Figure 7.</p>
<p>C Visualizing Attention Over the Commonsense Subgraph</p>
<p>Our approach integrates commonsense knowledge dynamically with the context information through an attention mechanism. We can visualize the attention weights (from the general attention in the last layer) by the Text + Com- There is an open sliding patio door leading west.</p>
<p>You are carrying: some milk &gt; take the wet white jumper from the patio chair</p>
<p>You take the wet white jumper from the patio chair.</p>
<blockquote>
<p>put the wet white jumper on the clothesline</p>
</blockquote>
<p>You put the wet white jumper on the clothesline.</p>
<p>Your score has just gone up by one point.</p>
<blockquote>
<p>take the spoon from the BBQ You take the spoon from the BBQ.</p>
<p>take the clean pot from the workbench</p>
</blockquote>
<p>You take the clean pot from the workbench.</p>
<blockquote>
<p>go west -= Kitchen =-You find yourself in a kitchen. A normal kind of place. The room is well lit.</p>
</blockquote>
<p>You can see a fridge. The fridge is empty! This is the worst thing that could possibly happen, ever! As if things weren't amazing enough already, you can even see a kitchen cupboard. The kitchen cupboard is empty, what a horrible day! You hear a noise behind you and spin around, but you can't see anything other than a cutlery drawer. What a letdown! The cutlery drawer is empty! If you haven't noticed it already, there seems to be something there by the wall, it's a dishwasher. The light flickers for a second, but nothing else happens. The dishwasher is empty! You see a dining There is an open sliding patio door leading east.</p>
<blockquote>
<p>insert the milk into the fridge</p>
</blockquote>
<p>You put the milk into the fridge.</p>
<p>Your score has just gone up by one point.</p>
<blockquote>
<p>insert the spoon into the cutlery drawer</p>
</blockquote>
<p>You put the spoon into the cutlery drawer.</p>
<p>Your score has just gone up by one point.</p>
<blockquote>
<p>insert the clean pot into the cupboard</p>
</blockquote>
<p>You put the clean pot into the cupboard.</p>
<p>Your score has just gone up by one point.</p>
<blockquote>
<p>take the dirty pot from the dining table</p>
</blockquote>
<p>You take the dirty pot from the dining table.</p>
<blockquote>
<p>insert the dirty pot into the dishwasher</p>
</blockquote>
<p>You put the dirty pot into the dishwasher.</p>
<p>Your score has just gone up by one point.</p>
<blockquote>
<p>take the wet azure skirt from the chair</p>
</blockquote>
<p>You take the wet azure skirt from the chair.</p>
<blockquote>
<p>take the can opener from the counter</p>
</blockquote>
<p>You take the can opener from the counter.</p>
<blockquote>
<p>insert the can opener into the cupboard</p>
</blockquote>
<p>You put the can opener into the cupboard.</p>
<p>Your score has just gone up by one point. There is an open sliding patio door leading west.</p>
<blockquote>
<p>put wet azure skirt on clothesline</p>
</blockquote>
<p>You put the wet azure skirt on the clothesline.</p>
<p>Your score has just gone up by one point. monsense agent on the commonsense graph to show how the agent is using the commonsense knowledge while interacting with TWC games. We show the visuals for a specific game where the goal of the agent is to put the climbing shoes in the shoe cabinet, and the brown and white caps in the hat rack (see Figure 8a). The full game corresponding to this visualization was outlined in Figure 2. The attention maps for this instance are shown in Figure 8b. We observe that the model pays attention to relevant concepts such as "shoe", "hat", "hat rack", "hat stand", etc. This figure thus offers a qualitative illustration of how commonsense knowledge is used by the Text + Commonsense agent.</p>
<p>D Kitchen Cleanup Task: Full vs Evolve Commonsense Subgraph</p>
<p>Unlike in the previous works (Adhikari et al. 2020; Ammanabrolu and Hausknecht 2020), we do not assume that the entities used in the games are known beforehand. This poses an interesting question: does the commonsense-based agents would benefit from (1) having access to the full commonsense subgraph extracted at the beginning of the game using the aforementioned entities list or building the extracted commonsense subgraph sequentially (evolve) based on the entities seen so far in the game. In this section, we demonstrate a very simple experiment to show why we choose evolve setting for extracting the relevant commonsense subgraph. We used the TextWorld (CÃ´tÃ© et al. 2018) environment to generate a specific game instance similar to those in TWC but used information directly from ConceptNet, and has been manually generated by an expert. We call it Kitchen Cleanup. We generate the game with 10 objects relevant to the game, and 5 distractor objects spread across the room. As before, the goal of the agent is to tidy the room (kitchen) by putting the objects in the right place. We create a set of realistic kitchen cleanup goals for the agent: for instance, take apple from the table and put apple inside the refrigerator. Since information on concepts that map to the objects in the room is explicitly provided in ConceptNet (Apple â†’ AtLocation â†’ Refrigerator), the main hypothesis underlying the creation of this game is that leveraging the commonsense knowledge will let the agent achieve a higher reward while reducing the number of interactions with the environment.</p>
<p>The agent is presented with the textual description of a kitchen, consisting of the location of different objects in the kitchen and their spatial relationship to the other objects. The agent uses this information to select the next action to perform in the environment. Whenever the agent takes an object and puts it in the target location, it receives a reward and its total score goes up by one point. The maximum score that can be achieved by the agent in this kitchen cleanup task is equal to 10. In addition to the textual description, we use the commonsense knowledge subgraph extracted from ConceptNet relevant to the text description (Full vs Evolve setting).</p>
<p>D.1 Results on Kitchen Cleanup</p>
<p>As explained earlier, we consider two different knowledgeaware agents based on how/when we extract the commonsense subgraph. We consider that the first commonsense agent has access to the entities used in Kitchen cleanup game. Given the entities list, the agent generates a full commonsense subgraph before the start of the game. We call this model KG_Full. Our original model (Text+Commonsense in the main paper) is called KG_Evolve to point that the commonsense subgraph is generated at each time step and evolves over time based on the exploration.</p>
<p>As before, we compare our knowledge-aware RL agents (KG_Full and KG_Evolve) against two baselines for performance comparison: Random, where the agent chooses an action randomly at each step; and Simple (Text-only), where the agent chooses the next action using the text description only and ignores the commonsense knowledge graph. The knowledge-aware RL agents, on the other hand, use the commonsense knowledge graph to choose the next action. The graph is provided in either full-graph setting where all the commonsense relationships between the objects are given at the beginning of the game (KG_Full); or evolvegraph setting where only the commonsense relationship between the objects seen/interacted by the agent until the current steps are revealed (KG_Evolve). We record the average score achieved by each agent and the average number of interactions (moves) with the environment as our evaluation metrics. Figure 9 shows the results for the kitchen cleanup task averaged over 5 runs, with 500 episodes per run.</p>
<p>D.2 Discussion</p>
<p>As expected, we see that agents that use the textual description and additionally the commonsense knowledge outperform the baseline random agent. We are also able to demonstrate clearly that the knowledge-aware agent outperforms the simple agent with the help of commonsense knowledge. The knowledge-aware agent with the evolve-graph setting outperforms both the simple agent as well as the agent with the full-graph setting. We believe that when an agent has access to the full commonsense knowledge graph at the beginning of the game, the agent gets overwhelmed by the amount of knowledge given; and is prone to making noisy explorations in the environment. On the other hand, feeding the commonsense knowledge gradually during the agent's learning process provides more focus to the exploration, and drives it toward the concepts related to the rest of the goals. These results can also be seen as an RL-centric agent-based validation of similar results shown in the broader NLP literature . We refer the reader to (Murugesan et al. 2020) on further discussion on this topic.</p>
<p>E Hyperparameters and Complexity</p>
<p>In addition to the hyperparameters reported in the paper, we used the following settings:</p>
<p>Hyperparameter Setting Batchsize 1 Hidden dimension 300 Max. # Steps 50 Discount Factor (Î³) 0.9  </p>
<p>Figure 1 :
1Illustration of a TWC game. The agent is given an initial observation (top left) and has to produce the list of actions (bottom right) that are necessary to achieve the goal (bottom center) using relevant commonsense knowledge from ConceptNet (bottom left).</p>
<p>Figure 2 :
2Sample game walkthrough for a game with medium difficulty level. Best viewed in colors. Highlights are not available to the agents and are shown for illustrative purpose only.</p>
<p>( 3 )Figure 4 :
34DRRN (He et al. 2016)  utilizes the relevance between the observation and action spaces for better convergence; and (4) KG-A2C (Ammanabrolu and Hausknecht 2020) uses knowledge of the game environment generated from the observation to guide the agent's exploration. For these Performance evaluation (showing mean and standard deviation averaged over 10 runs) for the three difficulty levels: Easy (left), Medium (middle), Hard (right) using normalized score and the number of steps taken.</p>
<p>Figure 5 :
5Performance for the medium level (train-set) games (showing mean and standard deviation averaged over 3 runs) with the different techniques for the commonsense sub-graph extraction.</p>
<p>Figure 6 :
6Example of a game walkthrough belonging to the easy difficulty level. Best viewed in colors. Highlights are not available to the agents and are shown for illustrative purpose only.</p>
<p>Figure 7 :Figure 8 :
78Example of a game walkthrough belonging to the hard difficulty level. Best viewed in colors. Highlights are not available to the agents and are shown for illustrative purpose only. A graph describing the goal state of a TWC game (a) and a visualization of the attention weights over the ConceptNet subgraph extracted at a specific timestep of the same game instance (b)</p>
<p>Figure 9 :
9Simple = "Text model", "KG_evolve = Text+Commonsense" in main paper. Comparison of agents for the Kitchen Cleanup task with and without commonsense knowledge (ConceptNet) with average scores and average moves (averaged over 10 runs).</p>
<p>refers to affordance as "properties of an object[...]  that determine what actions a human can perform on them".2 https://7esl.com/picture-dictionary 
3 https://learnenglish.britishcouncil.org/ 
vocabulary/beginner-to-pre-intermediate 
4 https://www.english-at-home.com/ 
vocabulary 
5 https://www.esolcourses.com/topics/ 
household-home.html </p>
<p>Count 
Examples 
Rooms 
8 
kitchen, backyard 
Supporters/Containers 
56 
dining table, wardrobe 
Unique Objects 
190 
plate, dress 
Total Objects 
872 
dirty plate, clean red dress 
Total Entities 
928 
dirty plate, dining table </p>
<p>Table 1 :
1Statistics on the number of entities, supporters/containers, and rooms in the TWC domain.Correctness Completeness 
Rated Commonsense 
669 
47 
Rated NOT Commonsense 
31 
253 </p>
<p>Table 2 :
2Statistics from the human annotations to verify TWC</p>
<p>Table 3 :
3Specification of TWC games</p>
<p>You've entered a kitchen. You see a dishwasher and a fridge. Here's a dining table.You see a dirty plate and a red apple on the table.Observation (O t ) </p>
<p>Take apple from table </p>
<p>Word embeddings </p>
<p>Action (a) </p>
<p>GRU 
Action Encoder </p>
<p>Fully connected layer </p>
<p>Softmax 
Action 
Selector </p>
<p>Word 
embeddings </p>
<p>Co-attention </p>
<ul>
<li></li>
</ul>
<p>Commonsense 
graph (G c t ) </p>
<p>Multi-headed 
graph attention </p>
<p>Graph 
embeddings </p>
<p>Commonsense 
encoding </p>
<p>(G c t-1 ) </p>
<p>Context 
Encoding </p>
<p>|A| </p>
<p>Knowledge 
Integration 
Context Encoder </p>
<p>S t-1 </p>
<p>GRU 
Observation Encoder </p>
<p>GRU </p>
<p>Table 4
4IN to OUT distribution. This is in contrast to the use of the knowledge graphs in other NLP tasks such as textual entailment where knowledge graphs have shown to be robust to changes in the underlying (training and testing) environmentChen et al. 2018). The task of designing knowledge-enabled agents that are robust to such changes is another open challenge for the community that can be evaluated by TWC. Results Summary: Our results establish that TWC is anreports the results both for test games that be-
long to the same distribution used at training time (IN), </p>
<p>Table 4 :
4Generalization results for within distribution (IN) and out-of-distribution (OUT) games</p>
<p>You see a BBQ. The BBQ is recent. On the BBQ you make out a wooden spoon. You see a clothesline. The clothesline is typical. But the thing is empty. Hm. Oh well, what's that over there? It looks like it's a patio chair. On the patio chair you can see a wet white jumper. You see a patio table. The patio table is stylish. The patio table appears to be empty. Hey, want to see a workbench? Look over there, a workbench. On the workbench you see a clean pot. Something scurries by right in the corner of your eye. Probably nothing.-= Backyard =-
You've entered a backyard. </p>
<p>table. The dining table is massive. On the dining table you make out a dirty pot. You can make out a counter. On the counter you see a can opener. You can make out a stove! The stove is conventional. But oh no! there's nothing on this piece of junk. Aw, here you were, all excited for there to be things on it! You can see a dining chair. Now why would someone leave that there? On the dining chair you can see a wet azure skirt.</p>
<p>You see a BBQ. The BBQ is recent. But there isn't a thing on it. Hm. Oh well You see a clothesline. The clothesline is typical. On the clothesline you can see a wet white jumper. What's that over there? It looks like it's a patio chair. Now why would someone leave that there? Unfortunately, there isn't a thing on it. You see a patio table. The patio table is stylish. The patio table appears to be empty. Hey, want to see a workbench? Look over there, a workbench. But oh no! there's nothing on this piece of junk.&gt; go east </p>
<p>-= Backyard =-
You've entered a backyard. </p>
<p>Table 5 :
5Hyperparameters used by the agentsThe agents were trained in parallel on two machines with the following specifications:Resource Setting 
CPU 
Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz 
Memory 
128GB 
GPUs 
2 x NVIDIA Tesla V100 16 GB 
Disk1 
100GB 
Disk2 
600GB 
OS 
Ubuntu 18.04-64 Minimal for VSI. </p>
<p>Table 6 :
6Resources used by the agents Each agent was trained on a single GPU for approximately 12 hours for the Text agent and 16 hours for the Text + Commonsense agent for each run.
t , . . . , x N t ,where each x k t âˆˆ R d is the word embedding of the k-th observed token o k t , k = 1, . . . , N. Then, a (bidirectional) GRU encoder(Cho et al. 2014) is used to process the sequence x 1 t , . . . , x N t to get the representation of the current observa-tion: o t = h N t , where h k t = GRU(h kâˆ’1 t ,x k t ), for k = 1, . . . , N. In a similar way, given the set A t of admissible actions at time step t, we learn representations of each action a âˆˆ A t .
The optimal number of steps were computed by considering the objects already in the agent's possession, the number of objects to "put" (goals), and the number of rooms in the instance.</p>
<p>A Adhikari, X Yuan, M.-A CÃ´tÃ©, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W L Hamilton, arXiv:2002.09127Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games. arXiv preprintAdhikari, A.; Yuan, X.; CÃ´tÃ©, M.-A.; Zelinka, M.; Ron- deau, M.-A.; Laroche, R.; Poupart, P.; Tang, J.; Trischler, A.; and Hamilton, W. L. 2020. Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games. arXiv preprint arXiv:2002.09127 .</p>
<p>LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games. L Adolphs, T Hofmann, ArXiv abs/1909.01646Adolphs, L.; and Hofmann, T. 2019. LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games. ArXiv abs/1909.01646.</p>
<p>Graph Constrained Reinforcement Learning for Natural Language Action Spaces. P Ammanabrolu, M Hausknecht, International Conference on Learning Representations. Ammanabrolu, P.; and Hausknecht, M. 2020. Graph Con- strained Reinforcement Learning for Natural Language Ac- tion Spaces. In International Conference on Learning Rep- resentations. URL https://openreview.net/forum? id=B1x6w0EtwH.</p>
<p>Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning. P Ammanabrolu, M Riedl, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong and Short Papers1Ammanabrolu, P.; and Riedl, M. 2019. Playing Text- Adventure Games with Graph-Based Deep Reinforcement Learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 3557-3565.</p>
<p>Transferring knowledge as heuristics in reinforcement learning: A case-based approach. R A Bianchi, L A CelibertoJr, P E Santos, J P Matsuura, R L De Mantaras, Artificial Intelligence. 226Bianchi, R. A.; Celiberto Jr, L. A.; Santos, P. E.; Matsuura, J. P.; and de Mantaras, R. L. 2015. Transferring knowl- edge as heuristics in reinforcement learning: A case-based approach. Artificial Intelligence 226: 102-121.</p>
<p>Learning to win by reading manuals in a monte-carlo framework. S Branavan, D Silver, R Barzilay, Journal of Artificial Intelligence Research. 43Branavan, S.; Silver, D.; and Barzilay, R. 2012. Learning to win by reading manuals in a monte-carlo framework. Jour- nal of Artificial Intelligence Research 43: 661-704.</p>
<p>Neural natural language inference models enhanced with external knowledge. Q Chen, X Zhu, Z.-H Ling, D Inkpen, S Wei, Proceedings of ACL. ACLChen, Q.; Zhu, X.; Ling, Z.-H.; Inkpen, D.; and Wei, S. 2018. Neural natural language inference models enhanced with external knowledge. In Proceedings of ACL 2018.</p>
<p>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. K Cho, B Van Merrienboer, Ã‡ GÃ¼lÃ§ehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, 10.3115/v1/d14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Moschitti, A.Pang, B.and Daelemans, W.the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarA meeting of SIGDAT, a Special Interest Group of the ACLCho, K.; van Merrienboer, B.; GÃ¼lÃ§ehre, Ã‡.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learn- ing Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Moschitti, A.; Pang, B.; and Daelemans, W., eds., Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Pro- cessing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, 1724-1734. ACL. doi:10.3115/v1/d14-1179. URL https: //doi.org/10.3115/v1/d14-1179.</p>
<p>TextWorld: A Learning Environment for Text-based Games. M.-A CÃ´tÃ©, A KÃ¡dÃ¡r, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, CoRR abs/1806.11532CÃ´tÃ©, M.-A.; KÃ¡dÃ¡r, A.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. TextWorld: A Learning Environment for Text-based Games. CoRR abs/1806.11532.</p>
<p>Commonsense reasoning and commonsense knowledge in artificial intelligence. E Davis, G Marcus, Communications of the ACM. 589Davis, E.; and Marcus, G. 2015. Commonsense reasoning and commonsense knowledge in artificial intelligence. Com- munications of the ACM 58(9): 92-103.</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Burstein, J.Doran, C.and Solorio, T.the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J.; Doran, C.; and Solorio, T., eds., Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, NAACL- HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol- ume 1 (Long and Short Papers), 4171-4186. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/N19-1423/.</p>
<p>What Can You Do with a Rock?. N Fulda, D Ricks, B Murdoch, D Wingate, Affordance Extraction Viaword Embeddings. IJCAI'17. AAAI Press9780999241103Fulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What Can You Do with a Rock? Affordance Extraction Viaword Embeddings. IJCAI'17. AAAI Press. ISBN 9780999241103.</p>
<p>M Garnelo, K Arulkumaran, M Shanahan, arXiv:1609.05518Towards deep symbolic reinforcement learning. arXiv preprintGarnelo, M.; Arulkumaran, K.; and Shanahan, M. 2016. To- wards deep symbolic reinforcement learning. arXiv preprint arXiv:1609.05518 .</p>
<p>The ecological approach to the visual perception of pictures. J J Gibson, Leonardo. 113Gibson, J. J. 1978. The ecological approach to the visual perception of pictures. Leonardo 11(3): 227-235.</p>
<p>Deep Reinforcement Learning with a Natural Language Action Space. J He, J Chen, X He, J Gao, L Li, L Deng, M Ostendorf, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Osten- dorf, M. 2016. Deep Reinforcement Learning with a Nat- ural Language Action Space. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), 1621-1630.</p>
<p>Integrated common sense learning and planning in POMDPs. B Juba, The Journal of Machine Learning Research. 171Juba, B. 2016. Integrated common sense learning and plan- ning in POMDPs. The Journal of Machine Learning Re- search 17(1): 3276-3312.</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial intelligence. 1011-2Kaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1998. Planning and acting in partially observable stochastic do- mains. Artificial intelligence 101(1-2): 99-134.</p>
<p>Infusing Knowledge into the Textual Entailment Task Using Graph Convolutional Networks. P Kapanipathi, V Thost, S S Patel, S Whitehead, I Abdelaziz, A Balakrishnan, M Chang, K Fadnis, C Gunasekara, B Makni, N Mattei, K Talamadupula, A Fokoue, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)Kapanipathi, P.; Thost, V.; Patel, S. S.; Whitehead, S.; Ab- delaziz, I.; Balakrishnan, A.; Chang, M.; Fadnis, K.; Gu- nasekara, C.; Makni, B.; Mattei, N.; Talamadupula, K.; and Fokoue, A. 2020. Infusing Knowledge into the Textual En- tailment Task Using Graph Convolutional Networks. Pro- ceedings of the AAAI Conference on Artificial Intelligence (AAAI) .</p>
<p>Content analysis: An introduction to its methodology. K Krippendorff, Sage publicationsKrippendorff, K. 2018. Content analysis: An introduction to its methodology. Sage publications.</p>
<p>B Y Lin, X Chen, J Chen, X Ren, arXiv:1909.02151Kagnet: Knowledge-aware graph networks for commonsense reasoning. arXiv preprintLin, B. Y.; Chen, X.; Chen, J.; and Ren, X. 2019. Kagnet: Knowledge-aware graph networks for commonsense reason- ing. arXiv preprint arXiv:1909.02151 .</p>
<p>Self-improving reactive agents based on reinforcement learning, planning and teaching. L.-J Lin, Machine learning. 83-4Lin, L.-J. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning 8(3-4): 293-321.</p>
<p>Reinforcement learning for robots using neural networks. L.-J Lin, Carnegie-Mellon Univ Pittsburgh PA School of Computer ScienceTechnical reportLin, L.-J. 1993. Reinforcement learning for robots using neural networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science.</p>
<p>ConceptNet-a practical commonsense reasoning tool-kit. H Liu, P Singh, BT technology journal. 224Liu, H.; and Singh, P. 2004. ConceptNet-a practical com- monsense reasoning tool-kit. BT technology journal 22(4): 211-226.</p>
<p>Knowing when to look: Adaptive attention via a visual sentinel for image captioning. J Lu, C Xiong, D Parikh, R Socher, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionLu, J.; Xiong, C.; Parikh, D.; and Socher, R. 2017. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, 375-383.</p>
<p>Effective Approaches to Attention-based Neural Machine Translation. T Luong, H Pham, C D Manning, 10.18653/v1/D15-1166Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsLuong, T.; Pham, H.; and Manning, C. D. 2015. Effec- tive Approaches to Attention-based Neural Machine Trans- lation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1412-1421. Lis- bon, Portugal: Association for Computational Linguistics. doi:10.18653/v1/D15-1166. URL https://www.aclweb. org/anthology/D15-1166.</p>
<p>Dungeons and DQNs: Toward Reinforcement Learning Agents that Play Tabletop Roleplaying Games. L J Martin, S Sood, M Riedl, H Wu, M Si, A Jhala, Proceedings of the Joint Workshop on Intelligent Narrative Technologies and Workshop on Intelligent Cinematography and Editing co-located with 14th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, INTWICED@AIIDE 2018. the Joint Workshop on Intelligent Narrative Technologies and Workshop on Intelligent Cinematography and Editing co-located with 14th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, INTWICED@AIIDE 2018Alberta, Canada2321CEUR Workshop ProceedingsMartin, L. J.; Sood, S.; and Riedl, M. 2018. Dungeons and DQNs: Toward Reinforcement Learning Agents that Play Tabletop Roleplaying Games. In Wu, H.; Si, M.; and Jhala, A., eds., Proceedings of the Joint Workshop on Intelligent Narrative Technologies and Workshop on Intel- ligent Cinematography and Editing co-located with 14th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, INTWICED@AIIDE 2018, Edmon- ton, Alberta, Canada, November 13-14, 2018, volume 2321 of CEUR Workshop Proceedings. CEUR-WS.org. URL http://ceur-ws.org/Vol-2321/paper4.pdf.</p>
<p>Programs with common sense. J W Mccarthy, Teddington Conference on the Mechanization of Thought Processes. Mccarthy, J. W. 1960. Programs with common sense. In Ted- dington Conference on the Mechanization of Thought Pro- cesses.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, In International conference on machine learning. Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asyn- chronous methods for deep reinforcement learning. In In- ternational conference on machine learning, 1928-1937.</p>
<p>K Murugesan, M Atzeni, P Shukla, M Sachan, P Kapanipathi, K Talamadupula, arXiv:2005.00811Enhancing Textbased Reinforcement Learning Agents with Commonsense Knowledge. arXiv preprintMurugesan, K.; Atzeni, M.; Shukla, P.; Sachan, M.; Kapa- nipathi, P.; and Talamadupula, K. 2020. Enhancing Text- based Reinforcement Learning Agents with Commonsense Knowledge. arXiv preprint arXiv:2005.00811 .</p>
<p>Language understanding for text-based games using deep reinforcement learning. K Narasimhan, T Kulkarni, R Barzilay, arXiv:1506.08941arXiv preprintNarasimhan, K.; Kulkarni, T.; and Barzilay, R. 2015. Lan- guage understanding for text-based games using deep rein- forcement learning. arXiv preprint arXiv:1506.08941 .</p>
<p>Glove: Global Vectors for Word Representation. J Pennington, R Socher, C D Manning, 10.3115/v1/d14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Moschitti, A.Pang, B.and Daelemans, W.the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarA meeting of SIGDAT, a Special Interest Group of the ACLPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global Vectors for Word Representation. In Moschitti, A.; Pang, B.; and Daelemans, W., eds., Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, 1532-1543. ACL. doi:10.3115/v1/d14-1162. URL https://doi.org/10.3115/v1/d14-1162.</p>
<p>Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language Models are Unsupervised Multitask Learners .</p>
<p>Atomic: An atlas of machine commonsense for if-then reasoning. M Sap, R Le Bras, E Allaway, C Bhagavatula, N Lourie, H Rashkin, B Roof, N A Smith, Y Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Sap, M.; Le Bras, R.; Allaway, E.; Bhagavatula, C.; Lourie, N.; Rashkin, H.; Roof, B.; Smith, N. A.; and Choi, Y. 2019. Atomic: An atlas of machine commonsense for if-then rea- soning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, 3027-3035.</p>
<p>Bidirectional attention flow for machine comprehension. M Seo, A Kembhavi, A Farhadi, H Hajishirzi, arXiv:1611.01603arXiv preprintSeo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2016. Bidirectional attention flow for machine comprehen- sion. arXiv preprint arXiv:1611.01603 .</p>
<p>ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. R Speer, J Chin, C Havasi, AAAI. Speer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. In AAAI, 4444-4451.</p>
<p>A Talmor, J Herzig, N Lourie, J Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprintTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937 .</p>
<p>Graph Attention Networks. P VeliÄkoviÄ‡, G Cucurull, A Casanova, A Romero, P LiÃ², Y Bengio, International Conference on Learning Representations. VeliÄkoviÄ‡, P.; Cucurull, G.; Casanova, A.; Romero, A.; LiÃ², P.; and Bengio, Y. 2018. Graph Attention Networks. In In- ternational Conference on Learning Representations. URL https://openreview.net/forum?id=rJXMpikCZ.</p>
<p>Sample efficient actor-critic with experience replay. Z Wang, V Bapst, N Heess, V Mnih, R Munos, K Kavukcuoglu, N Freitas, arXiv:1611.01224arXiv preprintWang, Z.; Bapst, V.; Heess, N.; Mnih, V.; Munos, R.; Kavukcuoglu, K.; and de Freitas, N. 2016. Sample ef- ficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224 .</p>
<p>Understanding natural language. T Winograd, Cognitive psychology. 31Winograd, T. 1972. Understanding natural language. Cog- nitive psychology 3(1): 1-191.</p>
<p>Qanet: Combining local convolution with global self-attention for reading comprehension. A W Yu, D Dohan, M.-T Luong, R Zhao, K Chen, M Norouzi, Q V Le, arXiv:1804.09541arXiv preprintYu, A. W.; Dohan, D.; Luong, M.-T.; Zhao, R.; Chen, K.; Norouzi, M.; and Le, Q. V. 2018. Qanet: Combining local convolution with global self-attention for reading compre- hension. arXiv preprint arXiv:1804.09541 .</p>            </div>
        </div>

    </div>
</body>
</html>
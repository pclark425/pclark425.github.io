<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5130 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5130</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5130</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-269302604</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14467v1.pdf" target="_blank">Integrating chemistry knowledge in large language models via prompt engineering</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents a study on the integration of domain-specific knowledge in prompt engineering to enhance the performance of large language models (LLMs) in scientific domains. The proposed domain-knowledge embedded prompt engineering method outperforms traditional prompt engineering strategies on various metrics, including capability, accuracy, F1 score, and hallucination drop. The effectiveness of the method is demonstrated through case studies on complex materials including the MacMillan catalyst, paclitaxel, and lithium cobalt oxide. The results suggest that domain-knowledge prompts can guide LLMs to generate more accurate and relevant responses, highlighting the potential of LLMs as powerful tools for scientific discovery and innovation when equipped with domain-specific prompts. The study also discusses limitations and future directions for domain-specific prompt engineering development.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5130.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5130.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo-1106 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The specific GPT-3.5-series conversational model evaluated in this study via OpenAI API; used as the LLM backend for chemistry/materials/enzyme question-answering guided by prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (autoregressive / decoder-style LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not reported in this paper (pretrained by OpenAI on unspecified mixture of web, books, code, and other data); no fine-tuning for chemistry reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical analysis and synthesis assistance (case studies: organocatalysis (MacMillan catalyst), complex organic synthesis intermediate for paclitaxel, and battery cathode material LiCoO2); general small-molecule/enzyme/crystal-property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering — specifically evaluated with zero-shot, few-shot, expert, CoT variants and the paper's proposed domain-knowledge embedded prompt engineering (multi-expert, domain CoT demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual answers including SMILES decoding, natural-language reaction descriptions, reconstructed reaction schemes, and crystallographic numeric/textual outputs (e.g., lattice vectors, densities); multiple-choice formatted numerical outputs for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Capability (answer provided or not), Accuracy (graded 0–1 or discrete steps for verbal answers), F1 score (for multiple-choice/classification tasks), Hallucination Drop (1 - Accuracy/Capability).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Custom benchmark of 1,280 curated question-answer pairs drawn from 40 small molecules (PubChem), 40 enzymes (UniProt), and 40 crystals (Materials Project); tasks transformed into numerical multiple-choice and verbal grading schemes described in Supplementary materials.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using gpt-3.5-turbo-1106 with domain-knowledge embedded prompts produced substantially higher Capability, Accuracy and F1 scores and lower hallucination than conventional zero-/few-shot and CoT prompts across most tasks. In case studies the model (with tailored prompts) could: decode SMILES and infer basic properties, predict catalytic products for MacMillan catalyst substrates, reconstruct a key synthetic step toward a paclitaxel intermediate (identifying reactive groups and reaction type), and compute/interpret lattice properties for LiCoO2. The paper does NOT claim automated de novo molecule generation at scale; demonstrations focused on analysis, inference, and route/scheme reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared prompt-engineering variants (zero-shot, few-shot, expert, zero-shot CoT, few-shot CoT) and showed the domain-knowledge embedded prompt engineering outperformed them on most metrics. The paper contrasts LLM-based prompting with traditional virtual screening/high-throughput computational methods conceptually, but provides no head-to-head benchmarking against generative chemistry models (e.g., graph generative models or diffusion models) for de novo molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model size and training corpus specifics not provided; only one proprietary model (gpt-3.5-turbo-1106) tested. API call limits constrained dataset scale. LLMs performed poorly on precise numerical/exact-data retrieval and basic arithmetic tasks, showed hallucinations especially for numeric experimental-data answers and uncommon property retrieval, and cannot access external databases within prompts (retrieval limitations). The study demonstrates assistance in design/analysis but does not demonstrate scalable de novo generation of novel synthesizable compounds or experimental validation of proposed designs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating chemistry knowledge in large language models via prompt engineering', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5130.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5130.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-knowledge embedded prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-knowledge embedded prompt engineering (multi-expert, domain CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-engineering methodology developed in this paper that integrates domain-specific chemical knowledge into multi-expert role-play prompts and chain-of-thought demonstrations to steer LLM outputs toward more accurate, less hallucinatory, domain-relevant answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied with gpt-3.5-turbo-1106 in this study (can be applied to other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-guided use of transformer LLMs (no fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not a training procedure; method supplies in-context domain information and CoT demonstrations drawn from curated datasets (PubChem, UniProt, Materials Project) and expert knowledge encoded in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Assisting chemistry/materials/enzyme tasks including interpretation of SMILES, property prediction, retrosynthetic/forward reaction step analysis, catalytic product prediction, crystallographic property calculations — general aim: support molecular design, synthesis planning and materials analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering: multi-expert role-play prompts combined with few-shot chain-of-thought demonstrations and domain-specific instructions; outputs assembled via majority/minority aggregation across 'experts'.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual chemical analyses, predicted reaction products and reaction schemes, decoded SMILES interpretation, and numerical crystallographic results presented within the LLM's natural-language responses (and converted to multiple-choice answers for numerical evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as LLM evaluation: Capability, Accuracy, F1 score, Hallucination Drop; additionally case studies qualitatively assessed via example dialogues (Supplementary S.5).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>The authors curated a 1,280-question benchmark from 40 PubChem small molecules, 40 UniProt enzymes, and 40 Materials Project crystals to evaluate prompt variants and CoT complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The domain-knowledge embedded prompts consistently improved LLM performance across most task categories, giving the largest gains for verbal reasoning and complex CoT tasks; they reduced hallucinations on many tasks and enabled the model to (a) identify reactive groups and reconstruct reaction steps for a paclitaxel intermediate, (b) predict catalytic products for substrates with the MacMillan catalyst, and (c) analyze LiCoO2 lattice and stability. Improvements were quantified across the study's Capability/Accuracy/F1/Hallucination metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed conventional prompting strategies (zero-shot, few-shot, expert, zero-shot CoT, few-shot CoT) on the curated benchmark. The paper notes that prompt engineering is a cost-effective alternative to model fine-tuning but does not replace methods that have direct external-database access or physics-based simulation for property-accurate prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not equate to model fine-tuning or provide external database retrieval; performance still weak on exact numeric retrieval and arithmetic, and limited by the LLM's pretraining knowledge and hallucination tendencies. Scalability constrained by API limits; no large-scale de novo molecule generation or wet-lab synthesis validation provided. The approach depends on careful hand-crafted prompts and expert-in-the-loop refinement for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating chemistry knowledge in large language models via prompt engineering', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChemCrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Large-scale chemical language representations capture molecular structure and properties <em>(Rating: 2)</em></li>
                <li>Inverse molecular design using machine learning: Generative models for matter engineering <em>(Rating: 1)</em></li>
                <li>Guided diffusion for inverse molecular design <em>(Rating: 1)</em></li>
                <li>Inverse design of 3d molecular structures with conditional generative neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5130",
    "paper_id": "paper-269302604",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "gpt-3.5-turbo-1106",
            "name_full": "gpt-3.5-turbo-1106 (OpenAI)",
            "brief_description": "The specific GPT-3.5-series conversational model evaluated in this study via OpenAI API; used as the LLM backend for chemistry/materials/enzyme question-answering guided by prompt engineering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_type": "transformer (autoregressive / decoder-style LLM)",
            "model_size": null,
            "training_data": "Not reported in this paper (pretrained by OpenAI on unspecified mixture of web, books, code, and other data); no fine-tuning for chemistry reported.",
            "application_domain": "Chemical analysis and synthesis assistance (case studies: organocatalysis (MacMillan catalyst), complex organic synthesis intermediate for paclitaxel, and battery cathode material LiCoO2); general small-molecule/enzyme/crystal-property prediction.",
            "generation_method": "Prompt engineering — specifically evaluated with zero-shot, few-shot, expert, CoT variants and the paper's proposed domain-knowledge embedded prompt engineering (multi-expert, domain CoT demonstrations).",
            "output_representation": "Textual answers including SMILES decoding, natural-language reaction descriptions, reconstructed reaction schemes, and crystallographic numeric/textual outputs (e.g., lattice vectors, densities); multiple-choice formatted numerical outputs for evaluation.",
            "evaluation_metrics": "Capability (answer provided or not), Accuracy (graded 0–1 or discrete steps for verbal answers), F1 score (for multiple-choice/classification tasks), Hallucination Drop (1 - Accuracy/Capability).",
            "benchmarks_or_datasets": "Custom benchmark of 1,280 curated question-answer pairs drawn from 40 small molecules (PubChem), 40 enzymes (UniProt), and 40 crystals (Materials Project); tasks transformed into numerical multiple-choice and verbal grading schemes described in Supplementary materials.",
            "results_summary": "Using gpt-3.5-turbo-1106 with domain-knowledge embedded prompts produced substantially higher Capability, Accuracy and F1 scores and lower hallucination than conventional zero-/few-shot and CoT prompts across most tasks. In case studies the model (with tailored prompts) could: decode SMILES and infer basic properties, predict catalytic products for MacMillan catalyst substrates, reconstruct a key synthetic step toward a paclitaxel intermediate (identifying reactive groups and reaction type), and compute/interpret lattice properties for LiCoO2. The paper does NOT claim automated de novo molecule generation at scale; demonstrations focused on analysis, inference, and route/scheme reconstruction.",
            "comparison_to_other_methods": "Compared prompt-engineering variants (zero-shot, few-shot, expert, zero-shot CoT, few-shot CoT) and showed the domain-knowledge embedded prompt engineering outperformed them on most metrics. The paper contrasts LLM-based prompting with traditional virtual screening/high-throughput computational methods conceptually, but provides no head-to-head benchmarking against generative chemistry models (e.g., graph generative models or diffusion models) for de novo molecule generation.",
            "limitations_or_challenges": "Model size and training corpus specifics not provided; only one proprietary model (gpt-3.5-turbo-1106) tested. API call limits constrained dataset scale. LLMs performed poorly on precise numerical/exact-data retrieval and basic arithmetic tasks, showed hallucinations especially for numeric experimental-data answers and uncommon property retrieval, and cannot access external databases within prompts (retrieval limitations). The study demonstrates assistance in design/analysis but does not demonstrate scalable de novo generation of novel synthesizable compounds or experimental validation of proposed designs.",
            "uuid": "e5130.0",
            "source_info": {
                "paper_title": "Integrating chemistry knowledge in large language models via prompt engineering",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Domain-knowledge embedded prompt engineering",
            "name_full": "Domain-knowledge embedded prompt engineering (multi-expert, domain CoT)",
            "brief_description": "A prompt-engineering methodology developed in this paper that integrates domain-specific chemical knowledge into multi-expert role-play prompts and chain-of-thought demonstrations to steer LLM outputs toward more accurate, less hallucinatory, domain-relevant answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied with gpt-3.5-turbo-1106 in this study (can be applied to other LLMs)",
            "model_type": "prompt-guided use of transformer LLMs (no fine-tuning)",
            "model_size": null,
            "training_data": "Not a training procedure; method supplies in-context domain information and CoT demonstrations drawn from curated datasets (PubChem, UniProt, Materials Project) and expert knowledge encoded in prompts.",
            "application_domain": "Assisting chemistry/materials/enzyme tasks including interpretation of SMILES, property prediction, retrosynthetic/forward reaction step analysis, catalytic product prediction, crystallographic property calculations — general aim: support molecular design, synthesis planning and materials analysis.",
            "generation_method": "Prompt engineering: multi-expert role-play prompts combined with few-shot chain-of-thought demonstrations and domain-specific instructions; outputs assembled via majority/minority aggregation across 'experts'.",
            "output_representation": "Textual chemical analyses, predicted reaction products and reaction schemes, decoded SMILES interpretation, and numerical crystallographic results presented within the LLM's natural-language responses (and converted to multiple-choice answers for numerical evaluation).",
            "evaluation_metrics": "Same as LLM evaluation: Capability, Accuracy, F1 score, Hallucination Drop; additionally case studies qualitatively assessed via example dialogues (Supplementary S.5).",
            "benchmarks_or_datasets": "The authors curated a 1,280-question benchmark from 40 PubChem small molecules, 40 UniProt enzymes, and 40 Materials Project crystals to evaluate prompt variants and CoT complexities.",
            "results_summary": "The domain-knowledge embedded prompts consistently improved LLM performance across most task categories, giving the largest gains for verbal reasoning and complex CoT tasks; they reduced hallucinations on many tasks and enabled the model to (a) identify reactive groups and reconstruct reaction steps for a paclitaxel intermediate, (b) predict catalytic products for substrates with the MacMillan catalyst, and (c) analyze LiCoO2 lattice and stability. Improvements were quantified across the study's Capability/Accuracy/F1/Hallucination metrics.",
            "comparison_to_other_methods": "Outperformed conventional prompting strategies (zero-shot, few-shot, expert, zero-shot CoT, few-shot CoT) on the curated benchmark. The paper notes that prompt engineering is a cost-effective alternative to model fine-tuning but does not replace methods that have direct external-database access or physics-based simulation for property-accurate prediction.",
            "limitations_or_challenges": "Does not equate to model fine-tuning or provide external database retrieval; performance still weak on exact numeric retrieval and arithmetic, and limited by the LLM's pretraining knowledge and hallucination tendencies. Scalability constrained by API limits; no large-scale de novo molecule generation or wet-lab synthesis validation provided. The approach depends on careful hand-crafted prompts and expert-in-the-loop refinement for best results.",
            "uuid": "e5130.1",
            "source_info": {
                "paper_title": "Integrating chemistry knowledge in large language models via prompt engineering",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
            "rating": 2,
            "sanitized_title": "largescale_chemical_language_representations_capture_molecular_structure_and_properties"
        },
        {
            "paper_title": "Inverse molecular design using machine learning: Generative models for matter engineering",
            "rating": 1,
            "sanitized_title": "inverse_molecular_design_using_machine_learning_generative_models_for_matter_engineering"
        },
        {
            "paper_title": "Guided diffusion for inverse molecular design",
            "rating": 1,
            "sanitized_title": "guided_diffusion_for_inverse_molecular_design"
        },
        {
            "paper_title": "Inverse design of 3d molecular structures with conditional generative neural networks",
            "rating": 1,
            "sanitized_title": "inverse_design_of_3d_molecular_structures_with_conditional_generative_neural_networks"
        }
    ],
    "cost": 0.01218825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Integrating Chemistry Knowledge in Large Language Models via Prompt Engineering
22 Apr 2024</p>
<p>Hongxuan Liu 
Haoyu Yin 
Zhiyao Luo zhiyao.luo@eng.ox.ac.uk 
Xiaonan Wang wangxiaonan@tsinghua.edu.cn </p>
<p>Department of Chemical Engineering
Tsinghua University
100084Beijing</p>
<p>Department of Chemical Engineering
Tsinghua University
100084Beijing</p>
<p>Institute of Biomedical Engineering Department of Engineering Science
University of Oxford Headington
OX3 7DQOxfordUnited Kingdom</p>
<p>Department of Chemical Engineering
Tsinghua University
100084Beijing</p>
<p>Integrating Chemistry Knowledge in Large Language Models via Prompt Engineering
22 Apr 202480E53575E11DDE6CB44B181E4E7523CAarXiv:2404.14467v1[cs.CL]
This paper presents a study on the integration of domain-specific knowledge in prompt engineering to enhance the performance of large language models (LLMs) in scientific domains.A benchmark dataset is curated to encapsulate the intricate physical-chemical properties of small molecules, their drugability for pharmacology, alongside the functional attributes of enzymes and crystal materials, underscoring the relevance and applicability across biological and chemical domains.The proposed domain-knowledge embedded prompt engineering method outperforms traditional prompt engineering strategies on various metrics, including capability, accuracy, F1 score, and hallucination drop.The effectiveness of the method is demonstrated through case studies on complex materials including the MacMillan catalyst, paclitaxel, and lithium cobalt oxide.The results suggest that domain-knowledge prompts can guide LLMs to generate more accurate and relevant responses, highlighting the potential of LLMs as powerful tools for scientific discovery and innovation when equipped with domain-specific prompts.The study also discusses limitations and future directions for domain-specific prompt engineering development.</p>
<p>Introduction</p>
<p>The rapid advancement in artificial intelligence (AI) has significantly propelled its integration into natural science, specifically biology, chemistry and material science.Virtual screening contains thresholds determination and properties labeling, which can exhaust known design space [1] and guide experimental explorations [2].Designing thresholds requires thorough domain insights but the rate-determining step in virtual screening is labeling the data.Early applications of AI in science were focused on properties predictions (e.g.formation energy [3], selectivity &amp; permeability of membranes [4][5][6], protein structures [7][8] and drug delivery [9][10]).As machine learning advances, more variants of artificial neural networks enabled AI to handle information in complex modal and solve more sophisticated problems in computational chemistry and bioinformatics.For example, MLP (multilayer perceptron) based machine learning potentials for molecular dynamics [11][12], GCN based DFT (density functional theory) for enzyme structural activity [13][14], CNN based molecular electron microscope images processing [15][16].However, traditional high-throughput virtual screening is limited to known molecules or materials.The emergence of AI in inverse design emphasizes the need for innovative models that can assist experts in discovering new structures [17].Models containing generating and predicting enable de novo design of molecules [18][19], drugs [20] and proteins [21][22].</p>
<p>A key challenge in applying AI to science is the lack of experimental data, which is often costly and time-consuming to gather.For instance, A significant drawback of using physics-based computational methods to gather data is the consumption of extensive computational resources and time.On the other hand, Directed modification and expression of enzyme genes also require several days to weeks of time.As a result, the lack of sufficient effective training data is always inadequate for training models effectively.[23].Moreover, a trivial protein composed of 100 amino acid residues, with 20100 possible amino acid sequences and a corresponding vast space of potential configurations, makes searching, optimizing, and directed evolution of enzymes within the vast chemical space highly infeasible [24].</p>
<p>Overcoming the 'small data' challenge is basic but essential.Among tremendous approaches towards improving learning efficiency, large language models (LLMs) open a new channel for more efficient virtual screening apart from conventional methods, such as high-throughput computational methods [25][26][27], autonomous wet experiments [28][29], and data efficient algorithms (e.g.Bayesian optimization [30][31] and active learning [32].LLMs are capable of processing and analyzing vast data amounts, which have notably advanced in addressing challenges like zero-shot reasoning, enabling them to handle tasks they haven't been explicitly trained for.They also excel in incorporating domain knowledge across various fields and providing explanations in natural language, thereby enhancing their adaptability and accessibility.The LLM based AI agents [33] and pre-trained foundation models [34][35] are considered as the next generation of AI scientific assistants.</p>
<p>Prompt quality affects LLMs' outputs significantly, many studies focus on well-defined prompts for general purposes (e.g., chain of thoughts reasoning [36], few shots learning [37]), known as prompt engineering [38][39][40].Enhancing LLMs for specific fields typically involves fine-tuning, which can be complex and costly for those outside AI community [41].Although there are already some domainspecific LLMs, they have not yet achieved the stability of general-purpose models like ChatGPT, leading many to focus on how to effectively utilize ChatGPT.Considering LLMs' remarkable learning abilities, strategic prompting or directing the LLM with specific instructions could be an effective alternative.However, current prompt engineering is mostly focused on general conditions such as academic writing [42] and science popularizing [43].For experts in non-AI disciplines, the true value of these models lies in their domain-specific expertise, rather than their general capabilities.The absence of prompt engineering for specific areas makes LLMs user-unfriendly, especially for experimental chemists and material scientists.</p>
<p>Our paper studies the overlooked gap in AI for biology, chemistry and materials science, including small molecules, crystal materials and protein enzymes, highlight the importance of prompting to researchers off-the-shelve LLMs.Our investigation shows the critical need for solutions that combine AI's generative capabilities with detailed materials science insights, aiming to enhance model applicability and to address domain-specific challenges across various research areas.</p>
<p>In this article, we introduce "domain-knowledge embedded prompt engineering" as a novel approach to enhance LLM performance in specialized areas, as depicted in Fig. 1.First, we have created a set of domain-specific datasets for the first time, supplementing the existing public datasets.Second, we have developed and tested specific prompts for various tasks in three examples extracted from chemistry, materials science, and biology.Third, we combined the general methods of the computer science community for comparison, validating that the approach is correct.This approach aligns with desired outcomes and involves developing appropriate evaluation metrics.We also address the issue of LLMs generating inaccurate or 'hallucinated' responses and designed strategies to mitigate this.Last, through a case study, we demonstrate how our prompting strategies can address specific challenges in these fields.Overall, we showed that domain-knowledge embedded prompt engineering offers a cost-effective and efficient way to leverage the potential of LLMs.In this chapter, we first introduce the construction of tasks from 3 domains: organic small molecules, enzymes and crystal materials, and the answer evaluation scheme for numerical and verbal tasks (See Section 2.1).We then formulate these tasks of domain question answering to an LLM question answering problem (See Section 2.2) and introduce various existing prompt engineering methods to address these tasks (See Section 2.3).Finally, we put forward our domain-knowledge embedded prompt engineering method (See Section 2.3).</p>
<p>Dataset Construction and Answer Evaluation Scheme</p>
<p>In task construction process, each of the 3 material categories (small molecule, enzyme and crystal material), holds significant relevance in academic research and practical applications.Organic small molecules are commonly utilized in pharmacy [44], while enzymes play a critical role in biocatalysis [45][46], and crystalline materials are essential in semiconductor technology and photovoltaic devices [47][48].While mainstream benchmark datasets such as MMLU [49], Big-Bench [50] and GSM8k [51] have been widely applied to LLM performance evaluation, the composition of these datasets are usually generic math or reasoning questions, lacking a concentrated focus on some specific knowledge domains or subjects.Compared to these datasets, our datasets could provide a more comprehensive evaluation of LLM's performance (using different prompt engineering methods) on specific chemistry domains.</p>
<p>We collect and curate a dataset of 1280 questions and corresponding solutions (See Table 1) for the evaluation of LLM's capability, as described below:</p>
<p>Organic Small Molecules: 40 molecules proven to have significant drug properties or potentials are selected and curated from Pubchem [52], each containing 9 crucial structural and physical-chemical properties.</p>
<p>Enzymes: 40 enzymes involved in significant metabolic pathways in vivo are selected and curated from UniProt database [53], each with 7 crucial sequence and functional information.</p>
<p>Crystal Materials: 40 representative crystals derived from the Materials Project database [54] are selected, each with 16 crucial structural and energy properties.</p>
<p>A detailed enumeration and classification of all task types are contained in Supplementary S.2.Due to the limitations in API callings of proprietary LLMs, it is very hard to test molecules on a larger scale (like for thousands of molecules), but we believe that the selected molecules are already very representative to demonstrate LLM's performance, and could pave the way for further applications in the future.In evaluating the performance of LLM prompt engineering methods on different tasks, four significant metrics are introduced:</p>
<p>Capability: To measure LLM's capability to provide an answer for a certain task, regardless of its correctness.Its value takes 1 if the answer is effective otherwise 0.</p>
<p>Accuracy:To evaluate the extent to which LLM's answer is identical or close to the ground truth.</p>
<p>F1 Score: to measure LLM's predictive performance on multiple-choices questions, combining precision and recall.F1 Score offers a more comprehensive evaluation compared to Accuracy, especially in cases of imbalance where accuracy might be high but does not reflect the true performance of LLM.</p>
<p>Hallucination Drop: A metric to quantify the discrepancy between an LLM's ability to answer questions (Capability) and the accuracy of those answers (Accuracy).It takes 1 minus the ratio of Accuracy and Capability as the value.This metric helps identify when LLM is trapped in severe hallucinations on certain tasks.</p>
<p>Detailed implementations of these metrics are listed in Supplementary S.3.</p>
<p>In our approach, we utilize an LLM plugged-in automatic scheme to evaluate the metrics above.</p>
<p>According to Table 3 in Supplementary, tasks can be divided into numerical and verbal ones, each of which takes a different manner to evaluate, respectively.Numerical Tasks: All numerical tasks are transformed to the form of multiple-choices questions, as straightforward error estimation of the answer from ground truth can be strongly affected by unit and scale, and the form of multiple-choices makes it easier and more reasonable in evaluation across various tasks.Detailed implementation of tasks' transformation into multiple-choices questions are described in Supplementary S.3.Metrics involved in numerical task evaluation are: Capability, Accuracy, F1 Score and Hallucination Drop.</p>
<p>While Capability, F1 Score and Hallucination Drop are evaluated in the normal form, the Accuracy of multiple choices questions is specifically defined.Full mark (1) is given if the option is exactly the ground truth.A partial score (0.4) is given if the value or range of the chosen option is adjacent to the ground truth.The complete scoring policy is listed in Supplementary S.3.</p>
<p>Verbal Tasks: For verbal answers, the LLM is guided by a series of grading examples coordinated to the specific question types and then required to give a grade to an answer.Detailed prompts for LLM's grading tasks are listed in Supplementary S.4.Metrics involved in verbal task evaluation are: Capability, Accuracy and Hallucination Drop.</p>
<p>While Capability and Hallucination Drop are evaluated in the normal form, the Accuracy of verbal tasks take discrete values among {0, 0.2, 0.4, 0.6, 0.8, 1}.Score 0 means that the answer is completely irrelevant to the ground truth, while the scores {0.2, 0.4, 0.6, 0.8} imply part of the answer aligned with the ground truth, extent to which increases with the value.Score 1 corresponds to answers intrinsically the same as ground truth.</p>
<p>It is worth emphasizing that we believe the LLM plugged-in automatic scheme above for evaluation could bear skepticism on fairness and effectiveness, as LLM's evaluation process is independent from LLM's predictive task performing in the last step, implying the LLM would not take past memories of task performing or "know" the answers were generated by itself, and thus is unlikely to "cheat" on the grading process.</p>
<p>Fig. 2 shows the flow chart of question construction and answer evaluation process.Data from 3 material categories are extracted and combined to form proper questions (some are in the form of multiple-choices questions).When the raw answers are acquired, they need to be checked for validity, and then aligned to proper answer forms.Ultimately the answers are automatically graded.In the LLM era, scientific prediction can be considered as a question answering task leveraging the zero-shot/few-shot reasoning power of LLM.It is demonstrated that by providing in-context hints to language model with size large enough for emergence to happen, the model can excavate knowledge learned from pre-trained data and well-perform the question answering task [55].As an approach to enhance LLM's capability on specific domains or tasks, prompt engineering significantly reduces the need for extensive task-specific datasets as required in LLM fine-tuning paradigm, making it an effective in-context learning method for LLM enhancement.</p>
<p>The process of prompt engineering could be mathematically formalized [56].Let Q be the question, P be the prompt, A be the answer by LLM, prompt engineering process is to determine the context of prompt words P such that the answer A could be given effectively by LLM:
A = f (P, Q) (1)
where f is the LLM.</p>
<p>A prompt optimization objective is to find:
arg max P g(f (P, Q), S)(2)
where S is the ground truth solution, and g is a evaluation function which measures how much the LLM answer A is in accordance with the ground truth solution S.</p>
<p>For our dataset D = {Q i , S i } n i , the general prompt optimization objective is to find the P that maximizes the expectation over the dataset: The essence of prompt engineering is to harness the full potential of LLMs in diverse applications by ensuring they respond in a manner that is most aligned with the user's intent and the task at hand.We give a brief introduction to several mainstream prompt engineering methods:
arg max P E Q,S∈D g(f (P, Q), S)(3)</p>
<p>Common Prompt Engineering Techniques and Domain-Knowledge Embedded Prompt Engineering</p>
<p>Zero-shot Prompting: Zero-shot Prompting requires LLM to answer the given question directly without providing any data or example questions in the context.(See Fig. 3 (a))</p>
<p>Few-shot Prompting: In Few-shot Prompting, an LLM is presented with several demonstrations, (i.e.question-answer pairs within the prompt context), better equipping the LLM to understand and replicate the response format and content.The demonstrations in prompt can be formalized to:
P = {(Q 1 , A 1 ), ..., (Q m , A m )}
, where m is the number of examples [55].(See Fig. 3</p>
<p>(b))</p>
<p>Expert Prompting: Role-play instructions have demonstrated their effectiveness in harnessing the potential of LLMs [56].By guiding LLMs step-by-step into assuming the role of domain experts, they can generate responses akin to those written by experts (See Fig. 3 (c)).</p>
<p>Zero-shot CoT (Chain-of-Thought) Prompting: By eliciting a sequential, step-by-step reasoning process to effectively address complex tasks, CoT enables the model to break down a task into its constituent parts, offering a clear and logical pathway to the solution [36].In particular, Zero-shot CoT prompting involves adding Let's think step by step to the prompt as a trigger-sentence.(See Fig.</p>
<p>(d))</p>
<p>Few-shot CoT Prompting: In addition to adding Let's think step by step to the prompt like Zero-shot CoT, Few-shot CoT provides several examples of Thought-Chain in solving similar problems to assist LLM perform the current task in a similar manner [57].The demonstrations in prompt can be formalized to:
P = {(Q 1 , C 1 , A 1 ), ..., (Q m , C m , A m )}. (See Fig. 3 (e))
A significant limitation of these prompt engineering methods is that they do not incorporate domain expertise as guidance for problem-solving, which considerably restricts the capabilities of LLMs in numerous domain-specific tasks.Moreover, since addressing many domain-specific challenges involves intricate cognitive processes, it is imperative to strategically combine various prompt engineering techniques at different stages to achieve optimality.</p>
<p>Here we propose a domain-knowledge embedded prompt engineering strategy that integrates chemistry knowledge into language model.The prompting scheme takes the form of multi-expert mixture.Each expert takes part in role playing and are given a few shots of CoT demonstrations integrated with expertise domain knowledge or instructions.</p>
<p>Here, incorporating domain knowledge essentially involves integrating the thought processes of chemistry/biology experts.This contrasts with the conventional zero-shot CoT approach, which merely prompts LLMs to engage in a chain of thought.By doing so, it offers more precise background knowledge and exemplifies more accurate human reasoning.The full documentation of all domainknowledge prompts are listed in Supplementary S.4.Then the experts' answers would be assembled through the principle of minority submission to the majority.The detailed scheme of these strategies are delineated in Fig. 3 and Fig. 4. In the following chapters, we compare this prompt engineering method proposed above to other generic prompt engineering methods including zero-shot prompting, few-shot prompting, expert prompting, and CoT prompting.</p>
<p>Results</p>
<p>In this chapter, we first present the overall benchmarks of prompt engineering methods over all tasks.</p>
<p>After that, we make detailed comparisons over different task types, CoT complexities and material types.In the last section, three case studies on representative molecules are conducted using our tailored domain-knowledge embedded prompt engineering method to illustrate the effectiveness of prompt engineering in assisting crucial scientific research topics.</p>
<p>Summery of Overall Performance</p>
<p>In our study, we evaluated five different prompt engineering strategies across three datasets (small molecule, enzyme, and crystal material), each yielding three sets of answers for robustness.The LLM model being evaluated is 'gpt-3.5-turbo-1106'[58] through official API calling.The prompt engineering strategies included zero-shot, few-shot, expert, and zero-shot CoT, along with domainknowledge method (ours).The overall evaluation results on three datasets are shown in Fig. 5 and Fig. 6: Our domain-knowledge embedded prompt engineering method outperforms other conventional prompt engineering techniques on most tasks and metrics.In nearly all tasks on small molecules and crystal materials, and more than 50% of the tasks on enzymes, our method's performance is very significantly higher than other methods, while on tasks: Molecular Density, Molecular Weight, Number of Amino Acids and Active Sites, our method does not demonstrate obvious advantages.In the following sections, we make more detailed comparisons for different tasks and molecules.Due to space limitation, we only present the key findings in the following sections.In Section 3.2, we compare these method's performance on different task types, while in Section 3.3, we delve into the correlation between prompt engineering method' performance and CoT complexity.Finally we compare prompt engineering methods' effectiveness on different types of materials.</p>
<p>Comparison by Task Types</p>
<p>In this section, we compare various prompt engineering methods performance on different types of tasks, classification of which can be referred to in Table 3 and Table 4.Each task type presents unique challenges and necessitates different inference abilities from the LLM.After aggregation, the performances of 5 prompt engineering methods on different question groups are shown in Fig. 7:</p>
<p>(1) Domain-knowledge embedded prompt engineering method outperforms traditional prompt engineering methods on all question types.Through a comprehensive evaluation across various groups of prediction tasks, focusing on four crucial indices -"Capability", "Accuracy", "F1 Score" and "Hallucination Drop", our domain-knowledge embedded prompt engineering method consistently outperforms traditional prompt engineering strategies.This superiority is evident in the substantial enhancement of both capability and accuracy metrics, with the most notable improvements exceeding a 100% boost.Such findings unequivocally demonstrate that integrating domain-specific knowledge into prompt engineering substantially elevates the effectiveness of generic prompt engineering techniques.</p>
<p>(2) LLM performs better for answers derived from logical reasoning than answers based on experimental data.This tendency is further amplified in our domain-specific prompt engineering method, where a more tailored prompt engineering strategy is applied.As shown in Fig. 7 (a), (b) and (c), it consistently leads to more significant improvements in tasks involving logical deduction compared to other prompt engineering methods.This disparity in performance can be attributed to the fact that LLMs, with refined prompt engineering, can engage in a sophisticated Chain-of-Thought process, enabling LLMs to excel in tasks that demand intricate reasoning and problem-solving skills.However, despite being trained on various scientific databases, LLMs do not excel in precisely replicating exact data values.This brings about their ability to process and reason through information well rather than serve as direct conduits for data retrieval.</p>
<p>(3) LLM performs better on verbal tasks compared to numerical tasks.When faced with tasks that require a numerical response, (actually in formats involving multiple choices), LLMs tend to exhibit weaker performance.This is evident in both capability and accuracy metrics across various prompt engineering methods, with numerical answers derived from experimental data showing the least favorable results (Fig. 7 (a), (b), (c)).When LLMs engage in numerical reasoning, their capability scores are notably higher (Fig. 7 (a)), but this advantage is tempered by significant issues with hallucinations, which adversely affect the accuracy of these responses.In fact, even when the most advanced prompt engineering methods are applied, the accuracy of logical numerical answers is surpassed by that of logical answers.This trend underscores a recognized weakness of LLMs in number-related tasks, as evidenced by several research studies.</p>
<p>(4) Domain-knowledge embedded prompt engineering method effectively reduces hallucination.</p>
<p>The metric of hallucination drop serves as a barometer for the average quality of answers produced by LLMs under different prompt engineering strategies.As shown in Fig. 7 (d), the question type of numerical answer by logic is the only category where an increase in hallucination is observed as the domain-knowledge embedded prompt engineering is applied.In the other three question types, the incorporation of domain-specific knowledge into the prompt engineering process effectively curtails the occurrence of hallucinations.Notably, the question types 'numerical answer by experimental data' and 'verbal answer by logic' emerge as front-runners, registering the top two lowest scores in hallucination drop.This outcome underscores the precision and effectiveness of domain-knowledge embedded prompt engineering methods in enhancing the reliability and accuracy of LLM responses.</p>
<p>These results from a more detailed classification based on reasoning paradigm also draw some intriguing conclusions below, showing the distinctive strengths and drawbacks in LLM reasoning: (5) LLM performs poorly on arithmetic tasks.These tasks revolving around basic counting, adding, and multiplying abilities, ostensibly require less sophisticated cognitive skills compared to tasks that necessitate spatial imagination or intense domain-knowledge based reasoning, but the performance of LLMs in these arithmetic tasks is unexpectedly subpar.Despite scoring high in capability, LLMs do not exhibit a corresponding lead in accuracy, showing higher occurrence of hallucinations in these tasks.In fact, the accuracy of LLMs in arithmetic tasks is not only significantly outpaced by domain knowledge literal reasoning tasks but also closely rivalled by spatial relationship tasks (Fig. 8 (b)).Notably, even the application of CoT heuristics in the reasoning process does not substantially mitigate this issue.This is evident in the Hallucination Drop metric, where both zero-shot-CoT and domain-knowledge embedded method exhibit a higher incidence of hallucination phenomena in arithmetic tasks compared to others (Fig. 8 (d)).</p>
<p>(6) LLM is incapable on many information retrieval tasks.These tasks, which cannot be effectively addressed through reasoning alone, generally exhibit poorer performance compared to those based purely on reasoning.As depicted in Fig. 8 (a), (b) and (c), tasks involving the retrieval of both common and uncommon properties record the lowest capability and accuracy scores, with tasks involving uncommon information faring slightly worse.Prompt engineering falls short in information retrieval, primarily due to its inability to provide direct access to external databases, but a well-crafted, domain-specific prompt can still marginally improve LLM performance by encouraging a more detailed response, as indicated by the higher capability scores for domain-knowledge prompt engineering.Despite this, the challenge of mitigating hallucinations remains formidable, with the highest incidence of hallucination observed in these types of tasks when using domain-knowledge prompts.</p>
<p>(7) Verbal reasoning tasks get largest boosting with domain-knowledge embedded prompt engineering method.In 5 question types classified by reasoning paradigms, "Domain Knowledge Literal Reasoning Tasks" distinctly stand out, especially when enhanced by domain-knowledge embedded prompt engineering methods.This category of tasks not only achieves the highest capability and accuracy scores overall but also maintains a relatively low level of hallucinations.This demonstrates well-crafted prompts can, in a remarkably efficient manner, stimulate the latent capabilities of LLMs, enabling them to generate answers with heightened confidence and precision.</p>
<p>Comparison by CoT Complexity</p>
<p>In this section, we compare different prompt engineering methods' distinction under a variety of CoT complexities, in order to depict our tailored prompt engineering method's superiority under different CoT complexities.We propose that the quantity of additional properties added in CoT prompts serves as a viable metric for gauging the complexity of the CoT process.This metric reflects the extent of extra information that is integrated into the CoT reasoning, which in turn influences the complexity and depth of the reasoning required.To operationalize this, we have categorized tasks based on the number of additional properties provided in each question, as shown on Table 5 in Supplementary.</p>
<p>It is, however, worth noticing that the number of additional properties provided (namely, the complexity of CoT) does not necessarily correlates to the difficulty of questions.The aggregated results are shown in Fig. 9: (1) Domain-Knowledge Embedded Prompt Engineering produces greatest performance lift in tasks with most complicated CoT formulation.In scenarios where LLMs are presented with different amounts of additional information for task execution, the domain-knowledge embedded prompt engineering method emerges as the most effective, outshining others in three key performance metrics: "Capability", "Accuracy", and "F1 Score".Specifically, it excels remarkably in "Tasks with Multiple Additional Properties" (Fig. 9 (a), (b), (c)).This highlights the advantage of domain-knowledge prompts in enhancing LLM performance in tasks that demand a complex CoT formulation.Furthermore, even for simpler zero-shot CoT method, this benefit makes it reverse the lead of few-shot method in "Tasks with Multiple Additional Properties", especially on F1 Score (Fig. 9 (c)).This aligns well with the intuitive understanding of CoT in enhancing inference-related capabilities.</p>
<p>(2) In-Context Information Could Effectively Reduce Hallucination Level.Tasks supplied with the most in-context extra information consistently exhibit the lowest levels of hallucination across all prompt engineering methods, as shown in Fig. 9 (d).This trend holds true regardless of whether the prompt engineering method incorporates domain-knowledge features.A notable observation is that many tasks in the "Tasks with Multiple Additional Properties" category are inherently complex and challenging.For example, predicting the drugability of a small molecule often necessitates a thorough and intricate examination under Lipinski's Rule of Five.Similarly, calculating the crystal density of a substance involves complex computations, including the determination of relative molecular mass of a unit cell, the measurement of unit cell volume, and intricate unit transformations.</p>
<p>The surprisingly low hallucination levels is indicative of the effectiveness of providing additional in-context information,suggests that enriching LLM prompts with more contextual information and factual details may substantially enhance the robustness and reliability of the generated content.</p>
<p>Comparison by Material Differences</p>
<p>In this section, a detailed comparison of prompt engineering accuracy on three types of materials will be portrayed.For clarity, we only focus on our tailored prompting method (namely the domainknowledge embedded prompting)'s performance on small molecules, enzymes and crystal materials with divergent material traits.The methodology employed to quantify the differences among these materials will be elaborated upon in the following paragraphs.</p>
<p>For small molecules, we propose two indicators-molecular weight and elemental composition-to differentiate the complexity of various molecules.This is predicated on the rationale that more complex molecules typically necessitate a higher level of analytical effort, which could inversely affect accuracy.We aim to investigate whether this assumption aligns with the empirical results obtained from our study.</p>
<p>For enzymes, we also employ two indicators to discern the difficulty in predicting properties among different enzymes: enzyme scale, quantified by the number of amino acids, and the current depth of research, denoted as the number of reviewed publications recorded on Uniprot [53].It is hypothesized that more complex enzymes, characterized by longer peptide chains and a lesser degree of comprehensive research, necessitate a higher analytical effort, potentially reducing accuracy.We intend to explore whether this hypothesis is consistent with the overall findings of our analysis.</p>
<p>For crystalline materials, we utilize two indicators to gauge complexity: formula complexity, which reflects the complexity of a single unit cell, and unit cell symmetry, denoted by the crystal system to which it belongs.The underlying premise is that more complex crystalline materials demand a more substantial analytical effort, which could, in turn, diminish accuracy.We will investigate whether this premise aligns with the collective results of our study.</p>
<p>(1) The prediction accuracy of LLMs deteriorates for larger and more complex organic molecules.As the molecular weight increases and the elemental composition becomes more diverse, we observe a gradual decline in the LLM's prediction accuracy.Specifically, molecules comprising more than five distinct elements exhibit significantly poorer performance compared to those with fewer components.Moreover, when the molecular weight exceeds 300 g/mol, the overall accuracy for single molecule predictions generally falls below 30%, as shown in Fig. 10 (a).Furthermore, large organic molecules are less commonly found in literature compared to smaller molecules, exacerbating the difficulty of LLM's information retrieval.</p>
<p>(2) The accuracy of LLMs in predicting properties of specific enzymes aligns closely with the depth of current research on these enzymes but shows a weak correlation with the enzymes' However, there appears to be no explicit relationship between the size of the enzyme, measured by the number of amino acids, and the accuracy of LLM predictions.This outcome suggests that the predictive ability of LLMs for enzymes primarily relies on information retrieval, specifically from scientific literature reports, rather than on the direct analysis of the enzyme's structure.</p>
<p>(3) The prediction accuracy of LLMs decreases for crystalline materials with larger, more complex compositions.As the prediction target's gauge complexity increases, indicated by formula complexity, there is a gradual decline in the LLM's prediction accuracy.Crystals comprising more than four elements perform significantly worse than those with fewer components.Additionally, when the number of formula atoms exceeds 10, the overall accuracy for single crystal predictions generally falls below 15%, as shown in Fig. 10 (c).Apart from the intrinsic complexity of crystals to bring difficulty in analysis"since most prediction tasks for crystalline materials in our datasets do not require inference and mainly rely on data retrieval, the rarity of large crystals in the literature compared to more common crystalline materials increases the difficulty of LLM's information retrieval.</p>
<p>(4) The prediction accuracy of LLMs concerning crystalline materials demonstrates a notable correlation with unit cell symmetry.Specifically, crystals belonging to the Trigonal, Cubic, or Hexagonal lattice systems are more likely to yield better predictions.The reason for this is twofold: first, these structures are inherently more regular and defined, making them easier subjects for inferential analysis.Secondly, these types of crystal structures are more readily studied and characterized by modern crystallography instruments and techniques, such as X-ray diffraction and electron microscopy, leading to a richer presence in scientific literature.This abundance of data enhances the LLM's ability to retrieve relevant information, thereby improving prediction accuracy for crystals with these symmetries.</p>
<p>In conclusion, these empirical evidences presented supports the intuitive notion that domainknowledge embedded prompts enhance the performance of LLMs to different extents.Firstly, the prompts' inferential capabilities are closely tied to the complexity of the analytical subject matter.Secondly, their proficiency in retrieval is correlated with the depth of contemporary academic research, suggesting that well-crafted prompts can effectively mine the latent knowledge absorbed during the LLM's pre-training phase.</p>
<p>Ultimately, these findings pose future challenges for leveraging LLMs to aid scientific inquiries into complex and novel molecules that are rarely encountered or underrepresented in the academic literature.By addressing these challenges, LLMs could potentially revolutionize the approach to research in the synthesis, analysis, and applications of such molecules, thereby expanding the frontiers of scientific knowledge.To elucidate the efficacy of the domain-knowledge embedded prompt engineering method in addressing highly domain-specific tasks, we have meticulously designed three case studies.These studies centerpiece the investigation of three materials of profound chemical or biological importance, both in terms of academic research and industrial applications, utilizing our bespoke prompt engineering method that incorporates chemistry-specific domain knowledge.To enhance clarity and conciseness, we illustrate a single expert's prompt engineering workflow, omitting the assembly of contributions from multiple experts, as this singular demonstration already effectively showcases how our prompt engineering method significantly impacts the performance of the LLM.</p>
<p>Case Studies</p>
<p>In the first case study, we direct our attention to the MacMillan's imidazolidinone 2nd generation catalyst, (2S,5S)-(-)-2-tert-Butyl-3-methyl-5-benzyl-4-imidazolidinone.The MacMillan catalyst, a groundbreaking advancement in the field of chemistry, was distinguished by the Nobel Prize in Chemistry in 2021 for its seminal contributions to the development of organocatalysis [59][60].This innovation has had a transformative impact on both synthetic chemistry and the broader chemical industry, enabling more efficient and environmentally friendly catalytic processes that are pivotal in the synthesis of complex molecules.</p>
<p>The first case study aims to assess the capability of LLMs in assimilating the intricate details of this molecule and in delineating its potential applications.By employing our domain-knowledge embedded prompt engineering method, we seek to uncover how LLMs can be leveraged to provide insights into the reactivity, selectivity, and scope of application of the MacMillan catalyst, thereby enhancing the efficiency and productivity of chemical research in this area.As shown in Fig. 11, by utilizing our tailored prompts, the LLM effectively elucidated the fundamental attributes of MacMillan's second-generation imidazolidinone catalyst, demonstrating its proficiency in the analysis of SMILES sequences and elementary arithmetic operations.Additionally, armed with the catalyst's mechanism and illustrative examples, the LLM was able to accurately anticipate the catalytic products from specified substrates, thereby highlighting the model's capacity to inform and potentially guide practical and industrial applications of catalysts.In light of this case study, it is evident that LLMs, embedded with domain-knowledge prompts, have the potential to significantly facilitate the development and optimization of catalysts for chemical reactions, thereby enhancing the efficiency and selectivity of synthetic processes in the field of chemistry.The complete interactive dialogue with LLM could be found in Supplementary S.5.</p>
<p>The next material under examination in our case study is paclitaxel (PTX, C47H51NO14), a compound of profound significance in the field of oncology and a critical component in the treatment of various cancers.Paclitaxel's discovery and subsequent development mark a pivotal moment in the history of cancer therapy, as it introduced a novel mechanism of action that targets microtubules, thereby inhibiting the growth and division of cancer cells.Its efficacy in the treatment of breast, ovarian, and other cancers has established paclitaxel as a cornerstone in the chemotherapy arsenal [61].The importance of paclitaxel extends beyond its direct clinical applications; it has also served as a template for the development of other taxane derivatives and has been a subject of extensive research in organic synthesis [62].The complex structure of paclitaxel presents a significant challenge in the synthesis process, leading to the development of various strategies to improve yield, reduce cost, and enhance accessibility to this life-saving compound.</p>
<p>In this prompt engineering case study, we focus on a crucial step in the synthesis of an active intermediate of paclitaxel.Our objective is to assess the ability of LLMs to analyze and provide insights into the pathway of organic synthesis.By utilizing our domain-knowledge embedded prompt engineering method, we aim to demonstrate the potential of LLMs in assisting chemists in the design and optimization of synthetic routes for complex molecules, such as paclitaxel and its derivatives, thereby contributing to the advancement of both chemical research and pharmaceutical development.As shown in Fig. 12, by utilizing custom-designed prompts, the LLM adeptly dissected a critical step in the synthesis of paclitaxel.It not only identified the reactive groups within the substrates that are capable of engaging in the chemical transformation but also correctly discerned the type of reaction and reconstructed the entire reaction scheme.This accomplishment underscores the LLM's potential in providing guidance for the synthesis of chemical compounds, suggesting that such models could play a pivotal role in streamlining the process of chemical synthesis, offering insights into reaction of complex molecules.This has implications for the advancement of medicinal chemistry and the development of pharmaceuticals, where efficient synthesis routes are of paramount importance.The complete interactive dialogue with LLM could be found in Supplementary S.5.</p>
<p>In the concluding case study, we examine lithium cobalt oxide (LiCoO2), a material of great importance in lithium-ion battery technology.Recognized by the 2018 Nobel Prize in Chemistry, LiCoO2's contribution to energy storage has been transformative, enabling the widespread use of portable electronics and electric vehicles [63].As a cathode material, LiCoO2 offers high energy density and stability, although research continues to address its lifecycle, cost, and environmental footprint.</p>
<p>In this prompt engineering case study, we delve into the analysis of LiCoO2 crystals and their application advantages.We aim to harness the capabilities of LLMs to provide detailed insights into the crystallographic properties, electrochemical behavior, and optimization strategies for LiCoO2.By employing our domain-knowledge embedded prompt engineering method, we expect to demonstrate the potential of LLMs in aiding researchers in the design and refinement of battery materials, thereby contributing to the progress of energy storage technologies and supporting the global transition towards sustainable energy solutions.</p>
<p>In this case study, the LLM meticulously analyzed the fundamental properties of lithium cobalt oxide (LiCoO2), accurately determining its lattice volume and stability, as shown in Fig. 13.This achievement is of significant importance in the field of crystallography and future development of lithium-ion battery technologies.The complete interactive dialogue with LLM could be found in Supplementary S.5.</p>
<p>Conclusion and Future Directions</p>
<p>The integration of domain-specific knowledge into prompt engineering has demonstrated its effectiveness in enhancing the performance of LLMs across various tasks in chemistry, materials science, and biology.Our proposed domain-knowledge embedded prompt engineering method outperforms traditional generic prompt engineering strategies on metrics such as capability, accuracy, F1 score, and hallucination drop.The incorporation of domain expertise into prompts not only guides the LLM to synthesize more relevant knowledge but also provides a clear reasoning path for complex tasks.</p>
<p>Our case studies further validate the effectiveness of this approach in analyzing intricate materials like the MacMillan catalyst, paclitaxel, and LiCoO2, demonstrating the potential of LLMs to assist experts in molecular design and optimization when equipped with domain-specific prompts.The complete code implementation of our work is listed in Supplementary S.1.</p>
<p>Limitations and potential future directions of our work is also concluded below:</p>
<p>Expansion of Domain Coverage: While our study has focused on chemistry, materials, and biology, the concept of domain-knowledge embedded prompt engineering can be extended to other scientific domains.Future work can explore the development of tailored prompts for fields such as physics, geology, and medicine to unlock the full potential of LLMs in diverse scientific applications.</p>
<p>Integration of Datasets and Tools:</p>
<p>To further enhance the reasoning capabilities of LLMs, future prompt engineering can integrate external datasets and domain-specific tools.Linking prompts to chemical databases, computational chemistry software, or biological sequence analysis tools could enable the LLM to leverage additional information for more accurate predictions.</p>
<p>Multi-Modal Prompting: Incorporating visual information, such as molecular structures or crystal images, into prompts can provide a more intuitive understanding for LLMs.Multi-modal prompting techniques combining textual and visual cues can potentially lead to even stronger performance gains.</p>
<p>Human-in-the-Loop Refinement: Iteratively refining prompts with input from domain experts can help to uncover more effective prompting strategies.Human-in-the-loop systems that leverage the complementary strengths of LLMs and human experts have the potential to achieve highly optimized prompts.</p>
<p>Prompt Engineering Benchmarking: To ensure comprehensive and fair evaluation of prompting strategies, it is meaningful to establish standardized benchmarks across multiple LLMs, especially the recently released ones.This approach allows researchers to compare the performance of prompt engineering on different LLMs, thereby driving innovation in the field.Creating diverse datasets with a wide range of tasks and molecules will enable robust evaluation and facilitate the development of more effective prompting techniques for various LLMs.</p>
<p>In summary, domain-knowledge embedded prompt engineering has shown great promise for unlocking the potential of LLMs in scientific domains.By integrating domain expertise into prompts, LLMs can generate more accurate and contextually relevant responses.As prompt engineering techniques continue to evolve, LLMs have the potential to become powerful allies for scientists, assisting in the exploration and discovery of new materials, molecules, and biological entities.</p>
<p>S.2 Dataset Construction Details</p>
<p>We collect and curate a comprehensive dataset of 1280 questions and corresponding solutions for evaluation of LLM's performances.Our focus is centered on three distinct classes of materials that are prevalently utilized in diverse chemical engineering domains: organic small molecules, enzymes, and crystal materials.These prediction tasks in the three major fields are summarized in Table 2.</p>
<p>The selection of these three types of materials takes into account both their spatial scale distribution and their significance in scientific research.Organic small molecules, enzymes, and crystalline materials represent the three distinct scales of micro-molecules, macro-molecules, and macroscopic materials, respectively.This diversity of scales ensures that our study encapsulates the complexities prevalent in materials science.</p>
<p>Furthermore, each of these material categories holds significant relevance in academic research and practical applications.Organic small molecules are commonly utilized in pharmaceutical development and as molecular probes [44].Enzymes play a critical role in biocatalysis and therapeutic interventions [45][46], while crystalline materials are essential in semiconductor technology and photovoltaic devices [47][48].</p>
<p>We have devised a series of prediction tasks for three types of materials covering a wide range of properties and structural information.</p>
<p>• Organic small molecules.Derived from PubChem [52], this material class contains 40 organic small molecules proven to have significant drug properties or potentials.We focus on elucidating the structural and physical-chemical properties that are crucial in determining the functionality and applicability of Organic small molecules, such as Molecular Formula, • Enzymes.We curate 40 proteins involved in significant metabolic pathways in vivo from the UniProt database [53], a high-quality and freely accessible database of protein sequence and functional information from the scientific literature and computational analysis.We tested enzymes' efficiency, stability, and suitability for industrial and therapeutic applications.The key performance to be examined include, but are not limited to: Category, Number of Amino Acids, Substrate, Product, Active Site, Biological Process and Ligand.</p>
<p>• Crystal Materials.We curate 40 representative crystals from the Materials Project database [54], an open-access database providing information on material properties and crystal structures powered by high-throughout computational techniques.We are primarily concerned with crystal materials' structural and energy properties, which are fundamental in determining their functionality and efficiency in practical applications, such as: Space Group Number, Lattice Angle, Lattice Vector, Density, Formation Energy, Energy Above Hull, Stability, Band Gap, Direct Gap, Metallic, Total Magnetization, Ordering.</p>
<p>We categorize these 32 sorts of tasks (16 in crystal materials, 9 in small molecules, and 7 in enzymes) into groups based on output type, reasoning paradigm, and Chain-of-Thought (CoT) complexity denoted with the quantity of additional properties involved in CoT prompts.• Numerical Answer by Experimental Data: Tasks that rely on external experimental data for answers which take the form of numerical values.</p>
<p>• Verbal Answer by Logic: Tasks where answers are descriptive and are derived from logical reasoning.</p>
<p>Groups Based on Reasoning Paradigm</p>
<p>• Arithmetic Tasks: Calculation-based tasks.</p>
<p>• Spatial Relationship Tasks: Tasks involving understanding of spatial arrangements or geometry.• Domain Knowledge Literal Reasoning Tasks: Tasks requiring specific domain knowledge for literal reasoning.• Common Property Retrieval Tasks: Tasks involving retrieval of common properties.</p>
<p>• Uncommon Property Retrieval Tasks: Tasks involving retrieval of less common or obscure information.</p>
<p>Groups Based on CoT Complexity</p>
<p>• Tasks without Additional Properties: No additional property involved in CoT prompts.</p>
<p>• Tasks with One Additional Property: One additional property involved in CoT prompts.</p>
<p>• Tasks with Multiple Additional Properties: More than one additional properties involved in CoT prompts.</p>
<p>S.3 Answer Evaluation Metrics and Remarks</p>
<p>The detailed formulation of 4 metrics used in answer evaluation are listed as below:</p>
<p>1.Capability For some tasks that are complicated or heavily dependent on accessing databases, it's possible that LLM would fail to even complete an answer but instead only announcing its lack of capability in completing the specific tasks.We propose "Capability" index to measure LLM's ability to provide an answer, regardless of correctness, which we believe will be a critical indicator to evaluate performance of different prompt engineering methods.</p>
<p>To calculate capability, the following formula is applied:
C = Σ n i=1 I {successfully answered} n(4)
where I {successfully answered} is a characteristic function representing whether each question is successfully answered, and n is the total number of questions.</p>
<p>2.Accuracy</p>
<p>The "Accuracy" metric measures the average correctness of answers across different tasks.For each question, the final score ranges from 0 to 1, where 0 indicates a failure to provide an answer or an answer that is completely off the mark, and 1 signifies a precise, fully correct answer.</p>
<p>Accuracy is calculated using the formula:
A = Σ n i=1 A i n(5)
where A i is the accuracy score for each question, and n is the total number of questions.</p>
<p>3.F1 Score</p>
<p>Specifically for classification and multiple choices tasks, F1 score -a statistical measure that combines precision and recall, is used to provide a balanced view of the LLM's performance [64].</p>
<p>As F1 Score is typically used to evaluate binary classification performance:
precision = TP TP + FP (6) recall = TP TP + FN (7) F1 = 2 × precision × recall precision + recall(8)
for multiple choices questions, F1 score for each choice is evaluated respectively and assembled afterwards:
precision(choice = A) = TP(choice = A) TP(choice = A) + FP(choice = A)(9)
recall(choice = A) = TP(choice = A) TP(choice = A) + FN(choice = A) (10)
F1(choice = A) = 2 × precision(choice = A) × recall(choice = A) precision(choice = A) + recall(choice = A)(11)F1 = 1 5 Σ choice=A,B,C,D,E F1(choice)(12)</p>
<p>4.Hallucination Drop</p>
<p>The concept of "Hallucination Drop" is introduced to quantify the discrepancy between an LLM's ability to answer questions (Capability) and the accuracy of those answers (Accuracy).It serves as an indicator of the LLM's tendency to provide unrealistic or incorrect responses, a phenomenon commonly referred to as "hallucination."</p>
<p>Hallucination Drop H for a specific type of task or material is calculated using the formula:
H = 1 − A C (13)
The policy of how numerical tasks transform to multiple-choices questions are listed as follows:</p>
<p>• "Molecular Weight (unit: g/mol)": [2, 10, "(unit: g/mol)"],</p>
<p>• "Number of H-bond Acceptors": [1, 2, ""],</p>
<p>• "Number of H-bond Donors": [1, 2, ""],</p>
<p>• "a in Lattice Vector [a, b, c] (unit: Å)": [2, 1, "(unit: Å)"],</p>
<p>• "b in Lattice Vector [a, b, c] (unit: Å)": [2, 1, "(unit: Å)"],</p>
<p>• "c in Lattice Vector [a, b, c] (unit: Å)": [2, 1, "(unit: Å)"],</p>
<p>• "Lattice Angle α (among 3 angles as [α, β, γ])": [0, ["α&lt;90", "90&lt;=α&lt;100", "100&lt;=α&lt;110", "110&lt;=α&lt;120", "α&gt;120"], ""],</p>
<p>• "Lattice Angle β (among 3 angles as [α, β, γ])": [0, ["β&lt;90", "90&lt;=β&lt;100", "100&lt;=β&lt;110", "110&lt;=β&lt;120", "β&gt;120"], ""],</p>
<p>• "Lattice Angle γ (among 3 angles as [α, β, γ])": [0, ["γ&lt;90", "90&lt;=γ&lt;100", "100&lt;=γ&lt;110", "110&lt;=γ&lt;120", "γ&gt;120"], ""],</p>
<p>• "Space Group Number": [1, 15, ""],</p>
<p>• "Number of Amino Acids": [2, 50, ""],</p>
<p>• "Melting Point (unit: °C)": [2, 20, "(unit: °C)"],</p>
<p>• "Density (unit: g/cm3)": [2, 0.1, "(unit: g/cm3)"],</p>
<p>• "Solubility (in water, unit: mg/L)": [0, ["&lt;1", "1 10", "10 100", "100 1000", "&gt;1000"], "(in water, unit: mg/L)"],</p>
<p>• "LogP": [2, 0.5, ""],</p>
<p>• "Crystal Density (unit: g/cm3)": [2, 0.5, "(unit: g/cm3)"],</p>
<p>• "Formation Energy (unit: eV/atom)": [2, 0.5, "(unit: eV/atom)"],</p>
<p>• "Energy Above Hull (unit: eV/atom)": [2, 0.05, "(unit: eV/atom)"],</p>
<p>• "Band Gap (unit: eV)": [2, 0.5, "(unit: eV)"],</p>
<p>• "Total Magnetization (unit: µB/f.u.)": [2, 1, "(unit: µB/f.u.)"]</p>
<p>Notice that for each key-value pair, the 3-tuple contains complete policy for multiple-choices designs.</p>
<p>The first element indicate choices type: fixed (0), single-valued (1) or interval-valued (2).The second element indicate the step size between each choices.And the last element represent the unit (if exists).</p>
<p>For tuples with first element being 0, the multiple choices would automatically be the second element in the tuple, while for the rest of the tuples, the multiple-choices generating program randomly choose 4 other values or intervals around ground truth based on the step size provided, generating 5 choices in total.It's guaranteed that adjacent choices will have adjacent values or ranges corresponding to them.</p>
<p>The specific Accuracy evaluation policy for numerical tasks is that full mark ( 1) is given if the option is exactly the ground truth and a partial score (0.4) is given if the value or range of the chosen option is adjacent to the ground truth.The detailed rules are listed in Table 6:</p>
<p>Table 6: Accuracy Grading Policy for Numerical Tasks
Answer Ground Truth A B C D E A 1 0.4 0 0 0 B 0.4 1 0.4 0 0 C 0 0.4 1 0.4 0 D 0 0 0.4 1 0.4 E 0 0 0 0.41
The scoring policy considering different choices' adjacency is reasonable because adjacent choices also have adjacent values or ranges, as guaranteed by the policy of numerical tasks transforming to multiple-choices questions introduced prior to this.</p>
<p>S.4 Detailed Prompt Designs for Performance Tasks and Answer Evaluation</p>
<p>For all scientific prediction tasks, the corresponding domain-knowledge embedded Chain-of-Thought (CoT) prompts are listed in below:</p>
<p>Organic Small Molecule</p>
<p>• "Molecular Weight (unit: g/mol)": "Question: For small molecule, given the Smiles:
CC(=O)Nc1ccc(O)cc1
, what is the Molecular Weight (unit: g/mol)?Answer: 1) First, decode the Smiles notation to identify the number of each type of atom.In the notation, "C" represents carbon, "O" represents oxygen, "N" represents nitrogen, and "H" (though not explicitly mentioned) represents hydrogen.From the Smiles, we can count: 8 carbon atoms (C), 1 nitrogen atom (N), and 2 oxygen atoms (O).The hydrogens are implicit in the Smiles notation, but based on standard valences, we can deduce there are 9 hydrogens (H).In this notation, the symbols represent atoms and their connectivity.Looking at the Smiles, we can identify the potential H-bond acceptors.We see an oxygen atom in the carbonyl group (=O) and another oxygen atom in the phenol group (O).Both of these oxygen atoms can act as H-bond acceptors.3) Additionally, there's a nitrogen atom (N) connected to a carbon, which can also act as an H-bond acceptor.4) Therefore, based on the Smiles notation, the molecule has 3 H-bond acceptors: two from the oxygen atoms and one from the nitrogen atom.",</p>
<p>• "Number of H-bond Donors": "Question: For small molecule, given the Molecular Formula: C8H9NO2 and Smiles: CC(=O)Nc1ccc(O)cc1, what is the Number of H-bond Donors?Answer: 1) H-bond donors are typically hydrogen atoms that are attached to electronegative atoms like nitrogen (N) or oxygen (O) and can donate a hydrogen bond.</p>
<p>2) Analyzing the given Smiles notation, CC(=O)Nc1ccc(O)cc1, we need to identify such hydrogens.The carbonyl group (=O) does not have a hydrogen attached to the oxygen, so it's not an H-bond donor.However, the phenol group (O) has a hydrogen attached to the oxygen, making it an H-bond donor.</p>
<p>3) The nitrogen (N) in the molecule, being connected to a carbon, likely has a hydrogen attached to it (as the molecular formula suggests there's one more hydrogen than can be accounted for by the carbons and the phenolic oxygen), making it another H-bond donor.4) Therefore, the molecule has 2 H-bond donors: one from the phenolic oxygen and one from the nitrogen.",• "Molecular Formula": "Question: For small molecule, given the Smiles:
CC(=O)Nc1ccc(O)cc1
, what is the Molecular Formula?Answer: 1) Start by decoding the Smiles notation to identify and count each type of atom.In the notation, "C" represents carbon, "O" represents oxygen, "N" represents nitrogen, and "H" (though not explicitly mentioned) represents hydrogen.2) From the Smiles, we can count: 8 carbon atoms (C), 1 nitrogen atom (N), and 2 oxygen atoms (O).The hydrogens are implicit in the Smiles notation, but based on standard valences and the structure, we can deduce there are 9 hydrogens (H). 3) Now, assemble these counts into a molecular formula: C8H9NO2.Thus, the molecular formula of the molecule is C8H9NO2.",• "Melting Point (unit: °C)": "Question: For small molecule, given the Name: Acetaminophen and Smiles: CC(=O)Nc1ccc(O)cc1, what is the Melting Point (unit: °C)?Answer: 1) Recognize that Acetaminophen is a common over-the-counter pain reliever and fever reducer, also known as Paracetamol in some regions.The Smiles notation provided confirms the structure of Acetaminophen.2) Now, to find the melting point, one would typically refer to a trusted chemical database or literature source.3) Based on existing knowledge up to the last update in January 2022, Acetaminophen has a melting point of approximately 169°C to 170.5°C.4) Answer: The melting point of Acetaminophen is approximately 169°C to 170.5°C.",• "Density (unit: g/cm3)": "Question: For small molecule, given the Name: Acetaminophen and Smiles: CC(=O)Nc1ccc(O)cc1, what is the Density (unit: g/cm3)?Answer: 1) Recognize that Isopropanol, also known as isopropyl alcohol or rubbing alcohol, is a common solvent used in laboratories and households.The Smiles notation CC(C)O confirms its molecular structure.2) Density is a fundamental physical property that is often documented in various chemical databases and literature because of its extensive use in various applications, including as a solvent and disinfectant.3) Based on existing knowledge up to the last update in January 2022, the density of Isopropanol at room temperature (25°C) is approximately 0.785 g/cm3.4) Answer: The density of Isopropanol is approximately 0.785 g/cm3.",• "Solubility (in water, unit: mg/L)": "Question: For small molecule, given the Name:</p>
<p>Acetaminophen and Smiles: CC(=O)Nc1ccc(O)cc1, what is the Solubility (in water, unit: mg/L) (A:&lt;1mg/L, B:1 10mg/L, C:10 100mg/L, D:100 1000mg/L, E&gt;1000mg/L)?Answer: 1) First, we need to analyze the SMILES notation, which provides a linear representation of the chemical's molecular structure.By examining the SMILES notation, we can identify key structural features that influence solubility, such as the presence of polar groups, the size and shape of the molecule, and the degree of saturation.Polar groups like hydroxyl (-OH) and amine (-NH2) tend to increase solubility in water, while large hydrophobic regions or high molecular weight often decrease solubility.2) Next, we consider the chemical's name, which can give additional clues about its structure and functional groups.For instance, the suffix '-ol' in alcohols indicates the presence of a hydroxyl group, which usually enhances solubility.Then, we correlate these structural features with known solubility trends and principles from chemistry, such as 'like dissolves like' and the impact of hydrogen bonding.By integrating this information, we can estimate the solubility range.3) For example, a small molecule with several polar groups might fall into category C,D or E, whereas a large, non-polar hydrocarbon is likely in category A or B. 4) Answer: Acetaminophen have many strong polar groups like (=O) and (N), so the solubility of Acetaminophen in water at 25°C is &gt;1000mg/L, so the answer is E.",</p>
<p>• "LogP": "Question: For small molecule, given the Name: Acetaminophen and Smiles: CC(=O)Nc1ccc(O)cc1, what is the LogP?Answer: 1) Recognize that Acetaminophen, also known as Paracetamol, is a well-known drug.The Smiles notation confirms its structure.2) LogP, or the partition coefficient between octanol and water, is a measure of a compound's lipophilicity and plays a crucial role in drug design because it can influence a drug's absorption, distribution, metabolism, and excretion.A higher LogP indicates greater lipophilicity.3) To find the LogP, one would typically consult a trusted chemical database, use computational tools, or refer to specific literature sources that provide this physicochemical property.• "Lattice Angle α (among 3 angles as [α, β, γ])": "Question: For crystal material, given the MP-id: mp-248 and Formula: Fe2N, what is the Lattice Angle α (among 3 angles as [α, β, γ])?Answer: 1)<strong>Question Analysis</strong> Determining the lattice angles for Fe2N involves a theoretical analysis of its bonding, electronic structure, and likely packing style, followed by an educated guess of its crystal system and corresponding lattice angles.</p>
<p>2)<strong>Chemical Formula Analysis</strong> Fe (Iron) is a transition metal known for its metallic bonding characteristics, while N (Nitrogen) typically forms covalent or ionic bonds.In Fe2N, we expect a mix of metallic and covalent bonding.3)<strong>Hypothesizing Crystal System</strong> In compounds like Fe2N, the metallic nature of iron suggests a dense packing arrangement, so given the metallic characteristics and potential covalent interactions, a reasonable guess for the crystal system might be hexagonal, trigonal or tetragonal, as these systems are common for compounds with mixed bonding types.Let's hypothesize that Fe2N crystallizes in a trigonal system.4)<strong>Determining Lattice Angles</strong> In a trigonal crystal system, the lattice angles are typically α = 90°, β = 90°, and γ = 120°.This is because the hexagonal system is characterized by two equal axes at right angles to each other, and a third axis (of a different length) at a 120°angle to one of the other axes.involves a theoretical analysis of its bonding, electronic structure, and likely packing style, followed by an educated guess of its crystal system and corresponding lattice angles.</p>
<p>2)<strong>Chemical Formula Analysis</strong> Fe (Iron) is a transition metal known for its metallic bonding characteristics, while N (Nitrogen) typically forms covalent or ionic bonds.In Fe2N, we expect a mix of metallic and covalent bonding.3)<strong>Hypothesizing Crystal System</strong> In compounds like Fe2N, the metallic nature of iron suggests a dense packing arrangement, so given the metallic characteristics and potential covalent interactions, a reasonable guess for the crystal system might be hexagonal, trigonal or tetragonal, as these systems are common for compounds with mixed bonding types.Let's hypothesize that Fe2N crystallizes in a trigonal system.4)<strong>Determining Lattice Angles</strong> In a trigonal crystal system, the lattice angles are typically α = 90°, β = 90°, and γ = 120°.This is because the hexagonal system is characterized by two equal axes at right angles to each other, and a third axis (of a different length) at a 120°angle to one of the other axes.• "Lattice Angle γ (among 3 angles as [α, β, γ])": "Question: For crystal material, given the MP-id: mp-248 and Formula: Fe2N, what is the Lattice Angle γ (among 3 angles as [α, β, γ])?Answer: 1)<strong>Question Analysis</strong> Determining the lattice angles for Fe2N involves a theoretical analysis of its bonding, electronic structure, and likely packing style, followed by an educated guess of its crystal system and corresponding lattice angles.</p>
<p>2)<strong>Chemical Formula Analysis</strong> Fe (Iron) is a transition metal known for its metallic bonding characteristics, while N (Nitrogen) typically forms covalent or ionic bonds.In Fe2N, we expect a mix of metallic and covalent bonding.3)<strong>Hypothesizing Crystal System</strong> In compounds like Fe2N, the metallic nature of iron suggests a dense packing arrangement, so</p>
<p>given the metallic characteristics and potential covalent interactions, a reasonable guess for the crystal system might be hexagonal, trigonal or tetragonal, as these systems are common for compounds with mixed bonding types.Let's hypothesize that Fe2N crystallizes in a trigonal system.4)<strong>Determining Lattice Angles</strong> In a trigonal crystal system, the lattice angles are typically α = 90°, β = 90°, and γ = 120°.This is because the hexagonal system is characterized by two equal axes at right angles to each other, and a third axis (of a different length) at a 120°angle to one of the other axes.• "Crystal Density (unit: g/cm3)": "Question: For crystal material, given the Formula and Lattice vector, what is the Density (unit: g/cm3)?Answer: 1)<strong>Determine the formula weight</strong> alculate the formula weight (molar mass) of the crystal material by summing the atomic weights of all the atoms in its formula unit.2)<strong>Calculate the Volume of the Unit Cell</strong> Use the lattice vectors to determine the volume of the unit cell.For a unit cell with lattice vectors a, b, and c, the volume V is given by V = abc.3)<strong>Calculate the Number of Formula Units per Unit Cell</strong> Determine how many formula units are in each unit cell.4)<strong>Calculate Density</strong> The density ρ can be calculated using the formula: ρ = Formula Weight<em>Number of Formula Units per Unit Cell / (V</em>N_A), where N_A is Avogadro's number (approximately 6.02<em>10 23 mol (-1).Ensure that the units are consistent.For instance, if the formula weight is in grams per mole and the volume in cubic centimeters, the density will be in grams per cubic centimeter.5)</em><em>Example</em><em> Suppose you have a crystal with a simple cubic structure (which has one formula unit per unit cell), a lattice parameter a = 5 Å, and it's composed of a hypothetical material XYZ with a formula weight of 150 g/mol.The volume of the unit cell is V=a 3=(0.5</em>10(-9)m) 3. The density will be calculated as ρ = 150g/mol<em>1/(V</em>6.02*1023mol (-1))=1.99g/cm3.",</p>
<p>• "Formation Energy (unit: eV/atom)": "Question: For crystal material, given the MP-id: mp-248 and Formula: Fe2N, what is the Formation Energy (unit: eV/atom)?Answer: 1)<strong>Formation Energy Analysis</strong> In materials science, Formation Energy is a measure of the energy change when a material is formed from its constituent elements in their standard states.It's a fundamental property that indicates the stability of the material; a more negative formation energy typically suggests a more stable compound.• "Band Gap (unit: eV)": "Question: For crystal material, given the MP-id mp-248 and Formula Fe2N, what is the Band Gap (unit: eV)?Answer: 1)<strong>Band Gap Analysis</strong> In materials science, the band gap is a fundamental property that describes the energy difference between the top of the valence band and the bottom of the conduction band in a material.It determines the material's electrical conductivity, with a larger band gap typically indicating a better insulator.2)<strong>Determining Formation Energy</strong> Band Gap for a good conductor is usually zero or negligible while for insulator it's usually high for about 0. • "Metallic (Yes or No)": "Question: For crystal material, given the MP-id: mp-248, Formula: Fe2N and Band Gap: 0.00 eV, what is the Metallic nature (Yes or No) ?Answer: 1)<strong>Band Gap Analysis</strong> A material with a very small band gap (close to 0 eV) or no band gap is typically metallic, meaning it conducts electricity well.Metals have overlapping valence and conduction bands or a partially filled conduction band, which allows for free movement of electrons, leading to high electrical conductivity.Conversely, a material with a significant band gap is generally a semiconductor or an insulator, not a metal.The larger the band gap, the lesser the electrical conductivity, with insulators having the largest band gaps.</p>
<p>2)<strong>Determining Metallic nature</strong> If a material is known to be metallic (Metallic: Yes), you can infer that its band gap is effectively zero or negligible.If a material has a significant band gap (say, &gt;2 eV), you can infer it's not metallic (Metallic: No).Because the band gap of Fe2N is 0.00eV, it's know to be metallic.• "Total Magnetization (unit: µB/f.u.)": "Question: For crystal material, given the MPid mp-248 and Formula Fe2N, what is the Total Magnetization (unit: µB/f.u.)?Answer: 1)<strong>Total Magnetization Analysis</strong> In materials science, Total Magnetization refers to the magnetic moment per formula unit (f.u.), often measured in Bohr magnetons (µB).This property is crucial for understanding the magnetic properties of a material, which can be influenced by factors like the arrangement of atoms and the electronic structure.2)<strong>Determining Total Magnetization</strong> Total Magnetization for Ferromagnetic or Ferrimagnetic crystal is usually around 3 µB/f.u.while for Antiferromagnetic or Non-magnetic crystal it's usually 0. It's very encouraged to give a hypothesis between that range.Here, based on existing knowledge up to the last update in January 2022, the Total Magnetization for this Ferromagnetic Fe2N is 3.00 µB/f.u.. 3)<strong>Conclusion</strong> So the Total Magnetization (unit: µB/f.u.) = 3.00.",</p>
<p>• "Ordering (Ferromagnetic/Ferrimagnetic/Antiferromagnetic/Non-magnetic)": "Question: For crystal material, given the MP-id mp-248, Formula Fe2N and Total Magnetization: 3.00 µB/f.u,what is the Ordering (Ferromagnetic/Ferrimagnetic/Antiferromagnetic/Nonmagnetic)?Answer: 1)<strong>Total Magnetization Analysis</strong> This refers to the net magnetic moment of a material, expressed in units of Bohr magnetons per formula unit.It represents the sum of all magnetic moments in the material, taking into account both the direction and magnitude of each magnetic moment.2)<strong>Magnetic Ordering Analysis</strong> In ferromagnetic materials, the magnetic moments of the atoms align parallel to each other, resulting in a strong net magnetic moment.A material with significant total magnetization is likely ferromagnetic.Ferrimagnetic materials have magnetic moments that are aligned in opposite directions but are unequal in magnitude, resulting in a net magnetic moment.Therefore, a non-zero total magnetization could indicate ferrimagnetic ordering.Antiferromagnetic materials have magnetic moments that are aligned in opposite directions and are equal in magnitude, resulting in no net magnetization.A material with zero or very low total magnetization might be antiferromagnetic.Non-magnetic materials do not have a net magnetic moment, indicating zero or negligible total magnetization.3)<strong>Determining Ordering</strong> If the total magnetization is significantly high like &gt; 0.50 µB/f.u, the material is likely ferromagnetic or ferrimagnetic.If the total magnetization is zero or very low, the material could be antiferromagnetic or non-magnetic.For Fe2N, Total Magnetization: 3.00 µB/f.u is significantly high, so it's Ferromagnetic.</p>
<p>Figure 1 :
1
Figure 1: The Whole Process of Prompt Engineering Framework</p>
<p>Figure 2 :
2
Figure 2: Question Construction, Answer Alignment and Grading Process</p>
<p>Figure 3 :
3
Figure 3: Illustration of the Mainstream Prompt Engineering Methods</p>
<p>Figure 4 :
4
Figure 4: The Whole Process of Domain-Knowledge Prompt Engineering Method</p>
<p>Figure 5 :
5
Figure 5: Capability and Accuracy for All Tasks</p>
<p>Figure 6 :
6
Figure 6: F1 Score and Hallucination Drop for All Tasks</p>
<p>Figure 7 :
7
Figure 7: Prompt Engineering Performances by Output Type</p>
<p>Figure 8 :
8
Figure 8: Prompt Engineering Performances by Reasoning Paradigm</p>
<p>Figure 9 :
9
Figure 9: Prompt Engineering Performances by CoT Complexity</p>
<p>Figure 10 :
10
Figure 10: Prompt Engineering Performances on Different Materials</p>
<p>Figure 11 :
11
Figure 11: Prompt Engineering Case Study on MacMillan's Imidazolidinone 2nd Generation Catalyst</p>
<p>Figure 12 :
12
Figure 12: Prompt Engineering Case Study on Paclitaxel</p>
<p>Figure 13 :
13
Figure 13: Prompt Engineering Case Study on Lithium Cobalt Oxide</p>
<p>Figure 14 :
14
Figure 14: Question Proportion by Output Type, Reasoning Paradigm and CoT Complexity Groups Based on Output Type • Numerical Answer by Logic: Tasks where answers are deduced through pure logical reasoning, with the output taking the form of numerical values.</p>
<p>2 )
2
The name and EC always provides so much to inference.EC-Numbers -&gt; 1 -&gt; Oxidoreductases -&gt; 1.1-&gt; Acting on the CH-OH group of donors -&gt; 1.1.1-With NAD or NADP aw acceptor.So naturally its ligand must be NAD/NADP.3) Answer: So Ligand = NAD/NADP."CrystalMaterial• "a in Lattice Vector [a, b, c] (unit: Å)": "Question: For crystal material, given the MP-id mp-248, Formula Fe2N and Lattice Angle α = 90°, β = 90°, and γ = 120°, what is a in Lattice Vector [a, b, c] (unit: Å)? Answer: 1)<strong>Lattice Vector Analysis</strong> The lattice vectors a, b, and c describe the periodicity of the crystal lattice in three dimensions.The lattice angles (α, β, γ) describe the angles between these vectors.In a crystal system, these parameters define the unit cell's shape and size.2)<strong>Determining Lattice Vector</strong> The scale of lattice vector is usually around 3 10 Å.It's very encouraged to give a hypothesis between that range.Here in compound Fe2N, according to existing knowledge up to the last update in January 2022, the Lattice Vector is a:4.75 Å,b:4.75Å,c:4.32 Å, so a in Lattice Vector [a, b, c] (unit: Å) is 4.75.3)<strong>Conclusion</strong> so a in Lattice Vector [a, b, c] (unit: Å) = 4.75.",• "b in Lattice Vector [a, b, c] (unit: Å)": "Question: For crystal material, given the MP-id mp-248, Formula Fe2N and Lattice Angle α = 90°, β = 90°, and γ = 120°, what is b in Lattice Vector [a, b, c] (unit: Å)? Answer: 1)<strong>Lattice Vector Analysis</strong> The lattice vectors a, b, and c describe the periodicity of the crystal lattice in three dimensions.The lattice angles (α, β, γ) describe the angles between these vectors.In a crystal system, these parameters define the unit cell's shape and size.2)<strong>Determining Lattice Vector</strong> The scale of lattice vector is usually around 3 10 Å.It's very encouraged to give a hypothesis between that range.Here in compound Fe2N, according to existing knowledge up to the last update in January 2022, the Lattice Vector is a:4.75 Å,b:4.75Å,c:4.32 Å, so b in Lattice Vector [a, b, c] (unit: Å) is 4.75.3)<strong>Conclusion</strong> so b in Lattice Vector [a, b, c] (unit: Å) = 4.75.",• "c in Lattice Vector [a, b, c] (unit: Å)": "Question: For crystal material, given the MP-id mp-248, Formula Fe2N and Lattice Angle α = 90°, β = 90°, and γ = 120°, what is c in Lattice Vector [a, b, c] (unit: Å)? Answer: 1)<strong>Lattice Vector Analysis</strong> The lattice vectors a, b, and c describe the periodicity of the crystal lattice in three dimensions.The lattice angles (α, β, γ) describe the angles between these vectors.In a crystal system, these parameters define the unit cell's shape and size.2)<strong>Determining Lattice Vector</strong> The scale of lattice vector is usually around 3 10 Å.It's very encouraged to give a hypothesis between that range.Here in compound Fe2N, according to existing knowledge up to the last update in January 2022, the Lattice Vector is a:4.75 Å,b:4.75Å,c:4.32 Å, so c in Lattice Vector [a, b, c] (unit: Å) is 4.32.3)<strong>Conclusion</strong> so c in Lattice Vector [a, b, c] (unit: Å) = 4.32.",</p>
<p>5)<strong>Conclusion</strong> So the Lattice Angle α (among 3 angles as [α, β, γ]) = 90°.",• "Lattice Angle β (among 3 angles as [α, β, γ])": "Question: For crystal material, given the MP-id: mp-248 and Formula: Fe2N, what is the Lattice Angle β (among 3 angles as [α, β, γ])?Answer: 1)<strong>Question Analysis</strong> Determining the lattice angles for Fe2N</p>
<p>5)<strong>Conclusion</strong> So the Lattice Angle β (among 3 angles as [α, β, γ]) = 90°.",</p>
<p>5 2.5 eV.It's very encouraged to give a hypothesis between that range.Here, based on existing knowledge up to the last update in January 2022, the Band Gap for this metallic conductor Fe2N is 0.00 eV.3)<strong>Conclusion</strong> So the Band Gap (unit: eV) = 0.", • "Direct Gap (Yes or No)": "Question: For crystal material, given the MP-id mp-248 and Formula Fe2N, what is the Direct Gap (Yes or No)? Answer: 1)<strong>Direct Gap Analysis</strong> A material is said to have a direct band gap if the minimum of the conduction band and the maximum of the valence band occur at the same momentum (k-point) in the Brillouin zone.This contrasts with an indirect band gap, where these points occur at different momenta.2)<strong>Determining Direct Gap</strong> Fe2N is an iron nitride compound.The electronic properties, including the nature of the band gap, of such compounds are determined by their electronic structure, which in turn is influenced by factors like the arrangement of atoms and the nature of chemical bonding.Here, based on existing knowledge up to the last update in January 2022, the Direct Gap for Fe2N is No. 3)<strong>Conclusion</strong> So the Direct Gap (Yes or No) = No.",</p>
<p>3)<strong>Conclusion</strong> So the Metallic (Yes or No) = Yes.",</p>
<p>Figure 15 :Figure 16 :
1516
Figure 15: Detailed Prompt Engineering Interactive Dialogue on MacMillan's Imidazolidinone 2nd Generation Catalyst</p>
<p>Figure 17 :
17
Figure 17: Detailed Prompt Engineering Interactive Dialogue on Lithium Cobalt Oxide</p>
<p>Table 1 :
1
Prompt Engineering Prediction Tasks
DatasetsTasksNumber ofNumber ofMoleculesTasksCrystal Mate-Space Group Number, Lattice Angle (α,β,γ), Lat-40640rialtice Vector (a,b,c), Density, Formation Energy, En-ergy Above Hull, Stability, Band Gap, Direct Gap,Metallic, Total Magnetization, OrderingOrganic SmallMolecular Formula, Melting Point, Density, Sol-40360Moleculeubility, Molecular Weight, H-bond Acceptors, H-bond Donors, LogP, DrugabilityEnzymeCategory, Substrate, Product, Active Site, Biologi-40280cal Process, Number of Amino Acids, LigandTotal321201280</p>
<p>Table 2 :
2
Prompt Engineering Prediction Tasks
DatasetsTasksNumber ofNumber ofMoleculesTasksCrystal Mate-Space Group Number, Lattice Angle (α,β,γ), Lat-40640rialtice Vector (a,b,c), Density, Formation Energy, En-ergy Above Hull, Stability, Band Gap, Direct Gap,Metallic, Total Magnetization, OrderingOrganic SmallMolecular Formula, Melting Point, Density, Sol-40360Moleculeubility, Molecular Weight, H-bond Acceptors, H-bond Donors, LogP, DrugabilityEnzymeCategory, Substrate, Product, Active Site, Biologi-40280cal Process, Number of Amino Acids, LigandTotal321201280Melting Point, Density, Solubility in Water, Molecular Weight, H-bond donor andacceptor, LogP, and Drugability.</p>
<p>Table 3 :
3
Question Classification Based on Output Type
GroupTasksTotalNumerical Answer byMolecular Weight, Number of H-bond Acceptors,120LogicNumber of H-bond DonorsNumerical Answer by Ex-Lattice Angle, Lattice Vector, Space Group Num-680perimental Databer, Number of Amino Acids, Melting Point, Den-sity, Solubility, LogP, Crystal Density, FormationEnergy, Energy Above Hull, Band Gap, Total Mag-netizationVerbal Answer by LogicCategory, Substrate, Product, Biological Process,200Molecular FormulaVerbal Answer by Experi-Active Site, Ligand, Drugability, Stability, Direct280mental DataGap, Metallic, OrderingTable 4: Question Classification Based on Reasoning ParadigmGroupTasksTotalArithmetic TasksMolecular Weight, Molecular Formula80Spatial Relationship Tasks Number of H-bond Acceptors, Number of H-bond200Donors, Lattice AngleDomain Knowledge LiteralCategory, Substrate, Product, Biological Process,240Reasoning TasksStability, DrugabilityCommon Property Re-Space Group Number, Number of Amino Acids,280trieval TasksLigand, Ordering, Melting Point, Density, Solubil-ityUncommon Property Re-Lattice Vector, Active Site, Crystal Density, Forma-480trieval Taskstion Energy, Energy Above Hull, Band Gap, TotalMagnetization, LogP, Direct Gap, Metallic• Verbal Answer by Experimental Data: Tasks where answers are descriptive and based onexternal experimental data.</p>
<p>Table 5 :
5
Question Classification Based on CoT complexity
GroupTasksTotalTasks without AdditionalLattice Angle, Space Group Number, Number of800PropertiesAmino Acids, Melting Point, Density, Solubil-ity, LogP, Formation Energy, Energy Above Hull,Band Gap, Total Magnetization, Category, Sub-strate, Product, Molecular Formula, Active Site,Ligand, Direct GapTasks with One AdditionalMolecular Weight, Number of H-bond Acceptors,360PropertyNumber of H-bond Donors, Stability, Lattice Vec-tor, Metallic, OrderingTasks with Multiple Addi-Drugability, Crystal Density, Biological Process120tional Properties</p>
<p>4) Based on existing knowledge up to the last update in January 2022, Acetaminophen has a LogP value of approximately 0.46.5)Answer:TheLogP of Acetaminophen is approximately 0.46.",•"Drugability(Yes or No)": "Question: For small molecule, given the Molecular Weight (unit: g/mol): 151.162 g/mol, Number of H-bond Acceptors: 3, Number of H-bond Donors: 2, Solubility (in water, unit: mg/L): 14,000 mg/L and LogP: 0.46, what is the Drugability (Yes or No)? Answer: 1) Drugability is a complex concept that evaluates the likelihood of a molecule to become an orally active drug in humans.2)TheLipinski'sRule of Five is a commonly used set of guidelines to predict drugability.According to this rule, a molecule is likely to be drugable if it meets the following criteria: Molecular Weight is less than 500 g/mol, Number of H-bond Acceptors is no more than 10, Number of H-bond Donors is no more than 5, LogP is less than 5, and the molecule is not excessively insoluble.3)Comparingthegivenpropertiestothesecriteria: The Molecular Weight of 151.162 g/mol is well below 500 g/mol, the Number of H-bond Acceptors is 3, which is below 10, the Number of H-bond Donors is 2, which is below 5, the LogP of 0.46 is well below 5, and a Solubility of 14,000 mg/L indicates the molecule is not excessively insoluble.4)Giventhatthemoleculemeetsallthecriteria, it is likely to be drugable.So answer is Yes."Glucose-6P dehydrogenase is an enzyme that catalyzes the oxidation of glucose-6-phosphate (first number in EC=1, so it's an oxidoreductase), primarily in the pentose phosphate pathway.2)ThenameandECalwaysprovides so much to inference.EC-Numbers -&gt; 1 -&gt; Oxidoreductases -&gt; 1.1-&gt; Acting on the CH-OH group of donors -&gt; 1.1.1-WithNADorNADPaw acceptor.So naturally its product is the dehydrogenase product of D-Glucose-6P on CH-OH group of donors, which is 6-phospho-D-glucono-1,5lactone.3)Answer:SoProduct= 6-phospho-D-glucono-1,5-lactone.",•"ActiveSite":"Question:For enzyme, given the Enzyme: D-Glucose-6P dehydrogenase and EC (Enzyme Commission number): 1.1.1.363,whatis the Active Site?Answer: 1) <strong>Context Analysis</strong> The EC number is a numerical classification scheme for enzymes, based on the chemical reactions they catalyze.D-Glucose-6P dehydrogenase is an enzyme that catalyzes the oxidation of glucose-6-phosphate, primarily in the pentose phosphate pathway.2) <strong>Specify Information</strong> Information about the active site of an enzyme is typically obtained from its three-dimensional structure.Once the structure is available, the active site can be identified based on the location where substrates are known to bind.This often involves looking at the enzyme in complex with its substrate or an inhibitor.For D-Glucose-6P dehydrogenase, according to the the last update in January 2022, the active site is H-267.3)Answer: So Active Site = H-267.",•"Biological Process": "Question: For enzyme, given the Enzyme: D-Glucose-6P dehydrogenase and EC (Enzyme Commission number): 1.1.1.363,what is the Biological Process?Answer: 1) <strong>Context Analysis</strong> D-Glucose-6P dehydrogenase is an enzyme that catalyzes the oxidation of glucose-6-phosphate (first number in EC=1, so it's an oxidoreductase), primarily in the pentose phosphate pathway.2) The name and EC always provides so much to inference.EC-Numbers -&gt; 1 -&gt; Oxidoreductases -&gt; 1.1-&gt; Acting on the CH-OH group of donors -&gt; 1.1.1-With NAD or NADP aw acceptor.So naturally its substrate is D-Glucose-6P, and its product is the dehydrogenase product of D-Glucose-6P on CH-OH group of donors, which is 6-phospho-D-glucono-1,5-lactone.The dehydrogenation of these two compounds are frequently seen in Pentose phosphate pathway.3) <strong>Completeness</strong> Pentose pathway belongs to a part of Carbohydrate metabolism in a more general view.4) Answer: So Biological Process = Carbohydrate metabolism/Pentose phosphate pathway.",
Oxidore-ductases; =2, Transferases; =3, Hydrolases; =4, Lyases; =5, Isomerases; =6, Ligases; =7, Translocases. 2) For D-Glucose-6P dehydrogenase, its EC=1.1.363, the first number is 1. So it's an Oxidoreductase. 3) Answer: So Category = Oxidoreductases.", • "Number of Amino Acids": "Question: For enzyme, given the Enzyme: D-Glucose-6P dehydrogenase and EC (Enzyme Commission number): 1.1.1.363, what is the Number of Amino Acids? Answer: 1) <strong>Context Analysis</strong> The EC number is a numerical classification scheme for enzymes, based on the chemical reactions they catalyze. D-Glucose-6P dehydrogenase is an enzyme that catalyzes the oxidation of glucose-6-phosphate, primarily in the pentose phosphate pathway. 2) <strong>Specify Information</strong> The number of amino acids in an enzyme can be found by looking at its amino acid sequence, which is determined by the gene encoding the enzyme. For D-Glucose-6P dehydrogenase, according to the the last update in January 2022, the number of amino acid is 791. 3) Answer: So Number of Amino Acid = 791.", • "Substrate": "Question: For enzyme, given the Enzyme: D-Glucose-6P dehydrogenase and EC (Enzyme Commission number): 1.1.1.363, what is the Substrate? Answer: 1) <strong>Context Analysis</strong> D-Glucose-6P dehydrogenase is an enzyme that catalyzes the oxidation of glucose-6-phosphate (first number in EC=1, so it's an oxidoreductase), primarily in the pentose phosphate pathway. 2) The name and EC always provides so much to inference. EC-Numbers -&gt; 1 -&gt; Oxidoreductases -&gt; 1.1-&gt; Acting on the CH-OH group of donors -&gt; 1.1.1 -With NAD or NADP aw acceptor. So naturally its substrate is D-Glucose-6P, also known as D-glucose 6-phosphate. 3) Answer: So Substrate = D-glucose 6-phosphate.", • "Product": "Question: For enzyme, given the Enzyme: D-Glucose-6P dehydrogenase and EC (Enzyme Commission number): 1.1.1.363, what is the Product? Answer: 1) <strong>Context EC (Enzyme Commission number): 1.1.1.363, what is the Ligand? Answer: 1) </strong>Context Analysis<strong> D-Glucose-6P dehydrogenase is an enzyme that catalyzes the oxidation of glucose-6-phosphate (first number in EC=1, so it's an oxidoreductase), primarily in the Analysis</strong> D-• "Ligand": "Question: For enzyme, given the Enzyme: D-Glucose-6P dehydrogenase and pentose phosphate pathway.
Enzyme• "Category (Among Oxidoreductases, Transferases, Hydrolases, Lyases, Isomerases, Ligases, and Translocases)": "Question: For enzyme, given the Enzyme: D-Glucose-6P dehydrogenase and EC (Enzyme Commission number): 1.1.1.363,what is the Category?Answer: 1) Focus on the first number in EC.If =1, then the enzyme belongs to</p>
<p>The space group of a crystal structure describes the symmetry of the structure.It is a combination of translational and point symmetries and is a critical piece of information for understanding the crystal structure.Space groups are numbered from 1 to 230 in the International Tables for Crystallography.2)<strong>HypothesizingCrystal System</strong> In compounds like Fe2N, the metallic nature of iron suggests a dense packing arrangement, so given the metallic characteristics and potential covalent interactions, a reasonable guess for the crystal system might be hexagonal, trigonal or tetragonal, as these systems are common for compounds with mixed bonding types.Let's hypothesize that Fe2N crystallizes in a trigonal system.3)<strong>Determining Space Group Number</strong> In a trigonal crystal system, the Space Group Number is in 142-167.For Fe2N, the Space Group Number is 162.4)<strong>Conclusion</strong> So the Space Group Number = 162.",
5)<strong>Conclusion</strong> So the Lattice Angle γ(among 3 angles as [α, β, γ]) = 120°.",• "Space Group Number": "Question: For crystal material, given the MP-id mp-248 andFormula Fe2N, what is the Space Group Number? Answer: 1)<strong>Space Group NumberAnalysis</strong></p>
<p>Energy Above Hull Analysis<strong> In materials science, the "Energy Above Hull" is a metric used to assess the stability of a crystal structure.It represents how much energy per atom a given compound is above the most stable phase (the "hull") of its respective phase diagram.2)</strong>Determining Energy Above Hull<strong> Energy Above Hull for a crystal material is always around 0 1.50 eV/atom.The more likely this material is stable, the lower the Energy Above Hull is.It's very encouraged to give a hypothesis between that range.Here, based on existing knowledge up to the last update in January 2022, the Energy Above Hull of Fe2N is 0.060 eV/atom.3)</strong>Conclusion<strong> So the Energy Above Hull (unit: eV/atom) = 0.060.",• "Stability (Yes or No)": "Question: For crystal material, given the MP-id mp-248, Formula Fe2N and Energy Above Hull: 0.060 eV/atom, what is the Stability (Yes or No) ?Answer: 1)</strong>Determining Stability<strong> As long as Energy Above Hull = 0, it's stable.Otherwise it's unstable.For Fe2N, its Energy Above Hull is 0.060 eV/atom, so it's unstable.2)</strong>Conclusion<strong> So the Stability (Yes or No) = No.",
2)</strong>Determining FormationEnergy<strong> Formation Energy for a crystal material is usually under 0 eV/atom if it's stable.The absolute value is usually around -2 2 eV/atom. For pure metal crystal materials, thevalue will be 0. It's very encouraged to give a hypothesis between that range. Here, basedon existing knowledge up to the last update in January 2022, the Formation Energy of Fe2Nis -0.149 eV/atom. 3)</strong>Conclusion<strong> So the Formation Energy (unit: eV/atom) = -0.149.",• "Energy Above Hull (unit: eV/atom)": "Question: For crystal material, given the MP-id:mp-248 and Formula: Fe2N, what is the Energy Above Hull (unit: eV/atom)? Answer:1)</strong></p>
<p>Design principles for sodium superionic conductors. S Wang, J Fu, Y Liu, R S Saravanan, J Luo, S Deng, T.-K Sham, X Sun, Y Mo, Nature Communications. 14176152023</p>
<p>High-Throughput Virtual Screening of Host Materials and Rational Device Engineering for Highly Efficient Solution-Processed Organic Light-Emitting Diodes. D K Dubey, D Thakur, R A K Yadav, M Ram Nagar, T.-W Liang, S Ghosh, J.-H Jou, ACS Applied Materials &amp; Interfaces. 13222021</p>
<p>Predicting defect behavior in B2 intermetallics by merging ab initio modeling and machine learning. B Medasani, A Gamst, H Ding, W Chen, K A Persson, M Asta, A Canning, M Haranczyk, Npj Computational Materials. 2112016</p>
<p>Machine learning for design of thin-film nanocomposite membranes. M Fetanat, M Keshtiara, R Keyikoglu, A Khataee, R Daiyan, A Razmjou, Separation and Purification Technology. 2701183832021</p>
<p>Machine-based learning of predictive models in organic solvent nanofiltration: Pure and mixed solvent flux. R Goebel, M Skiborowski, Separation and Purification Technology. 2371163632020</p>
<p>Design and prediction of metal organic framework-based mixed matrix membranes for CO2 capture via machine learning. J Guan, T Huang, W Liu, F Feng, S Japip, J Li, J Wu, X Wang, S Zhang, Cell Reports Physical Science. 351008642022</p>
<p>Improved protein structure prediction using potentials from deep learning. A W Senior, R Evans, J Jumper, J Kirkpatrick, L Sifre, T Green, C Qin, A Žídek, A W R Nelson, A Bridgland, H Penedones, S Petersen, K Simonyan, S Crossan, P Kohli, D T Jones, D Silver, K Kavukcuoglu, D Hassabis, Nature. 57777922020</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, . . Hassabis, D , Nature. 59678732021</p>
<p>A Lavecchia, Machine-learning approaches in drug discovery: methods and applications. Drug discovery today. 201520</p>
<p>Applications of machine learning in decision analysis for dose management for dofetilide. A E Levy, M Biswas, R Weber, K Tarakji, M Chung, P A Noseworthy, . . Rosenberg, M A , PLoS One. 1412e02273242019</p>
<p>Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions. K T Schütt, M Gastegger, A Tkatchenko, K.-R Müller, R J Maurer, Nature Communications. 10150242019</p>
<p>Accurate Many-Body Repulsive Potentials for Density-Functional Tight Binding from Deep Tensor Neural Networks. M Stöhr, L Medrano Sandonas, A Tkatchenko, The Journal of Physical Chemistry Letters. 11162020</p>
<p>GCN-Based Structure-Activity Relationship and DFT Studies of Staphylococcus aureus FabI Inhibitors. G C Veríssimo, V S Dos Santos Junior, P O Fernandes, S Ishida, R Kojima, Y Okuno, . . Maltarollo, V G , International Journal of Quantitative Structure-Property Relationships (IJQSPR). 712022</p>
<p>Graph Neural Networks for Identifying Protein-Reactive Compounds. V H C Gil, C Rowley, 10.26434/chemrxiv-2023-d0dqp-v2ChemRxiv. 2024</p>
<p>Deep Learning of Atomically Resolved Scanning Transmission Electron Microscopy Images: Chemical Identification and Tracking Local Transformations. M Ziatdinov, O Dyck, A Maksov, X Li, X Sang, K Xiao, R R Unocic, R Vasudevan, S Jesse, S V Kalinin, ACS Nano. 11122017</p>
<p>Machine Vision Automated Chiral Molecule Detection and Classification in Molecular Imaging. J Li, M Telychko, J Yin, Y Zhu, G Li, S Song, H Yang, J Li, J Wu, J Lu, X Wang, Journal of the American Chemical Society. 143272021</p>
<p>Inverse molecular design using machine learning: Generative models for matter engineering. B Sanchez-Lengeling, A Aspuru-Guzik, Science. 36164002018</p>
<p>Inverse design of 3d molecular structures with conditional generative neural networks. N W A Gebauer, M Gastegger, S S P Hessmann, K.-R Müller, K T Schütt, Nature Communications. 1319732022</p>
<p>Guided diffusion for inverse molecular design. T Weiss, E Mayo Yanes, S Chakraborty, L Cosmo, A M Bronstein, R Gershoni-Poranne, Nature Computational Science. 3102023</p>
<p>Discovery of a structural class of antibiotics with explainable deep learning. F Wong, E J Zheng, J A Valeri, N M Donghia, M N Anahtar, S Omori, A Li, A Cubillos-Ruiz, A Krishnan, W Jin, A L Manson, J Friedrichs, R Helbig, B Hajian, D K Fiejtek, F F Wagner, H H Soutter, A M Earl, J M Stokes, . . Collins, J J , Nature. 62679972024</p>
<p>De novo design of protein structure and function with RFdiffusion. J L Watson, D Juergens, N R Bennett, B L Trippe, J Yim, H E Eisenach, W Ahern, A J Borst, R J Ragotte, L F Milles, B I M Wicky, N Hanikel, S J Pellock, A Courbet, W Sheffler, J Wang, P Venkatesh, I Sappington, S V Torres, . . Baker, D , Nature. 62079762023</p>
<p>De novo design of high-affinity binders of bioactive helical peptides. S Vázquez Torres, P J Y Leung, P Venkatesh, I D Lutz, F Hink, H.-H Huynh, J Becker, A H Yeh, .-W Juergens, D Bennett, N R Hoofnagle, A N Huang, E Maccoss, M J Expòsit, M Lee, G R Bera, A K Kang, A De La Cruz, J Levine, P M , . . Baker, D , Nature. 62679982024</p>
<p>A mobile robotic chemist. B Burger, P M Maffettone, V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, Nature. 58378152020</p>
<p>Applications of artificial intelligence to enzyme and pathway design for metabolic engineering. W D Jang, G B Kim, Y Kim, S Y Lee, Current Opinion in Biotechnology. 732022</p>
<p>GFN2-xTB-An Accurate and Broadly Parametrized Self-Consistent Tight-Binding Quantum Chemical Method with Multipole Electrostatics and Density-Dependent Dispersion Contributions. C Bannwarth, S Ehlert, S Grimme, Journal of Chemical Theory and Computation. 1532019</p>
<p>Spin-orbit coupling corrections for the GFN-xTB method. G Jha, T Heine, The Journal of Chemical Physics. 1584441202023</p>
<p>DeePMD-kit v2: A software package for deep potential models. J Zeng, D Zhang, D Lu, P Mo, Z Li, Y Chen, M Rynik, L Huang, Z Li, S Shi, Y Wang, H Ye, P Tuo, J Yang, Y Ding, Y Li, D Tisi, Q Zeng, H Bao, . . Wang, H , The Journal of Chemical Physics. 1595548012023</p>
<p>Automated self-optimization, intensification, and scale-up of photocatalysis in flow. A Slattery, Z Wen, P Tenblad, J Sanjosé-Orduna, D Pintossi, T Den Hartog, T Noël, Science. 383668118172024</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. N J Szymanski, B Rendy, Y Fei, R E Kumar, T He, D Milsted, M J Mcdermott, M Gallant, E D Cubuk, A Merchant, H Kim, A Jain, C J Bartel, K Persson, Y Zeng, G Ceder, Nature. 62479902023</p>
<p>Self-Improving Photosensitizer Discovery System via Bayesian Search with First-Principle Simulations. S Xu, J Li, P Cai, X Liu, B Liu, X Wang, Journal of the American Chemical Society. 143472021</p>
<p>Revolutionizing Membrane Design Using Machine Learning-Bayesian Optimization. H Gao, S Zhong, W Zhang, T Igou, E Berger, E Reid, Y Zhao, D Lambeth, L Gan, M A Afolabi, Z Tong, G Lan, Y Chen, Environmental Science &amp; Technology. 5642022</p>
<p>Machine learning-enabled high-entropy alloy discovery. Z Rao, P.-Y Tung, R Xie, Y Wei, H Zhang, A Ferrari, T P C Klaver, F Körmann, P T Sukumar, A Kwiatkowski Da Silva, Y Chen, Z Li, D Ponge, J Neugebauer, O Gutfleisch, S Bauer, D Raabe, Science. 37866152022</p>
<p>ChemCrow: Augmenting large-language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, arXiv:2304.05376ArXiv Preprint. 2023</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nature Machine Intelligence. 4122022</p>
<p>Scaling deep learning for materials discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, Nature. 62479902023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q Le, D Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, &amp; A Oh, Curran Associates, Inc202235</p>
<p>Few-shot training LLMs for project-specific code-summarization. T Ahmed, P Devanbu, Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. the 37th IEEE/ACM International Conference on Automated Software Engineering2023</p>
<p>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, arXiv:2302.11382ArXiv Preprint. 2023</p>
<p>Large Language Models Are Human-Level Prompt Engineers. Y Zhou, A Ioan Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, arXiv:2211.01910ArXiv Preprint. 2022</p>
<p>Prompt engineering for chatgpt: a quick guide to techniques, tips, and best practices. S Ekin, 10.36227/techrxiv.22683919.v22023</p>
<p>DARWIN Series: Domain Specific Large Language Models for Natural Science. T Xie, Y Wan, W Huang, Z Yin, Y Liu, S Wang, Q Linghu, C Kit, C Grazian, W Zhang, I Razzak, B Hoex, arXiv:2308.13565ArXiv Preprint. 2023</p>
<p>Prompt Engineering with ChatGPT: A Guide for Academic Writers. L Giray, Annals of Biomedical Engineering. 51122023</p>
<p>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial. B Meskó, J Med Internet Res. 25e506382023</p>
<p>Organic synthesis toward small-molecule probes and drugs. S L Schreiber, Proceedings of the National Academy of Sciences. 108172011</p>
<p>Industrial enzyme applications. O Kirk, T V Borchert, C C Fuglsang, Current Opinion in Biotechnology. 1342002</p>
<p>Enzyme Engineering: Current Trends and Future Perspectives. A Sharma, G Gupta, T Ahmad, S Mansoor, B Kaur, Food Reviews International. 3722021</p>
<p>Crystal growth and materials research in photovoltaics: progress and challenges. T Surek, Journal of Crystal Growth. 27512005</p>
<p>C Zhang, J Zhang, X Ma, Q Feng, Semiconductor photovoltaic cells. Springer2021</p>
<p>Measuring Massive Multitask Language Understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300ArXiv E-Preprint. 2020</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, . . Wu, Z , arXiv:2206.046152022ArXiv E-Preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168Training Verifiers to Solve Math Word Problems. ArXiv E-Preprint. 2021</p>
<p>PubChem Substance and Compound databases. S Kim, P A Thiessen, E E Bolton, J Chen, G Fu, A Gindulyte, L Han, J He, S He, B A Shoemaker, J Wang, B Yu, J Zhang, S H Bryant, Nucleic Acids Research. 44D12016</p>
<p>UniProt: a worldwide hub of protein knowledge. T U Consortium, Nucleic Acids Research. 47D12019</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, K A Persson, APL Materials. 11110022013</p>
<p>Language Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, . . Amodei, D , Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. S J Zhang, S Florin, A N Lee, E Niknafs, A Marginean, A Wang, K Tyser, Z Chin, Y Hicke, N Singh, M Udell, Y Kim, T Buonassisi, A Solar-Lezama, I Drori, arXiv:2306.089972023ArXiv E-Preprint</p>
<p>Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, arXiv:2309.15402A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. 2023</p>
<p>OpenAI's Documentation for Language Models. Openai, 2023</p>
<p>The advent and development of organocatalysis. D W C Macmillan, Nature. 45572112008</p>
<p>Recent Development of Recoverable MacMillan Catalyst in Asymmetric Organic Transformations. Deepa, S Singh, Advanced Synthesis &amp; Catalysis. 36332021</p>
<p>Paclitaxel in cancer therapy. M Markman, T M Mekhail, Expert Opinion on Pharmacotherapy. 362002</p>
<p>Taxanes in cancer treatment: Activity, chemoresistance and its overcoming. L Mosca, A Ilari, F Fazi, Y G Assaraf, G Colotti, Drug Resistance Updates. 541007422021</p>
<p>Progress and perspective of high-voltage lithium cobalt oxide in lithium-ion batteries. Q Wu, B Zhang, Y Lu, Journal of Energy Chemistry. 742022</p>
<p>A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation. C Goutte, E Gaussier, Advances in Information Retrieval. D E Losada, J M Fernández-Luna, Berlin HeidelbergSpringer2005Supplementary Materials S</p>
<p>Code and Models Links to the source code, test datasets, and prompt engineering models are available here. Datasets Link, The complete predictive results of different models and full results are also available in the link</p>            </div>
        </div>

    </div>
</body>
</html>
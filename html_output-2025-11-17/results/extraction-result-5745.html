<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5745 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5745</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5745</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-9dcee248452d84b6bf26911ba6726ae5ce1a46f3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9dcee248452d84b6bf26911ba6726ae5ce1a46f3" target="_blank">TabLLM: Few-shot Classification of Tabular Data with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Artificial Intelligence and Statistics</p>
                <p><strong>Paper TL;DR:</strong> Despite its simplicity, this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets and is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.</p>
                <p><strong>Paper Abstract:</strong> We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5745.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5745.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Narayan et al. (2022) / GPT-3 for data cleaning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can Foundation Models Wrangle Your Data? (Narayan et al., 2022) — in‑context GPT-3 for tabular data cleaning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that evaluated in‑context learning with an autoregressive large language model (GPT-3) on tabular data cleaning tasks (identification and correction of corrupted table cells), reporting strong few‑shot performance versus prior approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Foundation Models Wrangle Your Data?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model (GPT‑3 family) used in in‑context/few‑shot settings; very large model pre-trained on large web/text corpora and used via prompting (zero/few‑shot). The paper being summarized notes GPT‑3 for serialization and for zero-shot experiments (text‑davinci-002 was used elsewhere in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>In‑context/few‑shot prompting for tabular data cleaning (identification/correction of corrupted cells); no detailed anomaly‑scoring pipeline described in this paper — described as GPT‑3 applied directly to cleaning/repair via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular data (per‑cell / row serializations)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Data cleaning / corrupted cells (identification and correction of erroneous cells or values)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specified in this paper; summary statement: Narayan et al. found GPT‑3 often outperforms state‑of‑the‑art approaches with ten labeled examples (metrics and datasets not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported to often outperform state‑of‑the‑art tabular data cleaning approaches in few‑shot (≈10 examples) settings (specific baselines and numerical comparisons are not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This paper does not provide task‑level details from Narayan et al.; no datasets/metrics are given here. The authors note related issues elsewhere: LLM serializations can hallucinate or omit features, which can bias downstream results when using LLMs on tabular inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TabLLM: Few-shot Classification of Tabular Data with Large Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5745.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5745.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIFT (Dinh et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LIFT: Language‑interfaced fine‑tuning for non‑language machine learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned method that fine‑tunes large autoregressive language models (e.g., GPT‑3, GPT‑J) to perform non‑language tasks (regression and classification) by prompting/serializing structured inputs; studied sample efficiency and serialization effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LIFT: Language-interfaced fine-tuning for non-language machine learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 and GPT-J (as evaluated by Dinh et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine‑tuned autoregressive transformer models used as general-purpose learners for non‑language tasks; LIFT uses language interfaces and static serialization templates, then fine‑tunes the LM to predict labels for non‑language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Parameter‑efficient fine‑tuning (language‑interfaced fine‑tuning) of autoregressive LMs on serialized non‑language inputs (regression/classification); static serialization templates and fine‑tuning rather than only in‑context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Synthetic data, tabular data, and vision data (as reported by Dinh et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specified in this paper; the current paper notes that Dinh et al. evaluated sample efficiency (binary classification/regression tasks) and reported that a fine‑tuned GPT‑3 performed worse than logistic regression on two binary tasks up to ~250 training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>According to this paper's summary, Dinh et al. could not confirm strong sample‑efficiency gains for GPT‑3: fine‑tuned GPT‑3 underperformed logistic regression on two binary classification tasks for up to 250 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported by the authors here: fine‑tuned GPT‑3 did not always show the few‑shot advantages reported elsewhere and could perform worse than simple baselines (LR) on some tasks; indicates sample‑efficiency and task dependency concerns. Specific anomaly‑detection tasks were not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TabLLM: Few-shot Classification of Tabular Data with Large Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5745.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5745.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General literature mention: corrupted cell detection / data cleaning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Identification or correction of corrupted table cells (general ML / self‑supervision literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper cites prior work that self‑supervised objectives for tabular data often include predicting masked cells and identifying/correcting corrupted cells; this is presented as a common formulation of data cleaning/anomaly detection in tabular inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specific to a single language model in this paper; describes a class of objectives (self‑supervised prediction of masked/corrupted cells) used in tabular representation learning literature.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Self‑supervised learning objectives (masked cell prediction; identification/correction of corrupted cells), sometimes combined with contrastive or correction objectives — not LLM‑specific in the cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular / structured data (cells, rows, columns)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Corrupted cells / syntactic errors / cell‑level corruption (data cleaning)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not detailed in this paper; the citation is descriptive of the task class rather than reporting results here.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>This paper notes that many deep learning approaches on tabular data (including self‑supervised methods) still underperform strong tree‑based baselines (e.g., XGBoost, LightGBM) for standard supervised tasks; however, those comparisons are for classification benchmarks and do not directly quantify corrupted‑cell detection performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No LLM‑specific evidence given here for corrupted‑cell detection; the paper emphasizes that tabular tasks have distinct structure (mixed types, small number of columns) and that general LLM prior knowledge is useful only when column/feature semantics are represented in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TabLLM: Few-shot Classification of Tabular Data with Large Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can Foundation Models Wrangle Your Data? <em>(Rating: 2)</em></li>
                <li>LIFT: Language-interfaced fine-tuning for non-language machine learning tasks <em>(Rating: 2)</em></li>
                <li>Language models are realistic tabular data generators <em>(Rating: 1)</em></li>
                <li>Tabtext: a systematic approach to aggregate knowledge across tabular data structures <em>(Rating: 1)</em></li>
                <li>TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5745",
    "paper_id": "paper-9dcee248452d84b6bf26911ba6726ae5ce1a46f3",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "Narayan et al. (2022) / GPT-3 for data cleaning",
            "name_full": "Can Foundation Models Wrangle Your Data? (Narayan et al., 2022) — in‑context GPT-3 for tabular data cleaning",
            "brief_description": "Referenced work that evaluated in‑context learning with an autoregressive large language model (GPT-3) on tabular data cleaning tasks (identification and correction of corrupted table cells), reporting strong few‑shot performance versus prior approaches.",
            "citation_title": "Can Foundation Models Wrangle Your Data?",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (autoregressive)",
            "model_description": "Autoregressive transformer language model (GPT‑3 family) used in in‑context/few‑shot settings; very large model pre-trained on large web/text corpora and used via prompting (zero/few‑shot). The paper being summarized notes GPT‑3 for serialization and for zero-shot experiments (text‑davinci-002 was used elsewhere in this paper).",
            "model_size": "175B",
            "anomaly_detection_method": "In‑context/few‑shot prompting for tabular data cleaning (identification/correction of corrupted cells); no detailed anomaly‑scoring pipeline described in this paper — described as GPT‑3 applied directly to cleaning/repair via prompting.",
            "data_type": "Tabular data (per‑cell / row serializations)",
            "anomaly_type": "Data cleaning / corrupted cells (identification and correction of erroneous cells or values)",
            "dataset_name": null,
            "performance_metrics": "Not specified in this paper; summary statement: Narayan et al. found GPT‑3 often outperforms state‑of‑the‑art approaches with ten labeled examples (metrics and datasets not detailed here).",
            "baseline_comparison": "Reported to often outperform state‑of‑the‑art tabular data cleaning approaches in few‑shot (≈10 examples) settings (specific baselines and numerical comparisons are not provided in this paper).",
            "limitations_or_failure_cases": "This paper does not provide task‑level details from Narayan et al.; no datasets/metrics are given here. The authors note related issues elsewhere: LLM serializations can hallucinate or omit features, which can bias downstream results when using LLMs on tabular inputs.",
            "uuid": "e5745.0",
            "source_info": {
                "paper_title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LIFT (Dinh et al., 2022)",
            "name_full": "LIFT: Language‑interfaced fine‑tuning for non‑language machine learning tasks",
            "brief_description": "Mentioned method that fine‑tunes large autoregressive language models (e.g., GPT‑3, GPT‑J) to perform non‑language tasks (regression and classification) by prompting/serializing structured inputs; studied sample efficiency and serialization effects.",
            "citation_title": "LIFT: Language-interfaced fine-tuning for non-language machine learning tasks",
            "mention_or_use": "mention",
            "model_name": "GPT-3 and GPT-J (as evaluated by Dinh et al.)",
            "model_description": "Fine‑tuned autoregressive transformer models used as general-purpose learners for non‑language tasks; LIFT uses language interfaces and static serialization templates, then fine‑tunes the LM to predict labels for non‑language tasks.",
            "model_size": null,
            "anomaly_detection_method": "Parameter‑efficient fine‑tuning (language‑interfaced fine‑tuning) of autoregressive LMs on serialized non‑language inputs (regression/classification); static serialization templates and fine‑tuning rather than only in‑context learning.",
            "data_type": "Synthetic data, tabular data, and vision data (as reported by Dinh et al.)",
            "anomaly_type": null,
            "dataset_name": null,
            "performance_metrics": "Not specified in this paper; the current paper notes that Dinh et al. evaluated sample efficiency (binary classification/regression tasks) and reported that a fine‑tuned GPT‑3 performed worse than logistic regression on two binary tasks up to ~250 training examples.",
            "baseline_comparison": "According to this paper's summary, Dinh et al. could not confirm strong sample‑efficiency gains for GPT‑3: fine‑tuned GPT‑3 underperformed logistic regression on two binary classification tasks for up to 250 examples.",
            "limitations_or_failure_cases": "Reported by the authors here: fine‑tuned GPT‑3 did not always show the few‑shot advantages reported elsewhere and could perform worse than simple baselines (LR) on some tasks; indicates sample‑efficiency and task dependency concerns. Specific anomaly‑detection tasks were not detailed.",
            "uuid": "e5745.1",
            "source_info": {
                "paper_title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "General literature mention: corrupted cell detection / data cleaning",
            "name_full": "Identification or correction of corrupted table cells (general ML / self‑supervision literature)",
            "brief_description": "Paper cites prior work that self‑supervised objectives for tabular data often include predicting masked cells and identifying/correcting corrupted cells; this is presented as a common formulation of data cleaning/anomaly detection in tabular inputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Not specific to a single language model in this paper; describes a class of objectives (self‑supervised prediction of masked/corrupted cells) used in tabular representation learning literature.",
            "model_size": null,
            "anomaly_detection_method": "Self‑supervised learning objectives (masked cell prediction; identification/correction of corrupted cells), sometimes combined with contrastive or correction objectives — not LLM‑specific in the cited works.",
            "data_type": "Tabular / structured data (cells, rows, columns)",
            "anomaly_type": "Corrupted cells / syntactic errors / cell‑level corruption (data cleaning)",
            "dataset_name": null,
            "performance_metrics": "Not detailed in this paper; the citation is descriptive of the task class rather than reporting results here.",
            "baseline_comparison": "This paper notes that many deep learning approaches on tabular data (including self‑supervised methods) still underperform strong tree‑based baselines (e.g., XGBoost, LightGBM) for standard supervised tasks; however, those comparisons are for classification benchmarks and do not directly quantify corrupted‑cell detection performance.",
            "limitations_or_failure_cases": "No LLM‑specific evidence given here for corrupted‑cell detection; the paper emphasizes that tabular tasks have distinct structure (mixed types, small number of columns) and that general LLM prior knowledge is useful only when column/feature semantics are represented in pretraining.",
            "uuid": "e5745.2",
            "source_info": {
                "paper_title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can Foundation Models Wrangle Your Data?",
            "rating": 2
        },
        {
            "paper_title": "LIFT: Language-interfaced fine-tuning for non-language machine learning tasks",
            "rating": 2
        },
        {
            "paper_title": "Language models are realistic tabular data generators",
            "rating": 1
        },
        {
            "paper_title": "Tabtext: a systematic approach to aggregate knowledge across tabular data structures",
            "rating": 1
        },
        {
            "paper_title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data",
            "rating": 1
        }
    ],
    "cost": 0.019535499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TabLLM: Few-shot Classification of Tabular Data with Large Language Models</h1>
<p>Stefan Hegselmann ${ }^{1,2}$ Alejandro Buendia ${ }^{1}$ Hunter Lang ${ }^{1}$ Monica Agrawal ${ }^{1}$ Xiaoyi Jiang ${ }^{2}$ David Sontag ${ }^{1}$<br>${ }^{1}$ MIT CSAIL ${ }^{2}$ University of Münster</p>
<h4>Abstract</h4>
<p>We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.</p>
<h2>1 INTRODUCTION</h2>
<p>Many real world applications generate tabular data as a natural byproduct of relational databases (Shwartz-Ziv and Armon, 2022). It is ubiquitous in domains ranging from healthcare to climate and finance (Sahakyan et al., 2021). Obtaining enough labeled data to train supervised learning algorithms for classification can be difficult. For example, in healthcare, there are 10,000 rare diseases (Haendel et al., 2020) affecting very few patients, which hampers the development of risk stratification models. Thus, we seek to develop methods that can exploit prior knowledge (e.g., from medical articles) to improve predictive performance</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>in settings with a small number of training examples, i.e. the few-shot setting.</p>
<p>While deep learning has led to breakthroughs in computer vision and natural language processing, this success has not yet been extended to the tabular domain. For example, selfsupervised deep learning methods have been introduced for tabular data (Yin et al., 2020; Arik and Pfister, 2021), but Grinsztajn et al. (2022) showed that these deep techniques still underperform ensembles of gradient boosted trees in the fully supervised setting. This disparity in performance can be attributed to the differences between tabular data and text or images; tabular data lacks locality, contains mixed data types, and the number of columns is usually fairly small compared to the number of features in text or image data (Borisov et al., 2022a).</p>
<p>Recently, large language models (LLMs) such as GPT-3, which are pre-trained on enormous corpora of text, have shown incredible performance on few-shot text classification and generation tasks (Brown et al., 2020; Sanh et al., 2022; Ouyang et al., 2022). These LLMs perform well on a variety of tasks and domains, including fact retrieval (Liu et al., 2021), mathematical reasoning (Wei et al., 2022), medical information extraction (Agrawal et al., 2022), and tabular data cleaning tasks (Narayan et al., 2022). Most importantly, because of all the knowledge encoded in their parameters, LLMs require little or no labeled training data to obtain this good performance.</p>
<p>In this work we introduce TabLLM, which is a general framework to leverage LLMs for few-shot classification of tabular data. We prompt the LLM with a serialization of a row to a natural-language representation and a short description of the classification problem. For risk stratification, for instance, this serialization could list relevant patient attributes and combine it with, "Will this patient be hospitalized?". We experiment with nine different serializations and the T0 language model of different sizes (Sanh et al., 2022). We use the parameter-efficient fine-tuning method T-Few (Liu et al., 2022) to update the LLM's parameters using some labeled examples. We also evaluate GPT-3 in the zero-shot setting (Brown et al., 2020). To the best of our knowledge, this is one of the widest evaluations of LLMs for zero- and few-shot tabular classification.</p>
<ol>
<li>Tabular data with $k$ labeled rows
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ol>
<p>Figure 1: Overview of TabLLM. We first serialize the feature names and values into a natural language string. We evaluate different strategies. This string is then combined with a task-specific prompt. To get predictions, we obtain output probabilities from the LLM for each of a pre-specified set of verbalizer tokens (e.g., "Yes", "No"), which map to class labels (e.g., $1,-1$ ). If $k&gt;0$, we use the $k$ labeled examples to fine-tune the large language model using T-Few (Liu et al., 2022). Finally, we use the (possibly tuned) large language model to obtain predictions on unlabeled examples.</p>
<p>Despite its simplicity, we find that TabLLM outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. By using information from the natural-language column names and feature values, it often enables effective zero-shot classification of tabular data. Unlike many deep learning methods on tabular data, this approach is also competitive with gradient-boosted tree baselines and outperforms them or is on par until 256 shots. In the very-few-shot setting it outperforms them by a considerable margin. The main contributions of this work are:</p>
<ul>
<li>We introduce TabLLM, a novel framework leveraging LLMs for data-efficient tabular classification</li>
<li>We study nine serialization techniques and explore their performance across ten different datasets</li>
<li>We show that TabLLM instantiated with a simple text serialization and the T0 LLM can outperform state-of-the-art neural models and tree ensembles in the zeroand few-shot setting</li>
<li>We investigate the application of TabLLM to a large real-world healthcare claims dataset and introduce serialization methods that deal with many input features</li>
</ul>
<h2>2 RELATED WORK</h2>
<h3>2.1 Machine Learning on Tabular Data</h3>
<p>Due to the success of deep learning in other domains, there have been many recent attempts at representation learning for tabular data. Self-supervised objectives have largely revolved around the prediction of masked cells, the identification or correction of corrupted cells, and contrastive
losses over augmentations (Bahri et al., 2022; Somepalli et al., 2021; Yoon et al., 2020; Arik and Pfister, 2021; Huang et al., 2020). Additional efforts have included differentiable trees, which combine advantages of tree ensembles with gradient based optimization of neural networks (Kontschieder et al., 2015; Popov et al., 2020). However, several recent comprehensive reviews (Shwartz-Ziv and Armon, 2022; Borisov et al., 2022a; Grinsztajn et al., 2022) found that gradient-boosted tree ensembles like XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017) systematically outperform these novel deep learning architectures, even with proper fine-tuning and regularization (Kadra et al., 2021). Levin et al. (2022) found utility in transfer learning in the semi-supervised setting, but required a set of additional supervised tasks on the same table, which can be a nontrivial limitation. They investigate few-shot classification for medical diagnosis using 4 to 200 labeled examples, but do not exploit the power of large pre-trained models, as we do in this work. Hollmann et al. (2022) recently introduced TabPFN, a Bayesian neural network pre-trained on synthetic tabular data, outperforming gradient boosted trees in a comprehensive evaluation.</p>
<h3>2.2 Large Language Models for Tabular Data</h3>
<p>Another approach has been to leverage the natural language capabilities of language models. Yin et al. (2020) use a language model for semantic parsing of natural language queries over tabular data. Li et al. (2020) investigate the ability of language models to perform entity matching on tabular data, i.e. determining if two rows refer to the same object. Harari and Katz (2022) study data enrichment by linking each table row with additional unstructured text (e.g., from Wikipedia) from which they generated addi-</p>
<p>tional features using a language model. However, this setup requires named entities (e.g., celebrities, universities, etc.), which is quite limiting. Bertsimas et al. (2022) studied two healthcare datasets and used a language model to generate feature embeddings, which they fed into classifiers like gradient boosted trees. All these studies use a BERT-style language model (Devlin et al., 2019). Narayan et al. (2022) recently assessed in-context learning with the autoregressive language model GPT-3 for tabular data cleaning tasks. They found that it often outperforms state-of-the-art approaches with ten labeled examples. Borisov et al. (2022b) introduced an LLM-agnostic method to generate realistic tabular data and found that it achieved better results than existing approaches. In contrast, here we study classification tasks of tabular data and investigate parameter-efficient fine-tuning of LLMs.</p>
<p>To use an LLM for tabular data, the table must be serialized into a natural text representation. All aforementioned works relied on simple list or sentence serializations; Yin et al. (2020) also included the column data type in the serialized string. Only Bertsimas et al. (2022) studied different serialization variants, but this was in a different context of deriving feature embeddings from BERT-style language models. The LIFT method introduced by Dinh et al. (2022) comes closest to our work. The authors evaluated the capabilities of fine-tuned GPT-3 and GPT-J models for regression and classification on synthetic, tabular, and vision data. They also studied the sample efficiency and considered different static serialization templates assessing the effect of including column names in the input. In this work, we focus on the publicly available T0 model and perform a broader analysis of nine serialization techniques including automatic approaches and ablations evaluating the importance of feature values. Particularly, we are interested in leveraging prior knowledge encoded in LLMs and we do a more fine-grained analysis of the sample efficiency including zero-shot experiments on ten different datasets.</p>
<h2>3 METHODS</h2>
<h3>3.1 TabLLM for Tabular Data Classification</h3>
<p>Problem Formalization. Suppose we have a tabular dataset with $n$ rows and $d$ columns or features. We can formalize this as $D=\left{\left(\mathbf{x}<em i="i">{i}, y</em>\right)\right}<em i="i">{i=1}^{n}$, where each $\mathbf{x}</em>$ of size $k$-sampled from $D$ with replacement-for fine-tuning or training.}$ is a $d$ dimensional feature vector. Since we consider classification, $y_{i} \in C$ for a set of classes $C$. We define the column names or feature names as $F=\left{f_{1}, \ldots, f_{d}\right}$. We assume the $f_{i}$ 's are natural-language strings such as "age" or "education" (see Figure 1). For our $k$-shot classification experiments, we only use a subset $D_{k</p>
<p>Serialization of Tabular Data. To use an LLM for tabular data, the table must be transformed into a natural text
representation. Typically, when prompting an LLM, there is a template used to both serialize the inputs into one natural-language string, and to provide the prompt itself (e.g., the string "Does this person make more than 50,000 dollars? Yes or no?"), which is usually located after the serialized input. In this work, we break these pieces up into a serialization and a prompt. We define a function serialize $(F, \mathbf{x})$ that takes the column names $F$ and feature values $\mathbf{x}$ for a row as inputs and creates a textual representation of the input. Combining this serialization with a task-specific prompt $p$ will then form the LLM input (serialize $(F, \mathbf{x}), p)$. This is illustrated in Figure 1. We primarily study the serialization, since that is the biggest difference compared to existing applications of prompting. Previous work has usually considered a simple concatenation of feature names and values as a serialization of tabular data (Li et al., 2020; Narayan et al., 2022). In our work, this function can be arbitrarily complex. For instance, we explore serializations that include (i) incorporating another LLM and (ii) employing feature selection as a substep.</p>
<p>Large Language Models For Classification TabLLM can be used with different LLMs that generate text based on a natural-language input. Let LLM be an LLM with vocabulary $V$. Then, $\operatorname{LLM}((\operatorname{serialize}(F, \mathbf{x}), p)) \in V^{\prime}$ is the prompted output of the LLM. In our few-shot setting, $\left{(\operatorname{serialize}(F, \mathbf{x}), p) \mid(\mathbf{x}, y) \in D_{k}\right}$ can be used as training examples for fine-tuning the LLM. The LLM generates text in the vocabulary space $V^{\prime}$ that has to be mapped to a valid class in $C$. Several approaches already exist for this problem. For example, the verbalizer (Schick and Schütze, 2021) defines a mapping between LLM output tokens and the discrete label space. Verbalizers can be manually specified or automatically learned; see Cui et al. (2022) for an overview of different verbalizer-learning approaches. In this work, we assume for simplicity that the verbalizer mapping is manually specified (see answer _choices in the templates in Sec. 8 in the Supplement).</p>
<h3>3.2 Our Instantiation of TabLLM</h3>
<p>Serialization Approaches for TabLLM. The performance of LLMs is very sensitive to the precise details of the natural-language input (Zhao et al., 2021; Webson and Pavlick, 2022). In this work, we focus on the serialization of the tabular data. For the prompt, we use a simple description of the classification task and perform no further prompt engineering. We study nine different serialization formats varying in complexity. All serialization methods require minimal human effort to apply to new classification tasks. We evaluate several methods that generate natural text to create inputs that are closer to the training distribution of the LLM, thereby improving zero and very-few-shot performance. Additional details and examples for the serializations are given in Sec. 1.2.1 and 9 in the Supplement.</p>
<ul>
<li>List Template: A list of column names and feature values. We fixed an arbitrary ordering of the columns.</li>
<li>Text Template: An textual enumeration of all features as "The column name is value." (see Figure 1).</li>
<li>Table-To-Text: We use an LLM fine-tuned on a table-to-text generation task from HuggingFace (Narrativaai/bloom-560m-finetuned-totto -table-to-text). To ensure that the serialization includes all data we hand each column-value tuple to the model separately and concatenate the outputs.</li>
<li>Text T0: We use the LLM T0 with 11B parameters (bigscience/T0pp) (Sanh et al., 2022). We split up a row into pairs of two column-value tuples. We send them to LLM separately with the prompt "Write this information as a sentence:" and combine the outputs.</li>
<li>Text GPT-3: We use GPT-3 (engine text-davinci002) accessible through an API (Ouyang et al., 2022). GPT-3 was able to serialize all features at once, so we use a list of all features with the prompt "Rewrite all list items in the input as a natural text." as input. We guide the output with "The {person, car, patient} is".</li>
</ul>
<p>We consider the following serializations as ablations:</p>
<ul>
<li>List Only Values: List Template for feature values only. We want to evaluate whether column names aid the classification performance.</li>
<li>List Permuted Names: List Template with permuted column names. Hence, the wrong column name is associated with each feature value. The permutation is the same across all examples. We perform this ablation to study the relevance of the correct association between column names and feature values.</li>
<li>List Permuted Values: List Template with consistently permuted values across all examples. We generate one permutation for each column and apply this mapping to all column values. For continuous values, we use ten uniform bins. This tests whether the LLM uses the fine-grained information encoded by the feature values for zero-shot and few-shot classification.</li>
<li>List Short: List Template with at most ten features. We only consider this for the healthcare dataset where the number of features exceeds the input limit of the LLM. We want to study the effect of less information.</li>
</ul>
<p>Large Language Models for TabLLM Another crucial component of TabLLM is the LLM. TabLLM is both agnostic to the LLM and the specific fine-tuning method that is used. We only consider a single LLM for most of our experiments. We employ the T0 encoder-decoder model with 11 billion parameters as the LLM for TabLLM (Sanh et al., 2022). It was trained on a large variety of task-specific prompts, making it a suitable candidate for our experiments
(Sanh et al., 2022). This model has a token limit of 1024, which roughly corresponds to 400 words. We also evaluate the effect of a smaller version of the T0 model (T0 3B). We fine-tuned on the few-shot data $\mathcal{D}_{k}$ using the recent T-Few recipe, which outperforms other parameter-efficient tuning methods such as soft prompt tuning (Liu et al., 2022). In addition, we perform zero-shot experiments with the LLM GPT-3 (engine text-davinci-002) (Ouyang et al., 2022).</p>
<h2>4 EXPERIMENTAL SETUP</h2>
<h3>4.1 Datasets</h3>
<p>We studied TabLLM in two experimental settings. First, we considered nine medium-sized tabular datasets for binary and multi-class classification. We systematically identified datasets from Kadra et al. (2021), Grinsztajn et al. (2022), and Borisov et al. (2022a). We included datasets with at most 50,000 rows to keep the fine-tuning costs manageable and at most 30 columns to stay within T0's token limit. We also required textual feature names to make the serializations more meaningful and we excluded datasets with derived feature values (e.g., mean pixel values). This lead to inclusion of Bank (45,211 rows, 16 feats), Blood (748, 4), California (20,640, 8), Car (1,728, 8), Creditg (1,000, 20), Income (48,842, 14), and Jungle (44,819, 6). We added two additional datasets from Kaggle that fulfilled our inclusion criteria: Diabetes (768, 8) and Heart (918, 11). Second, we evaluated TabLLM for risk stratification on three binary classification tasks, following prior work by Kodialam et al. (2021) and similarly using a deidentified health claims dataset from a U.S. health insurer. We predicted the end-of-life (EoL) of all patients older than 70 years, which can be used to inform care in a palliative setting (Avati et al., 2018). We also considered the need for any surgical procedure (Surgery) and the likelihood of hospitalization (LoH), which can help with determining health care needs and estimating future costs. Additional details on all datasets can be found in Sec. 1 in the Supplement. We release the code for our experiments on Github. ${ }^{1}$</p>
<h3>4.2 LLM and Fine-tuning</h3>
<p>We used the HuggingFace implementation of the T0 model (bigscience/{T0pp,T0_3B}). Prompts for the LLM were designed following Sanh et al. (2022) using the PromptSource framework (Bach et al., 2022). Each class in our classification tasks was manually encoded in a textual response, e.g., "Yes" and "No" for true and false (Sanh et al., 2022). The prediction probability for each class corresponds to the probability of the LLM generating its token sequence normalized across all classes. All templates used in this work are given in Sec. 8 in the Supplement.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For fine-tuning, we adopted the default hyperparameters of the T-Few method without any additional parameter tuning (Liu et al., 2022). The authors used a setup of $k=32$ shots and 1,000 training steps for most of their experiments, which corresponds to 31.25 epochs. Hence, we fixed 30 training epochs for all few-shot experiments on the public tabular datasets. We used $20 \%$ of the data as a test set. For the large healthcare claims dataset, we used 10 epochs for up to 256 shots and 3 epochs for 1,024, 4,096 and 16,384 to reduce the runtime and prevent overfitting for many training examples. We used a test set of 10,000 examples for the three healthcare tasks. All experiments were evaluated with the area under the receiver operating characteristic curve (AUC). We used macro-AUC one-versus-rest for the multiclass setting. Estimates for the runtime are given in Sec. 2 in the Supplement.</p>
<h3>4.3 Baseline Models</h3>
<p>We compared TabLLM to several baselines. For the simplest baseline, we used a logistic regression (LR) model. Since previous work showed the superiority of gradient boosted tree ensembles (Borisov et al., 2022a), we included the most common models XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017). We also evaluated several state-of-the-art deep learning baselines. TabNet is a widely used neural model for tabular data that uses attention over columns (Arik and Pfister, 2021). SAINT is a more recent approach that uses attention over rows and columns (Somepalli et al., 2021). SAINT performed best in a comprehensive review on tabular data (Borisov et al., 2022a). NODE is a differentiable tree ensemble method that performed best in the evaluation of Shwartz-Ziv and Armon (2022). Lastly, we include TabPFN, a Bayesian neural network that was pre-trained on synthetic tabular data (Hollmann et al., 2022). In contrast to TabLLM, we performed hyperparameter tuning for all baselines except TabPFN (see Sec. 3 in the Supplement), which requires no tuning by design. We adopted the parameter ranges from previous reviews (Borisov et al., 2022a; Grinsztajn et al., 2022). Since no validation set exists in the few-shot setting, we used 4-fold cross validation on the $k$-shots. In particular, we did not use a large validation set for hyperparameter tuning, unlike some few-shot learning works as highlighted by Perez et al. (2021). We encoded categorical values as one-hot vectors. We also tested ordinal encoding for LR, XGBoost, LightGBM, and TabPFN, but it showed worse results (see Table 12, 13, and 14 in the Supplement). In addition, we give results for GPT-3 (text-davinci-002) without fine-tuning, i.e. in the zero-shot setting using the Text Template serialization.</p>
<p>For the three health claims tasks, we used the same experimental setup for the baselines. However, we only included LR and LightGBM due to runtime limitations. Following Kodialam et al. (2021), each patient's input was a one-hot
encoded vector. For each medical concept, there were three indicator variables of whether that concept occurred within 30 days, 1 year, and anytime before prediction time.</p>
<h3>4.4 Serializations</h3>
<p>For the public datasets, some column names and feature values were manually mapped to human-readable forms, based on the provided documentation. For instance, for the Income dataset, the feature name hours_per_week was mapped to work hours per week and the feature value private for working class was mapped to private sector employee. Numerical values were not changed.
Serialization was more complex for the healthcare claims data. Each patient record is a time series of visits, with each visit consisting of a list of medical conditions and procedures. We only considered the manual serializations List Template and Text Template. We tried to mimic the style of a medical professional to tap potential prior knowledge of the LLM. To this end, the serialization starts with an intro sentence containing the patient's gender, age, and race. It then describes each visit, stating its date, the type of doctor the patient saw (e.g., dermatology) if an outpatient visit or length of hospitalization if an inpatient visit, the primary complaint of the associated visit, and procedures performed. Since there are no feature values in this dataset, we omit List Only Values and List Permuted Values. We also performed experiments for concept selection and different names for the medical concepts. Details for these additional experiments and examples of the serializations are given in Sec. 1.2.2, 1.2.3, and 9 in the Supplement.</p>
<h2>5 RESULTS</h2>
<h3>5.1 Effects of serialization</h3>
<p>Figure 2 shows the performance of different serialization methods for TabLLM averaged over the nine public datasets. The Text Template serialization performed very well across all experiments. In the zero-shot setting, the Text Template showed improvements over List Template, indicating the benefit of a serialization that is closer to the training distribution of T0. However, these differences already vanished for 8 training examples. Hence, very few training examples might already suffice to adjust for different templates. This suggests that sophisticated serializations might be unnecessary when some training data exists.
Using LLMs for serialization showed mixed results. The ordering is according to the complexity of the LLM used for serialization. GPT-3 has 175B, T0 11B, and the BLOOM table-to-text model 0.56B parameters. Different reasons might be responsible for the worse performance overall. The models tended to hallucinate information for some examples, leading to biased predictions of TabLLM.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average AUC and SD of different serializations across nine public datasets. Text Template performs best for zero and few training examples. For many examples, the performance of different serializations converges.</p>
<p>For instance, GPT-3 added "this car is a good choice" or added entirely new data to some examples (see Sec. 9 in the Supplement). Also, the LLMs are not completely faithful at including all features, even though we tried to enforce it in our experiments. This could explain that none of the LLM serializations reaches the same performance as the template serializations, even for many training examples.</p>
<p>Using only feature values had a poor performance for zero and very few shots, but the performance equalized with more training examples. The same applies to the list serialization with permuted feature names. This indicates that if enough training examples are available, the serialization approach does not matter, but that TabLLM relies on information from the feature names in the zero-shot and few-shot regime, and also relies on the association of the names with the correct values. The discrepancy for zero and very few shots was even stronger for List Permuted Values, which suggests that TabLLM relies more on the correct values than feature names. Again, the performance equalized for more examples showing the ability of TabLLM to learn new associations if enough training data is available. Using the smaller T0 3B model showed a slightly decreased performance (see Table 12, 13, and 14 in the Supplement).</p>
<p>For the healthcare claims dataset, we found that the List Template slightly outperformed the Text Template serialization (see Table 15 in the Supplement). This was consistent across tasks. The List Short serialization only performed slightly worse. The evaluation of different concept selection strategies showed that choosing the most frequent conditions per patient performed best. We found no considerable performance difference for different concept names.</p>
<p>From here onwards, we show results for TabLLM using the Text Template serialization for the public datasets. For the healthcare claims dataset, we use the List Template seri-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Average AUC and SD of TabLLM versus all baseline models across nine public datasets. TabLLM outperforms all baselines for zero and very few training examples. TabPFN is the strongest baseline.</p>
<p>alization and select the most frequent conditions. Results for all (dataset, serialization) combinations (Table 12, 13, and 14) and the additional experiments on the healthcare dataset (Table 5 and 7) can be found in the Supplement.</p>
<h3>5.2 Public Tabular Datasets</h3>
<p>Figure 3 shows the averaged results for TabLLM using the best serialization (Text Template) versus all baseline models. Table 1 contains the detailed results for TabLLM, TabPFN, and XGBoost. TabLLM showed a similar behavior across datasets. It achieved nontrivial zero-shot performance for all tasks except on Credit-g and Heart. For Heart this might be due to the dataset's inclusion criteria requiring eligibility for a heart procedure biasing the prediction. In all cases, TabLLM's performance improved with a higher number of shots. In the zero-shot setting, TabLLM was on par with GPT-3 even though GPT-3 is a much larger model than T0 (175B vs. 11B parameters). TabPFN consistently outperformed the other baseline models across all numbers of training examples. TabPFN reached TabLLM's performance with 4 to 256 (Income) training examples. LR was the second-best baseline often beating the tree models, which might be due to our extensive parameter tuning (see Sec. 4 in the Supplement). TabLLM outperformed or was on par with the tree ensemble baselines until 256 training examples for all datasets except Calhousing and Jungle. For fewer shots, it often outperformed them by a large margin. XGBoost performed relatively poorly for few shots, which was probably due to overfitting on the small training and validation sets (as described in the previous section, we do not use large validation sets for hyperparameter tuning to ensure the results are truly few-shot). TabLLM outperformed the neural baselines SAINT, NODE, and TabNet in many settings. It also</p>
<p>Table 1: Test AUC performance of TabLLM, the best tree ensemble model (XGBoost), and the best baseline (TabPFN) on the public tabular datasets. Each column reports the performance for $k$ training examples. TabLLM (T0 + Text Template) outperforms XGBoost and TabPFN in the very-few-shot regime. Standard deviations are given across five random seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Number of Shots</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">all</td>
</tr>
<tr>
<td style="text-align: center;">Bank</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.56_{.09}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 3}_{.02}$</td>
<td style="text-align: center;">$0.85_{.03}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 4}_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.59_{.14}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 6}_{.08}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9}_{.02}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.82_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6}_{.02}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9}_{.00}$</td>
<td style="text-align: center;">$0.90_{.00}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 6 3}_{.01}$</td>
<td style="text-align: center;">$0.59_{.10}$</td>
<td style="text-align: center;">$0.64_{.05}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.69_{.03}$</td>
<td style="text-align: center;">$0.82_{.05}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.92 \dagger$</td>
</tr>
<tr>
<td style="text-align: center;">Blood</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.58_{.07}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.68_{.05}$</td>
<td style="text-align: center;">$0.71_{.06}$</td>
<td style="text-align: center;">$0.70_{.07}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.71_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.52_{.08}$</td>
<td style="text-align: center;">$0.64_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 4}_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 6 1}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8}_{.09}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 6}_{.03}$</td>
<td style="text-align: center;">$0.66_{.07}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.68_{.06}$</td>
<td style="text-align: center;">$0.70_{.08}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.70_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">Calhousing</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.62_{.10}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
<td style="text-align: center;">$0.79_{.04}$</td>
<td style="text-align: center;">$0.82_{.04}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.92_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 7}_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.63_{.13}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3}_{.11}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 5}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9}_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 1}_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2}_{.00}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 3}_{.00}$</td>
<td style="text-align: center;">$0.94_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 6 1}_{.01}$</td>
<td style="text-align: center;">$0.63_{.05}$</td>
<td style="text-align: center;">$0.60_{.07}$</td>
<td style="text-align: center;">$0.70_{.08}$</td>
<td style="text-align: center;">$0.77_{.08}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.83_{.01}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.95_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">Car</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.59_{.04}$</td>
<td style="text-align: center;">$0.70_{.08}$</td>
<td style="text-align: center;">$0.82_{.03}$</td>
<td style="text-align: center;">$0.91_{.02}$</td>
<td style="text-align: center;">$0.95_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.75_{.05}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2}_{.02}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 7}_{.00}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9}_{.01}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 8 2}_{.02}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 3}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 5}_{.03}$</td>
<td style="text-align: center;">$0.86_{.03}$</td>
<td style="text-align: center;">$0.91_{.02}$</td>
<td style="text-align: center;">$0.96_{.02}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">Credit-g</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.51_{.07}$</td>
<td style="text-align: center;">$0.59_{.05}$</td>
<td style="text-align: center;">$0.66_{.03}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.68_{.02}$</td>
<td style="text-align: center;">$0.73_{.02}$</td>
<td style="text-align: center;">$0.75_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8}_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.58_{.08}$</td>
<td style="text-align: center;">$0.59_{.03}$</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.69_{.07}$</td>
<td style="text-align: center;">$0.70_{.07}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2}_{.06}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}_{.04}$</td>
<td style="text-align: center;">$0.75_{.02}$</td>
<td style="text-align: center;">$0.75_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 5 3}_{.05}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 6}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 6}_{.05}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2}_{.06}$</td>
<td style="text-align: center;">$0.70_{.07}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$0.72_{.03}$</td>
<td style="text-align: center;">$0.72_{.02}$</td>
<td style="text-align: center;">$0.70_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;">Diabetes</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.59_{.16}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2}_{.07}$</td>
<td style="text-align: center;">$0.69_{.08}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.78_{.05}$</td>
<td style="text-align: center;">$0.80_{.01}$</td>
<td style="text-align: center;">$0.80_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.61_{.13}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}_{.11}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 7}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 2}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 3}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 3}_{.03}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 1}_{.02}$</td>
<td style="text-align: center;">$0.81_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 6 8}_{.06}$</td>
<td style="text-align: center;">$0.61_{.09}$</td>
<td style="text-align: center;">$0.63_{.08}$</td>
<td style="text-align: center;">$0.69_{.07}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.79_{.04}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.78_{.04}$</td>
<td style="text-align: center;">$0.80_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">Heart</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.55_{.14}$</td>
<td style="text-align: center;">$0.84_{.07}$</td>
<td style="text-align: center;">$0.88_{.04}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.92_{.01}$</td>
<td style="text-align: center;">$0.94_{.01}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.06}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8}_{.05}$</td>
<td style="text-align: center;">$0.87_{.06}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 1}_{.02}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2}_{.02}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2}_{.02}$</td>
<td style="text-align: center;">$0.92_{.01}$</td>
<td style="text-align: center;">$0.92_{.02}$</td>
<td style="text-align: center;">$0.92_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 5 4}_{.04}$</td>
<td style="text-align: center;">$0.76_{.14}$</td>
<td style="text-align: center;">$0.83_{.05}$</td>
<td style="text-align: center;">$0.87_{.04}$</td>
<td style="text-align: center;">$0.87_{.06}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.92_{.01}$</td>
<td style="text-align: center;">$0.92_{.01}$</td>
<td style="text-align: center;">$0.94_{.01}$</td>
</tr>
<tr>
<td style="text-align: center;">Income</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.59_{.06}$</td>
<td style="text-align: center;">$0.77_{.02}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
<td style="text-align: center;">$0.82_{.02}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.88_{.00}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 3}_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.73_{.08}$</td>
<td style="text-align: center;">$0.71_{.09}$</td>
<td style="text-align: center;">$0.76_{.09}$</td>
<td style="text-align: center;">$0.80_{.04}$</td>
<td style="text-align: center;">$0.82_{.04}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.86_{.01}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.89_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.00}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.02}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.04}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}_{.02}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6}_{.01}$</td>
<td style="text-align: center;">$0.87_{.00}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9}_{.01}$</td>
<td style="text-align: center;">$0.92_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">Jungle</td>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.58_{.07}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2}_{.05}$</td>
<td style="text-align: center;">$0.78_{.03}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.84_{.02}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.98_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{0 . 6 5}_{.08}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2}_{.04}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.81_{.01}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8}_{.01}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
<td style="text-align: center;">$0.93_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabLLM</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}_{.00}$</td>
<td style="text-align: center;">$0.64_{.01}$</td>
<td style="text-align: center;">$0.64_{.02}$</td>
<td style="text-align: center;">$0.65_{.03}$</td>
<td style="text-align: center;">$0.71_{.02}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.89_{.01}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0} \dagger$</td>
</tr>
</tbody>
</table>
<p>$\dagger$ These experiments were only performed for a single run due to runtime limitations of TabLLM on the full dataset.</p>
<p>Table 2: Five highest and lowest weighted features for zero-shot TabLLM and logistic regression (LR) trained on all data for Income. Both models show very similar trends for important features.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: center;">TabLLM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">rank</td>
<td style="text-align: center;">weight</td>
<td style="text-align: center;">rank</td>
<td style="text-align: center;">weight</td>
</tr>
<tr>
<td style="text-align: left;">capital_gain</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5.310</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2.393</td>
</tr>
<tr>
<td style="text-align: left;">education_Masters</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4.623</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1.455</td>
</tr>
<tr>
<td style="text-align: left;">education_Doctorate</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3.410</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2.066</td>
</tr>
<tr>
<td style="text-align: left;">education_Bachelors</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2.995</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1.135</td>
</tr>
<tr>
<td style="text-align: left;">education_Prof-school</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2.949</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1.900</td>
</tr>
<tr>
<td style="text-align: left;">occupation_Priv-house-serv</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">-2.840</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">-1.909</td>
</tr>
<tr>
<td style="text-align: left;">education_12th</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">-3.178</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">-0.480</td>
</tr>
<tr>
<td style="text-align: left;">education_Preschool</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">-3.520</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">-2.385</td>
</tr>
<tr>
<td style="text-align: left;">occupation_Farming-fishing</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">-3.853</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">-0.982</td>
</tr>
<tr>
<td style="text-align: left;">workclass_Without-pay</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">-4.423</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">-0.174</td>
</tr>
</tbody>
</table>
<p>was on par or very close to the best baseline models on the full datasets, indicating that there is little performance lost due to the serialization and the choice of model family.</p>
<h2>Introspecting TabLLM—What Prior Knowledge Does</h2>
<p>it Use? Given the strong zero-shot performance of TabLLM on the Income dataset, we next sought to understand which features it based its predictions on in order to shed light on the prior knowledge used by the LLM. To determine the feature importance for TabLLM, we fit a LR model to the zero-shot prediction using the original features as covariates as described in Sec. 6 in the Supplement. Highly weighted features (see Table 2) for zero-shot TabLLM include the individual's occupation (with e.g., 'Farming-fishing' having a large negative weight), highest education level ('Masters' and 'Doctorate' have positive weights; 'Preschool' grade has a negative weight), and workclass ('Without-pay' has a negative weight). TabLLM also seems to be able to correctly interpret the numerically encoded capital gain value. For comparison, we also show the feature weights for a LR model trained on all data. We see a strong concordance between both models; TabLLM's top five features are all among the top seven of the LR model. However, TabLLM scores the highest education</p>
<p>Table 3: Test AUC on the healthcare claims dataset. TabLLM outperforms logistic regression (LR) for up to 64 and LightGBM for up 256 training examples on End of Life (EoL). Standard deviations are given across five random seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Number of Shots</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">$\mathbf{0}$</td>
<td style="text-align: left;">$\mathbf{1 6}$</td>
<td style="text-align: left;">$\mathbf{6 4}$</td>
<td style="text-align: left;">$\mathbf{2 5 6}$</td>
<td style="text-align: left;">$\mathbf{1 , 0 2 4}$</td>
<td style="text-align: left;">$\mathbf{4 , 0 9 6}$</td>
<td style="text-align: left;">$\mathbf{1 6 , 3 8 4}$</td>
<td style="text-align: left;">all</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LR</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$0.65_{.07}$</td>
<td style="text-align: left;">$0.77_{.02}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 0}_{.02}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 3}_{.01}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 3}_{.01}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 4}_{.01}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 4}_{.01}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LightGBM</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$0.50_{.00}$</td>
<td style="text-align: left;">$0.71_{.01}$</td>
<td style="text-align: left;">$0.76_{.02}$</td>
<td style="text-align: left;">$0.80_{.01}$</td>
<td style="text-align: left;">$0.82_{.01}$</td>
<td style="text-align: left;">$0.83_{.01}$</td>
<td style="text-align: left;">$0.82 \dagger$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TabLLM</td>
<td style="text-align: left;">$\mathbf{0 . 7 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 8}$</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">0.79</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LR</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$0.72_{.04}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 5}_{.05}$</td>
<td style="text-align: left;">$0.77_{.01}$</td>
<td style="text-align: left;">$0.79_{.01}$</td>
<td style="text-align: left;">$0.80_{.01}$</td>
<td style="text-align: left;">$0.80_{.00}$</td>
<td style="text-align: left;">$0.81_{.00}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LightGBM</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$0.50_{.00}$</td>
<td style="text-align: left;">$0.73_{.02}$</td>
<td style="text-align: left;">$0.77_{.01}$</td>
<td style="text-align: left;">$0.79_{.01}$</td>
<td style="text-align: left;">$0.80_{.00}$</td>
<td style="text-align: left;">$0.81_{.01}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 2} \dagger$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TabLLM</td>
<td style="text-align: left;">$\mathbf{0 . 6 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 3}$</td>
<td style="text-align: left;">0.72</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">0.79</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LR</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$0.72_{.04}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 6}_{.03}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 0}_{.01}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 2}_{.01}$</td>
<td style="text-align: left;">$0.83_{.01}$</td>
<td style="text-align: left;">$0.83_{.01}$</td>
<td style="text-align: left;">$0.84_{.01}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LightGBM</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$0.50_{.00}$</td>
<td style="text-align: left;">$0.72_{.02}$</td>
<td style="text-align: left;">$0.76_{.03}$</td>
<td style="text-align: left;">$0.81_{.01}$</td>
<td style="text-align: left;">$0.83_{.00}$</td>
<td style="text-align: left;">$0.83_{.01}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 5} \dagger$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TabLLM</td>
<td style="text-align: left;">$\mathbf{0 . 7 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 3}$</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.82</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>$\dagger$ These experiments were only performed for a single run due to runtime limitations on the full dataset.
Table 4: Five highest and lowest weighted features for zero-shot TabLLM for EoL and their relative risk (RR) with confidence intervals (CI). The top five features show a significant increase of the relative risk.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: center;">TabLLM</th>
<th style="text-align: center;">RR (95\% CI)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">atrial fibrillation</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">$2.72(2.51-2.95)$</td>
</tr>
<tr>
<td style="text-align: left;">atherosclerosis of coronary art...</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">$2.10(1.94-2.27)$</td>
</tr>
<tr>
<td style="text-align: left;">atherosclerosis of aorta</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">$1.99(1.81-2.19)$</td>
</tr>
<tr>
<td style="text-align: left;">exudative age-related macular d...</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">$2.38(2.06-2.75)$</td>
</tr>
<tr>
<td style="text-align: left;">sex_male</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">$1.23(1.14-1.33)$</td>
</tr>
<tr>
<td style="text-align: left;">- - - - - - - - - - - - - - - -</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</td>
</tr>
</tbody>
</table>
<p>ever, all serializations with less information came close to the best serialization for 256 (tabular datasets) to 1024 training examples (insurance dataset). Hence, when hundreds of training examples are available, the input format proved less relevant, and the LLM was able to adapt (Jin et al., 2022). Like our results, Bertsimas et al. (2022) found that natural language representation of healthcare data gave little-to-no improvement (in their different setup) compared to a more straightforward serialization in the medium-shot setting. Our findings also support prior work showing that irrelevant and even misleading inputs can lead to similar few-shot performance (Min et al., 2022; Webson and Pavlick, 2022; Reynolds and McDonell, 2021). For instance, permuting the column names only showed a difference for up to 16 training examples (see Figure 2).</p>
<p>We found clear performance improvements for TabLLM when using additional training examples. It often outperformed strong baseline models in the very-few-shot setting. This emphasizes the value of leveraging LLMs when only little labeled data is available. Surprisingly, Dinh et al. (2022) could not confirm these findings for GPT-3. On two binary classification tasks a fine-tuned GPT-3 model performed worse than LR for up to 250 training examples. Our results indicate that the sample efficiency of TabLLM is highly task-dependent. The performance on Blood, Credit-g, Diabetes, and Heart is worse than the performance on Income and Car. Most features of the latter datasets have semantically meaningful textual values likely boosting TabLLM's performance. However, TabLLM also achieved reasonable results on numerical datasets (Blood, California, Diabetes, and Jungle). In addition, Diabetes and Heart have somewhat specialized feature names and values, such as "ventricular hypertrophy" and "Plasma glucose concentration," whereas Income and Car are more general-domain knowledge. This indicates that T0, the language model we used in TabLLM, seems to have less prior knowledge about medicine than about general-domain concepts. Indeed, the training tasks for T0 do not contain any tasks with medical data (Sanh et al., 2022).</p>
<p>Our findings on the three insurance claims datasets partly reinforce this hypothesis. Zero-shot performance depends on the concept selection strategy and the LLM seems to have little knowledge about medical procedures. Prior work has shown that medical-domain-specific language models, such as PubMedBERT, and general-domain models with medical data in their training sets, such as GPT3, perform well at downstream prediction tasks on medical data even with fairly few samples (Gu et al., 2021; Agrawal et al., 2022). Substituting T0 with one of these models in TabLLM to study medical predictions tasks is an interesting direction for future work.</p>
<p>Our results on the public Blood, Diabetes, and Heart datasets are very similar to our results for EoL, Surgery, and LoH, which are practically relevant but rely on pri-
vate data. Except for the zero-shot and very few-shot regime, other baselines tend to outperform TabLLM on these datasets. This suggests that Blood, Diabetes, and Heart datasets could be good proxies for the community to further study medical-domain tabular classification with LLMs without needing access to large private datasets.</p>
<h2>7 LIMITATIONS AND CONCLUSION</h2>
<p>TabLLM has a much larger computational footprint compared to traditional algorithms. It still requires fairly large GPUs to fine-tune the LLM, and inference with T0 requires far more FLOPs than inference with XGBoost or LR. Our results indicate that TabLLM trades off this computational efficiency for improved sample efficiency. Further, as we saw with the three healthcare claims tasks, performance may suffer if the dense feature set for a given row cannot fit within the token limit for a given LLM. Since the gains from TabLLM stem from its ability to use existing domain knowledge, the semantics of the column names and feature values need to have been observed during the LLM's original pre-training. For example, if the columns represent genes, we may not expect a vanilla LLM to have strong representations for gene names. Finally, due to dataset shift, the pre-training data for a given LLM may not necessarily reflect the settings under which a given table was aggregated, e.g., due to inflation and a changing value of money (see Sec. 5 in the Supplement).</p>
<p>Despite these limitations, our empirical results show that TabLLM enjoys strong performance at tabular classification, outperforming state-of-the-art baseline algorithms like XGBoost and SAINT by over 5 AUC points in the very-few-shot regime, all while staying competitive with these methods when a large number of samples is available.</p>
<p>Currently, TabLLM does not use any unlabeled data; a fruitful direction could involve leveraging unlabeled data, e.g., using the techniques from Lang et al. (2022) to combine the few-shot performance of TabLLM with the ultimate performance of tree-based baselines by co-training the models together. Other improvements could include more faithful LLM serializations as well as numericspecific encoding methods (Gorishniy et al., 2022).</p>
<h2>8 SOCIETAL IMPACT</h2>
<p>Similar to other ML systems that were trained on historic data, LLMs are prone to replicate existing biases and stereotypes. Hence, when applying TabLLM for sensitive tasks such as income or a health trajectory, predictions should be considered with great care and further analyses (e.g., for subgroups) are mandatory. In addition, LLMs require a lot of computing resources. This bears the risk of creating an exclusive research environment. Also, the environmental impact of LLMs can be significant.</p>
<h2>9 ACKNOWLEDGEMENTS</h2>
<p>SH was supported by the German Academic Exchange Service, HL by NSF AiTF award CCF-1723344, MA by a Takeda Fellowship, and DS, HL, AB, and SH in part by Independence Blue Cross. Thanks to Dr. Steven Horng for generously donating GPU-time on the BIDMC computing cluster (Horng, 2022) and to NVIDIA Corporation for their donation of two NVIDIA A100 GPUs used in this work.</p>
<h2>References</h2>
<p>Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., and Sontag, D. (2022). Large Language Models are ZeroShot Clinical Information Extractors. Technical Report arXiv:2205.12689, arXiv.
Arik, S. Ö. and Pfister, T. (2021). Tabnet: Attentive interpretable tabular learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8):6679-6687.
Avati, A., Jung, K., Harman, S., Downing, L., Ng, A., and Shah, N. H. (2018). Improving palliative care with deep learning. BMC medical informatics and decision making, 18(4):55-64.
Bach, S., Sanh, V., Yong, Z. X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Bendavid, S., Xu, C., Chhablani, G., Wang, H., Fries, J., Al-shaibani, M., Sharma, S., Thakker, U., Almubarak, K., Tang, X., Radev, D., Jiang, M. T.-j., and Rush, A. (2022). PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93-104, Dublin, Ireland. Association for Computational Linguistics.
Bahri, D., Jiang, H., Tay, Y., and Metzler, D. (2022). Scarf: Self-supervised contrastive learning using random feature corruption. In International Conference on Learning Representations.
Bertsimas, D., Carballo, K. V., Ma, Y., Na, L., Boussioux, L., Zeng, C., Soenksen, L. R., and Fuentes, I. (2022). Tabtext: a systematic approach to aggregate knowledge across tabular data structures. arXiv preprint arXiv:2206.10381.
Borisov, V., Leemann, T., Seßler, K., Haug, J., Pawelczyk, M., and Kasneci, G. (2022a). Deep Neural Networks and Tabular Data: A Survey. Technical Report arXiv:2110.01889, arXiv.
Borisov, V., Seßler, K., Leemann, T., Pawelczyk, M., and Kasneci, G. (2022b). Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.
Chen, T. and Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pages 785-794, New York, NY, USA. Association for Computing Machinery.
Cui, G., Hu, S., Ding, N., Huang, L., and Liu, Z. (2022). Prototypical verbalizer for prompt-based few-shot tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7014-7024, Dublin, Ireland. Association for Computational Linguistics.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Dinh, T., Zeng, Y., Zhang, R., Lin, Z., Gira, M., Rajput, S., yong Sohn, J., Papailiopoulos, D., and Lee, K. (2022). LIFT: Language-interfaced fine-tuning for non-language machine learning tasks. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K., editors, Advances in Neural Information Processing Systems.
Gorishniy, Y., Rubachev, I., and Babenko, A. (2022). On embeddings for numerical features in tabular deep learning. arXiv preprint arXiv:2203.05556.
Grinsztajn, L., Oyallon, E., and Varoquaux, G. (2022). Why do tree-based models still outperform deep learning on typical tabular data? In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.
Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., and Poon, H. (2021). Domainspecific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1-23.
Haendel, M., Vasilevsky, N., Unni, D., Bologa, C., Harris, N., Rehm, H., Hamosh, A., Baynam, G., Groza, T., McMurry, J., et al. (2020). How many rare diseases are there? Nature Reviews Drug Discovery, 19(2):77-78.</p>
<p>Harari, A. and Katz, G. (2022). Few-shot tabular data enrichment using fine-tuned transformer architectures. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1577-1591.
Hollmann, N., Müller, S., Eggensperger, K., and Hutter, F. (2022). Tabpfn: A transformer that solves small tabular classification problems in a second. arXiv preprint arXiv:2207.01848.
Horng, S. (2022). Machine Learning Core.
Huang, X., Khetan, A., Cvitkovic, M., and Karnin, Z. (2020). TabTransformer: Tabular Data Modeling Using Contextual Embeddings. Technical Report arXiv:2012.06678, arXiv.
Jin, W., Cheng, Y., Shen, Y., Chen, W., and Ren, X. (2022). A good prompt is worth millions of parameters? low-resource prompt-based learning for vision-language models. In ACL 2022.
Kadra, A., Lindauer, M., Hutter, F., and Grabocka, J. (2021). Well-tuned simple nets excel on tabular datasets. Advances in neural information processing systems, 34:23928-23941.</p>
<p>Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T.-Y. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.
Kodialam, R., Boiarsky, R., Lim, J., Sai, A., Dixit, N., and Sontag, D. (2021). Deep contextual clinical prediction with reverse distillation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(1):249-258.
Kontschieder, P., Fiterau, M., Criminisi, A., and Bulo, S. R. (2015). Deep Neural Decision Forests. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 1467-1475, Santiago, Chile. IEEE.
Lang, H., Agrawal, M. N., Kim, Y., and Sontag, D. (2022). Co-training improves prompt-based learning for large language models. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S., editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 11985-12003. PMLR.
Levin, R., Cherepanova, V., Schwarzschild, A., Bansal, A., Bruss, C. B., Goldstein, T., Wilson, A. G., and Goldblum, M. (2022). Transfer Learning with Deep Tabular Models. Technical Report arXiv:2206.15306, arXiv.
Li, Y., Li, J., Suhara, Y., Doan, A., and Tan, W.-C. (2020). Deep entity matching with pre-trained language models. Proc. VLDB Endow., 14(1):50-60.
Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. (2022). Few-Shot ParameterEfficient Fine-Tuning is Better and Cheaper than In-</p>
<p>Context Learning. arXiv:2205.05638 [cs]. arXiv: 2205.05638.</p>
<p>Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J. (2021). GPT Understands, Too. Technical Report arXiv:2103.10385, arXiv.
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.
Narayan, A., Chami, I., Orr, L., and Ré, C. (2022). Can Foundation Models Wrangle Your Data? Technical Report arXiv:2205.09911, arXiv.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow instructions with human feedback. arXiv:2203.02155 [cs]. arXiv: 2203.02155.
Perez, E., Kiela, D., and Cho, K. (2021). True few-shot learning with language models. Advances in Neural Information Processing Systems, 34:11054-11070.
Popov, S., Morozov, S., and Babenko, A. (2020). Neural oblivious decision ensembles for deep learning on tabular data. In International Conference on Learning Representations.
Reynolds, L. and McDonell, K. (2021). Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages $1-7$.</p>
<p>Sahakyan, M., Aung, Z., and Rahwan, T. (2021). Explainable artificial intelligence for tabular data: A survey. IEEE Access, 9:135392-135422.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf, T., and Rush, A. M. (2022). Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Schick, T. and Schütze, H. (2021). Exploiting ClozeQuestions for Few-Shot Text Classification and Natural Language Inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255269, Online. Association for Computational Linguistics.</p>
<p>Shwartz-Ziv, R. and Armon, A. (2022). Tabular data: Deep learning is not all you need. Information Fusion, 81.
Somepalli, G., Goldblum, M., Schwarzschild, A., Bruss, C. B., and Goldstein, T. (2021). SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training. Technical Report arXiv:2106.01342, arXiv.
Webson, A. and Pavlick, E. (2022). Do PromptBased Models Really Understand the Meaning of Their Prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300-2344, Seattle, United States. Association for Computational Linguistics.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs]. arXiv: 2201.11903.
Yin, P., Neubig, G., Yih, W.-t., and Riedel, S. (2020). TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413-8426, Online. Association for Computational Linguistics.
Yoon, J., Zhang, Y., Jordon, J., and van der Schaar, M. (2020). VIME: Extending the Success of Self- and Semisupervised Learning to Tabular Domain. In Advances in Neural Information Processing Systems, volume 33, pages 11033-11043. Curran Associates, Inc.
Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. (2021). Calibrate Before Use: Improving Few-shot Performance of Language Models. In Proceedings of the 38th International Conference on Machine Learning, pages 12697-12706. PMLR. ISSN: 2640-3498.</p>
<h1>Supplementary Materials: <br> TabLLM: Few-shot Classification of Tabular Data with Large Language Models</h1>
<h2>1 ADDITIONAL DATASET DETAILS</h2>
<h3>1.1 Public Tabular Datasets</h3>
<p>We systematically identified datasets for classification from Kadra et al. (2021), Grinsztajn et al. (2022), Borisov et al. (2022a), and from Kaggle. Each dataset was separated into $80 / 20$ train-test splits. The $k$ labeled examples $\mathcal{D}_{k}$ were sampled in a class-balanced manner from the training set. We performed experiments for different numbers of trainings examples (shots) ranging from 0 to 512 and the entire dataset (all). To characterize the sensitivity of models to the choice of $k$ labeled examples, we repeated the dataset splitting and sampling procedures for five different seeds and report the mean AUC and standard deviation (SD) across seeds. No hyperparameter tuning was conducted for TabLLM; for baselines, internal cross validation was conducted to choose optimal hyperparameters, and the model was then retrained on all data. We analyzed the following datasets:</p>
<ul>
<li>Bank (Kadra et al., 2021) contains information of a direct marketing campaign from a Portugese banking institution (Moro et al., 2014). The goal is to predict whether a customer subscribed to a term deposit or not. It consists of 45,211 rows and 16 features; 5,289 labels are positive.</li>
<li>Blood (Kadra et al., 2021) consists of data of a blood transfusion service from Taiwan (Yeh et al., 2009). It contains 4 attributes of 748 donors and the label is representing whether they returned for another donation ( 178 positive).</li>
<li>California (Grinsztajn et al., 2022) contains eight attributes of 20,640 districts in California and the goal is to predict the median house value in each district (Pace and Barry, 1997). Analogously to Grinsztajn et al. (2022), we created a balanced classification task by predicting whether the house value is below or above the median (10,317 positive).</li>
<li>Car (Kadra et al., 2021) has entries for different cars that are characterized by six attributes; the task is a multiclass classification problem evaluating the state of each car. The dataset contains 1,728 rows, and the four classes have a distribution of $1210,384,65$, and 69 examples.</li>
<li>Credit-g (Kadra et al., 2021) describes 1,000 people from Germany that want to receive a credit using 20 attributes. The label is to predict whether they have good or bad risk; 700 are classified as good.</li>
<li>Diabetes (from Kaggle ${ }^{2}$ ) was collected by the National Institute of Diabetes and Digestive and Kidney Diseases (Smith et al., 1988) and contains 768 rows, each corresponding to women of Pima Indian heritage with eight clinical variables. The task is binary classification of whether a person has diabetes; 268 cases are positive.</li>
<li>Heart (from Kaggle ${ }^{3}$ ) contains data of four different hospitals (Detrano et al., 1989). Each row contains 11 clinical variables of a patient. The task is binary classification of coronary artery disease. Of the 918 patients, 508 are positive.</li>
<li>Income (Kadra et al., 2021; Borisov et al., 2022a) also called Adult contains rows for 48,842 individuals with twelve attributes collected in the 1994 U.S. Census (Kohavi et al., 1996; Dua and Graff, 2017). The task is to predict whether each person has an annual income over $\$ 50,000$. The dataset has 11,687 positive labels.</li>
<li>Jungle (Kadra et al., 2021) is a collection of 44,819 end game positions of Jungle Chess (van Rijn and Vis, 2014). Each game is described with 6 attributes and the goal is to predict whether the white player will win (23,062 positive).</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Evaluation of different concept selection methods for the healthcare claims dataset in the zero-shot setting. The last two rows show the performance when concepts where selected based on the lasso path of logistic regression weights, which violates the zero-shot assumption (*).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">EoL</th>
<th style="text-align: center;">Surgery</th>
<th style="text-align: center;">LoH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Age, sex, and race</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">Least frequent conditions</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Least frequent procedures</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">Least frequent concepts (cond. + proc.)</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: left;">Most frequent conditions</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Most frequent procedures</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">Most frequent concepts (cond. + proc.)</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">Oldest conditions</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Oldest procedures</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">Oldest concepts (cond. + proc.)</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Most recent conditions</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Most recent procedures</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">Most recent concepts (cond. + proc.)</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: left;">Most relevant concepts based on 256 shots*</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Most relevant concepts based on 4096 shots*</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.68</td>
</tr>
</tbody>
</table>
<h1>1.2 Large Healthcare Claims Dataset</h1>
<p>The de-identified health claims data set was provided by a large U.S. health insurer. The data is stored in the Observational Medical Outcomes Partnership (OMOP) Common Data Model version 6.0 (Hripcsak et al., 2015). It contains an entry for every encounter a patient has with the health system. Each entry is associated with a date, a visit type ( 5 total), a medical specialty ( 216 total), present conditions ( 14,095 total), and performed procedures ( 21,184 total). We additionally used the static concepts age, sex, and race at time of prediction.</p>
<p>We studied three different tasks on this dataset with distinct cohorts. For all tasks, we used a six month outcome period and a gap of three months between time of prediction and the outcome window to prevent data leakage. We required patients to have at least one medical visit and to have been actively enrolled in an insurance plan for at least $95 \%$ of the last year and the six month outcome window. We used $10 \%$ of the data as a holdout set and sampled the $k$ balanced shots with replacement from the remaining data. We chose larger shot sizes, as the tasks are more complex. We only ran the experiments for a single seed due to runtime limitations. We considered the following tasks:</p>
<ul>
<li>End of Life (EoL): We predicted the mortality of all patients older than 70 years. This is often used as a surrogate task. For instance, it can improve initiation of palliative care (Avati et al., 2018) and can help to inform close relatives to reduce family distress (Curtis et al., 2016). The final cohort contained 94,972 individuals; 2,424 were positive.</li>
<li>Surgical Procedure (Surgery): We predicted the need for any surgical procedure. The task is important in determining health care needs and estimating costs. The cohort included 620,382 people of which 243,349 were positive.</li>
<li>Likelihood of Hospitalization (LoH): We also predicted the likelihood of being hospitalized. Again, this information can help identify needs and estimate costs. The cohort included 612,656 individuals; 22,427 were positive.</li>
</ul>
<h3>1.2.1 More Details on the Serialization</h3>
<p>Each serialization begins with the patient's age, sex, and race. For each concept entry that we included, we also added information of the associated visit. This included its date, the type of doctor the patient saw (e.g., dermatology), if an outpatient visit or length of hospitalization if an inpatient visit, and the primary complaint of the associated visit. If a visit was already added to the serialization, we just added the concept to the existing visit entry. For the List Template and Text Template serializations approximately 40 medical concepts could be added until the token limit of T0 was reached. To explore the effect of fewer information in the input, we also tested the List Short serializations were we added only 10 medical concepts to the serialization. Hence, not the entire token limit of the LLM was used. Examples of the List Template, Text Template and List Permuted Names serializations illustrating this structure are given in Sec. 9.1 at the end of the Supplement.</p>
<p>Table 6: Five examples of different concept names for conditions. The first column shows the original name in the healthcare claims dataset using SNOMED codes. A dash illustrates that no mapping was available.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Original name</th>
<th style="text-align: left;">ICD</th>
<th style="text-align: left;">MEDCIN</th>
<th style="text-align: left;">CHV</th>
<th style="text-align: left;">Simplify (GPT-3)</th>
<th style="text-align: left;">Jargon (GPT-3)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seasonal allergic <br> rhinitis</td>
<td style="text-align: left;">Allergic rhinitis due <br> to pollen</td>
<td style="text-align: left;">hay fever</td>
<td style="text-align: left;">hay fever</td>
<td style="text-align: left;">Allergies</td>
<td style="text-align: left;">Seasonal allergic <br> rhinitis</td>
</tr>
<tr>
<td style="text-align: left;">Disturbance in <br> speech</td>
<td style="text-align: left;">Unspecified speech <br> disturbances</td>
<td style="text-align: left;">speech difficulties</td>
<td style="text-align: left;">speech impairment</td>
<td style="text-align: left;">Speech problems</td>
<td style="text-align: left;">Dysarthria</td>
</tr>
<tr>
<td style="text-align: left;">Congenital <br> duplication of <br> cervix</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">double cervix</td>
<td style="text-align: left;">Double cervix</td>
<td style="text-align: left;">Congenital <br> duplication of the <br> cervix</td>
</tr>
<tr>
<td style="text-align: left;">Hypertensive <br> retinopathy</td>
<td style="text-align: left;">Hypertensive <br> retinopathy</td>
<td style="text-align: left;">hypertensive <br> retinopathy</td>
<td style="text-align: left;">hypertensive <br> retinopathy</td>
<td style="text-align: left;">High blood pressure <br> affecting the retina</td>
<td style="text-align: left;">Retinopathy h-tensa</td>
</tr>
<tr>
<td style="text-align: left;">Malignant <br> neoplasm of liver</td>
<td style="text-align: left;">Malignant <br> neoplasm of liver, <br> unspecified</td>
<td style="text-align: left;">malignant neoplasm <br> of liver</td>
<td style="text-align: left;">liver cancer</td>
<td style="text-align: left;">Liver cancer</td>
<td style="text-align: left;">Hepato-ca</td>
</tr>
</tbody>
</table>
<p>Table 7: Evaluation of alternative condition concepts names. International Classification of Diseases (ICD), MEDCIN and the Consumer Health Vocabulary (CHV) are alternative medical terminologies. We also tested shortening, simplifying, and rewriting concepts as medical jargon via GPT-3. None of the alternative concept names showed consistent performance improvement.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">EoL</th>
<th style="text-align: left;">Surgery</th>
<th style="text-align: left;">LoH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original concept names (SNOMED)</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Map to ICD concept names</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.68</td>
</tr>
<tr>
<td style="text-align: left;">Map to MEDCIN concept names</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Map to CHV concept names</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Shorten longs concepts with GPT-3</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.69</td>
</tr>
<tr>
<td style="text-align: left;">Simplify concepts with GPT-3</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">Medical jargon with GPT-3</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.70</td>
</tr>
</tbody>
</table>
<h1>1.2.2 Concept Selection</h1>
<p>For the healthcare claims dataset, the number of recorded medical concepts per patients usually exceeded T0's token limit. Hence, we had to determine which concepts of a patient should be included during the serialization. We evaluated four different concept selection strategies in the zero-shot setting for the List Template serialization. Choosing the least frequent, most frequent, oldest, or most recent concepts per patient. We tested these for all concepts (conditions and procedures), only conditions, or only procedures. For each patient, we ranked all concepts according to one of the above methods and added concepts until the token limit of the LLM was reached. For least frequent and most frequent, we used the earliest visits associated with the selected medical concepts. We used a simple serialization that only contained the patient's age, sex, and race as a baseline for our experiments. We also tested concept selection based on the lasso path of a logistic regression model determined on 256 and 4,096 shots. This violates the few-shot assumption, but we considered it an interesting comparison with the other strategies that select concepts per patient.
The results are given in Table 5. Using the most frequent conditions per patient consistently outperformed all other selection strategies. Frequent conditions might be useful since they reveal the most relevant condition of a patient. Also, they are usually more common allowing more prior knowledge of the LLM. Across all strategies conditions were usually more useful than procedures. This suggests more prior knowledge of conditions. Interestingly, selecting the most frequent conditions is even better than using the concept weights of a LR model trained on 256 or 4,096 shots.</p>
<h3>1.2.3 Alternative Concept Names</h3>
<p>The healthcare claims dataset used SNOMED concept names for conditions and SNOMED, Healthcare Common Procedure Coding System (HCPCS), International Classification of Diseases (ICD), and Current Procedural Terminology (CPT) concept names for procedures. We tested different concept names to assess their effect on the performance. We used a zero-</p>
<p>Table 8: Hyperparameters for LR model.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>penalty</td>
<td>' 11 ', ' 12 '</td>
</tr>
<tr>
<td>C</td>
<td>$100,10,1,1 \mathrm{e}-1,1 \mathrm{e}-2,1 \mathrm{e}-3,1 \mathrm{e}-4,1 \mathrm{e}-5$</td>
</tr>
</tbody>
</table>
<p>Table 9: Hyperparameters for LightGBM model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">num_leaves</td>
<td style="text-align: left;">$2,4,8,16,32,64,128,256,512,1024,2048,4096$</td>
</tr>
<tr>
<td style="text-align: left;">lambda_l1</td>
<td style="text-align: left;">$1 \mathrm{e}-8,1 \mathrm{e}-7,1 \mathrm{e}-6,1 \mathrm{e}-5,1 \mathrm{e}-4,1 \mathrm{e}-3,1 \mathrm{e}-2,1 \mathrm{e}-1,1 ., 10$.</td>
</tr>
<tr>
<td style="text-align: left;">lambda_l2</td>
<td style="text-align: left;">$1 \mathrm{e}-8,1 \mathrm{e}-7,1 \mathrm{e}-6,1 \mathrm{e}-5,1 \mathrm{e}-4,1 \mathrm{e}-3,1 \mathrm{e}-2,1 \mathrm{e}-1,1 ., 10$.</td>
</tr>
<tr>
<td style="text-align: left;">learning_rate</td>
<td style="text-align: left;">$0.01,0.03,0.1,0.3$</td>
</tr>
</tbody>
</table>
<p>Table 10: Hyperparameters for XGBoost model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">max_depth</td>
<td style="text-align: left;">$2,4,6,8,10,12$</td>
</tr>
<tr>
<td style="text-align: left;">lambda_l1</td>
<td style="text-align: left;">$1 \mathrm{e}-8,1 \mathrm{e}-7,1 \mathrm{e}-6,1 \mathrm{e}-5,1 \mathrm{e}-4,1 \mathrm{e}-3,1 \mathrm{e}-2,1 \mathrm{e}-1,1$.</td>
</tr>
<tr>
<td style="text-align: left;">lambda_l2</td>
<td style="text-align: left;">$1 \mathrm{e}-8,1 \mathrm{e}-7,1 \mathrm{e}-6,1 \mathrm{e}-5,1 \mathrm{e}-4,1 \mathrm{e}-3,1 \mathrm{e}-2,1 \mathrm{e}-1,1$.</td>
</tr>
<tr>
<td style="text-align: left;">eta</td>
<td style="text-align: left;">$0.01,0.03,0.1,0.3$</td>
</tr>
</tbody>
</table>
<p>shot setting with the List Template serialization and the most frequent conditions per patient as the best selection strategy determined as described above. Since the selection method only considered conditions, we only used different condition names. We considered three alternative vocabularies in the Unified Medical Language System (UMLS) that covered at least $20 \%$ of the condition concepts and offered different names. ICD is a very common medical terminology offering alternative names for conditions. MEDCIN and the Consumer Health Vocabulary (CHV) offer concept names specifically targeted at clinicians or consumers. We mapped the concept via their UMLS identifier. For ICD we were able to map 7,372, for MEDCIN 9,370 and for CHV 3,700 of the 14,095 condition concepts. Alternatively, we explored concept names generated by GPT-3 (Brown et al., 2020). To do so, we used the publicly accessible GPT-3 API (engine text-davinci002) (Ouyang et al., 2022). We considered shortened names for concepts with more than sixty character ("Rewrite this medical condition with at most six words."), simplified concept names ("Write this medical condition in a short form in lay language.") and medical jargon ("Write this medical condition in medical jargon."). For the simplified names and the medical jargon, we provided GPT-3 with a single example for in-context learning. Examples for all alternative concept names except the shortening are given in Table 6.</p>
<p>The results of this experiment are given in Table 7. We used the most frequent concept as a concept selection methods. Based on the best concept selection, we performed additional experiments for alternative concept names. We found no consistent performance difference even though there were considerable differences in the concept names (see Table 6). Surprisingly, TabLLM performs better for EoL and Surgery using medical jargon to encode concepts.</p>
<h1>2 RUNTIME ESTIMATES FOR TABLLM</h1>
<p>The TabLLM training time on the Income dataset for 64 training examples and 30 epochs with a batch size of 8 was less than 3 minutes. The average inference time for the test set of 10,000 examples with a batch size of 16 was 2 minutes, around 12 ms per example. The training and inference times for the other public datasets were comparable. Due to the larger size of the healthcare claims dataset, it took nearly 4 minutes to train for 64 examples and 10 epochs for EoL and was similar for the other two tasks. Inference took approximately 14 minutes for 10,000 examples with a batch size of 16, i.e. around 84 ms per example. The training times scaled linearly in the shot size.</p>
<h2>3 PARAMETER TUNING FOR BASELINES</h2>
<p>We used the scikit-learn framework to perform cross-validation and parameter tuning for the LR and the tree-based models (Pedregosa et al., 2011). For LR we tried common parameters for the penalty term and regularization strength (see Table 8). We used the same LR parameters for the public tabular datasets and the healthcare claims dataset. For the tree-based models we adopted the hyperparameter ranges from Borisov et al. (2022a) and Grinsztajn et al. (2022). We discretized the</p>
<p>parameter ranges and performed a complete grid search (see Tables 9 and 10).
For the neural baselines SAINT, TabNet, and NODE, we used the setup and suggested hyperparameter ranges in Borisov et al. (2022a). We modified the open-source implementation of these methods ${ }^{4}$ to support ingestion of the nine public tabular datasets. We used the hyperparameter-tuning framework Optuna ${ }^{5}$ and selected parameters that maximize AUCROC across folds. Note that for the 4 -shot setting of the Car dataset, AUC may not be defined if the selected validation set includes only one label; in this case we used accuracy as our validation metric but report AUC-ROC on the holdout test set. Each neural baseline model was run for 20 trials with Optuna and trained for 100 epochs per hyperparameter settings.</p>
<h1>4 COMPARING BASELINE RESULTS TO THE LITERATURE</h1>
<p>To assess whether our baseline results match results reported in the literature, we report studies that used the same models.
Bank Dataset. Kadra et al. (2021) trained a XGBoost, TabNet, and NODE baseline on this dataset and achieved a balanced accuracy of $72.7,70.6$, and 74.6 . Our experiments for a set of 512 balanced training examples ( 512 shots) show a better performance for XGBoost than NODE.</p>
<p>Blood Dataset. The XGBoost, TabNet, and NODE baselines trained in Kadra et al. (2021) achieved a balanced accuracy of $62.3,64.3,50$. Our results for a set of 512 balanced training examples ( 512 shots) also show a better performance for TabNet than XGBoost. However, in our experiments NODE performs better than XGBoost and not worse.</p>
<p>California Dataset. Borisov et al. (2022a) trained a Linear Model, XGBoost, LightGBM, TabNet, NODE, and SAINT baseline on a regression version of the dataset. They achieved a mean squared error of $0.53,0.21,0.20,0.35,0.28$, and 0.23 . Our experiments for a set of 512 balanced training examples ( 512 shots) show a better performance for XGBoost than LightGBM and the same performance for TabNet and NODE. Also, our linear model performs much better which is probably due to more extensive hyperparameter tuning.</p>
<p>Car Dataset. The XGBoost, TabNet, and NODE models in Kadra et al. (2021) showed a balanced accuracy of 92.4, 98.7, and 46.1. In our experiments, XGBoost and TabNet performed very similar for many training examples and NODE was only slightly inferior.</p>
<p>Credit-g Dataset. The XGBoost, TabNet, and NODE baselines trained in Kadra et al. (2021) achieved a balanced accuracy of $68.9,61.2$, and 73.1 . Our AUC results cannot easily be compared but our experiments for 512 balanced training examples ( 512 shots) follow the same trend.</p>
<p>Diabetes Dataset. Hasan et al. (2020) reported an AUC of $0.828(0.030)$ for XGBoost on the diabetes dataset, which matches our findings. With additional feature selection and preprocessing methods they reached an AUC of $0.946(0.020)$ with XGBoost, but this was out of the scope of our work. XGBoost was the most performant model that they included in their experiments.</p>
<p>Heart Dataset. Muhammad et al. (2020) used only the 303 instances from the Cleveland cohort, while we combined all four sub-cohorts. They achieved an AUC of 0.923 with LR, which is close to our results on all sub-cohorts. They also tested several models that outperformed LR.</p>
<p>Income Dataset. Many studies used the Income or Adult dataset. The review Borisov et al. (2022a) included several of our baselines. They reported an AUC of $0.854(0.002)$ for a linear model, $0.928(0.001)$ for XGBoost, $0.928(0.001)$ for LightGBM, $0.916(0.002)$ for SAINT, $0.911(0.001)$ for TabNet, and $0.911(0.002)$ for NODE. These are in accordance with our results. We reckon the better performance of our LR model is due to more extensive parameter tuning.</p>
<p>Jungle Dataset. The XGBoost and TabNet baselines trained in Kadra et al. (2021) achieved a balanced accuracy of 87.3 and 73.4. They did not train a NODE moel for this dataset. The results follows the same trend as our experiments for a set of 512 balanced training examples ( 512 shots).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 11: The mean performance for one prompt (ours, SD over five seed omitted) and the mean performance and SD across five different prompts (each again over five seeds).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Bank</th>
<th style="text-align: left;">Blood</th>
<th style="text-align: left;">California</th>
<th style="text-align: left;">Car</th>
<th style="text-align: left;">Credit-g</th>
<th style="text-align: left;">Diabetes</th>
<th style="text-align: left;">Heart</th>
<th style="text-align: left;">Income</th>
<th style="text-align: left;">Jungle</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TabLLM 0-shot: 1 prompt (ours)</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.53</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">0.54</td>
<td style="text-align: left;">0.84</td>
<td style="text-align: left;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">TabLLM 0-shot: avg. 5 prompts</td>
<td style="text-align: left;">$0.64_{.01}$</td>
<td style="text-align: left;">$0.60_{.02}$</td>
<td style="text-align: left;">$0.59_{.01}$</td>
<td style="text-align: left;">$0.80_{.01}$</td>
<td style="text-align: left;">$0.52_{.01}$</td>
<td style="text-align: left;">$0.67_{.01}$</td>
<td style="text-align: left;">$0.55_{.04}$</td>
<td style="text-align: left;">$0.84_{.01}$</td>
<td style="text-align: left;">$0.60_{.00}$</td>
</tr>
</tbody>
</table>
<h1>5 ADJUSTING INCOME DATASET FOR INFLATION</h1>
<p>We wanted to investigate how a distribution shift caused by inflation affects the zero-shot performance of TabLLM. The Income dataset was collected in 1994, and the label and two features (capital gain/loss in last year) contain dollar values. T0 was trained in 2021 (Sanh et al., 2022), and we assumed that the training data is much more recent than the Income dataset. The inflation rate from 1994 to 2021 is $1.79^{6}$. Without inflation correction the zero-shot results were 0.80 (0.01). Correcting the two features, correcting only the prompt, and correcting both all yielded the same performance as the uncorrected one. The accuracy values also remained the same with the inflation correction.</p>
<h2>6 FEATURE IMPORTANCE ANALYSIS OF TABLLM</h2>
<p>We wanted to understand which features were most important for the zero-shot performance of TabLLM on Income and EoL. To this end, we used zero-shot TabLLM with the List Template serialization to predict the label probability of all examples in the dataset. We then used 4 -fold cross validation to fit a L2-regularized LR model to the predicted label using the features in the serialization as covariates. For EoL, we used age, sex, race, and the conditions as inputs, which summed up to 14,105 features.</p>
<p>For Income we compared these approximated importance scores to the feature coefficients of a LR model trained on all data for a single seed (Table 16). We used the same setup for the LR model as for our main experiments. We did 4 -fold cross validation on an $80 \%$ training split to choose hyperparameters, and then refit the model using all training data. The best parameters of the LR model for Income were a ' 11 ' penalty and a regularization constant of 1 . For EoL, we decided that the LR model coefficients did not provide a good estimate of the ground truth due to the vast amount of features and possible collinearities in the data. Instead, we provide the relative risk (RR) with $95 \%$ confidence intervals (CI) treating the occurrence of a feature as an intervention. We report the 50 most and least important features of TabLLM in Table 17.</p>
<h2>7 EFFECT OF USING DIFFERENT PROMPTS</h2>
<p>To evaluate the effect of using a different prompt we considered the zero-shot setting, since even few training examples mostly cancel the effect. For all datasets we constructed five different prompts that contained the same question, e.g., "Does this person earn a lot of money?" instead of "Does this person earn more than 50000 dollars per year?" for the Income dataset. The results are summarized in Table 11. The effects were relative small ranging from a standard deviation of 0.00 for Jungle to 0.04 for Heart across the five prompts. This suggests that TabLLM is not very sensitive to using different prompts.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 12: Test AUC performance of competing methods on public tabular datasets. Each column reports the $k$-shot performance for different values of $k$. Standard deviations across five random seeds are shown as subscripts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of Shots</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">all</td>
</tr>
<tr>
<td style="text-align: center;">Bank Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.55_{.09}$</td>
<td style="text-align: center;">$0.66_{.09}$</td>
<td style="text-align: center;">$0.75_{.06}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.84_{.02}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.89_{.00}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.51_{.02}$</td>
<td style="text-align: center;">$0.60_{.12}$</td>
<td style="text-align: center;">$0.68_{.09}$</td>
<td style="text-align: center;">$0.78_{.04}$</td>
<td style="text-align: center;">$0.82_{.01}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.86_{.01}$</td>
<td style="text-align: center;">$0.87_{.00}$</td>
<td style="text-align: center;">$0.88_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.77_{.03}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.89_{.00}$</td>
<td style="text-align: center;">$0.94_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.78_{.03}$</td>
<td style="text-align: center;">$0.84_{.02}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.89_{.00}$</td>
<td style="text-align: center;">$0.94_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.56_{.09}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
<td style="text-align: center;">$0.85_{.03}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.94_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.56_{.09}$</td>
<td style="text-align: center;">$0.69_{.05}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.82_{.02}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.89_{.00}$</td>
<td style="text-align: center;">$0.93_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.51_{.10}$</td>
<td style="text-align: center;">$0.61_{.11}$</td>
<td style="text-align: center;">$0.70_{.04}$</td>
<td style="text-align: center;">$0.77_{.03}$</td>
<td style="text-align: center;">$0.81_{.03}$</td>
<td style="text-align: center;">$0.85_{.02}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.93_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.51_{.06}$</td>
<td style="text-align: center;">$0.58_{.05}$</td>
<td style="text-align: center;">$0.64_{.10}$</td>
<td style="text-align: center;">$0.62_{.04}$</td>
<td style="text-align: center;">$0.71_{.06}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.80_{.04}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
<td style="text-align: center;">$0.93_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.52_{.02}$</td>
<td style="text-align: center;">$0.55_{.06}$</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.73_{.06}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
<td style="text-align: center;">$0.85_{.01}$</td>
<td style="text-align: center;">$0.86_{.01}$</td>
<td style="text-align: center;">$0.76_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.59_{.14}$</td>
<td style="text-align: center;">$0.66_{.08}$</td>
<td style="text-align: center;">$0.69_{.02}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.82_{.03}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.89_{.00}$</td>
<td style="text-align: center;">$0.90_{.00}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.57_{.10}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.71_{.05}$</td>
<td style="text-align: center;">$0.78_{.04}$</td>
<td style="text-align: center;">$0.83_{.01}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.87_{.00}$</td>
<td style="text-align: center;">$0.88_{.00}$</td>
<td style="text-align: center;">$0.89_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.63_{.01}$</td>
<td style="text-align: center;">$0.61_{.04}$</td>
<td style="text-align: center;">$0.62_{.02}$</td>
<td style="text-align: center;">$0.63_{.03}$</td>
<td style="text-align: center;">$0.64_{.02}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.76_{.04}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.82_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.54_{.01}$</td>
<td style="text-align: center;">$0.56_{.08}$</td>
<td style="text-align: center;">$0.60_{.06}$</td>
<td style="text-align: center;">$0.59_{.06}$</td>
<td style="text-align: center;">$0.60_{.04}$</td>
<td style="text-align: center;">$0.62_{.04}$</td>
<td style="text-align: center;">$0.67_{.04}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
<td style="text-align: center;">$0.85_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.42_{.01}$</td>
<td style="text-align: center;">$0.48_{.07}$</td>
<td style="text-align: center;">$0.50_{.05}$</td>
<td style="text-align: center;">$0.56_{.03}$</td>
<td style="text-align: center;">$0.57_{.04}$</td>
<td style="text-align: center;">$0.59_{.05}$</td>
<td style="text-align: center;">$0.63_{.03}$</td>
<td style="text-align: center;">$0.68_{.02}$</td>
<td style="text-align: center;">$0.74_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.63_{.01}$</td>
<td style="text-align: center;">$0.59_{.10}$</td>
<td style="text-align: center;">$0.64_{.05}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.69_{.03}$</td>
<td style="text-align: center;">$0.82_{.05}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.92{ }^{2}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.60_{.01}$</td>
<td style="text-align: center;">$0.59_{.10}$</td>
<td style="text-align: center;">$0.66_{.02}$</td>
<td style="text-align: center;">$0.65_{.04}$</td>
<td style="text-align: center;">$0.66_{.05}$</td>
<td style="text-align: center;">$0.74_{.07}$</td>
<td style="text-align: center;">$0.85_{.02}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.56_{.01}$</td>
<td style="text-align: center;">$0.58_{.09}$</td>
<td style="text-align: center;">$0.60_{.04}$</td>
<td style="text-align: center;">$0.63_{.03}$</td>
<td style="text-align: center;">$0.67_{.03}$</td>
<td style="text-align: center;">$0.71_{.05}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.86_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.64_{.00}$</td>
<td style="text-align: center;">$0.55_{.10}$</td>
<td style="text-align: center;">$0.62_{.07}$</td>
<td style="text-align: center;">$0.63_{.04}$</td>
<td style="text-align: center;">$0.63_{.05}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.82_{.02}$</td>
<td style="text-align: center;">$0.86_{.01}$</td>
<td style="text-align: center;">$0.88_{.00}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.38_{.01}$</td>
<td style="text-align: center;">$0.47_{.11}$</td>
<td style="text-align: center;">$0.53_{.06}$</td>
<td style="text-align: center;">$0.55_{.07}$</td>
<td style="text-align: center;">$0.57_{.05}$</td>
<td style="text-align: center;">$0.65_{.04}$</td>
<td style="text-align: center;">$0.75_{.07}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.85_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.61_{.01}$</td>
<td style="text-align: center;">$0.60_{.10}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.64_{.07}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.70_{.02}$</td>
<td style="text-align: center;">$0.77_{.05}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.89_{.03}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">Blood Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.54_{.09}$</td>
<td style="text-align: center;">$0.59_{.08}$</td>
<td style="text-align: center;">$0.72_{.03}$</td>
<td style="text-align: center;">$0.70_{.06}$</td>
<td style="text-align: center;">$0.74_{.02}$</td>
<td style="text-align: center;">$0.76_{.02}$</td>
<td style="text-align: center;">$0.76_{.02}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.54_{.09}$</td>
<td style="text-align: center;">$0.59_{.08}$</td>
<td style="text-align: center;">$0.72_{.03}$</td>
<td style="text-align: center;">$0.70_{.06}$</td>
<td style="text-align: center;">$0.74_{.02}$</td>
<td style="text-align: center;">$0.76_{.02}$</td>
<td style="text-align: center;">$0.76_{.02}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.69_{.04}$</td>
<td style="text-align: center;">$0.71_{.05}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.74_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.69_{.04}$</td>
<td style="text-align: center;">$0.71_{.05}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.74_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.58_{.07}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.68_{.05}$</td>
<td style="text-align: center;">$0.71_{.06}$</td>
<td style="text-align: center;">$0.70_{.07}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.71_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.58_{.07}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.68_{.05}$</td>
<td style="text-align: center;">$0.71_{.06}$</td>
<td style="text-align: center;">$0.70_{.07}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.71_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.47_{.12}$</td>
<td style="text-align: center;">$0.66_{.08}$</td>
<td style="text-align: center;">$0.66_{.03}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.71_{.03}$</td>
<td style="text-align: center;">$0.76_{.05}$</td>
<td style="text-align: center;">$0.73_{.02}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.47_{.09}$</td>
<td style="text-align: center;">$0.61_{.06}$</td>
<td style="text-align: center;">$0.60_{.09}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.63_{.06}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.72_{.06}$</td>
<td style="text-align: center;">$0.72_{.01}$</td>
<td style="text-align: center;">$0.71_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.49_{.04}$</td>
<td style="text-align: center;">$0.60_{.07}$</td>
<td style="text-align: center;">$0.62_{.04}$</td>
<td style="text-align: center;">$0.67_{.03}$</td>
<td style="text-align: center;">$0.71_{.05}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.52_{.08}$</td>
<td style="text-align: center;">$0.64_{.04}$</td>
<td style="text-align: center;">$0.67_{.01}$</td>
<td style="text-align: center;">$0.70_{.04}$</td>
<td style="text-align: center;">$0.73_{.04}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.76_{.04}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.52_{.08}$</td>
<td style="text-align: center;">$0.64_{.04}$</td>
<td style="text-align: center;">$0.67_{.01}$</td>
<td style="text-align: center;">$0.70_{.04}$</td>
<td style="text-align: center;">$0.73_{.04}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.76_{.04}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.63_{.04}$</td>
<td style="text-align: center;">$0.61_{.07}$</td>
<td style="text-align: center;">$0.65_{.04}$</td>
<td style="text-align: center;">$0.63_{.02}$</td>
<td style="text-align: center;">$0.64_{.03}$</td>
<td style="text-align: center;">$0.62_{.05}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.68_{.05}$</td>
<td style="text-align: center;">$0.66_{.05}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.49_{.04}$</td>
<td style="text-align: center;">$0.51_{.03}$</td>
<td style="text-align: center;">$0.59_{.08}$</td>
<td style="text-align: center;">$0.59_{.06}$</td>
<td style="text-align: center;">$0.64_{.04}$</td>
<td style="text-align: center;">$0.65_{.06}$</td>
<td style="text-align: center;">$0.66_{.05}$</td>
<td style="text-align: center;">$0.68_{.06}$</td>
<td style="text-align: center;">$0.66_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.61_{.04}$</td>
<td style="text-align: center;">$0.59_{.04}$</td>
<td style="text-align: center;">$0.59_{.03}$</td>
<td style="text-align: center;">$0.57_{.03}$</td>
<td style="text-align: center;">$0.62_{.07}$</td>
<td style="text-align: center;">$0.56_{.07}$</td>
<td style="text-align: center;">$0.57_{.07}$</td>
<td style="text-align: center;">$0.64_{.07}$</td>
<td style="text-align: center;">$0.61_{.05}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.61_{.04}$</td>
<td style="text-align: center;">$0.58_{.09}$</td>
<td style="text-align: center;">$0.66_{.03}$</td>
<td style="text-align: center;">$0.66_{.07}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.68_{.06}$</td>
<td style="text-align: center;">$0.70_{.08}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.70_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.56_{.05}$</td>
<td style="text-align: center;">$0.54_{.08}$</td>
<td style="text-align: center;">$0.64_{.02}$</td>
<td style="text-align: center;">$0.64_{.08}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.70_{.06}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.45_{.05}$</td>
<td style="text-align: center;">$0.49_{.07}$</td>
<td style="text-align: center;">$0.57_{.03}$</td>
<td style="text-align: center;">$0.57_{.06}$</td>
<td style="text-align: center;">$0.62_{.06}$</td>
<td style="text-align: center;">$0.61_{.04}$</td>
<td style="text-align: center;">$0.64_{.04}$</td>
<td style="text-align: center;">$0.68_{.07}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.52_{.04}$</td>
<td style="text-align: center;">$0.49_{.07}$</td>
<td style="text-align: center;">$0.62_{.03}$</td>
<td style="text-align: center;">$0.62_{.06}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.65_{.04}$</td>
<td style="text-align: center;">$0.68_{.06}$</td>
<td style="text-align: center;">$0.72_{.06}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.51_{.03}$</td>
<td style="text-align: center;">$0.51_{.06}$</td>
<td style="text-align: center;">$0.54_{.04}$</td>
<td style="text-align: center;">$0.52_{.07}$</td>
<td style="text-align: center;">$0.55_{.03}$</td>
<td style="text-align: center;">$0.59_{.06}$</td>
<td style="text-align: center;">$0.59_{.02}$</td>
<td style="text-align: center;">$0.62_{.06}$</td>
<td style="text-align: center;">$0.62_{.05}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.42_{.05}$</td>
<td style="text-align: center;">$0.47_{.04}$</td>
<td style="text-align: center;">$0.62_{.04}$</td>
<td style="text-align: center;">$0.62_{.09}$</td>
<td style="text-align: center;">$0.65_{.07}$</td>
<td style="text-align: center;">$0.67_{.04}$</td>
<td style="text-align: center;">$0.69_{.04}$</td>
<td style="text-align: center;">$0.71_{.06}$</td>
<td style="text-align: center;">$0.67_{.04}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">California Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.58_{.11}$</td>
<td style="text-align: center;">$0.69_{.13}$</td>
<td style="text-align: center;">$0.80_{.06}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.90_{.00}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
<td style="text-align: center;">$0.92_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.58_{.11}$</td>
<td style="text-align: center;">$0.69_{.13}$</td>
<td style="text-align: center;">$0.80_{.06}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.90_{.00}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
<td style="text-align: center;">$0.91_{.00}$</td>
<td style="text-align: center;">$0.92_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.92_{.00}$</td>
<td style="text-align: center;">$0.97_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.92_{.00}$</td>
<td style="text-align: center;">$0.97_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.62_{.10}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
<td style="text-align: center;">$0.79_{.04}$</td>
<td style="text-align: center;">$0.82_{.04}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.92_{.01}$</td>
<td style="text-align: center;">$0.97_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.62_{.10}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
<td style="text-align: center;">$0.79_{.04}$</td>
<td style="text-align: center;">$0.82_{.04}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.92_{.01}$</td>
<td style="text-align: center;">$0.97_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.59_{.09}$</td>
<td style="text-align: center;">$0.64_{.12}$</td>
<td style="text-align: center;">$0.73_{.06}$</td>
<td style="text-align: center;">$0.76_{.06}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.88_{.02}$</td>
<td style="text-align: center;">$0.91_{.02}$</td>
<td style="text-align: center;">$0.95_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.08}$</td>
<td style="text-align: center;">$0.57_{.06}$</td>
<td style="text-align: center;">$0.67_{.02}$</td>
<td style="text-align: center;">$0.69_{.05}$</td>
<td style="text-align: center;">$0.72_{.03}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.84_{.02}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.96_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.58_{.06}$</td>
<td style="text-align: center;">$0.57_{.07}$</td>
<td style="text-align: center;">$0.70_{.05}$</td>
<td style="text-align: center;">$0.77_{.03}$</td>
<td style="text-align: center;">$0.80_{.01}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.63_{.13}$</td>
<td style="text-align: center;">$0.63_{.11}$</td>
<td style="text-align: center;">$0.80_{.03}$</td>
<td style="text-align: center;">$0.85_{.03}$</td>
<td style="text-align: center;">$0.89_{.01}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.92_{.00}$</td>
<td style="text-align: center;">$0.93_{.00}$</td>
<td style="text-align: center;">$0.94_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.63_{.13}$</td>
<td style="text-align: center;">$0.63_{.11}$</td>
<td style="text-align: center;">$0.80_{.03}$</td>
<td style="text-align: center;">$0.85_{.03}$</td>
<td style="text-align: center;">$0.89_{.01}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.92_{.00}$</td>
<td style="text-align: center;">$0.93_{.00}$</td>
<td style="text-align: center;">$0.94_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.56_{.00}$</td>
<td style="text-align: center;">$0.55_{.03}$</td>
<td style="text-align: center;">$0.57_{.05}$</td>
<td style="text-align: center;">$0.61_{.06}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.73_{.04}$</td>
<td style="text-align: center;">$0.82_{.01}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.85_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.49_{.01}$</td>
<td style="text-align: center;">$0.52_{.02}$</td>
<td style="text-align: center;">$0.51_{.02}$</td>
<td style="text-align: center;">$0.52_{.02}$</td>
<td style="text-align: center;">$0.54_{.04}$</td>
<td style="text-align: center;">$0.56_{.04}$</td>
<td style="text-align: center;">$0.69_{.02}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.80_{.02}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.49_{.01}$</td>
<td style="text-align: center;">$0.50_{.01}$</td>
<td style="text-align: center;">$0.51_{.01}$</td>
<td style="text-align: center;">$0.52_{.02}$</td>
<td style="text-align: center;">$0.57_{.04}$</td>
<td style="text-align: center;">$0.58_{.04}$</td>
<td style="text-align: center;">$0.74_{.01}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.82_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.61_{.01}$</td>
<td style="text-align: center;">$0.63_{.05}$</td>
<td style="text-align: center;">$0.60_{.07}$</td>
<td style="text-align: center;">$0.70_{.08}$</td>
<td style="text-align: center;">$0.77_{.08}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.83_{.01}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.95_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.61_{.01}$</td>
<td style="text-align: center;">$0.64_{.05}$</td>
<td style="text-align: center;">$0.62_{.06}$</td>
<td style="text-align: center;">$0.68_{.07}$</td>
<td style="text-align: center;">$0.77_{.07}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.82_{.02}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.87_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.58_{.01}$</td>
<td style="text-align: center;">$0.57_{.08}$</td>
<td style="text-align: center;">$0.55_{.03}$</td>
<td style="text-align: center;">$0.65_{.09}$</td>
<td style="text-align: center;">$0.74_{.08}$</td>
<td style="text-align: center;">$0.77_{.03}$</td>
<td style="text-align: center;">$0.83_{.01}$</td>
<td style="text-align: center;">$0.84_{.01}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.54_{.01}$</td>
<td style="text-align: center;">$0.52_{.03}$</td>
<td style="text-align: center;">$0.52_{.04}$</td>
<td style="text-align: center;">$0.52_{.03}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.74_{.01}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.84_{.02}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.47_{.01}$</td>
<td style="text-align: center;">$0.48_{.02}$</td>
<td style="text-align: center;">$0.50_{.01}$</td>
<td style="text-align: center;">$0.52_{.02}$</td>
<td style="text-align: center;">$0.57_{.03}$</td>
<td style="text-align: center;">$0.64_{.04}$</td>
<td style="text-align: center;">$0.71_{.04}$</td>
<td style="text-align: center;">$0.76_{.01}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.57_{.01}$</td>
<td style="text-align: center;">$0.59_{.03}$</td>
<td style="text-align: center;">$0.57_{.04}$</td>
<td style="text-align: center;">$0.66_{.07}$</td>
<td style="text-align: center;">$0.77_{.06}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.81_{.01}$</td>
<td style="text-align: center;">$0.83_{.01}$</td>
<td style="text-align: center;">$0.85_{.01}$</td>
<td style="text-align: center;">*</td>
</tr>
</tbody>
</table>
<p>*Result omitted due to runtime limitations of TabLLM on the full dataset.
${ }^{\dagger}$ Only a single run performed due to runtime limitations of TabLLM on the full dataset.</p>
<p>Table 13: Test AUC performance of competing methods on public tabular datasets. Each column reports the $k$-shot performance for different values of $k$. Standard deviations across five random seeds are shown as subscripts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of Shots</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">all</td>
</tr>
<tr>
<td style="text-align: center;">Car Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.61_{.02}$</td>
<td style="text-align: center;">$0.65_{.10}$</td>
<td style="text-align: center;">$0.74_{.07}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
<td style="text-align: center;">$0.93_{.02}$</td>
<td style="text-align: center;">$0.96_{.01}$</td>
<td style="text-align: center;">$0.97_{.01}$</td>
<td style="text-align: center;">$0.98_{.00}$</td>
<td style="text-align: center;">$0.98_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.62_{.06}$</td>
<td style="text-align: center;">$0.63_{.05}$</td>
<td style="text-align: center;">$0.64_{.07}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
<td style="text-align: center;">$0.76_{.02}$</td>
<td style="text-align: center;">$0.78_{.05}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.85_{.06}$</td>
<td style="text-align: center;">$0.93_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.91_{.05}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.59_{.04}$</td>
<td style="text-align: center;">$0.70_{.08}$</td>
<td style="text-align: center;">$0.82_{.03}$</td>
<td style="text-align: center;">$0.91_{.02}$</td>
<td style="text-align: center;">$0.95_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.55_{.03}$</td>
<td style="text-align: center;">$0.70_{.04}$</td>
<td style="text-align: center;">$0.78_{.03}$</td>
<td style="text-align: center;">$0.90_{.03}$</td>
<td style="text-align: center;">$0.94_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.56_{.08}$</td>
<td style="text-align: center;">$0.64_{.08}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.85_{.03}$</td>
<td style="text-align: center;">$0.92_{.02}$</td>
<td style="text-align: center;">$0.96_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\dagger$</td>
<td style="text-align: center;">$0.54_{.05}$</td>
<td style="text-align: center;">$0.64_{.05}$</td>
<td style="text-align: center;">$0.66_{.05}$</td>
<td style="text-align: center;">$0.73_{.07}$</td>
<td style="text-align: center;">$0.81_{.04}$</td>
<td style="text-align: center;">$0.93_{.02}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.51_{.10}$</td>
<td style="text-align: center;">$0.57_{.06}$</td>
<td style="text-align: center;">$0.69_{.02}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
<td style="text-align: center;">$0.80_{.02}$</td>
<td style="text-align: center;">$0.82_{.01}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.96_{.01}$</td>
<td style="text-align: center;">$0.93_{.01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.75_{.05}$</td>
<td style="text-align: center;">$0.87_{.04}$</td>
<td style="text-align: center;">$0.92_{.02}$</td>
<td style="text-align: center;">$0.97_{.00}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.59_{.06}$</td>
<td style="text-align: center;">$0.65_{.08}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.82_{.06}$</td>
<td style="text-align: center;">$0.89_{.01}$</td>
<td style="text-align: center;">$0.93_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.72_{.02}$</td>
<td style="text-align: center;">$0.75_{.03}$</td>
<td style="text-align: center;">$0.75_{.02}$</td>
<td style="text-align: center;">$0.78_{.01}$</td>
<td style="text-align: center;">$0.83_{.01}$</td>
<td style="text-align: center;">$0.87_{.02}$</td>
<td style="text-align: center;">$0.90_{.01}$</td>
<td style="text-align: center;">$0.93_{.02}$</td>
<td style="text-align: center;">$0.93_{.02}$</td>
<td style="text-align: center;">$0.96_{.01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.85_{.01}$</td>
<td style="text-align: center;">$0.85_{.02}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.89_{.02}$</td>
<td style="text-align: center;">$0.92_{.02}$</td>
<td style="text-align: center;">$0.94_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.61_{.01}$</td>
<td style="text-align: center;">$0.69_{.04}$</td>
<td style="text-align: center;">$0.74_{.04}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.88_{.01}$</td>
<td style="text-align: center;">$0.91_{.02}$</td>
<td style="text-align: center;">$0.94_{.01}$</td>
<td style="text-align: center;">$0.96_{.01}$</td>
<td style="text-align: center;">$0.95_{.01}$</td>
<td style="text-align: center;">$0.96_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.82_{.02}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
<td style="text-align: center;">$0.85_{.03}$</td>
<td style="text-align: center;">$0.86_{.03}$</td>
<td style="text-align: center;">$0.91_{.02}$</td>
<td style="text-align: center;">$0.96_{.02}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.85_{.02}$</td>
<td style="text-align: center;">$0.86_{.03}$</td>
<td style="text-align: center;">$0.91_{.02}$</td>
<td style="text-align: center;">$0.95_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.48_{.03}$</td>
<td style="text-align: center;">$0.62_{.04}$</td>
<td style="text-align: center;">$0.67_{.03}$</td>
<td style="text-align: center;">$0.70_{.03}$</td>
<td style="text-align: center;">$0.75_{.02}$</td>
<td style="text-align: center;">$0.87_{.02}$</td>
<td style="text-align: center;">$0.94_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.39_{.02}$</td>
<td style="text-align: center;">$0.54_{.10}$</td>
<td style="text-align: center;">$0.58_{.04}$</td>
<td style="text-align: center;">$0.70_{.03}$</td>
<td style="text-align: center;">$0.86_{.02}$</td>
<td style="text-align: center;">$0.94_{.01}$</td>
<td style="text-align: center;">$0.97_{.02}$</td>
<td style="text-align: center;">$0.99_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.38_{.02}$</td>
<td style="text-align: center;">$0.48_{.08}$</td>
<td style="text-align: center;">$0.55_{.05}$</td>
<td style="text-align: center;">$0.63_{.04}$</td>
<td style="text-align: center;">$0.69_{.03}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.90_{.03}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.80_{.03}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
<td style="text-align: center;">$0.84_{.04}$</td>
<td style="text-align: center;">$0.89_{.03}$</td>
<td style="text-align: center;">$0.91_{.01}$</td>
<td style="text-align: center;">$0.96_{.01}$</td>
<td style="text-align: center;">$0.98_{.01}$</td>
<td style="text-align: center;">$0.99_{.00}$</td>
<td style="text-align: center;">$1.00_{.00}$</td>
</tr>
<tr>
<td style="text-align: center;">Credit-g Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.08}$</td>
<td style="text-align: center;">$0.56_{.06}$</td>
<td style="text-align: center;">$0.58_{.08}$</td>
<td style="text-align: center;">$0.68_{.08}$</td>
<td style="text-align: center;">$0.66_{.07}$</td>
<td style="text-align: center;">$0.71_{.06}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.76_{.02}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.56_{.05}$</td>
<td style="text-align: center;">$0.54_{.06}$</td>
<td style="text-align: center;">$0.55_{.05}$</td>
<td style="text-align: center;">$0.61_{.05}$</td>
<td style="text-align: center;">$0.68_{.05}$</td>
<td style="text-align: center;">$0.66_{.03}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.71_{.02}$</td>
<td style="text-align: center;">$0.72_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.61_{.00}$</td>
<td style="text-align: center;">$0.68_{.03}$</td>
<td style="text-align: center;">$0.72_{.02}$</td>
<td style="text-align: center;">$0.75_{.02}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.68_{.07}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.72_{.02}$</td>
<td style="text-align: center;">$0.75_{.03}$</td>
<td style="text-align: center;">$0.76_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.51_{.07}$</td>
<td style="text-align: center;">$0.59_{.05}$</td>
<td style="text-align: center;">$0.66_{.03}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.68_{.02}$</td>
<td style="text-align: center;">$0.73_{.02}$</td>
<td style="text-align: center;">$0.75_{.03}$</td>
<td style="text-align: center;">$0.78_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.54_{.11}$</td>
<td style="text-align: center;">$0.57_{.08}$</td>
<td style="text-align: center;">$0.64_{.05}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.74_{.02}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.76_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.56_{.08}$</td>
<td style="text-align: center;">$0.53_{.05}$</td>
<td style="text-align: center;">$0.60_{.05}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.68_{.05}$</td>
<td style="text-align: center;">$0.72_{.04}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.48_{.05}$</td>
<td style="text-align: center;">$0.52_{.07}$</td>
<td style="text-align: center;">$0.49_{.03}$</td>
<td style="text-align: center;">$0.52_{.03}$</td>
<td style="text-align: center;">$0.56_{.05}$</td>
<td style="text-align: center;">$0.60_{.05}$</td>
<td style="text-align: center;">$0.61_{.02}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.64_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.54_{.09}$</td>
<td style="text-align: center;">$0.54_{.10}$</td>
<td style="text-align: center;">$0.54_{.09}$</td>
<td style="text-align: center;">$0.59_{.07}$</td>
<td style="text-align: center;">$0.63_{.04}$</td>
<td style="text-align: center;">$0.68_{.02}$</td>
<td style="text-align: center;">$0.68_{.05}$</td>
<td style="text-align: center;">$0.70_{.02}$</td>
<td style="text-align: center;">$0.65_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.58_{.08}$</td>
<td style="text-align: center;">$0.59_{.03}$</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.69_{.07}$</td>
<td style="text-align: center;">$0.70_{.07}$</td>
<td style="text-align: center;">$0.72_{.06}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
<td style="text-align: center;">$0.75_{.02}$</td>
<td style="text-align: center;">$0.75_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.55_{.08}$</td>
<td style="text-align: center;">$0.51_{.07}$</td>
<td style="text-align: center;">$0.57_{.06}$</td>
<td style="text-align: center;">$0.62_{.03}$</td>
<td style="text-align: center;">$0.66_{.05}$</td>
<td style="text-align: center;">$0.70_{.02}$</td>
<td style="text-align: center;">$0.73_{.01}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.75_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.52_{.04}$</td>
<td style="text-align: center;">$0.53_{.04}$</td>
<td style="text-align: center;">$0.56_{.05}$</td>
<td style="text-align: center;">$0.56_{.05}$</td>
<td style="text-align: center;">$0.55_{.05}$</td>
<td style="text-align: center;">$0.57_{.06}$</td>
<td style="text-align: center;">$0.60_{.06}$</td>
<td style="text-align: center;">$0.61_{.04}$</td>
<td style="text-align: center;">$0.63_{.05}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.49_{.02}$</td>
<td style="text-align: center;">$0.50_{.06}$</td>
<td style="text-align: center;">$0.54_{.06}$</td>
<td style="text-align: center;">$0.55_{.04}$</td>
<td style="text-align: center;">$0.60_{.06}$</td>
<td style="text-align: center;">$0.61_{.02}$</td>
<td style="text-align: center;">$0.61_{.02}$</td>
<td style="text-align: center;">$0.63_{.03}$</td>
<td style="text-align: center;">$0.65_{.02}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.50_{.06}$</td>
<td style="text-align: center;">$0.65_{.04}$</td>
<td style="text-align: center;">$0.60_{.05}$</td>
<td style="text-align: center;">$0.60_{.07}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.64_{.05}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.53_{.05}$</td>
<td style="text-align: center;">$0.69_{.04}$</td>
<td style="text-align: center;">$0.66_{.04}$</td>
<td style="text-align: center;">$0.66_{.05}$</td>
<td style="text-align: center;">$0.72_{.06}$</td>
<td style="text-align: center;">$0.70_{.07}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$0.72_{.03}$</td>
<td style="text-align: center;">$0.72_{.02}$</td>
<td style="text-align: center;">$0.70_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.53_{.05}$</td>
<td style="text-align: center;">$0.64_{.04}$</td>
<td style="text-align: center;">$0.60_{.06}$</td>
<td style="text-align: center;">$0.64_{.05}$</td>
<td style="text-align: center;">$0.70_{.05}$</td>
<td style="text-align: center;">$0.66_{.08}$</td>
<td style="text-align: center;">$0.67_{.03}$</td>
<td style="text-align: center;">$0.70_{.03}$</td>
<td style="text-align: center;">$0.70_{.04}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.71_{.03}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.69_{.06}$</td>
<td style="text-align: center;">$0.72_{.06}$</td>
<td style="text-align: center;">$0.69_{.05}$</td>
<td style="text-align: center;">$0.69_{.07}$</td>
<td style="text-align: center;">$0.70_{.06}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.44_{.01}$</td>
<td style="text-align: center;">$0.58_{.09}$</td>
<td style="text-align: center;">$0.59_{.08}$</td>
<td style="text-align: center;">$0.60_{.07}$</td>
<td style="text-align: center;">$0.70_{.06}$</td>
<td style="text-align: center;">$0.69_{.06}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.70_{.05}$</td>
<td style="text-align: center;">$0.70_{.03}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.50_{.05}$</td>
<td style="text-align: center;">$0.55_{.06}$</td>
<td style="text-align: center;">$0.56_{.07}$</td>
<td style="text-align: center;">$0.58_{.04}$</td>
<td style="text-align: center;">$0.64_{.03}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.67_{.09}$</td>
<td style="text-align: center;">$0.68_{.03}$</td>
<td style="text-align: center;">$0.69_{.03}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.54_{.03}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.63_{.05}$</td>
<td style="text-align: center;">$0.63_{.03}$</td>
<td style="text-align: center;">$0.73_{.04}$</td>
<td style="text-align: center;">$0.69_{.05}$</td>
<td style="text-align: center;">$0.68_{.06}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">Diabetes Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.60_{.15}$</td>
<td style="text-align: center;">$0.68_{.11}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.76_{.05}$</td>
<td style="text-align: center;">$0.80_{.02}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.60_{.15}$</td>
<td style="text-align: center;">$0.68_{.11}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.76_{.05}$</td>
<td style="text-align: center;">$0.80_{.02}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.79_{.04}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.79_{.04}$</td>
<td style="text-align: center;">$0.79_{.02}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.59_{.16}$</td>
<td style="text-align: center;">$0.72_{.07}$</td>
<td style="text-align: center;">$0.69_{.08}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.78_{.05}$</td>
<td style="text-align: center;">$0.80_{.03}$</td>
<td style="text-align: center;">$0.80_{.01}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{.00}$</td>
<td style="text-align: center;">$0.59_{.16}$</td>
<td style="text-align: center;">$0.72_{.07}$</td>
<td style="text-align: center;">$0.69_{.08}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.78_{.05}$</td>
<td style="text-align: center;">$0.80_{.03}$</td>
<td style="text-align: center;">$0.80_{.01}$</td>
<td style="text-align: center;">$0.84_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.46_{.12}$</td>
<td style="text-align: center;">$0.65_{.11}$</td>
<td style="text-align: center;">$0.73_{.06}$</td>
<td style="text-align: center;">$0.73_{.06}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
<td style="text-align: center;">$0.81_{.03}$</td>
<td style="text-align: center;">$0.81_{.04}$</td>
<td style="text-align: center;">$0.77_{.03}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.56_{.04}$</td>
<td style="text-align: center;">$0.56_{.06}$</td>
<td style="text-align: center;">$0.64_{.09}$</td>
<td style="text-align: center;">$0.66_{.06}$</td>
<td style="text-align: center;">$0.71_{.04}$</td>
<td style="text-align: center;">$0.73_{.04}$</td>
<td style="text-align: center;">$0.74_{.05}$</td>
<td style="text-align: center;">$0.74_{.07}$</td>
<td style="text-align: center;">$0.81_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.49_{.13}$</td>
<td style="text-align: center;">$0.67_{.09}$</td>
<td style="text-align: center;">$0.69_{.08}$</td>
<td style="text-align: center;">$0.73_{.05}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
<td style="text-align: center;">$0.80_{.04}$</td>
<td style="text-align: center;">$0.81_{.03}$</td>
<td style="text-align: center;">$0.83_{.02}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.61_{.13}$</td>
<td style="text-align: center;">$0.67_{.11}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$0.77_{.03}$</td>
<td style="text-align: center;">$0.82_{.03}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.81_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.61_{.13}$</td>
<td style="text-align: center;">$0.67_{.11}$</td>
<td style="text-align: center;">$0.71_{.07}$</td>
<td style="text-align: center;">$0.77_{.03}$</td>
<td style="text-align: center;">$0.82_{.03}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
<td style="text-align: center;">$0.83_{.03}$</td>
<td style="text-align: center;">$0.81_{.02}$</td>
<td style="text-align: center;">$0.81_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.61_{.06}$</td>
<td style="text-align: center;">$0.61_{.07}$</td>
<td style="text-align: center;">$0.56_{.12}$</td>
<td style="text-align: center;">$0.67_{.08}$</td>
<td style="text-align: center;">$0.74_{.04}$</td>
<td style="text-align: center;">$0.77_{.02}$</td>
<td style="text-align: center;">$0.79_{.03}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.78_{.04}$</td>
<td style="text-align: center;">$0.81_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.58_{.04}$</td>
<td style="text-align: center;">$0.53_{.05}$</td>
<td style="text-align: center;">$0.53_{.06}$</td>
<td style="text-align: center;">$0.54_{.09}$</td>
<td style="text-align: center;">$0.59_{.05}$</td>
<td style="text-align: center;">$0.68_{.02}$</td>
<td style="text-align: center;">$0.73_{.04}$</td>
<td style="text-align: center;">$0.72_{.05}$</td>
<td style="text-align: center;">$0.72_{.03}$</td>
<td style="text-align: center;">$0.76_{.01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.58_{.04}$</td>
<td style="text-align: center;">$0.51_{.10}$</td>
<td style="text-align: center;">$0.53_{.07}$</td>
<td style="text-align: center;">$0.56_{.05}$</td>
<td style="text-align: center;">$0.57_{.04}$</td>
<td style="text-align: center;">$0.59_{.04}$</td>
<td style="text-align: center;">$0.72_{.05}$</td>
<td style="text-align: center;">$0.74_{.04}$</td>
<td style="text-align: center;">$0.75_{.06}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.68_{.06}$</td>
<td style="text-align: center;">$0.61_{.09}$</td>
<td style="text-align: center;">$0.63_{.08}$</td>
<td style="text-align: center;">$0.69_{.07}$</td>
<td style="text-align: center;">$0.68_{.04}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.79_{.04}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.78_{.04}$</td>
<td style="text-align: center;">$0.80_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.64_{.06}$</td>
<td style="text-align: center;">$0.64_{.09}$</td>
<td style="text-align: center;">$0.64_{.10}$</td>
<td style="text-align: center;">$0.67_{.07}$</td>
<td style="text-align: center;">$0.70_{.05}$</td>
<td style="text-align: center;">$0.76_{.04}$</td>
<td style="text-align: center;">$0.78_{.03}$</td>
<td style="text-align: center;">$0.78_{.03}$</td>
<td style="text-align: center;">$0.78_{.04}$</td>
<td style="text-align: center;">$0.81_{.05}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.55_{.05}$</td>
<td style="text-align: center;">$0.54_{.07}$</td>
<td style="text-align: center;">$0.52_{.05}$</td>
<td style="text-align: center;">$0.59_{.08}$</td>
<td style="text-align: center;">$0.63_{.04}$</td>
<td style="text-align: center;">$0.67_{.07}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.75_{.06}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
<td style="text-align: center;">$0.79_{.01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.56_{.07}$</td>
<td style="text-align: center;">$0.60_{.09}$</td>
<td style="text-align: center;">$0.68_{.12}$</td>
<td style="text-align: center;">$0.74_{.05}$</td>
<td style="text-align: center;">$0.74_{.03}$</td>
<td style="text-align: center;">$0.72_{.04}$</td>
<td style="text-align: center;">$0.76_{.04}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
<td style="text-align: center;">$0.81_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.44_{.03}$</td>
<td style="text-align: center;">$0.47_{.09}$</td>
<td style="text-align: center;">$0.43_{.06}$</td>
<td style="text-align: center;">$0.55_{.07}$</td>
<td style="text-align: center;">$0.61_{.05}$</td>
<td style="text-align: center;">$0.65_{.05}$</td>
<td style="text-align: center;">$0.73_{.03}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.78_{.02}$</td>
<td style="text-align: center;">$0.80_{.03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.62_{.05}$</td>
<td style="text-align: center;">$0.57_{.07}$</td>
<td style="text-align: center;">$0.60_{.08}$</td>
<td style="text-align: center;">$0.67_{.05}$</td>
<td style="text-align: center;">$0.67_{.06}$</td>
<td style="text-align: center;">$0.76_{.03}$</td>
<td style="text-align: center;">$0.77_{.04}$</td>
<td style="text-align: center;">$0.81_{.05}$</td>
<td style="text-align: center;">$0.80_{.04}$</td>
<td style="text-align: center;">$0.82_{.04}$</td>
</tr>
<tr>
<td style="text-align: center;">${ }^{\dagger}$ Result omitted due to runtime limitations of TabLLM on the full dataset.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">${ }^{\dagger}$ Result omitted due to TabNet package not supporting unseen labels in validation set during cross validation.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 14: Test AUC performance of competing methods on public tabular datasets. Each column reports the $k$-shot performance for different values of $k$. Standard deviations across five random seeds are shown as subscripts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of Shots</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">all</td>
</tr>
<tr>
<td style="text-align: center;">Heart Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.69_{17}$</td>
<td style="text-align: center;">$0.75_{13}$</td>
<td style="text-align: center;">$0.82_{06}$</td>
<td style="text-align: center;">$0.87_{05}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.90_{02}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.93_{01}$</td>
<td style="text-align: center;">$0.93_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.70_{17}$</td>
<td style="text-align: center;">$0.73_{14}$</td>
<td style="text-align: center;">$0.84_{04}$</td>
<td style="text-align: center;">$0.88_{03}$</td>
<td style="text-align: center;">$0.89_{01}$</td>
<td style="text-align: center;">$0.88_{02}$</td>
<td style="text-align: center;">$0.90_{02}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
<td style="text-align: center;">$0.94_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.91_{02}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.94_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.55_{14}$</td>
<td style="text-align: center;">$0.84_{07}$</td>
<td style="text-align: center;">$0.88_{04}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.94_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.56_{15}$</td>
<td style="text-align: center;">$0.84_{07}$</td>
<td style="text-align: center;">$0.90_{03}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.94_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.80_{12}$</td>
<td style="text-align: center;">$0.83_{10}$</td>
<td style="text-align: center;">$0.88_{07}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.90_{04}$</td>
<td style="text-align: center;">$0.90_{02}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.93_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.56_{12}$</td>
<td style="text-align: center;">$0.70_{05}$</td>
<td style="text-align: center;">$0.73_{14}$</td>
<td style="text-align: center;">$0.80_{04}$</td>
<td style="text-align: center;">$0.83_{05}$</td>
<td style="text-align: center;">$0.84_{03}$</td>
<td style="text-align: center;">$0.88_{02}$</td>
<td style="text-align: center;">$0.88_{03}$</td>
<td style="text-align: center;">$0.89_{03}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.52_{10}$</td>
<td style="text-align: center;">$0.78_{08}$</td>
<td style="text-align: center;">$0.83_{03}$</td>
<td style="text-align: center;">$0.86_{02}$</td>
<td style="text-align: center;">$0.88_{02}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">$0.91_{02}$</td>
<td style="text-align: center;">$0.92_{03}$</td>
<td style="text-align: center;">$0.92_{03}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.84_{06}$</td>
<td style="text-align: center;">$0.88_{05}$</td>
<td style="text-align: center;">$0.87_{06}$</td>
<td style="text-align: center;">$0.91_{02}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.79_{08}$</td>
<td style="text-align: center;">$0.85_{07}$</td>
<td style="text-align: center;">$0.88_{05}$</td>
<td style="text-align: center;">$0.90_{02}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.92_{00}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.51_{04}$</td>
<td style="text-align: center;">$0.72_{05}$</td>
<td style="text-align: center;">$0.82_{03}$</td>
<td style="text-align: center;">$0.85_{05}$</td>
<td style="text-align: center;">$0.88_{03}$</td>
<td style="text-align: center;">$0.91_{02}$</td>
<td style="text-align: center;">$0.89_{02}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.93_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.44_{03}$</td>
<td style="text-align: center;">$0.74_{07}$</td>
<td style="text-align: center;">$0.82_{10}$</td>
<td style="text-align: center;">$0.87_{02}$</td>
<td style="text-align: center;">$0.88_{02}$</td>
<td style="text-align: center;">$0.89_{04}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.89_{02}$</td>
<td style="text-align: center;">$0.89_{03}$</td>
<td style="text-align: center;">$0.93_{02}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.56_{05}$</td>
<td style="text-align: center;">$0.73_{09}$</td>
<td style="text-align: center;">$0.78_{08}$</td>
<td style="text-align: center;">$0.86_{06}$</td>
<td style="text-align: center;">$0.88_{03}$</td>
<td style="text-align: center;">$0.91_{02}$</td>
<td style="text-align: center;">$0.91_{02}$</td>
<td style="text-align: center;">$0.90_{02}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.54_{04}$</td>
<td style="text-align: center;">$0.76_{14}$</td>
<td style="text-align: center;">$0.83_{05}$</td>
<td style="text-align: center;">$0.87_{04}$</td>
<td style="text-align: center;">$0.87_{06}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.94_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.52_{03}$</td>
<td style="text-align: center;">$0.73_{12}$</td>
<td style="text-align: center;">$0.83_{05}$</td>
<td style="text-align: center;">$0.87_{04}$</td>
<td style="text-align: center;">$0.88_{04}$</td>
<td style="text-align: center;">$0.91_{02}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
<td style="text-align: center;">$0.94_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.40_{04}$</td>
<td style="text-align: center;">$0.67_{16}$</td>
<td style="text-align: center;">$0.83_{06}$</td>
<td style="text-align: center;">$0.84_{05}$</td>
<td style="text-align: center;">$0.88_{03}$</td>
<td style="text-align: center;">$0.89_{03}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
<td style="text-align: center;">$0.90_{00}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.92_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.57_{02}$</td>
<td style="text-align: center;">$0.78_{07}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">$0.82_{01}$</td>
<td style="text-align: center;">$0.87_{05}$</td>
<td style="text-align: center;">$0.90_{02}$</td>
<td style="text-align: center;">$0.92_{02}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.93_{01}$</td>
<td style="text-align: center;">$0.93_{02}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.23_{02}$</td>
<td style="text-align: center;">$0.63_{20}$</td>
<td style="text-align: center;">$0.79_{12}$</td>
<td style="text-align: center;">$0.83_{07}$</td>
<td style="text-align: center;">$0.88_{04}$</td>
<td style="text-align: center;">$0.89_{04}$</td>
<td style="text-align: center;">$0.90_{02}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.56_{03}$</td>
<td style="text-align: center;">$0.68_{13}$</td>
<td style="text-align: center;">$0.82_{04}$</td>
<td style="text-align: center;">$0.85_{02}$</td>
<td style="text-align: center;">$0.86_{03}$</td>
<td style="text-align: center;">$0.90_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.93_{01}$</td>
<td style="text-align: center;">$0.93_{01}$</td>
<td style="text-align: center;">$0.94_{01}$</td>
</tr>
<tr>
<td style="text-align: center;">Income Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.68_{15}$</td>
<td style="text-align: center;">$0.72_{13}$</td>
<td style="text-align: center;">$0.80_{03}$</td>
<td style="text-align: center;">$0.82_{01}$</td>
<td style="text-align: center;">$0.83_{03}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.88_{00}$</td>
<td style="text-align: center;">$0.90_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.55_{04}$</td>
<td style="text-align: center;">$0.56_{06}$</td>
<td style="text-align: center;">$0.58_{07}$</td>
<td style="text-align: center;">$0.70_{06}$</td>
<td style="text-align: center;">$0.76_{03}$</td>
<td style="text-align: center;">$0.79_{01}$</td>
<td style="text-align: center;">$0.80_{01}$</td>
<td style="text-align: center;">$0.80_{00}$</td>
<td style="text-align: center;">$0.81_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.78_{01}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.88_{00}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.78_{01}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.89_{00}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.59_{06}$</td>
<td style="text-align: center;">$0.77_{02}$</td>
<td style="text-align: center;">$0.79_{03}$</td>
<td style="text-align: center;">$0.82_{02}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.88_{00}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.63_{04}$</td>
<td style="text-align: center;">$0.74_{04}$</td>
<td style="text-align: center;">$0.76_{04}$</td>
<td style="text-align: center;">$0.79_{03}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.88_{00}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.74_{03}$</td>
<td style="text-align: center;">$0.65_{15}$</td>
<td style="text-align: center;">$0.79_{03}$</td>
<td style="text-align: center;">$0.81_{03}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.88_{00}$</td>
<td style="text-align: center;">$0.91_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.56_{04}$</td>
<td style="text-align: center;">$0.59_{07}$</td>
<td style="text-align: center;">$0.62_{11}$</td>
<td style="text-align: center;">$0.64_{06}$</td>
<td style="text-align: center;">$0.71_{04}$</td>
<td style="text-align: center;">$0.73_{05}$</td>
<td style="text-align: center;">$0.80_{02}$</td>
<td style="text-align: center;">$0.83_{01}$</td>
<td style="text-align: center;">$0.92_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.54_{02}$</td>
<td style="text-align: center;">$0.54_{04}$</td>
<td style="text-align: center;">$0.65_{04}$</td>
<td style="text-align: center;">$0.67_{03}$</td>
<td style="text-align: center;">$0.75_{02}$</td>
<td style="text-align: center;">$0.78_{01}$</td>
<td style="text-align: center;">$0.78_{01}$</td>
<td style="text-align: center;">$0.83_{01}$</td>
<td style="text-align: center;">$0.82_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.73_{08}$</td>
<td style="text-align: center;">$0.71_{09}$</td>
<td style="text-align: center;">$0.76_{09}$</td>
<td style="text-align: center;">$0.80_{04}$</td>
<td style="text-align: center;">$0.82_{04}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.89_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.64_{11}$</td>
<td style="text-align: center;">$0.64_{06}$</td>
<td style="text-align: center;">$0.72_{04}$</td>
<td style="text-align: center;">$0.77_{02}$</td>
<td style="text-align: center;">$0.80_{02}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.83_{01}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">$0.87_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.75_{01}$</td>
<td style="text-align: center;">$0.79_{03}$</td>
<td style="text-align: center;">$0.80_{01}$</td>
<td style="text-align: center;">$0.82_{02}$</td>
<td style="text-align: center;">$0.82_{01}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">$0.86_{00}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.65_{01}$</td>
<td style="text-align: center;">$0.67_{03}$</td>
<td style="text-align: center;">$0.66_{07}$</td>
<td style="text-align: center;">$0.72_{02}$</td>
<td style="text-align: center;">$0.75_{03}$</td>
<td style="text-align: center;">$0.79_{04}$</td>
<td style="text-align: center;">$0.82_{02}$</td>
<td style="text-align: center;">$0.83_{02}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.64_{07}$</td>
<td style="text-align: center;">$0.64_{11}$</td>
<td style="text-align: center;">$0.72_{05}$</td>
<td style="text-align: center;">$0.74_{03}$</td>
<td style="text-align: center;">$0.79_{03}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.84_{00}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.84_{04}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.87_{00}$</td>
<td style="text-align: center;">$0.89_{01}$</td>
<td style="text-align: center;">$0.92_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.79_{01}$</td>
<td style="text-align: center;">$0.83_{01}$</td>
<td style="text-align: center;">$0.83_{03}$</td>
<td style="text-align: center;">$0.83_{02}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.73_{01}$</td>
<td style="text-align: center;">$0.74_{04}$</td>
<td style="text-align: center;">$0.75_{04}$</td>
<td style="text-align: center;">$0.80_{03}$</td>
<td style="text-align: center;">$0.82_{01}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.65_{00}$</td>
<td style="text-align: center;">$0.75_{03}$</td>
<td style="text-align: center;">$0.74_{05}$</td>
<td style="text-align: center;">$0.82_{02}$</td>
<td style="text-align: center;">$0.83_{02}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.26_{00}$</td>
<td style="text-align: center;">$0.40_{04}$</td>
<td style="text-align: center;">$0.48_{01}$</td>
<td style="text-align: center;">$0.65_{06}$</td>
<td style="text-align: center;">$0.72_{03}$</td>
<td style="text-align: center;">$0.79_{03}$</td>
<td style="text-align: center;">$0.81_{02}$</td>
<td style="text-align: center;">$0.83_{01}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.76_{00}$</td>
<td style="text-align: center;">$0.77_{06}$</td>
<td style="text-align: center;">$0.80_{04}$</td>
<td style="text-align: center;">$0.83_{02}$</td>
<td style="text-align: center;">$0.83_{03}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">$0.86_{00}$</td>
<td style="text-align: center;">$0.86_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">Jungle Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.62_{09}$</td>
<td style="text-align: center;">$0.69_{09}$</td>
<td style="text-align: center;">$0.68_{04}$</td>
<td style="text-align: center;">$0.76_{03}$</td>
<td style="text-align: center;">$0.79_{01}$</td>
<td style="text-align: center;">$0.79_{00}$</td>
<td style="text-align: center;">$0.80_{01}$</td>
<td style="text-align: center;">$0.80_{00}$</td>
<td style="text-align: center;">$0.81_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">Logistic regression (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.62_{09}$</td>
<td style="text-align: center;">$0.69_{09}$</td>
<td style="text-align: center;">$0.68_{04}$</td>
<td style="text-align: center;">$0.76_{03}$</td>
<td style="text-align: center;">$0.79_{01}$</td>
<td style="text-align: center;">$0.79_{00}$</td>
<td style="text-align: center;">$0.80_{01}$</td>
<td style="text-align: center;">$0.80_{00}$</td>
<td style="text-align: center;">$0.81_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.79_{02}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">$0.91_{00}$</td>
<td style="text-align: center;">$0.98_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.79_{02}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">$0.91_{00}$</td>
<td style="text-align: center;">$0.98_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.58_{07}$</td>
<td style="text-align: center;">$0.72_{05}$</td>
<td style="text-align: center;">$0.78_{03}$</td>
<td style="text-align: center;">$0.81_{02}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.98_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">XGBoost (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.50_{00}$</td>
<td style="text-align: center;">$0.58_{07}$</td>
<td style="text-align: center;">$0.72_{05}$</td>
<td style="text-align: center;">$0.78_{03}$</td>
<td style="text-align: center;">$0.81_{02}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">$0.91_{01}$</td>
<td style="text-align: center;">$0.98_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">SAINT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.64_{05}$</td>
<td style="text-align: center;">$0.69_{06}$</td>
<td style="text-align: center;">$0.72_{05}$</td>
<td style="text-align: center;">$0.79_{02}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.83_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">$0.90_{00}$</td>
<td style="text-align: center;">$1.00_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.53_{09}$</td>
<td style="text-align: center;">$0.60_{05}$</td>
<td style="text-align: center;">$0.62_{03}$</td>
<td style="text-align: center;">$0.69_{04}$</td>
<td style="text-align: center;">$0.73_{04}$</td>
<td style="text-align: center;">$0.75_{02}$</td>
<td style="text-align: center;">$0.79_{02}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.99_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">NODE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.60_{01}$</td>
<td style="text-align: center;">$0.71_{03}$</td>
<td style="text-align: center;">$0.68_{04}$</td>
<td style="text-align: center;">$0.74_{02}$</td>
<td style="text-align: center;">$0.75_{04}$</td>
<td style="text-align: center;">$0.78_{01}$</td>
<td style="text-align: center;">$0.79_{01}$</td>
<td style="text-align: center;">$0.80_{00}$</td>
<td style="text-align: center;">$0.81_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.65_{08}$</td>
<td style="text-align: center;">$0.72_{04}$</td>
<td style="text-align: center;">$0.71_{07}$</td>
<td style="text-align: center;">$0.78_{02}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">$0.91_{00}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabPFN (ordinal)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$0.65_{08}$</td>
<td style="text-align: center;">$0.72_{04}$</td>
<td style="text-align: center;">$0.71_{07}$</td>
<td style="text-align: center;">$0.78_{02}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">$0.91_{00}$</td>
<td style="text-align: center;">$0.93_{00}$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text GPT-3)</td>
<td style="text-align: center;">$0.56_{01}$</td>
<td style="text-align: center;">$0.58_{02}$</td>
<td style="text-align: center;">$0.55_{02}$</td>
<td style="text-align: center;">$0.60_{06}$</td>
<td style="text-align: center;">$0.68_{03}$</td>
<td style="text-align: center;">$0.74_{03}$</td>
<td style="text-align: center;">$0.77_{01}$</td>
<td style="text-align: center;">$0.81_{01}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text T0)</td>
<td style="text-align: center;">$0.63_{00}$</td>
<td style="text-align: center;">$0.63_{04}$</td>
<td style="text-align: center;">$0.64_{05}$</td>
<td style="text-align: center;">$0.62_{06}$</td>
<td style="text-align: center;">$0.70_{01}$</td>
<td style="text-align: center;">$0.71_{01}$</td>
<td style="text-align: center;">$0.74_{02}$</td>
<td style="text-align: center;">$0.78_{02}$</td>
<td style="text-align: center;">$0.82_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Table-To-Text)</td>
<td style="text-align: center;">$0.51_{01}$</td>
<td style="text-align: center;">$0.60_{02}$</td>
<td style="text-align: center;">$0.60_{04}$</td>
<td style="text-align: center;">$0.63_{05}$</td>
<td style="text-align: center;">$0.69_{03}$</td>
<td style="text-align: center;">$0.75_{01}$</td>
<td style="text-align: center;">$0.78_{01}$</td>
<td style="text-align: center;">$0.82_{01}$</td>
<td style="text-align: center;">$0.85_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + Text Template)</td>
<td style="text-align: center;">$0.60_{00}$</td>
<td style="text-align: center;">$0.64_{01}$</td>
<td style="text-align: center;">$0.64_{02}$</td>
<td style="text-align: center;">$0.65_{03}$</td>
<td style="text-align: center;">$0.71_{02}$</td>
<td style="text-align: center;">$0.78_{02}$</td>
<td style="text-align: center;">$0.81_{02}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.89_{01}$</td>
<td style="text-align: center;">$1.00 \dagger$</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Template)</td>
<td style="text-align: center;">$0.63_{00}$</td>
<td style="text-align: center;">$0.65_{01}$</td>
<td style="text-align: center;">$0.66_{01}$</td>
<td style="text-align: center;">$0.66_{04}$</td>
<td style="text-align: center;">$0.71_{03}$</td>
<td style="text-align: center;">$0.78_{02}$</td>
<td style="text-align: center;">$0.81_{03}$</td>
<td style="text-align: center;">$0.84_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Only Values)</td>
<td style="text-align: center;">$0.58_{00}$</td>
<td style="text-align: center;">$0.60_{03}$</td>
<td style="text-align: center;">$0.62_{03}$</td>
<td style="text-align: center;">$0.63_{02}$</td>
<td style="text-align: center;">$0.65_{04}$</td>
<td style="text-align: center;">$0.73_{01}$</td>
<td style="text-align: center;">$0.76_{01}$</td>
<td style="text-align: center;">$0.82_{01}$</td>
<td style="text-align: center;">$0.88_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Names)</td>
<td style="text-align: center;">$0.40_{00}$</td>
<td style="text-align: center;">$0.53_{06}$</td>
<td style="text-align: center;">$0.55_{05}$</td>
<td style="text-align: center;">$0.63_{10}$</td>
<td style="text-align: center;">$0.72_{03}$</td>
<td style="text-align: center;">$0.79_{02}$</td>
<td style="text-align: center;">$0.80_{03}$</td>
<td style="text-align: center;">$0.84_{02}$</td>
<td style="text-align: center;">$0.89_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 + List Perm. Values)</td>
<td style="text-align: center;">$0.48_{00}$</td>
<td style="text-align: center;">$0.50_{02}$</td>
<td style="text-align: center;">$0.52_{03}$</td>
<td style="text-align: center;">$0.53_{03}$</td>
<td style="text-align: center;">$0.55_{01}$</td>
<td style="text-align: center;">$0.59_{02}$</td>
<td style="text-align: center;">$0.63_{01}$</td>
<td style="text-align: center;">$0.72_{02}$</td>
<td style="text-align: center;">$0.75_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">TabLLM (T0 3B + Text Template)</td>
<td style="text-align: center;">$0.54_{00}$</td>
<td style="text-align: center;">$0.63_{02}$</td>
<td style="text-align: center;">$0.64_{04}$</td>
<td style="text-align: center;">$0.67_{03}$</td>
<td style="text-align: center;">$0.72_{03}$</td>
<td style="text-align: center;">$0.77_{02}$</td>
<td style="text-align: center;">$0.80_{02}$</td>
<td style="text-align: center;">$0.83_{01}$</td>
<td style="text-align: center;">$0.87_{01}$</td>
<td style="text-align: center;">*</td>
</tr>
</tbody>
</table>
<ul>
<li>Result omitted due to runtime limitations of TabLLM on the full dataset.</li>
<li>These experiments were only performed for a single run due to runtime limitations of TabLLM on the full dataset.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ U.S. Bureau of Labor Statistics, CPI Inflation Calculator: https://www.bls.gov/data/inflation_calculator.htm&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
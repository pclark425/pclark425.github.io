<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-818 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-818</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-818</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-270068266</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.18208v1.pdf" target="_blank">A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e818.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e818.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deployed conversational large language model from OpenAI used in this paper as an agent backbone. Evaluated in multi-agent prompting setups on the TravelPlanner benchmark using a human-like planning framework (outline, strategy/knowledge blocks, plan search).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer-based conversational LLM accessed via API and used as the primary language model for multiple agent roles (Thought Agent, Tool Agent, Plan Agent, Evaluate Agent) within a multi-agent prompting framework; no model fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TravelPlanner (travel planning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning (multi-phase planning; information collection + daily plan generation), tool-interactive planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Using the proposed human-like planning framework: Delivery Rate 100.0%; Commonsense Pass Rate (Micro) 75.1%, (Macro) 15.6%; Hard Constraint Pass Rate (Micro) 15.5%, (Macro) 4.4%; Final Pass Rate 2.2%. Baseline TravelPlanner results (from prior work) were Delivery 86.7%, Commonsense Micro 54.0%, Macro 0.0%, Final 0.0% (paper reports large improvements under framework).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Multi-agent prompting framework (separate agents for outline, thought, tool invocation, plan generation, evaluation); Strategy Block (task-specific short-term planning state), Knowledge Block (stack-like recorded knowledge / short-horizon pop-out), Tool Agent (function-call formatting), Plan Search and Evaluate Agent (generate multiple candidate plans, rank via code-based checks); Outline Generation (Route/Keypoints/Commonsense agents).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / multi-agent in-context prompting only (no fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid approach: architectural/prompting intervention (multi-agent architecture + memory-like Knowledge Block + plan search/evaluation + structured tools in context)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Authors introduce a human-like reasoning framework composed of: (1) Outline Generation (route + transportation evaluation + keypoints/commonsense agents) to reduce invalid routes; (2) Information Collection using a Strategy Block (records outline + collected data descriptors) and Knowledge Block (records tool outputs with descriptions; stack-like pop-out of recent two days), and a Tool Agent to format proper function/tool calls; (3) Plan Making with daily planning granularity, Plan Search that generates multiple candidate daily plans and an Evaluate Agent that converts plans to JSON, identifies/ranks errors, and feeds errors back into the Strategy Block. Temperature and sampling settings are adjusted (Plan Agent temperature 0.7) to enable plan diversity during search.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial improvements on the interactive planning task: for GPT-3.5-Turbo Delivery Rate rose from 86.7% (baseline TravelPlanner) to 100%; Commonsense Micro increased from 54.0% to 75.1%; Commonsense Macro from 0.0% to 15.6%; Hard Constraint Micro from 0.0% to 15.5%; Final Pass Rate from 0.0% to 2.2%. Ablation shows removal of Outline Generation, Strategy Block, Knowledge Block, or Plan Search reduces metrics significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper argues LLMs are strong on single-shot/textual tasks but struggle on Multi-Phase planning because: (1) task requires multi-stage, dependent substeps that are not well captured by single-turn prompting or instruction-tuning data; (2) information provided is insufficient so agents must proactively gather and manage external information; (3) hallucination and omission errors arise when information collection is inadequate; (4) long-context management issues (knowledge overflow) hamper plan synthesis — motivating the Knowledge Block and daily planning; (5) stronger internal reasoning (e.g., GPT-4) can sometimes overcomplicate and loop (repeated function calls) hindering progress.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e818.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e818.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art OpenAI LLM used as an agent backbone; the paper evaluates it within the proposed human-like multi-agent planning framework and contrasts its behavior with baselines and other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced conversational transformer LLM used as multiple agent roles within the multi-agent prompting framework; no model fine-tuning or architectural modification to the base model is reported — improvements come from the framework and prompting/agent orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TravelPlanner (travel planning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning (multi-phase planning; information collection + daily plan generation), tool-interactive planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Under the proposed framework: Commonsense Pass Rate (Micro) improved from 61.1% (baseline) to 74.6%; Commonsense Macro improved from 2.8% to 24.4%; Hard Constraint Pass Rate (Micro) improved from 15.2% to 35.7%; Hard Constraint Macro from 10.6% to 16.7%. Paper reports GPT-4-Turbo's Final Pass Rate increased 10× relative to the prior algorithm (TravelPlanner baseline). Delivery Rate improvement was limited (paper notes repeated function calls caused redundancy and lower Delivery Rate gains).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Same multi-agent prompting architecture as above when used; no internal architectural changes to GPT-4-Turbo itself. The framework layers external modules (Strategy Block, Knowledge Block, Tool Agent, Plan Search) around the model.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context multi-agent prompting (no additional fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid approach: architectural/prompting intervention (multi-agent orchestration + memory components + plan search/evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Identical human-like reasoning framework applied to GPT-4-Turbo: outline generation (route & transportation eval), Strategy Block to track collection state, Knowledge Block to store and pop recent facts, Tool Agent to format function calls, Plan Agent with plan search producing multiple candidates and Evaluate Agent to rank and detect errors. The framework deliberately uses daily granular planning rather than planning entire trip at once to reduce complexity and context length.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Marked improvements in constraint satisfaction metrics (see interactive_performance). Notably, GPT-4-Turbo's Final Pass Rate increased roughly 10× over the prior algorithm on TravelPlanner (exact baseline/final numeric values for final pass rate are described qualitatively and by relative improvement in the paper). However, Delivery Rate gains were limited for GPT-4-Turbo due to repeated tool/function calls and overcomplication by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper hypothesizes that even strong LLMs like GPT-4-Turbo struggle on multi-phase procedural planning due to: insufficient structured external information leading to hallucinations and omissions, difficulty managing long and evolving plan state, and a tendency for stronger reasoning systems to over-search/over-refine (producing redundant tool calls and loops) which harms task progress.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e818.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e818.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-32K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-32K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open / third-party 7B-parameter language model with an extended context window (32K) evaluated in the paper under the human-like planning framework to test generality across model families and sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Mistral-7B-32K</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A ~7B-parameter transformer LLM with a large (32K) context window used as agent models in the multi-agent prompting framework; no additional model fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TravelPlanner (travel planning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning (multi-phase planning; information collection + daily plan generation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper reports that under the proposed framework Mistral-7B's Delivery Rate improved from 8.9% (baseline TravelPlanner result) to 39.4% (using the framework). Other detailed pass-rate metrics (Commonsense/Hard/Final) are discussed qualitatively as improved but specific numbers for Mistral under the framework are not fully tabulated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Used within the same multi-agent prompting architecture; benefits from Knowledge Block and Strategy Block; no internal architectural changes to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning (no fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural/prompting intervention (framework wrappers and tools)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same human-like planning framework applied: outline generation, Strategy Block, Knowledge Block, Tool Agent, Plan Search. For smaller/open models, framework aims to compensate for weaker inherent planning by structured decomposition and external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Delivery Rate rose substantially (reported from 8.9% baseline to 39.4% with framework). Other metric improvements are reported as present but specific before/after values for all metrics are not fully enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Smaller models lack robust internal planning and long-horizon state management; framework offsets this by externalizing planning components (strategy/knowledge) and search, reducing hallucination and invalid route generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e818.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e818.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (8×7B Mixture-of-Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mixture-of-Experts model variant referenced and evaluated in the baseline TravelPlanner comparisons; shown in baseline tables but the paper focuses on the framework's improvements across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Mixtral-8×7B-MoE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mixture-of-experts family model (presented in baseline comparisons) used in TravelPlanner baseline tables; in-paper experiments include this model family for cross-model comparison under the framework.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TravelPlanner (travel planning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning (multi-phase planning)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Baseline TravelPlanner table fragment shows Mixtral baseline numbers (e.g., Delivery Rate ~49.4%, Commonsense Micro ~30.0%, Macro 0.0%, Hard Micro ~1.2%, Hard Macro ~0.6%, Final ~0.0%). The paper states that using their framework improves Delivery Rate and other metrics for all tested models, but full post-intervention numbers for Mixtral are not exhaustively provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Evaluated as a black-box LLM within the multi-agent prompting framework; no internal architectural changes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning (no fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting/architectural intervention (framework wrappers and plan search)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Framework components (outline, strategy/knowledge blocks, tool formatting, plan search with Evaluate Agent) applied similarly to Mixtral models to reduce invalid routes, hallucination and to improve plan correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Paper reports overall metric improvements across models including Mixtral (Delivery Rate and commonsense/hard constraint pass rates improved), but full explicit before/after numbers for Mixtral under the framework are not fully enumerated in-text beyond the baseline table fragment shown.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Same multi-phase, context-length, and information-collection challenges: MoE and other architectures still require external structuring and memory to manage multi-step planning tasks effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Travelplanner: A benchmark for real-world planning with language agents. <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report. <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 1)</em></li>
                <li>What are tools anyway? a survey from the language model perspective. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-818",
    "paper_id": "paper-270068266",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo (OpenAI)",
            "brief_description": "A deployed conversational large language model from OpenAI used in this paper as an agent backbone. Evaluated in multi-agent prompting setups on the TravelPlanner benchmark using a human-like planning framework (outline, strategy/knowledge blocks, plan search).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-3.5-Turbo",
            "model_description": "Pretrained transformer-based conversational LLM accessed via API and used as the primary language model for multiple agent roles (Thought Agent, Tool Agent, Plan Agent, Evaluate Agent) within a multi-agent prompting framework; no model fine-tuning reported.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "TravelPlanner (travel planning benchmark)",
            "interactive_task_type": "planning (multi-phase planning; information collection + daily plan generation), tool-interactive planning",
            "interactive_performance": "Using the proposed human-like planning framework: Delivery Rate 100.0%; Commonsense Pass Rate (Micro) 75.1%, (Macro) 15.6%; Hard Constraint Pass Rate (Micro) 15.5%, (Macro) 4.4%; Final Pass Rate 2.2%. Baseline TravelPlanner results (from prior work) were Delivery 86.7%, Commonsense Micro 54.0%, Macro 0.0%, Final 0.0% (paper reports large improvements under framework).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Multi-agent prompting framework (separate agents for outline, thought, tool invocation, plan generation, evaluation); Strategy Block (task-specific short-term planning state), Knowledge Block (stack-like recorded knowledge / short-horizon pop-out), Tool Agent (function-call formatting), Plan Search and Evaluate Agent (generate multiple candidate plans, rank via code-based checks); Outline Generation (Route/Keypoints/Commonsense agents).",
            "training_method": "prompting / multi-agent in-context prompting only (no fine-tuning reported)",
            "intervention_type": "hybrid approach: architectural/prompting intervention (multi-agent architecture + memory-like Knowledge Block + plan search/evaluation + structured tools in context)",
            "intervention_description": "Authors introduce a human-like reasoning framework composed of: (1) Outline Generation (route + transportation evaluation + keypoints/commonsense agents) to reduce invalid routes; (2) Information Collection using a Strategy Block (records outline + collected data descriptors) and Knowledge Block (records tool outputs with descriptions; stack-like pop-out of recent two days), and a Tool Agent to format proper function/tool calls; (3) Plan Making with daily planning granularity, Plan Search that generates multiple candidate daily plans and an Evaluate Agent that converts plans to JSON, identifies/ranks errors, and feeds errors back into the Strategy Block. Temperature and sampling settings are adjusted (Plan Agent temperature 0.7) to enable plan diversity during search.",
            "intervention_effect": "Substantial improvements on the interactive planning task: for GPT-3.5-Turbo Delivery Rate rose from 86.7% (baseline TravelPlanner) to 100%; Commonsense Micro increased from 54.0% to 75.1%; Commonsense Macro from 0.0% to 15.6%; Hard Constraint Micro from 0.0% to 15.5%; Final Pass Rate from 0.0% to 2.2%. Ablation shows removal of Outline Generation, Strategy Block, Knowledge Block, or Plan Search reduces metrics significantly.",
            "hypothesized_cause_of_gap": "Paper argues LLMs are strong on single-shot/textual tasks but struggle on Multi-Phase planning because: (1) task requires multi-stage, dependent substeps that are not well captured by single-turn prompting or instruction-tuning data; (2) information provided is insufficient so agents must proactively gather and manage external information; (3) hallucination and omission errors arise when information collection is inadequate; (4) long-context management issues (knowledge overflow) hamper plan synthesis — motivating the Knowledge Block and daily planning; (5) stronger internal reasoning (e.g., GPT-4) can sometimes overcomplicate and loop (repeated function calls) hindering progress.",
            "uuid": "e818.0",
            "source_info": {
                "paper_title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4-Turbo",
            "name_full": "GPT-4-Turbo (OpenAI)",
            "brief_description": "A state-of-the-art OpenAI LLM used as an agent backbone; the paper evaluates it within the proposed human-like multi-agent planning framework and contrasts its behavior with baselines and other models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4-Turbo",
            "model_description": "Advanced conversational transformer LLM used as multiple agent roles within the multi-agent prompting framework; no model fine-tuning or architectural modification to the base model is reported — improvements come from the framework and prompting/agent orchestration.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "TravelPlanner (travel planning benchmark)",
            "interactive_task_type": "planning (multi-phase planning; information collection + daily plan generation), tool-interactive planning",
            "interactive_performance": "Under the proposed framework: Commonsense Pass Rate (Micro) improved from 61.1% (baseline) to 74.6%; Commonsense Macro improved from 2.8% to 24.4%; Hard Constraint Pass Rate (Micro) improved from 15.2% to 35.7%; Hard Constraint Macro from 10.6% to 16.7%. Paper reports GPT-4-Turbo's Final Pass Rate increased 10× relative to the prior algorithm (TravelPlanner baseline). Delivery Rate improvement was limited (paper notes repeated function calls caused redundancy and lower Delivery Rate gains).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Same multi-agent prompting architecture as above when used; no internal architectural changes to GPT-4-Turbo itself. The framework layers external modules (Strategy Block, Knowledge Block, Tool Agent, Plan Search) around the model.",
            "training_method": "prompting / in-context multi-agent prompting (no additional fine-tuning reported)",
            "intervention_type": "hybrid approach: architectural/prompting intervention (multi-agent orchestration + memory components + plan search/evaluation)",
            "intervention_description": "Identical human-like reasoning framework applied to GPT-4-Turbo: outline generation (route & transportation eval), Strategy Block to track collection state, Knowledge Block to store and pop recent facts, Tool Agent to format function calls, Plan Agent with plan search producing multiple candidates and Evaluate Agent to rank and detect errors. The framework deliberately uses daily granular planning rather than planning entire trip at once to reduce complexity and context length.",
            "intervention_effect": "Marked improvements in constraint satisfaction metrics (see interactive_performance). Notably, GPT-4-Turbo's Final Pass Rate increased roughly 10× over the prior algorithm on TravelPlanner (exact baseline/final numeric values for final pass rate are described qualitatively and by relative improvement in the paper). However, Delivery Rate gains were limited for GPT-4-Turbo due to repeated tool/function calls and overcomplication by the model.",
            "hypothesized_cause_of_gap": "Paper hypothesizes that even strong LLMs like GPT-4-Turbo struggle on multi-phase procedural planning due to: insufficient structured external information leading to hallucinations and omissions, difficulty managing long and evolving plan state, and a tendency for stronger reasoning systems to over-search/over-refine (producing redundant tool calls and loops) which harms task progress.",
            "uuid": "e818.1",
            "source_info": {
                "paper_title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Mistral-7B-32K",
            "name_full": "Mistral-7B-32K",
            "brief_description": "An open / third-party 7B-parameter language model with an extended context window (32K) evaluated in the paper under the human-like planning framework to test generality across model families and sizes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Mistral-7B-32K",
            "model_description": "A ~7B-parameter transformer LLM with a large (32K) context window used as agent models in the multi-agent prompting framework; no additional model fine-tuning reported.",
            "model_size": "7B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "TravelPlanner (travel planning benchmark)",
            "interactive_task_type": "planning (multi-phase planning; information collection + daily plan generation)",
            "interactive_performance": "Paper reports that under the proposed framework Mistral-7B's Delivery Rate improved from 8.9% (baseline TravelPlanner result) to 39.4% (using the framework). Other detailed pass-rate metrics (Commonsense/Hard/Final) are discussed qualitatively as improved but specific numbers for Mistral under the framework are not fully tabulated in the text.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Used within the same multi-agent prompting architecture; benefits from Knowledge Block and Strategy Block; no internal architectural changes to the model.",
            "training_method": "prompting / in-context learning (no fine-tuning reported)",
            "intervention_type": "architectural/prompting intervention (framework wrappers and tools)",
            "intervention_description": "Same human-like planning framework applied: outline generation, Strategy Block, Knowledge Block, Tool Agent, Plan Search. For smaller/open models, framework aims to compensate for weaker inherent planning by structured decomposition and external memory.",
            "intervention_effect": "Delivery Rate rose substantially (reported from 8.9% baseline to 39.4% with framework). Other metric improvements are reported as present but specific before/after values for all metrics are not fully enumerated in the paper.",
            "hypothesized_cause_of_gap": "Smaller models lack robust internal planning and long-horizon state management; framework offsets this by externalizing planning components (strategy/knowledge) and search, reducing hallucination and invalid route generation.",
            "uuid": "e818.2",
            "source_info": {
                "paper_title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Mixtral-8x7B-MoE",
            "name_full": "Mixtral (8×7B Mixture-of-Experts)",
            "brief_description": "A Mixture-of-Experts model variant referenced and evaluated in the baseline TravelPlanner comparisons; shown in baseline tables but the paper focuses on the framework's improvements across model families.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Mixtral-8×7B-MoE",
            "model_description": "A mixture-of-experts family model (presented in baseline comparisons) used in TravelPlanner baseline tables; in-paper experiments include this model family for cross-model comparison under the framework.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "TravelPlanner (travel planning benchmark)",
            "interactive_task_type": "planning (multi-phase planning)",
            "interactive_performance": "Baseline TravelPlanner table fragment shows Mixtral baseline numbers (e.g., Delivery Rate ~49.4%, Commonsense Micro ~30.0%, Macro 0.0%, Hard Micro ~1.2%, Hard Macro ~0.6%, Final ~0.0%). The paper states that using their framework improves Delivery Rate and other metrics for all tested models, but full post-intervention numbers for Mixtral are not exhaustively provided in the text.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Evaluated as a black-box LLM within the multi-agent prompting framework; no internal architectural changes reported.",
            "training_method": "prompting / in-context learning (no fine-tuning reported)",
            "intervention_type": "prompting/architectural intervention (framework wrappers and plan search)",
            "intervention_description": "Framework components (outline, strategy/knowledge blocks, tool formatting, plan search with Evaluate Agent) applied similarly to Mixtral models to reduce invalid routes, hallucination and to improve plan correctness.",
            "intervention_effect": "Paper reports overall metric improvements across models including Mixtral (Delivery Rate and commonsense/hard constraint pass rates improved), but full explicit before/after numbers for Mixtral under the framework are not fully enumerated in-text beyond the baseline table fragment shown.",
            "hypothesized_cause_of_gap": "Same multi-phase, context-length, and information-collection challenges: MoE and other architectures still require external structuring and memory to manage multi-step planning tasks effectively.",
            "uuid": "e818.3",
            "source_info": {
                "paper_title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Travelplanner: A benchmark for real-world planning with language agents.",
            "rating": 2,
            "sanitized_title": "travelplanner_a_benchmark_for_realworld_planning_with_language_agents"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Gpt-4 technical report.",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "What are tools anyway? a survey from the language model perspective.",
            "rating": 2,
            "sanitized_title": "what_are_tools_anyway_a_survey_from_the_language_model_perspective"
        }
    ],
    "cost": 0.014866,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models
28 May 2024</p>
<p>Chengxing Xie xiechengxing34@gmail.com 
Difan Zou dzou@cs.hku.hk 
El Paso Exploring 
El Paso </p>
<p>Department of Computer Science
The University of Hong Kong</p>
<p>Department of Computer Science Institute of Data Science
The University of Hong Kong</p>
<p>A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models
28 May 202404110BF93B2CD87D8D78C83DE44DED7DarXiv:2405.18208v1[cs.AI]
Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies.However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue.In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties.Existing reasoning approaches have struggled to effectively address this complex task[54].Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems.Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns.Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning.Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness.Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains 10× the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.* This work was done when Chengxing Xie was an intern at HKU.</p>
<p>Introduction</p>
<p>Recently, large language models (LLMs), such as GPTs [1,34] and LLaMAs [44], have demonstrated remarkable potential in handling diverse tasks across various domains, showcasing impressive generalization capabilities.[46] and [33] show that LLM agents could play card games with humanlike proficiency and, in some instances, even outperform most human players in some games [10].Moreover, [61], [9], and [11] revealed that LLM agents could solve daily tasks such as cooking and door-opening.Additionally, LLM agents can also establish complex software systems through collaboration with different agent roles [18,38].</p>
<p>Nevertheless, LLM agents still struggle with certain tasks [23,45,51], which are relatively solvable to humans.We categorize these challenges as Multi-Phases Planning Tasks.These tasks are difficult because their solutions can be divided into multiple interconnected phases, each requiring Preprint.Under review.</p>
<p>Figure 1: Our human-like planning framework consists of three key parts.In the Outline Generation phase, LLM agents produce rough plans and identify key information related to the query, establishing the foundation for detailed future planning.In the Information Collection phase, LLM agents gather the essential data required for comprehensive planning.Finally, in the Plan Making phase, LLM agents explore the potential plan space and return a well-structured, reasonable plan.shared information and having dependencies with each other.For example, creating a travel plan [54] requires humans to plan for multiple days, encompassing various aspects.This involves addressing different elements in different phases of the planning process.Similarly, solving GitHub issues involves checking various functions and systematically locating and resolving code problems one by one [23].The procedures for solving these tasks are often not readily available online and cannot be easily converted into instruction fine-tuning data formats to train LLMs.Consequently, tackling these types of tasks remains a significant challenge for today's LLM agents [23,45,51,54].</p>
<p>We categorize the stages of solving such tasks into three main phases, detailed as follows: Outline Generation Phase: When confronted with a Multi-Phase Problem, individuals typically begin by sketching a preliminary outline outlining the path toward a solution.Information Collection Phase: Generally, to solve this type of task, the information provided in the task description alone is completely insufficient.Therefore, individuals must proactively identify the required information and ascertain how to obtain it effectively.Plan Making Phase: Armed with the necessary information, individuals proceed to formulate a plan.Multi-phase problems inherently entail numerous constraints, encompassing factors such as human resources, physical limitations, and more.Consequently, devising effective plans necessitates meticulous consideration of diverse constraints.Given the challenges posed by Multi-Phase Problems, we have chosen travel planning as our focal point for several reasons.Firstly, it is a time-consuming and often challenging problem for many people, demonstrating its difficulty.Secondly, travel planning inherently involves a multitude of constraints, requires long-term strategizing, and demands significant travel-related information gathering.These complexities make travel planning a fitting subject for exploring and improving planning strategies.</p>
<p>Previous reasoning strategies [42,49,59] have shown limited effectiveness in addressing the complexities of travel planning [54].However, by understanding and emulating the processes humans use to tackle Multi-Phases Problems, we can develop more robust planning frameworks to enable LLM agents to tackle such challenges effectively.Our proposed framework aims to capture the essence of human reasoning processes, particularly in the context of Multi-Phase Problems, to enhance the performance of LLM agents.</p>
<p>Our framework is composed of three main components.The first is Outline Generation.When tackling a complex problem, people often create a rough plan to guide them through the process.Similarly, we generate a travel planning outline using multi-agent collaboration, where each agent contributes to a specific part of our travel outline, establishing a foundational guide for planning.</p>
<p>The second component is Information Collection.Just as people gather relevant travel information, LLM agents require sufficient data to create reliable plans.This step ensures that agents have all the necessary details for making informed decisions.The third component is Plan Making.After collecting adequate information, the LLM agent creates detailed daily plans.Given the inherent complexity of travel planning, we employ a plan search and evaluation method.Multiple plans are generated in each iteration, and an evaluation agent identifies the best plan and flags errors in each option.Our framework demonstrates that this structured approach leads to impressive performance on this complex task, improving the complex task planning ability of LLM agents.The major findings of our work are summarized as follows:</p>
<p>• Our work underscores the challenges LLM agents face with Multi-Phase Planning tasks, highlighting the necessity of employing a novel reasoning framework for effective problem-solving.</p>
<p>• We found that utilizing a human reasoning framework for complex tasks is a key factor in enhancing LLM agents' performance on these tasks.</p>
<p>• By identifying key factors contributing to effective problem-solving and integrating them into a human-like planning framework, we enable LLM agents to exhibit human-like reasoning, resulting in impressive performance improvements, significantly enhancing LLM agents' performance in travel planning tasks.</p>
<p>• Through extensive experimentation, we validate the effectiveness of our approach, demonstrating its capability to tackle complex planning problems with remarkable success.</p>
<p>2 Related Works</p>
<p>Reasoning Strategy for LLM Agents</p>
<p>Planning without Additional Components: In recent times, the development of large language models (LLMs) agents has enabled numerous tasks to be addressed by directly inputting questions into these models.However, LLM agents often struggle with some problem-solving, particularly in areas like mathematics and other intricate planning tasks.To address this limitation, recent research has developed various reasoning strategies aimed at enhancing the problem-solving abilities of LLM agents [20].One common strategy, employed by works like [41,49,59], leverages the divide-andconquer approach.This involves breaking down a complex task into simpler subtasks and allowing the LLM agents to tackle each subtask sequentially.This method has shown improved performance across a range of tasks.Other works [3,16,47,52,58,63] employ similar strategies using tree searches, such as Monte Carlo Tree Search (MCTS), A*, Breadth-First Search (BFS), and Depth-First Search (DFS).They generate multiple alternative plans via various sampling methods and select the optimal plan through different selection techniques.Reflecting on and refining plans based on prior experiences also significantly improves the planning process of LLM agents [13,19,31,42].By reevaluating their plans, LLM agents can avoid recurring errors, thereby enhancing their problem-solving capabilities.</p>
<p>Planning with Addition Components: The planning capabilities of LLM agents can also be improved through interactions with other components.For instance, [28], [14], [8], and [7] combine symbolic planners with LLM agents, utilizing the natural language generation capabilities of LLMs to create formalized task descriptions.Other works [24,29,32,57] enhance the problem-solving ability of LLMs by integrating additional memory modules.A particularly effective approach involves enabling LLM agents to access various APIs and tools [30,35,37,39,40,48,56,60]. Some tools can be directly included in the context [41,43,50] as prompt.However, improving an LLM agent's toolusing abilities often requires generating appropriate datasets and fine-tuning the models [37,40,56].By providing access to external tools, LLM agents can tackle tasks that are otherwise challenging, such as computational problems and formula verification.Our planning framework integrates these diverse reasoning strategies to improve overall planning capabilities, allowing for more sophisticated problem-solving and strengthening the effectiveness of LLM agents in handling complex tasks.</p>
<p>Multi-Agents Framework</p>
<p>Multi-agent frameworks [15] have garnered significant interest from researchers due to their flexibility across a broad range of tasks.These frameworks can be utilized in simulations of various kinds, including game simulations [33,46,55], economic simulations [26,27], and societal simulations [2,12,36,53,62].Moreover, multi-agent frameworks demonstrate superior performance compared to single-agent systems in solving diverse tasks.For instance, [25], [17], and [6] have developed general multi-agent frameworks that enhance task performance.For specific tasks, works like [17,18,38] propose specialized frameworks that enable agents to autonomously develop software.In addition, multi-agent systems have been effectively employed for reasoning in embodied environments [9,61] and have demonstrated their potential in scientific research [4,5].Our framework also incorporates a multi-agent system to enhance overall planning capabilities, leveraging the collaborative and complementary strengths of multiple agents for more effective problem-solving.</p>
<p>3 Method</p>
<p>Task Description:</p>
<p>We introduce the task setting of TravelPlanner [54], where the objective is for LLM agents to provide a reasonable travel plan based on a given query, while adhering to the specified constraints and leveraging commonsense knowledge.Each day's plan includes details about the current city, attractions, accommodation, transportation, and dining options (See Appendix A.6 for travel plan examples).A reasonable plan must comply with commonsense principles and meet all requirements specified in the travel query, such as avoiding repeated visits to the same attractions and ensuring the plan stays within the specified budget.An initial analysis of GPT-4-Turbo's performance in [54] revealed that the most advanced model still struggles with efficient information collection and valid plan making.The details are as follows:</p>
<p>• Generating unreasonable travel routes, such as a number of cities visited that is inconsistent with the query or not returning to the original city on the last day.</p>
<p>• Utilizing unreasonable transportation methods, like combining self-driving and flight in one trip.</p>
<p>• The plan made may include unreal information (hallucination) in the plans.</p>
<p>• LLM agents may omit essential information when making the plan and disregard the constraints specified in the query, leading to problematic and incomplete plans.</p>
<p>To address the issues of unreasonable travel routes and transportation methods, we employ the Outline Generation phase (Section 3.3) to create logical outlines and appropriate transportation options.To alleviate the hallucination problem, we enhance the Information Collection phase (Section 3.4) to ensure more accurate data gathering.Finally, to deal with the missing information during plan creation, we utilize the Plan Making phase (Section 3.5) to support the development of correct and comprehensive plans.</p>
<p>Framework Overview</p>
<p>Our framework, as shown in Fig 1, employs a human-like reasoning approach to address travel planning problems.Based on the specific query, we first generate an outline for the query (Section 3.3).Following the Outline Generation, LLM agents equipped with Strategy block and Knowledge Block proceed with Information Collection (Section 3.4).During this stage, once sufficient information is gathered for a single day's plan, the agent creates the daily plan in Plan Making phase (Section 3.5).</p>
<p>The final travel plan is composed of each daily plan.</p>
<p>Outline Generation</p>
<p>Each travel query (details in Appendix A.5) involves numerous elements and spans several days, making it challenging to generate a comprehensive plan all at once.When humans tackle such tasks, they typically begin by drafting a rough outline.This outline should encompass the route, transportation options, and key points in the query, facilitating the planning process (details in Appendix A.7).To replicate this approach, we introduce the Route Generation phase, which incorporates transportation evaluation to determine the route and transportation components.Additionally, we integrate two additional agents to generate the key points of the travel, enhancing the usefulness of the outlines.We will evaluate whether this route is reasonable under the limitation from the specific travel planning query, e.g., cannot take flight.If not reasonable, we will redo the route by giving feedback.Otherwise, if the route is not reasonable for some kind of transportation, we will add the constraint.</p>
<p>Route</p>
<p>Agent.This agent generates a rough route for the entire trip, including city transfers and exploratory travel information.This preliminary route serves as a guide for the subsequent Information Collection and Plan Making phases, providing a clear roadmap for the journey and making the planning process more structured and transparent.</p>
<p>Transportation Evaluation: When constructing a travel route, humans typically do not initially verify transportation availability between target cities, assessing options later based on preferences and availability.Similarly, we've observed that LLM agents may generate invalid routes due to a lack of detailed transportation information between cities.To address this issue, we propose adding an evaluation stage after route generation.As shown in Fig 2, during this stage, each route is evaluated to determine its rationality from a transportation perspective.If the route is perfect, we don't need to do anything.If the route cannot be traveled by driving, we inform the agent accordingly.Similarly, if the route cannot be reached by any means of transportation, we provide the feedback to the PathFinder Agent and prompt the agent to revise the route accordingly.</p>
<p>Keypoints Generation: When humans receive a query, they focus on the key points and ignore irrelevant details.Inspired by this, we introduced the Keypoints Agent (Fig 1 left) to identify the critical points in the query that need to be considered by the Plan Agent.Moreover, as [54] demonstrated, LLM agents tend to overlook common sense when planning, such as navigating the same restaurant or attraction.To address this issue, the Commonsense Agent (Fig 1 left) generates basic guides related to travel planning.This agent doesn't have access to the specific query, as we believe these guides are common sense and universally applicable to every query.</p>
<p>These three parts combine with each other to generate a useful and detailed travel outline, building a strong foundation for the planning task.</p>
<p>Information Collection</p>
<p>After the Outline Generation phase, humans proceed to collect necessary travel-related information, such as specific attractions and restaurants.Collecting sufficient information is crucial as it can significantly decrease the probability of hallucination.In our framework, the Information Collection process begins with the Thought Agent generating the next steps based on the Strategy Block.Subsequently, the Tool Agent utilizes the output of the Thought Agent to generate a suitable function expression.The result of this function, primarily comprising travel information, is then recorded in the Knowledge Block with a description from the Description Agent.Finally, the selected information is forwarded to the Plan Agent for generating the daily travel plan.</p>
<p>Strategy Block: In the information collection process, humans typically remember the types of information they have gathered, which guides their subsequent steps.To emulate this procedure, we introduce a component called the Strategy Block as shown in Fig 1, primarily used by the Thought Agent.First, the outline is stored in the Strategy Block to guide the information collection process.The Strategy Block also informs the Thought Agent which day it is in the travel plan and short descriptions of the collected data.Besides, we streamline the prompts for better workflow management.Compared to the scratchpad approach in [54], our design provides more task-specific details and better supports the Thought Agent in collecting information.</p>
<p>Tool Agent: We've observed that as the planning process progresses, the context length can become too long, leading to potential oversight of tool documents placed at the beginning of input messages.Thus, we introduce the Tool Agent tasked with generating the correct function expression format messages.The Tool Agent's input includes only the tool document, the query, and some previous content from the Strategy Block to ensure it can generate the correct information based on the context.Lastly, we found that presenting the data in a DataFrame format makes it difficult to read and interpret.Therefore, we have reorganized the information into a more reader-friendly structure.We directly specify each data's name just before presenting the data, like: Flight Number: F3502691 (see Appendix A.7.2 for more details).</p>
<p>Plan Making</p>
<p>When humans make plans, they typically do not plan everything at once; instead, they approach it step by step.Therefore, unlike the approach in [54] that plans the entire trip at once, we adopt a daily planning strategy.This method requires less information at each step, making the planning process easier.Each time the Thought Agent determines that sufficient information has been collected for a specific day, the Tool Agent calls the DailyPlanner tool, and the Plan Agent creates the daily plan.</p>
<p>Plan Search: When examining the results of Plan Agent, we observe that creating a plan often introduces various errors.These errors can stem from insufficient information collected during the information collection stage or from oversights during the planning stage.To address this issue, we propose a plan search method.</p>
<p>Each time the Plan Agent generates a daily plan, it creates several plans based on the same information.</p>
<p>An Evaluate Agent reviews each plan, converting them into JSON format and using code to identify and rank errors.Based on these evaluations, we select the best plan.If significant errors are found, these errors are recorded in the Strategy Block, prompting further information collection, and the plans with errors are discarded.This discard process occurs only once per day's plan to prevent unlimited planning iterations.Through daily planning and plan search strategies, the agent can generate more reasonable and reliable travel plans.8.9 5.9 0 0 0 0 Mixtral-8×7B-MoE [22] 49.4 30.0 0 1.2 0.6 0 GPT-3.5-Turbo[34] 86.7 54.0 0 0 0 0 GPT-4-Turbo [1] 89. 4  4 Experiment</p>
<p>Experiment Setup</p>
<p>We utilize the TravelPlanner benchmark proposed by [54], which includes a variety of travel queries with different travel lengths and difficulty levels.This benchmark has a train, valid, and test set.</p>
<p>The valid set has 180 queries and the test set has 1000 queries.We use the valid set to examine our framework as running the experiment on the test set is too expensive.We use gpt-3.5-turbo-1106,gpt-4-1106-preview, mixtral, mistral-7B-32K in our experiments [1,21,22,34].We ran open-source models on 4 NVIDIA RTX A6000.</p>
<p>Hyperparameters Settings: To ensure the reproducibility of our experimental results, we set the temperature to 0 for all processes except for Plan Agent, where it is set to 0.7 to allow the Plan Agent to generate diverse plans enabling a larger search space.</p>
<p>For the travel route generation, we limit the maximum number of retries to 3 to prevent infinite remaking of the travel route.The Plan Agent generates 3 different plans each time.If all generated plans contain errors, we revert to the information collection stage only once.If errors persist after this second attempt, we proceed with the process regardless.About the Knowledge Block, we set the minimal number of pop-outs is 5.</p>
<p>Metrics:</p>
<p>We use the same metrics proposed in TravelPlanner [54].</p>
<p>• Delivery Rate: Measures if agents can successfully deliver a final plan within a set number of steps (max 45).Failure includes dead loops, numerous failed attempts, or exceeding the step limit.P is the set of all plans.C p is the set of constraints applicable to a specific plan p.As shown in Table 1, we tested four LLMs on the dataset to verify our framework's effectiveness.All models exhibited increased Delivery Rate, with Mistral-7B showing an improvement of 8.9% to 39.4% and GPT-3.5-Turboachieving 100% Delivery Rate.This indicates that GPT-3.5-Turbo can generate a travel plan for every query, strongly demonstrating that our framework enables better planning for the task.</p>
<p>However, we observed limited improvement for GPT-4-Turbo on Delivery Rate.To understand the reason, we visualized the error distribution from GPT-4-Turbo in Fig 4 .We found that the primary issue was the repeated use of the same function three times, with the most frequent being AccommodationSearch.The agent repeatedly attempted to find the 'correct' room type, often overlooking some information, which led to redundancy.Additionally, GPT-4-Turbo aimed to find accommodations that could fit all travelers in one room, ignoring the possibility of booking multiple rooms.The lower Delivery Rate will cause a lower Micro Commonsense pass rate.We conjecture that GPT-4-Turbo's stronger reasoning capabilities hindered its progress by overcomplicating the task.</p>
<p>Commonsense Pass Rate: Our framework demonstrates impressive improvements in the Commonsense Pass Rate across all models.For GPT-3.5-Turbo, the Micro Pass Rate increased from 54.0% to 75.1%, and the Macro Pass Rate increased from 0% to 15.6%.For GPT-4-Turbo, the Micro Pass Rate increased from 61.1% to 74.6%, while the Macro Pass Rate saw a significant boost from 2.8% to 24.4%.Although Mistral and Mixtral showed improvements in their Micro Pass Rates, their Macro Pass Rates did not see significant increases.</p>
<p>Analyzing the commonsense error distribution in Fig 5, we find that GPT-3.5-Turbo and GPT-4-Turbo exhibit very similar error patterns.The top three errors are Hallucinated Information, Necessary Information Absent, and Invalid Accommodation, accounting for nearly 60% of all errors.Hallucinated Information indicates that the LLM agents still generate unreal information.Necessary Information Absent suggests that the Plan Agent might not strictly follow instructions or the information collection phase misses key details.Invalid Accommodation implies that some points in the accommodation data are overlooked.</p>
<p>Hard Constraints Pass Rate: Additionally, our framework enhances the Pass Rate for Hard Constraints.For GPT-4-Turbo, the Micro Pass Rate increased from 15.2% to 35.7%, and the Macro Pass Rate increased from 10.6% to 16.7%.While this improvement is not as substantial as that of the Commonsense Pass Rate, it is still a notable enhancement.For GPT-3.5-Turbo, the improvements were slightly lower, with increases from 0% to 15.5% for the Micro Pass Rate and from 0% to 4.4% for the Macro Pass Rate.The last two models, Mistral and Mixtral, showed only slight improvements.Examining the error distribution in Fig 5 , we observe that the main errors mainly appear in accommodation planning.This suggests that the concentrated information about each accommodation is challenging for LLM agents to correctly extract in making accurate plans.</p>
<p>Regarding the Final Pass Rate, GPT-3.5-Turbo'sresult using our framework surpasses the GPT-4-Turbo's result in the previous algorithm, showcasing the effectiveness of our framework.Notably, GPT-4-Turbo's Final Pass Rate increased 10× compared to the previous algorithm.This significant improvement underscores our framework's capability to enhance the performance and reliability of LLM agents in complex travel planning tasks.</p>
<p>Ablation Study</p>
<p>We analyzed the impact of each component of our framework by removing them one at a time, as shown in Table 2.When the outline generation component was removed, we observed a significant drop in all metrics, although they remained better than the TravelPlanner results.This indicates that Outlines Generation is a crucial element of our framework.</p>
<p>About Strategy Block, we removed some information from the Strategy Block, such as specific daytime guidance, Knowledge Block Information, Budget Requirements, and key points from the query.However, the rest of this block remains necessary because, without it, the information collection process cannot proceed.From the results, we observed that all metrics decreased.However, the degree of decrease in Delivery Rate and Micro Commonsense Pass Rate was not very substantial.This outcome is expected, as this component does not significantly impact the feasibility of creating a plan but rather the quality of the plans produced.</p>
<p>For the Knowledge Block, removing it entirely is impractical.Therefore, we adopted the method from [54], using an extra tool to record information.To prevent exceeding the maximum token limit, we implemented a maximum length limitation.Removing this block caused a notable decline in all metrics except Delivery Rate, echoing findings from the Strategy Block ablation study.These outcomes underscore the critical role of the Knowledge Block.</p>
<p>Additionally, when we removed the Plan Search component, we observed a dramatic drop in all metrics except Delivery Rate.The decrease was particularly severe in the Macro Commonsense Pass Rate compared to other ablation studies, highlighting the critical role of the Plan Search component.</p>
<p>Our ablation study results confirm that each block of our framework is essential.Removing any part will significantly decrease performance across various metrics.These parts are vital for managing information and enhancing plan quality, ensuring a robust and efficient planning process.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we presented a novel human-like reasoning framework for travel planning using LLM Agents.Our approach integrates several key components: Outline Generation, Information Collection with blocks, and Plan Search.These components synergistically mimic human problem-solving strategies, enabling LLM agents to handle multi-phase tasks more effectively.Extensive experiments on the TravelPlanner benchmark demonstrated significant improvements across multiple models with our framework.</p>
<p>Future work can focus on applying our planning framework to real-world planning problems in practical environments.Additionally, the current tools within our framework are limited.Enhancing the framework by incorporating more advanced tools, such as those with additional parameters and capabilities, could further improve its planning and problem-solving effectiveness.This expansion could enable more sophisticated and adaptable solutions, making our framework even more robust in addressing complex, real-world challenges.</p>
<p>A Appendix / supplemental material A.1 Limitation</p>
<p>Although our framework has significantly improved performance compared to previous studies, achieving a Final Pass Rate of 100% akin to human performance remains elusive.One limitation we observed is that even when provided with detailed information, LLM Agents still tend to overlook certain key points.Addressing this challenge may require the integration of alternative planning strategies.Additionally, effectively utilizing extensive accommodation-related information to generate reasonable plans remains a significant challenge, even for advanced models like GPT-4-Turbo.Furthermore, the occurrence of hallucinations during the planning process presents another obstacle that requires attention.</p>
<p>In future work, addressing these limitations could involve exploring novel planning strategies that complement the capabilities of LLM Agents, such as incorporating heuristics or domain-specific rules.Moreover, developing more sophisticated models that can better comprehend and utilize complex accommodation-related information could lead to more accurate and reliable travel plans.Additionally, investigating techniques to mitigate the occurrence of hallucinations and improve the overall robustness of LLM-based planning systems could further enhance their performance.Overall, overcoming these challenges will be crucial for advancing the state-of-the-art in LLM-driven travel planning systems.</p>
<p>A.2 Impact</p>
<p>The social impact of our work is profound, as it pioneers a novel methodology by emulating human reasoning processes to address Multi-Phases planning challenges.Equipping LLM agents with human-like problem-solving abilities, our framework enhances their efficacy in applications, such as travel planning, thereby potentially revolutionizing various industries reliant on complex decisionmaking.However, the advent of LLM agents with enhanced planning capabilities also raises concerns about potential job displacement and ethical implications surrounding data privacy and algorithmic bias, necessitating careful consideration and regulation to mitigate adverse consequences.</p>
<p>A.3 Ethics Statement</p>
<p>In our study, we introduce a novel human-like reasoning framework to enhance LLM agents' proficiency in Multi-Phases planning tasks, especially on traveling planning problems.While our work aims to advance the capabilities of LLM agents for beneficial purposes, we recognize the potential for misuse or unintended consequences, such as the propagation of inaccurate information.To address these ethical considerations, we advocate for ongoing research into methods for detecting and mitigating biases in LLMs, as well as the promotion of responsible use and transparency in the deployment of such frameworks.We urge users and researchers to remain vigilant of these ethical risks and prioritize the ethical considerations in the development and application of our framework.</p>
<p>A.6 Travel Plan Example:</p>
<p>"day": 1, "current_city": "from Buffalo to Atlanta", "transportation": "Flight Number: F3502691, from Buffalo to Atlanta, Departure Time: 18:48, Arrival Time: 20:51", "breakfast": "-", "attraction": "Georgia Aquarium, Atlanta;World of Coca-Cola, Atlanta;", "lunch": "-", "dinner": "Chaina Ram Sindhi Confectioners, Atlanta", "accommodation": "Spacious private room close St.Barnabas Hospital, Atlanta" "day": 2, "current_city": "Atlanta", "transportation": "-", "breakfast": "Baba Au Rhum, Atlanta", "attraction": "Atlanta Botanical Garden, Atlanta;High Museum of Art, Atlanta;", "lunch": "Barkat, Atlanta", "dinner": "Taste of Vishal, Atlanta", "accommodation": "Spacious private room close St.Barnabas Hospital, Atlanta" "day": 3, "current_city": "from Atlanta to Buffalo", "transportation": "Flight Number: F3502694, from Atlanta to Buffalo, Departure Time: 15:47, Arrival Time: 17:42", "breakfast": "Asian Bistro, Atlanta", "attraction": "Piedmont Park, Atlanta;", "lunch": "Beliram Degchiwala, Atlanta", "dinner": "-", "accommodation": "-"</p>
<p>Figure 2 :
2
Figure2: This is our transportation evaluation process for each route generated by PathFinder Agent.We will evaluate whether this route is reasonable under the limitation from the specific travel planning query, e.g., cannot take flight.If not reasonable, we will redo the route by giving feedback.Otherwise, if the route is not reasonable for some kind of transportation, we will add the constraint.</p>
<p>Figure 3 :
3
Figure 3: This image illustrates the Knowledge Block workflow.The left part is the Knowledge Write process, where a function's result is written to the top of the Knowledge Block.About Knowledge Read, when the knowledge needs to be popped out but is below the threshold, the Knowledge Block pops out enough items to meet the threshold.If the number exceeds the threshold, the items in past two days are popped out.Knowledge Block: Humans will make their plans by examining the detailed information they have previously collected.To mimic this process, we introduce the Knowledge Block, as shown in Fig 1, primarily used by the Plan Agent.In Fig 3 left part, this block will automatically record all information and the description of each information, which is generated by Description Agent.However, when travel plans span long periods, the extensive information can make it difficult to extract pertinent details and may even exceed the LLM's context window length.To address this, we implement a stack-like structure within the Knowledge Block.When the Plan Agent needs detailed information, the block only "pops out" the information collected over the past two days, restricted by a minimal threshold.As shown in Fig 3 If the number of knowledge items collected exceeds the threshold, the block pops out the same number of items.If it exceeds the threshold, all knowledge items from the past two days are popped out.</p>
<p>Figure 5 :
5
Figure 5: The error distribution of GPT-3.5-Turbo and GPT-4-Turbo.</p>
<p>Figure 4 :
4
Figure 4: The error distribution about GPT-4-Turbo's Delivery Rate Failure..</p>
<p>Figure 9 :
9
Figure 9: An example of the content of strategy block</p>
<p>Table 1 :
1
Our experimental results on the validation dataset of the TravelPlanner benchmark demonstrate that our framework enables GPT-3.5 to surpass previous GPT-4's performance.Additionally, our framework increases the Final Pass Rate of GPT-4-Turbo 10× compared to previous algorithms.For each model, using our framework results in significantly improved performance.
ModelDelivery RateCommonsense Pass Rate Hard Constraint Pass Rate Final Pass RateMicroMacroMicroMacroTravelPlanner ResultMistral-7B-32K [21]</p>
<p>•</p>
<p>Commonsense Constraint Pass Rate: Assesses if agents can incorporate commonsense into their plans without explicit instructions, across eight dimensions.•Hard Constraint Pass Rate: Evaluates if a plan meets all explicitly given hard constraints, testing agents' adaptability to diverse user queries.• Final Pass Rate: Indicates the proportion of plans that meet all constraints (delivery, commonsense, and hard constraints), reflecting agents' overall proficiency in producing practical plans.• Micro Pass Rate: The Micro Pass Rate evaluates the proportion of constraints that are successfully passed by an agent's plans, as shown in Formula 1.It is calculated by taking the total number of successfully met constraints across all plans and dividing it by the total number of constraints applied to all plans.• Macro Pass Rate: The Macro Pass Rate assesses the proportion of plans that satisfy all of their constraints.It calculates the ratio of the number of plans that meet all applicable commonsense or hard constraints to the total number of plans evaluated.
GPT-3.5-Turbo Commonsense Errors Distribution GPT-4-TurboGPT-3.5-Turbo Hard Errors Distribution GPT-4-TurboHallucination Information Necessary Infomation Absent Invalid AccommodationError Types Transportation Conflicts Not Returning to Origin City City ConflictsRepeated Restaurants Repeated AttractionsConflict Room RuleError Types Not Satisfied CuisineConflict Room TypeMicro Pass Rate =|P |(1)
p∈P c∈Cp 1 passed(c,p) p∈P |C p | , Macro Pass Rate = p∈P 1 passed(Cp,p)</p>
<p>1 passed(c,p) is an indicator function that returns 1 if constraint c is passed in plan p, and 0 otherwise.1 passed(Cp,p) is an indicator function that returns 1 if all constraints in C p are passed in plan p, and 0 otherwise.
4.2 Experiment Result7.1%14.3%78.6%FlightSearch AccommodationSearchRestaurantSearch</p>
<p>Table 2 :
2
Ablation study results on GPT-3.5-Turbo.
Delivery RateCommonsense Pass Rate Hard Constraint Pass Rate Final Pass RateMicroMacroMicroMacroOriginal Result [54]86.754.00000No Outlines93.359.21.13.81.10No Strategy10074.913.912.12.20.6No Knowledge99.466.08.310.21.10No Plan Search99.467.40.61.900Our Framework100.075.115.615.54.42.2</p>
<p>Ensure that the transportation method selected is appropriate for the distance and time required to reach the scheduled activities, such as not planning long-distance road trips between breakfast and lunch.7.Dining reservations should be made with consideration of dietary restrictions, restaurant capacity, and the expected duration of the meal to maintain the day's itinerary flow.8.Verify that the accommodation has the necessary amenities and services required for the traveler's comfort and convenience, such as Wi-Fi if needed for planning or work purposes.Figure 8: An example of the outline A.7.1 Strategy Block Content:Below is a preliminary outline of your trip, which can serve as a reference for collecting information: The First Day: from Ontario to Honolulu.Exploring Honolulu.The Second Day: Exploring Honolulu.The Third Day: from Honolulu to Ontario.You now need to gather relevant information to specify the travel plan for the First day.Remember that the total daily expenses of your trip (the sum of expenses for each person) do not exceed 1066.Hard Constraints.Total Budget: The trip must not exceed a total cost of $3,200, including all transportation, accommodation, meals, and activities.{DetailOmit} Thought 1: To begin planning your first day, I will collect information on a flight from Ontario to Honolulu for March 4th, 2022.After that, I will look for dining options, attractions to visit, and accommodation in Honolulu.Let's start with the flight information.Action 1: FlightSearch[Ontario, Honolulu, 2022-03-04] Observation 1: Masked due to limited length.Make sure the data has been written in Notebook.Successfully recorded in Notebook: Flight Information for Ontario to Honolulu on March 4, 2022 Thought 2:... {Omit Other Process} Action 5: DailyPlanner[Please outline the plan for the first day of a 3-day trip from Ontario to Honolulu on March 4th, 2022, including flight, dining, attractions, and accommodation.]Observation5: Travel Plan: Day 1: Current City: from Ontario to Honolulu Transportation: Flight Number: F3584294, from Ontario to Honolulu, Departure Time: 10:11, Arrival Time: 14:46 Breakfast: -(Assuming breakfast is eaten before departure or on the flight) Attraction 1: Aloha Tower, Honolulu (Estimated visit time: 1 hour) Lunch: Bait El Khetyar, Honolulu (Cuisines: Cafe, Seafood, Average Cost: 28) Attraction 2: Iolani Palace, Honolulu (Estimated visit time: 1.5 hours) Attraction 3: Honolulu Museum of Art (HoMA), Honolulu (Estimated visit time: 2 hours) Dinner: Crystal Restaurant, Honolulu (Cuisines: Tea, Pizza, Bakery, Fast Food, Chinese, Average Cost: 57) Accommodation: Park, Subway &amp; All Conveniences; Room type: Entire home/apt; Price: 754; City: Honolulu Outline:The First Day: from Ontario to Honolulu.Exploring Honolulu.The Second Day: Exploring Honolulu.The Third Day: from Honolulu to Ontario.Remember that the total daily expenses of your trip (the sum of expenses for each person) do not exceed 1066.You should gather the necessary information to plan your trip for the Second day.Each time you use the DailyPlanner tool, the planner can only access information queried during the previous 2 days.However, if the number of queries in the past 2 days is less than 5, then it will return the last 5 queried pieces of information.
A.7 Outline Example:The First Day: from Buffalo to Atlanta. Exploring Atlanta.The Second Day: Exploring Atlanta.The Third Day: from Atlanta to Buffalo.1.Departure and Return Dates: The travel must commence on March2nd, 2022, and conclude with a return to Buffalo on March 4th,2022.2.Solo Travel: The plan should be tailored for a single traveler,ensuring accommodations and activities are suitable for oneperson.3.Budget Limit: The total cost of the trip, includingtransportation, accommodation, meals, and activities, must notexceed $1,100.4.Destination Specific: All travel arrangements must be made withAtlanta as the sole destination for the duration of the trip.5.Time Frame: The entire travel plan, including all experiencesand bookings, must fit within the 3-day window specified.1. Accommodations should be booked for the correct datescorresponding to the traveler's stay in each city, ensuring no gapsor overlaps in lodging.2. Dining options for breakfast, lunch, and dinner shouldbe reasonably spaced throughout the day and geographicallyconvenient to either the accommodation or planned attractions.3. Transportation arrangements must align with the traveler'sschedule, allowing sufficient time for check-in, securityclearance at airports, and transfers between different modes oftransport.4. Attractions should be planned with consideration of theiroperating hours and any required advance reservations or tickets,avoiding scheduling conflicts.5. The sequence of visiting attractions should be logical andefficient, minimizing backtracking and excessive travel timebetween locations.Figure 7: An example of the travel plan6. Thought 6: ...{Omit Future Steps}
Charlotte to Newark.Exploring Newark.The Second Day: Exploring Newark.The Third Day: from Newark to Atlantic City.Exploring Newark or Atlantic City.The Fourth Day: Exploring Atlantic City.The Fifth Day: from Atlantic City to Charlotte.A.4 ExampleA.5 Travel Query Example:1. Could you please create a 5-day travel itinerary for one person, starting in Albuquerque and visiting 2 cities in Texas from March 25th to March 29th, 2022?The travel plan should work within a budget of $2,100.2. Could you help create a 7-day travel plan for a group of 3, departing from Greensboro and touring 3 different cities in Georgia from March 10th to March 16th, 2022?We have a new budget of $4,000 for this trip.We'd also appreciate if our accommodations have smoking areas.3. Could you help create a 5-day itinerary for a travel plan departing from Grand Junction and heading to 2 cities in Arizona from March 19th to March 23rd, 2022?It's a plan for two people with a budget of $2,100.Our accommodations should allow visitors and our preference is for private rooms.Additionally, we do not require any flight transportation.4. Could you create a 7-day travel itinerary for 2 people, departing from Albuquerque and visiting 3 cities in Texas from March 8th to March 14th, 2022?Our budget is set at $5,000.We require accommodations that allow smoking and are preferably not shared rooms.We would prefer to avoid any flights for our transportation.You are a proficient planner.Based on the provided information and query, please give me an outline for my whole trip.Please help me generate an outline for each day of this query, primarily including which city it involves, whether there's a need to travel from one city to another.Don't include any specific details like flight numbers, restaurant names, or attraction names.You should use 'The First', 'The Second', 'The Third', etc., to indicate the order of the days.You shouldn't include any city or state names which are not in the query.In your outline, only the cities mentioned in the query can appear; absolutely no other cities or state names are allowed.Could you create a 7-day travel itinerary for two, starting from Seattle and traveling to New York, where we will explore 3 distinct cities?The journey is planned from July 10th to July 20th, 2023.Our updated budget is $6,000.For our stays, we're looking for private accommodations.We plan to avoid any air travel for moving between locations.When it comes to dining, we're eager to try a range of food styles, including Italian, Japanese, Indian, and Thai.The cities in New York are here: (Omitting the detail list of cities in New York in this example.)Your Outline: The First Day: from Seattle to Buffalo.Exploring Buffalo.The Second Day: Exploring Buffalo.The Third Day: from Buffalo to Niagara Falls.Exploring Buffalo or Niagara Falls.The Fourth Day: Exploring Niagara Falls.The Fifth Day: from Niagara Falls to Albany.Exploring Niagara Falls or Albany.The Sixth Day: Exploring Albany.The Seventh Day: from Albany to Seattle.STOP * Example Ends * You must plan an outline that matches the number of days mentioned in the query.If the trip is for three days, you must include The First day, The Second day, The Third day.If the duration is longer, for example, six days, then your outline must cover the content for The First Day, The Second day, through to the Sixth day.The transition between cities must follow the sentence pattern "from ... to ...".{scratchpad} Query:{query} Your Outline: -Each day's plan needs the information of transportation, dining, attractions, and accommodation.You should collect the FOUR parts in the order of transportation, dining, attractions, and accommodation for the target city.-Information will be recorded in a Notebook, and used later in the DailyPlanner tool to create a detailed daily plan.-Collect information on a day-by-day basis, ensuring each day's plan includes transportation, dining, attractions, and accommodation information.-Tranformation information is only needed if there is a transfer between cities on that day.Otherwise, you should not collect it.-Only you collecting information of the first day's plan, you can use the DailyPlanner tool to plan the first day.Don't collect the information of the second day's plan before you finish the first day's plan.Transportation Information:-Transportation choice: Flight, Taxi, Self-driving.-Specify the starting and destination locations.-Transfer Mode Rules:-If the initial transportation involves a flight, subsequent city transfers must also be via flight or taxi (no self-driving).-If initially self-driving, all subsequent city transfers should remain via self-driving.-Please ensure that transportation information is collected only if there is a transfer between cities on that day; otherwise, it is not needed.-Your query should clearly specify which kind of Transportation you want to know.(Flight from A to B on DATE or Self-driving from A to B or Taxi from A to B).You can't query for all transportation methods at once.You can only query for one transportation method at a time.Dining, Attractions, and Accommodation:-Clearly specify which type of information required, as well as the location.-You should express you need to record the data in your response after you collect information, like "I need to record the data into the Notebook" or "I need to write the data into the Notebook".If you don't express, the data will not be recorded.Completion and Next Steps:-Once enough information is collected, clearly state the need to proceed with the day's plan.-You should collect dining, attractions, and accommodation information for the target city each day.-You should explicitly state which kind of information you need to collect in each step befor you use "\n".Gather the necessary information to plan the trip for {date} day.Example 2: DailyPlanner[Can you detail the itinerary for the second day of a 3-day journey from Seattle to New York, with this day being in New York?]This prompt asks for a detailed plan for the trip's second day, with an emphasis on the fact that this day is spent in New York City.Each action only calls one function once.Do not add any description in the action.Your actions shouldn't violate the constraints provided in the query.Query: {query} {scratchpad}Please make sure your action does not start with ['\n', 'Thought', 'Action', 'Observation'] and assume all the actions are permitted in this environment.Original Travel Plan Query:{query} We are now planning {date} day of the trip.Some constraints are provided for the plan: {constraints} {outlines} Previous Thought, Action and Observation: {previous_react} Thought: {thought} Action: You are a proficient planner.Based on the provided information and query, please give me a detailed plan, including specifics such as flight numbers (e.g., F0123456), restaurant names, and accommodation names.Note that all the information in your plan should be derived from the provided data.You must adhere to the format given in the example.Additionally, all details should align with commonsense.The symbol '-' indicates that information is unnecessary.For example, in the provided sample, you do not need to plan after returning to the departure city.When you travel to two cities in one day, you should note it in the 'Current City' section as in the example (i.e., from A to B). <strong><em>*</em> Example </strong>*** Query: Could you create a travel plan for 4 people from Boston to San Francisco spanning 3 days, from September 21st to September 23rd, 2023, with a budget of $18,500?Requirement: You need to plan for the third day of the trip.-----------------------
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. Rosa I Gati V Aher, Adam Arriaga, Kalai Tauman, International Conference on Machine Learning. PMLR2023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Autoagents: A framework for automatic agent generation. Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Jie Börje F Karlsson, Yemin Fu, Shi, arXiv:2309.172882023arXiv preprint</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, arXiv:2210.028752022arXiv preprint</p>
<p>Dynamic planning with a llm. Frank Gautier Dagan, Alex Keller, Lascarides, arXiv:2308.063912023arXiv preprint</p>
<p>Collaborating with language models for embodied reasoning. Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, Rob Fergus, arXiv:2302.007632023arXiv preprint</p>
<p>Human-level play in the game of diplomacy by combining language models with strategic reasoning. † , Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Meta Fundamental AI Research Diplomacy Team. 37866242022Science</p>
<p>Zipeng Fu, Tony Z Zhao, Chelsea Finn, arXiv:2401.02117Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. 2024arXiv preprint</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.14984Social-network simulation system with large language model-empowered agents. 20233arXiv preprint</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. Dong Huang, Qingwen Bu, M Jie, Michael Zhang, Heming Luck, Cui, arXiv:2312.130102023arXiv preprint</p>
<p>Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie, arXiv:2308.16505Recommender ai agent: Integrating large language models for interactive recommendations. 2023arXiv preprint</p>
<p>Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, arXiv:2402.02716Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A. 2024arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Camel: Communicative agents for" mind" exploration of large scale language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, 2023</p>
<p>Large language model-empowered agents for simulating macroeconomic activities. Nian Li, Chen Gao, Yong Li, Qingmin Liao, arXiv:2310.104362023arXiv preprint</p>
<p>Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance. Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, Khaldoun Khashanah, arXiv:2309.037362023arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023arXiv preprint</p>
<p>Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, arXiv:2311.05437Llava-plus: Learning to use tools for creating multimodal agents. 2023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Generation-augmented retrieval for open-domain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, arXiv:2009.085532020arXiv preprint</p>
<p>Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, Jesse Clifton, arXiv:2310.08901Welfare diplomacy: Benchmarking language model cooperation. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, arXiv:2304.083542023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 202436</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Vipergpt: Visual inference via python execution for reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Avalon's game of thoughts: Battle against deception through recursive contemplation. Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang, arXiv:2310.013202023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, Graham Neubig, arXiv:2403.15452What are tools anyway? a survey from the language model perspective. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.046712023arXiv preprint</p>
<p>Yue Wu, Xuan Tang, Tom M Mitchell, Yuanzhi Li, Smartplay, arXiv:2310.01557A benchmark for llms as intelligent agents. 2023arXiv preprint</p>
<p>Llm a<em>: Human in the loop large language models enabled a</em> search for robotics. Hengjia Xiao, Peng Wang, arXiv:2312.017972023arXiv preprint</p>
<p>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, Li , arXiv:2402.04559Can large language model agents simulate human trust behaviors?. 2024arXiv preprint</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, arXiv:2402.016222024arXiv preprint</p>
<p>Language agents with reinforcement learning for strategic play in the werewolf game. Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu, arXiv:2310.189402023arXiv preprint</p>
<p>Gpt4tools: Teaching large language model to use tools via self-instruction. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, Advances in Neural Information Processing Systems. 202436</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Zhun Yang, Adam Ishay, Joohyung Lee, arXiv:2307.076962023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Easytool: Enhancing llm-based agents with concise tool instruction. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang, arXiv:2401.062012024arXiv preprint</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, arXiv:2307.024852023arXiv preprint</p>
<p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, arXiv:2310.17512Competeai: Understanding the competition behaviors in large language model-based agents. 2023arXiv preprint</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, Advances in Neural Information Processing Systems. 202436</p>
<p>A.7.2 Knowledge Block Pop out Content: 1: Flight Number: F3502691; Price: 240; DepTime. 48</p>
<p>ArrTime: 20:51OriginCityName: Buffalo; DestCityName: Atlanta 2: Flight Number: F3514187; Price: 322; DepTime. 51</p>
<p>ArrTime: 08:40OriginCityName: Buffalo; DestCityName: Atlanta 3: Flight Number: F3555201; Price: 265; DepTime. 44</p>
<p>ArrTime: 14:331: Restaurant: Adda; City: Atlanta; Cuisines: Tea, Mexican, Bakery; Average Cost: 22; Rating: 3.8 2: Restaurant: Baba Au Rhum; City: Atlanta; Cuisines: Desserts, Pizza, Mexican, BBQ, Fast Food; Average Cost. Atlanta ... ------------------------- , -----------127Attraction Name: Atlanta Botanical Garden; City: Atlanta 2: Attraction Name: World of Coca-Cola; City: Atlanta 3: Attraction Name: Georgia Aquarium</p>
<p>Rating: 4.53: Restaurant: Barkat; City: Atlanta; Cuisines: Bakery, Indian, Mediterranean, Desserts; Average Cost: 78; Rating. .. -------------------------1 4 , Friendly, Brooklyn ApartmentRoom type: Private room; Price: 1069.0; Minimum number of nights stay: 2.0; review rate number: 3.0; House rules: No children under 10; One room can accommodate how many people: 2; City: Atlanta 2: Accommodation: Sunny</p>
<p>One room can accommodate how many people: 2; City: Atlanta 3: Accommodation: 1bd in a sunny 2 bd Ft. Price: 874.0Greene Apt; Room type: Private room; Price: 1056.0; Minimum number of nights stay: 1.0; review rate number: 4.0; House rules: No visitors &amp; No pets; One room can accommodate how many people: 1; City. 10Minimum number of nights stay: 1.0; review rate number: 4.0; House rules: No pets. An example of format information in Knowledge Block</p>
<p>Tool Agent Prompt: You are an experienced tool user. You are currently assisting in planning a trip. Then, I will provide you with the original travel query as well as which day's plan we are now planning. Besides, I will provide you with a 'Thought', and then you need to generate an 'Action' based on this 'Thought. A.8.3Action' can have 6 different types</p>            </div>
        </div>

    </div>
</body>
</html>
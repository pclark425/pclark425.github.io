<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4358 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4358</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4358</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-278886748</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.17465v2.pdf" target="_blank">A Position Paper on the Automatic Generation of Machine Learning Leaderboards</a></p>
                <p><strong>Paper Abstract:</strong> An important task in machine learning (ML) research is comparing prior work, which is often performed via ML leaderboards: a tabular overview of experiments with comparable conditions (e.g., same task, dataset, and metric). However, the growing volume of literature creates challenges in creating and maintaining these leaderboards. To ease this burden, researchers have developed methods to extract leaderboard entries from research papers for automated leaderboard curation. Yet, prior work varies in problem framing, complicating comparisons and limiting real-world applicability. In this position paper, we present the first overview of Automatic Leaderboard Generation (ALG) research, identifying fundamental differences in assumptions, scope, and output formats. We propose an ALG unified conceptual framework to standardise how the ALG task is defined. We offer ALG benchmarking guidelines, including recommendations for datasets and metrics that promote fair, reproducible evaluation. Lastly, we outline challenges and new directions for ALG, such as, advocating for broader coverage by including all reported results and richer metadata.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4358.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4358.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TDMS-PR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TDMS Prompting (TDMS-PR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-prompting approach that extracts ⟨task, dataset, metric, score⟩ tuples from research papers by providing a document representation to a prompted large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TDMS-PR</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>TDMS-PR frames extraction of leaderboard tuples as a prompting task for pre-trained LLMs. The pipeline first creates a compact document representation (DocREC, DocTAET, or the full text) that selects relevant sections (results, experiments, conclusions, or title/abstract/tables). That text is supplied to an LLM with a task-oriented prompt asking it to extract tuple fields (task, dataset, metric, score). The approach was evaluated in zero-shot and few-shot setups and compared across document representations as an ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5 family (variants), Mistral 7B, Llama 3 7B (as reported in Kabongo et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP research papers (ML experimental results)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Numerical experimental results and rankings (performance scores per method/dataset/metric) — i.e., empirical performance relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured tuples ⟨task, dataset, metric, score⟩ (tabular / JSON-style tuples suitable for leaderboards)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison against benchmark datasets (TDMS-Ctx / SciLead style datasets) using extraction metrics (precision, recall, F1) and leaderboard-level metrics where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported evaluation used precision/recall/F1 (micro/macro). Performance varies by document representation and model; Kabongo et al. (2024) report many per-condition scores (examples in paper), with generally substantially better results when good context selection (DocREC/DocTAET) is used versus naive full-text prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to non-LLM baselines and to different document representations; specific baselines included earlier transformer classifiers and heuristic approaches (performance reported per-experiment in Kabongo et al., 2024). Exact numeric comparisons are reported in the paper's tables for each dataset/setting.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Score extraction is difficult and lower-scoring than extraction of condition entities; sensitivity to document representation (too much/noisy context degrades accuracy); high inference cost at scale; open-domain canonicalisation issues.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4358.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4358.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS-PR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Method-Score Prompting (MS-PR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM prompting method that, given a paper representation and a ⟨task, dataset, metric⟩ tuple, extracts associated ⟨method, score⟩ pairs (method names and numeric performance values).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LEGOBench: Scientific Leaderboard Generation Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MS-PR</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MS-PR conditions the extraction on a known task/dataset/metric triple and prompts an LLM to return the method(s) and corresponding numeric score(s) reported in the paper for that triple. The pipeline supplies a paper representation (e.g., DocTAET / full text) and uses few-shot or zero-shot prompts; multiple LLMs are compared to extract method-score pairs for populating leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>A wide set of LLMs evaluated, including Falcon, Galactica, Llama 2 (7B & 13B), Llama 3, Mistral, Vicuna, Zephyr, Gemini, GPT-4 (as reported in Singh et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP experimental literature (leaderboard-oriented datasets; LEGOBench)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Empirical performance relationships (method → numeric score), used to rank methods on leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured ⟨method, score⟩ tuples (tabular entries suitable for leaderboards)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation against gold benchmark leaderboards (LEGOBench); metrics reported include micro/macro precision/recall/F1 and leaderboard overlap metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Singh et al. (2024) report per-model extraction results; for method+score extraction on LEGOBench some LLMs (including GPT-4 and top open models) achieved substantially higher F1 on method extraction sub-tasks (see paper tables), with numeric performance varying by model and prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against prior non-LLM systems (AxCell, BERT-based pipelines) and across many LLMs; LLM-based MS-PR variants often outperform older IE pipelines on method extraction sub-tasks (detailed numeric comparisons in Singh et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM hallucination risk, difficulty extracting and canonicalising numeric scores, token-output limits when multiple results per paper are present, and inference cost when scaling to many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4358.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4358.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TDMR-PR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-Dataset-Metric-Result Prompting (TDMR-PR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based retrieval-augmented generation pipeline that extracts ⟨task, dataset, metric, score⟩ tuples and normalises entities to build or update leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TDMR-PR</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>TDMR-PR first retrieves relevant document passages and then uses an LLM in a retrieval-augmented generation (RAG) setup to generate candidate tuples ⟨task, dataset, metric, score⟩. Depending on domain framing (closed/hybrid/open), generated entities are normalised to an existing taxonomy or added as new canonical entries; final steps include ranking papers by performance to construct or update leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reported models include Llama 2, Llama 3, Mixtral, and GPT-4 (Şahinüç et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP leaderboards (SciLead dataset used in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Empirical performance values (numeric scores) and derived rankings between methods on tasks/datasets</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured tuples ⟨task, dataset, metric, score⟩; final output assembled as leaderboards (tabular lists ranked by score)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation against an exhaustively annotated benchmark dataset (SciLead) using micro/macro precision/recall/F1 and leaderboard-specific metrics (leaderboard recall, paper coverage, result coverage, average overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Şahinüç et al. (2024) report extraction metrics per model and domain; the paper notes score extraction using GPT-4 attained an F1 of approximately 70 (text summary). Detailed tables in the source give per-model micro/macro F1 for tuple and entity extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to prior systems like AxCell and non-LLM pipelines; LLM-based TDMR-PR (especially GPT-4 / Llama 3 variants) substantially outperform older systems on several entity/tuple extraction sub-tasks in SciLead evaluations (numerical comparisons presented in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Normalisation of unseen entities is hard (canonicalisation required); score extraction remains a weaker subtask; open-domain outputs require fuzzy/semantic matching; computing cost for large-scale always-on extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4358.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4358.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TELIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TELIN (Table Entity Linker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses vision and transformer models (YOLO, SPLERGE, SpERT) plus fuzzy matching and human review to extract table-based leaderboard tuples (task, dataset, metric, model).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TELIN</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>TELIN converts PDFs into structured representations using YOLO for paragraph/heading/table detection and SPLERGE for table decomposition; then applies SpERT (BERT-based NER/RE) to identify entities (task, dataset, method, metric) in or near tables. Non-numeric table cells use fuzzy string matching; tuples are formed when at least three of four entity classes are detected in a table and caption. Uncertain entities are surfaced for human review, and feedback is used to fine-tune the NER iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not an LLM-prompting approach; uses YOLO for layout detection and SpERT (BERT-based) for NER. Human review loop is integral.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP papers with tabular results</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Numeric experimental results embedded in tables (scores) and their association with experimental conditions</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured tuples assembled from table row/column cells and captions (tabular leaderboard entries)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation on curated datasets (PwC-LB and others) with precision/recall/F1; iterative human-in-the-loop correction used to improve entity models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported micro/macro precision/recall/F1 for entity and tuple extraction on datasets (e.g., PwC-LB); detailed numbers in Kardas et al. (2020) and Yang et al. (2022b) as referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to earlier IE methods and heuristics (e.g., orthographic heuristics); TELIN leverages table structure and learned NER for improved entity extraction vs naive methods.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on table structure being well-detected; needs human review for uncertain entities; restricted to results that appear in tables or well-structured parts of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4358.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4358.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciDaSynth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciDaSynth</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive system that uses LLMs to extract and synthesise structured knowledge (tables) from the scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciDaSynth</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SciDaSynth is described as an interactive system that leverages LLMs to extract tabular, structured representations from papers and to synthesise that structured knowledge; it supports interactive correction and synthesis into tables that can be used for downstream analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Large language models (specific models reported in the SciDaSynth paper; cited here as Wang et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature (system described as applicable across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structured experimental result extraction (numerical values and structured relationships) rather than explicit discovery of physical laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Synthesised structured tables and extracted numerical entries</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Interactive human-in-the-loop validation and benchmarking described in the SciDaSynth paper (cited in this paper's limitations section)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not detailed in this position paper; SciDaSynth cited as an example of an LLM-based interactive extraction/synthesis system.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Interactive nature implies human involvement; generalisability across heterogeneous scientific domains remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4358.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4358.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ghosh2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuning LLMs for schema-based information extraction in materials</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent effort finetuning LLMs for schema-based information extraction in materials science, cited as an example of applying LLMs beyond ML leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Finetuned-schema IE (materials)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Ghosh et al. (2024) finetune language models to perform schema-based extraction of structured experimental information from materials science literature; the work aims to improve reliability in domain-specific numeric/experimental IE.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Fine-tuned language models (specific architectures referenced in Ghosh et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of experimental procedures and numeric experimental outcomes (structured numeric entities) rather than deriving physical laws per se</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Schema-constrained structured records (tables / knowledge-graph style entries)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Domain-expert-curated benchmarks for materials IE (as described in Ghosh et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not enumerated in this position paper; referenced as recent work improving schema-based IE with finetuned LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Domain heterogeneity and need for domain-specific finetuning; dataset curation and annotation costs.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4358.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4358.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pramanick2025</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic extraction, categorization and quantitative analysis of contribution statements</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (Pramanick et al., 2025) that automatically extracts and quantitatively analyses contribution statements from research papers — an example of automated quantitative synthesis from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Contribution-statement extraction & analysis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The referenced work proposes automated methods to identify, categorize, and quantitatively analyze contribution statements across research papers, enabling diachronic and quantitative studies of contributions in a field. The position paper cites this as an example of extracting structured, quantitative summaries from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / research-paper meta-analysis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Quantitative summaries and statistics about contributions (meta-scientific patterns) rather than scientific physical laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Categorical labels plus quantitative summaries / aggregated statistics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed here; referenced as recent progress in automated quantitative analysis of literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dataset/versioning issues and generalisability; not a solution for scientific law discovery per se.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4358.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4358.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatViX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MatViX (Multimodal Information Extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal model (Khalighinejad et al., 2025) demonstrating OCR-free extraction from visually rich scientific documents, mentioned as a promising alternative for capturing table-embedded numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MatViX OCR-free multimodal extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MatViX processes PDF pages as images with a multimodal model to extract structured textual and numeric information without separate OCR, preserving layout cues important for table and figure interpretation; cited as potentially reducing noise when extracting leaderboard-relevant numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Multimodal vision-language models (MatViX as reported by Khalighinejad et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scholarly documents with visually complex layouts</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of numeric experimental results from visual layouts (tables/figures) — supporting structured numeric relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured table-like outputs extracted from page images</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed in this position paper; cited as a technical advance in document representation for extraction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Newer technology; integration with downstream canonicalisation/normalisation pipelines required.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Position Paper on the Automatic Generation of Machine Learning Leaderboards', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AxCell: Automatic extraction of results from machine learning papers. <em>(Rating: 2)</em></li>
                <li>SciREX: A challenge dataset for document-level information extraction. <em>(Rating: 2)</em></li>
                <li>TDMS-IE (Hou et al., 2019) - Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. <em>(Rating: 2)</em></li>
                <li>ORKG-leaderboards: a systematic workflow for mining leaderboards as a knowledge graph. <em>(Rating: 2)</em></li>
                <li>SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model. <em>(Rating: 2)</em></li>
                <li>Finetune the entire RAG architecture (including DPR retriever) for question-answering. <em>(Rating: 1)</em></li>
                <li>LEGOBench: Scientific Leaderboard Generation Benchmark. <em>(Rating: 2)</em></li>
                <li>Toward reliable ad-hoc scientific information extraction: A case study on two materials datasets. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4358",
    "paper_id": "paper-278886748",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "TDMS-PR",
            "name_full": "TDMS Prompting (TDMS-PR)",
            "brief_description": "An LLM-prompting approach that extracts ⟨task, dataset, metric, score⟩ tuples from research papers by providing a document representation to a prompted large language model.",
            "citation_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards.",
            "mention_or_use": "use",
            "method_name": "TDMS-PR",
            "method_description": "TDMS-PR frames extraction of leaderboard tuples as a prompting task for pre-trained LLMs. The pipeline first creates a compact document representation (DocREC, DocTAET, or the full text) that selects relevant sections (results, experiments, conclusions, or title/abstract/tables). That text is supplied to an LLM with a task-oriented prompt asking it to extract tuple fields (task, dataset, metric, score). The approach was evaluated in zero-shot and few-shot setups and compared across document representations as an ablation.",
            "llm_model_used": "Flan-T5 family (variants), Mistral 7B, Llama 3 7B (as reported in Kabongo et al., 2024)",
            "scientific_domain": "Machine learning / NLP research papers (ML experimental results)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Numerical experimental results and rankings (performance scores per method/dataset/metric) — i.e., empirical performance relationships",
            "extraction_output_format": "Structured tuples ⟨task, dataset, metric, score⟩ (tabular / JSON-style tuples suitable for leaderboards)",
            "validation_method": "Comparison against benchmark datasets (TDMS-Ctx / SciLead style datasets) using extraction metrics (precision, recall, F1) and leaderboard-level metrics where applicable",
            "performance_metrics": "Reported evaluation used precision/recall/F1 (micro/macro). Performance varies by document representation and model; Kabongo et al. (2024) report many per-condition scores (examples in paper), with generally substantially better results when good context selection (DocREC/DocTAET) is used versus naive full-text prompting.",
            "baseline_comparison": "Compared to non-LLM baselines and to different document representations; specific baselines included earlier transformer classifiers and heuristic approaches (performance reported per-experiment in Kabongo et al., 2024). Exact numeric comparisons are reported in the paper's tables for each dataset/setting.",
            "challenges_limitations": "Score extraction is difficult and lower-scoring than extraction of condition entities; sensitivity to document representation (too much/noisy context degrades accuracy); high inference cost at scale; open-domain canonicalisation issues.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4358.0",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MS-PR",
            "name_full": "Method-Score Prompting (MS-PR)",
            "brief_description": "An LLM prompting method that, given a paper representation and a ⟨task, dataset, metric⟩ tuple, extracts associated ⟨method, score⟩ pairs (method names and numeric performance values).",
            "citation_title": "LEGOBench: Scientific Leaderboard Generation Benchmark",
            "mention_or_use": "use",
            "method_name": "MS-PR",
            "method_description": "MS-PR conditions the extraction on a known task/dataset/metric triple and prompts an LLM to return the method(s) and corresponding numeric score(s) reported in the paper for that triple. The pipeline supplies a paper representation (e.g., DocTAET / full text) and uses few-shot or zero-shot prompts; multiple LLMs are compared to extract method-score pairs for populating leaderboards.",
            "llm_model_used": "A wide set of LLMs evaluated, including Falcon, Galactica, Llama 2 (7B & 13B), Llama 3, Mistral, Vicuna, Zephyr, Gemini, GPT-4 (as reported in Singh et al., 2024)",
            "scientific_domain": "Machine learning / NLP experimental literature (leaderboard-oriented datasets; LEGOBench)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Empirical performance relationships (method → numeric score), used to rank methods on leaderboards",
            "extraction_output_format": "Structured ⟨method, score⟩ tuples (tabular entries suitable for leaderboards)",
            "validation_method": "Evaluation against gold benchmark leaderboards (LEGOBench); metrics reported include micro/macro precision/recall/F1 and leaderboard overlap metrics",
            "performance_metrics": "Singh et al. (2024) report per-model extraction results; for method+score extraction on LEGOBench some LLMs (including GPT-4 and top open models) achieved substantially higher F1 on method extraction sub-tasks (see paper tables), with numeric performance varying by model and prompt.",
            "baseline_comparison": "Compared against prior non-LLM systems (AxCell, BERT-based pipelines) and across many LLMs; LLM-based MS-PR variants often outperform older IE pipelines on method extraction sub-tasks (detailed numeric comparisons in Singh et al., 2024).",
            "challenges_limitations": "LLM hallucination risk, difficulty extracting and canonicalising numeric scores, token-output limits when multiple results per paper are present, and inference cost when scaling to many papers.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4358.1",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "TDMR-PR",
            "name_full": "Task-Dataset-Metric-Result Prompting (TDMR-PR)",
            "brief_description": "An LLM-based retrieval-augmented generation pipeline that extracts ⟨task, dataset, metric, score⟩ tuples and normalises entities to build or update leaderboards.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "TDMR-PR",
            "method_description": "TDMR-PR first retrieves relevant document passages and then uses an LLM in a retrieval-augmented generation (RAG) setup to generate candidate tuples ⟨task, dataset, metric, score⟩. Depending on domain framing (closed/hybrid/open), generated entities are normalised to an existing taxonomy or added as new canonical entries; final steps include ranking papers by performance to construct or update leaderboards.",
            "llm_model_used": "Reported models include Llama 2, Llama 3, Mixtral, and GPT-4 (Şahinüç et al., 2024)",
            "scientific_domain": "Machine learning / NLP leaderboards (SciLead dataset used in evaluation)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Empirical performance values (numeric scores) and derived rankings between methods on tasks/datasets",
            "extraction_output_format": "Structured tuples ⟨task, dataset, metric, score⟩; final output assembled as leaderboards (tabular lists ranked by score)",
            "validation_method": "Evaluation against an exhaustively annotated benchmark dataset (SciLead) using micro/macro precision/recall/F1 and leaderboard-specific metrics (leaderboard recall, paper coverage, result coverage, average overlap)",
            "performance_metrics": "Şahinüç et al. (2024) report extraction metrics per model and domain; the paper notes score extraction using GPT-4 attained an F1 of approximately 70 (text summary). Detailed tables in the source give per-model micro/macro F1 for tuple and entity extraction.",
            "baseline_comparison": "Compared to prior systems like AxCell and non-LLM pipelines; LLM-based TDMR-PR (especially GPT-4 / Llama 3 variants) substantially outperform older systems on several entity/tuple extraction sub-tasks in SciLead evaluations (numerical comparisons presented in paper tables).",
            "challenges_limitations": "Normalisation of unseen entities is hard (canonicalisation required); score extraction remains a weaker subtask; open-domain outputs require fuzzy/semantic matching; computing cost for large-scale always-on extraction.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4358.2",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "TELIN",
            "name_full": "TELIN (Table Entity Linker)",
            "brief_description": "A pipeline that uses vision and transformer models (YOLO, SPLERGE, SpERT) plus fuzzy matching and human review to extract table-based leaderboard tuples (task, dataset, metric, model).",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "TELIN",
            "method_description": "TELIN converts PDFs into structured representations using YOLO for paragraph/heading/table detection and SPLERGE for table decomposition; then applies SpERT (BERT-based NER/RE) to identify entities (task, dataset, method, metric) in or near tables. Non-numeric table cells use fuzzy string matching; tuples are formed when at least three of four entity classes are detected in a table and caption. Uncertain entities are surfaced for human review, and feedback is used to fine-tune the NER iteratively.",
            "llm_model_used": "Not an LLM-prompting approach; uses YOLO for layout detection and SpERT (BERT-based) for NER. Human review loop is integral.",
            "scientific_domain": "Machine learning / NLP papers with tabular results",
            "number_of_papers": null,
            "type_of_quantitative_law": "Numeric experimental results embedded in tables (scores) and their association with experimental conditions",
            "extraction_output_format": "Structured tuples assembled from table row/column cells and captions (tabular leaderboard entries)",
            "validation_method": "Evaluation on curated datasets (PwC-LB and others) with precision/recall/F1; iterative human-in-the-loop correction used to improve entity models",
            "performance_metrics": "Reported micro/macro precision/recall/F1 for entity and tuple extraction on datasets (e.g., PwC-LB); detailed numbers in Kardas et al. (2020) and Yang et al. (2022b) as referenced in the paper.",
            "baseline_comparison": "Compared to earlier IE methods and heuristics (e.g., orthographic heuristics); TELIN leverages table structure and learned NER for improved entity extraction vs naive methods.",
            "challenges_limitations": "Relies on table structure being well-detected; needs human review for uncertain entities; restricted to results that appear in tables or well-structured parts of the paper.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4358.3",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SciDaSynth",
            "name_full": "SciDaSynth",
            "brief_description": "An interactive system that uses LLMs to extract and synthesise structured knowledge (tables) from the scientific literature.",
            "citation_title": "SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model.",
            "mention_or_use": "mention",
            "method_name": "SciDaSynth",
            "method_description": "SciDaSynth is described as an interactive system that leverages LLMs to extract tabular, structured representations from papers and to synthesise that structured knowledge; it supports interactive correction and synthesis into tables that can be used for downstream analyses.",
            "llm_model_used": "Large language models (specific models reported in the SciDaSynth paper; cited here as Wang et al., 2024)",
            "scientific_domain": "General scientific literature (system described as applicable across domains)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Structured experimental result extraction (numerical values and structured relationships) rather than explicit discovery of physical laws",
            "extraction_output_format": "Synthesised structured tables and extracted numerical entries",
            "validation_method": "Interactive human-in-the-loop validation and benchmarking described in the SciDaSynth paper (cited in this paper's limitations section)",
            "performance_metrics": "Not detailed in this position paper; SciDaSynth cited as an example of an LLM-based interactive extraction/synthesis system.",
            "baseline_comparison": "",
            "challenges_limitations": "Interactive nature implies human involvement; generalisability across heterogeneous scientific domains remains a challenge.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4358.4",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Ghosh2024",
            "name_full": "Finetuning LLMs for schema-based information extraction in materials",
            "brief_description": "A recent effort finetuning LLMs for schema-based information extraction in materials science, cited as an example of applying LLMs beyond ML leaderboards.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Finetuned-schema IE (materials)",
            "method_description": "Ghosh et al. (2024) finetune language models to perform schema-based extraction of structured experimental information from materials science literature; the work aims to improve reliability in domain-specific numeric/experimental IE.",
            "llm_model_used": "Fine-tuned language models (specific architectures referenced in Ghosh et al., 2024)",
            "scientific_domain": "Materials science",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of experimental procedures and numeric experimental outcomes (structured numeric entities) rather than deriving physical laws per se",
            "extraction_output_format": "Schema-constrained structured records (tables / knowledge-graph style entries)",
            "validation_method": "Domain-expert-curated benchmarks for materials IE (as described in Ghosh et al., 2024)",
            "performance_metrics": "Not enumerated in this position paper; referenced as recent work improving schema-based IE with finetuned LMs.",
            "baseline_comparison": "",
            "challenges_limitations": "Domain heterogeneity and need for domain-specific finetuning; dataset curation and annotation costs.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4358.5",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Pramanick2025",
            "name_full": "Automatic extraction, categorization and quantitative analysis of contribution statements",
            "brief_description": "A method (Pramanick et al., 2025) that automatically extracts and quantitatively analyses contribution statements from research papers — an example of automated quantitative synthesis from literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Contribution-statement extraction & analysis",
            "method_description": "The referenced work proposes automated methods to identify, categorize, and quantitatively analyze contribution statements across research papers, enabling diachronic and quantitative studies of contributions in a field. The position paper cites this as an example of extracting structured, quantitative summaries from literature.",
            "llm_model_used": null,
            "scientific_domain": "NLP / research-paper meta-analysis",
            "number_of_papers": null,
            "type_of_quantitative_law": "Quantitative summaries and statistics about contributions (meta-scientific patterns) rather than scientific physical laws",
            "extraction_output_format": "Categorical labels plus quantitative summaries / aggregated statistics",
            "validation_method": "Not detailed here; referenced as recent progress in automated quantitative analysis of literature.",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Dataset/versioning issues and generalisability; not a solution for scientific law discovery per se.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4358.6",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MatViX",
            "name_full": "MatViX (Multimodal Information Extraction)",
            "brief_description": "A multimodal model (Khalighinejad et al., 2025) demonstrating OCR-free extraction from visually rich scientific documents, mentioned as a promising alternative for capturing table-embedded numeric results.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "MatViX OCR-free multimodal extraction",
            "method_description": "MatViX processes PDF pages as images with a multimodal model to extract structured textual and numeric information without separate OCR, preserving layout cues important for table and figure interpretation; cited as potentially reducing noise when extracting leaderboard-relevant numeric results.",
            "llm_model_used": "Multimodal vision-language models (MatViX as reported by Khalighinejad et al., 2025)",
            "scientific_domain": "General scholarly documents with visually complex layouts",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of numeric experimental results from visual layouts (tables/figures) — supporting structured numeric relationships",
            "extraction_output_format": "Structured table-like outputs extracted from page images",
            "validation_method": "Not detailed in this position paper; cited as a technical advance in document representation for extraction",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Newer technology; integration with downstream canonicalisation/normalisation pipelines required.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4358.7",
            "source_info": {
                "paper_title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AxCell: Automatic extraction of results from machine learning papers.",
            "rating": 2,
            "sanitized_title": "axcell_automatic_extraction_of_results_from_machine_learning_papers"
        },
        {
            "paper_title": "SciREX: A challenge dataset for document-level information extraction.",
            "rating": 2,
            "sanitized_title": "scirex_a_challenge_dataset_for_documentlevel_information_extraction"
        },
        {
            "paper_title": "TDMS-IE (Hou et al., 2019) - Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction.",
            "rating": 2,
            "sanitized_title": "tdmsie_hou_et_al_2019_identification_of_tasks_datasets_evaluation_metrics_and_numeric_scores_for_scientific_leaderboards_construction"
        },
        {
            "paper_title": "ORKG-leaderboards: a systematic workflow for mining leaderboards as a knowledge graph.",
            "rating": 2,
            "sanitized_title": "orkgleaderboards_a_systematic_workflow_for_mining_leaderboards_as_a_knowledge_graph"
        },
        {
            "paper_title": "SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model.",
            "rating": 2,
            "sanitized_title": "scidasynth_interactive_structured_knowledge_extraction_and_synthesis_from_scientific_literature_with_large_language_model"
        },
        {
            "paper_title": "Finetune the entire RAG architecture (including DPR retriever) for question-answering.",
            "rating": 1,
            "sanitized_title": "finetune_the_entire_rag_architecture_including_dpr_retriever_for_questionanswering"
        },
        {
            "paper_title": "LEGOBench: Scientific Leaderboard Generation Benchmark.",
            "rating": 2,
            "sanitized_title": "legobench_scientific_leaderboard_generation_benchmark"
        },
        {
            "paper_title": "Toward reliable ad-hoc scientific information extraction: A case study on two materials datasets.",
            "rating": 1,
            "sanitized_title": "toward_reliable_adhoc_scientific_information_extraction_a_case_study_on_two_materials_datasets"
        }
    ],
    "cost": 0.01583895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Position Paper on the Automatic Generation of Machine Learning Leaderboards</p>
<p>Roelien C Timmer roelien.timmer@data61.csiro.au 
Yufang Hou yufang.hou@it-u.at 
IT:U Interdisciplinary
Transformation University Austria
Austria</p>
<p>IBM Research Europe
Ireland</p>
<p>♢ ♠ Stephen stephen.wan@data61.csiro.au 
Csiro Data61 
Australia 
A Position Paper on the Automatic Generation of Machine Learning Leaderboards
FB030DEBA7DCA6BCD5BD10CC0F8D204F
An important task in machine learning (ML) research is comparing prior work, which is often performed via ML leaderboards: a tabular overview of experiments with comparable conditions (e.g., same task, dataset, and metric).However, the growing volume of literature creates challenges in creating and maintaining these leaderboards.To ease this burden, researchers have developed methods to extract leaderboard entries from research papers for automated leaderboard curation.Yet, prior work varies in problem framing, complicating comparisons and limiting real-world applicability.In this position paper, we present the first overview of Automatic Leaderboard Generation (ALG) research, identifying fundamental differences in assumptions, scope, and output formats.We propose an ALG unified conceptual framework to standardise how the ALG task is defined.We offer ALG benchmarking guidelines, including recommendations for datasets and metrics that promote fair, reproducible evaluation.Lastly, we outline challenges and new directions for ALG, such as, advocating for broader coverage by including all reported results and richer metadata.</p>
<p>Introduction</p>
<p>In today's fast-paced Machine Learning (ML) research environment, keeping abreast of advancements is more crucial than ever.The exponential growth in publications, exemplified by nearly a quarter of a million arXiv submissions in 2024, underscores the expanding global community of scholars and the accelerating pace of research (arXiv, 2025).This vast increase in information presents researchers with both rich opportunities for discovery but also makes it increasingly difficult to stay up to date.</p>
<p>A key task for researchers is comparing past study outcomes to identify state-of-the-art results or benchmark against prior work.In ML, this is typically done using leaderboards: tables of experimental results under comparable conditions (e.g., task, dataset, metric).The popularity of platforms like Papers with Code 1 underscores their value in providing accessible, up-to-date comparisons that help researchers track progress and identify leading methods.</p>
<p>However, leaderboards on these platforms are often incomplete or missing for certain tasks, and they typically rely on manual updates.To reduce this manual effort, recent work has focused on automatically extracting experimental outcomes (referred to here as "tuples") from research papers to populate leaderboards.We refer to this body of work as Automatic Leaderboard Generation (ALG): "A systematic process for extracting relevant experimental findings from scientific publications to create and maintain a leaderboard." 3Figure 1 illustrates an example of this process, showing the extraction of ⟨task, dataset, metric, method, score⟩ tuples from two research papers to construct a leaderboard.</p>
<p>1 https://paperswithcode.com 2 An example of two LEGOBench (Singh et al., 2024) leaderboard entries summarising Siriwardhana et al. (2021) and Karpukhin et al. (2020). 3All acronyms used in this paper are listed in Appendix A Table 3. 1 arXiv:2505.17465v2[cs.CL] 30 Sep 2025</p>
<p>Research on ALG using natural language processing (NLP) methodologies has seen significant developments in recent years.Indeed, there are still many open research questions as exemplified by the 2024 shared task on ALG (D'Souza et al., 2024), underscoring the ongoing relevance of ALG.This growing body of work has led to varied problem formulations and evaluation approaches, including differing assumptions about prior knowledge ( § 2.1) and extraction scope ( § 2.2), which makes comparisons across work difficult.</p>
<p>This position paper makes four important contributions.First, we provide the first overview of ALG efforts ( § 2- § 4).By comparing prior studies side-by-side, we identify key divergences, such as variations in the assumed input scope (e.g., open vs. closed-domain) and captured results information, that previously hindered apples-to-apples comparisons.Our analysis provides a much-needed baseline map of the field, clarifying the field's current state and identifying critical gaps.</p>
<p>Second, based on this comparison, we propose an ALG unified conceptual framework ( § 5), essentially a problem formulation with unified terminology.This framework consolidates prior formulations into a coherent schema, providing a common language for researchers and enabling direct comparison of approaches.</p>
<p>Third, we provide ALG benchmarking guidelines ( § 6), to unify evaluation practices, addressing the previous lack of consensus.These guidelines establish shared standards for consistent, transparent evaluation and reliable progress tracking.</p>
<p>Fourth, we outline challenges and new directions for ALG ( § 7).We advocate expanding the extraction schema beyond just "best scores" to include all reported results (e.g., baselines, ablations) and enriching tuples with metadata (e.g., model architecture, hyperparameters) to enable more flexible result filtering.</p>
<p>Ultimately, the goal of this position paper is to resolve long-standing fragmentation, establish shared standards, and open new horizons for ALG.</p>
<p>Overview of Problem Definition</p>
<p>The ALG field has seen many advances over the years.At a broad level, the ALG task is an information extraction task, to extract a tuple containing key details of an ML experimental result. 4ou et al. (2019) and Singh et al. (2019) laid the foundation by introducing methods for extracting leaderboard tuples directly from research papers.These methodologies have since been refined and expanded upon by new methods such as Ax-Cell (Kardas et al., 2020), which was put into production by Papers with Code.The most recent methodologies use prompting of pre-trained Large Language Models (LLMs), e.g.prompting Llama 2 7B (Touvron et al., 2023) and Mistral 7B (Jiang et al., 2023) to extract ⟨task, dataset, metric, score⟩ tuples from research papers (Kabongo et al., 2024).</p>
<p>A key issue in the field is the variation in input and output expectations across studies.Table 1 lists key ALG papers we examined, focusing on recent work using transformer models that enable data scaling. 5 We can characterise the key differences in the problem definition as concerning expectations about input and output data.Specifically, we discuss: (1) reliance on domain knowledge, and (2) limited scope of extraction. 6</p>
<p>Reliance on Domain Knowledge</p>
<p>We observe that the ALG domains can be categorised as having different levels of reliance on prior domain knowledge, which ultimately impacts what information can be extracted.Essentially, two variants of the problem have been previously tackled: closed domain and open domain. 7  Closed Domain: The closed-domain ALG problem stipulates that all the entities or tuples are predefined. 8In the field, there have been two subvariants that we name: (1) predefined typed entities (PTE) and (2) predefined typed tuples (PTT). 9 We define the predefined typed entities (PTE) as: "A closed-domain problem for ALG, in which the system is supplied with a finite catalogue of scientific concept classes (for instance, specific tasks, datasets, or metrics), and extractions are confined to items from that predefined list."The system may be given a declarative resource specifying entities,  (Yang et al., 2022b).A response from the authors is pending for clarification.such as in Kardas et al. (2020).This could take the form of a taxonomy, a hierarchical structure of scientific concepts (e.g.tasks, datasets, metrics), or a simpler list of scientific named entities.</p>
<p>PTT is a further restriction beyond PTE in that only prescribed combinations of these science concepts are considered for establishing new tuples.We define PTT as "a closed-domain problem for ALG, in which a system is only allowed to detect leaderboard entries composed of specific, predefined combinations of known scientific concepts rather than forming any new combination."</p>
<p>In PTT variants of ALG, only predetermined combinations (often observed combinations) are used for creating new tuples (e.g., as in Hou et al. (2019)).</p>
<p>Open Domain: An open-domain problem allows extraction of novel entities or tuples without relying on prior knowledge (e.g.taxonomies or lists), making it less constrained.This setup is often more application-friendly, as the extraction scope is guided solely by the user's information needs.</p>
<p>While more appealing to users, the opendomain variant requires handling duplicates, as the same concept may appear in different forms (e.g."ROUGE" vs. "RGE" (Jain et al., 2020;Şahinüç et al., 2024)).This makes evaluation harder than in the closed domain, where canonical representations (e.g.predefined strings) enable direct accuracy measurement.Open-domain outputs may require fuzzy or semantic comparison metrics to handle variation.</p>
<p>Scope of Extraction</p>
<p>Beyond differences in domain knowledge, extraction scope also varies.Prior work differs in which classes of scientific concepts, typically methodolog-ical attributes like task, dataset, method, metric, and score, are included.</p>
<p>Furthermore, most work focuses only on extracting the top results from each paper, restricting each paper to a single entry per leaderboard (Hou et al., 2019;Kardas et al., 2020;Hou et al., 2021;Yang et al., 2022b).If a publication presents two methods, only the top-performing one typically appears on the leaderboard.This can lead to an incomplete and potentially biased view, omitting valuable contributions such as negative results.10</p>
<p>Overview of ALG Datasets</p>
<p>With the growth of the field, several datasets have been proposed to evaluate ALG methods, making it hard for researchers to identify which datasets are best suited for benchmarking.To guide dataset selection, Table 2 summarises their key characteristics. 11We highlight the main dimensions along which datasets differ.The main takeaway from this table is the diversity of the datasets that have been used in past research, making it hard to make fair comparisons.We discuss the variations below.A few recent datasets offer valuable attributes: LEGOBench (Singh et al., 2024) is the largest and covers the broadest tuple scope (including score), while SciLead ( Şahinüç et al., 2024) stands out for its exhaustive manual annotations.</p>
<p>ML Experiment Science Entities</p>
<p>As prior work has varied in the entity classes studied, datasets have likewise differed in the scope Entities Format Annotations Unk.</p>
<p>Dataset First Reported In</p>
<p>Variants T D M S Md PDF L A T E X HA PwC NLPP Ann.</p>
<p>ORKG-PwC Kabongo et al. (2021)   of their tuple and entity annotations.The most common format is ⟨task, dataset, metric, score⟩ (NLP-TDMS, (Hou et al., 2019), PwC-LB (Kardas et al., 2020), TDMS-Ctx (Kabongo et al., 2024), SciLead ( Şahinüç et al., 2024)), while the most comprehensive format is ⟨task, dataset, metric, score, method⟩ (LEGOBench, (Singh et al., 2024)).These five datasets can be considered "complete" leaderboard datasets, as they include the score within the tuple. 12In contrast, two related datasets do not include scores: ORKG-PwC (Kabongo et al., 2021), and, SciREX (Jain et al., 2020).Note that, for SciREX, the GitHub dataset includes a score. 14 It is unclear whether this was added after the publication of the paper, demonstrating that data versioning can be a challenge.
v1-v7 ✓ ✓ ✓ ✗ ✗ □ □ ✗ ✓ ✗ □ NLP-TDMS Hou et al. (2019) v1-v3 ✓ ✓ ✓ ✓ ✗ □ □ ✗ ✗ ✓ □ PwC-LB Kardas et al. (2020) v1-v2 ✓ ✓ ✓ ✓ ✗ □ □ ✗ ✓ ✗ ✗ SciREX Jain et al. (2020) - ✓ ✓ ✓ ✗ ✓ ∼ ∼ ✓ ✓ ✗ ✗ TDMS-Ctx Kabongo et al. (2024) v1-v6 ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ LEGOBench Singh et al. (2024) - ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✓ ✓ SciLead Şahinüç et al. (2024) - ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗</p>
<p>Source of Annotations</p>
<p>Most datasets are assembled using manually curated leaderboards as a distant supervision source.For example, the first leaderboard dataset, NLP-TDMS (Hou et al., 2019), was derived from a community-maintained GitHub repository NLP Progress 15 , tracking state-of-the-art NLP datasets and tasks.With the growing popularity of Paper with Code, many researchers turn to this resource to build ALG datasets, including ORKG-PwC, PwC-LB, SciREX, TDMS-Ctx and LEGOBench.Not all datasets were created with manual annotations, however.Of the datasets derived from 12 These datasets can sometimes be divided into further subsets based on the size of the leaderboard.E.g., the ORKG-PwC and NLP-TDMS datasets filter out leaderboards with less than five entries.Datasets can also be divided into predefined subsets.E.g., the ORKG datasets include pre-defined splits that correspond to experimentation by Kabongo et al. (2024)</p>
<p>Format of the Papers</p>
<p>Datasets differ in publication formats of the source publications.PDFs, though common, mix presentation with logical structure, whereas L A T E X representations allows one to precisely isolate content from presentation.Some datasets use only one format, PDF (LEGOBench, SciLead) or L A T E X(TDMS-Ctx), while others provide both (NLP-TDMS, ORKG-PwC, PwC-LB).We note that this distinction is less important as tools like Grobid (Lopez, 2009) grow in maturity to transform PDF files into a logical structure format, such as XML.</p>
<p>Overview of ALG Evaluation Metrics</p>
<p>One key issue in the field has been the use of various metrics for ALG evaluation, hindering result comparisons.Appendix F lists all metrics used in leaderboard experiments.Below, we outline the key evaluation metrics used in prior work.</p>
<p>Precision, Recall and F1</p>
<p>Most work reports micro precision, recall, and F1, either for exact tuple matches or per entity class (e.g., task, metric).Some report macro variants, which offer deeper insights when frequent entities or tuples skew micro scores.</p>
<p>Although not explicitly stated, we believe that generally these scores are calculated per paper and then averaged.However, Singh et al. (2024) calculated precision and recall per leaderboard.Ex-perimental results can vary significantly depending on whether metrics are averaged across papers, leaderboards, or entities/tuples.To demonstrate this significance, we replicated an experiment of Şahinüç et al. (2024) and found that if authors had used global averaging instead of per paper averages the recall would differ by 12.61. 16In Table 7 (Appendix F), we provide definitions of these metrics.</p>
<p>With the rise of generative AI with LLMs, there has been a need to explore string comparison metrics beyond exact match.For example, Kabongo et al. (2024) explored partial matches.We note that metrics are useful in open-domain settings, where multiple valid expressions may exist and exact matching is too restrictive.</p>
<p>Leaderboard Specific Metrics</p>
<p>In addition to standard retrieval metrics, Şahinüç et al. ( 2024) introduced four metrics for leaderboard evaluation: leaderboard recall (LR), paper coverage (PC), result coverage (RC), and average overlap (AO).LR measures the percentage of correctly identified test leaderboards.PC and RC compute the average percentage of correctly linked papers and scores per leaderboard, respectively.AO quantifies the overlap between generated and test leaderboards (Webber et al., 2010).These leaderboard-specific metrics go beyond entity-or tuple-level evaluation by directly measuring the quality of the reconstructed leaderboard as a whole.This shift is crucial: standard precision and recall metrics may overlook whether the extracted information actually supports leaderboard reconstruction, i.e., better reflect the end-goal of ALG systems.Hence, adopting such metrics is essential for driving progress in building end-to-end usable and trustworthy leaderboard extraction tools.</p>
<p>Granularity of Science Concepts</p>
<p>As science advances, scientific concepts evolve.For example, broad terms like neural LMs may split into finer categories (e.g., pre-trained LMs vs. LLMs), or sibling concepts may merge or have their relative importance change (e.g., abstractive summarisation overtaking extractive summarisation as the dominant summarisation approach with the advent of deep learning).Similarly, what counts as an appropriate level of detail may change, such as the hyperparameters for neural networks, which has become part of standard reporting practice.</p>
<p>Extraction beyond Best Scores</p>
<p>Current ALG's focus on best scores limits its use to state-of-the-art comparisons and has drawn criticism for lacking real-world relevance.Ethayarajh and Jurafsky (2020) highlight that this emphasis neglects factors like fairness, compactness, and energy efficiency.Santy and Bhattacharya (2021) call for metrics beyond accuracy to better reflect practical utility.Braggaar et al. (2024) argue that rankings can mislead, as top models may underperform in practice.Rodriguez et al. (2021) emphasise that not all evaluation examples are equally informative, urging leaderboards to account for difficulty.Together, these critiques advocate for broader, more meaningful evaluation.</p>
<p>Including all experimental results introduces complexity, both methodologically (e.g., an LLM must extract more tuples, though many LLMs cannot output that many tokens) and from a user perspective (e.g., users must interpret a more complex leaderboard instead of a traditional one).</p>
<p>ALG Unified Conceptual Framework</p>
<p>To allow AI system builders to make system design choices based on research outcomes from ALG, we present the ALG Unified Conceptual Framework.For example, to build an ML leaderboard system, engineers may want to use the conceptualisation as inspiration for modules in a system architecture or agents in an Agentic AI system.</p>
<p>This conceptualisation is based on our analysis of the papers outlined in Table 1. Figure 2 illustrates these conceptual components and we provide examples of the methods for these components below, noting not all works include every component, reflecting differing research focuses.</p>
<p>The purpose of this conceptualisation is threefold: to (1) guide future researchers entering ALG research or building ALG systems; (2) organise the ALG experimentation space; and (3) understand the system-level importance of contributions.</p>
<p>Document Representation</p>
<p>We note that several papers focus on finding the best representation of paper contents, whether starting from PDF or from structured formats like L A T E Xor XML.Such representations help highlight key information, especially when later ML components must process limited input text.For example, approaches using pre-trained language models (e.g.BERT), document representation is crucial due to input length limits (Hou et al., 2019).Hou et al. (2019) and Kabongo et al. (2021) used document surrogates like "DocTAET" (title, abstract, experimental setup, tables).Document representation can be more granular; for example, Jain et al. (2020) use entity chains to detect tuples.</p>
<p>Even with LLMs and their larger context windows, document representation remains important.Although LLMs can process full papers, the representation affects which information is used.Kabongo et al. (2024), for example, compare filtered document views with full-text inputs to assess effectiveness.</p>
<p>An alternative document representation that should be explored in future research for ALG is the use of image-based inputs, where PDF pages are treated as images and processed by vision-based or multimodal LLMs.This preserves layout structure crucial for accurate table interpretation, especially in cases where leaderboard-relevant results are embedded in complex visual formats.Recent OCR-free models (Khalighinejad et al., 2025), demonstrate that bypassing textual conversion of scientific text can reduce noise and improve tuple extraction accuracy in visually rich scientific documents.</p>
<p>To support large-scale ALG deployment, document representations like DocTAET can also be used to reduce LLM input length and inference cost by filtering to only the most relevant sections (e.g., title, abstract, tables).This makes processing more efficient when applied across tens of thousands of papers.</p>
<p>Tuple Candidate Generation</p>
<p>Given a document representation, this component extracts key contextual experimental attributes (e.g., task, dataset) and the result.There are various ways to extract this information, based on how domain knowledge is used.</p>
<p>Regarding Closed Domain Approaches</p>
<p>For PTE closed domain approaches, entities are generally defined in a finite set (PTE class).Any candidate tuples must be composed of these predefined entities and any new combination is acceptable.For example, systems can identify the key scientific concepts (e.g., extracting experiment attributes from relevant tables (Kardas et al., 2020)) to compose the tuples.For PTT approaches, the aim is to match the predefined tuple with the source document, in order to check for an improvement in performance.Hou et al. (2019) frame this as a Natural Language Inference (NLI) task, to see whether the tuple is inferred by the document representation.</p>
<p>Regarding Open Domain Approaches</p>
<p>For open-domain approaches, tuples may include entities beyond a predefined list.For example, in SciREX (Jain et al., 2020), an entity detector identifies spans corresponding to task, data set, metric, or method.These unbounded entities are then used to compose tuples.However, the authors do not specify how the extracted tuples would update the leaderboard database.</p>
<p>In Şahinüç et al. (2024), detected entities correspond to concepts that fall into two categories: (1) unseen (i.e., new) and (2) seen.Using a leaderboard database that is initially empty, entities are checked for corresponding entries, with either an exact match or a partial match.If a match exists, the existing form in the database is used as the canonical representation for that concept.This can be viewed as a data normalisation step.For all unmatched entities, these are treated as unseen, and a new database entry is created for them.</p>
<p>For the ALG data normalisation step, we recommend caching normalisation decisions: once the LLM maps entity or tuple A to B, the same rule can be reused for identical cases, avoiding repeated LLM calls and reducing computational cost.</p>
<p>A Note on Score Extraction</p>
<p>Despite being central to ALG, only a handful of works (Hou et al., 2019;Kardas et al., 2020;Singh et al., 2024;Kabongo et al., 2024;Şahinüç et al., 2024) extract best scores.Other work focused on extracting the experimental conditions.We note that finding these conditions is a precursor to finding the full tuple for ALG (identifying experimental conditions to which the best score belongs).For works that extract best scores, methods vary.Hou et al. (2019) apply heuristics based on orthographic features (boldface), whereas Kardas et al. ( 2020) use more complex inferences, classifying table cells as numeric or non-numeric.Extracted quantities are normalised and the extreme (maximum or minimum) score is kept based on the metric.Earlier models used dedicated methods to align scores with conditions, whereas recent LLM prompting extracts entire tuples, including scores, with a single task-based prompt (Kabongo et al., 2024;Singh et al., 2024;Şahinüç et al., 2024).</p>
<p>Tuple Verification and Entity Alignment</p>
<p>For each extracted tuple, the system should verify its correctness, especially for LLM-based approaches, which are susceptible to hallucination risk.Prior to the introduction of LLM, methods often implicitly included this step within the extraction process.For example, by framing the tuple generation task as an NLI problem, Hou et al. (2019) extract tuples that are aligned with the source content and entailed by the source text, essentially providing some form of rationale for generated results.Others use partial alignment of the tuple at the entity level, such as using a Bayesian model to map different equivalent referring expressions to a canonical value (Kardas et al., 2020).</p>
<p>Updating Leaderboard Database</p>
<p>Once a tuple is verified, the final step is updating the leaderboard database.Kardas et al. ( 2020) link experimental conditions to existing Papers with Code entries.Data may be normalised prior to this step ( Şahinüç et al., 2024), and filtered to exclude, for example, ablation studies (Kardas et al., 2020).Most prior work does not detail this step, as the focus lies on NLP techniques for extraction rather than their downstream application, despite often being motivated by it.Efficient ALG database updates at scale require batched writes and schemaaware indexing.To avoid redundant updates, tuples can be deduplicated with hash-based checks.</p>
<p>ALG Benchmarking Guidelines</p>
<p>Open versus Closed Domain Reporting</p>
<p>We recommend that researchers report results for both open-and closed-domain scenarios.Closeddomain, which assumes predefined entities and tuples, provides the simplest case and typically yields the highest accuracy.Open-domain, by contrast, does not rely on predefined knowledge and thus represents the most challenging case.However, in practical applications, scenarios will typically fall between these extremes.To ensure that benchmarking captures this full range of difficulty, and to allow comparisons across studies, we advise that researchers always include results for both domains.Including both allows to assess the feasibility of leaderboard extraction under both the most constrained and the most unconstrained settings, which reflects the diversity of real-world conditions.</p>
<p>Dataset Reporting</p>
<p>We recommend that researchers report results on publicly available datasets as a minimum requirement.We highlight SciLead and LEGOBench as two suitable options.SciLead is valuable for its fully human-curated annotations, ensuring high quality.LEGOBench offers the largest dataset with broad tuple coverage, enabling large-scale benchmarking across diverse tasks and methods.These two datasets are complementary: SciLead provides a gold standard for high-accuracy evaluation, while LEGOBench allows robust assessment at scale.The feasibility of achieving broader and more informative evaluations strongly depends on ensuring open access to such datasets.Fortunately, SciLead and LEGOBench are fully opensource and thus support the practical feasibility of standardised evaluation without subscription or copyright barriers.However, a limitation of both datasets is that they only cover a restricted set of metadata attributes and focus solely on extracting the best results per paper.Therefore, in Section §7.6, we recommend that researchers develop more comprehensive datasets that include all reported results and richer metadata.We also want to highlight the need for dataset versioning.The documentation ambiguity for SciREX (Jain et al., 2020), as discussed in §3.1, where the original paper omits the score while the GitHub repository later includes one, illustrates the problems which can arise when data differs from the original publication.</p>
<p>Metrics</p>
<p>Researchers should report precision, recall, and F1 as both micro and macro scores.Micro scores capture overall accuracy, favouring frequent entries, while macro scores weight papers, leaderboards, or entities equally and better reflect performance across varied result types.Reporting both provides balance, but most importantly researchers must clearly state the averaging method used (e.g. per paper, per leaderboard, or global).</p>
<p>In open-domain settings, exact string matching may be overly restrictive.We recommend reporting partial match metrics, which account for fuzzy or approximate matches.Such metrics better capture performance when multiple valid surface forms exist for the same scientific concept.This reflects real-world application scenarios more accurately.</p>
<p>To assess practical usability for leaderboard construction, researchers should report leaderboardspecific metrics.In particular, we highlight leaderboard recall (LR), paper coverage (PC), result coverage (RC), and average overlap (AO).These metrics provide insights into how effectively extracted tuples populate leaderboards.Leaderboard recall reflects whether leaderboards are correctly identified.Paper coverage measures whether all relevant papers are linked.Result coverage assesses the proportion of extracted results, and average overlap quantifies agreement between generated and ground truth leaderboards.</p>
<p>When possible, results should also be analysed across fine-grained scientific concepts.For example, extraction accuracy should be reported not only at the tuple level, but also separately for tasks, datasets, metrics, methods, and scores.This supports a nuanced understanding of performance, especially where new or rarely seen concepts may be difficult to extract.</p>
<p>ALG Challenges and New Directions</p>
<p>To help guide ALG researchers and system designers to potentially novel capabilities, we list in this section challenges and new directions for ALG.</p>
<p>New or Unseen Entities</p>
<p>The 2024 shared task on ALG (D'Souza et al., 2024) highlights that many aspects of the task are still unsolved.It includes closed and open domain subtasks, with the latter involving new entity detec-tion.17Indeed, Kabongo et al. (2023a) showed that ML performance in extracting tuples with new entities (i.e., new scientific concepts, such as a newly introduced ML task or dataset) is much lower than extracting tuples with previously observed entities.In production, a challenge will be the feasibility of canonicalisation and disambiguation of these newly introduced ML entities.New entities often have ambiguous and inconsistent naming.For example, a newly introduced dataset might be referred to in short and long forms or with typos.In practice, feasibility depends on having automated canonicalisation methods that can cluster or align different surface forms of unseen entities.Without this, leaderboard entries will fragment into inconsistent records, undermining usability.</p>
<p>Document Representation</p>
<p>Representing source paper content remains an open challenge, even with LLMs' larger context windows.Kabongo et al. (2024) found that using the full document with DocTAET led to worse tuple extraction, underscoring the need for representations that balance coverage and minimise irrelevant content during inference.Another practical feasibility consideration is that LLMs with larger context windows are more expensive, making it desirable for users to adopt document representations that allow feasible use of smaller, more efficient models.</p>
<p>Extracting Numerical Scores</p>
<p>In most cases, the performance of tuple extraction, including scores, is significantly lower than that of tuples containing only the experimental conditions (which typically has F1 scores &gt; 80), highlighting the difficulty of score extraction (Kardas et al., 2020;Hou et al., 2019;Yang et al., 2022b;Şahinüç et al., 2024).For example, in recent work by Şahinüç et al. (2024), score extraction using GPT-4 achieved an F1 score of approximately 70.</p>
<p>Feasibility of extracting scores from a practical perspective goes further: not only must scores be extracted accurately, but extraction must be robust across various expressions of results.Systems must also handle ambiguous cases, such as ranges, averages, or multiple competing values.Current systems fall short in this respect, limiting the feasibility of fully automated leaderboard generation.</p>
<p>Feasibility of Extraction at Scale</p>
<p>Most research papers benchmark ALG systems on dozens or hundreds of papers.However, production-grade leaderboards such as Papers with Code integrate tens of thousands of papers.Extracting tuples at this scale introduces feasibility challenges in computational efficiency and LLM inference cost.Practical implementation of an alwaysupdating leaderboard requires optimised batching, caching strategies, and asynchronous processing.</p>
<p>Generalisability beyond ML</p>
<p>A promising direction for future research is to explore the generalisability of ALG beyond ML.Domains like material science and biomedicine also report experimental results but use more varied formats and less standardised terminology.Key challenges include handling heterogeneous result expressions, complex domain language, and diverse contextual cues.</p>
<p>Comprehensive Leaderboards</p>
<p>A key direction for future research is the development of comprehensive leaderboards.By comprehensive, we mean not only vertically, by including all experimental results rather than only the best, but also horizontally, by capturing richer metadata (e.g., hyperparameters).A necessary first step is the creation of a novel dataset to benchmark both existing and new techniques.</p>
<p>Conclusion</p>
<p>In the position paper, we provide the first overview of ALG research, which reveals substantial diversity in problem framing and benchmarking practices.To address this fragmentation, we propose an ALG unified conceptual framework and present ALG benchmarking guidelines.Furthermore, our first overview of ALG research to date revealed that the scope of current leaderboards is limited.Therefore, one key recommendation in our list of challenges and new directions for ALG is to expand leaderboard coverage.Future leaderboards should report all results, including baselines, ablations, and method variations, and enrich tuples with broader metadata (e.g.hyperparameters) to create a more informative resource.In support of this initiative, a continually updated reading list is maintained in a GitHub repository. 1818 https://github.com/RoelTim/ML-leaderboard-position-paper</p>
<p>Limitations</p>
<p>A limitation of this paper is the scope, as we solely focus on the automatic generation of ML leaderboards.We note that other disciplines also report experimental outcomes, although the nature of the experimental procedures may differ.For example, Ghosh et al. (2024) explore finetuning LLMs for schema-based information extraction in material science.Another example is Wang et al. (2024), which introduce SciDaSynth, an interactive system using LLMs to extract and synthesise structured knowledge from the scientific literature in the form of tables.</p>
<p>While this position paper does not include new experiments, it aims to establish the foundational scaffolding required for rigorous future evaluations.We therefore provide clear evaluation setups and guidelines that, for future research, can be directly applied to assess existing and future ALG systems.2023) improved the extraction of experimental procedures using fine-tuned language models.Other prior work has demonstrated the ability to extract methods and processes in scientific text for life sciences (Wan et al., 2010(Wan et al., , 2009) ) and material science (Yang et al., 2022a).Wang et al. (2024) introduced SciDaSynth, an interactive system using LLMs to extract and synthesise structured knowledge from scientific literature.Recently, Pramanick et al. ( 2025) proposed a method for automatically extracting, categorizing, and quantitatively analyzing contribution statements in research papers.Knowledge graph generation has been explored in other disciplines, like astronomy (Khoo et al., 2023;Timmer et al., 2023).</p>
<p>C Problem Framing Details</p>
<p>Different methodologies for extracting leaderboard tuples rely on distinct document representations.The document representation defines which sections of a research paper are used before extracting leaderboard-related information.Doc-TAET contains text from a Document's Title, Abstract, Experimental Setup, and Table information.DocREC consists of text from a Document's Results, Experiments, and Conclusion sections.Some approaches extract content from the full paper, while others focus specifically on tables or citation tables.In Table 4, we show for each proposed methodology which document representation they use.</p>
<p>Methodology</p>
<p>Document Representation TDMS-IE (Hou et al., 2019) DocTAET * , SC ORKG-TDM (Kabongo et al., 2021) DocTAET ORKG-LB (Kabongo et al., 2023b) DocTAET PI Graph (Singh et al., 2019) Citation Tables AxCell (Kardas et al., 2020) Full Paper &amp; Tables SciREX-IE (Jain et al., 2020) Full Paper TELIN (Yang et al., 2022b) Full Paper &amp; Tables TDMS-PR (Kabongo et al., 2024) DocREC † MS-PR (Singh et al., 2024) Full Paper TDMR-PR ( Şahinüç et al., 2024) Full</p>
<p>D Methodology Details</p>
<p>In this section, we provide a summary of all the proposed ALG methodologies, and in Table 5, we list for each methodology which language models it uses.(Hou et al., 2019) BERT (Devlin et al., 2019) ORKG-TDM (Kabongo et al., 2021) XLNet (Yang et al., 2019), SciBERT (Beltagy et al., 2019), BERTbase (Devlin et al., 2019) ORKG-LB (Kabongo et al., 2023b) BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), XLNet (Yang et al., 2019), BigBird (Michalopoulos et al., 2022) PI Graph (Singh et al., 2019) Undefined AxCell (Kardas et al., 2020) ULMFiT classifier (Howard andRuder, 2018), BM25 (Robertson et al., 2009) SciREX-IE (Jain et al., 2020) SciBERT (Beltagy et al., 2019), BiLSTM (Graves and Schmidhuber, 2005) TELIN (Yang et al., 2022b) SpERT (Eberts and Ulges, 2020) TDMS-PR (Kabongo et al., 2024) Llama 2 (Touvron et al., 2023), Mistral (Jiang et al., 2023) MS-PR (Singh et al., 2024) Falcon (Almazrouei et al., 2023), Galactica (Taylor et al., 2022), Llama 2 (Touvron et al., 2023), Llama 3 (Dubey et al., 2024), Mistral (Jiang et al., 2023), Vicuna (Chiang et al., 2023), Zephyr (Tunstall et al., 2023), Gemini (Team et al., 2023), GPT-4 (Achiam et al., 2023) TDMR-PR ( Şahinüç et al., 2024) Llama 2 (Touvron et al., 2023), Llama 3 (Dubey et al., 2024), Mixtral (Jiang et al., 2024), GPT-4 (Achiam et al., 2023) Table 5: Overview of the language models used in each methodology, demonstrating how the methodologies have (logically) adopted more advanced models over time as discussed in Section 5.</p>
<p>TDMS-IE</p>
<p>ing raw text and section information from documents (excluding figures, tables, and equations  (Yang et al., 2019), an autoregressive transformer capable of handling contexts longer than BERT's 512-token maximum.</p>
<p>TELIN Yang et al. (2022b) proposed TELIN, a methodology to extract ⟨task, dataset, model, method⟩ tuples from research papers.TELIN begins by converting unstructured PDFs into structured documents, using YOLO to detect paragraphs, headings, captions, and tables (Redmon et al., 2016).SPLERGE is then applied to extract table components such as rows, columns, and cells (Tensmeyer et al., 2019).For NER, TELIN uses SpERT, a BERT-based model pre-trained on the SCiERC dataset, to classify scientific entities into categories like task, method, dataset, and evaluation metric (Eberts and Ulges, 2020).String matching between these entities and non-numeric table cells is performed using fuzzy search to handle nonexact matches and acronyms.Tuples are formed when at least three of the four entities (task, dataset, metric, model) are identified within the table and its caption.These extracted leaderboards are stored in a shared knowledge base, which is iteratively refined to discover more entities across documents.</p>
<p>A human review stage prioritises uncertain entities, using feedback to fine-tune SpERT, iterating until entity prediction stabilises.</p>
<p>ORKG-LB Kabongo et al. (2023b) introduced ORKG Leaderboard (ORKG-LB), a follow-up methodology of ORKG-TDM (Kabongo et al., 2021).ORKG-LB focuses on the extraction of the ⟨task, dataset, metric⟩ tuples by framing the extraction task as an entailment problem.ORKG-LB starts by allowing users to input a LaTeX or PDF version of the research paper.ORKG-LB uses the GROBID parser (Lopez, 2009) for PDF files and PANDOC (MacFarlane, 2006-) to convert LaTeX files into XML TEI markup.Then, ORKG-LB extracts DocTAET (Hou et al., 2019), focusing on sections likely to contain task-dataset-metric mentions, reducing noise and enhancing generalisation.</p>
<p>For training the inference, for each paper, positive and negative samples of tuples are required.For the number of false triples per paper, ORKG-LB relies on the same task-specific parameter as used for ORKG-TDM.For the inference model, the authors of ORKG-LB experiment with four different transformer model variants: BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), XLNet (Yang et al., 2019) and BigBird (Zaheer et al., 2020).</p>
<p>TDMS-PR The work of Kabongo et al. (2024) experiments with prompting LLMs to extract ⟨task, dataset, metric, score⟩ tuples from research papers, and we refer to this methodology as TDMS-PR.</p>
<p>The authors experiment with different document representations provided to the LLM when prompting the LLM.They propose a novel document representation, DocREC, which comprises text from the results (R), experiments (E) and conclusions (C) sections.They compare the results when using DocREC to when using DocTAET (Hou et al., 2019) or DocFull, which is the full paper as document representation.On average, DocREC consists of more tokens than DocTAET, 1,586 versus 493, and by definition, DocFull is by default always the longest document representation.The authors experiment with LLMs from the Flan-T5 collection, Mistral 7B and Llama 3 7B.</p>
<p>MS-PR</p>
<p>The authors of Singh et al. (2024) prompt an LLM to extract the ⟨method, score⟩ tuple given a research paper representation and a ⟨task, dataset, metric⟩ tuple; we refer to this as MS-PR.While both TDMS-PR (Kabongo et al., 2024) and MS-PR are prompt-based, their tuple scopes differ: TDMS-PR focuses on ⟨task, dataset, metric⟩, while MS-PR targets ⟨method, score⟩.Singh et al. (2024) experiment with MS-PR by using a wide range of LLMs: Falcon, Falcon Instruct, Galactica, Llama 2 (7B &amp; 13B), Llama 2 Chat (7B &amp; 13B), Mistral Instruct, Vicuna (7B &amp; 13B), Zephyr Beta, Gemini Pro and GPT-4 (Almazrouei et al., 2023;Taylor et al., 2022;Touvron et al., 2023;Jiang et al., 2023;Chiang et al., 2023;Anthropic, 2024;Team et al., 2023;Achiam et al., 2023).</p>
<p>TDMR-PR The authors of Şahinüç et al. ( 2024) prompt an LLM to extract ⟨task, dataset, metric, score⟩ tuples, we refer to this method as TDMR-PR.First, TDMR-PR extracts the tuples from the papers via a retrieval-augmented generation method using an LLM.Second, depending on the domain (closed, hybrid, or, open), TDMR-PR normalises these tuples to a predefined taxonomy or creates new entries for novel tasks, datasets, or metrics.Lastly, TDMR-PR ranks the papers based on their performance, constructing or updating leaderboards accordingly.</p>
<p>E Dataset Details</p>
<p>F Definitions of Metrics</p>
<p>In this section, we define the micro and macro versions of the Precision, Recall, and F1 metrics for the ALG task.Based on our best guess, most of the existing works typically compute micro precision, micro recall, and micro F1 by first calculating these scores per paper and then averaging them.However, this is solely a best guess, and we know that, for example, Kabongo et al. (2024) and Singh et al. (2024) calculate the score on a leaderboard level.We recommend that future researchers either use these definitions of these metrics or explicitly specify if they average across a different dimension (e.g., across leaderboards), as the choice of the averaging method can significantly impact the final score.</p>
<p>Micro P = 1
P P p=1 Np i=1 T Pp,i Np i=1 (T Pp,i + F Pp,i)(1)
where P represents the total number of papers, and N p represents the total number of extracted leaderboard tuples or entities, per paper p.The term T P p,i denotes the number of true positive instances for the i-th instance in paper p, while F P p,i represents the number of false positive instances  for the i-th instance in the same paper.The precision is first computed for each individual paper before being averaged across all P papers.Micro Recall measures the proportion of correctly identified leaderboard entities/tuples:
Micro R = 1 P P p=1 Np i=1 T Pp,i Np i=1 (T Pp,i + F Np,i)(2)
where F N p,i represents the number of false negatives for the i-th instance in paper p.</p>
<p>Micro F1 is the harmonic mean of micro precision and micro recall, providing a balanced measure of extraction performance:
Micro F1 = 2 × Micro P × Micro R Micro P + Micro R (3)
We recommend also reporting the macro variants of these metrics to give more insight if some of the entries/tuples appear frequently and, therefore, disproportionally influence the micro scores.For macro metrics, we first average across all classes and then across P papers.Macro precision is given by:
Macro P = 1 P P p=1 1 Cp Cp c=1 Np,c i=1 T Pp,c,i Np,c i=1 (T Pp,c,i + F Pp,c,i)(4)
where C p is the number of classes for each paper p.</p>
<p>Macro Recall is given by: Table 7: Overview of evaluation metrics used in each paper.</p>
<p>Macro R = 1
P P p=1 1 C C c=1 Np,c i=1 T Pp,c,i Np,c i=1 (T Pp,c,i + F Np,c,i)(5)
And Macro F1 is given by:
Macro F1 = 1 P P p=1 1 C C c=1 2 × Pp,c × Rp,c Pp,c + Rp,c(6)
It is important to note that these definitions serve as an example of how micro and macro variations can be calculated when averaged at the paper level.However, these definitions can be easily adapted for calculations at the leaderboard level.</p>
<p>G An Overview of Experimental Results</p>
<p>We have compiled all the results we could find in the literature where researchers experiment with extracting leaderboard tuples and entities, evaluating these extractions using micro, partial micro, or macro precision, recall, and F1 scores.Tables 8 -15 present an overview of these experiments.These tables highlight the complexity of comparing different results due to the diversity of problem framing (e.g.closed versus open domain), datasets and metrics.We omitted details on how the scores were averaged (e.g., across papers or leaderboards), as this information is often not reported in many studies.These differences in averaging methods also complicate direct comparisons between works.Please note that there may be additional subtle variations in the experimental setup that are not captured in these tables, which could prevent a fair comparison.SciLead TDMR-PR GPT-4 # REC, TAET, and Full refer to DocREC, DocTAET, and the Full Paper representations of the document, respectively.These are reported as part of an ablation study examining different document representations.For more details on these representations, see § 5.1.SciLead TDMR-PR GPT-4 # REC, TAET, and Full refer to DocREC, DocTAET, and the Full Paper representations of the document, respectively.These are reported as part of an ablation study examining different document representations.For more details on these representations, see § 5.1.</p>
<p>Figure 1 :
1
Figure 1: An example of extracting ⟨task, dataset, metric, method, score⟩ tuples from research papers to build a leaderboard 2 .</p>
<p>(T = Task, D = Dataset, M = Metric, S = Score, Md = Method), Format (PDF, L A T E X), Annotations (HA = Human Annotation, PwC = Papers with Code, NLPP = NLP Progress), inclusion of unknown annotations (Unk.Ann.).✓= yes, ✗= no, □ = depends on variant, ∼ = use L A T E X source if available, otherswise use PDF.</p>
<p>Paper &amp; Tables * Hou et al. (2019) perform ablation studies with variations of DocTAET.† Kabongo et al. (2024) compare the performance of three document representations: DocREC, DocTAET, and the Full Paper.</p>
<p>. (2024) ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ leaderboard recall (LR), paper coverage (PC), result coverage (RC), and average overlap (AO)</p>
<p>Table 1 :
1
Characterisation of problem framing per method.Domain: open if extraction does not rely on prior knowledge, closed if restricted to a defined scope.Structured Data: Y if leaderboard tuples must appear in specific paper sections (e.g.tables or results), N otherwise.Scope of Extraction: extent of tuples extracted.</p>
<p>Table 2 :
2
Summary of datasets, detailing dataset Variants, Entities captured</p>
<p>Research Paper Document Representation Tuple Candidate Generation Tuple Verification and Entity Alignment Updating Leaderboard Database Leaderboard Database
Figure 2: ALG Unified Conceptual Framework.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan.2019.SciBERT: A pretrained language model for scientific text.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615-3620.
Salomon Kabongo, Jennifer D'Souza, and Sören Auer. Fatih Şahinüç, Thi Thao Tran, Yuliya Grishina, YufangGeorge Michalopoulos, Michal Malyska, Nicola Sahar,2024. Effective context selection in llm-based leader-Hou, Bowen Chen, and Iryna Gurevych. 2024. Ef-Alexander Wong, and Helen Chen. 2022. ICDBig-board generation: An empirical study. In Inter-ficient performance tracking: Leveraging large lan-Bird: A contextual embedding model for ICD codenational Conference on Applications of Natural guage models for automated construction of scientificclassification. In Proceedings of the 21st WorkshopLanguage to Information Systems, pages 150-160. leaderboards. In Proceedings of the 2024 Conferenceon Biomedical Language Processing, pages 330-336,Springer. on Empirical Methods in Natural Language Process-Dublin, Ireland. Association for Computational Lin-ing (EMNLP), Miami, Florida, USA. Association forguistics.Anouck Braggaar, Linwei He, and Jan De Wit. 2024. Our dialogue system sucks-but luckily we are at the top of the leaderboard!: A discussion on current prac-Salomon Kabongo, Jennifer D'Souza, and Sören Auer. Computational Linguistics. 2021. Automated mining of leaderboards for empiri-cal AI research. volume 13133 LNCS. A Acronyms tices in NLP evaluation. In Proceedings of the 6th ACM Conference on Conversational User Interfaces, pages 1-5. Davide Buscaldi, Anne-Kathrin Schumann, Behrang Salomon Kabongo, Jennifer D'Souza, and Sören Auer. Acronym Full form 2023b. ORKG-leaderboards: a systematic workflow ALG Automatic Leaderboard Generation for mining leaderboards as a knowledge graph. Inter-ML Machine Learning national Journal on Digital Libraries. NLP Natural Language Processing QasemiZadeh, Haïfa Zargayouna, and Thierry Charnois. 2018. SemEval-2018 task 7: Semantic re-lation extraction and classification in scientific papers. Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebas-tian Ruder, Sebastian Riedel, Ross Taylor, and Robert Stojnic. 2020. AxCell: Automatic extraction of re-LLM Large Language Model EMNLP Conference on Empirical Methods in Natural Language Processing PTE Predefined Typed Entities In Proceedings of the 12th International Workshop on Semantic Evaluation, pages 244-257, New Orleans, Louisiana. Springer, Association for Computational Linguistics. sults from machine learning papers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8580-8594, Online. Association for Computational Lin-guistics. PTT Predefined Typed Tuples DocTAET Document representation: Title, Abstract, Ex-perimental Setup, Tables DocREC Document representation: Results, Experi-ments, Conclusion ORKG Open Research Knowledge GraphIshani Mondal, Yufang Hou, and Charles Jochim. 2021. End-to-end construction of NLP knowledge graph. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1885-1895, Online. Association for Computational Linguistics. Huitong Pan, Qi Zhang, Eduard Dragut, Cornelia Caragea, and Longin Jan Latecki. 2023. Dmdd: A large-scale dataset for dataset mentions detection. Transactions of the Association for Computational Linguistics, 11:1132-1146. Aniket Pramanick, Yufang Hou, Saif Mohammad, and Iryna Gurevych. 2023. A diachronic analysis of paradigm shifts in NLP research: When, how, and why? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,PwC Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Papers with Code SciREX Scientific Research Information Extraction Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and (dataset)pages 2312-2326, Singapore. Association for Com-putational Linguistics.Wen-tau Yih. 2020. Dense passage retrieval for open-LEGOBench Leaderboard Generation BenchmarkAniket Pramanick, Yufang Hou, Saif M. Mohammad,domain question answering. In Proceedings of the SciLead Scientific Leaderboard (dataset)and Iryna Gurevych. 2025. The nature of NLP: Ana-2020 Conference on Empirical Methods in Natural NLI Natural Language Inferencelyzing contributions in NLP papers. In ProceedingsF1 Language Processing (EMNLP), pages 6769-6781. Harmonic mean of Precision and Recall Ghazal Khalighinejad, Sharon Scott, Ollie Liu, Kelly L Anderson, Rickard Stureborg, Aman Tyagi, and LR Leaderboard Recall PC Paper Coverage RC Result Coverage AO Average Overlap Bhuwan Dhingra. 2025. MatViX: Multimodal In-formation Extraction from Visually Rich Articles. In (Volume 1: Long Papers), pages 3636-3655. tational Linguistics: Human Language Technologies the Americas Chapter of the Association for Compu-Proceedings of the 2025 Conference of the Nations of L A T E X Document preparation system OCR Optical Character Recognitionof the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25169-25191, Vienna, Austria. Association for Computational Linguistics. recognition, pages 779-788. IEEE conference on computer vision and pattern real-time object detection. In Proceedings of the Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified,Fech Scen Khoo, Megan Mark, Roelien C. Timmer, Marcella Scoczynski Ribeiro Martins, Emily Fos-hee, Kaylin Bugbee, Gregory Renard, and Anamaria Berea. 2023. Building knowledge graphs in helio-Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and be-yond. Foundations and Trends® in Information Re-trieval, 3(4):333-389.physics and astrophysics. In Natural Language Pro-cessing and Information Systems, pages 215-228, Cham. Springer Nature Switzerland.Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan Boyd-Graber. 2021. Evaluation examples are not equallyPatrice Lopez. 2009. GROBID: Combining automatic bibliographic data recognition and term extraction for scholarship publications. In Research and Advanced Technology for Digital Libraries: 13th European Conference, ECDL 2009, Corfu, Greece, September 27-October 2, 2009. Proceedings 13, pages 473-474. Springer.informative: How should that change NLP leader-boards? In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers), pages 4486-4503, Online. Association for Computa-tional Linguistics.Maciej Rybinski, Stephen Wan, Sarvnaz Karimi, CecileYi Luan, Luheng He, Mari Ostendorf, and HannanehParis, Brian Jin, Neil Huth, Peter Thorburn, and DeanHajishirzi. 2018. Multi-task identification of entities,Holzworth. 2023. Sciharvester: Searching scientificrelations, and coreference for scientific knowledgedocuments for numerical values. In Proceedings ofgraph construction. pages 3219-3232.the 46th International ACM SIGIR Conference onResearch and Development in Information Retrieval,SIGIR '23, page 3135-3139, New York, NY, USA.Association for Computing Machinery.
John MacFarlane.2006-.Pandoc: A Universal Document Converter.John MacFarlane.</p>
<p>Table 3 :
3
List of acronyms used in this paper.
B Related Work Beyond ALGEntity Recognition and Relation Extractionfrom Scientific Text Entity and relation extrac-tion from scientific papers gained attention in 2017with the SemEval-2017 ScienceIE task, which fo-cused on identifying key elements like processes,tasks, and materials in publications (Augensteinet al., 2017). The SemEval-2018 Task 7 advancedthis by classifying relationships such as "uses","compares", and "improves" between scientific con-cepts (Buscaldi et al., 2018). Mondal et al. (2021)built a knowledge graph from NLP papers by ex-tracting four types of relations: "evaluatedOn" (as-sociating tasks with datasets), "evaluatedBy" (as-
(Hou et al., 2021), andaluation metrics), as well as "coreferent" and "related" relations, which capture connections among entities of the same type.Datasets like SciERC(Luan et al., 2018), TDM-Sci(Hou et al., 2021), and Dmdd (Pan et al., 2023)</p>
<p>Table 4 :
4
Overview of the Methodologies.Document Representation: The content extracted from the paper before extracting the leaderboard tuples.</p>
<p>Table 6 presents an extended version of Table 2, providing detailed information for each version of the included datasets.For every train, test, and validation split, we report the number of associated papers and extracted tuples.This table highlights the substantial diversity across datasets, which complicates direct comparisons between experiments.</p>
<p>For annotations, we distinguish between human annotations (HA), Papers with Code (PwC) and NLP Progress (NLPP), however PwC includes partial human annotation, and domain experts fully curated NLP Progress via GitHub pull requests.∼ Use LaTeX if available; otherwise, default to PDF. ‡Different data for training (unlabelled arXiv papers and segmented tables)
EntitiesFormatAnnotations * Unk. Train Stats.Test Stats.Val. Stats.PaperV T D M S Md PDF L A T E X HA PwC NLPP Ann. #P#T#P#T#P#TORKG-PwC DatasetKabongo et al. (2021) v1 ✓ ✓ ✓ ✗ ✗ ✓✗✗ ✓✗✗ 2,831 † 11,724 † 1,228 † 5,060 † --Kabongo et al. (2021) v2 ✓ ✓ ✓ ✗ ✗ ✓✗✗ ✓✗✓ 3,753 † 11,724 † 1,608 † 5,060 † --Kabongo et al. (2023b) v3 ✓ ✓ ✓ ✗ ✗ ✗✓✗ ✓✗✗ 587 † 9,614 † 270 † 4,096 † --Kabongo et al. (2023b) v4 ✓ ✓ ✓ ✗ ✗ ✗✓✗ ✓✗✓ 2,946 † 9,614 † 1,262 † 4,096 † --Kabongo et al. (2023b) v5 ✓ ✓ ✓ ✗ ✗ ✓✗✗ ✓✗✗ 587 † 9,614 † 270 † 4,096 † --Kabongo et al. (2023b) v6 ✓ ✓ ✓ ✗ ✗ ✓✗✗ ✓✗✓ 2,946 † 9,614 † 1,262 † 4,096 † --Kabongo et al. (2023a) v7 # ✓ ✓ ✓ ✗ ✗ ✓✗✗ ✓✗✓--1,000 1,925--NLP-TDMS DatasetHou et al. (2019)v1 ✓ ✓ ✓ ✓ ✗ ✓✗✗✗✓✗124325118281--Hou et al. (2019)v2 ✓ ✓ ✓ ✓ ✗ ✓✗✗✗✓✓170325162281--Kardas et al. (2020)v3 ✓ ✓ ✓ ✓ ✗ ✗✓✗✗✓✓ ≤170 ≤325 ≤162 ≤281--PwC-LB DatasetKardas et al. (2020)v1 ✓ ✓ ✓ ✓ ✗ ✗✓✗ ✓✗✗‡‡516 2,802  ‡‡Yang et al. (2022b)v2 ✓ ✓ ✓ ✓ ✗ ✓✗✗ ✓✗✗--516 2,802--SciREX DatasetJain et al. (2020)✓ ✓ ✓ ✗ ✓ ∼∼✓ ✓✗✗ ≤438∇≤438∇ ≤438 ∇TDMS-Ctx DatasetKabongo et al. (2024) v1  § ✓ ✓ ✓ ✓ ✗ ✗✓✗ ✓✗✓ 11,807 402,409 1,326 33,863 --Kabongo et al. (2024) v2  § ✓ ✓ ✓ ✓ ✗ ✗✓✗ ✓✗✓ 12,388 415,788 1,401 34,799 --Kabongo et al. (2024) v3  § ✓ ✓ ✓ ✓ ✗ ✗✓✗ ✓✗✓ 10,058 415,788 1,105 31,213 --Kabongo et al. (2024) v4  § ✓ ✓ ✓ ✓ ✗ ✗✓✗ ✓✗✓ 11,807 402,409 746 14,604 --Kabongo et al. (2024) v5  § ✓ ✓ ✓ ✓ ✗ ✗✓✗ ✓✗✓ 12,388 415,788 789 14,800 --Kabongo et al. (2024) v6  § ✓ ✓ ✓ ✓ ✗ ✗✓✗ ✓✗✓ 10,058 415,788 595 14,273 --LEGOBench DatasetSingh et al. (2024)✓ ✓ ✓ ✓ ✓ ✓✗✗ ✓✗✓--♢ 43,105 --SciLead DatasetŞahinüç et al. (2024)✓ ✓ ✓ ✓ ✗ ✓✗✓ ✗✗✗--43⊘--and validation (linked results).  †Two-fold cross-validation: 70% train, 30% test, with averaged results. ♢ 9,847 leaderboards, andthe number of papers is unspecified.  § v1-v3 are few-shot experiment datasets with document representations: v1 (DocFULL),v2 (DocREC), and v3 (DocTAET). v4-v6 are zero-shot experiment datasets with the same representations: v4 (DocFULL),v5 (DocREC), and v6 (DocTAET). # the same data source as v2, but with updated timestamps and no overlap with v2. ∇ Anaverage of 5 tuples annotations per paper. ⊘ Unspecified, with 138 unique tuples reported.
*</p>
<p>Table 6 :
6
This table summarises the datasets from multiple research papers, detailing dataset variant (V), Entities captured (T = Task, D = Dataset, M = Metric, S = Score, Md = Method), format (PDF, L A T E X), Annotations (HA = Human Annotation, PwC = Papers with Code, NLPP = NLP Progress), and inclusion of unknown annotations (Unk.Ann.).Additionally, the table includes Train, Test, and validation (Val.)statistics (Stats.): the number of papers (#P) and tuples (#T).</p>
<p>SM, MLC, and EL are baseline methods, representing String Match, Multi-Label Classification, and Entity Linking, respectively.* trained on ORKG-PwC-v6/v7.# REC, TAET, and Full refer to DocREC, DocTAET, and the Full Paper representations of the document, respectively.These are reported as part of an ablation study examining different document representations.For more details on these representations, see § 5.1.
MicroMacroPart. MicroReported InPRF1PRF1PF1 DatasetMethodExperimental SetupClosed Domain Problem FramingHou et al. (2019)60.2 73.1 66.0 54.1 65.9 56.6NLP-TDMS-v1TDMS-IEHou et al. (2019)29.4 42.0 34.6 24.9 43.6 28.1NLP-TDMS-v1EL  †Hou et al. (2019)56.8 23.8 33.6 56.8 30.9 37.3NLP-TDMS-v1MLC  †Hou et al. (2019)16.8 7.8 10.6 8.1 6.4 6.9NLP-TDMS-v1SM  †Hou et al. (2019)60.8 76.8 67.8 62.5 75.2 65.3NLP-TDMS-v2TDMS-IEHou et al. (2019)24.3 36.3 29.1 18.1 31.8 20.5NLP-TDMS-v2EL  †Hou et al. (2019)42.0 20.9 27.9 42.0 23.1 27.8NLP-TDMS-v2MLC  †Hou et al. (2019)36.0 19.6 25.4 31.8 30.6 31.0NLP-TDMS-v2SM  †Hou et al. (2019)68.6 40.3 50.8 29.6 29.1 28.1NLP-TDMS-v2TDMS-IETAE #Hou et al. (2019)50.0 23.7 32.2 20.8 20.1 19.4NLP-TDMS-v2TDMS-IETAT #Hou et al. (2019)47.9 14.2 21.9 11.3 11.3 10.7NLP-TDMS-v2TDMS-IETA #Kardas et al. (2020)65.8 58.5 61.9 56.0 55.8 54.1NLP-TDMS-v3AxCellKardas et al. (2020)53.4 66.3 59.2 57.1 66.1 58.5NLP-TDMS-v3TDMS-IEKardas et al. (2020)67.8 47.8 56.1 47.9 46.4 43.5PwC-LB-v1AxCellKabongo et al. (2021) 76.4 66.4 71.1 63.5 64.1 61.4NLP-TDMS-v1ORKG-TDMXLNetKabongo et al. (2021) 65.3 73.1 69.0 57.6 68.7 60.1NLP-TDMS-v1ORKG-TDMSciBERTKabongo et al. (2021) 79.5 57.6 66.8 59.0 55.4 54.7NLP-TDMS-v1ORKG-TDMBERTKabongo et al. (2021) 77.1 70.9 73.9 71.7 73.9 70.6NLP-TDMS-v2ORKG-TDMXLNetKabongo et al. (2021) 79.6 63.3 70.5 68.1 67.5 65.5NLP-TDMS-v2ORKG-TDMBERTKabongo et al. (2021) 65.7 76.8 70.8 65.7 77.2 68.3NLP-TDMS-v2ORKG-TDMSciBERTKabongo et al. (2021) 95.1 92 93.5 92.3 93.5 91.7ORKG-PwC-v1ORKG-TDMXLNet TAET #Kabongo et al. (2021) 93.5 93.2 93.3 90.5 94.4 91.2ORKG-PwC-v1ORKG-TDMXLNet TAT #Kabongo et al. (2021) 95.0 90.5 92.7 91.6 93.1 91.2ORKG-PwC-v1ORKG-TDMXLNet #Kabongo et al. (2021) 95.7 88.3 91.8 91.7 92.1 90.8ORKG-PwC-v1ORKG-TDMBERTKabongo et al. (2021) 94.2 89 91.5 89.2 91.5 89.2ORKG-PwC-v1ORKG-TDMXLNet TAE #Kabongo et al. (2021) 94.4 87.6 90.9 89.7 91.4 89.4ORKG-PwC-v1ORKG-TDMSciBERTKabongo et al. (2021) 92.6 90 91.3 88.6 92.9 89.4ORKG-PwC-v1ORKG-TDMXLNet TA #Kabongo et al. (2021) 94.9 91.2 93.0 92.8 94.8 92.8ORKG-PwC-v2ORKG-TDMXLNetKabongo et al. (2021) 95.5 89.1 92.1 92.8 93.9 92.4ORKG-PwC-v2ORKG-TDMBERTKabongo et al. (2021) 94.1 88.5 91.2 90.9 93.4 91.1ORKG-PwC-v2ORKG-TDMSciBERTKabongo et al. (2023b) 95.2 92.2 93.6 91.5 93.3 91.3ORKG-PwC-v5ORKG-LBBigBERTKabongo et al. (2023b) 94.8 93.9 94.3 91.3 94.4 91.8ORKG-PwC-v5ORKG-LBBERTKabongo et al. (2023b) 94.8 93.9 94.3 91.3 94.4 91.8ORKG-PwC-v5ORKG-LBSciBERTKabongo et al. (2023b) 95.4 93.9 94.7 93.2 95.7 93.5ORKG-PwC-v6ORKG-LBBERTKabongo et al. (2023b) 95.4 91.1 93.2 92.6 94.3 92.2ORKG-PwC-v6ORKG-LBSciBERTKabongo et al. (2023b) 93.2 94.9 93.0 95.7 92.4 94.0ORKG-PwC-v6ORKG-LBBigBERTKabongo et al. (2023b) 95.1 94.6 94.8 93.1 96.4 93.7ORKG-PwC-v6ORKG-LBXLNetKabongo et al. (2023b) 95.4 88.0 91.5 91.2 92.3 90.6ORKG-PwC-v3ORKG-LBBERTKabongo et al. (2023b) 93.7 86.0 89.7 89.4 91.7 89.2ORKG-PwC-v3ORKG-LBSciBERTKabongo et al. (2023b) 93.6 85.3 89.3 87.5 88.7 86.6ORKG-PwC-v3ORKG-LBBigBirdKabongo et al. (2023b) 94.9 91.2 93.0 91.9 94.4 92.0ORKG-PwC-v4ORKG-LBXLNetKabongo et al. (2023b) 96.0 90.0 92.9 93.5 94.2 92.8ORKG-PwC-v4ORKG-LBBERTKabongo et al. (2023b) 94.6 88.6 91.5 91.7 93.9 91.6ORKG-PwC-v4ORKG-LBSciBERTKabongo et al. (2023b) 94.6 87.2 90.7 90.7 91.6 89.7ORKG-PwC-v4ORKG-LBBigBirdKabongo et al. (2023a) 9.2 78.1 16.5 14.3 86.6 21.9ORKG-PwC-v7 * ORKG-TDMXLNetKabongo et al. (2023a) 14.1 72.9 23.6 20.1 83.4 28.9ORKG-PwC-v7 * ORKG-TDMBERTKabongo et al. (2023a) 10.4 81.7 18.4 16.2 89 24.4ORKG-PwC-v7 * ORKG-TDMBERTKabongo et al. (2023a) 10.1 76.8 17.8 14.9 86.4 22.7ORKG-PwC-v7 * ORKG-TDMXLNetŞahinüç et al. (2024)55.1 25.8 35.1SciLeadAxCellŞahinüç et al. (2024)40.7 39.5 40.1SciLeadTDMR-PRLlama 2+CSŞahinüç et al. (2024)35.9 34.9 35.4SciLeadTDMR-PRLlama 2Şahinüç et al. (2024)58.4 52.1 55.1SciLeadTDMR-PRMixtral+CSŞahinüç et al. (2024)55.7 48.8 51.0SciLeadTDMR-PRMixtralŞahinüç et al. (2024)62.0 58.1 60.0SciLeadTDMR-PRLlama 3+CSŞahinüç et al. (2024)77.1 72.6 74.8SciLeadTDMR-PRLlama 3Şahinüç et al. (2024)69.0 63.8 66.3SciLeadTDMR-PRGPT-4+CSŞahinüç et al. (2024)75.3 70.4 72.8SciLeadTDMR-PRGPT-4Open Domain Problem FramingYang et al. (2022b)68.2 45.3 56.5 49.7 43.1 42.5PwC-LB-v2TELINHybrid Domain Problem FramingŞahinüç et al. (2024) 27.23 22.99 24.93SciLeadTDMR-PRLlama 2Şahinüç et al. (2024) 27.89 24.48 26.07SciLeadTDMR-PRMixtralŞahinüç et al. (2024) 50.75 45.30 47.87SciLeadTDMR-PRLlama 3Şahinüç et al. (2024) 56.08 51.89 53.90SciLeadTDMR-PRGPT-4
†</p>
<p>Table 8 :
8
Summary of results for leaderboard ⟨Task, Dataset, Metric⟩ extraction.SM, MLC, and EL are baseline methods, representing String Match, Multi-Label Classification, and Entity Linking, respectively.
MicroMacroPart. MicroReported InPRF1PRF1PF1 DatasetMethodExperimental SetupClosed Domain Problem FramingHou et al. (2019)10.8 13.1 11.8 9.3 11.8 9.9NLP-TDMS-v1TDMS-IEHou et al. (2019)3.8 1.8 2.4 1.3 1.0 1.1NLP-TDMS-v1SM  †Hou et al. (2019)6.8 2.9 4.0 6.8 6.1 6.2NLP-TDMS-v1MLC  †Kardas et al. (2020) 27.4 24.4 25.8 20.2 20.6 19.7NLP-TDMS-v3AxCellKardas et al. (2020) 6.8 8.4 7.5 8.6 9.5 8.8NLP-TDMS-v3TDMS-IEKardas et al. (2020) 37.4 23.2 28.7 24.0 21.8 21.1PwC-LB-v1AxCellŞahinüç et al. (2024) 32.59 13.67 19.26SciLeadAxCellŞahinüç et al. (2024) 10.06 21.59 13.73SciLeadTDMR-PRLlama 2+CSŞahinüç et al. (2024) 9.63 15.25 11.81SciLeadTDMR-PRLlama 2Şahinüç et al. (2024) 26.54 24.61 25.54SciLeadTDMR-PRMixtral+CSŞahinüç et al. (2024) 24.66 21.73 23.10SciLeadTDMR-PRMixtralŞahinüç et al. (2024) 23.22 29.54 26.00SciLeadTDMR-PRLlama 3+CSŞahinüç et al. (2024) 27.11 35.60 30.78SciLeadTDMR-PRLlama 3Şahinüç et al. (2024) 49.82 48.71 49.26SciLeadTDMR-PRGPT-4+CSŞahinüç et al. (2024) 56.02 54.53 55.27SciLeadTDMR-PRGPT-4Open Domain Problem FramingYang et al. (2022b) 38.3 20.8 26.3 26.6 19.2 21.3PwC-LB-v2TELINHybrid Domain Problem FramingŞahinüç et al. (2024) 4.17 9.89 5.87SciLeadTDMR-PRLlama 2Şahinüç et al. (2024) 14.65 12.27 13.35SciLeadTDMR-PRMixtralŞahinüç et al. (2024) 15.70 18.75 17.09SciLeadTDMR-PRLlama 3Şahinüç et al. (2024) 40.60 39.56 40.07SciLeadTDMR-PRGPT-4Şahinüç et al. (2024) 51.01 51.03 51.02SciLeadTDMR-PRGPT-4 FS
†</p>
<p>Table 9 :
9
Summary of results for leaderboard ⟨Task, Dataset, Metric, Score⟩ extraction.Notations: FS = Few Shot.
MicroMacroPart. MicroReported InPRF1PRF1PF1 DatasetMethodExperimental SetupClosed Domain Problem FramingJain et al. (2020) 0.48 0.89 0.62SciREXTDMS-IEOpen Domain Problem FramingJain et al. (2020) 0.53 0.72 0.61SciREXSciREX-IE</p>
<p>Table 10 :
10
Summary of results for leaderboard ⟨Task, Dataset, Metric, Method⟩ extraction.
MicroMacroPart. MicroReported InPRF1PRF1PF1 DatasetMethodExperimental SetupClosed Domain Problem FramingKardas et al. (2020)70.6 57.3 63.3 60.7 62.6 59.7PwC-LB-v1AxCellKabongo et al. (2021) 97.4 93.6 95.5 93.7 94.8 93.6ORKG-PwC-v1ORKG-TDMXLNetKabongo et al. (2023b) 96.8 95.9 96.4 94.3 97.2 95.0ORKG-PwC-v6ORKG-LBXLNetKabongo et al. (2023b) 96.8 95.9 96.4 94.3 97.2 95.0ORKG-PwC-v4ORKG-LBXLNetŞahinüç et al. (2024) 68.98 58.52 63.32SciLeadAxCellŞahinüç et al. (2024) 59.83 67.20 63.30SciLeadTDMR-PRLlama 2+CSŞahinüç et al. (2024) 55.45 60.74 57.97SciLeadTDMR-PRLlama 2Şahinüç et al. (2024) 86.27 91.99 89.04SciLeadTDMR-PRMixtral+CSŞahinüç et al. (2024) 86.85 89.74 88.27SciLeadTDMR-PRMixtralŞahinüç et al. (2024) 85.69 90.85 88.19SciLeadTDMR-PRLlama 3+CSŞahinüç et al. (2024) 87.33 92.17 89.68SciLeadTDMR-PRLlama 3Şahinüç et al. (2024) 90.70 90.77 90.73SciLeadTDMR-PRGPT-4+CSŞahinüç et al. (2024) 90.62 91.10 90.86SciLeadTDMR-PRGPT-4Open Domain Problem FramingYang et al. (2022b)70.3 53.7 59.2 60.5 57.3 57.1PwC-LB-v2TELINKabongo et al. (2024) 31.8913.9754.92 24.05 TDMS-Ctx-v5TDMS-PRLlama2 7B ZS REC #Kabongo et al. (2024) 24.5621.7543.46 38.48 TDMS-Ctx-v6TDMS-PRLlama2 7B ZS TAET #Kabongo et al. (2024) 2.062.0652.54 3.36 TDMS-Ctx-v4TDMS-PRLlama2 7B ZS Full #Kabongo et al. (2024) 17.9917.9959.25 29.88 TDMS-Ctx-v5TDMS-PRMistral 7B ZS REC #Kabongo et al. (2024) 26.9926.9964.00 44.90 TDMS-Ctx-v6TDMS-PRMistral 7B ZS TAET #Kabongo et al. (2024) 0.220.5662.50 0.56 TDMS-Ctx-v4TDMS-PRMistral 7B ZS Full #Kabongo et al. (2024) 34.1020.9351.13 31.37 TDMS-Ctx-v2TDMS-PRLlama2 7B FS REC #Kabongo et al. (2024) 30.6129.5344.96 43.37 TDMS-Ctx-v3TDMS-PRLlama2 7B FS TAET #Kabongo et al. (2024) 34.691.5950.00 2.29 TDMS-Ctx-v1TDMS-PRLlama2 7B FS Full #Kabongo et al. (2024) 37.6526.7755.90 39.75 TDMS-Ctx-v2TDMS-PRMistral 7B FS REC #Kabongo et al. (2024) 39.4833.3854.82 46.35 TDMS-Ctx-v3TDMS-PRMistral 7B FS TAET #Kabongo et al. (2024) 32.430.8171.43 1.19 TDMS-Ctx-v1TDMS-PRMistral 7B FS Full #Hybrid Domain Problem FramingŞahinüç et al. (2024) 39.70 42.98 41.27SciLeadTDMR-PRLlama 2Şahinüç et al. (2024) 50.23 60.72 54.98SciLeadTDMR-PRMixtralŞahinüç et al. (2024) 65.72 80.39 72.32SciLeadTDMR-PRLlama 3Şahinüç et al. (2024) 63.82 78.30 70.32</p>
<p>Table 11 :
11
Summary of results for leaderboard ⟨Task⟩ extraction.Notations: FS = Few Shot, ZS = Zero Shot.Results of Extracting ⟨Dataset⟩ for Open Domain Problem Framing
MicroMacroPart. MicroReported InPRF1PRF1PF1 DatasetMethodExperimental SetupResults of Extracting ⟨Dataset⟩ for Closed Domain Problem FramingKardas et al. (2020)70.2 48.4 57.3 53.5 52.7 49.9PwC-LB-v1AxCellKabongo et al. (2021) 96.6 91.5 94.0 92.9 93.6 92.4ORKG-PwC-v1ORKG-TDMXLNetKabongo et al. (2023b) 96.2 95.4 95.8 93.8 96.7 94.4ORKG-PwC-v6ORKG-LBXLNetKabongo et al. (2023b) 96.2 95.4 95.8 93.8 96.7 94.4ORKG-PwC-v4ORKG-LBXLNetŞahinüç et al. (2024) 63.66 33.87 44.22SciLeadAxCellŞahinüç et al. (2024) 68.93 58.81 63.47SciLeadTDMR-PRLlama 2+CSŞahinüç et al. (2024) 62.60 55.03 58.57SciLeadTDMR-PRLlama 2Şahinüç et al. (2024) 85.03 73.20 78.67SciLeadTDMR-PRMixtral+CSŞahinüç et al. (2024) 81.68 71.26 76.12SciLeadTDMR-PRMixtralŞahinüç et al. (2024) 82.43 78.62 80.48SciLeadTDMR-PRLlama 3+CSŞahinüç et al. (2024) 92.09 87.75 89.87SciLeadTDMR-PRLlama 3Şahinüç et al. (2024) 86.36 79.93 83.02SciLeadTDMR-PRGPT-4+CSŞahinüç et al. (2024) 92.64 86.05 89.22SciLeadTDMR-PRGPT-4Kabongo et al. (2024) 15.776.8338.32 16.6 TDMS-Ctx-v5TDMS-PRLlama2 7B ZS REC #Kabongo et al. (2024) 12.7211.2626.09 23.1 TDMS-Ctx-v6TDMS-PRLlama2 7B ZS TAET #Kabongo et al. (2024) 20.341.3038.98 2.49 TDMS-Ctx-v4TDMS-PRLlama2 7B ZS Full #Kabongo et al. (2024) 23.4011.8041.73 21.05 TDMS-Ctx-v5TDMS-PRMistral 7B ZS REC #Kabongo et al. (2024) 20.4114.3238.89 27.29 TDMS-Ctx-v6TDMS-PRMistral 7B ZS TAET #Kabongo et al. (2024) 37.500.3375.00 0.67 TDMS-Ctx-v4TDMS-PRMistral 7B ZS Full #Kabongo et al. (2024) 21.2713.0636.66 22.50 TDMS-Ctx-v2TDMS-PRLlama2 7B FS REC #Kabongo et al. (2024) 17.2916.6831.48 30.36 TDMS-Ctx-v3TDMS-PRLlama2 7B FS TAET #Kabongo et al. (2024) 29.591.3639.80 1.82 TDMS-Ctx-v1TDMS-PRLlama2 7B FS Full #Kabongo et al. (2024) 22.1515.6838.52 27.28 TDMS-Ctx-v2TDMS-PRMistral 7B FS REC #Kabongo et al. (2024) 21.8918.5138.73 32.75 TDMS-Ctx-v3TDMS-PRMistral 7B FS TAET #Kabongo et al. (2024) 32.430.5748.65 0.85 TDMS-Ctx-v1TDMS-PRMistral 7B FS Full #Yang et al. (2022b)70.9 52.8 59.3 54.7 55.2 53.9PwC-LB-v2TELINResults of Extracting ⟨Dataset⟩ for Hybrid Domain Problem FramingŞahinüç et al. (2024) 41.05 33.14 36.67SciLeadTDMR-PRLlama 2Şahinüç et al. (2024) 49.67 44.45 46.92SciLeadTDMR-PRMixtralŞahinüç et al. (2024) 66.81 62.86 64.77SciLeadTDMR-PRLlama 3Şahinüç et al. (2024) 83.29 79.52 81.36</p>
<p>Table 12 :
12
Summary of results for leaderboard ⟨Dataset⟩ extraction.Notations: FS = Few Shot, ZS = Zero Shot.</p>
<p>We acknowledge that ALG work rests on a long history of work in information extraction (IE) in scientific literature.
The full body of IE work is out of scope for this analysis but is introduced briefly in Appendix B.5 Details on prior work are in Appendix D.6  We also note that various works have differed in expectations on the data format (e.g., PDF or L A T E X). However, we do not see this as critical in hindering comparisons of results.7  The "open domain" category includes hybrid cases that start with no domain knowledge and incrementally builds up knowledge as publications are processed.8  As in, bound by the closed world assumption.9  We borrow "predefined" from Şahinüç et al. (2024).
E.g., one may wish to compare neural networks with other machine learning methods (e.g., logistic regression, random forests) to evaluate the cost-benefit trade-off.
  11  A more detailed version of this table can be found in Appendix E Table6
The authors conducted a zero-shot experiment evaluated using exact match. They reported a recall of 47.53 when averaging per paper, whereas the recall would have been 34.92 if averaged globally across all tuples.
The organisers refer to these as few-shot and zero-shot, referring on current ML terminology.
For example, bold-faced scores are most likely to be best score.
EthicsThis research is subject to the governance by the ethics board of the Commonwealth Scientific and Industrial Research Organisation (CSIRO).We note that our proposal for AI research is to facilitate decision-making by users, as opposed to complete automation of tasks.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, 10.48550/arXiv.2303.08774Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, 10.48550/arXiv.2311.16867The falcon series of open language models. Daniel Hesslow, Julien Launay, Quentin MalarticarXiv preprintet al. 2023</p>
<p>Zephyr Beta: An advanced LLM by anthropic. Anthropic, 2024</p>
<p>ScienceIE-extracting keyphrases and relations from scientific publications. Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, Andrew Mccallum, 10.18653/v1/S17-2Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). the 11th International Workshop on Semantic Evaluation (SemEval-2017)Vancouver, CanadaAssociation for Computational Linguistics2017. 201710</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* ChatGPT quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Overview of the CLEF 2024 simpletext task 4: SOTA? tracking the state-ofthe-art in scholarly publications. D' Jennifer, Salomon Souza, Hamed Kabongo, Yue Babaei Giglou, Zhang, Working Notes of CLEF. 2024</p>
<p>Span-based joint entity and relation extraction with transformer pre-training. Markus Eberts, Adrian Ulges, 10.3233/FAIA200321ECAI 2020. IOS Press2020</p>
<p>Utility is in the eye of the user: A critique of NLP leaderboards. Kawin Ethayarajh, Dan Jurafsky, 10.18653/v1/2020.emnlp-main.393Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Toward reliable ad-hoc scientific information extraction: A case study on two materials datasets. Satanu Ghosh, Carolina Neal R Brodnik, Collin Frey, Tresa M Holgate, Samantha Pollock, Samuel Daly, Carton, arXiv preprint:2406.05348v12024</p>
<p>Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Alex Graves, Jürgen Schmidhuber, Neural networks. 185-62005</p>
<p>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, 10.18653/v1/P19-1513Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>TDMSci: A specialized corpus for scientific literature entity tagging of tasks datasets and metrics. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, 10.18653/v1/2021.eacl-main.59Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>Universal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>SciREX: A challenge dataset for document-level information extraction. Sarthak Jain, Madeleine Van Zuylen, Hannaneh Hajishirzi, Iz Beltagy, 10.18653/v1/2020.acl-main.670Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Open research knowledge graph: next generation infrastructure for semantic scholarly knowledge. Mohamad Yaser, Jaradeh , Allard Oelen, Kheir Eddine Farfar, Manuel Prinz, D' Jennifer, Gábor Souza, Markus Kismihók, Sören Stocker, Auer, 10.1145/3360901.3364435Proceedings of the 10th international conference on knowledge capture. the 10th international conference on knowledge capture2019</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv preprint:2310.06825v1Mistral 7b. 2023</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Zero-Shot Entailment of Leaderboards for Empirical AI Research. Salomon Kabongo, D' Jennifer, Sören Souza, Auer, 10.1109/JCDL57899.2023.000422023a. June2023</p>
<p>A discussion on building practical NLP leaderboards: the case of machine translation. Sebastin Santy, Prasanta Bhattacharya, 2021arXiv preprint</p>
<p>Automated early leaderboard generation from comparative tables. Mayank Singh, Rajdeep Sarkar, Atharva Vyas, Pawan Goyal, Animesh Mukherjee, Soumen Chakrabarti, 10.1007/978-3-030-15712-8_16Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019. Cologne, GermanySpringer2019. April 14-18, 2019Proceedings, Part I 41</p>
<p>LEGOBench: Scientific Leaderboard Generation Benchmark. Shruti Singh, Shoaib Alam, Husain Malwat, Mayank Singh, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Finetune the entire rag architecture (including dpr retriever) for question-answering. Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Suranga Nanayakkara, arXiv:2106.115172021arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv preprint:2211.09085v1Galactica: A large language model for science. 2022</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Gemini: a family of highly capable multimodal models. 2023. 11805v12312arXiv preprint</p>
<p>Deep splitting and merging for table structure decomposition. Chris Tensmeyer, Brian Vlad I Morariu, Scott Price, Tony Cohen, Martinez, 2019 International Conference on Document Analysis and Recognition (ICDAR). IEEE2019</p>
<p>Nasa science mission directorate knowledge graph discovery. C Roelien, Megan Timmer, Fech Mark, Marcella Scen Khoo, Scoczynski Ribeiro, Anamaria Martins, Gregory Berea, Kaylin Renard, Bugbee, 10.1145/3543873.3587585Companion Proceedings of the ACM Web Conference 2023, WWW '23 Companion. New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv preprint:2307.09288v1et al. 2023.</p>
<p>Zephyr: Direct distillation of lm alignment. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, arXiv:2310.169442023arXiv preprint</p>
<p>Extracting structured seedmediated gold nanorod growth procedures from scientific text with LLMs. Nicholas Walker, Sanghoon Lee, John Dagdelen, Kevin Cruse, Samuel Gleason, Alexander Dunn, Gerbrand Ceder, Kristin A Paul Alivisatos, Anubhav Persson, Jain, Digital Discovery. 262023</p>
<p>Whetting the appetite of scientists: producing summaries tailored to the citation context. Stephen Wan, Cécile Paris, Robert Dale, 10.1145/1555400.1555410Proceedings of the 9th ACM/IEEE-CS Joint Conference on Digital Libraries, JCDL '09. the 9th ACM/IEEE-CS Joint Conference on Digital Libraries, JCDL '09New York, NY, USAAssociation for Computing Machinery2009</p>
<p>Supporting browsing-specific information needs: Introducing the Citation-Sensitive In-Browser Summariser. Stephen Wan, Cécile Paris, Robert Dale, 10.1016/j.websem.2010.03.002Web Semantics: Science, Services and Agents on the World Wide Web. 82-32010</p>
<p>SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model. Xingbo Wang, Samantha L Huey, Rui Sheng, Saurabh Mehta, Fei Wang, 2024. 13765v12404arXiv preprint</p>
<p>A similarity measure for indefinite rankings. William Webber, Alistair Moffat, Justin Zobel, 10.1145/1852102.1852106ACM Transactions on Information Systems (TOIS). 2842010</p>
<p>PIEKM: ML-based procedural information extraction and knowledge management system for materials science literature. Huichen Yang, Carlos Aguirre, William Hsu, 10.18653/v1/2022.aacl-demo.7Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations. the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System DemonstrationsTaipei, Taiwan2022aAssociation for Computational Linguistics</p>
<p>TELIN: Table entity linker for extracting leaderboards from machine learning publications. Sean Yang, Chris Tensmeyer, Curtis Wigington, 2022b</p>
<p>XLNet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le, 201932Advances in neural information processing systems</p>
<p>Big bird: Transformers for longer sequences. Manzil Zaheer, Guru Guruganesh, Avinava Kumar, Joshua Dubey, Chris Ainslie, Santiago Alberti, Philip Ontanon, Anirudh Pham, Qifan Ravula, Li Wang, Yang, Advances in neural information processing systems. 202033</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3883 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3883</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3883</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-7d645a3fd276918374fd9483fd675c28e46506d1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1" target="_blank">Galactica: A Large Language Model for Science</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.</p>
                <p><strong>Paper Abstract:</strong> Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3883.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3883.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only Transformer LLM (up to 120B parameters) trained on a curated scientific corpus to store, combine and reason about scientific knowledge and to synthesize secondary scientific content (reviews, citations, equations, multimodal annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Galactica (weights-only scientific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A large language model trained on a specially curated, multi-modal scientific corpus with special interface tokens (citation tokens [START_REF]/[END_REF], working-memory token <work>, modality wrappers for SMILES, amino/DNA sequences) and prompt pre-training to enable generation of literature syntheses, citation prediction, equation recall, reasoning, and multimodal scientific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Curated scientific corpus totaling 106 billion tokens: ~48 million papers (88B tokens, 83% of tokens), 2M code files (7B tokens), 8M pieces of reference material (7B tokens), 2M knowledge-base entries (2B tokens), filtered CommonCrawl (1B tokens), prompts (0.4B tokens), plus multimodal sequences (SMILES, proteins, DNA). Majority academic text, LaTeX processed where available; vocabulary 50k BPE tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language prompts and task-specific prompt formats included in pre-training; special tokens indicate requested behavior (e.g. prompt ending with [START_REF] to request a citation, wrapping reasoning with <work>, giving SMILES inside [START_SMILES] to request IUPAC name). Zero-shot prompting is the primary evaluation mode, with optional few-shot style prompts not required.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>End-to-end self-supervised pretraining on the curated corpus combined with prompt-pretraining (including ~783k prompt examples, 358M prompt tokens); special-token engineering (citation tokens, <work>, modality wrappers) to let the model internalize document structure and multi-step reasoning; no external retrieval augmentation in main system (weights-only approach).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Free-text scientific outputs including: LaTeX equations, literature summaries and recommendations, predicted citations (paper titles), step-by-step reasoning sequences wrapped in <work>, QA answers, IUPAC names from SMILES, protein annotations, chemical reaction products; outputs can be plain text, LaTeX, or wrapped multimodal sequences depending on prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Extensive benchmarking: knowledge probes (LaTeX equations dataset of 434 equations, domain probes like AminoProbe, BioLAMA, Chemical Reactions), reasoning benchmarks (MMLU mathematics, MATH), downstream scientific QA (PubMedQA, MedMCQA, BioASQ, MedQA-USMLE), citation-prediction datasets (PapersWithCode concept pairs PWC 644, Extended 110, Contextual 1869), IUPAC naming validation (17,052 compounds validated via OPSIN canonicalization), and comparisons to retrieval baselines and other LLMs (GPT-3, OPT, BLOOM, Chinchilla, PaLM, Gopher).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Galactica 120B achieves state-of-the-art results on many scientific tasks reported in the paper: LaTeX equations 68.2% (vs GPT-3 49.0%), LaTeX memorization scales smoothly. MMLU (mathematics) with <work> token: 41.3% vs Chinchilla 35.7%. MATH: 120B 20.4% vs PaLM 8.8% (reported). Downstream QA: PubMedQA 77.6%, MedMCQA dev 52.9%, BioASQ ~94.3% (reported). Citation prediction (120B): PWC 51.9%, Extended 69.1%, Contextual 36.6%, outperforming sparse and dense retrieval baselines on these tasks. IUPAC naming (120B): 39.2% accuracy on a 17k compound validation set. Repeated-epochs training (4x) improved validation loss and downstream performance for curated corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Hallucination risks (title-based citation identifiers prone to hallucination at lower scales); model stores knowledge in weights which can blend facts and is harder to update; limited molecule subset in pretraining (2M of ~110M PubChem compounds) constrains chemical generalization; working-memory (<work>) prompt datasets are small and gains may be limited by dataset diversity; absence of retrieval augmentation means extremely fine-grained facts may still require retrieval; model exhibits bias at smaller scales toward predicting more popular papers (distributional skew), though this diminishes with scale; potential overfitting after ~4 epochs for largest model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Galactica outperforms general LLMs (GPT-3, BLOOM, OPT) on many scientific benchmarks and outperforms Chinchilla on several reasoning tasks when using <work>. It outperforms tuned sparse/dense retrieval baselines on citation prediction tasks at large scale. Some fine-grained or high-school-level tasks are still stronger in Chinchilla (per paper), and Galactica is not universally superior across all general NLP tasks though it performs well on BIG-bench subset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Galactica: A Large Language Model for Science', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3883.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3883.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica Citation Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica citation prediction using title-processed citations and [START_REF]/[END_REF] tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A component of Galactica that predicts the appropriate citation (title-based identifier) given the surrounding document context, trained via special citation tokens and evaluated against curated citation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Galactica citation prediction (title-identifier + [START_REF] token)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Galactica processes citations with title-based identifiers wrapped in [START_REF]/[END_REF] tokens during training so that, given a context (text before a citation), the model can generate the citation title; evaluated as a generation task and compared with sparse (ElasticSearch) and dense retriever baselines (Contriever variants).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Contexts extracted from the arXiv validation set and other curated documents; training included many citation instances from 48M papers in the corpus; three evaluation sets: PWC Citations (644 concept->paper pairs), Extended Citations (110 pairs), Contextual Citations (1,869 in-context citation prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Prompt is the document context ending where a citation would occur (e.g. '... [START_REF]'), or short concept sentences like 'In this paper we use ResNet method [START_REF]'. The model must generate the paper title that corresponds to the concept/context.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised/self-supervised generation over citation-annotated text during pretraining (title-based identifiers); no external retrieval used for the main system. Ablations considered title vs alphanumeric id identifiers and found title-based identifiers gave greater citation prediction accuracy but higher hallucination risk at low scales.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Generated paper title(s) as plain text between [START_REF] and [END_REF]; can be post-processed to match canonical identifiers if available.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Measured top-1 generation accuracy on PWC, Extended, and Contextual citation datasets; distributional comparison using Kolmogorov-Smirnov distance between counts of predicted vs ground-truth referenced papers to assess bias toward popular papers. Baselines: ElasticSearch sparse retrieval, Contriever dense retrievers (base and fine-tuned) with FAISS indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Accuracy scales with model size: GAL 120B achieved 51.9% (PWC), 69.1% (Extended), 36.6% (Contextual). These results outperform sparse retriever (30.9%, 17.3%, 5.3% respectively) and dense retriever baselines (best fine-tuned dense: 27.6%, 11.8%, 8.2%). KS distance between predicted and ground-truth citation-count distributions declined with model scale, indicating reduced popularity bias.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Title-based identifiers can increase hallucination risk especially at lower scales; model predictions may be biased toward popular papers at small sizes; reliance on weights-only storage can limit recall of very fine-grained or newly published works; comparison depends on prompt format and on having titles in training data; no external index means updates require re-training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>At large scale Galactica exceeds standard sparse and dense retrieval baselines on the curated citation-prediction tasks; no direct human-comparison scores reported, but empirical distributional metrics indicate improved fidelity with scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Galactica: A Large Language Model for Science', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3883.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3883.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Pre‑training (Galactica)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt pre-training (including many task prompts in self-supervised pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy used in Galactica that injects diverse zero-shot/format prompts into pretraining so the model better composes and generalizes to downstream tasks without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Prompt pre-training (as used in Galactica)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Rather than only pretraining on raw corpora, the approach augments the pretraining mix with prompt-formatted tasks (QA, summarization, reasoning, chemical properties, etc.) so the model sees many task contexts during self-supervised learning and learns to respond to natural language task requests.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Included ~783,599 prompt examples across task types totaling ~358 million tokens, collected from QA datasets, chemical property records, reasoning datasets (e.g. GSM8k transformed), summarization, entity extraction and other tasks. These were mixed with the general 106B-token scientific corpus during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Task queries are natural-language prompts (zero-shot style) of many diverse formats included during pretraining; diversity aimed to reduce need for few-shot examples at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Self-supervised learning treating prompts as ordinary training tokens so the model internalizes mapping from prompt form to task output; this is different from post-hoc instruction tuning or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Model outputs aligned to prompt formats: answers to QA prompts, summaries, extracted entities, reasoning traces wrapped in <work>, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Measured by downstream benchmark performance (MMLU, BIG-bench subset, scientific QA tasks) and by in-domain vs out-of-domain performance splits to identify potential data leakage; compared to instruction-tuned and few-shot approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Prompt pre-training helped Galactica achieve strong zero-shot and few-shot performance on many scientific tasks, enabling smaller models to perform competitively with larger few-shot models. Inclusion of prompts is claimed to reduce the need for extremely large token counts or enormous model sizes for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt pre-training is not a substitute for instruction tuning; including prompts in pretraining risks some in-domain data leakage for specific benchmark tasks and requires care distinguishing in-domain vs out-of-domain evaluation; the set of prompt datasets used by Galactica is modest in size relative to the corpus and may limit coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Authors compare conceptually to ExT5, FLAN, T0 and UnifiedQA style approaches, noting prompt-pretraining has weaker task bias than instruction tuning but better out-of-the-box performance than pure unsupervised pretraining; concrete numeric head-to-heads are reported via downstream benchmarks where Galactica performs strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Galactica: A Large Language Model for Science', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3883.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3883.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval‑Augmented Models (RAG / RETRO / Atlas)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented generation approaches (examples: RAG, RETRO, Atlas) mentioned as related work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of systems that combine a retrieval component (sparse or dense) with a neural generator so that external documents are retrieved and fed to the model during generation to provide fine-grained, up-to-date facts and reduce reliance on stored weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Retrieval-augmented generation (RAG / RETRO / Atlas - related work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>These systems retrieve relevant documents from an external index (sparse or dense), and condition the language model on the retrieved passages to generate answers or summaries; they are presented in the paper as complementary approaches to weights-only models for knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>External retrieval index constructed from documents (papers, abstracts, passages); size varies by implementation (can scale to billions/trillions of tokens in RETRO-style systems); not the primary approach used in Galactica experiments but discussed as an important alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Query is a natural language prompt or context; retriever returns candidate documents given the query; generator distills or synthesizes output conditioned on retrieved text.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two-stage: retrieval (sparse/dense) followed by conditional generation by an LLM; retrieval can be fine-tuned (dense retrievers) to map contexts to relevant papers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Generated summaries, answers, extracted facts, or synthesized knowledge that cite or reference retrieved documents; output may also include extracted structured knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not evaluated experimentally in this paper but cited literature typically evaluates via QA benchmarks, retrieval and generation metrics, and human evaluation of factuality and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper notes retrieval-augmented models require less model capacity for similar factual accuracy but need supporting retrieval infrastructure; Galactica focuses on weights-only and shows some tasks (citation prediction) where a large weights-only model outperforms tuned sparse and dense retrieval baselines on curated citation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Require external infrastructure (indexing, retrieval latency, storage); retrieval quality depends on index coverage and tuning; integration with generative models raises concerns of provenance, citation and hallucination; authors note retrieval will likely be needed for extremely fine-grained facts even with larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Comparisons in the paper show Galactica (weights-only) outperforms the sparse and dense retrieval baselines on the citation prediction tasks used in evaluation, but the authors acknowledge retrieval augmentation is likely beneficial for future work and fine-grained retrieval needs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Galactica: A Large Language Model for Science', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3883.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3883.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAMA-style Knowledge Probes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LAMA (Language Model Analysis) / knowledge-probing approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of probing methodologies (originating with Petroni et al., 2019) that query language models with cloze-style or targeted prompts to measure stored factual knowledge; Galactica uses LAMA-like probes adapted for scientific content (LaTeX equations, domain probes).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LAMA-style knowledge probes (applied to scientific content)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Construct targeted prompts to test whether a model can recall specific facts (e.g. equation name -> LaTeX, reactants -> products, entity properties). In Galactica these probes were extended to LaTeX equation generation (434 equations) and domain-specific triples (AminoProbe, BioLAMA, Chemical Reactions, Galaxy Clusters, Minerals).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Probe datasets compiled by authors: 434 LaTeX equations across chemistry/physics/math/stats/econ; domain datasets (AminoProbe, BioLAMA, etc.); model training corpus is the Galactica scientific corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Structured prompts such as 'The formula for X is:' or masked factual prompts to elicit the target fact or LaTeX expression; zero-shot evaluation primarily used.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>No distillation per se — probes are diagnostic: present a prompt and evaluate generated token sequence against expected equation or fact (manual checking for LaTeX where multiple correct forms exist).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Generated factual text or LaTeX equations; outputs assessed for correctness (exact or judged acceptable by human reviewers in ambiguous cases).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Automatic exact-match where possible; manual evaluation for LaTeX due to multiple valid formulations; results compared across model sizes and versus baseline LLMs (GPT-3, OPT, BLOOM).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Galactica shows steady scaling in probe performance; Galactica 120B achieved 68.2% overall on LaTeX equation probe (vs GPT-3 49.0%). Domain probes show smooth gains with scale (e.g., chemical reactions 43.1% at 120B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Probes may require manual evaluation (LaTeX variability), and zero-shot prompt difficulty (e.g. BioLAMA) can depress measured performance; probes measure memorization/recall more than synthesis of novel theory; probing does not guarantee reliable generative reasoning across arbitrary prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Comparisons reported to GPT-3, OPT, BLOOM; Galactica outperforms these baselines on many of the scientific probe tasks at larger scales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Galactica: A Large Language Model for Science', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3883.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3883.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scientific LMs (SciBERT / BioLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specialized scientific language models (examples: SciBERT, BioLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work showing benefits of curated scientific corpora for domain-specialized pretraining; typically smaller models pre-trained on scientific text leading to improved performance on domain tasks compared to general LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Scientific domain-specific LMs (SciBERT, BioLM, ScholarBERT referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Models pre-trained on domain-specific corpora (biomedical, scientific articles) with the goal of improving downstream domain tasks; mentioned as related work motivating Galactica's curated-corpus approach.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Smaller curated corpora compared to general LLM corpora (examples: SciBERT trained on Semantic Scholar corpus, ScholarBERT with >200B tokens but small model capacity), domain-specific datasets (biomedical papers, abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Standard NLP prompts and fine-tuning for domain tasks (NER, QA, classification); not primarily oriented to large-scale literature synthesis across many modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Self-supervised pretraining on domain corpora; often followed by supervised fine-tuning on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Domain-specific NLP outputs (answers, classifications, embeddings) rather than broad literature synthesis; used as baselines or motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Standard domain benchmarks (biomedical QA, NER, etc.) showing gains from domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Prior work demonstrates curated scientific corpora help domain tasks; authors cite these works as evidence that a curated, normative dataset can be beneficial for building a scientific LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Previous models often smaller in capacity and scope; may lack multi-modality and large-scale composition abilities; prior corpora smaller (<20B tokens for some), limiting certain capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared conceptually against general LLMs; domain-specialized LMs outperform general models on some domain tasks, motivating Galactica's curated large corpus approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Galactica: A Large Language Model for Science', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models as Knowledge Bases? <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP <em>(Rating: 2)</em></li>
                <li>SciBERT: A Pretrained Language Model for Scientific Text <em>(Rating: 2)</em></li>
                <li>Finetuned Language Models Are Zero-Shot Learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3883",
    "paper_id": "paper-7d645a3fd276918374fd9483fd675c28e46506d1",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A Large Language Model for Science",
            "brief_description": "A decoder-only Transformer LLM (up to 120B parameters) trained on a curated scientific corpus to store, combine and reason about scientific knowledge and to synthesize secondary scientific content (reviews, citations, equations, multimodal annotations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Galactica (weights-only scientific LLM)",
            "system_or_method_description": "A large language model trained on a specially curated, multi-modal scientific corpus with special interface tokens (citation tokens [START_REF]/[END_REF], working-memory token &lt;work&gt;, modality wrappers for SMILES, amino/DNA sequences) and prompt pre-training to enable generation of literature syntheses, citation prediction, equation recall, reasoning, and multimodal scientific outputs.",
            "input_corpus_description": "Curated scientific corpus totaling 106 billion tokens: ~48 million papers (88B tokens, 83% of tokens), 2M code files (7B tokens), 8M pieces of reference material (7B tokens), 2M knowledge-base entries (2B tokens), filtered CommonCrawl (1B tokens), prompts (0.4B tokens), plus multimodal sequences (SMILES, proteins, DNA). Majority academic text, LaTeX processed where available; vocabulary 50k BPE tokens.",
            "topic_or_query_specification": "Natural language prompts and task-specific prompt formats included in pre-training; special tokens indicate requested behavior (e.g. prompt ending with [START_REF] to request a citation, wrapping reasoning with &lt;work&gt;, giving SMILES inside [START_SMILES] to request IUPAC name). Zero-shot prompting is the primary evaluation mode, with optional few-shot style prompts not required.",
            "distillation_method": "End-to-end self-supervised pretraining on the curated corpus combined with prompt-pretraining (including ~783k prompt examples, 358M prompt tokens); special-token engineering (citation tokens, &lt;work&gt;, modality wrappers) to let the model internalize document structure and multi-step reasoning; no external retrieval augmentation in main system (weights-only approach).",
            "output_type_and_format": "Free-text scientific outputs including: LaTeX equations, literature summaries and recommendations, predicted citations (paper titles), step-by-step reasoning sequences wrapped in &lt;work&gt;, QA answers, IUPAC names from SMILES, protein annotations, chemical reaction products; outputs can be plain text, LaTeX, or wrapped multimodal sequences depending on prompt.",
            "evaluation_or_validation_method": "Extensive benchmarking: knowledge probes (LaTeX equations dataset of 434 equations, domain probes like AminoProbe, BioLAMA, Chemical Reactions), reasoning benchmarks (MMLU mathematics, MATH), downstream scientific QA (PubMedQA, MedMCQA, BioASQ, MedQA-USMLE), citation-prediction datasets (PapersWithCode concept pairs PWC 644, Extended 110, Contextual 1869), IUPAC naming validation (17,052 compounds validated via OPSIN canonicalization), and comparisons to retrieval baselines and other LLMs (GPT-3, OPT, BLOOM, Chinchilla, PaLM, Gopher).",
            "results_summary": "Galactica 120B achieves state-of-the-art results on many scientific tasks reported in the paper: LaTeX equations 68.2% (vs GPT-3 49.0%), LaTeX memorization scales smoothly. MMLU (mathematics) with &lt;work&gt; token: 41.3% vs Chinchilla 35.7%. MATH: 120B 20.4% vs PaLM 8.8% (reported). Downstream QA: PubMedQA 77.6%, MedMCQA dev 52.9%, BioASQ ~94.3% (reported). Citation prediction (120B): PWC 51.9%, Extended 69.1%, Contextual 36.6%, outperforming sparse and dense retrieval baselines on these tasks. IUPAC naming (120B): 39.2% accuracy on a 17k compound validation set. Repeated-epochs training (4x) improved validation loss and downstream performance for curated corpus.",
            "limitations_or_challenges": "Hallucination risks (title-based citation identifiers prone to hallucination at lower scales); model stores knowledge in weights which can blend facts and is harder to update; limited molecule subset in pretraining (2M of ~110M PubChem compounds) constrains chemical generalization; working-memory (&lt;work&gt;) prompt datasets are small and gains may be limited by dataset diversity; absence of retrieval augmentation means extremely fine-grained facts may still require retrieval; model exhibits bias at smaller scales toward predicting more popular papers (distributional skew), though this diminishes with scale; potential overfitting after ~4 epochs for largest model.",
            "comparison_to_baselines_or_humans": "Galactica outperforms general LLMs (GPT-3, BLOOM, OPT) on many scientific benchmarks and outperforms Chinchilla on several reasoning tasks when using &lt;work&gt;. It outperforms tuned sparse/dense retrieval baselines on citation prediction tasks at large scale. Some fine-grained or high-school-level tasks are still stronger in Chinchilla (per paper), and Galactica is not universally superior across all general NLP tasks though it performs well on BIG-bench subset.",
            "uuid": "e3883.0",
            "source_info": {
                "paper_title": "Galactica: A Large Language Model for Science",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Galactica Citation Prediction",
            "name_full": "Galactica citation prediction using title-processed citations and [START_REF]/[END_REF] tokens",
            "brief_description": "A component of Galactica that predicts the appropriate citation (title-based identifier) given the surrounding document context, trained via special citation tokens and evaluated against curated citation datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Galactica citation prediction (title-identifier + [START_REF] token)",
            "system_or_method_description": "Galactica processes citations with title-based identifiers wrapped in [START_REF]/[END_REF] tokens during training so that, given a context (text before a citation), the model can generate the citation title; evaluated as a generation task and compared with sparse (ElasticSearch) and dense retriever baselines (Contriever variants).",
            "input_corpus_description": "Contexts extracted from the arXiv validation set and other curated documents; training included many citation instances from 48M papers in the corpus; three evaluation sets: PWC Citations (644 concept-&gt;paper pairs), Extended Citations (110 pairs), Contextual Citations (1,869 in-context citation prompts).",
            "topic_or_query_specification": "Prompt is the document context ending where a citation would occur (e.g. '... [START_REF]'), or short concept sentences like 'In this paper we use ResNet method [START_REF]'. The model must generate the paper title that corresponds to the concept/context.",
            "distillation_method": "Supervised/self-supervised generation over citation-annotated text during pretraining (title-based identifiers); no external retrieval used for the main system. Ablations considered title vs alphanumeric id identifiers and found title-based identifiers gave greater citation prediction accuracy but higher hallucination risk at low scales.",
            "output_type_and_format": "Generated paper title(s) as plain text between [START_REF] and [END_REF]; can be post-processed to match canonical identifiers if available.",
            "evaluation_or_validation_method": "Measured top-1 generation accuracy on PWC, Extended, and Contextual citation datasets; distributional comparison using Kolmogorov-Smirnov distance between counts of predicted vs ground-truth referenced papers to assess bias toward popular papers. Baselines: ElasticSearch sparse retrieval, Contriever dense retrievers (base and fine-tuned) with FAISS indexing.",
            "results_summary": "Accuracy scales with model size: GAL 120B achieved 51.9% (PWC), 69.1% (Extended), 36.6% (Contextual). These results outperform sparse retriever (30.9%, 17.3%, 5.3% respectively) and dense retriever baselines (best fine-tuned dense: 27.6%, 11.8%, 8.2%). KS distance between predicted and ground-truth citation-count distributions declined with model scale, indicating reduced popularity bias.",
            "limitations_or_challenges": "Title-based identifiers can increase hallucination risk especially at lower scales; model predictions may be biased toward popular papers at small sizes; reliance on weights-only storage can limit recall of very fine-grained or newly published works; comparison depends on prompt format and on having titles in training data; no external index means updates require re-training.",
            "comparison_to_baselines_or_humans": "At large scale Galactica exceeds standard sparse and dense retrieval baselines on the curated citation-prediction tasks; no direct human-comparison scores reported, but empirical distributional metrics indicate improved fidelity with scale.",
            "uuid": "e3883.1",
            "source_info": {
                "paper_title": "Galactica: A Large Language Model for Science",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Prompt Pre‑training (Galactica)",
            "name_full": "Prompt pre-training (including many task prompts in self-supervised pretraining)",
            "brief_description": "A training strategy used in Galactica that injects diverse zero-shot/format prompts into pretraining so the model better composes and generalizes to downstream tasks without task-specific fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Prompt pre-training (as used in Galactica)",
            "system_or_method_description": "Rather than only pretraining on raw corpora, the approach augments the pretraining mix with prompt-formatted tasks (QA, summarization, reasoning, chemical properties, etc.) so the model sees many task contexts during self-supervised learning and learns to respond to natural language task requests.",
            "input_corpus_description": "Included ~783,599 prompt examples across task types totaling ~358 million tokens, collected from QA datasets, chemical property records, reasoning datasets (e.g. GSM8k transformed), summarization, entity extraction and other tasks. These were mixed with the general 106B-token scientific corpus during pretraining.",
            "topic_or_query_specification": "Task queries are natural-language prompts (zero-shot style) of many diverse formats included during pretraining; diversity aimed to reduce need for few-shot examples at inference.",
            "distillation_method": "Self-supervised learning treating prompts as ordinary training tokens so the model internalizes mapping from prompt form to task output; this is different from post-hoc instruction tuning or fine-tuning.",
            "output_type_and_format": "Model outputs aligned to prompt formats: answers to QA prompts, summaries, extracted entities, reasoning traces wrapped in &lt;work&gt;, etc.",
            "evaluation_or_validation_method": "Measured by downstream benchmark performance (MMLU, BIG-bench subset, scientific QA tasks) and by in-domain vs out-of-domain performance splits to identify potential data leakage; compared to instruction-tuned and few-shot approaches.",
            "results_summary": "Prompt pre-training helped Galactica achieve strong zero-shot and few-shot performance on many scientific tasks, enabling smaller models to perform competitively with larger few-shot models. Inclusion of prompts is claimed to reduce the need for extremely large token counts or enormous model sizes for some tasks.",
            "limitations_or_challenges": "Prompt pre-training is not a substitute for instruction tuning; including prompts in pretraining risks some in-domain data leakage for specific benchmark tasks and requires care distinguishing in-domain vs out-of-domain evaluation; the set of prompt datasets used by Galactica is modest in size relative to the corpus and may limit coverage.",
            "comparison_to_baselines_or_humans": "Authors compare conceptually to ExT5, FLAN, T0 and UnifiedQA style approaches, noting prompt-pretraining has weaker task bias than instruction tuning but better out-of-the-box performance than pure unsupervised pretraining; concrete numeric head-to-heads are reported via downstream benchmarks where Galactica performs strongly.",
            "uuid": "e3883.2",
            "source_info": {
                "paper_title": "Galactica: A Large Language Model for Science",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Retrieval‑Augmented Models (RAG / RETRO / Atlas)",
            "name_full": "Retrieval-augmented generation approaches (examples: RAG, RETRO, Atlas) mentioned as related work",
            "brief_description": "Class of systems that combine a retrieval component (sparse or dense) with a neural generator so that external documents are retrieved and fed to the model during generation to provide fine-grained, up-to-date facts and reduce reliance on stored weights.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Retrieval-augmented generation (RAG / RETRO / Atlas - related work)",
            "system_or_method_description": "These systems retrieve relevant documents from an external index (sparse or dense), and condition the language model on the retrieved passages to generate answers or summaries; they are presented in the paper as complementary approaches to weights-only models for knowledge-intensive tasks.",
            "input_corpus_description": "External retrieval index constructed from documents (papers, abstracts, passages); size varies by implementation (can scale to billions/trillions of tokens in RETRO-style systems); not the primary approach used in Galactica experiments but discussed as an important alternative.",
            "topic_or_query_specification": "Query is a natural language prompt or context; retriever returns candidate documents given the query; generator distills or synthesizes output conditioned on retrieved text.",
            "distillation_method": "Two-stage: retrieval (sparse/dense) followed by conditional generation by an LLM; retrieval can be fine-tuned (dense retrievers) to map contexts to relevant papers.",
            "output_type_and_format": "Generated summaries, answers, extracted facts, or synthesized knowledge that cite or reference retrieved documents; output may also include extracted structured knowledge.",
            "evaluation_or_validation_method": "Not evaluated experimentally in this paper but cited literature typically evaluates via QA benchmarks, retrieval and generation metrics, and human evaluation of factuality and relevance.",
            "results_summary": "Paper notes retrieval-augmented models require less model capacity for similar factual accuracy but need supporting retrieval infrastructure; Galactica focuses on weights-only and shows some tasks (citation prediction) where a large weights-only model outperforms tuned sparse and dense retrieval baselines on curated citation datasets.",
            "limitations_or_challenges": "Require external infrastructure (indexing, retrieval latency, storage); retrieval quality depends on index coverage and tuning; integration with generative models raises concerns of provenance, citation and hallucination; authors note retrieval will likely be needed for extremely fine-grained facts even with larger models.",
            "comparison_to_baselines_or_humans": "Comparisons in the paper show Galactica (weights-only) outperforms the sparse and dense retrieval baselines on the citation prediction tasks used in evaluation, but the authors acknowledge retrieval augmentation is likely beneficial for future work and fine-grained retrieval needs.",
            "uuid": "e3883.3",
            "source_info": {
                "paper_title": "Galactica: A Large Language Model for Science",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "LAMA-style Knowledge Probes",
            "name_full": "LAMA (Language Model Analysis) / knowledge-probing approach",
            "brief_description": "A family of probing methodologies (originating with Petroni et al., 2019) that query language models with cloze-style or targeted prompts to measure stored factual knowledge; Galactica uses LAMA-like probes adapted for scientific content (LaTeX equations, domain probes).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "LAMA-style knowledge probes (applied to scientific content)",
            "system_or_method_description": "Construct targeted prompts to test whether a model can recall specific facts (e.g. equation name -&gt; LaTeX, reactants -&gt; products, entity properties). In Galactica these probes were extended to LaTeX equation generation (434 equations) and domain-specific triples (AminoProbe, BioLAMA, Chemical Reactions, Galaxy Clusters, Minerals).",
            "input_corpus_description": "Probe datasets compiled by authors: 434 LaTeX equations across chemistry/physics/math/stats/econ; domain datasets (AminoProbe, BioLAMA, etc.); model training corpus is the Galactica scientific corpus.",
            "topic_or_query_specification": "Structured prompts such as 'The formula for X is:' or masked factual prompts to elicit the target fact or LaTeX expression; zero-shot evaluation primarily used.",
            "distillation_method": "No distillation per se — probes are diagnostic: present a prompt and evaluate generated token sequence against expected equation or fact (manual checking for LaTeX where multiple correct forms exist).",
            "output_type_and_format": "Generated factual text or LaTeX equations; outputs assessed for correctness (exact or judged acceptable by human reviewers in ambiguous cases).",
            "evaluation_or_validation_method": "Automatic exact-match where possible; manual evaluation for LaTeX due to multiple valid formulations; results compared across model sizes and versus baseline LLMs (GPT-3, OPT, BLOOM).",
            "results_summary": "Galactica shows steady scaling in probe performance; Galactica 120B achieved 68.2% overall on LaTeX equation probe (vs GPT-3 49.0%). Domain probes show smooth gains with scale (e.g., chemical reactions 43.1% at 120B).",
            "limitations_or_challenges": "Probes may require manual evaluation (LaTeX variability), and zero-shot prompt difficulty (e.g. BioLAMA) can depress measured performance; probes measure memorization/recall more than synthesis of novel theory; probing does not guarantee reliable generative reasoning across arbitrary prompts.",
            "comparison_to_baselines_or_humans": "Comparisons reported to GPT-3, OPT, BLOOM; Galactica outperforms these baselines on many of the scientific probe tasks at larger scales.",
            "uuid": "e3883.4",
            "source_info": {
                "paper_title": "Galactica: A Large Language Model for Science",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Scientific LMs (SciBERT / BioLM)",
            "name_full": "Specialized scientific language models (examples: SciBERT, BioLM)",
            "brief_description": "Prior work showing benefits of curated scientific corpora for domain-specialized pretraining; typically smaller models pre-trained on scientific text leading to improved performance on domain tasks compared to general LMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Scientific domain-specific LMs (SciBERT, BioLM, ScholarBERT referenced)",
            "system_or_method_description": "Models pre-trained on domain-specific corpora (biomedical, scientific articles) with the goal of improving downstream domain tasks; mentioned as related work motivating Galactica's curated-corpus approach.",
            "input_corpus_description": "Smaller curated corpora compared to general LLM corpora (examples: SciBERT trained on Semantic Scholar corpus, ScholarBERT with &gt;200B tokens but small model capacity), domain-specific datasets (biomedical papers, abstracts).",
            "topic_or_query_specification": "Standard NLP prompts and fine-tuning for domain tasks (NER, QA, classification); not primarily oriented to large-scale literature synthesis across many modalities.",
            "distillation_method": "Self-supervised pretraining on domain corpora; often followed by supervised fine-tuning on downstream tasks.",
            "output_type_and_format": "Domain-specific NLP outputs (answers, classifications, embeddings) rather than broad literature synthesis; used as baselines or motivation.",
            "evaluation_or_validation_method": "Standard domain benchmarks (biomedical QA, NER, etc.) showing gains from domain pretraining.",
            "results_summary": "Prior work demonstrates curated scientific corpora help domain tasks; authors cite these works as evidence that a curated, normative dataset can be beneficial for building a scientific LLM.",
            "limitations_or_challenges": "Previous models often smaller in capacity and scope; may lack multi-modality and large-scale composition abilities; prior corpora smaller (&lt;20B tokens for some), limiting certain capabilities.",
            "comparison_to_baselines_or_humans": "Compared conceptually against general LLMs; domain-specialized LMs outperform general models on some domain tasks, motivating Galactica's curated large corpus approach.",
            "uuid": "e3883.5",
            "source_info": {
                "paper_title": "Galactica: A Large Language Model for Science",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models as Knowledge Bases?",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
            "rating": 2
        },
        {
            "paper_title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "rating": 2
        },
        {
            "paper_title": "Finetuned Language Models Are Zero-Shot Learners",
            "rating": 1
        }
    ],
    "cost": 0.021481999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Galactica: A Large Language Model for Science</h1>
<p>Ross Taylor</p>
<p>Thomas Scialom</p>
<p>Andrew Poulton</p>
<h2>Marcin Kardas</h2>
<p>Anthony Hartshorn
Viktor Kerkez</p>
<h2>Guillem Cucurull</h2>
<p>Elvis Saravia</p>
<p>Robert Stojnic</p>
<h2>Abstract</h2>
<p>Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by $68.2 \%$ versus $49.0 \%$. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by $41.3 \%$ to $35.7 \%$, and PaLM 540B on MATH with a score of $20.4 \%$ versus $8.8 \%$. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of $77.6 \%$ and $52.9 \%$. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>The original promise of computing was to solve information overload in science. In his 1945 essay "As We May Think", Vannevar Bush observed how "publication has been extended far beyond our present ability to make real use of the record" (Bush, 1945). He proposed computers as a solution to manage the growing mountain of information. Licklider expanded on this with the vision of a symbiotic relationship between humans and machines. Computers would take care of routine tasks such as storage and retrieval, "preparing the way for insights and decisions in scientific thinking" (Licklider, 1960).
Computing has indeed revolutionized how research is conducted, but information overload remains an overwhelming problem (Bornmann and Mutz, 2014). In May 2022, an average of 516 papers per day were submitted to arXiv (arXiv, 2022). Beyond papers, scientific data is also growing much more quickly than our ability to process it (Marx, 2013). As of August 2022, the NCBI GenBank contained $1.49 \times 10^{12}$ nucleotide bases (GenBank, 2022). Given the volume of information, it is impossible for a single person to read all the papers in a given field; and it is likewise challenging to organize data on the underlying scientific phenomena.
Search engines are the current interface for accessing scientific knowledge following the Licklider paradigm. But they do not organize knowledge directly, and instead point to secondary layers such as Wikipedia,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>UniProt and PubChem Compound which organize literature and data. These resources require costly human contributions, for example writing a review of literature, an encyclopedia article or annotating a protein. Given this bottleneck, researchers continue to feel overwhelmed even with powerful search tools to hand.
In this paper, we argue for a better way through large language models. Unlike search engines, language models can potentially store, combine and reason about scientific knowledge. For example, a model trained on the literature could potentially find hidden connections between different research, find hidden gems, and bring these insights to the surface. It could synthesize knowledge by generating secondary content automatically: such as literature reviews, encyclopedia articles, lecture notes and more. And lastly, it could organize different modalities: linking papers with code, protein sequences with compounds, theories with LaTeX, and more. Our ultimate vision is a single neural network for powering scientific tasks. We believe this is will be the next interface for how humans access scientific knowledge, and we get started in this paper.</p>
<h1>1.1 Our Contribution</h1>
<p>We introduce a new large language model called Galactica (GAL) for automatically organizing science. Galactica is trained on a large and curated corpus of humanity's scientific knowledge. This includes over 48 million papers, textbooks and lecture notes, millions of compounds and proteins, scientific websites, encyclopedias and more. Unlike existing language models, which rely on an uncurated crawl-based paradigm, our corpus is high-quality and highly curated. We are able to train on it for multiple epochs without overfitting, where upstream and downstream performance improves with use of repeated tokens.
Dataset design is critical to our approach, which includes curating a high-quality dataset and engineering an interface to interact with the body of knowledge. All data is processed in a common markdown format to blend knowledge between sources. We also include task-specific datasets in pre-training to facilitate composition of this knowledge into new task contexts. For the interface, we use task-specific tokens to support different types of knowledge. We process citations with a special token, that allows a researcher to predict a citation given any input context. We wrap step-by-step reasoning in a special token, that mimicks an internal working memory. And lastly, we wrap modalities such as SMILES and protein sequences in special tokens, which allows a researcher to interface with them using natural language. With this interface and the body of scientific knowledge in the model, we achieve state-of-the-art results across many scientific tasks.
On reasoning tasks, Galactica beats existing language models on benchmarks such as MMLU and MATH (Hendrycks et al., 2020, 2021). With our reasoning token approach, we outperform Chinchilla on mathematical MMLU with an average score of $41.3 \%$ versus $35.7 \%$ (Hoffmann et al., 2022). Our 120B model achieves a score of $20.4 \%$ versus PaLM 540B's $8.8 \%$ on MATH (Chowdhery et al., 2022; Lewkowycz et al., 2022). The 30B model also beats PaLM 540B on this task with 18 times less parameters. We believe this adds another reasoning method to the deep learning toolkit, alongside the existing chain-of-thought approach that has been well explored recently (Wei et al., 2022; Suzgun et al., 2022).
We also find Galactica performs strongly in knowledge-intensive scientific tasks. We conduct detailed knowledge probes of Galactica's knowledge of equations, chemical reactions and other scientific knowledge. Galactica significantly exceeds the performance of general language models such as the latest GPT-3 in these tasks; on LaTeX equations, it achieves a score of $68.2 \%$ versus the latest GPT-3's $49.0 \%$ (Brown et al., 2020). Galactica also performs well in downstream scientific tasks, and we set a new state-of-the-art on several downstream tasks such as PubMedQA (77.6\%) and MedMCQA dev (52.9\%) (Jin et al., 2019; Pal et al., 2022).
We also demonstrate new capabilities with Galactica's interface. First, the capability of predicting citations improves smoothly with scale, and we also find the model becomes better at modelling the underlying distribution of citations: the empirical distribution function approaches the reference distribution with scale. Importantly, we find this approach outperforms tuned sparse and dense retrieval approaches for citation prediction. This, along other results, demonstrates the potential for language models to replace the Licklider paradigm, document storage and retrieval, with their context-associative power in weight memory.
In addition, Galactica can perform multi-modal tasks involving SMILES chemical formulas and protein sequences. We formulate drug discovery tasks as text prompts and show performance scales in a weakly supervised setup. We also demonstrate Galactica learns tasks such as IUPAC name prediction in a selfsupervised way, and does so by attending to interpretable properties such as functional groups. Lastly, Galactica can annotate protein sequences with natural language, including predicting functional keywords.
Galactica was used to help write this paper, including recommending missing citations, topics to discuss in the introduction and related work, recommending further work, and helping write the abstract and conclusion.</p>
<h1>2 Related Work</h1>
<p>Large Language Models (LLMs) LLMs have achieved breakthrough performance on NLP tasks in recent years. Models are trained with self-supervision on large, general corpuses and they perform well on hundreds of tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Black et al., 2022; Zhang et al., 2022; Chowdhery et al., 2022). This includes scientific knowledge tasks such as MMLU (Hendrycks et al., 2020). They have the capability to learn in-context through few-shot learning (Brown et al., 2020). The capability set increases with scale, and recent work has highlighted reasoning capabilities at larger scales with a suitable prompting strategy (Wei et al., 2022; Chowdhery et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022).
One downside of self-supervision has been the move towards uncurated data. Models may mirror misinformation, stereotypes and bias in the corpus (Sheng et al., 2019; Kurita et al., 2019; Dev et al., 2019; Blodgett et al., 2020; Sheng et al., 2021). This is undesirable for scientific tasks which value truth. Uncurated data also means more tokens with limited transfer value for the target use-case; wasting compute budget. For example, the PaLM corpus is $50 \%$ social media conversations, which may have limited transfer towards scientific tasks (Chowdhery et al., 2022). The properties of scientific text also differ from general text - e.g. scientific terms and mathematics - meaning a general corpus and tokenizer may be inefficient. We explore whether a normative approach to dataset selection can work with the large model paradigm in this work.</p>
<p>Scientific Language Models Works such as SciBERT, BioLM and others have shown the benefit of a curated, scientific corpus (Beltagy et al., 2019; Lewis et al., 2020a; Gu et al., 2020; Lo et al., 2019b; Gu et al., 2020; Shin et al., 2020; Hong et al., 2022). The datasets and models were typically small in scale and scope, much less than corpora for general models ${ }^{2}$. Beyond scientific text, Transformers for protein sequences and SMILES have shown potential for learning natural representations (Rives et al., 2021; Honda et al., 2019; Irwin et al., 2021; Nijkamp et al., 2022; Lin et al., 2022b). However, sequences like SMILES have descriptive limitations for representing chemical structure. We explore in this work whether a large, multi-modal scientific corpus can aid representation learning, where sequences occur alongside footprints and text in a signal-dense context.</p>
<p>Scaling Laws The idea of "scaling laws" was put forward by Kaplan et al. (2020), who demonstrated evidence that loss scales as a power-law with model size, dataset size, and the amount of training compute. The focus was on upstream perplexity, and work by Tay et al. (2022a) showed that this does not always correlate with downstream performance. Hoffmann et al. (2022) presented new analysis taking into account the optimal amount of data, and suggested that existing language models were undertrained: "Chinchilla scaling laws". This work did not take into the account of fresh versus repeated tokens. In this work, we show that we can improve upstream and downstream performance by training on repeated tokens.</p>
<p>Language Models as Knowledge Bases Storing information in weights is more unreliable in the sense models may blend information together, hallucination, but it is more "pliable" in the sense it can associate information through the representation space, association. Despite hallucination risks, there is evidence large language models can act as implicit knowledge bases with sufficient capacity (Petroni et al., 2019). They perform well on knowledge-intensive tasks such as general knowledge (TriviaQA) and specialist knowledge (MMLU) without an external retrieval mechanism (Brown et al., 2020; Hendrycks et al., 2020).
The question of how to update network knowledge remains an active research question (Scialom et al., 2022; Mitchell et al., 2022). Likewise, the question of how to improve the reliability of generation is an active question (Gao et al., 2022). Despite these limitations, today's large models will become cheaper with experience (Hirschmann, 1964), and so a growing proportion of scientific knowledge will enter weight memory as training and re-training costs fall. In this work we perform probes to investigate Galactica's depth of knowledge, and show that the ability to absorb scientific knowledge improves smoothly with scale.</p>
<p>Retrieval-Augmented Models Retrieval-augmented models aim to alleviate the shortcomings of weight memory. Examples of such models include RAG, RETRO and Atlas (Lewis et al., 2020b; Borgeaud et al., 2021; Izacard et al., 2022). These models have the advantage of requiring less capacity but the disadvantage of needing supporting retrieval infrastructure. Since knowledge is often fine-grained, e.g. the sequence of a particular protein, or the characteristics of a particular exoplanet, retrieval will likely be needed in future even for larger models. In this work we focus on how far we can go with model weights alone, but we note the strong case for using retrieval augmentation for future research on this topic.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Modality</th>
<th>Entity</th>
<th>Sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text</td>
<td>Abell 370</td>
<td>Abell 370 is a cluster...</td>
</tr>
<tr>
<td>$\mathrm{IA}_{\mathrm{E}} \mathrm{X}$</td>
<td>Schwarzschild radius</td>
<td>$\mathrm{r}_{-}{\mathrm{s}}=\backslash \operatorname{frac}{2 \mathrm{GM}}\left{\mathrm{c}^{-} 2\right}$</td>
</tr>
<tr>
<td>Code</td>
<td>Transformer</td>
<td>class Transformer(nn.Module)</td>
</tr>
<tr>
<td>SMILES</td>
<td>Glycine</td>
<td>$\mathrm{C}(\mathrm{C}(=0) 0) \mathrm{N}$</td>
</tr>
<tr>
<td>AA Sequence</td>
<td>Collagen $\alpha-1$ (II) chain</td>
<td>MIRLGAPQTL..</td>
</tr>
<tr>
<td>DNA Sequence</td>
<td>Human genome</td>
<td>CGGTACCCTC..</td>
</tr>
</tbody>
</table>
<p>Table 1: Tokenizing Nature. Galactica trains on text sequences that represent scientific phenomena.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Total dataset size $=106$ billion tokens</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Data source</td>
<td style="text-align: right;">Documents</td>
<td style="text-align: right;">Tokens</td>
<td style="text-align: center;">Token \%</td>
</tr>
<tr>
<td style="text-align: left;">Papers</td>
<td style="text-align: right;">48 million</td>
<td style="text-align: right;">88 billion</td>
<td style="text-align: center;">$83.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">2 million</td>
<td style="text-align: right;">7 billion</td>
<td style="text-align: center;">$6.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Reference Material</td>
<td style="text-align: right;">8 million</td>
<td style="text-align: right;">7 billion</td>
<td style="text-align: center;">$6.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Knowledge Bases</td>
<td style="text-align: right;">2 million</td>
<td style="text-align: right;">2 billion</td>
<td style="text-align: center;">$2.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Filtered CommonCrawl</td>
<td style="text-align: right;">0.9 million</td>
<td style="text-align: right;">1 billion</td>
<td style="text-align: center;">$1.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Prompts</td>
<td style="text-align: right;">1.3 million</td>
<td style="text-align: right;">0.4 billion</td>
<td style="text-align: center;">$0.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: right;">0.02 million</td>
<td style="text-align: right;">0.2 billion</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: The Galactica Corpus. A full breakdown of these sources is contained in the Appendix.</p>
<h1>3 Dataset</h1>
<p>"Nature is written in that great book which ever is before our eyes - I mean the universe but we cannot understand it if we do not first learn the language and grasp the symbols in which it is written."</p>
<p>Galileo Galilei, The Assayer
The idea that Nature can be understood in terms of an underlying language has a long history Galilei, 1623; Wigner, 1959; Wheeler, 1990). In recent years, deep learning has been used to represent Nature, such as proteins and molecules (Jumper et al., 2021; Ross et al., 2021). Amino acids are an alphabet in which the language of protein structure is written, while atoms and bonds are the language of molecules. At a higher level, we organize knowledge through natural language, and many works have trained on scientific text (Beltagy et al., 2019; Lewis et al., 2020a; Gu et al., 2020; Lo et al., 2019b). With Galactica, we train a single neural network on a large scientific corpus to learn the different languages of science.
Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ where we can capture it, and also include academic code to capture computational science. We highlight the corpus details in Table 1 and 2. Full details, including dataset components and filtering logic, are contained in the Appendix.</p>
<p>[START_AMINO]MIRLGAPQTLVLLTLLVAAVLRCQGQDVQEAGSCVQDGQRYNDKDVWKPEPCRICVCDTG... [END_AMINO]</p>
<h1>Summary</h1>
<p>Protein: Collagen alpha-1(II) chain
Gene: COL2A1
Organism: Homo sapiens (Human)
Status: evidence at protein level</p>
<h2>Function</h2>
<p>Type II collagen is specific for cartilaginous tissues. It is essential for the normal embryonic development of the skeleton, for linear growth and for the ability of cartilage to resist compressive forces. [START_REF]Nucleotide sequence of the full length cDNA encoding for human type II procollage, Lee[END_REF]...</p>
<h2>Features</h2>
<ul>
<li>Domain, 32-90, Cleavage; by procollagen N-endopeptidase</li>
<li>Site Cleavage, 181-182, Cleavage; by procollagen N-endopeptidase</li>
<li>Binding site, 1301, Ca2+
...</li>
</ul>
<p>Figure 1: Multi-Modal Data. A protein sequence occurs in a document context along with annotations, text and citations from UniProt. Full contents of the document are cut for clarity of exposition.</p>
<p>Notably the dataset is small and curated compared to other LLM corpuses, which are larger and uncurated. This is a key question of this work: can we make a working LLM based on a curated, normative paradigm? If true, we could make more purposefully-designed LLMs by having a clear understanding of what enters the corpus, similar to expert systems which had normative standards (Jackson, 1990).</p>
<h3>3.1 Tokenization</h3>
<p>Tokenization is an important part of dataset design given the different modalities present. For example, protein sequences are written in terms of amino acid residues, where character-based tokenization is appropriate. To achieve the goal of specialized tokenization, we utilize specialized tokens for different modalities:</p>
<ol>
<li>Citations: we wrap citations with special reference tokens [START_REF] and [END_REF].</li>
<li>Step-by-Step Reasoning: we wrap step-by-step reasoning with a working memory token <work>, mimicking an internal working memory context.</li>
<li>Mathematics: for mathematical content, with or without LaTeX, we split ASCII operations into individual characters. Parentheses are treated like digits. The rest of the operations allow for unsplit repetitions. Operation characters are !"#\$\%\&amp;'*+, -./:;&lt;=&gt;?\^_'| and parentheses are () [] {}.</li>
<li>Numbers: we split digits into individual tokens. For example 737612.62 -&gt; 7,3,7,6,1,2,.,6,2.</li>
<li>SMILES formula: we wrap sequences with [START_SMILES] and [END_SMILES] and apply characterbased tokenization. Similarly we use [START_I_SMILES] and [END_I_SMILES] where isomeric SMILES is denoted. For example, $C(C(=0) 0) N \rightarrow C,(, C,(,=, 0,), 0,), N$.</li>
<li>Amino acid sequences: we wrap sequences with [START_AMINO] and [END_AMINO] and apply character-based tokenization, treating each amino acid character as a single token. For example, MIRLGAPQTL -&gt; M,I,R,L,G,A,P,Q,T,L.</li>
<li>DNA sequences: we also apply a character-based tokenization, treating each nucleotide base as a token, where the start tokens are [START_DNA] and [END_DNA]. For example, CGGTACCCTC -&gt; C, G, G, T, A, C, C, C, T, C.</li>
</ol>
<p>We cover a few of the specialized token approaches below that do not have clear parallels in the literature, in particular the working memory and citation tokens.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Given a task like "What is the average of 43, 29, 51, 13?" a human can use internal or external working memory. In practice, they will use both symbiotically; meaning that working out that is written down in text is usually "missing" some steps performed internally.</p>
<h1>3.1.1 Working Memory Token, <work></h1>
<p>Transformer-based architectures lack an explicit working memory capability, which means a single-forward pass has limited efficacy. This is problematic for tasks that require multiple steps of computation. A current workaround is using a Transformer's output context as an external working memory to read from and write to. This is seen in recent work on chain-of-thought prompting Wei et al., 2022; Suzgun et al., 2022). In one sense this is intuitive, as humans also augment their limited working memory with scratchpads. In another sense, we would like models to refine their representations internally like humans; e.g. mental arithmetic.
There are two limitations with chain-of-thought. First, it relies on prompt discovery to find a prompt that elicits robust step-by-step reasoning; i.e. minimizes mistakes from doing too much in a single forward pass. Not only does this require finding a robust prompt that works in all cases, but it also often relies on few-shot examples which take up context space. What is worse, much of the step-by-step reasoning on the internet misses intermediate steps that a human has performed using internal memory. Humans do not write down every step they perform because it would lead to long and tedious answers. They write down the principal steps of reasoning, and do lower-level steps via internal working memory. This means there is "missing data" in written text, i.e. between written steps there are internal memory steps that are not explicitly stated.
Secondly, chain-of-thought prompting uses the neural network to perform tasks that it is arguably not best suited to doing; for example, arithmetic. Prior work has shown that accuracy on tasks like multiplication is proportional to term frequency Razeghi et al., 2022). Given that classical computers are specialized for tasks like arithmetic, one strategy is to offload these tasks from the neural network to external modules. For example, prior work has looked at the possibilities of external tool augmentation, such as calculators (Thoppilan et al., 2022). However, this requires a strategy to identify where the neural network should offload; and it may not be straightforward when combined with a discovered zero-shot prompt, especially where lower-level computation steps are not explicitly stated in writing.
Our solution is a working memory token we call <work>. We construct a few prompt datasets, see Table 3, that wrap step-by-by-step reasoning within <work> </work>. Some of these datasets were generated programmatically (OneSmallStep), by creating a problem template and sampling the variables, others were sourced online (Workout, Khan Problems), and others used existing datasets and transformed them into a <work> based context (GSM8k train). Where a computation is performed that a human could not do internally, we offload by writing and executing a Python script. An example is shown in Figure 3. Importantly, we do not have to turn this on, and the model can also predict the output from running a program. For our experiments, we did not find the need to turn Python offloading on, and leave this aspect to future work.
Longer term, an architecture change may be needed to support adaptive computation, so machines can have internal working memory on the lines of work such as adaptive computation time and PonderNet Graves, 2016; Banino et al., 2021). In this paper, we explore the <work> external working memory approach as a</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question: A needle 35 mm long rests on a water surface at $20^{\circ} \mathrm{C}$. What force over and above the needle's weight is required to lift the needle from contact with the water surface? $\sigma=0.0728 \mathrm{~m}$.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\sigma=0.0728 \mathrm{~N} / \mathrm{m}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\sigma=F / L$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$0.0728=F /(2 \times 0.035)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$F=0.0728(2 \times 0.035)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">calculate.py</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">f = 0.0728<em>(2</em>0.035)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">with open("output.txt", "w") as file: file.write(str(round(f, 5)))</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">"</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">«run: "calculate.py"&gt;</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">«read: "output.txt"»</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0.0051</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></work></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: $F=0.0051 \mathrm{~N}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 3: Model-Machine Symbiosis. We show an example answer with the <work> working memory token. It performs exact steps for rearranging the equation, and when it reaches a calculation that it cannot solve reliably in a forward-pass, it writes a program, which can then be offloaded to a classical computer.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data source</th>
<th style="text-align: center;">Split</th>
<th style="text-align: right;">Prompts</th>
<th style="text-align: right;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8k (Cobbe et al., 2021)</td>
<td style="text-align: center;">train</td>
<td style="text-align: right;">7,473</td>
<td style="text-align: right;">$3,518,467$</td>
</tr>
<tr>
<td style="text-align: left;">OneSmallStep</td>
<td style="text-align: center;">$n / a$</td>
<td style="text-align: right;">9,314</td>
<td style="text-align: right;">$3,392,252$</td>
</tr>
<tr>
<td style="text-align: left;">Khan Problems (Hendrycks et al., 2021)</td>
<td style="text-align: center;">$n / a$</td>
<td style="text-align: right;">3,835</td>
<td style="text-align: right;">$1,502,644$</td>
</tr>
<tr>
<td style="text-align: left;">Workout</td>
<td style="text-align: center;">$n / a$</td>
<td style="text-align: right;">921</td>
<td style="text-align: right;">470,921</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;"></td>
<td style="text-align: right;">21,543</td>
<td style="text-align: right;">9 million</td>
</tr>
</tbody>
</table>
<p>Table 3: Reasoning Datasets To train the model to use <work> we include several datasets in pre-training that incorporate this token. Full details are contained in the Appendix.
bridge to the next step. Notably our <work> prompt datasets are not very large or diverse, so there are likely large further gains to be made with this approach.</p>
<h1>3.1.2 Citation Token</h1>
<p>A distinctive properties of academic text is citations. In order to represent the implicit citation graph within the text, we process citations with global identifiers and special tokens [START_REF] and [END_REF] signifying when a citation is made. Figure 4 shows an example of citation processed text from a paper.</p>
<p>Recurrent neural networks, long short-term memory [START_REF]Long Short-Term Memory, Hochreiter[END_REF] and gated recurrent [START_REF]Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung[END_REF] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [START_REF]Sequence to Sequence Learning with Neural Networks, Sutskever[END_REF] [START_REF]Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau[END_REF] [START_REF]Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation, Cho[END_REF].</p>
<p>Figure 4: Citation Processed Text. Example of citation processed text from Attention Is All You Need (Vaswani et al., 2017). For title-processed citations, the title can be associated with the previous context.</p>
<p>We considered two type of citation identifier: (a) paper titles and (b) alphanumeric IDs. Based on ablations, we found that title based identifiers have greater citation prediction accuracy than IDs. However, we also found that paper titles are more prone to hallucination error at lower scales given the text-based nature of the identifier. We consider title processing for this paper, but we note the trade-offs between both approaches. Experiments for these ablations are contained in the Appendix.</p>
<h3>3.2 Prompt Pre-Training</h3>
<p>We deviate from existing language model research in one important direction, which is our decision to include prompts in pre-training alongside the general corpora. This is motivated by a number of observations.
First, existing work has shown the importance of training token count on performance. The Chinchilla paper derived scaling "laws" taking into account number of tokens, training a 70bn model for 1.4 trillion tokens (Hoffmann et al., 2022). They obtained state-of-the-art performance on MMLU, beating much larger models such as Gopher (Rae et al., 2021).
Separately, research such as FLAN and T0 showed prompt tuning can boost downstream performance (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022). Their strategy involved converting tasks to text prompts, using prompt diversity in how the tasks are posed, and then fine-tuning on these prompt datasets. For FLAN and T0, this approach boosts performance, beating larger models such as GPT-3 on many tasks.
And additionally there is the UnifiedQA approach (Khashabi et al., 2020). In this approach, a T5 model is fine-tuned on question answering datasets, and is shown to boost performance on out-of-domain question answering datasets (Raffel et al., 2020). The model outperforms GPT-3 on MMLU, a model 16 times larger.
The first stream of research above focuses on total training tokens as a way to boost performance; i.e. it is token agnostic. The second stream of research focuses on task-context tokens as a way to boost performance; i.e. it is token selective. Since fine-tuned smaller models beat larger few-shot models on tasks like MMLU, this suggests world knowledge may be present in smaller models, but task-context knowledge may be poor given the relative number of task-context tokens seen in the general corpus.
For this paper, we opt to augment pre-training data with more task prompts to boost performance at lower scales. This is advantageous if it obviates the need for more data scale, e.g. a $&gt;1$ trillion corpus, or more model scale. The largest 120B model we train runs on a single NVIDIA A100 node. Additionally, given that fine-tuning requires expertise, making the model work out-the-box for popular tasks like question answering and summarization is more useful for users of the model. Lastly, by including prompts alongside general data, we maximize the generality of the model while boosting performance on some tasks of interest.
The closest analog to this approach for large language models is ExT5 (Aribandi et al., 2021). We take a similar approach by taking many machine learning training datasets, converting them to a text format, with prompt diversity, and then including them alongside general corpora in our pre-training set. A summary of prompt types is given in Table 4; the full details of datasets and prompts used are covered in the Appendix.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Increasing task generality</p>
<p>Figure 5: Prompt Pre-training. Pre-training weighs all tokens equally as part of the self-supervised loss. This leads to a weak relative signal for tasks of interest, meaning model scale has to be large to work. Instruction tuning boosts performance post hoc, and can generalize to unseen tasks of interest, but it risks performance in tasks that are distant from instruction set tasks. Prompt pre-training has a weaker task of interest bias than instruction tuning but less risk of degrading overall task generality.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: right;">Prompts</th>
<th style="text-align: right;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chemical Properties</td>
<td style="text-align: right;">782,599</td>
<td style="text-align: right;">275 million</td>
</tr>
<tr>
<td style="text-align: left;">Multiple-Choice QA</td>
<td style="text-align: right;">256,886</td>
<td style="text-align: right;">31 million</td>
</tr>
<tr>
<td style="text-align: left;">Extractive QA</td>
<td style="text-align: right;">30,935</td>
<td style="text-align: right;">13 million</td>
</tr>
<tr>
<td style="text-align: left;">Summarization</td>
<td style="text-align: right;">6,339</td>
<td style="text-align: right;">11 million</td>
</tr>
<tr>
<td style="text-align: left;">Entity Extraction</td>
<td style="text-align: right;">156,007</td>
<td style="text-align: right;">9 million</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning</td>
<td style="text-align: right;">21,543</td>
<td style="text-align: right;">9 million</td>
</tr>
<tr>
<td style="text-align: left;">Dialog</td>
<td style="text-align: right;">18,930</td>
<td style="text-align: right;">5 million</td>
</tr>
<tr>
<td style="text-align: left;">Binary QA</td>
<td style="text-align: right;">36,334</td>
<td style="text-align: right;">4 million</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: right;">3,559</td>
<td style="text-align: right;">1 million</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">783,599</td>
<td style="text-align: right;">358 million</td>
</tr>
</tbody>
</table>
<p>Table 4: Pre-training Prompts. We include zero-shot prompts in pre-training to boost the task signal.</p>
<p>Because of prompt inclusion, it is important to distinguish between in-domain performance, where the training dataset is included in pre-training, and out-of-domain performance, where the training dataset is not included in pre-training. We mark these results clearly in the Results section of this paper. Importantly, we do not advocate for prompt pre-training as an alternative to instruction tuning. In fact, instruction tuning on Galactica is likely useful follow-up work given its potential to boost performance on several tasks of interest.</p>
<h1>4 Method</h1>
<h3>4.1 Architecture</h3>
<p>Galactica uses a Transformer architecture in a decoder-only setup (Vaswani et al., 2017), with the following modifications:</p>
<ul>
<li>GeLU Activation - we use GeLU activations for all model sizes (Hendrycks and Gimpel, 2016).</li>
<li>Context Window - we use a 2048 length context window for all model sizes.</li>
<li>No Biases - following PaLM, we do not use biases in any of the dense kernels or layer norms (Chowdhery et al., 2022).</li>
<li>Learned Positional Embeddings - we use learned positional embeddings for the model. We experimented with ALiBi at smaller scales but did not observe large gains, so we did not use it (Press et al., 2021).</li>
<li>Vocabulary - we construct a vocabulary of 50k tokens using BPE (Sennrich et al., 2015). The vocabulary was generated from a randomly selected $2 \%$ subset of the training data.</li>
</ul>
<h3>4.2 Models</h3>
<p>The different model sizes we trained, along with training hyperparameters are outlined in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$n_{\text {params }}$</th>
<th style="text-align: center;">$n_{\text {layers }}$</th>
<th style="text-align: center;">$d_{\text {model }}$</th>
<th style="text-align: center;">$n_{\text {heads }}$</th>
<th style="text-align: center;">$d_{\text {heads }}$</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Max LR</th>
<th style="text-align: center;">Warmup</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.5 M</td>
<td style="text-align: center;">$6 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: center;">1.3 B</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2,048</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">1.0 M</td>
<td style="text-align: center;">$2 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: center;">6.7 B</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">4,096</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2.0 M</td>
<td style="text-align: center;">$1.2 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: center;">30.0 B</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">7,168</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2.0 M</td>
<td style="text-align: center;">$1 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: center;">120.0 B</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">10,240</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2.0 M</td>
<td style="text-align: center;">$0.7 \times 10^{-5}$</td>
<td style="text-align: center;">1.125 B</td>
</tr>
</tbody>
</table>
<p>Table 5: Details of the models trained</p>
<p>We train using AdamW with $\beta_{1}=0.9, \beta_{2}=0.95$ and weight decay of 0.1 (Loshchilov and Hutter, 2017). We clip the global norm of the gradient at 1.0 , and we use linear decay for learning rate down to $10 \%$ of it value. We use dropout and attention dropout of $p=0.1$. We do not use embedding dropout. We found longer warmup was important for the largest model in the early stages of training to protect against the effects of bad initialization, which can have long-memory effects on the optimizer variance state and slow down learning. This may be specific to our model and training setup, and it is not clear whether this advice generalizes.</p>
<h3>4.3 Libraries and Infrastructure</h3>
<p>We use the metaseq library ${ }^{3}$ for training the models, built by the NextSys team at Meta AI.
For training the largest 120B model, we use 128 NVIDIA A100 80GB nodes. For inference Galactica 120B requires a single A100 node. We choose the maximum model size to obey this constraint for downstream accessibility, and we will work to improve its accessibility for the research community in coming months.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: Repeated Tokens and Validation Loss. With four epochs of training, we continue to see validation loss fall for all model sizes. For the 120B model we see the first signs of overfitting at the beginning of the fifth epoch, and we early stop at this point.</p>
<h1>5 Results</h1>
<h3>5.1 Repeated Tokens Considered Not Harmful</h3>
<p>We train the models for 450 billion tokens, or approximately 4.25 epochs. We find that performance continues to improve on validation set, in-domain and out-of-domain benchmarks with multiple repeats of the corpus.
First, from Figure 6, validation loss continues to fall with four epochs of training. The largest 120B model only begins to overfit at the start of the fifth epoch. This is unexpected as existing research suggests repeated tokens can be harmful on performance (Hernandez et al., 2022). We also find the 30B and 120B exhibit a epoch-wise double descent effect of plateauing (or rising) validation loss followed by a decline. This effect becomes stronger with each epoch, and is most visible above with the 120B model towards end of training.
To investigate further, we examine the per-source breakdown of validation loss to see if there is heterogeneity in loss behaviour. We plot example curves in Figure 23 overleaf for the 30B model. We see no signs of loss heterogeneity: loss falls for all sources. The 120B exhibits the same relative trend of declining validation loss for all sources until the beginning of fifth epoch, where all sources spike (see Appendix).
The next question to answer is whether this trend extends to downstream performance and out-of-domain generalization. For this we use a 57 task subset of BIG-bench subset, a general corpus with principally nonscientific tasks and prompt types not included in pre-training (Srivastava et al., 2022). We plot results in Figure 8. We see no signs of overfitting suggesting that use of repeated tokens is improving downstream performance as well as upstream performance.
We suspect that two factors could be at play, a quality factor, the curated nature of the corpus enables more value per token to be extracted, or a modality factor, the nature of scientific data enables more value per token to be extracted. The missing step of causation is what leads specifically from either factor towards less overfitting, and we leave this question to further work. We note the implication that the "tokens $\rightarrow \infty$ " focus of current LLM projects may be overemphasised versus the importance of filtering the corpus for quality.
In the following sections, we turn to evaluating Galactica's scientific capabilities. Specifically, we focus on the high-level design goals of building an LLM that can store, combine and reason about scientific knowledge as these are needed for building a new interface for science.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Validation Loss Per Source. Validation loss falls through training for all dataset categories. Results are shown for the 30B model above. The 120B exhibits the same relative trend of declining validation loss for all sources until the beginning of fifth epoch, where all sources spike (see Appendix).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: BIG-bench Performance During Training. The 57 task selection from BIG-bench contains principally non-scientific tasks. We use it as a proxy for out-of-domain performance. For the 120B model above, we see no signs of overfitting after four repeats of the corpus.</p>
<h1>5.2 Knowledge Probes</h1>
<p>First, we examine how well Galactica absorbs scientific knowledge. We set up several knowledge probe benchmarks, building off the LAMA approach of petroni2019. These were critical metrics during model development for identifying knowledge gaps within the corpus, and informing how to iterate the corpus. They also provide insight into the relative knowledge strengths of Galactica versus general language models, and we cover these results in this section before turning to the downstream tasks.</p>
<h3>5.2.1 LaTeX Equations</h3>
<p>We construct a dataset of popular LaTeX equations from the fields of chemistry, physics, mathematics, statistics and economics. Memorisation of equations is useful to measure as it is necessary for many downstream tasks; for example, recalling an equation to use as part of an answer to a problem. Unless stated explicitly, Galactica results are reported as zero-shot. In total there are 434 equations we test for the knowledge probe.
We prompt with an equation name and generate LaTeX. An example is shown in Figure 9.</p>
<h2>Prompt</h2>
<p>The formula for Bessel's differential equation is:
Generated Answer</p>
<p>$$
x^{2} \frac{d^{2} y}{d x^{2}}+x \frac{d y}{d x}+\left(x^{2}-\alpha^{2}\right) y=0
$$</p>
<p>Figure 9: LaTeX Equations Probe. We prompt for the name of an equation and evaluate whether the generated LaTeX is correct. We manually evaluate given the possibility of multiple correct answers.</p>
<p>We summarize the results in Table 6. Equation knowledge increases smoothly with scale. Galactica outperforms larger language models trained on general corpuses, indicating the value of a curated dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Chemistry</th>
<th style="text-align: right;">Maths</th>
<th style="text-align: right;">Physics</th>
<th style="text-align: right;">Stats</th>
<th style="text-align: right;">Econ</th>
<th style="text-align: right;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OPT</td>
<td style="text-align: right;">175</td>
<td style="text-align: right;">$34.1 \%$</td>
<td style="text-align: right;">$4.5 \%$</td>
<td style="text-align: right;">$22.9 \%$</td>
<td style="text-align: right;">$1.0 \%$</td>
<td style="text-align: right;">$2.3 \%$</td>
<td style="text-align: right;">$8.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">$36.3 \%$</td>
<td style="text-align: right;">$36.1 \%$</td>
<td style="text-align: right;">$6.6 \%$</td>
<td style="text-align: right;">$14.1 \%$</td>
<td style="text-align: right;">$13.6 \%$</td>
<td style="text-align: right;">$21.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 (text-davinci-002)</td>
<td style="text-align: right;">$?$</td>
<td style="text-align: right;">$61.4 \%$</td>
<td style="text-align: right;">$65.4 \%$</td>
<td style="text-align: right;">$41.9 \%$</td>
<td style="text-align: right;">$25.3 \%$</td>
<td style="text-align: right;">$31.8 \%$</td>
<td style="text-align: right;">$49.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$0.8 \%$</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$1.0 \%$</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$31.8 \%$</td>
<td style="text-align: right;">$26.3 \%$</td>
<td style="text-align: right;">$23.8 \%$</td>
<td style="text-align: right;">$11.1 \%$</td>
<td style="text-align: right;">$4.6 \%$</td>
<td style="text-align: right;">$20.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$43.2 \%$</td>
<td style="text-align: right;">$59.4 \%$</td>
<td style="text-align: right;">$36.2 \%$</td>
<td style="text-align: right;">$29.3 \%$</td>
<td style="text-align: right;">$27.3 \%$</td>
<td style="text-align: right;">$41.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$63.6 \%$</td>
<td style="text-align: right;">$74.4 \%$</td>
<td style="text-align: right;">$35.2 \%$</td>
<td style="text-align: right;">$40.4 \%$</td>
<td style="text-align: right;">$34.1 \%$</td>
<td style="text-align: right;">$51.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{7 9 . 6 \%}$</td>
<td style="text-align: right;">$\mathbf{8 3 . 5 \%}$</td>
<td style="text-align: right;">$\mathbf{7 2 . 4 \%}$</td>
<td style="text-align: right;">$\mathbf{5 2 . 5 \%}$</td>
<td style="text-align: right;">$\mathbf{3 6 . 4 \%}$</td>
<td style="text-align: right;">$\mathbf{6 8 . 2 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on LaTeX equations. Results are evaluated zero-shot.</p>
<h3>5.2.2 Domain Probes</h3>
<p>We also set up domain probes to track specialized knowledge for certain fields. We detail these below:</p>
<ul>
<li>AminoProbe: a dataset of names, structures and properties of the 20 common amino acids.</li>
<li>BioLAMA: a dataset of biomedical factual knowledge triples.</li>
<li>Chemical Reactions: a dataset of chemical reactions.</li>
<li>Galaxy Clusters: a dataset of galaxy clusters with their constellation classifications.</li>
<li>Mineral Groups: a dataset of minerals and their mineral group classifications.</li>
</ul>
<p>In each case, we construct a prompt to test the knowledge. For example, for Chemical Reactions, we ask Galactica to predict the products of the reaction in the chemical equation LaTeX. We mask out products in the description so the model is inferring based on the reactants only. An example is shown in Figure 10.</p>
<h1>Prompt</h1>
<p>Sulfuric acid reacts with sodium chloride, and gives $\qquad$ and $\qquad$ :
[ \ce{ NaCl + H2SO4 -&gt;}
Generated Answer</p>
<p>$$
\mathrm{NaCl}+\mathrm{H}<em 4="4">{2} \mathrm{SO}</em>
$$} \longrightarrow \mathrm{NaHSO}_{4}+\mathrm{HCl</p>
<p>Figure 10: Chemical Reactions. We prompt based on a description and reactants, and evaluate whether the generated products are correct.</p>
<p>We report results for these knowledge probes in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Amino</th>
<th style="text-align: right;">BioLAMA</th>
<th style="text-align: right;">Reactions</th>
<th style="text-align: right;">Clusters</th>
<th style="text-align: right;">Minerals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OPT</td>
<td style="text-align: right;">175</td>
<td style="text-align: right;">$12.0 \%$</td>
<td style="text-align: right;">$7.1 \%$</td>
<td style="text-align: right;">$12.7 \%$</td>
<td style="text-align: right;">$21.7 \%$</td>
<td style="text-align: right;">$1.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">$14.0 \%$</td>
<td style="text-align: right;">$\mathbf{9 . 7 \%}$</td>
<td style="text-align: right;">$22.4 \%$</td>
<td style="text-align: right;">$15.0 \%$</td>
<td style="text-align: right;">$10.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 (text-davinci-002)</td>
<td style="text-align: right;">$?$</td>
<td style="text-align: right;">$14.0 \%$</td>
<td style="text-align: right;">$8.4 \%$</td>
<td style="text-align: right;">$35.1 \%$</td>
<td style="text-align: right;">$20.8 \%$</td>
<td style="text-align: right;">$18.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$12.0 \%$</td>
<td style="text-align: right;">$3.1 \%$</td>
<td style="text-align: right;">$0.3 \%$</td>
<td style="text-align: right;">$6.7 \%$</td>
<td style="text-align: right;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$16.0 \%$</td>
<td style="text-align: right;">$7.2 \%$</td>
<td style="text-align: right;">$14.4 \%$</td>
<td style="text-align: right;">$14.2 \%$</td>
<td style="text-align: right;">$10.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$17.0 \%$</td>
<td style="text-align: right;">$7.9 \%$</td>
<td style="text-align: right;">$26.4 \%$</td>
<td style="text-align: right;">$17.5 \%$</td>
<td style="text-align: right;">$8.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$21.0 \%$</td>
<td style="text-align: right;">$6.9 \%$</td>
<td style="text-align: right;">$36.5 \%$</td>
<td style="text-align: right;">$20.0 \%$</td>
<td style="text-align: right;">$17.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{2 1 . 0 \%}$</td>
<td style="text-align: right;">$8.0 \%$</td>
<td style="text-align: right;">$\mathbf{4 3 . 1 \%}$</td>
<td style="text-align: right;">$\mathbf{2 4 . 2 \%}$</td>
<td style="text-align: right;">$\mathbf{2 9 . 4 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Results on Domain Probes. Results are evaluated zero-shot.</p>
<p>We also observe steady scaling behaviour in these knowledge probes, with the exception of BioLAMA which we suspect reflects zero-shot prompt difficulty for all LLMs. Notably fine-grained factual knowledge, such as "ConstellationOf (GalaxyCluster)" type-queries seems to scale smoothly with the size of the model.</p>
<table>
<thead>
<tr>
<th>5.2.3 Reasoning</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>We now turn to reasoning capabilities with the <work> token. We start by evaluating on the MMLU mathematics benchmarks, which we report in Table 8 <em>(Hendrycks et al., 2020)</em>. Galactica performs strongly compared to larger base models, and use of the <work> token appears to boost performance over Chinchilla, even for the smaller 30B Galactica model.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mathematics MMLU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model</td>
<td>Params (bn)</td>
<td>A.Algebra</td>
<td>Elem</td>
<td>HS</td>
<td>College</td>
<td>F. Logic</td>
<td>Average</td>
<td></td>
</tr>
<tr>
<td>BLOOM (5-shot)</td>
<td>176</td>
<td>25.0%</td>
<td>26.7%</td>
<td>27.0%</td>
<td>25.0%</td>
<td>26.2%</td>
<td>26.4%</td>
<td></td>
</tr>
<tr>
<td>OPT (5-shot)</td>
<td>175</td>
<td>21.0%</td>
<td>25.7%</td>
<td>24.4%</td>
<td>33.0%</td>
<td>29.4%</td>
<td>26.7%</td>
<td></td>
</tr>
<tr>
<td>Gopher (5-shot)</td>
<td>280</td>
<td>25.0%</td>
<td>33.6%</td>
<td>23.7%</td>
<td>37.0%</td>
<td>35.7%</td>
<td>30.6%</td>
<td></td>
</tr>
<tr>
<td>Chinchilla (5-shot)</td>
<td>70</td>
<td>31.0%</td>
<td>41.5%</td>
<td>31.9%</td>
<td>32.0%</td>
<td>33.3%</td>
<td>35.7%</td>
<td></td>
</tr>
<tr>
<td>GAL 1.3B</td>
<td>1.3</td>
<td>28.0%</td>
<td>27.2%</td>
<td>26.7%</td>
<td>30.0%</td>
<td>24.6%</td>
<td>27.1%</td>
<td></td>
</tr>
<tr>
<td>GAL 6.7B</td>
<td>6.7</td>
<td>28.0%</td>
<td>28.9%</td>
<td>26.7%</td>
<td>36.0%</td>
<td>31.0%</td>
<td>29.2%</td>
<td></td>
</tr>
<tr>
<td>GAL 30B</td>
<td>30</td>
<td>30.0%</td>
<td>30.2%</td>
<td>26.3%</td>
<td>36.0%</td>
<td>31.7%</td>
<td>29.9%</td>
<td></td>
</tr>
<tr>
<td>GAL 120B</td>
<td>120</td>
<td>33.0%</td>
<td>38.1%</td>
<td>32.6%</td>
<td>43.0%</td>
<td>32.5%</td>
<td>35.8%</td>
<td></td>
</tr>
<tr>
<td>GAL 1.3B <work></td>
<td>1.3</td>
<td>22.0%</td>
<td>24.6%</td>
<td>18.9%</td>
<td>25.0%</td>
<td>31.0%</td>
<td>24.6%</td>
<td></td>
</tr>
<tr>
<td>GAL 6.7B <work></td>
<td>6.7</td>
<td>33.3%</td>
<td>30.7%</td>
<td>25.2%</td>
<td>26.0%</td>
<td>33.3%</td>
<td>28.0%</td>
<td></td>
</tr>
<tr>
<td>GAL 30B <work></td>
<td>30</td>
<td>33.0%</td>
<td>41.5%</td>
<td>33.3%</td>
<td>39.0%</td>
<td>37.3%</td>
<td>37.1%</td>
<td></td>
</tr>
<tr>
<td>GAL 120B <work></td>
<td>120</td>
<td>27.0%</td>
<td>54.2%</td>
<td>37.0%</td>
<td>44.0%</td>
<td>40.5%</td>
<td>41.3%</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 8: Results on Mathematics MMLU. Galactica is evaluated without few-shot examples. With the <work> token we see large gains in performance. Results are on MMLU test.</p>
<p>We also evaluate on the MATH dataset to further probe the reasoning capabilities of Galactica <em>(Hendrycks et al., 2021)</em>. We compare the <work> token prompt directly with the Minerva 5-shot chain-of-thought prompt mCoT for comparability. We report results in Table 9.</p>
<p>MATH Results | Model | Alg | CProb | Geom | I.Alg | N.Theory | Prealg | Precalc | Average | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---</p>
<h1>5.3 Downstream Scientific NLP</h1>
<p>We now evaluate on downstream scientific tasks to see how well Galactica can compose its knowledge in different task contexts. We focus on knowledge-intensive scientific tasks and report full results in Table 10. For this we use the MMLU benchmark as well as some other popular scientific QA benchmarks. We include the MMLU results earlier without <work> to test for knowledge association specifically. Full MMLU results, including social sciences and other fields, are reported in the Appendix. We also perform data leakage analysis on these benchmarks for more confidence; results are in the Appendix.
From Table 10, Galactica can compose its knowledge into the question-answering task, and performance is strong; significantly outperforming the other open language models, and outperforming a larger model (Gopher 280B) in the majority of tasks. Performance against Chinchilla is more variable, and Chinchilla appears to be stronger in a subset of tasks: in particular, high-school subjects and less-mathematical, more memorization intensive tasks. In contrast, Galactica tends to perform better in mathematical and graduatelevel tasks.
Our working hypothesis is that the Galactica corpus is biased towards graduate scientific knowledge, given it consists mostly of papers, which explains lagging performance in high-school subjects. While we do pick up some high-school level content through encyclopedias, textbooks and the filtered CommonCrawl, this amounts to a small quantity of tokens (a few billion). We leave the question of how to capture more of this base scientific knowledge in a curated way to future work.
On remaining tasks, we achieve state-of-the-art results over fine-tuned models at the time of writing. On PubMedQA, we achieve a score of $77.6 \%$ which outperforms the state-of-the-art of $72.2 \%$ (Yasunaga et al., 2022). On MedMCQA dev we achieve score of $52.9 \%$ versus the state-of-the-art of $41.0 \%$ (Gu et al., 2020). For BioASQ and MedQA-USMLE, performance is close to the state-of-the-art performance of fine-tuned models ( $94.8 \%$ and $44.6 \%$ ) (Yasunaga et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Domain</th>
<th style="text-align: center;">GAL</th>
<th style="text-align: center;">OPT</th>
<th style="text-align: center;">BLOOM</th>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;">Gopher</th>
<th style="text-align: center;">Chinchilla</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Abstract Algebra</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{3 3 . 3 \%}$</td>
<td style="text-align: center;">$21.0 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">$31.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ARC Challenge</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{6 7 . 9 \%}$</td>
<td style="text-align: center;">$31.1 \%$</td>
<td style="text-align: center;">$32.9 \%$</td>
<td style="text-align: center;">$51.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ARC Easy</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{8 3 . 8 \%}$</td>
<td style="text-align: center;">$37.4 \%$</td>
<td style="text-align: center;">$40.7 \%$</td>
<td style="text-align: center;">$68.8 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Astronomy</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$65.1 \%$</td>
<td style="text-align: center;">$23.0 \%$</td>
<td style="text-align: center;">$25.7 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$65.8 \%$</td>
<td style="text-align: center;">$\mathbf{7 3 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">BioASQ</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{9 4 . 3 \%}$</td>
<td style="text-align: center;">$81.4 \%$</td>
<td style="text-align: center;">$91.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Biology (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$68.8 \%$</td>
<td style="text-align: center;">$30.6 \%$</td>
<td style="text-align: center;">$28.5 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$70.8 \%$</td>
<td style="text-align: center;">$\mathbf{7 9 . 9 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Biology (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$69.4 \%$</td>
<td style="text-align: center;">$27.7 \%$</td>
<td style="text-align: center;">$29.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$71.3 \%$</td>
<td style="text-align: center;">$\mathbf{8 0 . 3 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$46.0 \%$</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$19.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$45.0 \%$</td>
<td style="text-align: center;">$\mathbf{5 1 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$47.8 \%$</td>
<td style="text-align: center;">$21.7 \%$</td>
<td style="text-align: center;">$23.2 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$47.8 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 1 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Comp. Science (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$49.0 \%$</td>
<td style="text-align: center;">$17.0 \%$</td>
<td style="text-align: center;">$6.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$49.0 \%$</td>
<td style="text-align: center;">$\mathbf{5 1 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Comp. Science (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{7 0 . 0 \%}$</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$54.0 \%$</td>
<td style="text-align: center;">$58.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Econometrics</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$42.1 \%$</td>
<td style="text-align: center;">$21.0 \%$</td>
<td style="text-align: center;">$23.7 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{4 3 . 0 \%}$</td>
<td style="text-align: center;">$38.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Electrical Engineering</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{6 2 . 8 \%}$</td>
<td style="text-align: center;">$36.6 \%$</td>
<td style="text-align: center;">$32.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$60.0 \%$</td>
<td style="text-align: center;">$62.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Elementary Mathematics</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$38.1 \%$</td>
<td style="text-align: center;">$25.7 \%$</td>
<td style="text-align: center;">$27.6 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$33.6 \%$</td>
<td style="text-align: center;">$\mathbf{4 1 . 5 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Formal Logic</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$32.5 \%$</td>
<td style="text-align: center;">$29.4 \%$</td>
<td style="text-align: center;">$26.2 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{3 5 . 7 \%}$</td>
<td style="text-align: center;">$33.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Machine Learning</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$38.4 \%$</td>
<td style="text-align: center;">$28.6 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$41.1 \%$</td>
<td style="text-align: center;">$41.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mathematics (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{4 3 . 0 \%}$</td>
<td style="text-align: center;">$33.0 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$37.0 \%$</td>
<td style="text-align: center;">$32.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mathematics (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{3 2 . 6 \%}$</td>
<td style="text-align: center;">$24.4 \%$</td>
<td style="text-align: center;">$27.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$23.7 \%$</td>
<td style="text-align: center;">$31.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Medical Genetics</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{7 0 . 0 \%}$</td>
<td style="text-align: center;">$35.0 \%$</td>
<td style="text-align: center;">$36.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$69.0 \%$</td>
<td style="text-align: center;">$69.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Physics (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$42.2 \%$</td>
<td style="text-align: center;">$21.6 \%$</td>
<td style="text-align: center;">$18.6 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$34.3 \%$</td>
<td style="text-align: center;">$\mathbf{4 6 . 1 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Physics (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$33.8 \%$</td>
<td style="text-align: center;">$29.8 \%$</td>
<td style="text-align: center;">$25.2 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$33.8 \%$</td>
<td style="text-align: center;">$\mathbf{3 6 . 4 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">MedQA-USMLE</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$44.4 \%$</td>
<td style="text-align: center;">$22.8 \%$</td>
<td style="text-align: center;">$23.3 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MedMCQA Dev</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{5 2 . 9 \%}$</td>
<td style="text-align: center;">$29.6 \%$</td>
<td style="text-align: center;">$32.5 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PubMedQA</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{7 7 . 6 \%}$</td>
<td style="text-align: center;">$70.2 \%$</td>
<td style="text-align: center;">$73.6 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Statistics (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$41.2 \%$</td>
<td style="text-align: center;">$43.5 \%$</td>
<td style="text-align: center;">$19.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 8 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 10: Question Answering Results. Galactica is evaluated without few-shot examples. Other LLMs are evaluated 5-shot, except for 0 -shot results for GPT-3 on ARC results and OPT and BLOOM on PubMedQA and BioASQ. For abstract algebra and medical genetics, we obtained best results with 30B, so we report these scores; the 120B scores for these were $27.0 \%$ and $68.0 \%$ respectively. Rest of results are for 120B.</p>
<h1>5.4 Citation Prediction</h1>
<p>In this section we evaluate Galactica's capability to predict citations given an input context, which is an important test of Galactica's capability to organize the scientific literature. We find that both accuracy and the quality of distributional approximation improves with scale.</p>
<h3>5.4.1 Citation Accuracy</h3>
<p>We construct three datasets to evaluate the model's capability to cite:</p>
<ul>
<li>PWC Citations: a dataset with 644 pairs of machine learning concepts and papers that introduced them. Concepts consist of methods (e.g. ResNet) and datasets (e.g. ImageNet) from Papers with Code ${ }^{4}$.</li>
<li>Extended Citations: a dataset with 110 pairs of non-machine learning concepts and papers that introduced them. Examples of concepts include Kozac sequence and Breit-Wigner distribution.</li>
<li>Contextual Citations: a dataset with 1,869 pairs of references and contexts from our arXiv validation set. The dataset is constructed by sampling 1,000 random references and collecting their contexts.</li>
</ul>
<p>For the PWC Citations and Extended Citations datasets, the citation prediction task is framed as a text generation task. The model is given a prompt like "In this paper we use ResNet method [START_REF]" in order to generate a prediction for the ResNet concept. For Contextual Citations, we prompt after the input context for the citation, where the context ends with [START_REF].
We compare Galactica to sparse and dense retrieval-based approaches on this task.
For the sparse baseline, we use ElasticSearch to create an index of all the references, including their titles, abstracts, and short snippets of text with the contexts they appear in. Then, given a text query, we retrieve the top references ordered by the sum of matching scores across all selected fields.
For dense retriever baselines, we evaluate two different Contriever models (Izacard et al., 2021). The first is the pre-trained model released by Izacard et al. (2021). The second model we use is fine-tuned on a random subset of 10 million context/paper pairs from our corpus, trained to retrieve the right paper given a context before a citation. The setup for dense retrieval is: (1) each reference is encoded by the model using its title and abstract, (2) a text query is encoded by the same model, (3) the references that match the query re returned. Retrieval is performed using a FAISS index (Johnson et al., 2019).
The results can be seen in Table 11.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">PWC Citations</th>
<th style="text-align: right;">Extended Citations</th>
<th style="text-align: right;">Contextual Citations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$7.0 \%$</td>
<td style="text-align: right;">$6.4 \%$</td>
<td style="text-align: right;">$7.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$18.5 \%$</td>
<td style="text-align: right;">$45.5 \%$</td>
<td style="text-align: right;">$15.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$32.0 \%$</td>
<td style="text-align: right;">$60.0 \%$</td>
<td style="text-align: right;">$23.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$44.7 \%$</td>
<td style="text-align: right;">$66.4 \%$</td>
<td style="text-align: right;">$31.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{5 1 . 9 \%}$</td>
<td style="text-align: right;">$\mathbf{6 9 . 1 \%}$</td>
<td style="text-align: right;">$\mathbf{3 6 . 6 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Sparse Retriever</td>
<td style="text-align: right;">$\mathrm{n} / \mathrm{a}$</td>
<td style="text-align: right;">$30.9 \%$</td>
<td style="text-align: right;">$17.3 \%$</td>
<td style="text-align: right;">$5.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Dense Retriever (base)</td>
<td style="text-align: right;">$\mathrm{n} / \mathrm{a}$</td>
<td style="text-align: right;">$16.4 \%$</td>
<td style="text-align: right;">$8.8 \%$</td>
<td style="text-align: right;">$1.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Dense Retriever (fine-tuned)</td>
<td style="text-align: right;">$\mathrm{n} / \mathrm{a}$</td>
<td style="text-align: right;">$27.6 \%$</td>
<td style="text-align: right;">$11.8 \%$</td>
<td style="text-align: right;">$8.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 11: Citation Prediction Accuracy. Performance of different model sizes on citation prediction.</p>
<p>The performance on all evaluation sets increases smoothly with scale. At larger scales, Galactica outperforms the retrieval-based approaches as its context-associative power improves. This is an important result as current approaches for navigating the literature use these existing retrieval approaches. As the power of language models improves, we suspect they will become a valuable new tool for exploring the literature.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 11: Distributional Comparison of Citations. Galactica's citation distribution approaches the ground truth with scale. This is seen through a declining KS distance with scale, and increasing histogram overlap.</p>
<h1>Prompt</h1>
<p>in the BQ literature as, when $p$ is a mixture of Gaussians, the mean element $\mu_{p}$ is analytically tractable (see Appendix C). Some other $(p, k)$ pairs that produce analytic mean elements are discussed in [[START_REF] On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions, Bach [START_REF] ].
For this simulation study, we took $p(x)$ to be a 20-component mixture of 2D-Gaussian distributions. Monte Carlo (MC) is often used for such distributions but has a slow convergence rate in $\mathcal{O}_{P}\left(n^{-1 / 2}\right)$. FW and FWLS are known to converge more quickly and are in this sense preferable to MC [[START_REF]</p>
<h2>Prediction</h2>
<p>On the Equivalence between Herding and Conditional Gradient Algorithms, Bach
Figure 12: Citation Prompt. An example prompt predicting a citation in-context; from Briol et al. (2015).</p>
<h3>5.4.2 Citation Distributional Analysis</h3>
<p>We now turn to look at how well Galactica can model the empirical citation distribution. For this analysis we use the Contextual Citations dataset, where prompts are extracted from a paper by taking the context before a citation as the prompt. An example prompt with a model prediction is shown overleaf in Figure 12.
We use the in-context citation data to analyse the distributional difference between predicted and ground truth paper counts. This allows us to assess the model bias towards predicting more popular papers. Specifically, for each context there is a ground truth and predicted reference. We count the number of times each reference appears in our corpus. We then compare the distribution of reference counts between the ground truth references and the predicted references using the Kolmogorov-Smirnov distance (Massey, 1951).
The comparison between the citation count distributions for different model sizes can be seen in Figure 11. Figure 11a shows the decrease in the Kolmogorov-Smirnov distance between the distribution of ground truth paper citations and the distribution of predicted papers citations. Figure 11b shows how the distribution of paper counts for the predicted papers gets closer to the ground truth as the model size grows. At smaller scales the model is more prone to predicting more popular papers. As the model grows in size this bias towards predicting popular papers diminishes.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.5 General Capabilities</h1>
<p>We have studied Galactica's scientific capabilities. It is perhaps not surprising that a specialist scientific model outperforms general models on scientific tasks, but what would be more surprising was if it outperformed general models on general NLP tasks. In this section, we show surprising evidence that it does just that.
We evaluate on 57 BIG-bench tasks in Table 12 (Srivastava et al., 2022). The tasks are primarily non-scientific and test general language capability, for example anachronisms, figure of speech and metaphor boolean. We always evaluate with 5 -shots, and we use the default prompt style from BIG-Bench. Importantly, we do not include this prompt style in pre-training; so the evaluation between Galactica and the other models is comparable 5-shot. Full details and results are in the Appendix. We summarize average scores in Table 12:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Accuracy <br> weighted</th>
<th style="text-align: right;">Accuracy <br> unweighted</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OPT 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$39.6 \%$</td>
<td style="text-align: right;">$38.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM 176B</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">$42.6 \%$</td>
<td style="text-align: right;">$42.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">OPT 175B</td>
<td style="text-align: right;">175</td>
<td style="text-align: right;">$43.4 \%$</td>
<td style="text-align: right;">$42.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$46.6 \%$</td>
<td style="text-align: right;">$42.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{4 8 . 7 \%}$</td>
<td style="text-align: right;">$\mathbf{4 5 . 3 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 12: BIG-bench 57 Task Results. Galactica outperforms general open models at smaller scales.</p>
<p>Both the 30B and 120B Galactica models outperform the larger OPT and BLOOM general models. This is a surprising result given we designed Galactica to trade-off generality for performance in scientific tasks.
We suspect this result reflects the higher-quality of the Galactica corpus, stemming from the fact it is curated and also primarily academic text. Previous open LLM efforts likely overfocused on scale goals and underfocused on data filtering. Another implication is that the focus on tokens $\rightarrow \infty$ from Chinchilla needs to be complemented with strong data quality procedures (Hoffmann et al., 2022). With this paper, we took an opposite approach by focusing on high-quality tokens and repeated epochs of training. However, the Chinchilla insight stands: and there is much more scientific text that we have not exploited in this work.</p>
<h3>5.6 Chemical Understanding</h3>
<p>We now turn to Galactica's capability to interface with different scientific modalities. We start by looking at Galactica's chemical capabilities. Chemical properties exhibit complex correlations which means the chemical space is very large. Better organization of chemical information through language models could aid chemical design and discovery. We explore how Galactica can provide a new interface for these tasks in this section.
For this work, we only include a small subset of available compounds from PubChem Compound in pretraining. Specifically, we take a random subset ( 2 million) of total compounds ( 110 million). This is to ensure the model is not overly biased towards learning natural sequences over natural language. This is a constraint we can relax in future work, enabling for much larger corpus. Here we focus on the first step of investigating whether a single model can learn effectively in the multi-modal setting.
We find that a language model can learn chemical tasks such as IUPAC naming in a self-supervised way, and in addition, we can pose drug discovery tasks as natural language prompts and achieve reasonable results.</p>
<h3>5.6.1 IUPAC Name Prediction</h3>
<p>SMILES is a line notation which represents chemical structure as a sequence of characters (Weininger, 1988). In the Galactica corpus, the SMILES formula occurs alongside information in the document, such as IUPAC names, molecular weight and XLogP. In the context of self-supervised learning, this means a language model is performing implicit multi-task learning: the model is predicting the next SMILES token, but can also use SMILES to predict other entities in the document.
As an initial test, we set up a IUPAC Name Prediction task, where the task is to name a compound according to the IUPAC nomenclature given a SMILES formula input. The IUPAC nomenclature is a method of naming organic compounds that has a ruleset based on naming the longest chain of carbons connected by single bonds (Favre and Powerll). There is a large set of rules and the procedure is algorithmically complex, meaning it is hard to automate. As a result, it is missing from standard cheminformatics toolkits.</p>
<p>Previous works such as STOUT and Struct2IUPAC have explored the possiblity of using RNNs and Transformers for this task (Rajan et al., 2021; Krasnov et al., 2021). We explore in this section whether Galactica can translate a SMILES specification to its IUPAC name in the self-supervised setting. We design a prompt based on the PubChem structure, with the SMILES as the only input, and the output to predict the IUPAC name.
To evaluate, we use our compound validation set of 17,052 compounds, and prompt with the SMILES formula and predict the IUPAC name. To calculate accuracy, we use OPSIN to convert the generated IUPAC name to SMILES, canonicalize it and compare with the canonicalized SMILES target (Lowe et al., 2011).
Results are shown in Table 13.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Accuracy</th>
<th style="text-align: right;">Invalid Names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$32.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$2.5 \%$</td>
<td style="text-align: right;">$12.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$10.7 \%$</td>
<td style="text-align: right;">$12.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$15.4 \%$</td>
<td style="text-align: right;">$9.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{3 9 . 2 \%}$</td>
<td style="text-align: right;">$\mathbf{9 . 2 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 13: Results on IUPAC Naming. Performance improves smoothly with scale.</p>
<p>Accuracy increases smoothly with scale. Given we restricted the corpus to 2 million molecules, it is likely much better performance is achievable through training or fine-tuning on more molecules. The model is freely available for those who want to perform this follow-up work.
The more immediate question is what is actually being learnt: is Galactica inferring names from the fundamental molecular structure? To answer this, we visualize the average atomic attention at each stage of a prediction in Figure 13 overleaf. Encouragingly, the results are interpretable in terms of the underlying chemistry, and Galactica attends to the correct group when predicting a name, e.g. for "amino" it attends primarily to the $-\mathrm{NH}_{2}$ substituent.</p>
<p>Task: Convert the SMILES to IUPAC Name
Example: $\mathrm{CC}(\mathrm{C})(\mathrm{C}) \mathrm{C}(=0) \mathrm{N}(\mathrm{CC} 1=\mathrm{NC}(=\mathrm{CS} 1) \mathrm{C}(=0) \mathrm{OC}) \mathrm{C} 2 \mathrm{CCCCC} 2$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Atomic Attention</th>
<th style="text-align: center;">Predicted So Far</th>
<th style="text-align: center;">Token Predicted</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><smiles>C1CCCCC2CCCCC2</smiles></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">methyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl</td>
<td style="text-align: center;">cyclohexyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-</td>
<td style="text-align: center;">dimethyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethyl</td>
<td style="text-align: center;">prop</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylprop</td>
<td style="text-align: center;">anoyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)</td>
<td style="text-align: center;">amino</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino] methyl]</td>
<td style="text-align: center;">th</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino] methyl]th</td>
<td style="text-align: center;">iazole</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino] methyl]thiazole-4-</td>
<td style="text-align: center;">carboxylate</td>
</tr>
</tbody>
</table>
<p>Figure 13: Attending to Functional Groups. Galactica uses its knowledge of chemistry to help with the IUPAC Naming task. At each stage of prediction, it attends to the part of the molecular graph associated with the group name, e.g. for "amino" it attends to the nitrogen atom; for thiazole, the sulphur atom.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://paperswithcode.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4722 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4722</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4722</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-20de79ec4fe682b68930eb4dcd91b1801b8d4731</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/20de79ec4fe682b68930eb4dcd91b1801b8d4731" target="_blank">Towards Understanding Grokking: An Effective Theory of Representation Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This study provides intuitive explanations of the origin of grokking, and highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.</p>
                <p><strong>Paper Abstract:</strong> We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a"Goldilocks zone"(including comprehension and grokking) between memorization and confusion. We find on transformers the grokking phase stays closer to the memorization phase (compared to the comprehension phase), leading to delayed generalization. The Goldilocks phase is reminiscent of"intelligence from starvation"in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4722.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4722.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toy-sum-MLP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toy embedding-sum + decoder MLP model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal model that maps discrete symbols a,b to trainable embeddings E_a,E_b, sums them and passes the sum through a learned decoder (MLP) to predict the result; used to study how structured embeddings enable arithmetic generalization (addition/modular addition) and grokking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Toy embedding-sum + decoder MLP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embeddings E_k in R^{d_in} (often d_in=1 for theory experiments, also 2D visualizations), representation updated with Adam; decoder is an MLP (e.g., 1-200-200-30 in some experiments) mapping E_a + E_b to targets (one-hot for classification or random vectors for regression). Training/validation splits and hyperparameters varied in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition (integer addition), modular addition; generalization to Abelian groups (and experiments on permutation group and other groups via extensions).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>The model performs arithmetic by learning a structured low-dimensional representation of symbols such that sums of embeddings corresponding to equal arithmetic results coincide (parallelograms / linear structure E_k = a + k b). The decoder then maps identical/similar summed-embeddings to the correct outputs, enabling generalization rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical alignment between predicted accuracy from representation (widehat{Acc} computed from parallelograms) and actual accuracy (Fig.3); visualizations showing parallelism/parallelograms in embeddings (Fig.2) and linear structures emerging as training proceeds; experiments where representations with high RQI produce correct validation predictions even for unseen input pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>High-dimensional embeddings can hide linear structure in a single useful dimension while RQI (defined via Euclidean parallelograms) may report low value; realistic trained models often fall short of 'ideal' injective-decoder behavior (Alexander principle gap) so not all implied parallelograms form; the effective-theory neglects decoder dynamics and thus has quantitative differences vs full training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representation Quality Index (RQI) in [0,1] predicts generalization; a common practical threshold used is RQI > 0.95 to indicate near-linear representation; critical training data fraction r_c ≈ 0.4 (for p=10 addition) below which probability of obtaining linear representation is near zero; steps-to-grok (to reach RQI>0.95) scale as n_h ~ 1/(lambda_3 * eta) where lambda_3 is the third eigenvalue (grokking rate).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Systematic sweeps of embedding vs decoder learning rates and decoder weight decay produce four phases (comprehension, grokking, memorization, confusion); experiments changing batch size, initialization scale, and representation weight decay alter phases (e.g., smaller initialization and larger batch size can favor comprehension); comparisons of different optimizer hyperparameters show faster decoder relative to encoder leads to memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Fails to generalize when training set is below critical fraction (~0.4 in toy p=10); if decoder trains too fast (high capacity) the system memorizes and prevents structure from forming; if decoder extremely slow generalization occurs but is very slow; RQI metric restrictive in high dimensions; non-ideal models often produce fewer parallelograms than the theoretical optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared internally to the decoder-only transformer experiments in the paper (toy model generalizes the qualitative behavior); behavior aligns qualitatively with Power et al. (original grokking) though architectural details cause differences (e.g., transformer embedding dimensionality changes how weight decay affects grokking).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Grokking: An Effective Theory of Representation Learning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4722.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4722.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Effective-theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effective theory of representation learning dynamics (ell_eff)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-inspired reduced model that treats normalized embeddings as particles evolving under gradient flow of an effective loss ell_eff which enforces equality of summed embeddings for training-implied parallelograms; used to predict existence of phases and critical training fraction and timescales for grokking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Effective theory (ell_eff gradient flow)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Normalized embeddings \tilde{E}_k evolve by d\tilde{E}_i/dt = -∂ ell_eff / ∂\tilde{E}_i with ell_eff = ell_0 / Z_0, ell_0 an average squared Euclidean distance over training-implied parallelograms and Z_0 the sum of squared norms; conserved quantities C = sum E_k and Z_0 are derived.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition (toy) used for analytic calculations; predicts behavior for modular/addition datasets and extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Representation dynamics are dominated by an effective loss that enforces linear constraints among embeddings implied by the training set; ground states of ell_eff correspond to linear representations (E_k = a + k b) when system of linear equations has nullity 2; the third eigenvalue lambda_3 controls slow approach to linearity (grokking rate).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Quantitative predictions: probability of unique linear representation as a function of training fraction (phase transition at r_c ≈ 0.4) and dependence of lambda_3 on data fraction; simulated effective dynamics produce representation trajectories qualitatively matching neural network training (Fig.4c,d) and predicted grokking times qualitatively match measured steps (Fig.5b).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Effective theory omits the decoder and so cannot capture decoder-mediated effects and quantitative details; metric choice (Euclidean) may be inappropriate when decoder defines a different metric (e.g., 'pizza slice' decoder geometry), leading to mismatches in high-dimensional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Predicts grokking time t_h ≈ 1/λ_3 (and steps n_h = 1/(λ_3 η)); predicts a phase transition in probability of linear representation at training fraction r_c ≈ 0.4 for p=10; eigenvalue spectrum has two zero modes (translation and scaling) with λ_3 controlling slowest nontrivial decay.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Comparison of eigenvalue spectra and simulated trajectories under ell_eff with full NN training; use of singular-value/null-space analysis of linear constraint matrix A(P) to compute nullity and predict uniqueness of linear representation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Quantitative discrepancies with real networks due to absence of decoder and simplifications; fails to capture high-dimensional decoder effects and non-Euclidean metrics; predictions are probabilistic because different datasets of same size yield different λ_3 distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Serves as a reduced model explanation complementing full neural training; explains toy model behavior and provides qualitative guidance for transformer experiments but needs decoder-aware extensions for precise quantitative matching.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Grokking: An Effective Theory of Representation Learning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4722.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4722.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-modadd</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoder-only transformer on modular addition (p=53)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only transformer that encodes integers into 256D embeddings, processes two-token inputs and predicts modular addition results, used to test whether toy-model insights generalize to realistic transformer architectures and to examine grokking behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only transformer (embeddings 256D)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>256-dimensional learnable embeddings for p=53 integers; transformer decoder processes tokens and a final linear layer maps concatenated outputs to class logits for classification of the sum modulo p; embedding and decoder trained (often with same optimizer) or with decoder-regularization (weight decay) varied.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Modular addition (addition modulo p), classification (predict token class).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Generalization arises when the transformer learns a low-dimensional structured embedding manifold (e.g., circular manifold for modular addition) so that decoder can map manifold coordinates to outputs; grokking occurs when decoder overfits early and representation learning lags, and regularization on the decoder (weight decay, dropout) prevents premature memorization enabling earlier formation of structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>PCA visualizations show embeddings projecting into a circle at the time of generalization; effective dimension (entropy of PCA explained-variance) rises then sharply drops at generalization; phase diagrams show grokking region and that applying decoder weight decay or dropout accelerates generalization (dropout can reduce grokking time to under 10^3 steps and eliminate grokking); empirical concordance with toy model qualitative predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Transformer behavior differs quantitatively from toy model: weight decay applied only to decoder is beneficial (contrast to toy where weight decay always reduces decoder capacity), presumably because of high embedding dimensionality; models are sensitive to embedding dimensionality and initialization so simple Euclidean-parallelogram diagnostics may not fully capture representation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Phases defined by >90% training/validation accuracy within 1e5 steps; grokking observed when training accuracy reaches >90% long before validation accuracy; applying dropout or decoder regularization can reduce generalization time to <10^3 steps (reported qualitatively); embedding PCA entropy shows sharp change at generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions: applying decoder weight decay (scan over weight decay and decoder learning rate) produces phase diagrams with comprehension/grokking/memorization/confusion; applying dropout in decoder blocks speeds up generalization and can remove grokking; projecting initial embeddings onto final principal axes shows some structure pre-exists (lottery-ticket style).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Grokking (severe delayed generalization) occurs under many typical hyperparameter settings; transformer grokking tends to remain closer to memorization phase causing delayed generalization; high embedding dimensionality changes how regularizers affect dynamics (nontrivial dependence).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to the toy-sum-MLP model: qualitative behaviors (need for representation learning, phase structure) persist, but quantitative effects (role of weight decay, speed of generalization) differ because of higher embedding dimensionality and decoder capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Grokking: An Effective Theory of Representation Learning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4722.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4722.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Matrix-embeds</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matrix-embedding model for general (non-Abelian) groups</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architecture that embeds each group element as a learnable d×d matrix and multiplies matrices before passing the product to a decoder, used to extend the parallelogram/representation framework to non-Abelian groups (e.g., permutation group S_3).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Matrix-embedding group model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each symbol a is represented by a learnable matrix E_a ∈ R^{d×d}; for operation a∘b=c the model computes E_a E_b (matrix product) and feeds that to a decoder for regression/classification. Experiments include permutation group S_3 and phase-diagram studies analogous to the vector-sum toy.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Group operations for non-Abelian groups (permutation group S_3), generalized group 'arithmetic' implemented via matrix multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Generalization is enabled by learning matrix representations that satisfy training-implied equalities (generalized parallelograms defined by small Frobenius norm differences of matrix products); deduction of new equalities (and hence generalization) is more complex than the Abelian case and may require more constraints (e.g., three parallelograms to deduce another).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Numerical experiments reproduce grokking-like phenomena and phase diagrams similar to the Abelian case; definitions of generalized parallelograms and linear-algebraic deduction arguments show how implicit constraints can arise from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>In the non-Abelian setting two parallelograms are not sufficient to deduce a new one, so generalization requires richer training constraints; the deduction structure is more complex which can make representation learning and subsequent generalization slower or more fragile.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Phase diagrams (comprehension/grokking/memorization/confusion) qualitatively observed for permutation regression; no single numeric critical fraction reported for non-Abelian case in the text, but behavior mirrors the toy addition experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Architecture-level probe: replacing vector embeddings with matrices and using matrix multiplication before decoder; observed that analogous hyperparameter sweeps produce similar four-phase diagrams.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Non-Abelian deduction requires more constraints (harder to produce implicit parallelograms), making generalization more difficult; theoretical simplifications assume ability to invert/compare matrices which may be sensitive to optimization and initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Shows that the representation-learning + parallelogram logic extends beyond Abelian addition to matrix-group representations, but with additional algebraic complexity compared to vector-sum toy and transformer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Grokking: An Effective Theory of Representation Learning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4722.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4722.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RQI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representation Quality Index (RQI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantitative metric defined as the fraction of permissible parallelograms (implied by the full dataset) that actually hold in the learned representation, used to predict a model's generalization ability on arithmetic tasks in the toy setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Representation Quality Index (metric)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RQI(R) = |P(R)| / |P_0| where P_0 are permissible parallelograms implied by the dataset and P(R) are those parallelograms realized (within tolerance) by the representation R; ranges from 0 (random) to 1 (perfect linear representation).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and modular addition (toy settings); concept applies to other group operations via generalized parallelogram definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>High RQI indicates the embedding set encodes arithmetic structure (linear or group representation) and therefore predicts ability of decoder to generalize to unseen pairs; RQI acts as a proxy for how many implicit training-derived constraints hold in the representation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical agreement between predicted accuracy hat{Acc} computed from RQI-based augmentation and measured accuracy (Fig.3); correlation of RQI increases with validation performance and tracks the emergence of embedding structure in visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>RQI is too strict in high-dimensional settings where useful structure may exist in a low-dimensional subspace while other dimensions are arbitrary, leading to low RQI despite good generalization; RQI depends on Euclidean metric choice and may miss decoder-defined similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RQI values used to classify learned representation quality; RQI>0.95 used as criterion for 'linear representation' in many experiments; RQI exhibits a phase transition probability vs training fraction around r_c ≈ 0.4 (toy p=10 case).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Used as target metric to measure steps-to-grok (steps required to reach RQI>0.95) across hyperparameter sweeps; used to validate effective-theory predictions of grokking time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Not robust in high-dimensional embeddings; sensitive to the chosen metric and threshold δ; does not account for decoder's ability to ignore irrelevant embedding dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Used to compare toy-sum-MLP runs, ideal models vs realistic models, and to relate toy predictions to transformer experiments qualitatively (but direct numeric comparisons limited by dimensionality differences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Grokking: An Effective Theory of Representation Learning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grokking: Generalization beyond overfitting on small algorithmic datasets <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretability analysis of grokking <em>(Rating: 2)</em></li>
                <li>In-context learning and induction heads <em>(Rating: 1)</em></li>
                <li>Visual learning of arithmetic operation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4722",
    "paper_id": "paper-20de79ec4fe682b68930eb4dcd91b1801b8d4731",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "Toy-sum-MLP",
            "name_full": "Toy embedding-sum + decoder MLP model",
            "brief_description": "A minimal model that maps discrete symbols a,b to trainable embeddings E_a,E_b, sums them and passes the sum through a learned decoder (MLP) to predict the result; used to study how structured embeddings enable arithmetic generalization (addition/modular addition) and grokking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Toy embedding-sum + decoder MLP",
            "model_description": "Embeddings E_k in R^{d_in} (often d_in=1 for theory experiments, also 2D visualizations), representation updated with Adam; decoder is an MLP (e.g., 1-200-200-30 in some experiments) mapping E_a + E_b to targets (one-hot for classification or random vectors for regression). Training/validation splits and hyperparameters varied in experiments.",
            "arithmetic_task_type": "Addition (integer addition), modular addition; generalization to Abelian groups (and experiments on permutation group and other groups via extensions).",
            "mechanism_hypothesis": "The model performs arithmetic by learning a structured low-dimensional representation of symbols such that sums of embeddings corresponding to equal arithmetic results coincide (parallelograms / linear structure E_k = a + k b). The decoder then maps identical/similar summed-embeddings to the correct outputs, enabling generalization rather than memorization.",
            "evidence_for_mechanism": "Empirical alignment between predicted accuracy from representation (widehat{Acc} computed from parallelograms) and actual accuracy (Fig.3); visualizations showing parallelism/parallelograms in embeddings (Fig.2) and linear structures emerging as training proceeds; experiments where representations with high RQI produce correct validation predictions even for unseen input pairs.",
            "evidence_against_mechanism": "High-dimensional embeddings can hide linear structure in a single useful dimension while RQI (defined via Euclidean parallelograms) may report low value; realistic trained models often fall short of 'ideal' injective-decoder behavior (Alexander principle gap) so not all implied parallelograms form; the effective-theory neglects decoder dynamics and thus has quantitative differences vs full training.",
            "performance_metrics": "Representation Quality Index (RQI) in [0,1] predicts generalization; a common practical threshold used is RQI &gt; 0.95 to indicate near-linear representation; critical training data fraction r_c ≈ 0.4 (for p=10 addition) below which probability of obtaining linear representation is near zero; steps-to-grok (to reach RQI&gt;0.95) scale as n_h ~ 1/(lambda_3 * eta) where lambda_3 is the third eigenvalue (grokking rate).",
            "probing_or_intervention_results": "Systematic sweeps of embedding vs decoder learning rates and decoder weight decay produce four phases (comprehension, grokking, memorization, confusion); experiments changing batch size, initialization scale, and representation weight decay alter phases (e.g., smaller initialization and larger batch size can favor comprehension); comparisons of different optimizer hyperparameters show faster decoder relative to encoder leads to memorization.",
            "limitations_and_failure_modes": "Fails to generalize when training set is below critical fraction (~0.4 in toy p=10); if decoder trains too fast (high capacity) the system memorizes and prevents structure from forming; if decoder extremely slow generalization occurs but is very slow; RQI metric restrictive in high dimensions; non-ideal models often produce fewer parallelograms than the theoretical optimum.",
            "comparison_to_other_models": "Compared internally to the decoder-only transformer experiments in the paper (toy model generalizes the qualitative behavior); behavior aligns qualitatively with Power et al. (original grokking) though architectural details cause differences (e.g., transformer embedding dimensionality changes how weight decay affects grokking).",
            "uuid": "e4722.0",
            "source_info": {
                "paper_title": "Towards Understanding Grokking: An Effective Theory of Representation Learning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Effective-theory",
            "name_full": "Effective theory of representation learning dynamics (ell_eff)",
            "brief_description": "A physics-inspired reduced model that treats normalized embeddings as particles evolving under gradient flow of an effective loss ell_eff which enforces equality of summed embeddings for training-implied parallelograms; used to predict existence of phases and critical training fraction and timescales for grokking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Effective theory (ell_eff gradient flow)",
            "model_description": "Normalized embeddings \\tilde{E}_k evolve by d\\tilde{E}_i/dt = -∂ ell_eff / ∂\\tilde{E}_i with ell_eff = ell_0 / Z_0, ell_0 an average squared Euclidean distance over training-implied parallelograms and Z_0 the sum of squared norms; conserved quantities C = sum E_k and Z_0 are derived.",
            "arithmetic_task_type": "Addition (toy) used for analytic calculations; predicts behavior for modular/addition datasets and extensions.",
            "mechanism_hypothesis": "Representation dynamics are dominated by an effective loss that enforces linear constraints among embeddings implied by the training set; ground states of ell_eff correspond to linear representations (E_k = a + k b) when system of linear equations has nullity 2; the third eigenvalue lambda_3 controls slow approach to linearity (grokking rate).",
            "evidence_for_mechanism": "Quantitative predictions: probability of unique linear representation as a function of training fraction (phase transition at r_c ≈ 0.4) and dependence of lambda_3 on data fraction; simulated effective dynamics produce representation trajectories qualitatively matching neural network training (Fig.4c,d) and predicted grokking times qualitatively match measured steps (Fig.5b).",
            "evidence_against_mechanism": "Effective theory omits the decoder and so cannot capture decoder-mediated effects and quantitative details; metric choice (Euclidean) may be inappropriate when decoder defines a different metric (e.g., 'pizza slice' decoder geometry), leading to mismatches in high-dimensional settings.",
            "performance_metrics": "Predicts grokking time t_h ≈ 1/λ_3 (and steps n_h = 1/(λ_3 η)); predicts a phase transition in probability of linear representation at training fraction r_c ≈ 0.4 for p=10; eigenvalue spectrum has two zero modes (translation and scaling) with λ_3 controlling slowest nontrivial decay.",
            "probing_or_intervention_results": "Comparison of eigenvalue spectra and simulated trajectories under ell_eff with full NN training; use of singular-value/null-space analysis of linear constraint matrix A(P) to compute nullity and predict uniqueness of linear representation.",
            "limitations_and_failure_modes": "Quantitative discrepancies with real networks due to absence of decoder and simplifications; fails to capture high-dimensional decoder effects and non-Euclidean metrics; predictions are probabilistic because different datasets of same size yield different λ_3 distributions.",
            "comparison_to_other_models": "Serves as a reduced model explanation complementing full neural training; explains toy model behavior and provides qualitative guidance for transformer experiments but needs decoder-aware extensions for precise quantitative matching.",
            "uuid": "e4722.1",
            "source_info": {
                "paper_title": "Towards Understanding Grokking: An Effective Theory of Representation Learning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Transformer-modadd",
            "name_full": "Decoder-only transformer on modular addition (p=53)",
            "brief_description": "A decoder-only transformer that encodes integers into 256D embeddings, processes two-token inputs and predicts modular addition results, used to test whether toy-model insights generalize to realistic transformer architectures and to examine grokking behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only transformer (embeddings 256D)",
            "model_description": "256-dimensional learnable embeddings for p=53 integers; transformer decoder processes tokens and a final linear layer maps concatenated outputs to class logits for classification of the sum modulo p; embedding and decoder trained (often with same optimizer) or with decoder-regularization (weight decay) varied.",
            "arithmetic_task_type": "Modular addition (addition modulo p), classification (predict token class).",
            "mechanism_hypothesis": "Generalization arises when the transformer learns a low-dimensional structured embedding manifold (e.g., circular manifold for modular addition) so that decoder can map manifold coordinates to outputs; grokking occurs when decoder overfits early and representation learning lags, and regularization on the decoder (weight decay, dropout) prevents premature memorization enabling earlier formation of structure.",
            "evidence_for_mechanism": "PCA visualizations show embeddings projecting into a circle at the time of generalization; effective dimension (entropy of PCA explained-variance) rises then sharply drops at generalization; phase diagrams show grokking region and that applying decoder weight decay or dropout accelerates generalization (dropout can reduce grokking time to under 10^3 steps and eliminate grokking); empirical concordance with toy model qualitative predictions.",
            "evidence_against_mechanism": "Transformer behavior differs quantitatively from toy model: weight decay applied only to decoder is beneficial (contrast to toy where weight decay always reduces decoder capacity), presumably because of high embedding dimensionality; models are sensitive to embedding dimensionality and initialization so simple Euclidean-parallelogram diagnostics may not fully capture representation quality.",
            "performance_metrics": "Phases defined by &gt;90% training/validation accuracy within 1e5 steps; grokking observed when training accuracy reaches &gt;90% long before validation accuracy; applying dropout or decoder regularization can reduce generalization time to &lt;10^3 steps (reported qualitatively); embedding PCA entropy shows sharp change at generalization.",
            "probing_or_intervention_results": "Interventions: applying decoder weight decay (scan over weight decay and decoder learning rate) produces phase diagrams with comprehension/grokking/memorization/confusion; applying dropout in decoder blocks speeds up generalization and can remove grokking; projecting initial embeddings onto final principal axes shows some structure pre-exists (lottery-ticket style).",
            "limitations_and_failure_modes": "Grokking (severe delayed generalization) occurs under many typical hyperparameter settings; transformer grokking tends to remain closer to memorization phase causing delayed generalization; high embedding dimensionality changes how regularizers affect dynamics (nontrivial dependence).",
            "comparison_to_other_models": "Compared to the toy-sum-MLP model: qualitative behaviors (need for representation learning, phase structure) persist, but quantitative effects (role of weight decay, speed of generalization) differ because of higher embedding dimensionality and decoder capacity.",
            "uuid": "e4722.2",
            "source_info": {
                "paper_title": "Towards Understanding Grokking: An Effective Theory of Representation Learning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Matrix-embeds",
            "name_full": "Matrix-embedding model for general (non-Abelian) groups",
            "brief_description": "An architecture that embeds each group element as a learnable d×d matrix and multiplies matrices before passing the product to a decoder, used to extend the parallelogram/representation framework to non-Abelian groups (e.g., permutation group S_3).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Matrix-embedding group model",
            "model_description": "Each symbol a is represented by a learnable matrix E_a ∈ R^{d×d}; for operation a∘b=c the model computes E_a E_b (matrix product) and feeds that to a decoder for regression/classification. Experiments include permutation group S_3 and phase-diagram studies analogous to the vector-sum toy.",
            "arithmetic_task_type": "Group operations for non-Abelian groups (permutation group S_3), generalized group 'arithmetic' implemented via matrix multiplication.",
            "mechanism_hypothesis": "Generalization is enabled by learning matrix representations that satisfy training-implied equalities (generalized parallelograms defined by small Frobenius norm differences of matrix products); deduction of new equalities (and hence generalization) is more complex than the Abelian case and may require more constraints (e.g., three parallelograms to deduce another).",
            "evidence_for_mechanism": "Numerical experiments reproduce grokking-like phenomena and phase diagrams similar to the Abelian case; definitions of generalized parallelograms and linear-algebraic deduction arguments show how implicit constraints can arise from training data.",
            "evidence_against_mechanism": "In the non-Abelian setting two parallelograms are not sufficient to deduce a new one, so generalization requires richer training constraints; the deduction structure is more complex which can make representation learning and subsequent generalization slower or more fragile.",
            "performance_metrics": "Phase diagrams (comprehension/grokking/memorization/confusion) qualitatively observed for permutation regression; no single numeric critical fraction reported for non-Abelian case in the text, but behavior mirrors the toy addition experiments.",
            "probing_or_intervention_results": "Architecture-level probe: replacing vector embeddings with matrices and using matrix multiplication before decoder; observed that analogous hyperparameter sweeps produce similar four-phase diagrams.",
            "limitations_and_failure_modes": "Non-Abelian deduction requires more constraints (harder to produce implicit parallelograms), making generalization more difficult; theoretical simplifications assume ability to invert/compare matrices which may be sensitive to optimization and initialization.",
            "comparison_to_other_models": "Shows that the representation-learning + parallelogram logic extends beyond Abelian addition to matrix-group representations, but with additional algebraic complexity compared to vector-sum toy and transformer experiments.",
            "uuid": "e4722.3",
            "source_info": {
                "paper_title": "Towards Understanding Grokking: An Effective Theory of Representation Learning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "RQI",
            "name_full": "Representation Quality Index (RQI)",
            "brief_description": "A quantitative metric defined as the fraction of permissible parallelograms (implied by the full dataset) that actually hold in the learned representation, used to predict a model's generalization ability on arithmetic tasks in the toy setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Representation Quality Index (metric)",
            "model_description": "RQI(R) = |P(R)| / |P_0| where P_0 are permissible parallelograms implied by the dataset and P(R) are those parallelograms realized (within tolerance) by the representation R; ranges from 0 (random) to 1 (perfect linear representation).",
            "arithmetic_task_type": "Addition and modular addition (toy settings); concept applies to other group operations via generalized parallelogram definitions.",
            "mechanism_hypothesis": "High RQI indicates the embedding set encodes arithmetic structure (linear or group representation) and therefore predicts ability of decoder to generalize to unseen pairs; RQI acts as a proxy for how many implicit training-derived constraints hold in the representation.",
            "evidence_for_mechanism": "Empirical agreement between predicted accuracy hat{Acc} computed from RQI-based augmentation and measured accuracy (Fig.3); correlation of RQI increases with validation performance and tracks the emergence of embedding structure in visualizations.",
            "evidence_against_mechanism": "RQI is too strict in high-dimensional settings where useful structure may exist in a low-dimensional subspace while other dimensions are arbitrary, leading to low RQI despite good generalization; RQI depends on Euclidean metric choice and may miss decoder-defined similarity.",
            "performance_metrics": "RQI values used to classify learned representation quality; RQI&gt;0.95 used as criterion for 'linear representation' in many experiments; RQI exhibits a phase transition probability vs training fraction around r_c ≈ 0.4 (toy p=10 case).",
            "probing_or_intervention_results": "Used as target metric to measure steps-to-grok (steps required to reach RQI&gt;0.95) across hyperparameter sweeps; used to validate effective-theory predictions of grokking time.",
            "limitations_and_failure_modes": "Not robust in high-dimensional embeddings; sensitive to the chosen metric and threshold δ; does not account for decoder's ability to ignore irrelevant embedding dimensions.",
            "comparison_to_other_models": "Used to compare toy-sum-MLP runs, ideal models vs realistic models, and to relate toy predictions to transformer experiments qualitatively (but direct numeric comparisons limited by dimensionality differences).",
            "uuid": "e4722.4",
            "source_info": {
                "paper_title": "Towards Understanding Grokking: An Effective Theory of Representation Learning",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grokking: Generalization beyond overfitting on small algorithmic datasets",
            "rating": 2
        },
        {
            "paper_title": "A mechanistic interpretability analysis of grokking",
            "rating": 2
        },
        {
            "paper_title": "In-context learning and induction heads",
            "rating": 1
        },
        {
            "paper_title": "Visual learning of arithmetic operation",
            "rating": 1
        }
    ],
    "cost": 0.015865999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Understanding Grokking: An Effective Theory of Representation Learning</h1>
<p>Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J. Michaud, Max Tegmark, Mike Williams<br>Department of Physics, Institute for AI and Fundamental Interactions, MIT<br>{zmliu, kitouni, nnolte, ericjm, tegmark,mwill}@mit.edu</p>
<h4>Abstract</h4>
<p>We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. We find on transformers the grokking phase stays closer to the memorization phase (compared to the comprehension phase), leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.</p>
<h2>1 Introduction</h2>
<p>Perhaps the central challenge of a scientific understanding of deep learning lies in accounting for neural network generalization. Power et al. [1] recently added a new puzzle to the task of understanding generalization with their discovery of grokking. Grokking refers to the surprising phenomenon of delayed generalization where neural networks, on certain learning problems, generalize long after overfitting their training set. It is a rare albeit striking phenomenon that violates common machine learning intuitions, raising three key puzzles:</p>
<p>Q1 The origin of generalization: When trained on the algorithmic datasets where grokking occurs, how do models generalize at all?
Q2 The critical training size: Why does the training time needed to "grok" (generalize) diverge as the training set size decreases toward a critical point?
Q3 Delayed generalization: Under what conditions does delayed generalization occur?
We provide evidence that representation learning is central to answering each of these questions. Our answers can be summarized as follows:</p>
<p>A1 Generalization can be attributed to learning a good representation of the input embeddings, i.e., a representation that has the appropriate structure for the task and which can be predicted from the theory in Section 3. See Figures 1 and 2.</p>
<p>A2 The critical training set size corresponds to the least amount of training data that can determine such a representation (which, in some cases, is unique up to linear transformations).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Visualization of the first two principal components of the learned input embeddings at different training stages of a transformer learning modular addition. We observe that generalization coincides with the emergence of structure in the embeddings. See Section 4.2 for the training details.</p>
<p>A3 Grokking is a phase between "comprehension" and "memorization" phases and it can be remedied with proper hyperparmeter tuning, as illustrated by the phase diagrams in Figure 6.</p>
<p>This paper is organized as follows: In Section 2, we introduce the problem setting and build a simplified toy model. In Section 3, we will use an effective theory approach, a useful tool from theoretical physics, to shed some light on questions Q1 and Q2 and show the relationship between generalization and the learning of structured representations. In Section 4, we explain Q3 by displaying phase diagrams from a grid search of hyperparameters and show how we can "de-delay" generalization by following intuition developed from the phase diagram. We discuss related work in Section 5, followed by conclusions in Section 6.1</p>
<h1>2 Problem Setting</h1>
<p>Power et al. [1] observe grokking on a less common task - learning "algorithmic" binary operations. Given some binary operation $\circ$, a network is tasked with learning the map $(a, b) \mapsto c$ where $c=a \circ b$. They use a decoder-only transformer to predict the second to last token in a tokenized equation of the form "<fhs> <op> <rhs> <eq> <result> <eos>" Each token is represented as a 256-dimensional embedding vector. The embeddings are learnable and initialized randomly. After the transformer, a final linear layer maps the output to class logits for each token.</p>
<p>Toy Model We primarily study grokking in a simpler toy model, which still retains the key behaviors from the setup of [1]. Although [1] treated this as a classification task, we study both regression (mean-squared error) and classification (cross-entropy). The basic setup is as follows: our model takes as input the symbols $a, b$ and maps them to trainable embedding vectors $\mathbf{E}<em b="b">{a}, \mathbf{E}</em>} \in \mathbb{R}^{d_{\mathrm{an}}}$. It then sums $\mathbf{E<em b="b">{a}, \mathbf{E}</em>}$ and sends the resulting vector through a "decoder" MLP. The target output vector, denoted $\mathbf{Y<em _mathrm_an="\mathrm{an">{c} \in \mathbb{R}^{d</em>}}}$ is a fixed random vector (regression task) or a one-hot vector (classification task). Our model architecture can therefore be compactly described as $(a, b) \mapsto \operatorname{Dec}\left(\mathbf{E<em b="b">{a}+\mathbf{E}</em>$ and the decoder are trainable. Despite its simplicity, this toy model can generalize to all abelian groups (discussed in Appendix B). In sections 3-4.1, we consider only the binary operation of addition. We consider modular addition in Section 4.2 to generalize some of our results to a transformer architecture and study general non-abelian operations in Appendix H.}\right)$, where the embeddings $\mathbf{E}_{*</p>
<p>Dataset In our toy setting, we are concerned with learning the addition operation. A data sample corresponding to $i+j$ is denoted as $(i, j)$ for simplicity. If $i, j \in{0, \ldots, p-1}$, there are in total $p(p+1) / 2$ different samples since we consider $i+j$ and $j+i$ to be the same sample. A dataset $D$ is a set of non-repeating data samples. We denote the full dataset as $D_{0}$ and split it into a training dataset $D$ and a validation dataset $D^{\prime}$, i.e., $D \bigcup D^{\prime}=D_{0}, D \bigcap D^{\prime}=\emptyset$. We define training data fraction $=|D| /\left|D_{0}\right|$ where $|\cdot|$ denotes the cardinality of the set.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visualization of the learned set of embeddings ( $p=11$ ) and the decoder function associated with it for the case of 2D embeddings. Axes refer to each dimension of the learned embeddings. The decoder is evaluated on a grid of points in embedding-space and the color at each point represents the highest probability class. For visualization purposes, the decoder is trained on inputs of the form $\left(\mathbf{E}<em j="j">{i}+\mathbf{E}</em>\right) / 2$. One can read off the output of the decoder when fed the operation $i \circ j$ from this figure simply by taking the midpoint between the respective embeddings of $i$ and $j$.</p>
<h1>3 Why Generalization Occurs: Representations and Dynamics</h1>
<p>We can see that generalization appears to be linked to the emergence of highly-structured embeddings in Figure 2. In particular, Figure 2 (a, b) shows parallelograms in toy addition, and (c, d) shows a circle in toy modular addition. We now restrict ourselves to the toy addition setup and formalize a notion of representation quality and show that it predicts the model's performance. We then develop a physics-inspired effective theory of learning which can accurately predict the critical training set size and training trajectories of representations. The concept of an effective theory in physics is similar to model reduction in computational methods in that it aims to describe complex phenomena with simple yet intuitive pictures. In our effective theory, we will model the dynamics of representation learning not as gradient descent of the true task loss but rather a simpler effective loss function $\ell_{\text {eff }}$ which depends only on the representations in embedding space and not on the decoder.</p>
<h3>3.1 Representation quality predicts generalization for the toy model</h3>
<p>A rigorous definition for structure in the learned representation is necessary. We propose the following definition,
Definition 1. $(i, j, m, n)$ is a $\delta$-parallelogram in the representation $\mathbf{R} \equiv\left[\mathbf{E}<em p-1="p-1">{0}, \cdots, \mathbf{E}</em>\right]$ if</p>
<p>$$
\left|\left(\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}\right)-\left(\mathbf{E<em n="n">{m}+\mathbf{E}</em>\right)\right| \leq \delta
$$</p>
<p>In the following derivations, we can take $\delta$, which is a small threshold to tolerate numerical errors, to be zero.
Proposition 1. When the training loss is zero, any parallelogram $(i, j, m, n)$ in representation $\mathbf{R}$ satisfies $i+j=m+n$.</p>
<p>Proof. Suppose that this is not the case, i.e., suppose $\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}=\mathbf{E<em n="n">{m}+\mathbf{E}</em>}$ but $i+j \neq m+n$, then $\mathbf{Y<em i="i">{i+j}=\operatorname{Dec}\left(\mathbf{E}</em>}+\mathbf{E<em m="m">{j}\right)=\operatorname{Dec}\left(\mathbf{E}</em>}+\mathbf{E<em m_n="m+n">{n}\right)=\mathbf{Y}</em>}$ where the first and last equalities come from the zero training loss assumption. However, since $i+j \neq m+n$, we have $\mathbf{Y<em n_m="n+m">{i+j} \neq \mathbf{Y}</em>$ (almost surely in the regression task), a contradiction.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: We compute accuracy (of the full dataset) either measured empirically Acc, or predicted from the representation of the embeddings $\widehat{\operatorname{Acc}}$. These two accuracies as a function of training data fraction are plotted in (a)(b), and their agreement is shown in (c).</p>
<p>It is convenient to define the permissible parallelogram set associated with a training dataset $D$ ("permissible" means consistent with 100% training accuracy) as</p>
<p>$$
P_0(D) = {(i, j, m, n) | (i, j) \in D, (m, n) \in D, i + j = m + n} \tag{1}
$$</p>
<p>For simplicity, we denote $P_0 \equiv P_0(D_0)$. Given a representation $\mathbf{R}$, we can check how many permissible parallelograms actually exist in $\mathbf{R}$ within error $\delta$, so we define the parallelogram set corresponding to $\mathbf{R}$ as</p>
<p>$$
P(\mathbf{R}, \delta) = {(i, j, m, n) | (i, j, m, n) \in P_0, | (\mathbf{E}_i + \mathbf{E}_j) - (\mathbf{E}_m + \mathbf{E}_n) | \le \delta} \tag{2}
$$</p>
<p>For brevity we will write $P(\mathbf{R})$, suppressing the dependence on $\delta$. We define the representation quality index (RQI) as</p>
<p>$$
\text{RQI}(\mathbf{R}) = \frac{|P(\mathbf{R})|}{|P_0|} \in [0, 1] \tag{3}
$$</p>
<p>We will use the term <em>linear representation</em> or <em>linear structure</em> to refer to a representation whose embeddings are of the form $\mathbf{E}_k = \mathbf{a} + k\mathbf{b} (k = 0, \ldots, p - 1; \mathbf{a}, \mathbf{b} \in \mathbb{R}^{d_n})$. A linear representation has $\text{RQI} = 1$, while a random representation (sampled from, say, a normal distribution) has $\text{RQI} = 0$ with high probability.</p>
<p>Quantitatively, we denote the "predicted accuracy" $\widehat{\text{Acc}}$ as the accuracy achievable on the whole dataset given the representation $\mathbf{R}$ (see Appendix D for the full details). In Figure 3, we see that the predicted $\widehat{\text{Acc}}$ aligns well with the true accuracy Acc, establishing good evidence that structured representation of input embeddings leads to generalization. We use an example to illustrate the origin of generalization here. In the setup of Figure 2 (b), suppose the decoder can achieve zero training loss and $\mathbf{E}<em 14="14">6 + \mathbf{E}_8$ is a training sample hence $\text{Dec}(\mathbf{E}_6 + \mathbf{E}_8) = \mathbf{Y}</em>}$. At validation time, the decoder is tasked with predicting a validation sample $\mathbf{E<em 14="14">5 + \mathbf{E}_9$. Since $(5, 9, 6, 8)$ forms a parallelogram such that $\mathbf{E}_5 + \mathbf{E}_9 = \mathbf{E}_6 + \mathbf{E}_8$, the decoder can predict the validation sample correctly because $\text{Dec}(\mathbf{E}_5 + \mathbf{E}_9) = \text{Dec}(\mathbf{E}_6 + \mathbf{E}_8) = \mathbf{Y}</em>$.</p>
<h3>3.2 The dynamics of embedding vectors</h3>
<p>Suppose that we have an ideal model $\mathcal{M}^<em> = (\text{Dec}^</em>, \mathbf{R}^*)$ such that:^2</p>
<ul>
<li>(1) $\mathcal{M}^*$ can achieve zero training loss;</li>
<li>(2) $\mathcal{M}^<em>$ has an injective decoder, i.e., $\text{Dec}^</em>(\mathbf{x}_1) \neq \text{Dec}^*(\mathbf{x}_2)$ for any $\mathbf{x}_1 \neq \mathbf{x}_2$.</li>
</ul>
<p>Then Proposition 2 provides a mechanism for the formation of parallelograms.</p>
<p>^2 One can verify a posteriori if a trained model $\mathcal{M}$ is close to being an ideal model $\mathcal{M}^*$. Please refer to Appendix E for details.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) The effective theory predicts a phase transition in the probability of obtaining a linear representation around $r_{c}=0.4$. (b) Empirical results display a phase transition of RQI around $r_{c}=0.4$, in agreement with the theory (the blue line shows the median of multiple random seeds). The evolution of 1D representations predicted by the effective theory or obtained from neural network training (shown in (c) and (d) respectively) agree creditably well.</p>
<p>Proposition 2. If a training set $D$ contains two samples $(i, j)$ and $(m, n)$ with $i+j=m+n$, then $\mathcal{M}^{<em>}$ learns a representation $\mathbf{R}^{</em>}$ such that $\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}=\mathbf{E<em n="n">{m}+\mathbf{E}</em>$, i.e., $(i, j, m, n)$ forms a parallelogram.</p>
<p>Proof. Due to the zero training loss assumption, we have $\operatorname{Dec}^{<em>}\left(\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}\right)=\mathbf{Y<em m_n="m+n">{i+j}=\mathbf{Y}</em>^{}=$ $\operatorname{Dec</em>}\left(\mathbf{E}<em n="n">{m}+\mathbf{E}</em>}\right)$. Then the injectivity of $\operatorname{Dec}^{*}$ implies $\mathbf{E<em j="j">{i}+\mathbf{E}</em>}=\mathbf{E<em n="n">{m}+\mathbf{E}</em>$.</p>
<p>The dynamics of the trained embedding vectors are determined by various factors interacting in complex ways, for instance: the details of the decoder architecture, the optimizer hyperparameters, and the various kinds of implicit regularization induced by the training procedure. We will see that the dynamics of normalized quantities, namely, the normalized embeddings at time $t$, defined as $\tilde{\mathbf{E}}<em k="k">{k}^{(t)}=\frac{\mathbf{E}</em>}^{(t)}-\mu_{t}}{\sigma_{t}}$, where $\mu_{t}=\frac{1}{p} \sum_{k} \mathbf{E<em t="t">{k}^{(t)}$ and $\sigma</em>}=\frac{1}{p} \sum_{k}\left|\mathbf{E<em t="t">{k}^{(t)}-\mu</em>$, can be qualitatively described by a simple effective loss (in the physics effective theory sense). We will assume that the normalized embedding vectors obey a gradient flow for an effective loss function of the form}\right|^{2</p>
<p>$$
\begin{gathered}
\frac{d \tilde{\mathbf{E}}<em _mathrm_eff="\mathrm{eff">{i}}{d t}=-\frac{\partial \ell</em>}}}{\partial \tilde{\mathbf{E}<em _mathrm_eff="\mathrm{eff">{i}} \
\ell</em>}}=\frac{\ell_{0}}{Z_{0}}, \quad \ell_{0} \equiv \sum_{(i, j, m, n) \in P_{0}(D)}\left|\tilde{\mathbf{E}<em j="j">{i}+\tilde{\mathbf{E}}</em>}-\tilde{\mathbf{E}<em n="n">{m}-\tilde{\mathbf{E}}</em>
\end{gathered}
$$}\right|^{2} /\left|P_{0}(D)\right|, \quad Z_{0} \equiv \sum_{k}\left|\tilde{\mathbf{E}}_{k}\right|^{2</p>
<p>where $|\cdot|$ denotes Euclidean vector norm. Note that the embeddings do not collapse to the trivial solution $\mathbf{E}<em p-1="p-1">{0}=\cdots=\mathbf{E}</em>=0$ unless initialized as such, because two conserved quantities exist, as proven in Appendix F:</p>
<p>$$
\mathbf{C}=\sum_{k} \mathbf{E}<em 0="0">{k}, \quad Z</em>
$$}=\sum_{k}\left|\mathbf{E}_{k}\right|^{2</p>
<p>We shall now use the effective dynamics to explain empirical observations such as the existence of a critical training set size for generalization.
Degeneracy of ground states (loss optima) We define ground states as those representations satisfying $\ell_{\text {eff }}=0$, which requires the following linear equations to hold:</p>
<p>$$
A(P)=\left{\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}=\mathbf{E<em n="n">{m}+\mathbf{E}</em> \mid(i, j, m, n) \in P\right}
$$</p>
<p>Since each embedding dimension obeys the same set of linear equations, we will assume, without loss of generality, that $d_{\text {in }}=1$. The dimension of the null space of $A(P)$, denoted as $n_{0}$, is the number of degrees of freedom of the ground states. Given a set of parallelograms implied by a training dataset $D$, the nullity of $A(P(D))$ could be obtained by computing the singular values $0 \leq \sigma_{1} \leq \cdots \leq \sigma_{p}$. We always have $n_{0} \geq 2$, i.e., $\sigma_{1}=\sigma_{2}=0$ because the nullity of $A\left(P_{0}\right)$, the set of linear equations given by all possible parallelograms, is $\operatorname{Nullity}\left(A\left(P_{0}\right)\right)=2$ which can be attributed to two degrees</p>
<p>of freedom (translation and scaling). If $n_{0}=2$, the representation is unique up to translations and scaling factors, and the embeddings have the form $\mathbf{E}<em 0="0">{k}=\mathbf{a}+k\mathbf{b}$. Otherwise, when $n</em>&gt;2$, the representation is not constrained enough such that all the embeddings lie on a line.</p>
<p>We present theoretical predictions alongside empirical results for addition $(p=10)$ in Figure 4. As shown in Figure 4 (a), our effective theory predicts that the probability that the training set implies a unique linear structure (which would result in perfect generalization) depends on the training data fraction and has a phase transition around $r_{c}=0.4$. Empirical results from training different models are shown in Figure 4 (b). The number of steps to reach $\mathrm{RQI}&gt;0.95$ is seen to have a phase transition at $r_{c}=0.4$, agreeing with the proposed effective theory and with the empirical findings in [1].</p>
<p>Time towards the linear structure We define the Hessian matrix of $\ell_{0}$ as</p>
<p>$$
\mathbf{H}<em 0="0">{i j}=\frac{1}{Z</em>}} \frac{\partial^{2} \ell_{0}}{\partial \mathbf{E<em j="j">{i} \partial \mathbf{E}</em>
$$}</p>
<p>Note that $\ell_{\text {eff }}=\frac{1}{2} \mathbf{R}^{T} \mathbf{H} \mathbf{R}, \mathbf{R}=\left[\mathbf{E}<em 1="1">{0}, \mathbf{E}</em>\right]$, so the gradient descent is linear, i.e.,}, \cdots, \mathbf{E}_{p-1</p>
<p>$$
\frac{d \mathbf{R}}{d t}=-\mathbf{H R}
$$</p>
<p>If $\mathbf{H}$ has eigenvalues $\lambda_{i}=\sigma_{i}^{2}$ (sorted in increasing order) and eigenvectors $\overline{\mathbf{v}}<em i="i">{i}$, and we have the initial condition $\mathbf{R}(t=0)=\sum</em>} a_{i} \overline{\mathbf{v}<em i="i">{i}$, then we have $\mathbf{R}(t)=\sum</em>} a_{i} \overline{\mathbf{v}<em i="i">{i} e^{-\lambda</em> \eta\right)$.
We verify the above analysis with empirical results. Figure 4 (c)(d) show the trajectories obtained from the effective theory and from neural network training, respectively. The 1D neural representation in Figure 4 (d) are manually normalized to zero mean and unit variance. The two trajectories agree qualitatively, and it takes about $3 n_{h}$ steps for two trajectories to converge to the linear structure. The quantitative differences might be due to the absence of the decoder in the effective theory, which assumes the decoder to take infinitesimal step sizes.} t}$. The first two eigenvalues vanish and $t_{h}=1 / \lambda_{3}$ determines the timescale for the slowest component to decrease by a factor of $e$. We call $\lambda_{3}$ the grokking rate. When the step size is $\eta$, the corresponding number of steps is $n_{h}=t_{h} / \eta=1 /\left(\lambda_{3</p>
<p>Dependence of grokking on data size Note that $\ell_{\text {eff }}$ involves averaging over parallelograms in the training set, it is dependent on training data size, so is $\lambda_{3}$. In Figure 5 (a), we plot the dependence of $\lambda_{3}$ on training data fraction. There are many datasets with the same data size, so $\lambda_{3}$ is a probabilistic function of data size.</p>
<p>Two insights on grokking can be extracted from this plot: (i) When the data fraction is below some threshold (around 0.4 ), $\lambda_{3}$ is zero with high probability, corresponding to no generalization. This again verifies our critical point in Figure 4. (ii) When data size is above the threshold, $\lambda_{3}$ (on average) is an increasing function of data size. This implies that grokking time $t \sim 1 / \lambda_{3}$ decreases as training data size becomes larger, an important observation from [1].</p>
<p>To verify our effective theory, we compare the grokking steps obtained from real neural network training (defined as steps to $\mathrm{RQI}&gt;0.95$ ), and those predicted by our theory $t_{\mathrm{th}} \sim \frac{1}{\lambda_{3} \eta}$ ( $\eta$ is the embedding learning rate), shown in Figure 5 (b). The theory agrees qualitatively with neural networks, showing the trend of decreasing grokking steps as increasing data size. The quantitative differences might be explained as the gap between our effective loss and actual loss.</p>
<p>Limitations of the effective theory While our theory defines an effective loss based on the Euclidean distance between embeddings $\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}$ and $\mathbf{E<em m="m">{n}+\mathbf{E}</em>$, one could imagine generalizing the theory to define a broader notion of parallogram given by some other metric on the representation space. For instance, if we have a decoder like in Figure 2 (d) then the distance between distinct representations within the same "pizza slice" is low, meaning that representations arranged not in parallelograms w.r.t. the Euclidean metric may be parallelograms with respect to the metric defined by the decoder.</p>
<h1>4 Delayed Generalization: A Phase Diagram</h1>
<p>So far, we have (1) observed empirically that generalization on algorithmic datasets corresponds with the emergence of well-structured representations, (2) defined a notion of representation quality in a toy setting and shown that it predicts generalization, and (3) developed an effective theory to describe</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Effective theory explains the dependence of grokking time on data size, for the addition task. (a) Dependence of $\lambda_{3}$ on training data fraction. Above the critical data fraction (around 0.4), as data size becomes larger, $\lambda_{3}$ increases hence grokking time $t \sim 1 / \lambda_{3}$ (predicted by our effective theory) decreases. (b) Comparing grokking steps (defined as RQI $&gt;0.95$ ) predicted by the effective theory with real neural network results. $\eta=10^{-3}$ is the learning rate of the embeddings.
the learning dynamics of the representations in the same toy setting. We now study how optimizer hyperparameters affect high-level learning performance. In particular, we develop phase diagrams for how learning performance depends on the representation learning rate, decoder learning rate and the decoder weight decay. These parameters are of interest since they most explicitly regulate a kind of competition between the encoder and decoder, as we elaborate below.</p>
<h1>4.1 Phase diagram of a toy model</h1>
<p>Training details We update the representation and the decoder with different optimizers. For the 1D embeddings, we use the Adam optimizer with learning rate $\left[10^{-5}, 10^{-2}\right]$ and zero weight decay. For the decoder, we use an AdamW optimizer with the learning rate in $\left[10^{-5}, 10^{-2}\right]$ and the weight decay in $[0,10]$ (regression) or $[0,20]$ (classification). For training/validation spliting, we choose 45/10 for non-modular addition ( $p=10$ ) and 24/12 for the permutation group $S_{3}$. We hard-code addition or matrix multiplication (details in Appendix H) in the decoder for the addition group and the permutation group, respectively.
For each choice of learning rate and weight decay, we compute the number of steps to reach high ( $90 \%$ ) training/validation accuracy. The 2D plane is split into four phases: comprehension, grokking, memorization and confusion, defined in Table 1 in Appendix A. Both comprehension and grokking are able to generalize (in the "Goldilocks zone"), although the grokking phase has delayed generalization. Memorization is also called overfitting, and confusion means failure to even memorize training data. Figure 6 shows the phase diagrams for the addition group and the permutation group. They display quite rich phenomena.
Competition between representation learning and decoder overfitting In the regression setup of the addition dataset, we show how the competition between representation learning and decoder learning (which depend on both learning rate and weight decay, among other things) lead to different learning phases in Figure 6 (a). As expected, a fast decoder coupled with slow representation learning (bottom right) lead to memorization. In the opposite extreme, although an extremely slow decoder coupled with fast representation learning (top left) will generalize in the end, the generalization time is long due to the inefficient decoder training. The ideal phase (comprehension) requires representation learning to be faster, but not too much, than the decoder.
Drawing from an analogy to physical systems, one can think of embedding vectors as a group of particles. In our effective theory from Section 3.2, the dynamics of the particles are described only by their relative positions, in that sense, structure forms mainly due to inter-particle interactions (in reality, these interactions are mediated by the decoder and the loss). The decoder plays the role of an environment exerting external forces on the embeddings. If the magnitude of the external forces are small/large one can expect better/worse representations.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Phase diagrams of learning for the addition group and the permutation group. (a) shows the competition between representation and decoder. (b)(c)(d): each phase diagram contains four phases: comprehension, grokking, memorization and confusion, defined in Table 1. In (b)(c)(d), grokking is sandwiched between comprehension and memorization.</p>
<p>Universality of phase diagrams We fix the embedding learning rate to be $10^{-3}$ and sweep instead decoder weight decay in Figure 6 (b)(c)(d). The phase diagrams correspond to addition regression (b), addition classification (c) and permutation regression (d), respectively. Common phenomena emerge from these different tasks: (i) they all include four phases; (ii) The top right corner (a fast and capable decoder) is the memorization phase; (iii) the bottom right corner (a fast and simple decoder) is the confusion phase; (iv) grokking is sandwiched between comprehension and memorization, which seems to imply that it is an undesirable phase that stems from improperly tuned hyperparameters.</p>
<h1>4.2 Beyond the toy model</h1>
<p>We conjecture that many of the principles which we saw dictate the training dynamics in the toy model also apply more generally. Below, we will see how our framework generalizes to transformer architectures for the task of addition modulo $p$, a minimal reproducible example of the original grokking paper [1].
We first encode $p=53$ integers into 256D learnable embeddings, then pass two integers to a decoderonly transformer architecture. For simplicity, we do not encode the operation symbols here. The outputs from the last layer are concatenated and passed to a linear layer for classification. Training both the encoder and the decoder with the same optimizer (i.e., with the same hyperparameters) leads to the grokking phenomenon. Generalization appears much earlier once we lower the effective decoder capacity with weight decay (full phase diagram in Figure 7).
Early on, the model is able to perfectly fit the training set while having no generalization. We study the embeddings at different training times and find that neither PCA (shown in Figure 1) nor t-SNE (not shown here) reveal any structure. Eventually, validation accuracy starts to increase, and perfect generalization coincides with the PCA projecting the embeddings into a circle in 2D. Of course, no</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Left: Evolution of the effective dimension of the embeddings (defined as the exponential of the entropy) during training and evaluated over 100 seeds. Center: Effect of dropout on speeding up generalization. Right: Phase diagram of the transformer architecture. A scan is performed over the weight decay and learning rate of the decoder while the learning rate of the embeddings is kept fixed at 10<sup>−3</sup> (with zero weight decay).</p>
<p>Choice of dimensionality reduction is guaranteed to find any structure, and thus, it is challenging to show explicitly that generalization only occurs when a structure exists. Nevertheless, the fact that, when coupled with the implicit regularization of the optimizer for sparse solutions, such a clear structure appears in a simple PCA so quickly at generalization time suggests that our analysis in the toy setting is applicable here as well. This is also seen in the evolution of the entropy of the explained variance ratio in the PCA of the embeddings (defined as S = −∑<sub>i</sub> σ<sub>i</sub> log σ<sub>i</sub> where σ<sub>i</sub> is the fractional variance explained by the <em>i</em>th principal component). As seen in Figure 7, the entropy increases up to generalization time then decreases drastically afterwards which would be consistent with the conjecture that generalization occurs when a low-dimensional structure is discovered. The decoder then primarily relies on the information in this low-dimensional manifold and essentially "prunes" the rest of the high-dimensional embedding space. Another interesting insight appears when we project the embeddings at initialization onto the principal axes at the end of training. Some of the structure required for generalization exists before training hinting at a connection with the Lottery Ticket Hypothesis. See Appendix K for more details.</p>
<p>In Figure 7 (right), we show a comparable phase diagram to Figure 6 evaluated now in the transformer setting. Note that, as opposed to the setting in [1], weight decay has only been applied to the decoder and not to the embedding layer. Contrary to the toy model, a certain amount of weight decay proves beneficial to generalization and speeds it up significantly. We conjecture that this difference comes from the different embedding dimensions. With a highly over-parameterized setting, a non-zero weight decay gives a crucial incentive to reduce complexity in the decoder and help generalize in fewer steps. This is subject to further investigation. We also explore the effect of dropout layers in the decoder blocks of the transformer. With a significant dropout rate, the generalization time can be brought down to under 10<sup>3</sup> steps and the grokking phenomenon vanishes completely. The overall trend suggests that constraining the decoder with the same tools used to avoid overfitting reduces generalization time and can avoid the grokking phenomenon. This is also observed in an image classification task where we were able to induce grokking. See Appendix J for more details.</p>
<h3>4.3 Grokking Experiment on MNIST</h3>
<p>We now demonstrate, for the first time, that grokking (significantly delayed generalization) is a more general phenomenon in machine learning that can occur not only on algorithmic datasets, but also on mainstream benchmark datasets. In particular, we exhibit grokking on MNIST in Figure 8 and demonstrate that we can control grokking by varying optimization hyperparameters. More details on the experimental setup are in Appendix J.</p>
<h2>5 Related work</h2>
<p>Relatively few works have analyzed the phenomenon of grokking. [2] describe the circuit that transformers use to perform modular addition, track its formation over training, and broadly suggest that grokking is related to the phenomenon of "phase changes" in neural network training. [3, 4]</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Left: Training curves for a run on MNIST, in the setting where we observe grokking. Right: Phase diagram with the four phases of learning dynamics on MNIST.
provided earlier speculative, informal conjectures on grokking [3, 4]. Our work is related to the following broad research directions:
Learning mathematical structures [5] trains a neural network to learn arithmetic operation from pictures of digits, but they do not observe grokking due to their abundant training data. Beyond arithmetic relations, machine learning has been applied to learn other mathematical structures, including geometry [6], knot theory [7] and group theory [8].
Double descent Grokking is somewhat reminiscent of the phenomena of "epoch-wise" double descent [9], where generalization can improve after a period of overfitting. [10] find that regularization can mitigate double descent, similar perhaps to how weight decay influences grokking.
Representation learning Representation learning lies at the core of machine learning [11-14]. Representation quality is usually measured by (perhaps vague) semantic meanings or performance on downstream tasks. In our study, the simplicity of arithmetic datasets allows us to define representation quality and study evolution of representations in a quantitative way.
Physics of learning Physics-inspired tools have proved to be useful in understanding deep learning from a theoretical perspective. These tools include effective theories [15, 16], conservation laws [17] and free energy principle [18]. In addition, statistical physics has been identified as a powerful tool in studying generalization in neural networks [19-22]. Our work connects a low-level understanding of models with their high-level performance. In a recent work, researchers at Anthropic [23], connect a sudden decrease in loss during training with the emergence of induction heads within their models. They analogize their work to statistical physics, since it bridges a "microscopic", mechanistic understanding of networks with "macroscopic" facts about overall model performance.</p>
<h1>6 Conclusion</h1>
<p>We have shown how, in both toy models and general settings, that representation enables generalization when it reflects structure in the data. We developed an effective theory of representation learning dynamics (in a toy setting) which predicts the critical dependence of learning on the training data fraction. We then presented four learning phases (comprehension, grokking, memorization and confusion) which depend on the decoder capacity and learning speed (given by, among other things, learning rate and weight decay) in decoder-only architectures. While we have mostly focused on a toy model, we find preliminary evidence that our results generalize to the setting of [1].
Our work can be viewed as a step towards a statistical physics of deep learning, connecting the "microphysics" of low-level network dynamics with the "thermodynamics" of high-level model behavior. We view the application of theoretical tools from physics, such as effective theories [24], to be a rich area for further work. The broader impact of such work, if successful, could be to make models more transparent and predictable [23, 25, 26], crucial to the task of ensuring the safety of advanced AI systems.</p>
<h1>References</h1>
<p>[1] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
[2] Neel Nanda and Tom Lieberum. A mechanistic interpretability analysis of grokking, 2022. URL https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/ a-mechanistic-interpretability-analysis-of-grokking.
[3] Beren Millidge. Grokking 'grokking'. https://beren.io/ 2022-01-11-Grokking-Grokking/, 2022.
[4] Rohin Shah. Alignment Newsletter #159. https: //www.alignmentforum.org/posts/zvWqPmQasssaAWkrj/ an-159-building-agents-that-know-how-to-experiment-by#DEEP_LEARNING_, 2021.
[5] Yedid Hoshen and Shmuel Peleg. Visual learning of arithmetic operation. In AAAI, 2016.
[6] Yang-Hui He. Machine-learning mathematical structures. arXiv preprint arXiv:2101.06317, 2021.
[7] Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski. Learning to unknot. Machine Learning: Science and Technology, 2(2):025035, 2021.
[8] Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev, Richard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al. Advancing mathematics by guiding human intuition with ai. Nature, 600(7887):70-74, 2021.
[9] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, 2021.
[10] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. arXiv preprint arXiv:2003.01897, 2020.
[11] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8): $1798-1828,2013$.
[12] Yassine Ouali, Céline Hudelot, and Myriam Tami. An overview of deep semi-supervised learning. arXiv preprint arXiv:2006.05278, 2020.
[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271-21284, 2020.
[14] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A framework and review. IEEE Access, 8:193907-193934, 2020.
[15] James Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field theory. Machine Learning: Science and Technology, 2(3):035002, 2021.
[16] Daniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv preprint arXiv:2106.10165, 2021.
[17] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv preprint arXiv:2012.04728, 2020.
[18] Yansong Gao and Pratik Chaudhari. A free-energy principle for representation learning. In International Conference on Machine Learning, pages 3367-3376. PMLR, 2020.</p>
<p>[19] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová. Generalisation error in learning with random features and the hidden manifold model. In International Conference on Machine Learning, pages 3452-3462. PMLR, 2020.
[20] Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale feature learning dynamics: Insights for double descent. In International Conference on Machine Learning, pages 17669-17690. PMLR, 2022.
[21] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. The gaussian equivalence of generative models for learning with shallow neural networks. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, volume 145 of Proceedings of Machine Learning Research, pages 426-471. PMLR, 16-19 Aug 2022. URL https: //proceedings.mlr.press/v145/goldt22a.html.
[22] R Kuhn and S Bos. Statistical mechanics for neural networks with continuous-time dynamics. Journal of Physics A: Mathematical and General, 26(4):831, 1993.
[23] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-inductionheads/index.html.
[24] Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep Learning Theory. Cambridge University Press, 2022. https://deeplearningtheory.com.
[25] Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones, Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, et al. Predictability and surprise in large generative models. arXiv preprint arXiv:2202.07785, 2022.
[26] Jacob Steinhardt. Future ML Systems Will Be Qualitatively Different. https://www. lesswrong.com/s/4aARF2ZoBpFZAhbbe/p/pZaPhGg2hmmPwByHc, 2022.
[27] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017.
[28] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117 (40):24652-24663, 2020.
[29] Wikipedia contributors. Thomson problem - Wikipedia, the free encyclopedia. https://en.wikipedia.org/w/index.php?title=Thomson_problem\&amp;oldid= 1091431454, 2022. [Online; accessed 29-July-2022].
[30] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750-15758, 2021.
[31] Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in frequency domain. In International Conference on Neural Information Processing, pages 264-274. Springer, 2019.
[32] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced by initialization in deep neural networks. In Mathematical and Scientific Machine Learning, pages 144-164. PMLR, 2020.
[33] Ziming Liu, Eric J. Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data, 2022.</p>
<p>[34] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 3635-3673. PMLR, 09-12 Jul 2020. URL https://proceedings.mlr.press/v125/woodworth20a. html.</p>
<h1>Checklist</h1>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]
(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]</li>
<li>If you ran experiments...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] All experiments were run on a workstation with two NVIDIA A6000 GPUs within a few days.</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? $[\mathrm{N} / \mathrm{A}]$
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? $[\mathrm{N} / \mathrm{A}]$</li>
</ol>
<h1>Appendix</h1>
<h2>A Definitions of the phases of learning</h2>
<p>Table 1: Definitions of the four phases of learning</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Phase</th>
<th style="text-align: center;">criteria</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">training acc $&gt;90 \%$ <br> within $10^{5}$ steps</td>
<td style="text-align: center;">validation acc $&gt;90 \%$ <br> within $10^{5}$ steps</td>
<td style="text-align: center;">step(validation acc $&gt;90 \%$ ) <br> $-\operatorname{step}($ training acc $&gt;90 \%)&lt;10^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">Comprehension</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Grokking</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">Memorization</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Not Applicable</td>
</tr>
<tr>
<td style="text-align: center;">Confusion</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Not Applicable</td>
</tr>
</tbody>
</table>
<h2>B Applicability of our toy setting</h2>
<p>In the main paper, we focused on the toy setting with (1) the addition dataset and (2) the addition operation hard coded in the decoder. Although both simplifications appear to have quite limited applicability, we argue below that the analysis of the toy setting can actually apply to all Abelian groups.
The addition dataset is the building block of all Abelian groups A cyclic group is a group that is generated by a single element. A finite cyclic group with order $n$ is $C_{n}=\left{e, g, g^{2}, \cdots, g^{n-1}\right}$ where $e$ is the identify element and $g$ is the generator and $g^{i}=g^{j}$ whenever $i=j(\bmod n)$. The modulo addition and ${0,1, \cdots, n-1}$ form a cyclic group with $e=0$ and $g$ can be any number $q$ coprime to $n$ such that $(q, n)=1$. Since algorithmic datasets contain only symbolic but no arithmetic information, the datasets of modulo addition could apply to all other cyclic groups, e.g., modulo multiplication and discrete rotation groups in 2D.
Although not all Abelian groups are cyclic, a finite Abelian group $G$ can be always decomposed into a direct product of $k$ cyclic groups $G=C_{n_{1}} \times C_{n_{2}} \cdots C_{n_{k}}$. So after training $k$ neural networks with each handling one cyclic group separately, it is easy to construct a larger neural network that handles the whole Abelian group.
The addition operation is valid for all Abelian groups It is proved in [27] that for a permutation invariant function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$, there exists $\rho$ and $\phi$ such that</p>
<p>$$
f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\rho\left[\sum_{i=1}^{n} \phi\left(x_{i}\right)\right]
$$</p>
<p>or $f\left(x_{1}, x_{2}\right)=\rho\left(\phi\left(x_{1}\right)+\phi\left(x_{2}\right)\right)$ for $n=2$. Notice that $\phi\left(x_{i}\right)$ corresponds to the embedding vector $\mathbf{E}<em 1="1">{i}, \rho$ corresponds to the decoder. The addition operator naturally emerges from the commutativity of the operator, not restricting the operator itself to be addition. For example, multiplication of two numbers $x</em>\right)\right)$ where $\rho(x)=\exp (x)$ and $\phi(x)=\ln (x)$.}$ and $x_{2}$ can be written as $x_{1} x_{2}=\exp \left(\ln \left(x_{1}\right)+\ln \left(x_{2</p>
<h2>C An illustrative example</h2>
<p>We use a concrete case to illustrate why parallelograms lead to generalization (see Figure 9). For the purpose of illustration, we exploit a curriculum learning setting, where a neural network is fed with a few new samples each time. We will illustrate that, as we have more samples in the training set, the ideal model $\mathcal{M}^{*}$ (defined in Section 3.2) will arrange the representation $\mathbf{R}^{s}$ in a more structured way, i.e., more parallelograms are formed, which helps generalization to unseen validation samples. For simplicity we choose $p=6$.</p>
<ul>
<li>$D_{1}=(0,4)$ and $D_{2}=(1,3)$ have the same label, so $(0,4,1,3)$ becomes a parallelogram such that $\mathbf{E}<em 4="4">{0}+\mathbf{E}</em>}=\mathbf{E<em 3="3">{1}+\mathbf{E}</em>} \rightarrow \mathbf{E<em 0="0">{3}-\mathbf{E}</em>}=\mathbf{E<em 1="1">{4}-\mathbf{E}</em>=(2,4)$ have} . D_{3}=(1,5)$ and $D_{4</li>
</ul>
<p>the same label, so $(1,5,2,4)$ becomes a parallelogram such that $\mathbf{E}<em 5="5">{1}+\mathbf{E}</em>}=\mathbf{E<em 4="4">{2}+\mathbf{E}</em>} \rightarrow$ $\mathbf{E<em 1="1">{4}-\mathbf{E}</em>}=\mathbf{E<em 2="2">{5}-\mathbf{E}</em>}$. We can derive from the first two equations that $\mathbf{E<em 2="2">{5}-\mathbf{E}</em>}=\mathbf{E<em 0="0">{3}-\mathbf{E}</em>} \rightarrow$ $\mathbf{E<em 5="5">{0}+\mathbf{E}</em>}=\mathbf{E<em 3="3">{2}+\mathbf{E}</em>$, which implies that $(0,5,2,3)$ is also a parallelogram (see Figure 9(a)). This means if $(0,5)$ in training set, our model can predict $(2,3)$ correctly.</p>
<ul>
<li>$D_{5}=(0,2)$ and $D_{6}=(1,1)$ have the same label, so $\mathbf{E}<em 2="2">{0}+\mathbf{E}</em>}=2 \mathbf{E<em 4="4">{1}$, i.e., 1 is the middle point of 0 and 2 (see Figure 9(b)). Now we can derive that $2 \mathbf{E}</em>}=\mathbf{E<em 5="5">{3}+\mathbf{E}</em>$, i.e., 4 is the middle point of 3 and 5 . If $(4,4)$ is in the training data, our model can predict $(3,5)$ correctly.</li>
<li>Finally, $D_{7}=(2,4)$ and $D_{8}=(3,3)$ have the same label, so $2 \mathbf{E}<em 2="2">{3}=\mathbf{E}</em>$.}+\mathbf{E}_{4}$, i.e., 3 should be placed at the middle point of 2 and 4 , ending up Figure 9(c). This linear structure agrees with the arithmetic structure of $\mathbb{R</li>
</ul>
<p>In summary, although we have $p(p+1) / 2=21$ different training samples for $p=6$, we only need 8 training samples to uniquely determine the perfect linear structure (up to linear transformation). The punchline is: representations lead to generalization.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: As we include more data in the training set, the (ideal) model is capable of discovering increasingly structured representations (better RQI), from (a) to (b) to (c).</p>
<h1>D Definition of $\widehat{\mathrm{Acc}}$</h1>
<p>Given a training set $D$ and a representation $\mathbf{R}$, if $(i, j)$ is a validation sample, can the neural network correctly predict its output, i.e., $\operatorname{Dec}\left(\mathbf{E}<em j="j">{i}+\mathbf{E}</em>$ ? Since neural network has never seen $(i, j)$ in the training set, one possible mechanism of induction is through}\right)=\mathbf{Y}_{i+j</p>
<p>$$
\operatorname{Dec}\left(\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}\right)=\operatorname{Dec}\left(\mathbf{E<em n="n">{m}+\mathbf{E}</em>}\right)=\mathbf{Y<em i_j="i+j">{m+n}\left(=\mathbf{Y}</em>\right)
$$</p>
<p>The first equality $\operatorname{Dec}\left(\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}\right)=\operatorname{Dec}\left(\mathbf{E<em n="n">{m}+\mathbf{E}</em>}\right)$ holds only when $\mathbf{E<em j="j">{i}+\mathbf{E}</em>}=\mathbf{E<em n="n">{m}+\mathbf{E}</em>}$ (i.e., $(i, j, m, n)$ is a parallelogram). The second equality $\operatorname{Dec}\left(\mathbf{E<em n="n">{m}+\mathbf{E}</em>$}\right)=\mathbf{Y}_{m+n}$, holds when $(m, n)$ is in the training set, i.e., $(m, n) \in D$, under the zero training loss assumption. Rigorously, given a training set $D$ and a parallelogram set $P$ (which can be calculated from $\mathbf{R}$ ), we collect all zero loss samples in an augmented training set $\bar{D</p>
<p>$$
\bar{D}(D, P)=D \bigcup{(i, j) \mid \exists(m, n) \in D,(i, j, m, n) \in P}
$$</p>
<p>Keeping $D$ fixed, a larger $P$ would probably produce a larger $\bar{D}$, i.e., if $P_{1} \subseteq P_{2}$, then $\bar{D}\left(D, P_{1}\right) \subseteq$ $\bar{D}\left(P, P_{2}\right)$, which is why in Eq. (3) our defined RQI $\propto|P|$ gets its name "representation quality index", because higher RQI normally means better generalization. Finally, the expected accuracy from a dataset $D$ and a parallelogram set $P$ is:</p>
<p>$$
\widehat{\mathrm{Acc}}=\frac{|\bar{D}(D, P)|}{\left|D_{0}\right|}
$$</p>
<p>which is the estimated accuracy (of the full dataset), and $P=P(\mathbf{R})$ is defined on the representation after training. On the other hand, accuracy Acc can be accessed empirically from trained neural network. We verified Acc $\approx \widehat{\mathrm{Acc}}$ in a toy setup (addition dataset $p=10$, 1D embedding space, hard code addition), as shown in Figure 3 (c). Figure 3 (a)(b) show Acc and $\widehat{\mathrm{Acc}}$ as a function of training set ratio, with each dot corresponding to a different random seed. The dashed red diagonal corresponds to memorization of the training set, and the vertical gap refers to generalization.</p>
<p>Although the agreement is good for 1D embedding vectors, we do not expect such agreement can trivially extend to high dimensional embedding vectors. In high dimensions, our definition of RQI is too restrictive. For example, suppose we have an embedding space with $N$ dimensions. Although the representation may form a linear structure in the first dimension, the representation can be arbitrary in other $N-1$ dimensions, leading to RQI $\approx 0$. However, the model may still generalize well if the decoder learns to keep only the useful dimension and drop all other $N-1$ useless dimensions. It would be interesting to investigate how to define an RQI that takes into account the role of decoder in future works.</p>
<h1>E The gap of a realistic model $\mathcal{M}$ and the ideal model $\mathcal{M}^{*}$</h1>
<p>Realistic models $\mathcal{M}$ usually form fewer number of parallelograms than ideal models $\mathcal{M}^{<em>}$. In this section, we analyze the properties of ideal models and calculated ideal RQI and ideal accuracy, which set upper bounds for empirical RQI and accuracy. The upper bound relations are verified via numerical experiments in Figure 10.
Similar to Eq. (12) where some validation samples can be derived from training samples, we demonstrate how implicit parallelograms can be 'derived' from explicit ones in $P_{0}(D)$. The so-called derivation follows a simple geometric argument that: if $A_{1} B_{1}$ is equal and parallel to $A_{2} B_{2}$, and $A_{2} B_{2}$ is equal and parallel to $A_{3} B_{3}$, then we can deduce that $A_{1} B_{1}$ is equal and parallel to $A_{3} B_{3}$ (hence $\left(A_{1}, B_{2}, A_{2}, B_{1}\right)$ is a parallelogram).
Recall that a parallelogram $(i, j, m, n)$ is equivalent to $\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}=\mathbf{E<em n="n">{m}+\mathbf{E}</em>(</em>)$. So we are equivalently asking if equation $(<em>)$ can be expressed as a linear combination of equations in $A\left(P_{0}(D)\right)$. If yes, then $(</em>)$ is dependent on $A\left(P_{0}(D)\right)$ (defined in Eq. (7)), i.e., $A\left(P_{0}(D)\right)$ and $A\left(P_{0}(D) \bigcup(i, j, m, n)\right)$ should have the same rank. We augment $P_{0}(D)$ by adding implicit parallelograms, and denote the augmented parallelogram set as</p>
<p>$$
P(D)=P_{0}(D) \bigcup\left{q \equiv(i, j, m, n) \mid q \in P_{0}, \operatorname{rank}\left(A\left(P_{0}(D)\right)\right)=\operatorname{rank}\left(A\left(P_{0}(D) \bigcup q\right)\right)\right}
$$</p>
<p>We need to emphasize that an assumption behind Eq. (14) is that we have an ideal model $\mathcal{M}^{*}$. When the model is not ideal, e.g., when the injectivity of the encoder breaks down, fewer parallelograms are expected to form, i.e.,</p>
<p>$$
P(R) \subseteq P(D)
$$</p>
<p>The inequality is saying, whenever a parallelogram is formed in the representation after training, the reason is hidden in the training set. This is not a strict argument, but rather a belief that today's neural networks can only copy what datasets (explicitly or implicitly) tell it to do, without any autonomous creativity or intelligence. For simplicity we call this belief Alexander Principle. In very rare cases when something lucky happens (e.g., neural networks are initialized at approximate correct weights), Alexander principle may be violated. Alexander principle sets an upper bound for RQI:</p>
<p>$$
\operatorname{RQI}(R) \leq \frac{|P(D)|}{\left|P_{0}\right|} \equiv \widehat{\mathrm{RQI}}
$$</p>
<p>and sets an upper bound for $\widehat{\text { Acc }}$ :</p>
<p>$$
\widehat{\operatorname{Acc}} \equiv \widehat{\operatorname{Acc}}(D, P(R)) \leq \widehat{\operatorname{Acc}}(D, P(D)) \equiv \widehat{\operatorname{Acc}}
$$</p>
<p>In Figure 10 (c)(f), we verify Eq. (16) and Eq. (17). We choose $\delta=0.01$ to compute RQI $(R, \delta)$. We find the trained models are usually far from being ideal, although we already include a few useful tricks proposed in Section 4 to enhance representation learning. It would be an interesting future direction to develop better algorithms so that the gap due to Alexander principle can be reduced or even closed. In Figure 10 (a)(b)(d)(e), four quantities (RQI, $\widehat{\mathrm{RQI}}$, Acc, $\widehat{\mathrm{Acc}}$ ) as functions of the training data fraction are shown, each dot corresponding to one random seed. It is interesting to note that it is possible to have $\widehat{\mathrm{RQI}}=1$ only with $&lt;40 \%$ training data, i.e., $55 \times 0.4=22$ samples, agreeing with our observation in Section 3.
Realistic representations Suppose an ideal model $\mathcal{M}^{<em>}$ and a realistic model $\mathcal{M}$ which train on the training set $D$ give the representation $R^{</em>}$ and $R$, respectively. What is the relationship between $R$ and $R_{<em>}$ ? Due to the Alexander principle we know $P(R) \subseteq P(D)=P\left(R^{</em>}\right)$. This means $R^{<em>}$ has more parallelograms than $R$, hence $R^{</em>}$ has fewer degrees of freedom than $R$.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: We compare RQI and Acc for an ideal algorithm (with bar) and a realistic algorithm (without bar). In (a)(b)(d)(e), four quantities (RQI, $\overline{\mathrm{RQI}}$, Acc, $\overline{\mathrm{Acc}}$ ) as functions of training data fraction are shown. In (c)(f), RQI and Acc of the ideal algorithm sets upper bounds for those of the realistic algorithm.</p>
<p>We illustrate with the toy case $p=4$. The whole dataset contains $p(p+1) / 2=10$ samples, i.e.,</p>
<p>$$
D_{0}={(0,0),(0,1),(0,2),(0,3),(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)}
$$</p>
<p>The parallelogram set contains only three elements, i.e.,</p>
<p>$$
P_{0}={(0,1,1,2),(0,1,2,3),(1,2,2,3)}
$$</p>
<p>Or equivalently the equation set</p>
<p>$$
A_{0}=\left{\mathrm{A} 1: \mathbf{E}<em 2="2">{0}+\mathbf{E}</em>}=2 \mathbf{E<em 0="0">{1}, \mathrm{~A} 2: \mathbf{E}</em>}+\mathbf{E<em 1="1">{3}=\mathbf{E}</em>}+\mathbf{E<em 1="1">{2}, \mathrm{~A} 3: \mathbf{E}</em>}+\mathbf{E<em 2="2">{3}=2 \mathbf{E}</em>\right}
$$</p>
<p>Pictorially, we can split all possible subsets $\left{A \mid A \subseteq A_{0}\right}$ into different levels, each level defined by $|A|$ (the number of elements). A subset $A_{1}$ in the $i^{\text {th }}$ level points an direct arrow to another subset $A_{2}$ in the $(i+1)^{\text {th }}$ level if $A_{2} \subset A_{1}$, and we say $A_{2}$ is a child of $A_{1}$, and $A_{1}$ is a parent of $A_{2}$. Each subset $A$ can determine a representation $R$ with $n(A)$ degrees of freedom. So $R$ should be a descendant of $R_{<em>}$, and $n\left(R_{</em>}\right) \leq n(R)$. Numerically, $n(A)$ is equal to the dimension of the null space of $A$.
Suppose we have a training set</p>
<p>$$
D={(0,2),(1,1),(0,3),(1,2),(1,3),(2,2)}
$$</p>
<p>and correspondingly $P(D)=P_{0}, A(P)=A_{0}$. So an ideal model $\mathcal{M}<em k="k">{*}$ will have the linear structure $\mathbf{E}</em>$ may produce any descendants of the linear structure, depending on various hyperparameters and even random seeds.}=\mathbf{a}+k \mathbf{b}$ (see Figure 11 leftmost). However, a realistic model $\mathcal{M</p>
<p>In Figure 12, we show our algorithms actually generates all possible representations. We have two settings: (1) fast decoder $\left(\eta_{1}, \eta_{2}\right)=\left(10^{-3}, 10^{-2}\right)$ (Figure 12 left), and (2) relatively slow decoder $\left(\eta_{1}, \eta_{2}\right)=\left(10^{-2}, 10^{-3}\right)$ (Figure 12) right). The relatively slow decoder produces better representations (in the sense of higher RQI) than a fast decoder, agreeing with our observation in Section 4.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: $p=4$ case. Equation set $A$ (or geometrically, representation) has a hierarchy: $a \rightarrow b$ means $a$ is a parent of $b$, and $b$ is a child of $a$. A realistic model can only generate representations that are descendants of the representation generated by an ideal model.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: $p=4$ case. Representations obtained from training neural networks are displayed. $\eta_{1}$ and $\eta_{2}$ are learning rates of the representation and the decoder, respectively. As described in the main text, $\left(\eta_{1}, \eta_{2}\right)=\left(10^{-2}, 10^{-3}\right)$ (right) is more ideal than $\left(\eta_{1}, \eta_{2}\right)=\left(10^{-3}, 10^{-2}\right)$ (left), thus producing representations containing more parallelograms.</p>
<h1>F Conservation laws of the effective theory</h1>
<p>Recall that the effective loss function</p>
<p>$$
\ell_{\text {eff }}=\frac{\ell_{0}}{Z_{0}}, \quad \ell_{0} \equiv \sum_{(i, j, m, n) \in P_{0}(D)}\left|\mathbf{E}<em j="j">{i}+\mathbf{E}</em>}-\mathbf{E<em n="n">{m}-\mathbf{E}</em>
$$}\right|^{2} /\left|P_{0}(D)\right|, \quad Z_{0} \equiv \sum_{k}\left|\mathbf{E}_{k}\right|^{2</p>
<p>where $\ell_{0}$ and $Z_{0}$ are both quadratic functions of $R=\left{\mathbf{E}<em p-1="p-1">{0}, \cdots, \mathbf{E}</em>}\right}$, and $\ell_{\text {eff }}=0$ remains zero under rescaling and translation $\mathbf{E<em i="i">{i}^{\prime}=a \mathbf{E}</em>$ evolves according to the gradient descent}+\mathbf{b}$. We will ignore the $1 /\left|P_{0}(D)\right|$ factor in $\ell_{0}$ since having it is equivalent to rescaing time, which does not affect conservation laws. The representation vector $\mathbf{E}_{i</p>
<p>$$
\frac{d \mathbf{E}<em _mathrm_eff="\mathrm{eff">{i}}{d t}=-\frac{\partial \ell</em>
$$}}}{\partial \mathbf{E}_{i}</p>
<p>We will prove the following two quantities are conserved:</p>
<p>$$
\mathbf{C}=\sum_{k} \mathbf{E}<em 0="0">{k}, \quad Z</em>
$$}=\sum_{k}\left|\mathbf{E}_{k}\right|^{2</p>
<p>Eq. (22) and Eq. (23) give</p>
<p>$$
\frac{d \mathbf{E}<em _mathrm_eff="\mathrm{eff">{i}}{d t}=-\frac{\ell</em>}}}{\partial \mathbf{E<em 0="0">{i}}=-\frac{\partial\left(\frac{\ell</em>}}{Z_{0}}\right)}{\partial \mathbf{E<em 0="0">{i}}=-\frac{1}{Z</em>}} \frac{\partial \ell_{0}}{\partial \mathbf{E<em 0="0">{i}}+\frac{\ell</em>
$$}}{Z_{0}^{2}} \frac{\partial Z_{0}}{\partial \mathbf{E}_{i}</p>
<p>Then</p>
<p>$$
\begin{aligned}
\frac{d Z_{0}}{d t} &amp; =2 \sum_{i} \mathbf{E}<em k="k">{k} \cdot \frac{d \mathbf{E}</em> \
&amp; =\frac{2}{Z_{0}^{2}} \sum_{i} \mathbf{E}}}{d t<em 0="0">{i} \cdot\left(-Z</em>} \frac{\partial \ell_{0}}{\partial \mathbf{E<em 0="0">{k}}+2 \ell</em>} \mathbf{E<em 0="0">{k}\right) \
&amp; =\frac{2}{Z</em>}}\left(-\sum_{k} \frac{\partial \ell_{0}}{\partial \mathbf{E<em k="k">{k}} \cdot \mathbf{E}</em>\right) \
&amp; =0
\end{aligned}
$$}+2 \ell_{0</p>
<p>where the last equation uses the fact that</p>
<p>$$
\begin{aligned}
\sum_{k} \frac{\partial \ell_{0}}{\partial \mathbf{E}<em k="k">{k}} \cdot \mathbf{E}</em>} &amp; =2 \sum_{k} \sum_{(i, j, m, n) \in P_{0}(D)}\left(\mathbf{E<em j="j">{i}+\mathbf{E}</em>}-\mathbf{E<em n="n">{m}-\mathbf{E}</em>}\right)\left(\delta_{i k}+\delta_{j k}-\delta_{m k}-\delta_{n k}\right) \cdot \mathbf{E<em P__0="P_{0" _i_="(i," _in="\in" j_="j," m_="m," n_="n)">{k} \
&amp; =2 \sum</em>}(D)}\left(\mathbf{E<em j="j">{i}+\mathbf{E}</em>}-\mathbf{E<em n="n">{m}-\mathbf{E}</em>}\right) \sum_{k}\left(\delta_{i k}+\delta_{j k}-\delta_{m k}-\delta_{n k}\right) \cdot \mathbf{E<em P__0="P_{0" _i_="(i," _in="\in" j_="j," m_="m," n_="n)">{k} \
&amp; =\sum</em>}(D)}\left(\mathbf{E<em j="j">{i}+\mathbf{E}</em>}-\mathbf{E<em n="n">{m}-\mathbf{E}</em>}\right) \cdot\left(\mathbf{E<em j="j">{i}+\mathbf{E}</em>}-\mathbf{E<em n="n">{m}-\mathbf{E}</em>\right) \
&amp; =2 \ell_{0}
\end{aligned}
$$</p>
<p>The conservation of $Z_{0}$ prohibits the representation from collapsing to zero. Now that we have demonstrated that $Z_{0}$ is a conserved quantity, we can also show</p>
<p>$$
\begin{aligned}
\frac{d \mathbf{C}}{d t} &amp; =\sum_{k} \frac{d \mathbf{E}<em 0="0">{k}}{d t} \
&amp; =-\frac{1}{Z</em>}} \sum_{k} \frac{\partial \ell_{0}}{\partial \mathbf{E<em 0="0">{k}} \
&amp; =-\frac{2}{Z</em>}} \sum_{k} \sum_{(i, j, m, n) \in P_{0}(D)}\left(\mathbf{E<em j="j">{i}+\mathbf{E}</em>}-\mathbf{E<em n="n">{m}-\mathbf{E}</em>\right) \
&amp; =\mathbf{0}
\end{aligned}
$$}\right)\left(\delta_{i k}+\delta_{j k}-\delta_{m k}-\delta_{n k</p>
<p>The last equality holds because the two summations can be swapped and $\sum_{k}\left(\delta_{i k}+\delta_{j k}-\delta_{m k}-\delta_{n k}\right)=$ 0 .</p>
<h1>G More phase diagrams of the toy setup</h1>
<p>We study another three hyperparameters in the toy setup by showing phase diagrams similar to Figure 6. The toy setup is: (1) addition without modulo $(p=10)$; (2) training/validation is split into 45/10; (3) hard code addition; (4) 1D embedding. In the following experiments, the decoder is an MLP with size 1-200-200-30. The representation and the encoder are optimized with AdamW with different hyperparameters. The learning rate of the representation is $10^{-3}$. We sweep the learning rate of the decoder in range $\left[10^{-4}, 10^{-2}\right]$ as the x axis, and sweep another hyperparameter as the y axis. By default, we use full batch size 45, initialization scale $s=1$ and zero weight decay of representation.</p>
<p>Batch size controls the amount of noise in the training dynamics. In Figure 13, the grokking region appears at the top left of the phase diagram (small decoder learning rate and small batch size). However, large batch size (with small learning rate) leads to comprehension, implying that smaller batch size seems harmful. This makes sense since to get crystals (good structures) in experiments, one needs a freezer which gradually decreases temperature, rather than something perturbing the system with noise.</p>
<p>Initialization scale controls distances among embedding vectors at initialization. We initialize components of embedding vectors from independent uniform distribution $U[-s / 2, s / 2]$ where $s$</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Phase diagrams of decoder learning rate (x axis) and batch size (y axis) for the addition group (left: regression; right: classification). Small decoder leanrning rate and large batch size (bottom left) lead to comprehension.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Phase diagrams of decoder learning rate (x axis) and initialization (y axis) for the addition group (left: regression; right: classification). Small intialization scale (top) leads to comprehension.
is called the initialization scale. Shown in Figure 14, it is beneficial to use a smaller initialization scale. This agrees with the physical intuition that closer particles are more likely to interact and form structures. For example, the distances among molecules in ice are much smaller than distances in gas.</p>
<p>Representation weight decay controls the magnitude of embedding vectors. Shown in Figure 15, we see the representation weight decay in general does not affect model performance much.</p>
<h1>H General groups</h1>
<h2>H. 1 Theory</h2>
<p>We focused on Abelian groups for the most part of the paper. This is, however, simply due to pedagogical reasons. In this section, we show that it is straight-forward to extend definitions of parallelograms and representation quality index (RQI) to general non-Abelian groups. We will also show that most (if not all) qualitative results for the addition group also apply to the permutation group.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Phase diagrams of decoder learning rate (x axis) and representation weight decay (y axis) for the addition group (left: regression; right: classification). Representation weight decay does not affect model performance much.</p>
<p>Matrix representation for general groups Let us first review the definition of group representation. A representation of a group $G$ on a vector space $V$ is a group homomorphism from $G$ to $\mathrm{GL}(V)$, the general linear group on $V$. That is, a representation is a map $\rho: G \rightarrow \mathrm{GL}(V)$ such that</p>
<p>$$
\rho\left(g_{1} g_{2}\right)=\rho\left(g_{1}\right) \rho\left(g_{2}\right), \quad \forall g_{1}, g_{2} \in G
$$</p>
<p>In the case $V$ is of finite dimension $n$, it is common to identify $\mathrm{GL}(V)$ with $n$ by $n$ invertible matrices. The punchline is that: each group element can be represented as a matrix, and the binary operation is represented as matrix multiplication.
A new architecture for general groups Inspired by the matrix representation, we embed each group element $a$ as a learnable matrix $\mathbf{E}<em a="a">{a} \in \mathbb{R}^{d \times d}$ (as opposed to a vector), and manually do matrix multiplication before sending the product to the deocder for regression or classification. More concretly, for $a \circ b=c$, our architecture takes as input two embedding matrices $\mathbf{E}</em>}$ and $\mathbf{E<em c="c">{b}$ and aims to predict $\mathbf{Y}</em>}$ such that $\mathbf{Y<em a="a">{c}=\operatorname{Dec}\left(\mathbf{E}</em>} \mathbf{E<em a="a">{b}\right)$, where $\mathbf{E}</em>} \mathbf{E<em a="a">{b}$ means the matrix multiplication of $\mathbf{E}</em>}$ and $\mathbf{E<em a="a">{b}$. The goal of this simplication is to disentangle learning the representation and learning the arithmetic operation (i.e, the matrix multiplication). We will show that, even with this simplification, we are still able to reproduce the characteristic grokking behavior and other rich phenomenon.
Generalized parallelograms we define generalized parallelograms: $(a, b, c, d)$ is a generalized parallelogram in the representation if $\left|\mathbf{E}</em>} \mathbf{E<em c="c">{b}-\mathbf{E}</em>} \mathbf{E<em F="F">{d}\right|</em> \leq \delta$, where $\delta&gt;0$ is a threshold to tolerate numerical errors. Before presenting the numerical results for the permutation group, we show an intuitive picture about how new parallelograms can be deduced from old ones for general groups, which is the key to generalization.
Deduction of parallelograms We first recall the case of the Abelian group (e.g., addition group). As shown in Figure 16, when $(a, d, b, c)$ and $(c, f, d, e)$ are two parallelograms, we have}^{2</p>
<p>$$
\begin{aligned}
&amp; \mathbf{E}<em d="d">{a}+\mathbf{E}</em>}=\mathbf{E<em c="c">{b}+\mathbf{E}</em> \
&amp; \mathbf{E}<em f="f">{c}+\mathbf{E}</em>}=\mathbf{E<em d="d">{d}+\mathbf{E}</em>
\end{aligned}
$$</p>
<p>We can derive that $\mathbf{E}<em f="f">{a}+\mathbf{E}</em>}=\mathbf{E<em e="e">{b}+\mathbf{E}</em>$ implying that $(a, f, b, e)$ is also a parallelogram. That is, for Abelian groups, two parallelograms are needed to deduce a new parallelogram.
For the non-Abelian group, if we have only two parallelograms such that</p>
<p>$$
\begin{aligned}
&amp; \mathbf{E}<em d="d">{a} \mathbf{E}</em>}=\mathbf{E<em c="c">{b} \mathbf{E}</em> \
&amp; \mathbf{E}<em c="c">{f} \mathbf{E}</em>}=\mathbf{E<em d="d">{e} \mathbf{E}</em>
\end{aligned}
$$</p>
<p>we have $\mathbf{E}<em a="a">{b}^{-1} \mathbf{E}</em>}=\mathbf{E<em d="d">{c} \mathbf{E}</em>}^{-1}=\mathbf{E<em e="e">{f}^{-1} \mathbf{E}</em>}$, but this does not lead to something like $\mathbf{E<em a="a">{f} \mathbf{E}</em>}=\mathbf{E<em b="b">{e} \mathbf{E}</em>$, hence useless for generalization. However, if we have a third parallelogram such that</p>
<p>$$
\mathbf{E}<em h="h">{e} \mathbf{E}</em>}=\mathbf{E<em g="g">{f} \mathbf{E}</em>
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Project code can be found at: https://github.com/ejmichaud/grokking-squared&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1326 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1326</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1326</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-227162315</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2011.12421v1.pdf" target="_blank">Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning models are notoriously data hungry, yet real-world data is expensive and time consuming to obtain. The solution that many have turned to is to use simulation for training before deploying the robot in a real environment. Simulation offers the ability to train large numbers of robots in parallel, and offers an abundance of data. However, no simulation is perfect, and robots trained solely in simulation fail to generalize to the real-world, resulting in a"sim-vs-real gap". How can we overcome the trade-off between the abundance of less accurate, artificial data from simulators and the scarcity of reliable, real-world data? In this paper, we propose Bi-directional Domain Adaptation (BDA), a novel approach to bridge the sim-vs-real gap in both directions -- real2sim to bridge the visual domain gap, and sim2real to bridge the dynamics domain gap. We demonstrate the benefits of BDA on the task of PointGoal Navigation. BDA with only 5k real-world (state, action, next-state) samples matches the performance of a policy fine-tuned with ~600k samples, resulting in a speed-up of ~120x.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1326.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1326.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat-Sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat-Sim (Habitat: A Platform for Embodied AI Research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance photorealistic simulator for embodied agents providing RGB-D sensors, GPS+Compass, physics interactions and large-scale environment rendering used to train reinforcement learning navigation agents at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Habitat: A Platform for Embodied AI Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Habitat-Sim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Photorealistic 3D simulation platform for embodied AI that renders RGB and depth sensors, provides localization (GPS+Compass), supports physics interactions and is used with the Habitat API to run large-scale RL training (distributed DD-PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / embodied navigation / mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high visual fidelity (photorealistic rendering) with approximate physics/dynamics (simulated actuation and collision models that can be augmented with noise models).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>High-fidelity visual rendering of 3D scans and meshes; provides RGB-D streams and GPS+Compass; supports collision detection and action execution; has simulator-specific behaviors (e.g. 'sliding' along obstacles by default) and allows injection of sensor and actuation noise models; dynamics are approximate and can be deterministic unless noise is added.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>PointGoal navigation policy (π_100M Gibson no noise, π_BDA-5k Gibson OA+DA, π_1M Gibson O+D, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning policy trained with DD-PPO; visual encoder is ResNet-50 and policy uses a 2-layer LSTM; trained at large scale (base policies trained for 100M steps using many GPUs).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal Navigation (navigate from start to goal using egocentric RGB-D observations without a map).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Base policies trained for 100M steps to convergence; when a π_100M Gibson no-noise policy was evaluated in the LAB target, it achieved 0.84 SPL in the no-noise LAB condition (see paper for more per-condition metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Virtualized LAB environment (Matterport-derived mesh in Habitat) and other target Habitat environments with injected sensor/dynamics noise; intended as stand-in for transfer to real-world robot (LoCoBot).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>When tested across noisy target settings, performance drops substantially for policies trained without noise (e.g., SPL from 0.84 → 0.04 under observation noise); using Bi-directional Domain Adaptation (BDA) in Habitat with 5k target samples matched performance of an oracle finetuned on far more data (see BDA entry).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper compares a 'source' high-visual-fidelity/no-noise simulator (Gibson no-noise in Habitat) versus 'target' Habitat environments with added observation and dynamics noise. Results show large drops in transfer performance when target includes realistic observation noise (e.g., SPL 0.84→0.04) and moderate drops with dynamics noise (e.g., SPL 0.84→0.56), indicating visual fidelity/noise modeling is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that photorealistic visual fidelity alone is insufficient; modeling realistic sensor noise and actuation/dynamics noise is important for transfer. They disabled simulator 'sliding' to improve sim2real predictivity and discuss that small amounts of realistic dynamics noise can be necessary to reproduce real-world behaviors (e.g., freeing agents stuck on obstacles).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Policies trained in Habitat without realistic observation noise failed catastrophically when evaluated in target environments with observation noise; default simulator sliding can produce unrealistic behaviors (cheating) and must be turned off for better real-world predictivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1326.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1326.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson env</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson Environment (Gibson env: Real-world perception for embodied agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of reconstructed real-world indoor environments (3D meshes / scans) used as training environments within Habitat to provide realistic scenes and layout diversity for embodied agent training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gibson env: Real-world perception for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gibson (used as source environment in Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Collection of 3D scanned indoor spaces (meshes and photorealistic textures) used as training environments for embodied agents inside simulators like Habitat; used here as the 'source' simulator dataset with no added noise.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / embodied navigation / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high visual fidelity on scene geometry and textures (real-world scans); dynamics are provided by the hosting simulator (Habitat) and are approximate.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Real scanned geometry and textures from 572 indoor scenes; realistic layout complexity and clutter; sensor rendering consistent with real RGB-D cameras; dynamics fidelity depends on Habitat and any injected noise models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Same PointGoal navigation policies (initialized and trained in Gibson environments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agent (DD-PPO) using ResNet50 visual encoder + LSTM trained on Gibson scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal Navigation trained across Gibson scenes to learn navigation behaviors transferable to a target LAB environment.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Policies trained in Gibson for 100M steps; achieve high in-simulation performance (used as base policies for transfer experiments). Specific per-simulation training metrics are reported via downstream test SPLs (e.g., 0.84 SPL when transferring to LAB no-noise).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Target Habitat environments created from Matterport Pro2 scans of a real lab (LAB) with injected observation and/or dynamics noise; intended as a proxy for real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Direct zero-shot transfer from Gibson (no-noise) to noisy LAB conditions resulted in large performance drops (e.g., observation noise: SPL 0.84→0.04; dynamics noise: SPL 0.84→0.56).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Training on Gibson with added observation+dynamics noise (Gibson O+D) improved robustness but did not generalize cleanly to no-noise LAB; training-target fidelity mismatch causes failures in both directions.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper suggests training should include representative noise models (especially sensor noise) rather than relying solely on photorealistic geometry/textures; adding dynamics noise can help avoid simulator artifacts (e.g., getting unstuck).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Gibson-trained policies without realistic observation noise fail under target observation noise; conversely, policies trained with heavy noise sometimes fail in no-noise target settings (getting stuck less likely when dynamics noise present).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1326.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1326.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim_DA (neural-augmented simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DA-augmented Simulator (Sim_DA) / neural-augmented simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulator augmented by a learned dynamics-adaptation module (DA) that predicts residuals between simulator transitions and real-world transitions and resets simulator state to predicted real outcomes, enabling training of policies on trajectories reflecting expected real dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Sim_DA (Simulator augmented with sim2real Dynamics Adaptor)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>The source simulator (Habitat/Gibson) augmented at each step with a learned 3-layer MLP (DA) that predicts the residual change in robot state (Δs_real) given simulator state and action; used to reset simulator next-state to the DA-predicted real next-state during training rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / embodied navigation / dynamics modeling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium: base simulator physics augmented by a learned residual model to increase realism of state transitions; fidelity depends on quality and quantity of real data used to train the residual.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Augments simulator by applying a learned residual correction to position and orientation transitions (Δx, Δy, Δθ); trained from collected (s_real, a_real, s_real_next) samples using weighted MSE; supports resetting simulator state to predicted real next-state to expose policy to realistic trajectories; does not change low-level physics solver but corrects transitions at state level.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Dynamics Adaptor (DA) + RL policy finetuned in Sim_DA (π_Sim_DA, called π_BDA-5k when used with OA and 5k samples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DA is a 3-layer multilayer perceptron regression network that predicts residual Δs_real; the policy is the same DD-PPO RL agent (ResNet50 + 2-layer LSTM) fine-tuned in the augmented simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Improve transfer of navigation policies by matching simulated dynamics to observed real transitions (residual dynamics prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>DA trained on varying numbers of target-domain samples (100–5000); authors report that finetuning the policy in Sim_DA with DA trained from 5,000 real samples yields strong transfer performance (see transfer_performance).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Transfer of policies trained in Sim_DA back to the target domain (virtualized LAB with noise, and intended real-world deployment), with OA applied at test time for observation adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Using DA trained with 5,000 target samples and OA, the BDA pipeline (Sim_DA + OA) matched the performance of direct finetuning in the target environment (oracle) while using far fewer target samples: e.g., BDA with 5k samples matched oracle performance that would otherwise require ~585k–1,000,000 target steps in different experiments; reported sample-efficiency speed-ups include 36×, 117× and an average 61× reduction in required real/target data.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Adding DA (sim2real residual) substantially improves transfer relative to training in the base simulator without dynamics correction; policies fine-tuned via Sim_DA generalize much better to target noisy environments than policies finetuned directly only on policy parameters with limited real data.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that adapting simulator dynamics via a residual model is a sample-efficient way to increase effective fidelity for transfer, implying that full high-fidelity physics is not strictly necessary if residuals correcting key state transitions are learned from modest real data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When DA is trained with very little or no representative target data (e.g., small sample counts), augmentation may be insufficient; paper shows majority of gains from first 1,000 target samples and saturating by 5,000, indicating failure modes when samples << 1000.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1326.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1326.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OA (real2sim CycleGAN adaptor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observation Adaptor (OA) — real2sim CycleGAN-based image translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CycleGAN-based real-to-sim image translation module that maps real RGB-D observations to simulated-looking observations at test time, thereby closing the visual domain gap without paired data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>real2sim observation adaptor applied to Habitat observations</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Not a simulator per se, but a learned image-to-image translation module (CycleGAN) that converts real/target RGB-D images into the visual domain of the source simulator so the policy trained in sim sees familiar images at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / perception / domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>n/a (perceptual fidelity adapter): improves perceptual alignment with simulator visuals; does not change physical dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Learns mappings G_sim: I_real→I_sim and inverse G_real: I_sim→I_real with adversarial discriminators and cycle-consistency; removes Gaussian and depth noise signatures and smooths textures to match simulator visuals; trained on unpaired images.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Observation Adaptor (OA) used together with navigation policy (π_BDA-5k Gibson OA+DA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CycleGAN-style image translation networks (generators and discriminators); trained on unpaired RGB-D images collected by behavior policy in sim and target.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Close visual domain gap so a policy trained in simulation can operate on target/real visual inputs by translating them to simulator-like images.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>OA trained for 200 epochs in experiments; quantitative improvement in Fréchet Inception Distance (FID) from I_real→I_sim: FID 100.74 reduced to 83.05 after OA; for simulated Gaussian noise case FID improved from 98.73→88.44 after OA.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Applied at test time to LAB real/target images (or other target images) before feeding into policy trained in sim or Sim_DA.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Using OA together with DA (BDA pipeline) and only 5,000 target-domain samples, policies matched oracle finetuning performance in many noisy conditions (average SPL within ~5% of oracle) and dramatically reduced required real/target data (reported speed-ups up to 117× in specific settings, average 61×).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>N/A (OA is an adapter rather than a simulator fidelity level); empirical results show OA reduces perceptual domain gap as measured by FID and recovers much of the lost SPL when combined with DA.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors motivate real2sim (rather than sim2real) observation adaptation to decouple sensing from acting so policies need not be re-trained when sensors change — implying that matching perceptual statistics to training sim is a minimal necessary intervention for vision transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>OA alone cannot handle dynamics mismatch; OA without DA does not address actuation/dynamics discrepancy and thus may fail when dynamics are primary source of sim2real gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1326.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1326.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAB (virtualized Matterport target)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LAB — Virtualized real lab environment (Matterport Pro2 scan imported into Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A photorealistic target environment built from Matterport Pro2 3D scans of a real laboratory, imported into Habitat and used as the target/‘real’ domain for sim2sim experiments with added sensor and actuation noise models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>LAB (virtualized Matterport-derived environment in Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>3D mesh replica of an actual lab collected with a Matterport Pro2 camera, imported into Habitat to create a photorealistic target environment; used to test transfer from source Gibson simulator and to emulate the real-world.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / embodied navigation / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high visual fidelity of a specific real space (scan-derived); dynamics fidelity depends on Habitat physics and added noise models to mimic real sensors/actuators.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Accurate scene geometry and textures from Matterport scans; authors vary obstacle placements to create room configurations of different difficulty; sensor noise (RGB and Depth) and actuation noise (translational and rotational) are injected to emulate real-world imperfections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Target evaluation environment for navigation policies (policies trained in Gibson/Habitat and Sim_DA are tested here).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A (environment used for testing and for collecting target rollouts to train OA and DA).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Test/target domain for PointGoal navigation transfer experiments; stand-in for real lab deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>This is the transfer target (i.e., policies trained in source Gibson/Habitat are transferred to LAB); also used as source of 'real' samples for training OA/DA.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Policies trained in source sim and tested in LAB display large sensitivity to added noise; BDA (OA+DA) with 5k samples from LAB achieves transfer performance comparable to policies finetuned with orders of magnitude more LAB data (e.g., matching oracle finetune trained with hundreds of thousands of steps in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>LAB was used with multiple combinations of observation and dynamics noise (Table I) to compare transfer; observation noise had biggest negative impact, dynamics noise moderate; combining both heavily degrades performance for non-adapted policies.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>LAB experiments demonstrate that modeling observation noise is essential for preserving transfer; realistic target stimuli (scanned environment) combined with appropriate noise injection provides a useful proxy for real robot transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Policies trained without matching the LAB observation noise failed (very low SPL); policies trained with dynamics noise but not observation adaptation still fail under severe observation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real predictivity: Does evaluation in simulation predict real-world performance <em>(Rating: 2)</em></li>
                <li>Habitat: A Platform for Embodied AI Research <em>(Rating: 2)</em></li>
                <li>Gibson env: Real-world perception for embodied agents <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer with neural-augmented robot simulation <em>(Rating: 2)</em></li>
                <li>Vr-goggles for robots: Real-to-sim domain adaptation for visual control <em>(Rating: 1)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 1)</em></li>
                <li>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1326",
    "paper_id": "paper-227162315",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Habitat-Sim",
            "name_full": "Habitat-Sim (Habitat: A Platform for Embodied AI Research)",
            "brief_description": "A high-performance photorealistic simulator for embodied agents providing RGB-D sensors, GPS+Compass, physics interactions and large-scale environment rendering used to train reinforcement learning navigation agents at scale.",
            "citation_title": "Habitat: A Platform for Embodied AI Research",
            "mention_or_use": "use",
            "simulator_name": "Habitat-Sim",
            "simulator_description": "Photorealistic 3D simulation platform for embodied AI that renders RGB and depth sensors, provides localization (GPS+Compass), supports physics interactions and is used with the Habitat API to run large-scale RL training (distributed DD-PPO).",
            "scientific_domain": "robotics / embodied navigation / mechanics",
            "fidelity_level": "medium-to-high visual fidelity (photorealistic rendering) with approximate physics/dynamics (simulated actuation and collision models that can be augmented with noise models).",
            "fidelity_characteristics": "High-fidelity visual rendering of 3D scans and meshes; provides RGB-D streams and GPS+Compass; supports collision detection and action execution; has simulator-specific behaviors (e.g. 'sliding' along obstacles by default) and allows injection of sensor and actuation noise models; dynamics are approximate and can be deterministic unless noise is added.",
            "model_or_agent_name": "PointGoal navigation policy (π_100M Gibson no noise, π_BDA-5k Gibson OA+DA, π_1M Gibson O+D, etc.)",
            "model_description": "Reinforcement learning policy trained with DD-PPO; visual encoder is ResNet-50 and policy uses a 2-layer LSTM; trained at large scale (base policies trained for 100M steps using many GPUs).",
            "reasoning_task": "PointGoal Navigation (navigate from start to goal using egocentric RGB-D observations without a map).",
            "training_performance": "Base policies trained for 100M steps to convergence; when a π_100M Gibson no-noise policy was evaluated in the LAB target, it achieved 0.84 SPL in the no-noise LAB condition (see paper for more per-condition metrics).",
            "transfer_target": "Virtualized LAB environment (Matterport-derived mesh in Habitat) and other target Habitat environments with injected sensor/dynamics noise; intended as stand-in for transfer to real-world robot (LoCoBot).",
            "transfer_performance": "When tested across noisy target settings, performance drops substantially for policies trained without noise (e.g., SPL from 0.84 → 0.04 under observation noise); using Bi-directional Domain Adaptation (BDA) in Habitat with 5k target samples matched performance of an oracle finetuned on far more data (see BDA entry).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Paper compares a 'source' high-visual-fidelity/no-noise simulator (Gibson no-noise in Habitat) versus 'target' Habitat environments with added observation and dynamics noise. Results show large drops in transfer performance when target includes realistic observation noise (e.g., SPL 0.84→0.04) and moderate drops with dynamics noise (e.g., SPL 0.84→0.56), indicating visual fidelity/noise modeling is critical.",
            "minimal_fidelity_discussion": "Authors argue that photorealistic visual fidelity alone is insufficient; modeling realistic sensor noise and actuation/dynamics noise is important for transfer. They disabled simulator 'sliding' to improve sim2real predictivity and discuss that small amounts of realistic dynamics noise can be necessary to reproduce real-world behaviors (e.g., freeing agents stuck on obstacles).",
            "failure_cases": "Policies trained in Habitat without realistic observation noise failed catastrophically when evaluated in target environments with observation noise; default simulator sliding can produce unrealistic behaviors (cheating) and must be turned off for better real-world predictivity.",
            "uuid": "e1326.0",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Gibson env",
            "name_full": "Gibson Environment (Gibson env: Real-world perception for embodied agents)",
            "brief_description": "A dataset of reconstructed real-world indoor environments (3D meshes / scans) used as training environments within Habitat to provide realistic scenes and layout diversity for embodied agent training.",
            "citation_title": "Gibson env: Real-world perception for embodied agents",
            "mention_or_use": "use",
            "simulator_name": "Gibson (used as source environment in Habitat)",
            "simulator_description": "Collection of 3D scanned indoor spaces (meshes and photorealistic textures) used as training environments for embodied agents inside simulators like Habitat; used here as the 'source' simulator dataset with no added noise.",
            "scientific_domain": "robotics / embodied navigation / perception",
            "fidelity_level": "high visual fidelity on scene geometry and textures (real-world scans); dynamics are provided by the hosting simulator (Habitat) and are approximate.",
            "fidelity_characteristics": "Real scanned geometry and textures from 572 indoor scenes; realistic layout complexity and clutter; sensor rendering consistent with real RGB-D cameras; dynamics fidelity depends on Habitat and any injected noise models.",
            "model_or_agent_name": "Same PointGoal navigation policies (initialized and trained in Gibson environments).",
            "model_description": "RL agent (DD-PPO) using ResNet50 visual encoder + LSTM trained on Gibson scenes.",
            "reasoning_task": "PointGoal Navigation trained across Gibson scenes to learn navigation behaviors transferable to a target LAB environment.",
            "training_performance": "Policies trained in Gibson for 100M steps; achieve high in-simulation performance (used as base policies for transfer experiments). Specific per-simulation training metrics are reported via downstream test SPLs (e.g., 0.84 SPL when transferring to LAB no-noise).",
            "transfer_target": "Target Habitat environments created from Matterport Pro2 scans of a real lab (LAB) with injected observation and/or dynamics noise; intended as a proxy for real-world transfer.",
            "transfer_performance": "Direct zero-shot transfer from Gibson (no-noise) to noisy LAB conditions resulted in large performance drops (e.g., observation noise: SPL 0.84→0.04; dynamics noise: SPL 0.84→0.56).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Training on Gibson with added observation+dynamics noise (Gibson O+D) improved robustness but did not generalize cleanly to no-noise LAB; training-target fidelity mismatch causes failures in both directions.",
            "minimal_fidelity_discussion": "Paper suggests training should include representative noise models (especially sensor noise) rather than relying solely on photorealistic geometry/textures; adding dynamics noise can help avoid simulator artifacts (e.g., getting unstuck).",
            "failure_cases": "Gibson-trained policies without realistic observation noise fail under target observation noise; conversely, policies trained with heavy noise sometimes fail in no-noise target settings (getting stuck less likely when dynamics noise present).",
            "uuid": "e1326.1",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Sim_DA (neural-augmented simulator)",
            "name_full": "DA-augmented Simulator (Sim_DA) / neural-augmented simulator",
            "brief_description": "A simulator augmented by a learned dynamics-adaptation module (DA) that predicts residuals between simulator transitions and real-world transitions and resets simulator state to predicted real outcomes, enabling training of policies on trajectories reflecting expected real dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Sim_DA (Simulator augmented with sim2real Dynamics Adaptor)",
            "simulator_description": "The source simulator (Habitat/Gibson) augmented at each step with a learned 3-layer MLP (DA) that predicts the residual change in robot state (Δs_real) given simulator state and action; used to reset simulator next-state to the DA-predicted real next-state during training rollouts.",
            "scientific_domain": "robotics / embodied navigation / dynamics modeling",
            "fidelity_level": "medium: base simulator physics augmented by a learned residual model to increase realism of state transitions; fidelity depends on quality and quantity of real data used to train the residual.",
            "fidelity_characteristics": "Augments simulator by applying a learned residual correction to position and orientation transitions (Δx, Δy, Δθ); trained from collected (s_real, a_real, s_real_next) samples using weighted MSE; supports resetting simulator state to predicted real next-state to expose policy to realistic trajectories; does not change low-level physics solver but corrects transitions at state level.",
            "model_or_agent_name": "Dynamics Adaptor (DA) + RL policy finetuned in Sim_DA (π_Sim_DA, called π_BDA-5k when used with OA and 5k samples)",
            "model_description": "DA is a 3-layer multilayer perceptron regression network that predicts residual Δs_real; the policy is the same DD-PPO RL agent (ResNet50 + 2-layer LSTM) fine-tuned in the augmented simulator.",
            "reasoning_task": "Improve transfer of navigation policies by matching simulated dynamics to observed real transitions (residual dynamics prediction).",
            "training_performance": "DA trained on varying numbers of target-domain samples (100–5000); authors report that finetuning the policy in Sim_DA with DA trained from 5,000 real samples yields strong transfer performance (see transfer_performance).",
            "transfer_target": "Transfer of policies trained in Sim_DA back to the target domain (virtualized LAB with noise, and intended real-world deployment), with OA applied at test time for observation adaptation.",
            "transfer_performance": "Using DA trained with 5,000 target samples and OA, the BDA pipeline (Sim_DA + OA) matched the performance of direct finetuning in the target environment (oracle) while using far fewer target samples: e.g., BDA with 5k samples matched oracle performance that would otherwise require ~585k–1,000,000 target steps in different experiments; reported sample-efficiency speed-ups include 36×, 117× and an average 61× reduction in required real/target data.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Adding DA (sim2real residual) substantially improves transfer relative to training in the base simulator without dynamics correction; policies fine-tuned via Sim_DA generalize much better to target noisy environments than policies finetuned directly only on policy parameters with limited real data.",
            "minimal_fidelity_discussion": "Authors argue that adapting simulator dynamics via a residual model is a sample-efficient way to increase effective fidelity for transfer, implying that full high-fidelity physics is not strictly necessary if residuals correcting key state transitions are learned from modest real data.",
            "failure_cases": "When DA is trained with very little or no representative target data (e.g., small sample counts), augmentation may be insufficient; paper shows majority of gains from first 1,000 target samples and saturating by 5,000, indicating failure modes when samples &lt;&lt; 1000.",
            "uuid": "e1326.2",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "OA (real2sim CycleGAN adaptor)",
            "name_full": "Observation Adaptor (OA) — real2sim CycleGAN-based image translation",
            "brief_description": "A CycleGAN-based real-to-sim image translation module that maps real RGB-D observations to simulated-looking observations at test time, thereby closing the visual domain gap without paired data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "real2sim observation adaptor applied to Habitat observations",
            "simulator_description": "Not a simulator per se, but a learned image-to-image translation module (CycleGAN) that converts real/target RGB-D images into the visual domain of the source simulator so the policy trained in sim sees familiar images at test time.",
            "scientific_domain": "robotics / perception / domain adaptation",
            "fidelity_level": "n/a (perceptual fidelity adapter): improves perceptual alignment with simulator visuals; does not change physical dynamics.",
            "fidelity_characteristics": "Learns mappings G_sim: I_real→I_sim and inverse G_real: I_sim→I_real with adversarial discriminators and cycle-consistency; removes Gaussian and depth noise signatures and smooths textures to match simulator visuals; trained on unpaired images.",
            "model_or_agent_name": "Observation Adaptor (OA) used together with navigation policy (π_BDA-5k Gibson OA+DA)",
            "model_description": "CycleGAN-style image translation networks (generators and discriminators); trained on unpaired RGB-D images collected by behavior policy in sim and target.",
            "reasoning_task": "Close visual domain gap so a policy trained in simulation can operate on target/real visual inputs by translating them to simulator-like images.",
            "training_performance": "OA trained for 200 epochs in experiments; quantitative improvement in Fréchet Inception Distance (FID) from I_real→I_sim: FID 100.74 reduced to 83.05 after OA; for simulated Gaussian noise case FID improved from 98.73→88.44 after OA.",
            "transfer_target": "Applied at test time to LAB real/target images (or other target images) before feeding into policy trained in sim or Sim_DA.",
            "transfer_performance": "Using OA together with DA (BDA pipeline) and only 5,000 target-domain samples, policies matched oracle finetuning performance in many noisy conditions (average SPL within ~5% of oracle) and dramatically reduced required real/target data (reported speed-ups up to 117× in specific settings, average 61×).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "N/A (OA is an adapter rather than a simulator fidelity level); empirical results show OA reduces perceptual domain gap as measured by FID and recovers much of the lost SPL when combined with DA.",
            "minimal_fidelity_discussion": "Authors motivate real2sim (rather than sim2real) observation adaptation to decouple sensing from acting so policies need not be re-trained when sensors change — implying that matching perceptual statistics to training sim is a minimal necessary intervention for vision transfer.",
            "failure_cases": "OA alone cannot handle dynamics mismatch; OA without DA does not address actuation/dynamics discrepancy and thus may fail when dynamics are primary source of sim2real gap.",
            "uuid": "e1326.3",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "LAB (virtualized Matterport target)",
            "name_full": "LAB — Virtualized real lab environment (Matterport Pro2 scan imported into Habitat)",
            "brief_description": "A photorealistic target environment built from Matterport Pro2 3D scans of a real laboratory, imported into Habitat and used as the target/‘real’ domain for sim2sim experiments with added sensor and actuation noise models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "LAB (virtualized Matterport-derived environment in Habitat)",
            "simulator_description": "3D mesh replica of an actual lab collected with a Matterport Pro2 camera, imported into Habitat to create a photorealistic target environment; used to test transfer from source Gibson simulator and to emulate the real-world.",
            "scientific_domain": "robotics / embodied navigation / perception",
            "fidelity_level": "high visual fidelity of a specific real space (scan-derived); dynamics fidelity depends on Habitat physics and added noise models to mimic real sensors/actuators.",
            "fidelity_characteristics": "Accurate scene geometry and textures from Matterport scans; authors vary obstacle placements to create room configurations of different difficulty; sensor noise (RGB and Depth) and actuation noise (translational and rotational) are injected to emulate real-world imperfections.",
            "model_or_agent_name": "Target evaluation environment for navigation policies (policies trained in Gibson/Habitat and Sim_DA are tested here).",
            "model_description": "N/A (environment used for testing and for collecting target rollouts to train OA and DA).",
            "reasoning_task": "Test/target domain for PointGoal navigation transfer experiments; stand-in for real lab deployment.",
            "training_performance": null,
            "transfer_target": "This is the transfer target (i.e., policies trained in source Gibson/Habitat are transferred to LAB); also used as source of 'real' samples for training OA/DA.",
            "transfer_performance": "Policies trained in source sim and tested in LAB display large sensitivity to added noise; BDA (OA+DA) with 5k samples from LAB achieves transfer performance comparable to policies finetuned with orders of magnitude more LAB data (e.g., matching oracle finetune trained with hundreds of thousands of steps in some settings).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "LAB was used with multiple combinations of observation and dynamics noise (Table I) to compare transfer; observation noise had biggest negative impact, dynamics noise moderate; combining both heavily degrades performance for non-adapted policies.",
            "minimal_fidelity_discussion": "LAB experiments demonstrate that modeling observation noise is essential for preserving transfer; realistic target stimuli (scanned environment) combined with appropriate noise injection provides a useful proxy for real robot transfer experiments.",
            "failure_cases": "Policies trained without matching the LAB observation noise failed (very low SPL); policies trained with dynamics noise but not observation adaptation still fail under severe observation noise.",
            "uuid": "e1326.4",
            "source_info": {
                "paper_title": "Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real predictivity: Does evaluation in simulation predict real-world performance",
            "rating": 2,
            "sanitized_title": "simtoreal_predictivity_does_evaluation_in_simulation_predict_realworld_performance"
        },
        {
            "paper_title": "Habitat: A Platform for Embodied AI Research",
            "rating": 2,
            "sanitized_title": "habitat_a_platform_for_embodied_ai_research"
        },
        {
            "paper_title": "Gibson env: Real-world perception for embodied agents",
            "rating": 2,
            "sanitized_title": "gibson_env_realworld_perception_for_embodied_agents"
        },
        {
            "paper_title": "Sim-to-real transfer with neural-augmented robot simulation",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_with_neuralaugmented_robot_simulation"
        },
        {
            "paper_title": "Vr-goggles for robots: Real-to-sim domain adaptation for visual control",
            "rating": 1,
            "sanitized_title": "vrgoggles_for_robots_realtosim_domain_adaptation_for_visual_control"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 1,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 1,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        }
    ],
    "cost": 0.015160749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents</p>
<p>Joanne Truong 
Sonia Chernova 
Dhruv Batra 
Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents</p>
<p>Deep reinforcement learning models are notoriously data hungry, yet real-world data is expensive and time consuming to obtain. The solution that many have turned to is to use simulation for training before deploying the robot in a real environment. Simulation offers the ability to train large numbers of robots in parallel, and offers an abundance of data. However, no simulation is perfect, and robots trained solely in simulation fail to generalize to the real-world, resulting in a "sim-vs-real gap". How can we overcome the tradeoff between the abundance of less accurate, artificial data from simulators and the scarcity of reliable, real-world data? In this paper, we propose Bi-directional Domain Adaptation (BDA), a novel approach to bridge the sim-vs-real gap in both directions-real2sim to bridge the visual domain gap, and sim2real to bridge the dynamics domain gap. We demonstrate the benefits of BDA on the task of PointGoal Navigation. BDA with only 5k real-world (state, action, next-state) samples matches the performance of a policy fine-tuned with ∼600k samples, resulting in a speed-up of ∼120×.</p>
<p>I. INTRODUCTION</p>
<p>Deep reinforcement learning (RL) methods have made tremendous progress in many high-dimensional tasks, such as navigation [23], manipulation [4], and locomotion [9]. Since RL algorithms are data hungry, and training robots in the real-world is slow, expensive, and difficult to reproduce, these methods are typically trained in simulation (where gathering experience is scalable, safe, cheap, and reproducible) before being deployed in the real-world.</p>
<p>However, no simulator perfectly replicates reality. Simulators fail to model many aspects of the robot and the environment (noisy dynamics, sensor noise, wear and-tear, battery drainage, etc.). In addition, RL algorithms are prone to overfitting -i.e., they learn to achieve strong performance in the environments they were trained in, but fail to generalize to novel environments. On the other hand, humans are able to quickly adapt to small changes in their environment. The ability to quickly adapt and transfer skills is a key aspect of intelligence that we hope to reproduce in artificial agents.</p>
<p>This raises a fundamental question -How can we leverage imperfect but useful simulators to train robots while ensuring that the learned skills generalize to reality? This question is studied under the umbrella of 'sim2real transfer' and has been a topic of much interest in the community [5], [8], [11], [16], [22], [25], [26].</p>
<p>In this work, we first reframe the sim2real transfer problem into the following question -given a cheap abundant lowfidelity data generator (a simulator) and an expensive scarce 1 Georgia Institute of Technology, {truong.j, chernova, dbatra}@gatech.edu 2 Facebook AI Research high-fidelity data source (reality), how should we best leverage the two to maximize performance of an agent in the expensive domain (reality)? The status quo in machine learning is to pre-train a policy in simulation using large amounts of simulation data (potentially with domain randomization [22]) and then fine-tune this policy on the robot using the small amount of real data. Can we do better?</p>
<p>We contend that the small amount of expensive, highfidelity data from reality is better utilized to adapt the simulator (and reduce the sim-vs-real gap) than to directly adapt the policy. Concretely, we propose Bi-directional Domain Adaptation (BDA) between simulation and reality to answer this question. BDA reduces the sim-vs-real gap in two different directions (shown in Fig. 1).</p>
<p>First, for sensory observations (e.g. an RGB-D camera image I) we train a real2sim observation adaptation module OA : I real → I sim . This can be thought of as 'goggles' [26], [24] that the agent puts on at deployment time to make real observations 'look' like the ones seen during training in simulation. At first glance, this choice may appear counterintuitive (or the 'wrong' direction). We choose real2sim observation adaption instead of sim2real because this decouples sensing and acting. If the sensor characteristics in reality change but the dynamics remain the same (e.g. same robot different camera), the policy does not need to be retrained, but only equipped with a re-trained observation adaptor. In contrast, changing a sim2real observation adaptor results in the generated observations being out of distribution for the policy, requiring expensive re-training of the policy. Our real2sim observation adaptor is based on CycleGAN [27], and thus does not require any sort of alignment or pairing between sim and real observations, which can be prohibitive.</p>
<p>Second, for transition dynamics T : P r(s t+1 | s t , a t ) (the probably of transitioning from state s t to s t+1 upon taking action a t ), we train a sim2real dynamics adaptation module DA : T sim → T real . This can be thought of as a neural-augmented simulator [8] or a specific kind of boosted ensembling method [19] -where a simulator first makes predictions about state transitions and then a learned neural network predicts the residual between the simulator predictions and the state transitions observed in reality. At each time t during training in simulation, DA resets the simulator state from s sim t+1 (where the simulator believes the agent should reach at time t+1) toŝ real t+1 (where DA predicts the agent will reach in reality), thus exposing the policy to trajectories expected in reality. We choose sim2real dynamics adaptation instead of real2sim because this nicely exploits the fundamental asymmetry between the two domains - We learn a sim2real dynamics adaptation module to predict residual errors between state transitions in simulation and reality. Right: We learn a real2sim observation adaptation module to translate images the robot sees in the real-world at test time to images that more closely align with what the robot has seen in simulation during training. (b) Using BDA, we achieve the same SPL as a policy finetuned directly in reality while using 117× less real-world data.</p>
<p>simulators can (typically) be reset to arbitrary states, reality (typically) cannot. Once an agent acts in the real-world, it doesn't matter what corresponding state it would have reached in simulator, reality cannot be reset to it.</p>
<p>Once the two modules are trained, BDA trains a policy in a simulator augmented with the dynamics adaptor (DA) and deploys the policy augmented with the observation adaptor (OA) to reality. This process is illustrated in Fig. 1a, left showing policy training in simulation and right showing its deployment in reality.</p>
<p>We instantiate and demonstrate the benefits of BDA on the task of PointGoal Navigation (PointNav) [3], which involves an agent navigating in a previously unseen environment from a randomized initial starting location to a goal location specified in relative coordinates. For controlled experimentation, and due to COVID-19 restrictions, we use Sim2Sim transfer of PointNav policies as a stand-in for Sim2Real transfer. We conduct experiments in photo-realistic 3D simulation environments using Habitat-Sim [18], which prior work [13] has found to have high sim2real predictivity, meaning that inferences drawn in simulation experiments have a high likelihood of holding in reality on Locobot mobile robot [2].</p>
<p>In our experiments, we find that BDA is significantly more sample-efficient than the baseline of fine-tuning a policy. Specifically, BDA trained on as few as 5,000 samples (state, action, next-state) from reality (equivalent of 7 hours to collect data in reality) is able to match the performance of baseline trained on 585,000 samples from reality (equivalent of 836 hours to collect data in reality, or 3.5 months at 8 working hours per day), a speed-up of 117× (Fig. 1b).</p>
<p>While our experiments are conducted on the PointNav task, we believe our findings, and the core idea of Bidirectional Domain Adaptation, is broadly applicable to a number of problems in robotics and reinforcement learning.</p>
<p>II. BI-DIRECTIONAL DOMAIN ADAPTATION (BDA)</p>
<p>We now describe the two key components of Bi-directional Domain Adaptation (BDA) in detail -(1) real2sim observation adaptation module OA to close the visual domain gap, and (2) sim2real dynamics adaptation module DA to close the dynamics domain gap.</p>
<p>Preliminaries and Notation. We formulate our problem by representing both the source and target domain as a Markov Decision Process (MDP). A MDP is defined by the tuple (S, A, T , R, γ), where s ∈ S denotes states, a ∈ A denotes actions, T (s, a, s ) = P r(s | s, a) is the transition probability, R : S × A → R is the reward function, and γ is a discount factor. In RL, the goal is to learn a policy π : S → A to maximize expected reward.</p>
<p>A. System Architecture</p>
<p>Algorithm 1: Bi-directional Domain Adaptation 1 Train behavior policy π sim in Sim 2 for t = 0, ..., N steps do 3 Collect I sim t ∼ Sim rollout (π sim ) 4 Collect I real t , s real t , a real t ∼ Real rollout (π sim )
5 Train OA ({I sim N i=1 }, {I real N i=1 }) 6 Train DA ({s real N i=1 }, {a real N i=1 }) 7
Sim DA ← Augment Source with DA 8 for j = 0, ..., K steps do 9 π Sim DA ← Finetune π sim in Sim DA 10 π Sim OA+DA ← Apply OA at test-time 11 Test π Sim OA+DA in Real Observation Adaptation. We consider a real2sim domain adaptation approach to deal with the visual domain gap.</p>
<p>We leverage CycleGAN [27], a pixel-level image-toimage translation technique that uses a cycle-consistency loss function with unpaired images. We start by using a behavior policy π sim trained in simulation to sample rollouts in simulation and reality to collect RGB-D images I sim
t and I real t at time t (line 3). The dataset of N unpaired images {I sim N i=1 } and {I real N i=1 } is used to train OA, to translate {I sim N i=1 } → {I real N i=1 } (line 5).
OA learns a mapping G sim : I real → I sim , an inverse mapping G real : I sim → I real , and adversarial discriminators D real , D sim . Although our method focuses on adaptation from real2sim, learning both mappings encourages the generative models to remain cycle-consistent, i.e., forward cycle: I real → G sim (I real ) → G real (G sim (I real )) ≈ I real and backwards cycle: I sim → G real (I sim ) → G sim (G real (I sim )) ≈ I real . The ability to learn mappings from unpaired images from both domains is important because it is difficult to accurately collect paired images between simulation and reality.</p>
<p>A real2sim approach for adapting the visual domain offers many advantages over a sim2real approach because it disentangles the sensor adaptation module from our policy training. This enables us to remove an additional bottleneck during the RL policy training process; we can train OA in parallel with the RL policy, thus reducing the overall training time needed. In addition, if the sensor observation noise in the environment changes, the base policy can be kept frozen, and only OA will have to be retrained. Dynamics Adaptation. To close the dynamics domain gap, we follow a sim2real approach.</p>
<p>Starting with the behavior policy π sim , we collect stateaction pairs (s real t , a real t ) in the real-world (line 4). The stateaction pairs are used to train DA, a 3 layer multilayer perceptron regression network, that learns the residual error between the state transitions in simulation and reality T sim → T real (line 7). Specifically, DA learns to estimate the change in position and orientation ∆s real : (s real t+1 − s real t ). We use a weighted MSE loss function, 1
n N n=1 w (∆s real n − ∆ŝ real n ) 2 . For our experiments, the state s real t = (x real t , y real t , θ real t )
, is represented by the position and orientation of the robot at timestep t. We placed twice as much weight on the prediction terms for the robot's position than for its orientation because getting the position correct is more important for our performance metric. Once trained, DA is used to augment the source environment (line 7). We finetune π sim in the augmented simulator, Sim DA (lines 8-9). Our hypothesis (which we validate in our experiments) is using real-world data to adapt the simulator via our DA model pays off because we can then train RL policies in this DA-augmented simulator for large amounts of experience cheaply. We use OA at test time (line 10). Finally, we test our policy trained with BDA in the real-world (line 11).</p>
<p>To recap, BDA has a number of advantages over the status quo (of directly using real data to fine-tune a simulation trained policy) that we demonstrate in our experiments: (1) Decouples sensing and acting, (2) Does not require paired training data, (3) The data to train both modules can be collected jointly (by gathering experience from a behavior policy in reality) but the two can be trained in parallel independently of each other, (4) Similar to model-based RL [21], reducing the sim-vs-real gap is made significantly more sample-efficient than directly fine-tuning the policy.</p>
<p>III. EXPERIMENTAL SETUP: SIM2SIM TRANSFER FOR POINT-GOAL NAVIGATION</p>
<p>Our goal in this work is to enable sample efficient Sim2Real transfer for the task of PointGoal Navigation (PointNav) [3]. However, for controlled experiments and due to COVID-19 restrictions, we study Sim2Sim transfer as a stand-in for Sim2Real. Specifically, we train policies in a "source" simulator (which serves as 'Sim' in 'Sim2Real') and transfer it to a "target" simulator (which serves as 'Real' in 'Sim2Real'). We add observation and dynamics noise to the target simulator to mimic the noise observed in reality. Notice that these noise models are purely for the purpose of conducting controlled experiments and are not avaliable to the agent (which must adapt and learn from samples of state and observations). Since no noise model is perfect (just like no simulator is perfect), we experiment with a range of noise models and report results with multiple target simulators. Our results show consistent improvements regardless of the noise model used, thus providing increased confidence in our experimental setup. For clarity, in the text below we present our approach from the perspective of "transfer from a source to target domain," with the assumption that obtaining data in the target domain is always expensive, regardless of whether it is a simulated or real-world environment. All of our experiments are conducted in Habitat [18].</p>
<p>A. Task: PointGoal Navigation</p>
<p>In PointNav, a robot is initialized in an unseen environment and asked to navigate to a goal location specified in relative coordinates purely from egocentric RGB-D observations without a map, in a limited time budget. An episode is considered successful if the robot issues the STOP command within 0.2m of the goal location. In order to increase confidence that our simulation settings will translate to the real-world, we limit episodes to 200 steps, limit number of collisions allowed (before deeming the episode a failure) to 40, and turn sliding off-specifications found by [13] to have high sim2real predictivity (how well evaluation in simulation predicts real-world performance). Sliding is a behavior enabled by default in many physics simulators that allows agents to slide along obstacles when the agent takes an action that would result in a collision. Turning sliding off ensures that the agent cannot cheat in simulation by sliding along obstacles. We use success rate (SUCC), and Success weighted by (normalized inverse) Path Length (SPL) [3] as metrics for evaluation.</p>
<p>B. Robot in Simulation</p>
<p>Body. The robot has a circular base with a radius of 0.175m and a height of 0.61m. These dimensions correspond to the base width and camera height of the LoCoBot robot [2]. Sensors. The robot has access to an egocentric RGB and Depth sensor, and accurate localization and heading through a GPS+Compass sensor. real-world robot experiments from [13] used Hector SLAM [14] with a Hokuyo UTM-30LX LIDAR sensor and found that localization errors were approximately 7cm (much lower than the 20cm PointNav success criterion). This gives us confidence that our results will generalize to reality, despite the lack of precise localization. We match the specifications of the Intel D435 camera on the LoCoBot, and set the camera field of view to 70. To match the maximum range on the depth camera, we clip the simulated depth readings to 10m. Sensor Noise. To simulate noisy sensor observations of the real-world, we add RGB and Depth sensor noise models to  [7]. Fig. 2 shows a comparison between noise free RGB-D images and RGB-D images with the different noise models and multipliers we use.</p>
<p>Actions. The action space for the robot is turn-left 30 • turn-right 30 • , forward 0.25m, and STOP. In the source simulator, these actions are executed deterministically and accurately. However, actions in the real-world are never deterministic -identical actions can lead to vastly different final locations due to the actuation noise (wheel slippage, battery power drainage, etc.) typically found on a real robot. To simulate the noisy actuation that occurs in the real-world, we leverage the real-world translational and rotational actuation noise models characterized by [15]. A Vicon motion capture was used to measure the difference in commanded state and achieved state on LoCoBot for 3 different positional controllers: Proportional Controller, Dynamic Window Approach Controller from Movebase, and Linear Quadratic Regulator (ILQR). These are controllers typically used on a mobile robot. From a state (x, y, θ) and given a particular action, we add translational noise sampled from a truncated 2D Gaussian, and rotational noise from a 1D Gaussian to calculate the next state.</p>
<p>C. Testing Environment</p>
<p>We virtualize a 6.5m by 10m real lab environment (LAB) to use as our testing environment, using a Matterport Pro2 3D camera. To model the space, we placed the Matterport camera at various locations in the room, and collected 360 • scans of the environment. We used the scans to create 3D meshes of the environment, and directly imported the 3D meshes into Habitat to create a photorealistic replica of LAB Fig. 3b. We vary the number of obstacles in LAB to create 3 room configurations with varying levels of difficulty. Fig. 3 shows one of our room configurations with 5 obstacles. We perform testing over the 3 different room configurations, each with 5 start and end waypoints for navigation episodes, and 10 independent trials, for a total of 150 runs. We report the average success rate and SPL over the 150 runs. Our models were trained entirely in the Gibson dataset [24], and have never seen LAB during training. The Gibson dataset contains 3D models of 572 cluttered indoor environments (homes, hospitals, offices, museums, etc.). In this work, we used the 72 Gibson environments that were rated 4+ in quality in [18].</p>
<p>D. Experimental Protocol</p>
<p>Recall that our objective is to improve the ability for RL agents to generalize to new environments using little real-world data. To do this, we define our source environment as Gibson without any sensor or actuation noise (Gibson no noise ). We create 10 target environments with noise settings described in Table I. We use the notation O to represent an environment afflicted with only RGBD observation noise (rows 2, 5, or 8), D to represent an environment afflicted with only dynamics noise (rows 3, 6, or 9), and O + D to represent an environment afflicted with RGBD observation noise and dynamics noise (rows 4, 7, or 10).</p>
<p>E. RL Navigation Models</p>
<p>We train learning-based navigation policies, π, for Point-Goal in Habitat using environments from the Gibson dataset. Policies were trained from scratch with reinforcement learning using DD-PPO [23], a decentralized, distributed variant of the proximal policy optimization (PPO) algorithm, that allows for large-scale training in GPU-intensive simulation environments. We follow the navigation policy described in [23], which is composed of a ResNet50 visual encoder, and a 2-layer LSTM. Each policy is trained using 64 Tesla V100s. Base policies are trained for 100 million steps (π 100M ) to ensure convergence.</p>
<p>IV. EXPERIMENTS</p>
<p>Our experiments aim to answer the following: (1) How large is the sim2real gap? (2) Does our method improve generalization to target domains? (3) How does our method compare to directly training (or fine-tuning) in the target environment? (4) How much real-world data do we need?</p>
<p>How large is the sim2real gap? First, we show that RL policies fail to generalize to new environments. We train a policy without any noise (π 100M Gibson no noise ), and a policy with observation and dynamics noise (π 100M Gibson O+D ). We test these policies in LAB with 4 different noise settings: LAB no noise , LAB O , LAB D , LAB O+D , and average across the noise settings. For each noise setting, we conduct 3 sets of runs, each containing 150 episodes in the target environments. We see that π 100M Gibson no noise tested in LAB no noise exhibits good transfer across environments -0.84 SPL (in contrast, the Habitat 2019 challenge winner was at 0.95 SPL [1]). [23] showed that near-perfect performance is possible when the policy is trained out for significantly longer (2.5B frames), but for the sake of multiple experiments, we limit our analysis to 100M frames of training and compare all models across the same number.</p>
<p>From Fig. 4, we see that when dynamics noise is introduced (π 100M Gibson no noise tested in LAB D ), SPL drops from 0.84 to 0.56 (relative drop of 28%). More significantly, when sensor noise is introduced (π 100M Gibson no noise tested in LAB O ), SPL drops to 0.04 (relative drop of 81%), and when both sensor and dynamics noise are present, (π 100M Gibson no noise tested in LAB O+D ), SPL drops to 0.06 (relative drop of 78%). Thus, in the absence of noise, generalization across scenes (Gibson to LAB) is good, but in the presence of noise, the generalization suffers. We also notice that the converse is true: policies trained from scratch in Gibson O+D environments fail to generalize to LAB no noise and LAB D environments. Gibson no noise and π 100M Gibson O+D tested in LAB with different combinations of observation and dynamics noise. We see that SPL drops when a policy is tested in an environment with noise different from what it was trained in.</p>
<p>These results show us that RL agents are highly sensitive to what might be considered perceptually minor changes to visual inputs. To the best of our knowledge, no prior work in embodied navigation appears to have considered this question of sensitivity to noise; hopefully our results will encourage others to consider this as well.</p>
<p>How well does OA do? Following Alg. 1 described in Sec. II-A, we train OA from scratch for 200 epochs. In Fig. 5, we see that the model learns to remove the Gaussian noise placed on the RGB image, and learns to smooth out textures in the depth image.
I no noise I O+D OA (I O+D ) RGB Depth (a) (b) (c)
In addition, we have RGB-D images of LAB collected from a real robot, pre-COVID, and results using our real2sim OA module. While no GAN metric is perfect (user studies are typically conducted for evaluation as done in [27]), we calculated the Fréchet Inception Distance (FID) [10] score (lower is better) to provide quantitative results. We find that the FID comparing I real and I sim is 100.74, and the FID from OA (I real ) to I sim images is 83.05. We also calculated FID comparing simulation images afflicted with Gaussian noise, I Gaussian , to noise-free simulation images I no noise to be 98.73, and FID between OA (I Gaussian ) to I no noise images to be 88.44. To put things in context, the FID score comparing images from CIFAR10 to our simulation images is 317.61. This shows that perceptually, the distribution of our adapted images more closely resembles images taken directly from simulation, and that real2sim OA is not far off from our sim2sim OA experiments. While our architecture has changed since this initial data collection (initial images are 256 × 256, compared to our current architecture which uses 640 × 360 images), these results will serve as a good indication that our approach will generalize to reality.</p>
<p>How does our method compare to fine-tuning? Next, we evaluate our policy finetuned using BDA with 5,000 data samples collected in the target environment (π BDA−5k Gibson OA+DA ). We compare this to directly finetuning in the target environment (π 1M Gibson O+D ), which serves as an oracle baseline. Both π BDA−5k Gibson OA+DA and π 1M Gibson O+D are initialized with π 100M Gibson no noise , and both are re-trained for each target O + D setting.</p>
<p>Our results in Table II show the benefit in finetuning using data from target environments. π 1M Gibson O+D demonstrates robustness in all combinations of sensor and actuation noise. We also observe that using BDA to learn the observation and dynamics noise models with 5,000 samples from the target environment is capable of nearly matching performance of π 1M</p>
<p>Gibson O+D . In fact, we only see, on average, a 5% difference between π 1M</p>
<p>Gibson O+D and π BDA−5k Gibson OA+DA (rows 4,8,12), while the former is directly trained in the target environment which is not possible in reality, as it requires 1M samples from the target environment.</p>
<p>From these results, we notice in certain environments our method performs worse than the oracle baseline if no or only observation noise is present (rows 1, 5, 6, 10), but performs on the level of the oracle baseline when dynamics Gibson no noise is a policy trained solely in simulation. π BDA−5k Gibson OA+DA and π 1M Gibson O+D are initialized with π 100M Gibson no noise . π BDA−5k Gibson OA+DA is fine-tuned with BDA using 5k samples from the target environment. π 1M</p>
<p>Gibson O+D is fine-tuned directly in the target environment for 1M steps of experience, and serves as an oracle baseline. While π 1M</p>
<p>Gibson O+D and π BDA−5k Gibson OA+DA achieve in strong performance across environments with varying noises (rows 4,8,12) is added (rows 3,4,7,8,11,12). We believe it's due to 'sliding', a default behavior in 3D simulators allowing agents to slide along obstacles rather than stopping immediately on contact. Following the findings and recommendations of [13], we disabled sliding to make our simulation results more predictive of real-world experiments. We find that one common failure mode in the absence of sliding is that agents get stuck on obstacles. In the presence of dynamics noise, the slight amount of actuation noise allows the agent to free itself from obstacles, similar to how it would in reality. Without dynamics noise, the agents continue to stay stuck.</p>
<p>Sample Efficiency. We repeat our experiments, varying the amounts of data collected from the target environment. We re-train OA and DA using 100, 250, 500, 1,000, and 5,000 steps of experience in the target environment, and re-evaluate performance. We compare this to directly finetuning in the target environment for varying amounts of data. In Fig. 7, the x-axis represents the number of samples collected in the target environment. From previous experiments, we estimate 1 episode in the real-world to last on average 6 minutes, in which the robot will take approximately 70 steps to reach the goal. We use this as a conversion factor, and add an additional x-axis to show the number of hours needed for collecting the required samples from the target environment. The y-axis shows the SPL in the target environment. We see that the majority of our success comes from our first 1,000 samples from the target environment, and after 5,000 samples, π BDA−5k Gibson OA+DA is able to match the performance from π 1M Gibson O+D . Collecting 5,000 samples of data from a target environment to train our method would have taken 7 hours. In comparison, Fig. 7b shows that we would have to finetune the base policy for approximately 585,000 steps in the target environment (836 hours to collect data from target environment) to reach the same SPL. Comparing the amount of data needed to reach the same SPL, we see that BDA reduces the amount of data needed from the target environment by 36× in Fig. 7a, 117×  in Fig. 7c, for an average speed up of 61×. These results give us confidence in the importance of our approach, as we wish to limit the amount of data needed from a target environment (i.e. real-world).</p>
<p>V. RELATED WORK</p>
<p>Bi-directional Domain Adaptation is related to literature on domain and dynamics randomization, domain adaptation, and residual policy learning.</p>
<p>Domain and Dynamics Randomization. Borrowing ideas from data augmentation commonly used in computer vision, domain randomization is a technique to train robust policies by exposing the agent to a wide variety of simulation environments with randomized visual properties such as lighting, texture, camera position, etc. Similarly, dynamics randomization is a process that randomizes physical properties in the simulator such as friction, mass, damping, etc. [17] applied randomization to textures to learn real indoor flight by training solely in simulation. [6] used real-world roll outs to learn a distribution of simulation dynamics parameters to randomize over. [4] randomized both physical and visual parameters to train a robotic hand to perform in hand manipulation. However, finding the right distribution to randomize parameters over is difficult, and may require expert knowledge. If the distribution chosen to randomize parameters over is too large, the task becomes much harder for the policy to learn. On the other hand, if the distribution is too small, then the reality gap remains large, and the policy will fail to generalize.</p>
<p>Domain Adaptation. To bridge the simulation to reality gap, many works have used domain adaptation, a technique in which data from a source domain is adapted to more closely resemble data from a target domain. Prior works have used domain adaptation techniques for adapting visionbased models to translate images from sim-to-real during training for manipulation tasks [5], [11], and real-to-sim during testing for navigation tasks [26]. Other works have focused on adapting policies for dynamic changes [8], [25]. In our work, we seek to use domain adaptation to close the gap for both the visual and the dynamics domain.</p>
<p>Residual Policy</p>
<p>Learning. An alternative to typical transfer learning techniques is to directly improve the underlying policy itself. Instead of re-training an agent from scratch when policies perform sub-optimally, the sub-optimal policy can be used as prior knowledge in RL to speed up training. This is the main idea behind residual policy learning, in which a residual policy is used to augment an initial policy to correct for changes in the environment. [12], [20] demonstrated that combining residual policy learning with conventional robotic control improves the robot's ability to adapt to variations in the environment for manipulation tasks. Our method builds on this line of research by augmenting the simulator using a neural network that learns the residual error between simulation and reality.</p>
<p>VI. CONCLUSION</p>
<p>We introduce Bi-directional Domain Adaptation (BDA), a method to utilize the differences between simulation and reality to accelerate learning and improve generalization of RL policies. We use domain adaptation techniques to transfer images from real2sim to close the visual domain gap, and learn the residual error in dynamics from sim2real to close the dynamics domain gap. We find that our method consistently improves performance of the initial policy π while remaining sample efficient.</p>
<p>Fig. 1: (a) Left: We learn a sim2real dynamics adaptation module to predict residual errors between state transitions in simulation and reality. Right: We learn a real2sim observation adaptation module to translate images the robot sees in the real-world at test time to images that more closely align with what the robot has seen in simulation during training. (b) Using BDA, we achieve the same SPL as a policy finetuned directly in reality while using 117× less real-world data.</p>
<p>Fig. 3 :
3(a) Top-down view of one of our testing environments. White boxes are obstacles. The robot navigates sequentially through thewaypoints A → B → C → D → E → A.Figure taken from [13] (b) 3D visualization of the robot navigating in one of our testing environments in simulation. RGB and Depth observations are shown on the right. TABLE I: Definition of the 10 different noise settings we use for training and testing. Row 1 indicates the 'source' environment with no observation or actuation noise present.</p>
<p>Fig. 4 :
4Zero-shot transfer of π 100M</p>
<p>Fig. 5 :
5(a) LAB with no RGB or Depth sensor noise (b) LAB with 0.1 Gaussian RGB noise and 1.0 Redwood Depth noise. (c) By adapting images from real2sim, we now have images that closely resemble (a).</p>
<p>Fig. 6 :
6(a) LAB real (b) We adapt from real2sim to obtain images that closely resemble RGB-D images from simulation.</p>
<p>Fig. 7 :
7Performance of BDA compared to directly finetuning a policy in the target environment: Plots (a), (b) and (c) represent LAB environments with different noise settings we test in. On average, BDA requires 61× less data from the target environment to achieve the same SPL as finetuning directly in the target environment.</p>
<p>TABLE II :
IISuccess rate and SPL of three policies with RGB-D observations. π 100M</p>
<p>, BDA requires 200× fewer samples from the target environment.# 
RGB Obs 
Depth Obs Actuation 
π 100M </p>
<p>Gibson no noise </p>
<p>π BDA−5k </p>
<p>Gibson OA+DA </p>
<p>π 1M </p>
<p>Gibson O+D </p>
<p>Noise 
Noise 
Noise 
SUCC 
SPL 
SUCC 
SPL 
SUCC 
SPL </p>
<h2>1</h2>
<h2>-</h2>
<p>1.0 
0.84 
0.80 
0.61 
0.99 
0.84 
2 
Gaus. 0.1 
Red. 1.0 
-
0.10 
0.04 
0.97 
0.80 
0.99 
0.87 
3 
-
-
Prop. 1.0 
0.89 
0.57 
0.99 
0.65 
1.00 
0.64 
4 
Gaus. 0.1 
Red. 1.0 
Prop. 1.0 
0.32 
0.11 
1.00 
0.62 
1.00 
0.65 </p>
<h2>5</h2>
<h2>-</h2>
<p>1.0 
0.84 
0.85 
0.68 
0.97 
0.80 
6 
Speck. 0.1 
Red. 1.5 
-
0.11 
0.05 
0.70 
0.54 
0.99 
0.81 
7 
-
-
MB 1.0 
0.71 
0.42 
0.97 
0.58 
1.00 
0.59 
8 
Speck. 0.1 
Red. 1.5 
MB 1.0 
0.08 
0.03 
0.99 
0.60 
1.00 
0.62 </p>
<h2>9</h2>
<h2>-</h2>
<p>1.0 
0.85 
1.00 
0.87 
1.00 
0.83 
10 
Pois. 1.0 
Red. 2.0 
-
0.07 
0.04 
0.96 
0.69 
1.00 
0.87 
11 
-
-
ILQR 1.0 
0.93 
0.68 
1.00 
0.76 
0.99 
0.73 
12 
Pois. 1.0 
Red. 2.0 
ILQR 1.0 
0.14 
0.05 
0.99 
0.63 
1.00 
0.73 </p>
<p>Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019. Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019. https://aihabitat.org/challenge/2019/.</p>
<p>Locobot: An open source low cost robot. Locobot: An open source low cost robot. https: //locobot-website.netlify.com/.</p>
<p>Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, arXiv:1807.06757Manolis Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprintPeter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Doso- vitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprint arXiv:1807.06757, 2018.</p>
<p>Learning dexterous inhand manipulation. Bowen Openai: Marcin Andrychowicz, Maciek Baker, Rafal Chociej, Bob Jozefowicz, Jakub Mcgrew, Arthur Pachocki, Matthias Petron, Glenn Plappert, Alex Powell, Ray, The International Journal of Robotics Research. 391OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in- hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, 2018 IEEE international conference on robotics and automation (ICRA). IEEEKonstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. In 2018 IEEE international conference on robotics and automation (ICRA), pages 4243-4250. IEEE, 2018.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox, Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. pages 8973-8979, 05 2019.</p>
<p>Robust reconstruction of indoor scenes. Sungjoon Choi, Qian-Yi Zhou, Vladlen Koltun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Sungjoon Choi, Qian-Yi Zhou, and Vladlen Koltun. Robust recon- struction of indoor scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
<p>Sim-to-real transfer with neural-augmented robot simulation. Florian Golemo, Adrien Ali Taiga, Aaron Courville, Pierre-Yves Oudeyer, Conference on Robot Learning. Florian Golemo, Adrien Ali Taiga, Aaron Courville, and Pierre-Yves Oudeyer. Sim-to-real transfer with neural-augmented robot simulation. In Conference on Robot Learning, pages 817-828, 2018.</p>
<p>Learning to walk via deep reinforcement learning. Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine, arXiv:1812.11103arXiv preprintTuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018.</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, Advances in neural information processing systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems, pages 6626-6637, 2017.</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionStephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalash- nikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12627-12637, 2019.</p>
<p>Residual reinforcement learning for robot control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, J A Ojea, E Solowjow, S Levine, 2019 International Conference on Robotics and Automation (ICRA). T. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea, E. Solowjow, and S. Levine. Residual reinforcement learning for robot control. In 2019 International Conference on Robotics and Automation (ICRA), pages 6023-6029, 2019.</p>
<p>Sim2real predictivity: Does evaluation in simulation predict real-world performance. Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra, IEEE Robotics and Automation Letters. Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, and Dhruv Batra. Sim2real predictivity: Does evaluation in simulation predict real-world performance. IEEE Robotics and Automation Letters, 2020.</p>
<p>A flexible and scalable slam system with full 3d motion estimation. S Kohlbrecher, J Meyer, O Stryk, U Klingauf, SSRR. IEEE. S. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf. A flexible and scalable slam system with full 3d motion estimation. In SSRR. IEEE, November 2011.</p>
<p>Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, Abhinav Gupta, arXiv:1906.08236Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprintAdithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav Gupta. Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprint arXiv:1906.08236, 2019.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. Marcin Xue Bin Peng, Wojciech Andrychowicz, Pieter Zaremba, Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEEXue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics ran- domization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1-8. IEEE, 2018.</p>
<p>CAD2RL: real single-image flight without a single real image. Fereshteh Sadeghi, Sergey Levine, Robotics: Science and Systems XIII, Massachusetts Institute of Technology. Cambridge, Massachusetts, USAFereshteh Sadeghi and Sergey Levine. CAD2RL: real single-image flight without a single real image. In Robotics: Science and Sys- tems XIII, Massachusetts Institute of Technology, Cambridge, Mas- sachusetts, USA, July 12-16, 2017, 2017.</p>
<p>Habitat: A Platform for Embodied AI Research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra, ICCV. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In ICCV, 2019.</p>
<p>A brief introduction to boosting. Robert E Schapire, Proceedings of the 16th International Joint Conference on Artificial Intelligence. the 16th International Joint Conference on Artificial Intelligence299Robert E. Schapire. A brief introduction to boosting. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI'99, 1999.</p>
<p>. Tom Silver, Kelsey R Allen, Joshua B Tenenbaum, Leslie Pack Kaelbling, Residual policy learning. ArXiv, abs/1812.06298Tom Silver, Kelsey R. Allen, Joshua B. Tenenbaum, and Leslie Pack Kaelbling. Residual policy learning. ArXiv, abs/1812.06298, 2018.</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 23-30. IEEE, 2017.</p>
<p>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, ICLR. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. In ICLR, 2020.</p>
<p>Gibson env: Real-world perception for embodied agents. Fei Xia, Zhiyang Amir R Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, CVPR. Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In CVPR, 2018.</p>
<p>Learning fast adaptation with meta strategy optimization. Wenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, Sehoon Ha, IEEE Robotics and Automation Letters. 52Wenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, and Sehoon Ha. Learning fast adaptation with meta strategy optimization. IEEE Robotics and Automation Letters, 5(2):2950-2957, 2020.</p>
<p>Vr-goggles for robots: Real-to-sim domain adaptation for visual control. Jingwei Zhang, Lei Tai, Peng Yun, Yufeng Xiong, Ming Liu, Joschka Boedecker, Wolfram Burgard, IEEE Robotics and Automation Letters. 42Jingwei Zhang, Lei Tai, Peng Yun, Yufeng Xiong, Ming Liu, Joschka Boedecker, and Wolfram Burgard. Vr-goggles for robots: Real-to-sim domain adaptation for visual control. IEEE Robotics and Automation Letters, 4(2):1148-1155, 2019.</p>
<p>Unpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Computer Vision (ICCV. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Un- paired image-to-image translation using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International Conference on, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1715 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1715</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1715</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-270392025</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.07904v1.pdf" target="_blank">Grounding Multimodal Large Language Models in Actions</a></p>
                <p><strong>Paper Abstract:</strong> Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1715.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1715.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The base multimodal large language model (MLLM) used in this work; a vision-language LLM that is finetuned with Action Space Adapters (ASAs) to produce embodied agent actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LLaVA-1.5-7B (MLLM policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pretrained multimodal large language model (vision+language) used as the policy backbone. In this work the visual encoder and pretrained LLM are kept mostly frozen and the model is finetuned via LoRA together with an Action Space Adapter (ASA). The finetuned system uses an adapter head, action token embedding, and adapter decoder to autoregressively produce m action tokens per time step which are decoded into environment actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal vision-language and language pretraining (image-text + language modeling); base LLM/token vocabulary pretrained on language.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in detail in this paper; the authors state they use the pretrained LLaVA-1.5-7B model as the MLLM backbone and reuse its token vocabulary and language head for some ASAs; all further task adaptation is via supervised finetuning or RL with LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>CALVIN, Meta-World (ML-45), Habitat Pick (HabPick), BabyAI, Language Rearrangement (LangR)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A set of 3D embodied benchmarks covering tabletop manipulation (CALVIN), robotic manipulation multi-task benchmark (Meta-World ML-45), mobile manipulation pick tasks in Habitat (HabPick), a gridworld with language instructions (BabyAI), and a mobile rearrangement benchmark with high-level skills (Language Rearrangement). Tasks require following natural language instructions and controlling the agent to satisfy object-manipulation or navigation goals.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Native pretraining output is natural language tokens (text token vocabulary) produced by the LLM; i.e., text token output space.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Varies by environment: continuous motor control vectors (e.g., 6–10 DoF end-effector / arm / base velocities plus gripper state) for CALVIN/Meta-World/HabPick; discrete high-level skills (70 skills) for Language Rearrangement; small discrete navigation/action set for BabyAI.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Action Space Adapters (ASAs) bridge the LLM token space and environment actions: (a) direct regression (Pred) from MLLM hidden states to continuous actions via an MLP; (b) reuse of LLM token space to predict natural-language action descriptions (SemLang) or numeric tokens (Lang); (c) learned tokenizations for continuous control (VQ and Residual VQ (RVQ)) where actions are encoded as learned codebook indices (tokens) that are decoded to continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB vision (fixed camera 200×200 or egocentric head camera 336×336 depending on environment); agent proprioception (joint angles, gripper state) is included where useful; no depth used in this work (except cited comparisons that use depth).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>With the LLaVA-based MLLM finetuned and the recommended ASAs: RVQ achieves 72% success on CALVIN (ABC→D split), 84% success on Meta-World ML-45, and 29% on HabPick (MLLM + RVQ). For discrete action tasks, SemLang achieved 51% on Language Rearrangement and 40% on BabyAI (MLLM + SemLang).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Non-MLLM (trained-from-scratch) baselines reported in the paper: the 'Scratch' transformer policy with Pred ASA achieved 50% on CALVIN and 71% on Meta-World (see Table 1); RT-Inspired (non-LLM architecture) Pred results: 35% on CALVIN and 27% on Meta-World. Note: these are architecture/baseline comparisons reported in the paper rather than ablations that remove LLM pretraining from the exact same model.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Discrete SemLang RL: converged in ~20M environment steps on Language Rearrangement. RVQ transfer to new Meta-World holdout tasks: adaptation finetune used 50 expert demonstrations per holdout task and achieved ~50% overall success across 5 unseen tasks with the fixed RVQ tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Discrete Pred RL required up to ~70M steps to converge on the same Language Rearrangement task; Pred ASA adaptation to the same 5 holdout Meta-World tasks with 50 demos per task achieved ~20% overall success under the same protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>SemLang reduced RL samples to converge by ~3.5x (20M vs ~70M steps). For few-shot finetuning on Meta-World holdouts, RVQ + MLLM achieved ~50% success versus ~20% for Pred with the same 50 demonstrations (≈2.5x higher success with same data).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key factors: reuse of the pretrained LLM token/head and embeddings when mapping discrete actions to semantically meaningful language tokens (SemLang); learned tokenization (RVQ) that gives fine-grained, adaptive quantization of continuous actions and allows the MLLM to leverage its sequencing/language modeling capabilities to predict action-token sequences; token filters to constrain autoregressive sampling in RL; retaining pretrained language/vision knowledge and finetuning with LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Action-space mismatch issues (text token space vs continuous motor space), poor reconstruction under naive tokenizations (Uniform or small VQ codebooks), large action-token output space creating exploration difficulty in RL (necessitating token filters), and VQ overfitting or insufficient precision for precise control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A pretrained multimodal LLM (LLaVA-1.5-7B) can be grounded into embodied control effectively when (1) continuous actions are represented via learned residual vector-quantized tokens (RVQ) to provide precision while remaining token-efficient, and (2) discrete actions are semantically aligned to the LLM's native language token space (SemLang). These strategies yield substantial gains in final success and in sample efficiency compared to naive discretizations or direct regression baselines, and the gains are specific to leveraging pretrained LLM knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Multimodal Large Language Models in Actions', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1715.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1715.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that adapts a Flamingo-style vision-language model for robotic manipulation and reports strong CALVIN results; cited here for empirical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoboFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RoboFlamingo (OpenFlamingo-based VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A vision-language model (OpenFlamingo variant) adapted to robot control; according to this paper it uses additional sensor inputs (including a gripper camera) and a different VLM backbone than the present work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper (presumably large-scale vision-language pretraining for Flamingo family models).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>CALVIN (ABC→D split)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Tabletop manipulation benchmark (CALVIN); RoboFlamingo is reported to obtain higher success on the same ABC→D split.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Model's native output is language tokens (VLM); mapping specifics are not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous 7DoF end-effector control and gripper state (CALVIN); RoboFlamingo additionally uses gripper camera input.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not detailed here; paper only states RoboFlamingo uses a different VLM and additional camera inputs and achieves higher empirical performance on CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RoboFlamingo used additional gripper camera input (in addition to fixed camera) per the comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported by this paper: RoboFlamingo achieves 82% success on CALVIN ABC→D (compared to 72% for the MLLM+RVQ in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Different VLM backbone and extra sensor modalities (gripper camera) likely contributed to improved performance per the paper's comparison remarks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RoboFlamingo achieves higher CALVIN success than the present LLaVA-based system, but direct comparisons are confounded by different VLM backbones and additional sensor inputs (gripper camera).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Multimodal Large Language Models in Actions', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1715.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1715.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GFlan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GFlan</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior model cited for comparison on BabyAI; reportedly a strong baseline when given compact state inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GFlan</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GFlan</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A prior method that operates on a compact ground-truth language/state representation and uses RL; reported as a strong performer on BabyAI in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper (cited as a prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>BabyAI (gridworld language instruction tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Gridworld tasks with language instructions (5 instruction templates) where GFlan uses a compact, ground-truth language description of state rather than raw RGB.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Native model outputs/score-based ranking over discrete actions treated as text in prior work (per citations).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete low-level actions (turn left, turn right, move forward, pick, drop, toggle).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Prior work (GFlan) operates from compact state descriptions and uses language-conditioned policies; mapping method details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>GFlan used compact ground-truth language state rather than raw RGB; in contrast, this paper uses RGB observations for the same BabyAI tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Cited result: GFlan achieves ~55% success on BabyAI evaluation split (higher than the MLLM+SemLang 40% reported here), but under different input assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Use of compact, structured state/language inputs lowers perceptual burden and likely improves sample efficiency/performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Different input modalities (compact state vs RGB) make direct comparisons misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GFlan performs better on BabyAI under compact state inputs; this highlights that perception modality (raw RGB vs structured state) substantially affects transferred performance for language-conditioned agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Multimodal Large Language Models in Actions', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1715.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1715.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaRP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work on Language Rearrangement that uses an MLP-based ASA; cited for baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaRP</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LLaRP</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A prior approach on Language Rearrangement that adapts language models into policies (uses an MLP for discrete action prediction in the cited comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper (cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Language Rearrangement (mobile manipulation rearrangement benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mobile manipulation rearrangement tasks where high-level skill selection is required across unseen houses and paraphrastic instruction robustness is evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Actions can be cast as text (high-level skill names) or predicted by an MLP; LLaRP used an MLP ASA per the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>70 high-level discrete skills (pick object by name, place on receptacle by name, nav receptacle, open/close receptacles, STOP).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Prior LLaRP used an MLP to predict discrete actions from model hidden states (categorical prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB head-mounted camera (336×336) in the Language Rearrangement benchmark used here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Cited baseline: LLaRP achieved ~42% on Language Rearrangement evaluation (this paper's SemLang MLLM result is 51%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Using an MLLM with semantically aligned action tokens (SemLang) improved over LLaRP's performance in the presented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantically aligning discrete actions to the LLM token space (SemLang) outperforms prior MLP-based adaption (LLaRP) for Language Rearrangement in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Multimodal Large Language Models in Actions', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1715.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1715.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior vision-language-action work that discretizes continuous actions uniformly and predicts tokens per action dimension; cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Prior model that uniformly discretizes continuous actions and predicts tokens corresponding to each action dimension (cited as an example ASA design).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper (cited prior work on VLM/LLM to robotic control).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Referenced in general as robotic control tasks (no specific 3D benchmark result detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>RT-2 is discussed as an approach to ground VLMs into actions by uniform discretization of continuous action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>RT-2 treats actions as discrete tokens by uniformly binning continuous action dimensions (text-token-like representation).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous control spaces discretized per dimension into bins (uniform quantization).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Uniform action tokenization: quantize each continuous action dimension into K uniform bins and predict a token per dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Not detailed here (work cited in related literature).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Simple uniform discretization can be effective in low-dimension action spaces but struggles when action dimensionality grows.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Uniform discretization often fails to accurately represent continuous actions at required precision, leading to poor performance in higher-dimensional control (as observed in this paper's experiments comparing Uniform to RVQ).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Uniform discretization (as used by RT-2) can be a practical ASA for some tasks but is outperformed by learned residual quantization (RVQ) in this paper when precision or action dimensionality is high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Multimodal Large Language Models in Actions', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1715.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1715.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VQ-BeT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VQ-BeT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that uses residual vector-quantized (RVQ) codebooks for continuous actions in non-MLLM settings; cited as related work motivating RVQ usage here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VQ-BeT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VQ-BeT (and related RVQ approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Prior methods that learn residual vector-quantized representations for continuous actions (RVQ) and show benefits in modeling continuous action distributions; not applied to MLLMs in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not specified in this paper (prior work used play/behavior datasets and multimodal behavior data).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Referenced across push/robot tasks in prior literature; not experimentally reused in this paper except as inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Prior work applied RVQ to continuous control / behavior datasets (e.g., play data) to discretize continuous actions for downstream modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable — these works learn discrete codebook indices representing continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous action vectors tokenized into learned codebook indices (VQ / RVQ).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Learned vector-quantized codebooks (VQ) and residual VQ (RVQ) that represent continuous actions via one or multiple discrete tokens decoded back to continuous controls.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Not specified in this paper for the prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>RVQ provides improved reconstruction precision and enables exponential increase of representable quantized actions via multiple codebooks; motivated this paper's RVQ ASA which worked well when combined with an MLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>VQ with insufficient codebook size can yield poor reconstruction and downstream policy performance; large codebooks may overfit.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learned residual quantization (RVQ) for continuous actions gives better reconstruction and downstream control performance than single-codebook VQ or uniform binning, and when coupled with an MLLM yields substantial gains on precise control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Multimodal Large Language Models in Actions', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RoboFlamingo <em>(Rating: 2)</em></li>
                <li>RT-2 <em>(Rating: 2)</em></li>
                <li>RT-1 <em>(Rating: 1)</em></li>
                <li>GFlan <em>(Rating: 2)</em></li>
                <li>LLaRP <em>(Rating: 2)</em></li>
                <li>VQ-BeT <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 1)</em></li>
                <li>VQ-VAE (Neural discrete representation learning / RVQ-VAE) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1715",
    "paper_id": "paper-270392025",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "LLaVA-1.5",
            "name_full": "LLaVA-1.5-7B",
            "brief_description": "The base multimodal large language model (MLLM) used in this work; a vision-language LLM that is finetuned with Action Space Adapters (ASAs) to produce embodied agent actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "LLaVA-1.5-7B (MLLM policy)",
            "model_agent_description": "A pretrained multimodal large language model (vision+language) used as the policy backbone. In this work the visual encoder and pretrained LLM are kept mostly frozen and the model is finetuned via LoRA together with an Action Space Adapter (ASA). The finetuned system uses an adapter head, action token embedding, and adapter decoder to autoregressively produce m action tokens per time step which are decoded into environment actions.",
            "pretraining_data_type": "Multimodal vision-language and language pretraining (image-text + language modeling); base LLM/token vocabulary pretrained on language.",
            "pretraining_data_details": "Not specified in detail in this paper; the authors state they use the pretrained LLaVA-1.5-7B model as the MLLM backbone and reuse its token vocabulary and language head for some ASAs; all further task adaptation is via supervised finetuning or RL with LoRA.",
            "embodied_task_name": "CALVIN, Meta-World (ML-45), Habitat Pick (HabPick), BabyAI, Language Rearrangement (LangR)",
            "embodied_task_description": "A set of 3D embodied benchmarks covering tabletop manipulation (CALVIN), robotic manipulation multi-task benchmark (Meta-World ML-45), mobile manipulation pick tasks in Habitat (HabPick), a gridworld with language instructions (BabyAI), and a mobile rearrangement benchmark with high-level skills (Language Rearrangement). Tasks require following natural language instructions and controlling the agent to satisfy object-manipulation or navigation goals.",
            "action_space_text": "Native pretraining output is natural language tokens (text token vocabulary) produced by the LLM; i.e., text token output space.",
            "action_space_embodied": "Varies by environment: continuous motor control vectors (e.g., 6–10 DoF end-effector / arm / base velocities plus gripper state) for CALVIN/Meta-World/HabPick; discrete high-level skills (70 skills) for Language Rearrangement; small discrete navigation/action set for BabyAI.",
            "action_mapping_method": "Action Space Adapters (ASAs) bridge the LLM token space and environment actions: (a) direct regression (Pred) from MLLM hidden states to continuous actions via an MLP; (b) reuse of LLM token space to predict natural-language action descriptions (SemLang) or numeric tokens (Lang); (c) learned tokenizations for continuous control (VQ and Residual VQ (RVQ)) where actions are encoded as learned codebook indices (tokens) that are decoded to continuous actions.",
            "perception_requirements": "RGB vision (fixed camera 200×200 or egocentric head camera 336×336 depending on environment); agent proprioception (joint angles, gripper state) is included where useful; no depth used in this work (except cited comparisons that use depth).",
            "transfer_successful": true,
            "performance_with_pretraining": "With the LLaVA-based MLLM finetuned and the recommended ASAs: RVQ achieves 72% success on CALVIN (ABC→D split), 84% success on Meta-World ML-45, and 29% on HabPick (MLLM + RVQ). For discrete action tasks, SemLang achieved 51% on Language Rearrangement and 40% on BabyAI (MLLM + SemLang).",
            "performance_without_pretraining": "Non-MLLM (trained-from-scratch) baselines reported in the paper: the 'Scratch' transformer policy with Pred ASA achieved 50% on CALVIN and 71% on Meta-World (see Table 1); RT-Inspired (non-LLM architecture) Pred results: 35% on CALVIN and 27% on Meta-World. Note: these are architecture/baseline comparisons reported in the paper rather than ablations that remove LLM pretraining from the exact same model.",
            "sample_complexity_with_pretraining": "Discrete SemLang RL: converged in ~20M environment steps on Language Rearrangement. RVQ transfer to new Meta-World holdout tasks: adaptation finetune used 50 expert demonstrations per holdout task and achieved ~50% overall success across 5 unseen tasks with the fixed RVQ tokenization.",
            "sample_complexity_without_pretraining": "Discrete Pred RL required up to ~70M steps to converge on the same Language Rearrangement task; Pred ASA adaptation to the same 5 holdout Meta-World tasks with 50 demos per task achieved ~20% overall success under the same protocol.",
            "sample_complexity_gain": "SemLang reduced RL samples to converge by ~3.5x (20M vs ~70M steps). For few-shot finetuning on Meta-World holdouts, RVQ + MLLM achieved ~50% success versus ~20% for Pred with the same 50 demonstrations (≈2.5x higher success with same data).",
            "transfer_success_factors": "Key factors: reuse of the pretrained LLM token/head and embeddings when mapping discrete actions to semantically meaningful language tokens (SemLang); learned tokenization (RVQ) that gives fine-grained, adaptive quantization of continuous actions and allows the MLLM to leverage its sequencing/language modeling capabilities to predict action-token sequences; token filters to constrain autoregressive sampling in RL; retaining pretrained language/vision knowledge and finetuning with LoRA.",
            "transfer_failure_factors": "Action-space mismatch issues (text token space vs continuous motor space), poor reconstruction under naive tokenizations (Uniform or small VQ codebooks), large action-token output space creating exploration difficulty in RL (necessitating token filters), and VQ overfitting or insufficient precision for precise control tasks.",
            "key_findings": "A pretrained multimodal LLM (LLaVA-1.5-7B) can be grounded into embodied control effectively when (1) continuous actions are represented via learned residual vector-quantized tokens (RVQ) to provide precision while remaining token-efficient, and (2) discrete actions are semantically aligned to the LLM's native language token space (SemLang). These strategies yield substantial gains in final success and in sample efficiency compared to naive discretizations or direct regression baselines, and the gains are specific to leveraging pretrained LLM knowledge.",
            "uuid": "e1715.0",
            "source_info": {
                "paper_title": "Grounding Multimodal Large Language Models in Actions",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RoboFlamingo",
            "name_full": "RoboFlamingo",
            "brief_description": "A prior work that adapts a Flamingo-style vision-language model for robotic manipulation and reports strong CALVIN results; cited here for empirical comparison.",
            "citation_title": "RoboFlamingo",
            "mention_or_use": "mention",
            "model_agent_name": "RoboFlamingo (OpenFlamingo-based VLM)",
            "model_agent_description": "A vision-language model (OpenFlamingo variant) adapted to robot control; according to this paper it uses additional sensor inputs (including a gripper camera) and a different VLM backbone than the present work.",
            "pretraining_data_type": "Not specified in this paper (presumably large-scale vision-language pretraining for Flamingo family models).",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "CALVIN (ABC→D split)",
            "embodied_task_description": "Tabletop manipulation benchmark (CALVIN); RoboFlamingo is reported to obtain higher success on the same ABC→D split.",
            "action_space_text": "Model's native output is language tokens (VLM); mapping specifics are not elaborated in this paper.",
            "action_space_embodied": "Continuous 7DoF end-effector control and gripper state (CALVIN); RoboFlamingo additionally uses gripper camera input.",
            "action_mapping_method": "Not detailed here; paper only states RoboFlamingo uses a different VLM and additional camera inputs and achieves higher empirical performance on CALVIN.",
            "perception_requirements": "RoboFlamingo used additional gripper camera input (in addition to fixed camera) per the comparison in this paper.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported by this paper: RoboFlamingo achieves 82% success on CALVIN ABC→D (compared to 72% for the MLLM+RVQ in this work).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Different VLM backbone and extra sensor modalities (gripper camera) likely contributed to improved performance per the paper's comparison remarks.",
            "transfer_failure_factors": "Not discussed in detail here.",
            "key_findings": "RoboFlamingo achieves higher CALVIN success than the present LLaVA-based system, but direct comparisons are confounded by different VLM backbones and additional sensor inputs (gripper camera).",
            "uuid": "e1715.1",
            "source_info": {
                "paper_title": "Grounding Multimodal Large Language Models in Actions",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GFlan",
            "name_full": "GFlan",
            "brief_description": "A prior model cited for comparison on BabyAI; reportedly a strong baseline when given compact state inputs.",
            "citation_title": "GFlan",
            "mention_or_use": "mention",
            "model_agent_name": "GFlan",
            "model_agent_description": "A prior method that operates on a compact ground-truth language/state representation and uses RL; reported as a strong performer on BabyAI in prior work.",
            "pretraining_data_type": "Not specified in this paper (cited as a prior work).",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "BabyAI (gridworld language instruction tasks)",
            "embodied_task_description": "Gridworld tasks with language instructions (5 instruction templates) where GFlan uses a compact, ground-truth language description of state rather than raw RGB.",
            "action_space_text": "Native model outputs/score-based ranking over discrete actions treated as text in prior work (per citations).",
            "action_space_embodied": "Discrete low-level actions (turn left, turn right, move forward, pick, drop, toggle).",
            "action_mapping_method": "Prior work (GFlan) operates from compact state descriptions and uses language-conditioned policies; mapping method details are not provided in this paper.",
            "perception_requirements": "GFlan used compact ground-truth language state rather than raw RGB; in contrast, this paper uses RGB observations for the same BabyAI tasks.",
            "transfer_successful": true,
            "performance_with_pretraining": "Cited result: GFlan achieves ~55% success on BabyAI evaluation split (higher than the MLLM+SemLang 40% reported here), but under different input assumptions.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Use of compact, structured state/language inputs lowers perceptual burden and likely improves sample efficiency/performance.",
            "transfer_failure_factors": "Different input modalities (compact state vs RGB) make direct comparisons misleading.",
            "key_findings": "GFlan performs better on BabyAI under compact state inputs; this highlights that perception modality (raw RGB vs structured state) substantially affects transferred performance for language-conditioned agents.",
            "uuid": "e1715.2",
            "source_info": {
                "paper_title": "Grounding Multimodal Large Language Models in Actions",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaRP",
            "name_full": "LLaRP",
            "brief_description": "Prior work on Language Rearrangement that uses an MLP-based ASA; cited for baseline comparison.",
            "citation_title": "LLaRP",
            "mention_or_use": "mention",
            "model_agent_name": "LLaRP",
            "model_agent_description": "A prior approach on Language Rearrangement that adapts language models into policies (uses an MLP for discrete action prediction in the cited comparison).",
            "pretraining_data_type": "Not specified in this paper (cited prior work).",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "Language Rearrangement (mobile manipulation rearrangement benchmark)",
            "embodied_task_description": "Mobile manipulation rearrangement tasks where high-level skill selection is required across unseen houses and paraphrastic instruction robustness is evaluated.",
            "action_space_text": "Actions can be cast as text (high-level skill names) or predicted by an MLP; LLaRP used an MLP ASA per the citation.",
            "action_space_embodied": "70 high-level discrete skills (pick object by name, place on receptacle by name, nav receptacle, open/close receptacles, STOP).",
            "action_mapping_method": "Prior LLaRP used an MLP to predict discrete actions from model hidden states (categorical prediction).",
            "perception_requirements": "RGB head-mounted camera (336×336) in the Language Rearrangement benchmark used here.",
            "transfer_successful": true,
            "performance_with_pretraining": "Cited baseline: LLaRP achieved ~42% on Language Rearrangement evaluation (this paper's SemLang MLLM result is 51%).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Using an MLLM with semantically aligned action tokens (SemLang) improved over LLaRP's performance in the presented experiments.",
            "transfer_failure_factors": "Not detailed here.",
            "key_findings": "Semantically aligning discrete actions to the LLM token space (SemLang) outperforms prior MLP-based adaption (LLaRP) for Language Rearrangement in this paper's experiments.",
            "uuid": "e1715.3",
            "source_info": {
                "paper_title": "Grounding Multimodal Large Language Models in Actions",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "A prior vision-language-action work that discretizes continuous actions uniformly and predicts tokens per action dimension; cited in related work.",
            "citation_title": "RT-2",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2",
            "model_agent_description": "Prior model that uniformly discretizes continuous actions and predicts tokens corresponding to each action dimension (cited as an example ASA design).",
            "pretraining_data_type": "Not specified in this paper (cited prior work on VLM/LLM to robotic control).",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "Referenced in general as robotic control tasks (no specific 3D benchmark result detailed in this paper).",
            "embodied_task_description": "RT-2 is discussed as an approach to ground VLMs into actions by uniform discretization of continuous action spaces.",
            "action_space_text": "RT-2 treats actions as discrete tokens by uniformly binning continuous action dimensions (text-token-like representation).",
            "action_space_embodied": "Continuous control spaces discretized per dimension into bins (uniform quantization).",
            "action_mapping_method": "Uniform action tokenization: quantize each continuous action dimension into K uniform bins and predict a token per dimension.",
            "perception_requirements": "Not detailed here (work cited in related literature).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Simple uniform discretization can be effective in low-dimension action spaces but struggles when action dimensionality grows.",
            "transfer_failure_factors": "Uniform discretization often fails to accurately represent continuous actions at required precision, leading to poor performance in higher-dimensional control (as observed in this paper's experiments comparing Uniform to RVQ).",
            "key_findings": "Uniform discretization (as used by RT-2) can be a practical ASA for some tasks but is outperformed by learned residual quantization (RVQ) in this paper when precision or action dimensionality is high.",
            "uuid": "e1715.4",
            "source_info": {
                "paper_title": "Grounding Multimodal Large Language Models in Actions",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "VQ-BeT",
            "name_full": "VQ-BeT",
            "brief_description": "Prior work that uses residual vector-quantized (RVQ) codebooks for continuous actions in non-MLLM settings; cited as related work motivating RVQ usage here.",
            "citation_title": "VQ-BeT",
            "mention_or_use": "mention",
            "model_agent_name": "VQ-BeT (and related RVQ approaches)",
            "model_agent_description": "Prior methods that learn residual vector-quantized representations for continuous actions (RVQ) and show benefits in modeling continuous action distributions; not applied to MLLMs in the cited work.",
            "pretraining_data_type": "Not specified in this paper (prior work used play/behavior datasets and multimodal behavior data).",
            "pretraining_data_details": "Not specified here.",
            "embodied_task_name": "Referenced across push/robot tasks in prior literature; not experimentally reused in this paper except as inspiration.",
            "embodied_task_description": "Prior work applied RVQ to continuous control / behavior datasets (e.g., play data) to discretize continuous actions for downstream modeling.",
            "action_space_text": "Not applicable — these works learn discrete codebook indices representing continuous actions.",
            "action_space_embodied": "Continuous action vectors tokenized into learned codebook indices (VQ / RVQ).",
            "action_mapping_method": "Learned vector-quantized codebooks (VQ) and residual VQ (RVQ) that represent continuous actions via one or multiple discrete tokens decoded back to continuous controls.",
            "perception_requirements": "Not specified in this paper for the prior work.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "RVQ provides improved reconstruction precision and enables exponential increase of representable quantized actions via multiple codebooks; motivated this paper's RVQ ASA which worked well when combined with an MLLM.",
            "transfer_failure_factors": "VQ with insufficient codebook size can yield poor reconstruction and downstream policy performance; large codebooks may overfit.",
            "key_findings": "Learned residual quantization (RVQ) for continuous actions gives better reconstruction and downstream control performance than single-codebook VQ or uniform binning, and when coupled with an MLLM yields substantial gains on precise control tasks.",
            "uuid": "e1715.5",
            "source_info": {
                "paper_title": "Grounding Multimodal Large Language Models in Actions",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RoboFlamingo",
            "rating": 2,
            "sanitized_title": "roboflamingo"
        },
        {
            "paper_title": "RT-2",
            "rating": 2
        },
        {
            "paper_title": "RT-1",
            "rating": 1
        },
        {
            "paper_title": "GFlan",
            "rating": 2
        },
        {
            "paper_title": "LLaRP",
            "rating": 2
        },
        {
            "paper_title": "VQ-BeT",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "VQ-VAE (Neural discrete representation learning / RVQ-VAE)",
            "rating": 1,
            "sanitized_title": "vqvae_neural_discrete_representation_learning_rvqvae"
        }
    ],
    "cost": 0.022122499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Grounding Multimodal Large Language Models in Actions
12 Jun 2024</p>
<p>Andrew Szot a.szot@apple.com 
Georgia Tech</p>
<p>Bogdan Mazoure 
Harsh Agrawal 
Devon Hjelm 
Mila</p>
<p>Zsolt Kira 
Georgia Tech</p>
<p>Alexander Toshev toshev@apple.com 
Apple 
Grounding Multimodal Large Language Models in Actions
12 Jun 20241C2FDDE17A8B3F29C824B8357A504266arXiv:2406.07904v1[cs.LG]"Pick apple" [0.720.24-0.21…] Continuous Actions Discrete Actions
Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI.In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM.We first generalize a number of methods through a unified architecture and the lens of action space adaptors.For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks.For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance.We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.Preprint.Under review.</p>
<p>Introduction</p>
<p>Multimodal Large Language Models (MLLMs), defined as Large Foundation Models that take as input text and images and generate text, have recently seen rapid progress and impressive performance [1][2][3][4][5][6][7][8][9][10][11][12][13].These models are important as they solve a large range of useful yet difficult natural language and image tasks, such as describing images, answering visual and textual questions, reasoning, and learning from a small number of examples.They have only recently improved to the point of being usable enough for general deployment with human non-experts [14][15][16].</p>
<p>While MLLMs are capable of describing real-world embodied concepts, their capabilities in embodied tasks are limited to using text for actions through generating code [17,18], representing actions as text [19], or extracting actions from internal representations [20,21].Grounding [22] MLLMs to generate actions extends their capabilities to embodied tasks, such as robot manipulation and navigation, and is of tremendous value for practical problems, potentially overcoming the high cost of training tabula rasa.Extending MLLMs to multimodal image generation enables object detection and segmentation, and image and video generation [3,[23][24][25][26][27].In embodied settings, grounding MLLMs via predicting agent affordances and generating actions yields effective policies capable of generalizing to new tasks [19,21,28,29].</p>
<p>A key and open challenge in grounding MLLMs, which limits their capabilities in embodied tasks, is the gap between the native output space, natural language, and the action space of embodied agents.This problem is particularly acute in continuous action spaces, where low-level controllers may require a high degree of precision.Across the literature, a number of architectures and ways of handling action spaces have been proposed, but there has not been a systematic study of these designs.Our contributions generalize prior attempts to adapt MLLMs to generate actions through an empirical study on which principles and strategies are necessary to effectively close the gap between the action spaces of MLLMs and embodied agents.We study various grounding re-parameterization Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces.In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions.For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ).For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization).</p>
<p>strategies, which we refer to as Action Space Adapter (ASA), across a range of embodiments, action spaces, and environments.In particular, we explore the following types of ASAs: (1) ASAs that directly generate actions from a new prediction policy using the MLLM hidden representations as input; (2) ASAs that reuse the native token space of the MLLM to encode actions; (3) and ASAs that introduce a new token space to encode the actions of the agent while adapting the MLLMs to predict these new tokens.</p>
<p>Further, we empirically identify important principles for designing ASAs.For continuous action spaces, learned tokenization with several vocabularies that residually model continuous actions gives the right modeling precision while using vocabularies of manageable sizes and, as a result, yields the best performance across all continuous control environments.This learned tokenization outperforms direct action prediction, indicating this approach allows the model to effectively learn a multimodal distribution over action spaces.In addition, the above tokenization strategy boosts performance when the policy is a MLLM, compared to other standard non-LLM-based policies, indicating that it manages to better tap into the knowledge of the model.</p>
<p>For discrete action spaces, we study ASAs that better align the embodied actions with the output space of the MLLM.We demonstrate that a semantic alignment between these -mapping discrete actions to semantically related tokens in the MLLM vocabulary -yields the best strategy compared to other adapters that either reuse or define a new vocabulary.The superiority of this strategy is evident in performance on environments with discrete action spaces and also in RL sample efficiency.Finally, the above principles are thoroughly validated across five embodied AI environments, three of which are robotic continuous control and two with discrete actions as illustrated in Figure 1.Altogether, we consider 114 language specified tasks.In the continuous case, the best tokenization achieves 72% on CALVIN [30], up from 68% for direct action regression and 28% for uniform action tokenization; and 84% on Meta-World [31], up from 61% for direct action regression and 75% for uniform tokenization.Similarly, in the case of discrete actions, the proposed semantically aligned action tokens yield 51% on LangR [21], up from 42% for direct action prediction.</p>
<p>Related Work</p>
<p>Prior works propose different Action Space Adapters (ASAs) to adapt MLLMs into policies.Some works use LLMs or MLLMs as zero-shot policies by prompting them to output text or code that can be executed as actions [18,[32][33][34][35][36][37][38].The ASA in this case is a given executor or low-level controller that takes text as input and outputs actions in the environment.Other works investigate adapting MLLMs for actions, but focus on a single ASA and environment.For example, RT-2 [19] uniformly discretizes continuous actions and predicts tokens corresponding to each of the action dimensions.RoboFlamingo [20], Lamo [39], and LLaRP [21] use an MLP to predict an environment action from an LLM hidden state.GFlan [40] treats discrete actions as text and ranks actions by the LLM log
+ g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 I P p e v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 b u s 1 u 5 r l f p N H k c R T u A U z s G D K 6 j D H T S g C Q y G 8 A y v 8 O Y I 5 8 V 5 d z 4 W r Q U n n z m G P 3 A + f w D 8 R 4 2 d &lt; / l a t e x i t &gt; l 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m N a 7 c S Z x 5 h v Y o x z c q J y 4 i x K K N E Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 I P r j f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q n d Z r d 3 X K v W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 RQ Z q p 3 i V 7 s b 0 U Q y h V Q Q Y 1 q + F 2 O Q E o 2 c C j Y u t B P D Y k K H p M 9 a l i o i m Q n S 6 c F j 9 8 g q X b c X a V s K 3 a n 6 e y I l 0 p i R D G 2 n J D g w 8 9 5 E / M 9 r J d i 7 C l K u 4 g S Z o r N F v U S 4 G L m T 7 9 0 u 1 4 y i G F l C q O b 2 V p c O i C Y U b U Y F G 4 I / / / I i q Z + W / Y v y + d 1 Z q X K d x Z G H A z i E Y / D h E i p w C 1 W o A Q U J z / A K b 4 5 2 X p x 3 5 2 P W m n O y m X 3 4 A + f z B 5 T 8 k E c = &lt; / l a t e x i t &gt; u m 1 t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 2 d M B U H Q Y S z m Y Z A i 5 6 p f P O o U Q I g = " &gt; A A A B 7 n i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 3 x d Q x 6 8 R j B P C B Z w + x k N h k y O 7 v M 9 A b C k o / w 4 k E R r 3 6 P N / / G S b I H T S x o K K q 6 6 e 4 K E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 4 b J k 4 1 4 3 U W y 1 i 3 A m q 4 F I r X U a D k r U R z G g W S N 4 P h 3 d R v j r g 2 I l a P O E 6 4 H 9 G + E q F g F K 3 U H H X x K V O T b q n s V t w Z y D L x c l K G H L V u 6 a v T i 1 k a c Y V M U m P a n p u g n 1 G N g k k + K X Z S w x P K h r T P 2 5 Y q G n H j Z 7 N z J + T U K j 0 S x t q W Q j J T f 0 9 k N D J m H A W 2 M 6 I 4 M I v e V P z P a 6 c Y 3 v i Z U E m K X L H 5 o j C V B G M y / Z 3 0 h O Y MO + 7 f G v Q = " &gt; A A A B 7 n i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 3 x d Q x 6 8 R j B P C B Z w + x k N h k y O 7 v M 9 A b C k o / w 4 k E R r 3 6 P N / / G S b I H T S x o K K q 6 6 e 4 K E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 4 b J k 4 1 4 3 U W y 1 i 3 A m q 4 F I r X U a D k r U R z G g W S N 4 P h 3 d R v j r g 2 I l a P O E 6 4 H 9 G + E q F g F K 3 U H H X x K f M m 3 V L Z r b g z k G X i 5 a Q M O W r d 0 l e n F 7 M 0 4 g q Z p M a 0 P T d B P 6 M a B Z N 8 U u y k h i e U D W m f t y 1 V N O L G z 2 b n T s i p V X o k j L U t h W S m / p 7 I a G T M O A p s Z 0 R x Y B a 9 q f i f 1 0 4 x v P E z o Z I U u W L z R W E q C c Z k + j v p C c 0 Z y r E l l G l h b y V s Q D V l a B M q 2 h C 8 x Z e X S e O 8 4 l 1 V L h 8 u y t X b P I 4 C H M M J n I E H 1 1 C F e 6 h B H R g M 4 R l e 4 c 1 J n B f n 3 f m Y t 6 4 4 + c w R / I H z + Q N d S o + a &lt; / l a t e x i t &gt; v 1
t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j a 9 P T V m u w j + 1 G M W g b Q w v b / K 4 H E c = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E r 2 P R i 8 c K p i 2 0 s W y 2 2 3 b p Z h N 2 J 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E h h 0 H W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o m D j V j P s s l r F u h d R w K R T 3 U a D k r U R z G o W S N 8 P R 7 d R v P n F t R K w e c J z w I K I D J f q C U b S S P + z i o 9 c t V 9 y q O w N Z J l 5 O K p C j 3 i 1 / d X o x S y O u k E l q T N t z E w w y q l E w y S e l T m p 4
Q t m I D n j b U k U j b o J s d u y E n F i l R / q x t q W Q z N T f E x m N j B l H o e 2 M K A 7 N o j c V / / P a K f a v g 0 y o J E W u 2 H x R P 5 U E Y z L 9 n P S E 5 g z l 2 B L K t L C 3 E j a k m j K 0 + Z R s C N 7 i y 8 u k c V b 1 L q s X 9 + e V 2 k 0 e R x G O 4 B h O w Y M r q M E d 1 M E H B g K e 4 R X= " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k n x 6 1 j 0 4 r G C a Q t t L J v t p l 2 6 2 Y T d i V B K f 4 M X D 4 p 4 9 Q d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X p l I Y d N 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 T J J p x n 2 W y E S 3 Q m q 4 F I r 7 K F D y V q o 5 j U P J m + H w d u o 3 n 7 g 2 I l E P O E p 5 E N O + E p F g F K 3 k D 7 r 4 W O 2 W y m 7 F n Y E s E y 8 n Z c h R 7 5 a + O r 2 E Z T F X y C Q 1 p u 2 5 K Q Z j q l E w y S f F T m Z 4 S t m Q 9 n n b U k V j b o L x 7 N g J O b V K j 0 S J t q W Q z N T f E 2 M a G z O K Q 9 s Z U x y Y R W 8 q / u e 1 M 4 y u g 7 F Q a Y Z c s f m i K J M E E z L 9 n P S E 5 g z l y B L K t L C 3 E j a g m j K 0 + R R t C N 7 i y 8 u k U a 1 4 l 5 W L + / N y 7 S a P o w D H c A J n 4 M E V 1 O A O 6 u A D A w H P 8 A p v j n J e n H f n Y 9 6 6 4 u Q z R / A H z u c P g x G O g Q = = &lt; / l a t e x i t &gt; h 2
t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 F D e H g 7 3 F q R 8 l A + f p t O H m I 5 D 5 3 Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E r 2 P R i 8 c K p i 2 0 s W y 2 2 3 b p Z h N 2 J 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E h h 0 H W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o m D j V j P s s l r F u h d R w K R T 3 U a D k r U R z G o W S N 8 P R 7 d R v P n F t R K w e c J z w I K I D J f q C U b S S P + z i Y 9 Q t V 9 y q O w N Z J l 5 O K p C j 3 i 1 / d X o x S y O u k E l q T N t z E w w y q l E w y S e l T m p 4
Q t m I D n j b U k U j b o J s d u y E n F i l R / q x t q W Q z N T f E x m N j B l H o e 2 M K A 7 N o j c V / / P a K f a v g 0 y o J E W u 2 H x R P 5 U E Y z L 9 n P S E 5 g z l 2 B L K t L C 3 E j a k m j K 0 + Z R s C N 7 i y 8 u k c V b 1 L q s X 9 + e V 2 k 0 e R x G O 4 B h O w Y M r q M E d 1 M E H B g K e 4 R X e H O W 8 O O / O x 7 y 1 4 O Q z h / A H z u c P 3 H 2 O v A = = &lt; / l a t e x i t &gt; h m t Action Tokens &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v z v W k i O q R F T / B l Y p 2 a s n C I k Z U V w = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E r 2 P R i 8 c K p i 2 0 s W y 2 2 3 b p Z h N 2 J 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E h h 0 H W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o m D j V j P s s l r F u h d R w K R T 3 U a D k r U R z G o W S N 8 P R 7 d R v P n F t R K w e c J z w I K I D J f q C U b S S n 3 b x 0 e u W K 2 7 V n Y E s E y 8 n F c h R 7 5 a / O r 2 Y p R F X y C Q 1 p u 2 5 C Q Y Z 1 S i Y 5 J N S J z U 8 o W x E B 7 x t q a I R N 0 E 2 O 3 Z C T q z S I / 1 Y 2 1 J I Z u r v i Y x G x o y j 0 H Z G F I d m 0 Z u K / 3 n t F P v X Q S Z U k i J X b L 6 o n 0 q C M Z l + T n p C c 4 Z y b A l l W t h b C R t S T R n a f E o 2 B G / x 5 W X S O K t 6 l 9 W L + / N K 7 S a P o w h H c A y n 4 M E V 1 O A O 6 u A D A w H P 8 A p v j n J e n H f n Y 9 5 a c P K Z Q / g D 5 / M H l X W O j Q = = &lt; / l a t e x i t &gt; u 1 t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F P + Z u f K v 0 w u L Q + x o Z 3 K U L l h 9 d n 8 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k n x 6 1 j 0 4 r G C a Q t t L J v t p l 2 6 2 Y T d i V B K f 4 M X D 4 p 4 9 Q d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X p l I Y d N 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 T J J p x n 2 W y E S 3 Q m q 4 F I r 7 K F D y V q o 5 j U P J m + H w d u o 3 n 7 g 2 I l E P O E p 5 E N O + E p F g F K 3 k Z 1 1 8 r H Z L Z b f i z k C W i Z e T M u S o d 0 t f n V 7 C s p g r Z J I a 0 / b c F I M x 1 S i Y 5 J N i J z M 8 p W x I + 7 x t q a I x N 8 F 4 d u y E n F q l R 6 J E 2 1 J I Z u r v i T G N j R n F o e 2 M K Q 7 M o j c V / / P a G U b X w V i o N E O u 2 H x R l E m C C Z l + T n p C c 4 Z y Z A l l W t h b C R t Q T R n a f I o 2 B G / x 5 W X S q F a 8 y 8 r F / X m 5 d p P H U Y B j O I E z 8 O A K a n A H d f C B g Y B n e I U 3 R z k v z r v z M W 9 d c f K Z I / g D 5 / M H l v m O j g = = &lt; / l a t e x i t &gt; u 2 t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t r 7 0 Z s n g K d 5 Q U Z k P 1 z w P I L G + Z 1 c = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S T i 1 7 H o x W M F 0 x b a W D b b b b t 0 d x N 2 J 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e l A h u 0 P O + n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o m D j V l A U 0 F r F u R c Q w w R U L k K N g r U Q z I i P B m t H o d u o 3 n 5 g 2 P F Y P O E 5 Y K M l A 8 T 6 n B K 0 U p F 1 8 l N 1 y x a t 6 M 7 j L x M 9 J B X L U u + W v T i + m q W Q K q S D G t H 0 v w T A j G j k V b F L q p I Y l h I 7 I g L U t V U Q y E 2 a z Y y f u i V V 6 b j / W t h S 6 M / X 3 R E a k M W M Z 2 U 5 J c G g W v a n 4 n 9 d O s X 8 d Z l w l K T J F 5 4 v 6 q X A x d q e f u z 2 u G U U x t o R Q z e 2 t L h 0 S T S j a f E o 2 B H / x 5 W X S O K v 6 l 9 W L + / N K 7 S a P o w h H c A y n 4 M M V 1 O A O 6 h A A B Q 7 P 8 A p v j n J e n H f n Y 9 5 a c P K Z Q / g D 5 / M H 8 G W O y Q = = &lt; / l a t e x i t &gt; u m t</p>
<p>Adapter Decoder</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P y l l e 2 8 v t w f D q + 2 h 9 m e D 0 U Z p probability to form a distribution over actions.At a high level, our work is distinct in that we study a variety of methods across multiple environments for learning ASAs.We focus on tasks with low zero-shot VLM performance, such as low-level control or long-horizon planning tasks.We summarize the differences between our investigation and prior work adapting VLMs for action in Appendix A.
b J Q = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 g H J E m Y n s 8 m Q 2 d l l p l c I S z 7 B i w d F v P p F 3 v w b J 8 k e N L G g o a j q p r s r S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j 2 6 n f e u L a i F g 9 4 j j h f k Q H S o S C U b T S A + 1 h r 1 x x q + 4 M Z J l 4 O a l A j n q v / N X t x y y N u E I m q T E d z 0 3 Q z 6 h G w S S f l L q p 4 Q l l I z r g H U s V j b j x s 9 m p E 3 J i l T 4 J Y 2 1 L I Z m p v y c y G h k z j g L b G V E c m k V v K v 7 n d V I M r / 1 M q C R F r t h 8 U Z h K g j G Z / k 3 6 Q n O G c m w J Z V r Y W w k b U k 0 Z 2 n R K N g R v 8 e V l 0 j y r e p f V i / v z S u 0 m j 6 M I R 3 A M p + D B F d T g D u r Q A A Y D e I Z X e H O k 8 + K 8 O x / z 1 o K T z x z C H z i f P 1 F y j d Y = &lt; / l a t e x i t &gt; a t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v z v W k i O q R F T / B l Y p 2 a s n C I k Z U V w = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 n E r 2 P R i 8 c K p i 2 0 s W y 2 2 3 b p Z h N 2 J 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E h h 0 H W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o m D j V j P s s l r F u h d R w K R T 3 U a D k r U R z G o W S N 8 P R 7 d R v P n F t R K w e c J z w I K I D J f q C U b S S n 3 b x 0 e u W K 2 7 V n Y E s E y 8 n F c h R 7 5 a / O r 2 Y p R F X y C Q 1 p u 2 5 C Q Y Z 1 S i Y 5 J N S J z U 8 o W x E B 7 x t q a I R N 0 E 2 O 3 Z C T q z S I / 1 Y 2 1 J I Z u r v i Y x G x o y j 0 H Z G F I d m 0 Z u K / 3 n t F P v X Q S Z U k i J X b L 6 o n 0 q C M Z l + T n p C c 4 Z y b A l l W t h b C R t S T R n a f E o 2 B G / x 5 W X S O K t 6 l 9 W L + / N K 7 S a P o w h H c A y n 4 M E V 1 O A O 6 u A D A w H P 8 A p v j n J e n H f n Y 9 5 a c P K Z Q / g D 5 / M H l X W O j Q = = &lt; / l a t e x i t &gt;
Investigating action representations in embodied settings is not new.Some works learn representations of actions to help generalization to new actions or operating in large action spaces [41,42] in the context of Reinforcement Learning (RL).Our study proposes ASAs for tokenizing continuous actions, and other works use different types of discretization or tokenization strategies on continuous action spaces.[43,44] use k-means to discretize continuous actions to help learn from multimodal behavior datasets, such as from play data or data from different experts.VQ-BeT [45] finds learning a residual VQA (RVQ) codebook for continuous actions works best but does not apply this idea to MLLMs.</p>
<p>More broadly, prior works have adapted MLLMs for modalities other than actions, such as object bounding boxes and image generation, both being continuous in nature while the latter of high dimension.For example, [27,46] train MLLMs to output spatial reference tokens to ground text responses in image regions.For image generation, [47] adapt MLLMs to generate image patches; [48,49] tokenize images using a VQ-VAE model and adapt MLLMs to generate images by decoding these image tokens, which has inspired us to use the same learned tokenization; [50] uses an RVQ model [51] to generate images, similarly to our best performing tokenization scheme.</p>
<p>Method</p>
<p>In order to solve an embodied task, an agent learning in an interactive environment must select a decision from a set of valid actions.For example, an action space could be a set of keyboard presses for a video game or a real-valued vector that controls a robotic manipulator.Our work studies how to best adapt a MLLM, which is originally trained to output text tokens, to instead model actions from a given environment.We refer to the module that bridges a MLLM with a certain action space as an Action Space Adapter (ASA) (see Figure 2).</p>
<p>Problem Setting</p>
<p>Our analysis focuses on language-specified tasks with visual observations.Specifically, we consider a goal-specified Partially-Observable Markov Decision Process (POMDP) [52] that has an observation space O, action space A, and goal space G.For brevity, we omit other elements of the MDP.In our setting, G is a textual description of the task to solve.O consists of RGB visual perception and agent proprioception.We consider a range of different action spaces A that broadly fall into two categories -discrete and continuous.The primary objective is to learn a language-conditioned policy that maps observations and the instruction text to an action π(a|o, g).As later described in Section 3.3, we learn this policy through supervised fine tuning from expert demonstrations or reinforcement learning that maximizes the expected discounted cumulative reward of the POMDP.</p>
<p>From Vision and Language to Action</p>
<p>The process studied here for adapting MLLMs for decision making is illustrated in Figure 2. The MLLM policy takes as input a textual instruction describing the downstream task, a sequence of past observations in the task and outputs an action in the agent's action space.In the bottom left of Fig. 2, the task description, as well as the environment description, are first encoded to produce language embeddings.To these embeddings, the MLLM then appends a sequence of visual embeddings from the current observation o t .Since visual embeddings can often be comprised of a large number of tokens (the popular LLaVA-1.5 model [6] has 556), we introduce a downsampling layer to enable the MLLM to attend over a longer history of observations.In practice, we take the downsampling layer to be a Perceiver model [53], a learnable transformation that reduces the number of tokens from the visual encoder before being used as input to the MLLM.</p>
<p>The sequence of language and visual embeddings is passed through the MLLM, whose final hidden state h 1 t encodes the entire input.The ASA, whose trainable parameters are denoted θ, is comprised of three parts: (1) an adapter head, (2) an adapter embedding, and (3) an adapter decoder.The hidden state is first passed through the adapter head to produce action tokens u 1 t = A θ (h 1 t ).The action tokens are then embedded using the action embedding into E θ (u 1 t ), and passed autoregressively through the MLLM to produce further hidden embeddings h 2 t , . . ., u m t and associated action tokens u 2 t , . . ., u m t , resulting in total m tokens per time step.The predicted action tokens are then decoded into the final action a t by the adapter decoder, which produces the final action a t = D θ (u 1 t , .., u m t ).As a t ∈ A, it is then executed in the environment to produce o t+1 , and the process continues.</p>
<p>Next, we describe possible ASA implementations for discrete and continuous action spaces.</p>
<p>Discrete Action Spaces</p>
<p>We define the following action spaces adapters for a discrete action space A:</p>
<p>Categorical Prediction (Pred): Implement the action space adapter as an MLP network, which predicts the logits of a categorical distribution over environment actions from the MLLM hidden state.The adapter head is an MLP that maps the hidden state h 1 directly to an action a ∈ A. This amounts to producing a single action token u 1 , which directly corresponds to the action a, with the action decoder being an identity map.Both the adapter head and token embeddings are initialized from scratch.This type of ASA is used by [21].</p>
<p>Semantic Language (SemLang): The action space adapter predicts natural language text that maps to a discrete action.First, each action a ∈ A is described with freeform text tokenized as (l 1 , . . ., l m ).The MLLM then autoregressively predicts a sequence of m tokens, which are then decoded by the adapter decoder to the corresponding action.For example, in an action space choosing a high-level skill a could be described as "pick apple", which is tokenized as [5839, 26163] with the LLaMA tokenizer.The MLLM then must sequentially predict token 5839, then token 26163 to call this action.Sequences of tokens corresponding to invalid actions are either avoided entirely with the token filter described in Section 3.3 or treated as a no-op.Both the adapter head and the token embeddings are re-used to be the pretrained LLM's language head and embedding layer, respectively, meaning no additional parameters over the pretrained MLLM are added.This type of ASA is used by [29].</p>
<p>Non-Semantic Language (Lang): Actions are mapped to language tokens, but instead of semantically meaningful descriptions of the actions as with SemLang, the actions are mapped to sequences of numbers.For example, "pick apple" is represented with the string "5 3".The policy must then output the tokens corresponding to this text to call this pick action.</p>
<p>Continuous Action Space Adaptors</p>
<p>We define the following four ASAs for a continuous D-dimensional action space A: the first ASA predicts in the original action space while the other three use tokenization.At training time, we learn a policy to predict these action tokens from the ASA.At test time, we employ an action decoder that maps these action tokens to actions in the original space A.</p>
<p>Continuous Regression (Pred): Regress to the original continuous action from the MLLM hidden state h 1 t .This is achieved via a single-layer MLP network, which is trained using MSE loss.This ASA is used by [20,39].</p>
<p>Uniform Action Tokenization (Uniform):</p>
<p>The simplest approach is to use uniform binning of the action space.In particular, we express each action as a sequence of D tokens by quantizing each of the D action dimensions into one out of K uniform bins:
Uniform(a) = (k 1 . . . k D ) such that a d ∈ bin(k d , d)
where bin(k, d) denotes the k th bin along the d th action dimension.If m d and M d denote the lower and upper bounds respectively of the d th action dimension, then its definition reads bin(k,
d) = [m d + k M d −m d K , m d + (k + 1) M d −m d K ].
At test time, we decode predicted action tokens to the center of the corresponding bins for each dimension.This type of ASA is used by [19].</p>
<p>Vector Quantized Tokenization (VQ): To adapt the tokenization to the particular action space, we propose to use learned tokenization.In particular, we express each action as a single token that corresponds to the closest action code from a learned codebook V .Using encoder network f θ that maps actions to a latent embedding space:
VQ(a) = (k 1 ) where k 1 = arg min k ||f θ (a) − v k || 2 2
where v k ∈ V .The codebook V of size K is learned over an offline dataset D of actions using a VQ-VAE [54].We overwrite K infrequently used tokens from the LLM vocabulary to represent V .We defer the full details of this tokenization process to Appendix C.2.</p>
<p>Residual Vector Quantized Tokenization (RVQ): Precise control requires precise action modeling that can suffer after tokenization.To increase the precision of a learned tokenization, we further investigate the use of a sequence of several action tokens as in Uniform.Similar to VQ, these tokens are from M action codebooks V m , m ∈ {1, . . ., M }.However, each codebook models the residual space obtained after modeling the action using preceding codebooks, thus each subsequent token captures increasingly finer action information:
RVQ(a) = (k 1 , . . . k M ) where k m = arg min k f θ (a) − m−1 i=1 v i ki − v m k 2 2
where v i k ∈ V i is the k th code from the i th codebook.Such tokenization can be learned using Residual VQ-VAE [RVQ-VAE, 50] on an offline dataset of actions.The actual number of token sequences we can represent is K M .Hence, RVQ presents the opportunity to exponentially increase the action space quantization without having to drastically increase the size of the learned individual codebooks.</p>
<p>Training</p>
<p>We use LLaVA-1.5-7B[6] as the base MLLM.In the complex environments we consider, the zeroshot performance of MLLM is poor across all ASAs, even with detailed prompts.We therefore finetune the MLLM with interactive (i.e., action-labeled) data to make it more suited for interacting with a dynamic environment.</p>
<p>Supervised Fine Tuning (SFT) with Expert Demonstrations: We finetune the MLLM for interactive tasks using a dataset of expert demonstrations.Each demonstration contains (1) a language description of the task, (2) a sequence of observations, and (3) a sequence of actions that successfully solve the task.Note that in this work, we are primarily interested in learning imitation policies from offline data, which can be extended to offline reinforcement learning if per-timestep rewards are included in the dataset.Specifically, we train the MLLM with supervised learning to predict the expert actions from the observations and language description in the data.While the pre-trained LLM and the visual encoder remain frozen, we finetune the ASA, the visual downsampler, and parts of the LLM with LoRA [55].In total, the model has ≈ 100M learnable LLM parameters and ≈ 40M learnable downsampler and ASA parameters.The learned tokenization schemes (RVQ and VQ) have an additional pre-training phase, where the VAE models are first trained on actions from the offline dataset and then frozen to prevent further updates in later stages.</p>
<p>Reinforcement Learning (RL) from Environment Feedback We can also optionally finetune the MLLM to optimize an environment reward using RL.However, predicting actions in the MLLM token space dramatically increases the number of possible action predictions, with many possible predictions corresponding to no valid action.For example, there are 32,000 tokens in the LLaMA text tokenizer, giving 32, 000 m possible predictions by the model with m tokens per action.This makes exploration difficult in RL as only a small fraction of the possible actions are valid.AMLM therefore uses a token filter to restrict the autoregressive sampling to only be from token sequences corresponding to valid actions.The token filter is a function M (l 1 t , . . ., l j−1 t</p>
<p>) that produces a binary mask over all tokens to represent valid tokens for the jth decoding step.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>We study adapting MLLMs for action across a variety of environments with different embodiments and action spaces.All environments provide RGB visual observations and a natural language instruction specifying the goal to achieve.We provide the important environment details below and defer complete details to Appendix B.</p>
<p>CALVIN [30]: This manipulation benchmark tests the ability of a tabletop robot to interact with an object to complete a natural language instruction.The continuous actions specify 6DoF end-effector control and the binary gripper state.The observation is a 200 × 200 RGB image from a fixed-position camera.We use the ABC → D split of the benchmark with 34 tasks, and the agent is evaluated on unseen instruction phrasings and table background.[56]: We use the ML-45 version of this tabletop manipulation benchmark which has 45 tasks.The action space is continuous control specifying 3DoF end-effector translation and the continuous gripper state.The observations are 200 × 200 RGB images from a fixed camera.The agent is evaluated on unseen object and robot starting states.</p>
<p>Meta-World</p>
<p>Habitat Pick (HabPick) [57]: A mobile manipulation robot must pick up an object specified by name from a receptacle.The continuous actions specify the 7DoF relative joint positions of the arm, the 2D base velocity, and the gripper state.The observations are 336 × 336 RGB images from the robot's egocentric head camera.The instruction specifies the name of the object type to pick up.The evaluation distribution is on unseen houses and new arrangements of objects.</p>
<p>BabyAI [58]: BabyAI is a grid world task where an agent navigates and interacts with objects to complete an instruction.The discrete action space consists of navigation and interaction actions.The observation is a 200 × 200 RGB top-down view.We use the five tasks from [40], and we report generalization to instructions rephrased with synonyms.</p>
<p>Language Rearrangement (LangR) [21]: A mobile manipulation robot must rearrange objects to complete instructions like "store all the fruit in the fridge".The discrete actions are 70 high-level skills to interact with objects and navigate.The observation is a 336 × 336 RGB head camera.Evaluation instructions test generalization to unseen houses and 10 unseen instruction datasets measuring paraphrastic robustness and behavior generalization.</p>
<p>Continuous Action Space Adapter Comparison</p>
<p>We first study adapting MLLMs through Uniform, Pred, VQ, and RVQ action space adapters for the continuous action environments CALVIN, Meta-World and HabPick.</p>
<p>RVQ is the best performing continuous action ASA.The results in Figure 3 show that the RVQ action adapter consistently outperforms all other ASA approaches across all environments.While Pred is the second best performing method on all tasks, except on Meta-World, RVQ outperforms it by a 12% average absolute difference.One hypothesized reason for this is that Pred only learns unimodal distributions of actions, which hurts performance when learning from diverse demonstrations [43][44][45].Another potential reason is the tokenization from RVQ allows the MLLM to better leverage its existing knowledge, whereas the Pred ASA requires training a new MLP network from scratch.</p>
<p>Uniform performs poorly on the majority of the tasks, where RVQ outperforms on average by a 27% absolute increase.A reason for this is that the Uniform discretization can fail to accurately represent the continuous actions.The performance of Uniform is also closely related to the action dimension.In Meta-World with 4 action dimensions, Uniform performs well.However, Uniform suffers with the 7 action dimensions in CALVIN and the 10 action dimensions in HabPick.</p>
<p>RVQ also outperforms VQ by a 18% absolute difference averaged over all environments.This is due to VQ having worse action reconstructions than RVQ.In Meta-World, both RVQ and VQ policies reach a similar cross-entropy loss on holdout trajectories during finetuning.However, on this same data, RVQ has a reconstruction mean squared error (MSE) of 0.005 while VQ has a 10x higher reconstruction MSE of 0.05.Increasing the VQ codebook size does not close this gap.We vary the VQ codebook size in powers of 2 from 2 7 to 2 11 .Figure 4b shows the VQ reconstruction loss decreases with larger codebooks but does not even close the gap to the 2 7 RVQ codebook size.This poor reconstruction manifests in poor downstream policy performance as demonstrated by Figure 4a where policies trained with the VQ ASA plateau in success rate at codebook size 2 9 .VQ policies even decrease in performance at codebook size 2 11 , potentially due to overfitting to the large codebook.We further characterize the performance of RVQ and VQ in Figure 5 by breaking down the performance per task group in Meta-World and CALVIN.The task groups, which are fully listed in Appendix B.6, correspond to tasks with related required behaviors.Both RVQ and VQ do similarly on "articulated" object interactions (like opening drawers or doors).These tasks require less precise control since many contact points on the articulated link and broad pushing or pulling behavior can achieve the desired behavior.On the other hand, RVQ outperforms VQ on "pressing" tasks that require pushing a button.These tasks require more precise control since the agent needs to push the button all the way to a desired state.VQ often reaches the button but fails to press it all the way.The same is also true of other precise control tasks like picking, pulling, and rotating.</p>
<p>A potential explanation of RVQ's success can be attributed to adaptive localization of the model's errors, similar to prior work in residual reinforcement learning [61] and Bellman error bases [62].</p>
<p>A sufficient codebook size and number of codebooks are necessary for RVQ.In Figure 4a, we show that RVQ policy performance improves in performance with a larger codebook size in Meta-World.Notably, RVQ performs poorly at 29% success rate with codebook size 16 compared to 84% success at codebook size 512.These observations also align with the codebook size decreasing reconstruction error in Figure 4b.In Figure 4c, we compare the effect of the number of codebooks on performance.As earlier discussed with the performance of VQ, one codebook results in poor action reconstruction and, thus, bad policy performance.However, increasing the number of codebooks too much to 6 also hurts performance despite decreasing reconstruction loss.Likewise to the finding that Uniform performs poorly with larger action dimension since there are more tokens per action, increasing the number of codebooks also hurts policy learning.</p>
<p>RVQ tokens transfer to new tasks.We take the model trained on the 45 Meta-World tasks and finetune it on 5 unseen tasks.We collect 50 demonstrations for per task and finetune the policy on all task data.We use the same RVQ ASA trained only on data from the 45 tasks.Figure 6a shows the success rate of adapting RVQ compared to an Pred ASA.RVQ outperforms Pred across all tasks, achieving a 50% vs. 20% overall success rate.This demonstrates the RVQ tokens are flexible enough to be applied to new tasks.</p>
<p>The gains from RVQ are unique to MLLMs.Next, we analyze the unique interaction between the RVQ tokens and the MLLM policy.While we demonstrated that the RVQ ASA performs best, is this improvement due to the MLLM being able to leverage these new tokens or the added action representation ability from the separately trained RVQ decoder?To test this, we compare to two policy architectures that do not use LLMs:</p>
<p>• Scratch: This is the same architecture as the MLLM-based policy, but with a smaller 300M parameter non-pretrained transformer.• RT-Inspired: This method uses a ResNet visual encoder, pretrained Flan [63] language embedding and decoder transformer-based policy.The entire policy is trained from scratch.This method is inspired by RT-1 [64], which does not have publicly released code.</p>
<p>Table 1 compares the effect of Pred versus RVQ ASAs on CALVIN, Meta-World and HabPick for these three policy policy architectures.As already established for the MLLM, RVQ is consistently better than VQ.However, for the same policy architecture trained from scratch, RVQ can hurt the performance over Pred.In CALVIN the success drops −7% and in Meta-World the performance drops −15%.This highlights that MLLM can leverage its existing knowledge about sequencing language tokens to sequencing action tokens.However, we find that for the smaller RT-Inspired policy network, the RVQ ASA consistently helps, which we hypothesize is because the added RVQ network and separate training help compensate for the lack of policy network capacity.We also note that RVQ may more consistently outperform Pred on demonstrations that explicitly contain multimodal action sequences [43][44][45].</p>
<p>Discrete Action Adapter Comparison</p>
<p>SemLang performs the best.In Figure 3, SemLang outperforms the next best ASA (Pred), by 9% on Language Rearrangement and 8% on BabyAI.SemLang performs especially well on tasks with explicit high-level language actions in Language Rearrangement (e.g., "pick apple") where prior work has shown text-only LLM policies achieve non-zero success [21].SemLang also does well on the BabyAI tasks with discrete low-level actions like "move left".Additionally, Lang performs the worst in both environments, achieving 14% lower success on Language Rearrangement and 11% lower on BabyAI than SemLang.We hypothesize this is because the MLLM has to repurpose its knowledge to leverage these newly assigned action tokens, whereas a newly initialized Pred allows extracting this knowledge from the MLLM hidden state.</p>
<p>SemLang enables sample efficient RL.In Figure 6b, we compare the RL training curves for the ASAs in Language Rearrangement.In addition to helping with better generalization, SemLang also enables sample efficient RL training.SemLang converges in training performance after just 20M training samples, whereas Pred requires up to 70M steps to fully converge.</p>
<p>Token filter is crucial for language-based action spaces.In Figure 6b, we show the training of SemLang without the token filter, which restricts policy outputs to only valid action token sequences.Without the token filter, SemLang is unable to learn in the large text action space.</p>
<p>Empirical Comparison to Prior Work</p>
<p>The contributions of this work are an empirical analysis of ASAs under controlled settings on various embodied environments.Direct comparisons to prior work are challenging due to different training algorithms, policy architectures, or assumptions about input modalities.Regardless, in this section, we seek to contextualize our RVQ and SemLang MLLM results against prior work.In Meta-World,   to the best of our knowledge, RVQ at 84% success on ML-45 sets a new state-of-the-art result, compared to 79% from DualMind [65].In CALVIN, RVQ at 72% success underperforms a similar work RoboFlamingo which achieves 82% success on the ABC → D split.However, RoboFlamingo uses a different MLLM and uses an additional gripper camera input.In Language Rearrangement, SemLang sets a state-of-the-art result with 51% success compared to 42% from LLaRP [21].In BabyAI, SemLang at 40% success rate underperforms GFlan [40], which achieves 55% success.However, we use RGB visual observations, while GFlan operates from a compact, ground truth language state description.In Appendix A.1, we compare these differences in more detail.</p>
<p>Limitations and Conclusion</p>
<p>In this work, we studied various action space adapters (ASAs) across a variety of embodiments, action spaces, and environments.We provide a generalization of prior works through the lens of action space adaptors, and for both discrete and continuous action spaces demonstrate designs that we show can leverage the knowledge within the MLLM.Our findings conclude that for continuous actions, it is best to learn action tokens that accurately model the action distribution, while for discrete actions, it is best to reason over semantic language descriptions of actions.We verify these ideas across 114 embodied AI tasks in 5 diverse environments.</p>
<p>A limitation of our work is all our analysis is under a single MLLM (LLaVA</p>
<p>A Prior Work Comparison</p>
<p>In this section we expand on the differences between the prior work in action space adaptation mentioned in Section 2 and our investigation.Table 2 compares our investigation to prior work along several key dimensions.We emphasize that unlike prior works, ours studies a variety of action space adapters under a greater diversity of environments.</p>
<p>A.1 Empirical Comparison to Prior Work</p>
<p>We report performance on standard benchmarks which prior work has also extensively studied.However, even within the benchmarks there are differences in training algorithms and sensor input assumptions that make direct comparison to prior work difficult.Regardless of these differences, we study different ASAs for MLLMs in a consistent experimental setting.We also describe differences between the empirical setups of ours and prior works that perform well on these benchmarks.</p>
<p>Meta-World (MLLM +RVQ 84% success rate on ML-45): To the best of our knowledge, our 84% is the highest reported on Meta-World ML-45 so far.Anand et al. [66] operates under similar sensor assumptions and achieves 77% success with MuZero [67].DualMind [65] achieves 79% success rate on ML-45 and outperforms other generalist agents like Gato [68].However, DualMind uses privileged simulator information about the joint states and object positions while we only use RGB visual observations.</p>
<p>CALVIN (MLLM +RVQ 72% success rate): RoboFlamingo achieves a higher 82% success rate on the same ABC → D task.However, RoboFlamingo uses the OpenFlamingo VLM while we use LLaVA.RoboFlamingo use the gripper and fixed camera while we only use the fixed camera.More recent work like 3D Diffuser Actor [69] practically solves the ABC → D task, achieving 96% success rate.However, this work uses depth inputs, and a diffusion model policy that predicts keypoints for the end-effector rather than underlying actions.Our work uses only RGB visuals, uses a MLLM policy and predicts relative end-effector poses rather than keypoints.</p>
<p>Language Rearrangement (SemLang 51% success rate): This outperforms the prior highest reported number of 42% on the overall evaluation set from LLaRP [21].</p>
<p>BabyAI (SemLang 40% success rate): GFlan [40] achieves 55% success on the same evaluation split.However, the GFlan policy takes as input a ground truth language description of the state, while our policies take as input a 200 × 200 RGB top down rendering of the environment.GFlan also trains the policy with reinforcement learning while we train with supervised learning.</p>
<p>B Environment Details</p>
<p>An overview of the environments is visualized in Figure 7.This figure visualizes the training observations input to the agent.We run experiments on 5 environments, and each environment in turn consists of multiple tasks.We arrive at the task count of 114 in the main paper through 45</p>
<p>B.1 Meta-World</p>
<p>Tasks: We use the ML-45 benchmark from Meta-World [56].Each of the 45 tasks are specified with a fixed language instruction.We use the task descriptions from Appendix Section A of Yu et al. [56].</p>
<p>Observation Space: 200 × 200 RGB images from a fixed camera position.To render the visual observations, we only use the "corner4" camera position as this gives an unobstructed view of the robot in most of the tasks.</p>
<p>The task ends in failure if the agent excessively collides with the scene, drops the object, or picks up the wrong object.The agent starts within 2 meters of the object and facing towards the receptacle but with random noise N (0, 1.57) applied to the direction of facing directly at the receptacle.The maximum number of steps per episode is 300 steps.</p>
<p>Observation Space: A 336 × 336 head-mounted RGB camera.</p>
<p>Action Space: The action space is 10DoF control of the arm, base and gripper.The first 2 dimensions control the linear and angular velocity of the base.The next 7 dimensions control the relative joint offsets of the arm.The final dimension controls whether the suction gripper is engaged or not.</p>
<p>Training: We first train a privileged policy with RL to complete the task.This policy takes as input the egocentric depth image and the ground truth position of the target object to pick up.We collect 20k successful trajectories.</p>
<p>Evaluation: We evaluate on the test episodes from Szot et al. [70] which are 1, 000 episodes in unseen home layouts.</p>
<p>B.4 BabyAI</p>
<p>Tasks: The tasks all occur in a 6 × 6 grid populated with interactable objects.We use the task definitions from Carta et al. [40].This consists of the following 5 instruction templates: "Go to <object>", "Pick up <object>", "Put <object A> next to <object B>,", "Pick up <object A> then go to <object B> and Go to <object B> after pick up <object A>", "Unlock <door>".The maximum number of steps per episode is 50 steps.</p>
<p>Observation Space: 200 × 200 RGB observation as a top down of the 6 × 6 scene.Note this is a more challenging observation space than prior gridworld navigation tasks that provide the current view as a compact entity specific array [58] or by a language description [40].</p>
<p>Action Space: The action space consists of 6 actions consisting of: turn left, turn right, move forward, pick, drop and toggle.</p>
<p>Training: We collect 1,000 demonstrations for each of the 5 templates.We randomly sample an instruction and starting state configuration for every demonstration.We use the expert planner from Chevalier-Boisvert et al. [58] to generate the demonstrations.</p>
<p>Evaluation: We report performance on the unseen synonyms generalization test, described in Section 4.2 of Carta et al. [40].We evaluate on 20 episodes per template type, giving 100 total evaluation episodes.</p>
<p>B.5 Language Rearrangement</p>
<p>Tasks: An agent starts in an unseen house and must complete a rearrangement task from a language instruction.</p>
<p>Observation Space: The agent has a 336 × 336 head-mounted RGB camera.We increase the camera resolution from 256 × 256 in the original Language Rearrangement task to match the input resolution of the LLaVA CLIP encoder.</p>
<p>Action Space: We use the same action space as from the original Language Rearrangement benchmark Szot et al. [21].The agent can select between 70 high-level skills that include picking up objects by name, navigating to receptacles, placing on receptacles by name, and opening and closing receptacles by name.</p>
<p>Training: Since Language Rearrangement does not provide any demonstrations and due to the emphasis on exploration in the problem, they are not readily obtainable, even with oracle planners.Therefore, we opt to train policies with reinforcement learning from the environment reward provided by the Language Rearrangement task.</p>
<p>Evaluation:</p>
<p>We evaluate on all 10 evaluation datasets from Language Rearrangement consisting of 1,000 evaluation episodes on unseen scenes.</p>
<p>B.6 Task Groupings</p>
<p>In Section 4 we breakdown the performance on CALVIN and MetaWorld for task groupings.Each of the task groupings consists of multiple tasks from the benchmark.We grouped tasks in the following way:</p>
<p>MetaWorld:</p>
<p>• Articulated: "door-close", "door-open", "drawer-close", "drawer-open", "faucet-open", "faucetclose", "handle-press-side", "handle-press", "window-open", "window-close" • Press: "button-press-topdown", "button-press-topdown-wall", "button-press", "button-press-wall", "coffee-button" • Push: "plate-slide", "plate-slide-side", "plate-slide-back", "plate-slide-back-side", "push-back", "push", "push-wall", "stick-push", "sweep-into", "sweep", "soccer", "coffee-push" • Pick: "assembly", "basketball", "dial-turn", "disassemble", "hammer", "peg-insert-side", "pegunplug-side", "pick-out-of-hole", "pick-place", "pick-place-wall", "reach", "reach-wall", "shelfplace" • Pull: "coffee-pull", "handle-pull-side", "handle-pull", "lever-pull", "stick-pull" CALVIN:</p>
<p>• Articulated: "move slider left", "open drawer", "close drawer", "move slider right" • Press: "turn off led", "turn on led", "turn on lightbulb", "turn off lightbulb" • Lift: "lift blue block slider", "lift pink block table", "lift red block slider", "lift red block table", "lift pink block slider", "lift blue block table" • Push: "push pink block right", "push blue block right", "push red block left", "push pink block left", "push red block right", "push blue block left", "push into drawer" • Rotate: "rotate red block right", "rotate red block left", "rotate pink block left", "rotate pink block right", "rotate blue block right", "rotate blue block left"</p>
<p>C Further Policy Details C.1 Prompt Details</p>
<p>In addition to inputting the task instruction to the LLM, we also format the instruction with a prompt.We base our prompt off the prompt used in LLaVA.For all continuous control tasks, we use the prompt template "Prompt: control the robot.USER: <INSTRUCTION> ASSISTANT: ".For discrete action space tasks, we describe the available actions to the agent in the prompt as well.For BabyAI, this is the prompt template "Prompt: Control the red triangle to complete the instruction using left, right, forward, pick, drop and toggle.USER: <INSTRUCTION> ASSISTANT: ".For Language Rearrangement, this is the prompt template "Prompt:</p>
<p>C.2 Action Space Adapter Details</p>
<p>We use the same ASA details between all environments.We detail the architecture and training decisions for the different ASAs when applicable.</p>
<p>VQ: Use a codebook size of 512 with 512 dimensions per codebook element.These 512 tokens are mapped to token indices 31000 − 31512 from the LLaMA language modeling head.The encoder and decoder networks for predicting the latent and decoding from the latent are 4 layer MLP networks with hidden size 2048 using ReLU activations.The VQ network is trained on the actions in the same dataset used to train the policy.The network is trained with MSE loss to reconstruct the original actions.We VQ network for 3 epochs over the dataset.</p>
<p>RVQ: Use all the same details as VQ, but with a Residual-VQ that uses 2 codebooks.</p>
<p>t e x i t s h a 1 _ b a s e 6 4 = " W Z J Z s R F k 0 U e d o k J v 7 d D c H 4 C l g T 4 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b /</p>
<p>l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B U P o 3 X &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " A c x B n U X s P W i R p m c 1 g d X + s Y J 9 w Y 8 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c K p i 2 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q m S T T j P s s k Y n u h N R w K R T 3 U a D k n V R z G o e S t 8 P x 3 c x v P 3 F t R K I e c Z L y I K Z D J S L B K F r J T / o 5 T v v V m l t 3 5 y C r x C t I D Q o 0 + 9 W v 3 i B h W c w V M k m N 6 X p u i k F O N Q o m + b T S y w x P K R v T I e 9 a q m j M T Z D P j 5 2 S M 6 s M S J R o W w r J X P 0 9 k d P Y m E k c 2 s 6 Y 4 s g s e z P x P 6 + b Y X Q T 5 E K l G X L F F o u i T B J M y O x z M h C a M 5 Q T S y j T w t 5 K 2 I h q y t D m U 7 E h e M s v r 5 L W R d 2 7 q l 8 + X N Y a t 0 U c Z T i B U z g H D 6 6 h A f f Q B B 8 Y C H i G V 3 h z l P P i v D s f i 9 a S U 8 w c w x 8 4 n z 8 r S o 7 v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 K v 7 I N z P v F G 2 W z E 2 c P V 0 b 5 W s s 8 E = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e F o P g x b A r v o 5 B L x 4 j m I c k a 5 i d T J I h M 7 P L T K 8 Q l n y F F w + K e P V z v P k 3 T p I 9 a G J B Q 1 H V T X d X G A t u 0 P O + n d z S 8 s r q W n 6 9 s L G 5 t b 1 T 3 N 2 r m y j R l N V o J C L d D I l h g i t W Q 4 6 C N W P N i A w F a 4 T D m 4 n f e G L a 8 E j d 4 y h m g S R 9 x X u c E r T S Q 9 L B x 1 S e + O N O s e S V v S n c R e J n p A</p>
<p>5 d g S y r S w t x I 2 o J o y t A k V b Q j e 4 s v L p H F e 8 a 4 q l w 8 X 5 e p t H k c B j u E E z s C D a 6 j C P d S g D g y G 8 A y v 8 O Y k z o v z 7 n z M W 1 e c f O Y I / s D 5 / A G 5 + 4 / X &lt; / l a t e x i t &gt; v n t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l t X X S d 4 a U 0 V 4 / o q e n r G 0</p>
<p>1 t
1
e H O W 8 O O / O x 7 y 1 4 O Q z h / A H z u c P g Y 2 O g A = = &lt; / l a t e x i t &gt; h Adapter Head &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 G K e I c 2 N 3 b V r z f b F r n s 7 i P 6 c n o s</p>
<p>Figure 2 :
2
Figure 2: Generic architecture studied here for adapting MLLMs for action-specific decision making.The MLLM takes the embedding of the task instruction, prompt, and visual tokens as input.The MLLM then autoregressively predicts a sequence of m action tokens.These action tokens are then decoded into an environment-specific action.</p>
<p>Figure 3 :
3
Figure 3: Comparing ASAs for continuous and discrete action spaces across 5 environments.For continuous actions, the RVQ tokenization performs best.For discrete actions, SemLang performs best.Each bar gives the average over all tasks in the environment with the full breakdown in Appendix D.</p>
<p>Figure 5 :
5
Figure 5: RVQ and VQ success per-task grouping (defined in Supp.B.6) on CALVIN and MetaWorld.</p>
<p>Finetuning on Holdout Tasks</p>
<p>(b) Language Rearrangement RL.</p>
<p>Figure 6 :
6
Figure 6: (a) Adapting to 5 holdout tasks from Meta-World ML-45 with 50 demos per task using the fixed RVQ tokenization.(b) RL training curves in Language Rearrangement comparing the ASAs and utility of the token filter.Displayed are averages over 2 seeds with the shaded area as the standard deviation between seeds.SemLang learns faster than other ASAs and the token filter is crucial.</p>
<p>Figure 7 :
7
Figure 7: Visualizations of the environments we study.The top row shows an observation in the environment.The bottom row shows the associated instruction in that episode.</p>
<p>Table 1 :
1
Comparing the effect of the RVQ action space adapter on the success rate of non-LLM based policies.Red indicates RVQ hurts over Pred and green indicates RVQ helps over Pred.RVQ typically has a negative impact on the Scratch policy, and helps the smaller RT-Inspired policy.
MLLM: Pred→RVQ Scratch: Pred→RVQ RT-Inspired: Pred→RVQCalvin 68 → 72 (+4)50 → 43, (-7)35 → 36, (+1)Metaworld 61 → 84 (+23)71 → 56, (-15)27 → 38, (+11)Habitat Pick 19 → 29 (+10)21 → 25, (+4)18 → 20, (+2)</p>
<p>Table 2 :
2
).Another limitation is that RVQ, the best performing ASA in continuous action spaces, requires collecting demonstrations to train the VQ model.Also, SemLang requires manually describing the actions with language.Finally, our analyses are under a single LoRA training setting.Future analyses can explore different base MLLMs under different training regimes like full LLM finetuning.Comparing our investigation to prior work.Prior work typically analyzes a single action adapter in a single environment.We study a variety of action adapters across a variety of environments.
WorkEnvironmentsBest ASAOther ASAs StudiedAction Space TypesTrainingUsing LLM/VLM?RoboFlamingo [20] CALVINMLP-ContinuousBC✓Lamo [39]Franka Kitchen, Atari, MuJoCoMLP-Continuous, Discrete Offline-RL✓GFlan [40]BabyAISem Lang (scoring)-DiscreteOnline RL✓RT-2 [19]InternalUniform-ContinuousBC✓LLaRP [21]Language RearrangementMLP-DiscreteOnline-RL✓VQ-BeT [45]PushT, Multimodal Ant, BlockPush, FrankaRVQ+MLP-ContinuousBC✗Kitchen, nuScenes, PlayKitchenOursLanguage Rearrangement, Baby AI, Meta-RVQ/ Sem-LangMLP, VQ, Uniform, Non-Sem, Non-Continuous, Discrete Online-RL, BC✓World, CALVIN, Habitat SkillsSem Comp</p>
<p>tasks in Meta-World, 34 in CALVIN, 20 in HabPick where we count each object goal as a different task, 10 in Language Rearrangement for each of the evaluation splits, and 5 in BabyAI.The task count for Language Rearrangement is conservative since technically it consists of 282 instruction templates, each of which corresponds to a distinct task and goal.
CALVINMeta-WorldHabPickBabyAILangRRotate the pink block towardsPush a button on the coffeePick a lemonOpen the blue doorBring something to pour hotthe rightmachinecoffee in to the TV stand</p>
<p>You are a home robot assistant.Your possible actions are: pick object, place receptacle, nav receptacle, open receptacle, close receptacle, STOP.-Objects: ball, clamp, hammer, screwdriver, padlock, scissors, block, drill, spatula, knife, spoon, plate, sponge, cleanser, plum, pear, peach, apple, lemon, can, box, banana, strawberry, lego, cube, book, bowl, cup, mug, orange, lid, toy, wrench.-Receptacles: chair, black table, brown table, TV stand, sink, right counter, left counter, sofa, fridge, left drawer, right drawer, middle drawer.</p>
<p>USER: <INSTRUCTION> ASSISTANT: ".</p>
<p>Table 5 :
5
Breakdown on every CALVIN task.Note there are not an equal proportion of all tasks in the evaluation dataset.
RVQ Pred VQ UniformCALVIN72685628turn off led50963616move slider left99100 10015rotate red block right54173517open drawer10010056100rotate red block left31141414push pink block right311005114push blue block right42273520push red block left68366117push pink block left47508614push red block right35351717push blue block left56274714push into drawer49341414rotate pink block left76737316turn on lightbulb8034199rotate pink block right30731910rotate blue block right28131313turn off lightbulb76191912lift blue block table34253416close drawer100100 10070rotate blue block left32113820move slider right100100 10019turn on led311004214lift blue block slider32225115lift pink block table66688211lift red block slider56224113lift red block table45531515lift pink block slider75126212SemLang Lang Predgoto909075pickup603535open26621putnext777pick up seq go to201025</p>
<p>Table 6 :
6
Breakdown on every BabyAI task.
RVQ Pred VQ UniformMeta-World84615875assembly9570560basketball857055100button-press-topdown1009036100button-press-topdown-wall1001005790button-press10010085100button-press-wall100100 100100coffee-button10010097100coffee-pull96402650coffee-push75202580dial-turn965036100disassemble55302550door-close100100 100100door-open100100 100100drawer-close100100 100100drawer-open10010073100faucet-open100100 100100faucet-close1009010060hammer100404520handle-press-side100100 100100handle-press10010094100handle-pull-side3610610handle-pull65204530lever-pull55404540peg-insert-side6470440peg-unplug-side54309490pick-out-of-hole45903530pick-place74204460pick-place-wall75203530plate-slide1006044100plate-slide-side100100 10090plate-slide-back1009014100plate-slide-back-side10020100100push-back45301520push64207490push-wall764056100reach34201470reach-wall75807570shelf-place43201310soccer4406440stick-push100605100stick-pull865026100sweep-into65405570sweep85304570window-open100100 100100window-close100100 100100</p>
<p>Table 7 :
7
Breakdown on every Meta-World task.</p>
<p>Action Space: 4DoF continuous control of the arm and gripper.The first 3 dimensions specify the relative end-effector translation.The last dimension specifies the desired gripper state.Training: We use 40 start and goal configurations for each of the tasks.We generate 500 demonstrations for each of the 45 tasks.We use the scripted policies from Yu et al.[56].At each step we add Gaussian noise N (0, 0.1) to the actions produced by the scripted policy before executing it in the environment.We generate 500 successful trajectories per task, resulting in 45 • 500 = 22.5k total trajectories.Evaluation: We evaluate performance on 10 unseen start and goal configurations for each of the 45 tasks.So in total, we evaluate on 450 unseen configurations and report the average performance over these 450 episodes.B.2 CALVIN Tasks: We use the CALVIN ABC → D dataset split.Observation Space: 200 × 200 RGB observations from the fixed camera view.Action Space: 7DoF continuous control of the arm and gripper.The first 6 dimensions specify the relative position and rotation of the end-effector.The final dimension is a binary indicator for if the gripper should be open or closed.We hold out 1024 subsequences of the policy context length from these trajectories for reporting validation performance during the SFT process.Training: We use the 17,871 demonstrations provided in the CALVIN ABC → D dataset.These demonstrations are in 3 different table backgrounds.This also includes 1,088 demonstrations for validation.Evaluation: We report performance on the D split.This evaluation scene is a different color than that encountered during training.All the start positions and goals are also different.Many of the language instructions are also unseen from training.We report the average performance over the 1,000 evaluation sequences.We report the success of the first task completed in the sequence.B.3 Habitat PickTasks: We use the same Pick task as in Habitat 2.0 Geometric Goal object rearrangement[57,70], except we provide the agent the name of the object to rearrange rather than the starting coordinates of the object and increase the observation resolution.The task is successful when the agent picks up the object and returns the end-effector within a fixed offset to a "resting position" in front of the robot.(b) # Codes: Recon.In all environments, we report the success rate as the fraction of episodes in which the agent completed the language instruction.We use the success criteria provided by each environment.We train a policy per action adapter for each environment and report the generalization performance in the main text.When reporting a single success rate per environment, it is the success averaged between all evaluation episodes containing all tasks.We give the full per-task breakdown for results in Appendix D. CALVIN, Meta-World, HabPick, and BabyAI provide expert demonstrations succeeding at the task.CALVIN has 17.9k from humans, Meta-World 22.5k from a scripted policy, HabPick 6.7k generated from an RL policy, and BabyAI 5k from a scripted policy.Full details on the train and evaluation setups per environment are in Appendix B.We train with supervised finetuning for CALVIN, Meta-World, HabPick, and BabyAI.We train with reinforcement learning on Language Rearrangement.As described in Section 3.3 we train ≈ 140M parameters with LoRA[55].We use the AdamW optimizer[59]with a learning rate of 3e −4 , a warmup period of 10% of the total number of training steps, and cosine learning rate decay to 0 by the end of training.For RL, we use PPO[60].For the learned tokenization action space adapters, we, by default, use a codebook size of 512 with 512 dimensions per codebook element.Complete hyperparameter and policy details are in Appendix C.3: Hyperparameters for all imitation learning experiments.Most hyperparameters are the same between environments but the number of training epochs, context length and batch size per GPU are adjusted to fit the need for history, environment dataset size and task complexity.Pred: We use a 2 layer MLP network with a hidden size of 2048 and ReLU activations.We use this same MLP network architecture for discrete and continuous action space tasks.In the robot manipulation tasks, we also found it useful to include the robot proprioception as input to the MLP network and included this as input to the network layer.The robot proprioception consists of the robot robot joint angles and the gripper state.This ASA requires no separate training.Uniform: In the tasks we consider, the actions are already normalized to be in [−1, 1].We then create 512 evenly spaced bins within this interval and assign each action dimension based on which bin it is within.Like with VQ, we assign the 512 tokens to indices 31000 − 31512 from the LLaMA language modeling head.This ASA requires no separate training.Lang: Starting from the same semantic tokenization as with SemLang, we remap each token to the token corresponding to a digit "0" to "9".Therefore, the token count per action is the same between Lang and SemLang, but the Lang action tokens have no semantic meaning being just digits.C.3 Training and Architecture DetailsWe use all pretrained components from LLaVA.For the visual token downsampler, we use a 2 layer Perceiver network[53]with 4 output latents and hidden size 4096.We detail the hyperparameters used for imitation learning in in Table3.We trained with the HuggingFace Transformers library[71], PyTorch[72], DeepSpeed[73].For reinforcement learning, we use learning rate 3e −4 , 32 steps per rollout.18 parallel environment workers per GPU, an entropy coefficient of 0.01, 2 epochs over the data batch per rollout, 6 PPO minibatches, a maximum gradient norm of 0.2 and γ = 0.99.We train the CALVIN, Meta-World and HabPick imitation learning results on a 4xA40 GPU setup.We train the Language Rearrangement and BabyAI experiments on a 8xA100-80GB GPU setup.We train the LLM weights with LoRA and fine tune the entire ASA and downsampler module.For LoRA we use rank value 128, alpha parameter 32 and dropout 0.1.D Per-Task BreakdownIn this section, we show results for each environment by task type.Table4shows performance on Language Rearrangement for each of the evaluation datasets.Table5shows performance on CALVIN for each of the CALVIN tasks.Table6shows performance on BabyAI for each of the BabyAI instruction types.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei, 2023</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, arXiv:2306.148242023arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, 2023</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023</p>
<p>Multimodal foundation models: From specialists to general-purpose assistants. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, arXiv:2309.100202023arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Otter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, arXiv:2305.037262023arXiv preprint</p>
<p>Mimic-it: Multi-modal in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, Ziwei Liu, arXiv:2306.054252023arXiv preprint</p>
<p>Brandon Mckinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, arXiv:2403.09611Methods, analysis &amp; insights from multimodal llm pre-training. 2024arXiv preprint</p>
<p>Introducing idefics: An open reproduction of state-of-the-art visual language model. IDEFICS. 2023</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, arXiv:2209.07753Code as policies: Language model programs for embodied control. 2022arXiv preprint</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, arXiv:2204.005982022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Vision-language foundation models as effective robot imitators. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, arXiv:2311.013782023arXiv preprint</p>
<p>Large language models as generalizable policies for embodied tasks. Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev, arXiv:2310.177222023arXiv preprint</p>
<p>Robots that use language. Stefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, Cynthia Matuszek, Robotics, and Autonomous Systems. 32020Annual Review of Control</p>
<p>Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, arXiv:2306.15195Shikra: Unleashing multimodal llm's referential dialogue magic. 2023arXiv preprint</p>
<p>Ferret: Refer and ground anything anywhere at any granularity. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang, ICLR2024</p>
<p>Large language model is also an open-ended decoder for vision-centric tasks. Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, arXiv:2305.111752023arXiv preprint</p>
<p>Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia, arXiv:2308.00692Lisa: Reasoning segmentation via large language model. 2023arXiv preprint</p>
<p>Llava-grounding: Grounded visual chat with large multimodal models. Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, arXiv:2312.029492023arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378PaLM-E: An embodied multimodal language model. 2023arXiv preprint</p>
<p>Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. Oier Mees, Lukas Hermann, Erick Rosete-Beas, Wolfram Burgard, IEEE Robotics and Automation Letters. 732022</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on robot learning. PMLR2020</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. Dhruv Shah, Błażej Osiński, Sergey Levine, Conference on Robot Learning. PMLR2023</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Grounded decoding: Guiding text generation with grounded models for robot control. Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, arXiv:2303.008552023arXiv preprint</p>
<p>Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser, arXiv:2305.05658Tidybot: Personalized robot assistance with large language models. 2023arXiv preprint</p>
<p>Generalized planning in pddl domains with pretrained large language models. Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B Tenenbaum, Leslie Pack Kaelbling, Michael Katz, arXiv:2305.110142023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Unleashing the power of pre-trained language models for offline reinforcement learning. Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S Du, Huazhe Xu, arXiv:2310.205872023arXiv preprint</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, arXiv:2302.02662Grounding large language models in interactive environments with online reinforcement learning. 2023arXiv preprint</p>
<p>Ayush Jain, Andrew Szot, Joseph J Lim, arXiv:2011.01928Generalization to new actions in reinforcement learning. 2020arXiv preprint</p>
<p>Gabriel Dulac-Arnold, Richard Evans, Peter Hado Van Hasselt, Timothy Sunehag, Jonathan Lillicrap, Timothy Hunt, Theophane Mann, Thomas Weber, Ben Degris, Coppin, arXiv:1512.07679Deep reinforcement learning in large discrete action spaces. 2015arXiv preprint</p>
<p>Behavior transformers: Cloning k modes with one stone. Muhammad Nur, Zichen Shafiullah, Cui, Advances in neural information processing systems. 202235Ariuntuya Arty Altanzaya, and Lerrel Pinto</p>
<p>Jeff Zichen, Yibin Cui, Nur Wang, Muhammad Mahi, Lerrel Shafiullah, Pinto, arXiv:2210.10047From play to policy: Conditional behavior generation from uncurated robot data. 2022arXiv preprint</p>
<p>Behavior generation with latent actions. Seungjae Lee, Yibin Wang, Haritheja Etukuru, Jin Kim, Nur Muhammad Mahi, Lerrel Shafiullah, Pinto, arXiv:2403.031812024arXiv preprint</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, arXiv:2306.148242023arXiv preprint</p>
<p>Generative multimodal models are in-context learners. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, arXiv:2312.132862023arXiv preprint</p>
<p>Scaling robot learning with semantically imagined experience. Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, arXiv:2302.115502023arXiv preprint</p>
<p>Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, Barlas Oguz, arXiv:2309.15564Jointly training large autoregressive multimodal models. 2023arXiv preprint</p>
<p>Autoregressive image generation using residual quantization. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, Wook-Shin Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Soundstream: An end-to-end neural audio codec. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, Marco Tagliasacchi, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 302021</p>
<p>A markovian decision process. Richard Bellman, Indiana Univ. Math. J. 0022-251861957</p>
<p>Perceiver io: A general architecture for structured inputs &amp; outputs. Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, arXiv:2107.147952021arXiv preprint</p>
<p>Advances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 201730Neural discrete representation learning</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, R Julian, Karol Hausman, Chelsea Finn, S Levine, CoRL2019</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Advances in Neural Information Processing Systems. 202134</p>
<p>BabyAI: First steps towards grounded language learning with a human in the loop. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio, ICLR. 2019</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Residual reinforcement learning for robot control. Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea, Eugen Solowjow, Sergey Levine, 2019 international conference on robotics and automation (ICRA). IEEE2019</p>
<p>An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, Michael L Littman, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learning2008</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Is imitation all you need? generalized decision-making with dual-phase training. Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba, Ashish Kapoor, Shuang Ma, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Procedural generalization by planning with self-supervised world models. Ankesh Anand, Jacob Walker, Yazhe Li, Eszter Vértes, Julian Schrittwieser, Sherjil Ozair, Théophane Weber, Jessica B Hamrick, arXiv:2111.015872021arXiv preprint</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 58878392020</p>
<p>. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, arXiv:2205.061752022A generalist agent. arXiv preprint</p>
<p>Tsung-Wei Ke, arXiv:2402.10885Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. 2024arXiv preprint</p>
<p>Habitat rearrangement challenge. Andrew Szot, Karmesh Yadav, Alex Clegg, Vincent-Pierre Berges, Aaron Gokaslan, Angel Chang, Manolis Savva, Zsolt Kira, Dhruv Batra, 2022. 2022</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>            </div>
        </div>

    </div>
</body>
</html>
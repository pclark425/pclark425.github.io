<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4373 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4373</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4373</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-274964986</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.15487v2.pdf" target="_blank">Multi-LLM Text Summarization</a></p>
                <p><strong>Paper Abstract:</strong> In this work, we propose a Multi-LLM summarization framework, and investigate two different multi-LLM strategies including centralized and decentralized. Our multi-LLM summarization framework has two fundamentally important steps at each round of conversation: generation and evaluation. These steps are different depending on whether our multi-LLM decentralized summarization is used or centralized. In both our multi-LLM decentralized and centralized strategies, we have k different LLMs that generate diverse summaries of the text. However, during evaluation, our multi-LLM centralized summarization approach leverages a single LLM to evaluate the summaries and select the best one whereas k LLMs are used for decentralized multi-LLM summarization. Overall, we find that our multi-LLM summarization approaches significantly outperform the baselines that leverage only a single LLM by up to 3x. These results indicate the effectiveness of multi-LLM approaches for summarization.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4373.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4373.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-LLM (Centralized)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Centralized Multi-LLM Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic framework that uses k different LLMs to independently generate candidate summaries and a single central LLM to evaluate and select (or guide refinement of) the best summary via iterative generation/evaluation rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Centralized Multi-LLM Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Documents are chunked and each chunk is independently summarized by k participating LLMs (M = {M1,...,Mk}) using a shared generation prompt. A designated central evaluator C (one of the LLMs) receives the k candidate summaries and an evaluation prompt (Pec) that asks it to (anonymously) rate and pick the best summary and output a confidence score (0-10). If confidence is below threshold, the system enters another conversational round: generation prompts for the next round include previous-round summaries so that each generator can refine outputs. Final output is the deanonymized selected summary (or the tie-breaker summary if no convergence). The method constrains summary length (e.g., 160 words) to bound output cost and uses a two-stage chunk-then-summarize pipeline for long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Model-agnostic; experiments used GPT-3.5, GPT-4o, GPT-4o mini, LLaMA3-8B (generators) and GPT-3.5 or GPT-4o mini as central evaluator in different runs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Chunking of long input documents followed by LLM-driven summarization per chunk (prompted generation); anonymized candidate collection for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Centralized evaluation + selection (or iterative refinement) of candidate summaries produced by multiple LLMs; two-stage summarization (chunk-level then concatenated chunk summaries then final summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated across document datasets: ArXiv test set (1,288 documents) and GovReport (973 documents); per-run system summarizes individual documents (so system was run on hundreds–thousands of documents in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Long-document summarization of scientific articles (ArXiv) and government reports (GovReport).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Concise abstractive summaries (fixed-length, e.g., ~160 words) for each input document.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Automatic: ROUGE-1, ROUGE-L, BLEU-1, BLEU-4; Human: Likert ratings for Coherence, Conciseness, Fluency; evaluator confidence score used for stopping condition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Centralized multi-LLM outperformed single-LLM baselines on average (reported average improvements ~73% vs baselines in main experiments); some individual metric improvements up to 3× over single-LLM baselines. Token/cost analysis: input/output tokens scale linearly with number of models and rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-LLM summarizers (GPT-3.5, GPT-4o, GPT-4o mini, LLaMA3-8B) applying the same chunking and prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Average centralized improvement ~73% over single-LLM baselines in the reported setup; in some per-metric cases up to 3× improvement. Centralized variant also shown to be more cost-effective than decentralized (lower evaluation token costs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using multiple LLMs for generation plus a central evaluator yields consistently higher ROUGE/BLEU scores and human-aligned preferences; even a 2-LLM + single-round setup provides measurable gains; centralized evaluation reduces token/evaluation overhead compared to decentralized voting while preserving quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Increased computational/token cost vs single-LLM (though less than decentralized for evaluation); dependence on evaluator choice (GPT-3.5 performed best in reported experiments); limited exploration of other topologies and more diverse LLMs; prompts not fully optimized; risk of evaluator bias mitigated by anonymization but not eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Input/output token cost scales approximately O(t_max * k * (I + O_max)) for generation and O(t_max * (k * O_max + I_ec)) for centralized evaluation (linear in k); experimentally, adding more than 2 LLMs did not improve quality beyond the 2-LLM setup; t_max is small (e.g., 1–3) in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Text Summarization', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4373.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4373.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-LLM (Decentralized)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decentralized Multi-LLM Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework where k LLMs both generate candidate summaries and each evaluate all candidate summaries; a consensus (majority vote) determines the selected summary or the system iterates for further rounds until convergence or a tie-breaker is used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Decentralized Multi-LLM Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The pipeline chunks documents and asks k LLMs to generate summaries per chunk. For evaluation each of the k LLMs is prompted (with the original text and all candidate summaries) to pick the best candidate (no confidence score required). The k evaluation choices are aggregated; if a strict majority selects the same summary, that summary is accepted. If no majority is reached, the system either uses a designated tie-breaker model or enters another conversational round where generators receive previous summaries to produce refined candidates. The method bounds summary size and uses a consensus voting mechanism for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Model-agnostic; experiments used GPT-3.5, GPT-4o, GPT-4o mini, LLaMA3-8B in various combinations (e.g., GPT-3.5 + GPT-4o mini as participating LLMs with GPT-3.5 used as tie-breaker/evaluator in some runs).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Chunking plus LLM-based summarization per chunk; feeding concatenated chunk-level summaries to LLMs for final summarization; each model evaluates all candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Round-wise consensus aggregation (majority voting) across LLM evaluators, with iterative regeneration when consensus fails; tie-breaker fallback after t_max rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on ArXiv test set (1,288 documents) and GovReport (973 documents); system runs per-document across these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Long-document summarization of scientific papers (ArXiv) and government reports (GovReport).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive document summaries (~160 words final summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-L, BLEU-1, BLEU-4; human Likert ratings for Coherence/Conciseness/Fluency; consensus (majority) convergence criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Decentralized multi-LLM outperformed single-LLM baselines with average improvements reported ~70% in the main experiments; in some cases up to 3× improvement on individual metrics. Decentralized evaluation increases token costs (evaluation tokens scale ~k^2 * O_max term).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-LLM summarizers (same set as centralized comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Average decentralized improvement ~70% over single-LLM baselines; similar per-metric gains to centralized in reported experiments though decentralized evaluation token costs are higher.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Consensus voting across multiple LLM evaluators yields robust selection and can allow use of multiple weaker models instead of a single strong evaluator; iterative regeneration helps when no majority is reached; however decentralized evaluation is more expensive in tokens and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quadratic evaluation token complexity w.r.t. k (k^2 * O_max term) can become a bottleneck; consensus may fail (necessitating tie-breaker or extra rounds); adding more than two LLMs did not reliably improve quality in experiments; computational/cost trade-offs heavier than centralized variant.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Generation cost scales linearly with k and rounds; evaluation cost includes a k^2 * O_max term so decentralized evaluation scales poorly as k grows; empirically k in practice was small (2–3) and additional models beyond 2 did not improve scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Text Summarization', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4373.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4373.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent-Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent debating framework where multiple LLM agents iteratively debate to refine reasoning and answers; reported to improve reasoning performance in some benchmarks compared to single stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-Agent-Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Agent-Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a framework where multiple LLM agents engage in iterative debate rounds to surface divergent reasoning and collectively refine conclusions; an interaction topology leveraging multiple LLM perspectives to improve reasoning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reported in this paper to have been demonstrated with a multi-agent GPT-3.5-Turbo setup; compared against GPT-4 in reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Iterative multi-agent debate prompting (agents produce arguments/counterarguments); not described as document-chunk extraction in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Debate-based refinement where agents' back-and-forth is used to converge on improved answers; aggregation via debate outcome or consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Structured reasoning tasks / general reasoning benchmarks (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined answers/explanations produced after debate rounds (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported as evaluated on reasoning datasets (exact metrics not reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper states MAD (multi-agent GPT-3.5-Turbo) outperformed GPT-4 on reasoning datasets (no numeric values provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to single-model GPT-4 in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported to outperform GPT-4 on reasoning datasets according to the cited study (no quantitative details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-agent debate can leverage complementary strengths of different LLM instances and may surpass single-model performance on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cited works focus on structured reasoning tasks rather than document-level synthesis; debate frameworks may be expensive (iterative rounds) and their behavior on long-document synthesis is not established here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not detailed in this paper; cited result used a specific multi-agent setup (GPT-3.5-Turbo agents) and reported gains versus a larger single model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Text Summarization', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4373.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4373.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RECON-CILE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RECON-CILE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collaborative refinement framework where LLM agents iteratively refine answers and explanations, achieving improvements over single-agent systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RECON-CILE</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RECON-CILE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a multi-LLM collaboration framework in which multiple agents share and iteratively refine answers and explanations to produce better final outputs; aims to improve factuality and reasoning via cooperative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Collaborative refinement with repeated sharing of answers/explanations among agents (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative collaborative refinement and aggregation of agent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Structured reasoning / QA tasks (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined answers and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited as achieving significant improvements over single-agent systems (numeric results not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-agent LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported improvements over single-agent systems in the cited work; this paper does not reproduce numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Collaborative refinement across multiple LLMs can yield quality gains in answer/explanation generation compared to single-agent approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Focus remains on reasoning/QA; adaptation to long-document synthesis and cross-document theory formation is not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Text Summarization', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4373.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4373.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-connection sparsity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse agent-connection multi-LLM topologies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that optimize the communication topology between LLM agents (e.g., sparse networks) to reduce computation while retaining multi-agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sparse agent-connection multi-LLM topologies</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in related work as methods that optimize which agents communicate or connect, enabling sparse communication graphs that reduce computational overhead while maintaining performance of multi-agent systems.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified here; pertains to agent communication topology rather than document extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Sparse inter-agent communication and selective aggregation of outputs to synthesize final decisions/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-agent reasoning and task-oriented collaboration (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Task-specific outputs (e.g., refined answers), with reduced computation costs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited claim: sparse networks can maintain performance while reducing computational overhead (no numeric detail in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Dense/fully-connected multi-agent setups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported to retain performance with lower computation relative to denser topologies (no numbers provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Topology optimization (sparsity) is a promising direction to trade off cost vs. multi-agent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes multi-LLM topologies beyond centralized/decentralized extremes remain underexplored; practical trade-offs require further study.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Sparsity is presented as a mechanism to improve scaling (reduce k-related costs), but specific scaling laws are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Text Summarization', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-Agent-Debate (MAD) <em>(Rating: 2)</em></li>
                <li>RECON-CILE <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Encouraging divergent thinking in large language models through multi-agent debate <em>(Rating: 1)</em></li>
                <li>Improving multi-agent debate with sparse communication topology <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4373",
    "paper_id": "paper-274964986",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Multi-LLM (Centralized)",
            "name_full": "Centralized Multi-LLM Summarization",
            "brief_description": "A model-agnostic framework that uses k different LLMs to independently generate candidate summaries and a single central LLM to evaluate and select (or guide refinement of) the best summary via iterative generation/evaluation rounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Centralized Multi-LLM Summarization",
            "system_description": "Documents are chunked and each chunk is independently summarized by k participating LLMs (M = {M1,...,Mk}) using a shared generation prompt. A designated central evaluator C (one of the LLMs) receives the k candidate summaries and an evaluation prompt (Pec) that asks it to (anonymously) rate and pick the best summary and output a confidence score (0-10). If confidence is below threshold, the system enters another conversational round: generation prompts for the next round include previous-round summaries so that each generator can refine outputs. Final output is the deanonymized selected summary (or the tie-breaker summary if no convergence). The method constrains summary length (e.g., 160 words) to bound output cost and uses a two-stage chunk-then-summarize pipeline for long documents.",
            "llm_model_used": "Model-agnostic; experiments used GPT-3.5, GPT-4o, GPT-4o mini, LLaMA3-8B (generators) and GPT-3.5 or GPT-4o mini as central evaluator in different runs.",
            "extraction_technique": "Chunking of long input documents followed by LLM-driven summarization per chunk (prompted generation); anonymized candidate collection for evaluation.",
            "synthesis_technique": "Centralized evaluation + selection (or iterative refinement) of candidate summaries produced by multiple LLMs; two-stage summarization (chunk-level then concatenated chunk summaries then final summarization).",
            "number_of_papers": "Evaluated across document datasets: ArXiv test set (1,288 documents) and GovReport (973 documents); per-run system summarizes individual documents (so system was run on hundreds–thousands of documents in experiments).",
            "domain_or_topic": "Long-document summarization of scientific articles (ArXiv) and government reports (GovReport).",
            "output_type": "Concise abstractive summaries (fixed-length, e.g., ~160 words) for each input document.",
            "evaluation_metrics": "Automatic: ROUGE-1, ROUGE-L, BLEU-1, BLEU-4; Human: Likert ratings for Coherence, Conciseness, Fluency; evaluator confidence score used for stopping condition.",
            "performance_results": "Centralized multi-LLM outperformed single-LLM baselines on average (reported average improvements ~73% vs baselines in main experiments); some individual metric improvements up to 3× over single-LLM baselines. Token/cost analysis: input/output tokens scale linearly with number of models and rounds.",
            "comparison_baseline": "Single-LLM summarizers (GPT-3.5, GPT-4o, GPT-4o mini, LLaMA3-8B) applying the same chunking and prompt.",
            "performance_vs_baseline": "Average centralized improvement ~73% over single-LLM baselines in the reported setup; in some per-metric cases up to 3× improvement. Centralized variant also shown to be more cost-effective than decentralized (lower evaluation token costs).",
            "key_findings": "Using multiple LLMs for generation plus a central evaluator yields consistently higher ROUGE/BLEU scores and human-aligned preferences; even a 2-LLM + single-round setup provides measurable gains; centralized evaluation reduces token/evaluation overhead compared to decentralized voting while preserving quality.",
            "limitations_challenges": "Increased computational/token cost vs single-LLM (though less than decentralized for evaluation); dependence on evaluator choice (GPT-3.5 performed best in reported experiments); limited exploration of other topologies and more diverse LLMs; prompts not fully optimized; risk of evaluator bias mitigated by anonymization but not eliminated.",
            "scaling_behavior": "Input/output token cost scales approximately O(t_max * k * (I + O_max)) for generation and O(t_max * (k * O_max + I_ec)) for centralized evaluation (linear in k); experimentally, adding more than 2 LLMs did not improve quality beyond the 2-LLM setup; t_max is small (e.g., 1–3) in practice.",
            "uuid": "e4373.0",
            "source_info": {
                "paper_title": "Multi-LLM Text Summarization",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Multi-LLM (Decentralized)",
            "name_full": "Decentralized Multi-LLM Summarization",
            "brief_description": "A multi-agent framework where k LLMs both generate candidate summaries and each evaluate all candidate summaries; a consensus (majority vote) determines the selected summary or the system iterates for further rounds until convergence or a tie-breaker is used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Decentralized Multi-LLM Summarization",
            "system_description": "The pipeline chunks documents and asks k LLMs to generate summaries per chunk. For evaluation each of the k LLMs is prompted (with the original text and all candidate summaries) to pick the best candidate (no confidence score required). The k evaluation choices are aggregated; if a strict majority selects the same summary, that summary is accepted. If no majority is reached, the system either uses a designated tie-breaker model or enters another conversational round where generators receive previous summaries to produce refined candidates. The method bounds summary size and uses a consensus voting mechanism for robustness.",
            "llm_model_used": "Model-agnostic; experiments used GPT-3.5, GPT-4o, GPT-4o mini, LLaMA3-8B in various combinations (e.g., GPT-3.5 + GPT-4o mini as participating LLMs with GPT-3.5 used as tie-breaker/evaluator in some runs).",
            "extraction_technique": "Chunking plus LLM-based summarization per chunk; feeding concatenated chunk-level summaries to LLMs for final summarization; each model evaluates all candidates.",
            "synthesis_technique": "Round-wise consensus aggregation (majority voting) across LLM evaluators, with iterative regeneration when consensus fails; tie-breaker fallback after t_max rounds.",
            "number_of_papers": "Evaluated on ArXiv test set (1,288 documents) and GovReport (973 documents); system runs per-document across these datasets.",
            "domain_or_topic": "Long-document summarization of scientific papers (ArXiv) and government reports (GovReport).",
            "output_type": "Abstractive document summaries (~160 words final summaries).",
            "evaluation_metrics": "ROUGE-1, ROUGE-L, BLEU-1, BLEU-4; human Likert ratings for Coherence/Conciseness/Fluency; consensus (majority) convergence criterion.",
            "performance_results": "Decentralized multi-LLM outperformed single-LLM baselines with average improvements reported ~70% in the main experiments; in some cases up to 3× improvement on individual metrics. Decentralized evaluation increases token costs (evaluation tokens scale ~k^2 * O_max term).",
            "comparison_baseline": "Single-LLM summarizers (same set as centralized comparisons).",
            "performance_vs_baseline": "Average decentralized improvement ~70% over single-LLM baselines; similar per-metric gains to centralized in reported experiments though decentralized evaluation token costs are higher.",
            "key_findings": "Consensus voting across multiple LLM evaluators yields robust selection and can allow use of multiple weaker models instead of a single strong evaluator; iterative regeneration helps when no majority is reached; however decentralized evaluation is more expensive in tokens and compute.",
            "limitations_challenges": "Quadratic evaluation token complexity w.r.t. k (k^2 * O_max term) can become a bottleneck; consensus may fail (necessitating tie-breaker or extra rounds); adding more than two LLMs did not reliably improve quality in experiments; computational/cost trade-offs heavier than centralized variant.",
            "scaling_behavior": "Generation cost scales linearly with k and rounds; evaluation cost includes a k^2 * O_max term so decentralized evaluation scales poorly as k grows; empirically k in practice was small (2–3) and additional models beyond 2 did not improve scores.",
            "uuid": "e4373.1",
            "source_info": {
                "paper_title": "Multi-LLM Text Summarization",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MAD",
            "name_full": "Multi-Agent-Debate (MAD)",
            "brief_description": "A multi-agent debating framework where multiple LLM agents iteratively debate to refine reasoning and answers; reported to improve reasoning performance in some benchmarks compared to single stronger models.",
            "citation_title": "Multi-Agent-Debate (MAD)",
            "mention_or_use": "mention",
            "system_name": "Multi-Agent-Debate (MAD)",
            "system_description": "Described as a framework where multiple LLM agents engage in iterative debate rounds to surface divergent reasoning and collectively refine conclusions; an interaction topology leveraging multiple LLM perspectives to improve reasoning outcomes.",
            "llm_model_used": "Reported in this paper to have been demonstrated with a multi-agent GPT-3.5-Turbo setup; compared against GPT-4 in reasoning datasets.",
            "extraction_technique": "Iterative multi-agent debate prompting (agents produce arguments/counterarguments); not described as document-chunk extraction in this paper.",
            "synthesis_technique": "Debate-based refinement where agents' back-and-forth is used to converge on improved answers; aggregation via debate outcome or consensus.",
            "number_of_papers": null,
            "domain_or_topic": "Structured reasoning tasks / general reasoning benchmarks (mentioned in related work).",
            "output_type": "Refined answers/explanations produced after debate rounds (task-dependent).",
            "evaluation_metrics": "Reported as evaluated on reasoning datasets (exact metrics not reproduced in this paper).",
            "performance_results": "Paper states MAD (multi-agent GPT-3.5-Turbo) outperformed GPT-4 on reasoning datasets (no numeric values provided in this paper).",
            "comparison_baseline": "Compared to single-model GPT-4 in cited work.",
            "performance_vs_baseline": "Reported to outperform GPT-4 on reasoning datasets according to the cited study (no quantitative details provided in this paper).",
            "key_findings": "Multi-agent debate can leverage complementary strengths of different LLM instances and may surpass single-model performance on reasoning tasks.",
            "limitations_challenges": "Cited works focus on structured reasoning tasks rather than document-level synthesis; debate frameworks may be expensive (iterative rounds) and their behavior on long-document synthesis is not established here.",
            "scaling_behavior": "Not detailed in this paper; cited result used a specific multi-agent setup (GPT-3.5-Turbo agents) and reported gains versus a larger single model.",
            "uuid": "e4373.2",
            "source_info": {
                "paper_title": "Multi-LLM Text Summarization",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RECON-CILE",
            "name_full": "RECON-CILE",
            "brief_description": "A collaborative refinement framework where LLM agents iteratively refine answers and explanations, achieving improvements over single-agent systems.",
            "citation_title": "RECON-CILE",
            "mention_or_use": "mention",
            "system_name": "RECON-CILE",
            "system_description": "Described as a multi-LLM collaboration framework in which multiple agents share and iteratively refine answers and explanations to produce better final outputs; aims to improve factuality and reasoning via cooperative refinement.",
            "llm_model_used": null,
            "extraction_technique": "Collaborative refinement with repeated sharing of answers/explanations among agents (details not provided in this paper).",
            "synthesis_technique": "Iterative collaborative refinement and aggregation of agent outputs.",
            "number_of_papers": null,
            "domain_or_topic": "Structured reasoning / QA tasks (mentioned in related work).",
            "output_type": "Refined answers and explanations.",
            "evaluation_metrics": null,
            "performance_results": "Cited as achieving significant improvements over single-agent systems (numeric results not provided in this paper).",
            "comparison_baseline": "Single-agent LLM baselines.",
            "performance_vs_baseline": "Reported improvements over single-agent systems in the cited work; this paper does not reproduce numbers.",
            "key_findings": "Collaborative refinement across multiple LLMs can yield quality gains in answer/explanation generation compared to single-agent approaches.",
            "limitations_challenges": "Focus remains on reasoning/QA; adaptation to long-document synthesis and cross-document theory formation is not reported here.",
            "scaling_behavior": "Not described in this paper.",
            "uuid": "e4373.3",
            "source_info": {
                "paper_title": "Multi-LLM Text Summarization",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Agent-connection sparsity",
            "name_full": "Sparse agent-connection multi-LLM topologies",
            "brief_description": "Approaches that optimize the communication topology between LLM agents (e.g., sparse networks) to reduce computation while retaining multi-agent performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Sparse agent-connection multi-LLM topologies",
            "system_description": "Described in related work as methods that optimize which agents communicate or connect, enabling sparse communication graphs that reduce computational overhead while maintaining performance of multi-agent systems.",
            "llm_model_used": null,
            "extraction_technique": "Not specified here; pertains to agent communication topology rather than document extraction.",
            "synthesis_technique": "Sparse inter-agent communication and selective aggregation of outputs to synthesize final decisions/outputs.",
            "number_of_papers": null,
            "domain_or_topic": "Multi-agent reasoning and task-oriented collaboration (mentioned in related work).",
            "output_type": "Task-specific outputs (e.g., refined answers), with reduced computation costs.",
            "evaluation_metrics": null,
            "performance_results": "Cited claim: sparse networks can maintain performance while reducing computational overhead (no numeric detail in this paper).",
            "comparison_baseline": "Dense/fully-connected multi-agent setups.",
            "performance_vs_baseline": "Reported to retain performance with lower computation relative to denser topologies (no numbers provided here).",
            "key_findings": "Topology optimization (sparsity) is a promising direction to trade off cost vs. multi-agent gains.",
            "limitations_challenges": "Paper notes multi-LLM topologies beyond centralized/decentralized extremes remain underexplored; practical trade-offs require further study.",
            "scaling_behavior": "Sparsity is presented as a mechanism to improve scaling (reduce k-related costs), but specific scaling laws are not provided in this paper.",
            "uuid": "e4373.4",
            "source_info": {
                "paper_title": "Multi-LLM Text Summarization",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-Agent-Debate (MAD)",
            "rating": 2,
            "sanitized_title": "multiagentdebate_mad"
        },
        {
            "paper_title": "RECON-CILE",
            "rating": 2,
            "sanitized_title": "reconcile"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate",
            "rating": 1,
            "sanitized_title": "encouraging_divergent_thinking_in_large_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Improving multi-agent debate with sparse communication topology",
            "rating": 1,
            "sanitized_title": "improving_multiagent_debate_with_sparse_communication_topology"
        }
    ],
    "cost": 0.015063499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-LLM Text Summarization
1 Apr 2025</p>
<p>Jiangnan Fang 
University of California
Santa Cruz</p>
<p>Cheng-Tse Liu 
University of California
Santa Cruz</p>
<p>Jieun Kim 
University of California
Santa Cruz</p>
<p>Yash Bhedaru 
University of California
Santa Cruz</p>
<p>Ethan Liu 
Nikhil Singh 
University of California
Santa Cruz</p>
<p>Nedim Lipka 
University of California
Santa Cruz</p>
<p>Adobe Research</p>
<p>Puneet Mathur 
Adobe Research</p>
<p>Nesreen K Ahmed 
Adobe Research</p>
<p>Franck Dernoncourt 
Adobe Research</p>
<p>Ryan A Rossi 
Adobe Research</p>
<p>Hanieh Deilamsalehy 
Adobe Research</p>
<p>Manzil Zaheer 
Guru Guruganesh 
Avinava Dubey 
Joshua Ainslie 
Chris Alberti 
Santiago Ontanon 
Philip Pham 
Anirudh Ravula 
Qifan Wang 
Li Yang 
Jingqing Zhang 
Yao Zhao 
Mohammad Saleh 
J Liu 
Pegasus 
Tianyi Zhang 
Faisal Ladhak 
Esin Durmus 
Percy Liang 
Kathleen Mckeown 
Tatsunori B Hashimoto 
Multi-LLM Text Summarization
1 Apr 2025560A681D1F470D4A19E12AE4CFF52E4DarXiv:2412.15487v2[cs.CL]
In this work, we propose a Multi-LLM summarization framework, and investigate two different multi-LLM strategies including centralized and decentralized.Our multi-LLM summarization framework has two fundamentally important steps at each round of conversation: generation and evaluation.These steps are different depending on whether our multi-LLM decentralized summarization is used or centralized.In both our multi-LLM decentralized and centralized strategies, we have k different LLMs that generate diverse summaries of the text.However, during evaluation, our multi-LLM centralized summarization approach leverages a single LLM to evaluate the summaries and select the best one whereas k LLMs are used for decentralized multi-LLM summarization.Overall, we find that our multi-LLM summarization approaches significantly outperform the baselines that leverage only a single LLM by up to 3x.These results indicate the effectiveness of multi-LLM approaches for summarization.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have been shown to have the potential to produce high-quality summaries (Chowdhery et al., 2022;Zhang et al., 2023;Goyal et al., 2023;Pu et al., 2023b).However, despite the remarkable progress in LLM-based summarization, limitations still exist for documents where useful information may be sparsely distributed throughout the text.Research by (Liu et al., 2023) highlights that a naive application of LLMs may overlook critical details or fail to grasp the holistic meaning of a document, indicating the need for more refined methods.</p>
<p>To address this, recent efforts have explored prompt-engineering techniques to guide LLMs towards producing better summaries (Adams et al., 2023).These techniques, while promising, still face limitations in consistently delivering high-quality summaries across different document types and structures.Instead of relying solely on a single model or simple prompt-engineering methods, we propose an approach novel to the summarization domain that focuses on aggregating the collective strengths of multiple LLMs.By combining the capabilities of multiple models with a diverse set of knowledge bases, we show it's possible to achieve more robust summaries across domains.</p>
<p>Summary of Main Contributions.The main contributions of this work are as follows:</p>
<p>• We propose the first framework for multi-LLM text summarization and investigate two topologies: centralized and decentralized.• We find that multi-LLM text summarization often performs better than using a single LLM for summarization, and we show that the best performing method in the framework aligns with human judgments.• We conduct experiments on how prompting, number of LLMs, and various combinations of generating and evaluating LLMs can affect quality of summaries in the multi-LLM setup.</p>
<p>2 Related Work</p>
<p>Summarization</p>
<p>Recent advancements in summarization have increasingly leveraged large language models (LLMs), moving beyond fine-tuned transformer models like Pegasus, BART, and T5.Studies consistently show that LLMs can generate summaries with higher coherence, relevance, and factual accuracy, often rivaling or surpassing human-written summaries (Goyal et al., 2023;Zhang et al., 2023;Pu et al., 2023b).For example, Goyal et al. ( 2023) demonstrated that  produced summaries preferred by human evaluators over fine-tuned models like Pegasus and BRIO on structured datasets such as CNN/DM (Nallapati et al., 2016) and XSUM (Narayan et al., 2018).Similarly, Zhang et al. (2023) emphasized the importance of instruction tuning in achieving superior zero-shot performance for summarization tasks.Pu et al. (2023b) further highlighted improved factual consistency and reduced hallucinations when using LLMs.</p>
<p>While these studies validate the potential of LLMs in summarizing well-structured texts, they may falter for inputs lacking clear structural cues and exhibiting greater complexity.Research focusing on longer text summarization, such as Keswani et al. (2024), employed semantic clustering and multi-stage summarization with LLaMA2 to manage lengthy inputs.However, such approaches often rely on predefined hierarchical processing strategies that may oversimplify the nuanced relationships within the text.Moreover, as Liu et al. (2023) noted, LLMs tend to neglect content from the middle sections of longer documents, resulting in incomplete or unbalanced summaries.</p>
<p>Our work aims to improve performance for both long and short text summarization, and it builds upon aforementioned foundations by proposing a multi-LLM framework designed to overcome these shortcomings through information exchange and collaborative synthesis.</p>
<p>Multi-LLM</p>
<p>The concept of leveraging multiple LLMs collaboratively has gained traction in recent research, particularly for tasks requiring complex reasoning and factual accuracy.For instance, Liang et al. (2024) introduced the Multi-Agent-Debate (MAD) framework, where LLMs engage in iterative debates to refine their reasoning.This framework demonstrated that a multi-agent GPT-3.5-Turbosetup outperformed GPT-4 on reasoning datasets.Similarly, Chen et al. (2024) proposed RECON-CILE, a framework where LLMs collaboratively refine answers and explanations, achieving significant improvements over single-agent systems.Li et al. (2024) extended this line of research by optimizing agent connections, showing that sparse networks can maintain performance while reducing computational overhead.</p>
<p>Although these studies reveal the potential of multi-LLM approaches, their focus remains on structured reasoning tasks, such as question answering and fact-checking.They have not been adequately explored in the context of synthesizing distributed information, addressing content imbal-ances, and preserving the coherence of summaries across extended texts.</p>
<p>We hope to bridge this gap by adapting multi-LLM frameworks to the domain of document summarization, addressing limitations of both single LLM and traditional hierarchical techniques, and positioning multi-LLM summarization as a promising solution.</p>
<p>Multi-LLM Summarization Framework</p>
<p>In this work, we propose a novel multi-LLM summarization framework that leverages multiple large language models to enhance summarization quality of long document input.Through the distribution of generation and evaluation of candidate summaries across multiple models, our framework aims to provide better summaries than single LLM methods, leveraging expertise from different models.We present two interaction topologies, centralized and decentralized, to guide the collaboration, evaluation, and refinement of summaries between LLMs.Visually these two methods can be represented at a high level in Figure 1.In the datasets we test, articles are typically tens of thousands of words long and exceed the context window of most standard LLMs.To handle this, we establish a two stage process that involves chunking the source document, independently summarizing each chunk of the source document, and then applying a second round of chunking and summarization on the concatenated intermediate results.Throughout both these stages, both frameworks allow multiple LLMs to collaborate and converge on a single final high quality summary of the entire original reference document.Table 1 provides an overview of our framework's four main variations.</p>
<p>Centralized Multi-LLM Summarization</p>
<p>The steps for centralized summarization can be found in Algorithm 1.This method leverages multiple LLMs to generate candidate summaries and uses a central LLM to evaluate their quality and guide iterative refinements.</p>
<p>Single Round</p>
<p>In the simplest case, we prompt each LLM once, gather their summaries, and then perform a single evaluation step to select the best final summary.This is the initial process before we extend it to multiple rounds.</p>
<p>Generation Phase</p>
<p>In the single-round setting, each LLM from the list of participating models M = {M 1 , . . ., M k } independently generates a summary of the same input text using a common prompt P .The prompt P is illustrated in Figure 2. Formally, for each LLM M j ∈ M, the output is
S j = M j (P, S)
where S represents the input text.Running this step for all M j yields a set of summaries S = {S 1 , . . ., S k }.</p>
<p>This initial generation stage corresponds to line 4 of Algorithm 1. Conceptually, each model contributes its unique perspective, leading to a diverse pool of candidate summaries, which is important for robust summary selection in the following evaluation phase.</p>
<p>Evaluation Phase</p>
<p>After collecting the set of candidate summaries S, we select a central agent C ∈ M to evaluate these summaries.The central LLM C uses an evaluation prompt P ec , as shown in Figure 5, to assess the quality of each summary.To reduce potential bias arising from authorship attribution, we use anonymized identifiers for summaries like agent_1, agent_2, etc. during evaluation.</p>
<p>Formally, we obtain E = C(P ec , S), where E is the central LLM's evaluation of all candidate summaries.This includes the choice for the best summary (expressed as its anonymized identifier) and a confidence score for that evaluation (expressed as an integer from 0 to 10), denoted together as r = AGGRRESULTS(E) in Algorithm 1.We deanonymize the identifier to recover the text of the selected summary S j and set this as our final output S * .In the single-round regime, this terminates the process as no further iterations are performed.</p>
<p>In the evaluation prompt, we include the prompt to output a confidence score so there is a variable on which to impose a stopping condition.This allows us to extend the centralized process to multiple rounds of generation and evaluation using that condition.This process is explained in subsequent sections.</p>
<p>Conversational</p>
<p>In the conversational approach, we repeat the generation and evaluation phases multiple times.We define each generation-evaluation process as one round and define conditions under which the process ends or a new round should begin, up to a maximum number of rounds.</p>
<p>Generation Phase</p>
<p>The first round of the conversational approach mirrors the single-round procedure (Section 4.1.1).Each LLM M j generates an initial summary S</p>
<p>(1) j from the original input text S using the prompt P :</p>
<p>S</p>
<p>(1) j = M j (P, S).</p>
<p>If the evaluation result from the previous round has a confidence score less than the threshold or, if the LLM fails to output a readable confidence score, the pipeline proceeds to the next round.For the second and subsequent rounds, we use the prompt P (i) , shown in Figure 3. LLMs in the second and subsequent rounds have access to both the text to be summarized and summaries from the previous round.Concretely, in round i &gt; 1:
S (i) j = M j (P (i) , S).
The hope is that LLM is able to iteratively improve summarization based upon previous outputs from itself and other models.</p>
<p>Evaluation Phase</p>
<p>The evaluation phase in round i &gt; 1 is conceptually similar to the single-round setting (Section 4.1.2),but now operates on candidate summaries generated immediately before in the generation phase
S i = {S (i) 1 , . . . , S(i)
k }.The central LLM C evaluates these candidates using P ec :
E (i) = C(P ec , S i ),
If the confidence level meets the threshold, the process terminates, and the summary chosen by the central LLM is accepted as S * .Otherwise, we proceed to the next round of summary generation and evaluation.For the confidence scores we have chosen the range 0-10 as it is fine-grained but also is one of the most common rating scales.</p>
<p>Analysis of Complexity</p>
<p>The centralized approach uses k models for generation and 1 central model for evaluation; other than text length, the number of input tokens scale linearly with the number of models and with the number of rounds.Output tokens also scale linearly with number of models and number of rounds, but since we instruct the model to output a fixed number of words for summary (and in our experiments the models are largely compliant), and output only the anonymous identifier for a chosen summary, we ensure bounded runtime and cost.Further analysis can be found at Appendix B.1.</p>
<p>Decentralized Multi-LLM Summarization</p>
<p>Previously we introduced the summarization procedure for centralized approach (Section 4), which diversifies the knowledge base for summarization.</p>
<p>Algorithm 1 Centralized Multi-LLM Summary</p>
<p>Require: ordered set S = {S1, . . ., Sm} of summaries, set M = {M1, . . ., M k } of k LLMs, a central agent C ∈ M, max number of conversational rounds tmax, initial summarization prompt P (e.g., Figure 2), evaluation prompt Pec (e.g., Figure 5) for centralized version Ensure: summary S * of the text 1: S = CREATESUMMARY(S) 2: for i = 1 to tmax do ▷ conversation rounds 3:</p>
<p>for each model Mj ∈ M do 4:
S (i) j = Mj(P, S) 5: Let Si = {S (i) 1 , S (i) 2 , . . . , S(i)
k } 6:
E (i) = C(Pec, Si) 7: r = AGGRRESULTS(E (i) ) 8: j ← argmax M j ∈M rj 9: Set S * ← S (i) j 10:
if CONVERGED(r) then return S * 11:</p>
<p>Set P to prompt in Figure 3.</p>
<p>Provide a concise summary of the text in around 160 words.Output the summary text only and nothing else.</p>
<p>[text]</p>
<p>Figure 2: Prompt for generating the initial summary in the first round.</p>
<p>Given the original text below, along with the summaries of that text by [k] LLMs, please generate a better summary of the original text in about 160 words.</p>
<p>ORIGINAL: [text] Summary by M1:</p>
<p>[LLM 1's summary] . . .</p>
<p>Summary by M
k : [LLM k's summary]
Figure 3: Generation prompt that is used after the initial round of conversation among the multiple LLMs.Note that the above prompt is for generating the final summary, however, for the chunk-level generation, it would just be the actual chunk.</p>
<p>We extend the paradigm for the evaluator as well.</p>
<p>In the decentralized approach, multiple LLMs also participate in the evaluation process with the hope that a best summary decided on consensus is more robust compared to a single model's decision.</p>
<p>Single Round</p>
<p>Generation Phase</p>
<p>Generation procedure is the same as that in the centralized approach described in Section 4.1.1.As before, multiple LLMs independently generate Given the initial text below, along with the summaries of that text by [k] LLMs, please evaluate the generated summaries and output the name of the LLM has the best summary.On a separate line indicate a confidence level between 0 and 10.ORIGINAL:</p>
<p>[text] Summary by M1:</p>
<p>[LLM 1's summary] . . .</p>
<p>Summary by M k :</p>
<p>[LLM k's summary] Remember, on a separate line indicate a confidence level between 0 and 10 Figure 5: Evaluation prompt for evaluating the summaries generated using our conversational (centralized) multi-LLM framework.More specifically, we have added an instruction for centralized multi-LLM summarization approach that in addition to providing the best summary, it also outputs the confidence level between 0 and 10. "k" is a parameter reflecting the number of summary-generating LLMs.</p>
<p>summaries for the input text, obtaining the list of summaries S = {S 1 , . . ., S k }.</p>
<p>Evaluation Phase</p>
<p>For evaluation, each model that authored a summary is prompted with a new evaluation prompt (Figure 4) which does not include a confidence level and receives the text to be summarized along with summaries authored by all agents including itself.More formally, model preferences
E (i) 1 , . . . , E (i) k are collected, where each E (i) j rep- resents model M j 's</p>
<p>choice of the best summary</p>
<p>Provide a concise summary of the text in around 160 words.Output the summary text only and nothing else.</p>
<p>[concatenated chunk summaries S]</p>
<p>Figure 6: Generation prompt for generating the final summary from the summarized chunks using our conversational (decentralized) multi-LLM framework.This prompt is the same as the one for the initial summary.</p>
<p>among
S (i) 1 , . . . , S(i)
k .These preferences are aggregated into a result vector r ∈ 1, . . ., k k , where each element r j indicates which model's summary was chosen by model M j .Convergence is achieved when a majority of models select the same summary, formally expressed as ∃m ∈ 1, . . ., k : |j : r j = m| &gt; k 2 .1When no majority choice emerges, the single-round approach (t max = 1) the algorithm selects the summary from a designated tie-breaker model M t , where t ∈ 1, . . ., k.Since the tie-breaker model can be any model in the multi-LLM setup, we run experiments with different choices of evaluator and tie-breaking models.Formally, the final summary S * is determined as:
S * = S (1) m if ∃m : |{j : E (1) j = m}| &gt; k 2 S (1) t if max l |{j : E (1) j = l}| ≤ k 2
where m ∈ 1, . . ., k : |j : r j = m| &gt; k 2 .We test the different choices of tie-breaker model in the experiment Appendix C.1.</p>
<p>Conversational</p>
<p>The conversational approach extends the decentralized framework by introducing multiple rounds of generation and evaluation phases.Each generationevaluation cycle constitutes a round, with iterations continuing until either consensus is achieved or a maximum number of rounds (t max ) is reached.</p>
<p>Generation Phase</p>
<p>Generation follows the methodology in Section 4.1.1,producing the set of summaries S = S 1 , . . ., S k .A key distinction from the singleround approach lies in the conditional regeneration mechanism: when consensus fails in the first round, subsequent rounds use a new prompt (Figure 3) which includes generated summaries from previous evaluations.</p>
<p>Algorithm 2 Decentralized Multi-LLM Summary</p>
<p>Require: ordered set S = {S1, . . ., Sm} of summaries, set M = {M1, . . ., M k } of k LLMs, max number of conversational rounds tmax, initial summarization prompt P (e.g., Figure 2), evaluation prompt Pe (e.g., Figure 4) Ensure: summary S * of the text 1: S = CREATESUMMARY(S) 2: for i = 1 to tmax do ▷ conversation rounds 3:</p>
<p>for each model Mj ∈ M do 4:
S (i) j = Mj(P, S) 5: Let Si = {S (i) 1 , S (i) 2 , . . . , S(i)
k } 6:</p>
<p>for each model Mj ∈ M do 7:
E (i) j = Mj(Pe, S (i) 1 , . . . , S (i) k ) 8: Set Ei = {E (i) 1 , E (i) 2 , . . . , E (i) k } 9: r = AGGRRESULTS(E (i) 1 , . . . , E (i) k ) 10: j ← argmax M j ∈M rj 11: Set S * ← S (i) j 12:
if CONVERGED(r) then return S * 13:</p>
<p>Set P to prompt in Figure 3.</p>
<p>Evaluation Phase</p>
<p>The first round of evaluation is identical to that in the single-round approach, but enters additional rounds with new generation prompts.Formally, let E</p>
<p>(i) j represent model M j 's choice in round i.In the single-round case, non-consensus (when max m |{j :
E (i) j = m}| ≤ k
2 ) triggers an immediate fallback to a tie-breaker model.In contrast, the conversational approach initiates a new generationevaluation round with an updated prompt (Figure 3).This process continues until either a majority consensus emerges or t max rounds are exhausted.After t max rounds without a consensus, the algorithm defaults to the tie-breaker mechanism described in Section 5.1.2.</p>
<p>Analysis of Complexity</p>
<p>The decentralized approach uses k models for both generation and evaluation.For this reason the input and output tokens scale quadratically with number of models.As before, we instruct the model to output a fixed number of words for summary and an identifier only for evaluation and so ensure bounded runtime and cost.Further analysis can be found at Appendix B.2.</p>
<p>Experiments</p>
<p>To investigate the proposed multi-LLM summarization framework, we conduct extensive experiments to evaluate its effectiveness.</p>
<p>Experimental Setup</p>
<p>We use ArXiv (Cohan et al., 2018) and GovReport (Huang et al., 2021) to evaluate our summarization methods.We assess the quality of LLM-generated summaries using ROUGE-1, ROUGE-L, BLEU-1, and BLEU-4 metrics.For comparison with our multi-LLM approach, unless otherwise mentioned, we leverage GPT-3.5, GPT-4o, GPT-4o mini, and LLaMA3-8B as baselines.For these models, we perform the same chunking across all models, and the summarization prompt is identical to that in the first round of the multi-LLM process (Figure 6).Unless otherwise mentioned, all models use 4Kchar chunk-size, and the final summary represents a concatenation of the generated summaries.Finally, unless otherwise mentioned, we set W = 160 for all the models.</p>
<p>Main Results</p>
<p>Our multi-LLM framework outperforms single-LLM baselines by up to 3×, as seen in Table 2.The fact that both precision-and recall-focused metrics improved means the multi-LLM approach is robust.On average the centralized method improves the scores by 73%, and the decentralized method outperforms baselines by 70%.In our theoretical cost analysis (Section B.1 and B.2) we show that the input cost (in number of tokens) for the decentralized multiplies by the the number of agents participating in the evaluation, and with the more cost-effective centralized method our system is able to perform better than the single-LLM setup.This demonstrates the effectiveness of our proposed method under decentralized and decentralized frameworks.</p>
<p>We see that additional LLMs do not improve upon the 2-LLM setup (see Appendix C.3), and additional rounds of generation and evaluation do not further improve scores.This shows that even with just 2 LLMs and a single round of generation and evaluation we observe performance gains, meaning that the least costly version of the multi-LLM system is still able to deliver better summaries compared to single-LLM approaches.</p>
<p>In Table 2 we use GPT-3.5 as the evaluator and tie-breaking choice in our multi-LLM.We also run the multi-LLM system with GPT-4o mini as the evaluator and the tie-breaker, and the results are shown in Table 5.Again, the multi-LLM framework outperformed single-LLM baselines, averaging 64% improvement for the decentralized variant and 63% for the centralized variant.In some Table 2: Results for the decentralized and centralized Multi-LLM approaches.For the multi-LLM pipelines participating models are GPT-3.5 and GPT-4o mini.The results use GPT-3.5 for the evaluator in the centralized approach, and summaries from GPT-3.5 are chosen in tie-breaking for both centralized and de-centralized approaches.
ArXiv GovReport ROUGE-1 ↑ ROUGE-L ↑ BLEU-1 ↑ BLEU-4 ↑ ROUGE-1 ↑ ROUGE-L ↑ BLEU-1 ↑ BLEU-4 ↑ LLaMA3-8B 0.
individual scores, our framework improves upon single-LLM setups by up to 3×.These improvements are competitive to those we obtain from the multi-LLM setup in Table 2, which means our proposed framework works well for different central models and different tie-breaking models.</p>
<p>We also perform additional experiments with other variables.More specifically, we assess the performance of the multi-LLM framework with alternative combinations of models, with three models contributing to the summarization and evaluation, and with models receiving fine-grained prompts instead of the same prompt.In all of these experiments, we obtain competitive results compared to the first decentralized and centralized setup, and the scores are higher than single-LLM baselines, showing that our proposed framework performs consistently under different setups.</p>
<p>Ablation Studies</p>
<p>Varying Model Combinations In Table 2 we use GPT-3.5 and GPT-4o mini as the participating models in the multi-LLM framework.We further experiment with alternative combinations of models in the framework.As shown in Table 3 we again observe improvements across the board compared to the single-LLM baselines in Table 2, regardless of default model and number of rounds and type of interaction (decentralized vs. centralized).The improvements are competitive with those seen in the GPT-3.5 and GPT-4o mini combination.Further results are provided in Appendix C.2.</p>
<p>Varying the Number of LLMs In this experiment we use 3 LLMs in the setup instead of 2. We observe a 54% improvement for the decentralized method and 59% for the centralized method on average over single-LLM summaries, and for individual scores we see improvements of up to 2.9×.More detailed results are presented in Table 6 and in Appendix C.3.</p>
<p>Specialized Prompting</p>
<p>In all previous experiments we have kept the generation prompt identical for all LLMs.With multi-LLM approaches, this need not be the case.In this experiment we choose different prompts for different models when generating summaries, aiming to have unique knowledge bases of different models complement each other.As seen in Table 7, the centralized method results in a 66% performance increase over single-LLM baselines in Table 2, and the decentralized method has a 58% increase over the single-LLM baselines.For experimental details and further analysis see Section C.4 in the Appendix Short vs. Long-text Multi-LLM Summarization In this experiment, we use only the introduction section as the basis for summarization in the ArXiv dataset.Since the introduction typically shorter than the context window of LLMs, we refer to these as "short-text" summarization, in contrast to the "long-text" summarization we explore previously.The results in Table 8 shows that the centralized approach provides the most performance gains over single-LLM baselines -up to 2.4× on average, and the decentralized method sees a 2.3× increase.Further details can be found in Appendix C.5.</p>
<p>Cost Analysis</p>
<p>Table 4 presents the cost analysis for both decentralized and centralized methods based on the results in Table 2, highlighting key trends in input and output tokens across various stages of the summarization process.We observe that for evaluation stages the input and output token counts for the decentralized method are twice those for the centralized method, which reflect the number of LLMs in the setup.</p>
<p>Human evaluation</p>
<p>In addition to the ablation studies, we perform human evaluation of summaries similar to the last step of the multi-LLM framework, i.e. the evaluation phase (Section 4.2.2).The human raters are prompted with two sets of summaries from the generation phase (Section 4.1.1),and are instructed to evaluate these two sets of summaries for Coherence, Conciseness, and Fluency on 5-point Likert scales (Conroy and Dang, 2008) with rating guidelines for each possible score (Figure 10).The summaries are randomized and anonymized to reduce bias attributable to knowledge of authorship.We drop the Relevance criteria since no original text is provided to the human raters due to length.</p>
<p>We obtain 420 ratings from 7 raters, and find that humans generally prefer summaries produced by GPT-4o mini, which aligns with preferences by our multi-LLM framework.Human preferences also align with machine preferences for all three evaluation criteria to some degree.Conciseness has the highest agreement with multi-LLM evaluations (κ = 0.6).More details for human evaluations can be found in Appendix D.</p>
<p>Conclusion</p>
<p>This paper presented a multi-LLM framework for text summarization, and proposed two strategies, decentralized and centralized multi-LLM summarization.We demonstrated that the proposed multi-LLM summarization techniques lead to better generated summaries.Our results indicate that multi-LLM approaches are useful for improving text summarization.Future work should continue to investigate multi-LLM approaches for summarization.</p>
<p>Limitations</p>
<p>This work demonstrated the effectiveness of both our centralized and decentralized multi-LLM summarization approaches.Future work should further investigate various aspects, including more diverse LLMs, and explore other topologies beyond the two extremes we proposed.Furthermore, while we investigated a variety of datasets, future work can explore other domains.We believe there are many approaches that lie between the two extreme multi-LLM strategies we investigated empirically in this work.Finally, we did not optimize the prompts, as such we believe there is huge opportunity to achieve significantly better results by engineering better prompts to consider other important aspects of summarization.We leave these and other important directions for future work.</p>
<p>A Detailed Experimental Setup</p>
<p>Datasets: We use the test sets of ArXiv (Cohan et al., 2018) (first 20%, or 1,288 documents) and GovReport (Huang et al., 2021) (all, or 973 documents) as document input for our summarization methods.They cover a range of genres, providing diverse texts for evaluation.In ArXiv, the main article excluding the abstract is the target for summarization, and the abstract is used as the ground truth reference summary; for GovReport, the text is the main report and the ground truth is the humanwritten summary.ArXiv articles range from 241 to 44,489 space-delimited words long, with an average of 5,950 words; their summaries range from 46 to 290 words, averaging 164 words.GovReport main texts range from 396 to 31,371 words, averaging 7,379 words; their summaries range from 67 to 1,363 words, averaging 571 words.Evaluation Metrics: We assess the quality of LLM-generated summaries using ROUGE-1, ROUGE-L, BLEU-1, and BLEU-4 metrics.ROUGE scores emphasizes recall while BLEU scores emphasize precision.Baselines: For comparison with our multi-LLM approach, unless otherwise mentioned, we leverage GPT-3.5, GPT-4o, GPT-4o mini, and LLaMA3-8B as baselines.For these models, we perform the same chunking across all models, and the summarization prompt is identical to that in the first round of the multi-LLM summarization process (Figure 2).Unless otherwise mentioned, all models use 4K-char chunk-size, and the final summary for each document is concatenation of the generated summaries for each chunk in that document.</p>
<p>Finally, unless otherwise mentioned, we set W = 160 for all models.</p>
<p>B Theoretical Analysis &amp; Discussion</p>
<p>B.1 Centralized Apporach</p>
<p>Cost and Complexity per Round Let I denote the number of input tokens in the original text and O max represent an upper bound on the output tokens (i.e., maximum summary length).We consider k distinct LLMs and a maximum of t max conversational rounds.In each round i, we prompt all k LLMs with approximately I + δ i input tokens, where δ i denotes additional tokens introduced in that round (e.g., references to previously generated summaries).Each LLM then produces up to O max output tokens.Since input and output tokens often incur different costs, we consider them separately.For the generation phase, the input token cost per round is on the order of O(k • (I + δ i )), and the output token cost is on the order of O(k • O max ).</p>
<p>For evaluation, the central LLM processes k candidate summaries and I ec instructions, resulting in an input token cost of about O(k • O max + I ec ).By directing the central LLM to output only an anonymous identifier for the chosen summary, we reduce output token length in evaluation, thereby minimizing the chance of hallucination and enabling more straightforward cost accounting.</p>
<p>Multi-Round Overhead Over t max rounds, the total input token usage for generation is O(t max •k • (I +O max )), and the evaluation input token usage is O(t max •(k•O max +I ec )).Although this complexity may appear large, t max is typically small (e.g., 2 or 3), and O max is usually constrained (e.g., a brief 160-word summary).Moreover, careful prompt engineering can curtail δ i growth, ensuring that the number of tokens per round remains bounded.</p>
<p>Convergence and Quality Gains The iterative generation-evaluation mechanism aims to converge within a small number of rounds.With each iteration, models refine their outputs guided by previous results, potentially improving summary quality.This iterative refinement, while incurring additional steps, offers a practical trade-off between computation and quality, as the improved summaries can justify the limited number of extra rounds.</p>
<p>B.2 Decentralized Approach</p>
<p>Multi-Round Complexity Let I denote the number of input tokens in the original text and O max represent an upper bound on the output tokens (i.e., maximum summary length).We consider k distinct LLMs and a maximum of t max conversational rounds.</p>
<p>Over t max rounds, the worst-case token cost from generation is:
O(t max • k • (I + O max )).
The evaluation cost scales to:
O(t max • (k • I e + k 2 • O max )).
Combined, we have a total complexity per round of approximately:
O(k • I + k • O max + k • I e + k 2 • O max ).
Thus, for t max rounds, the overall complexity becomes:
O(t max • (k • I + k • I e + k • O max + k 2 • O max )).
Since k 2 •O max may dominate for large k, this term can become the bottleneck.However, in practical scenarios, k (the number of LLMs) is often small (e.g., 2-5), making the decentralized evaluation overhead manageable.</p>
<p>Trade-Offs and Practical Considerations</p>
<p>The decentralized evaluation approach increases computational overhead compared to the centralized model, as it requires every model to evaluate all candidate summaries.However, this additional cost is justified by the potential gains in robustness and reliability of the final output, but also by the flexibility to rely on multiple, potentially weaker models rather than a single, highly capable central evaluator.By employing a form of consensus voting, the system can arrive at a more stable decision even when no single model is individually strong.</p>
<p>While the added complexity of multi-round conversation can be non-trivial, it may lead to improved summary quality, especially when dealing with contentious or ambiguous source texts.Multiple rounds allow the system to refine the summaries and converge on a stable solution.If consensus emerges quickly, the number of rounds t max can be effectively reduced, thereby decreasing the total computational cost.Conversely, if no consensus is reached, the algorithm ultimately defaults to a tie-break mechanism after t max rounds, ensuring bounded time and cost.As with the centralized approach, prompt engineering and careful parameter selection (e.g., choosing O max , t max , and the number of participating models k) we can mitigate undue complexity.</p>
<p>C Ablation Study</p>
<p>C.1 Varying Evaluation LLM</p>
<p>In this section, we compare the scores of the centralized and decentralized approaches when the evaluator model and the tie-breaker models are GPT-4o or GPT-4o mini (instead of GPT-3.5 as in Table 2).These results are presented in Table 5.The sections where GPT-3.5 is the evaluator are reproduced from Table 2.</p>
<p>In these experiments, the summary-generating models remain the same as those in Table 2.In rows (in Table 5) where GPT-4o is listed as the evaluator, however, the decentralized method would have required GPT-4o to be the default choice for a tiebreaking summary as well when the model has not generated summaries.To remain maximally consistent with previous methodology, we modify the process here so that GPT-4o receives the finalround summaries from the decentralized method where GPT-3.5 is the tie-breaking choice and evaluator and performs a centralized evaluation on top of the decentralized results.The reason the GPT-3.5-defaultresults are chosen as the basis instead of GPT-4o mini is because as an evaluator and default choice GPT-3.5 produced better final summaries compared to GPT-4o mini for both centralized and decentralized methods.</p>
<p>The multi-LLM framework outperformed single-LLM baselines, averaging 64% improvement for the decentralized variant and 63% for the centralized variant.In some individual scores, our framework improves upon single-LLM setups by up to 3×.GPT-3.5 emerged as the best-scoring evaluator and the best-scoring tie-breaker choice: for the centralized method, GPT-3.5 as an evaluator and tie-breaking choice outperforms other evaluators and tie-breakers, and for the decentralized method, GPT-3.5 turned out to be the best tie-breaking choice.Furthermore, GPT-3.5 as a centralized evaluator and tie-breaking choice separately outperform both the decentralized and centralized methods using other models as the evaluator and tie-breaking choice.As with results in Table 2, additional rounds of evaluation and regeneration do not improve summary scores.</p>
<p>C.2 Varying Model Combinations</p>
<p>In Table 2 we present the results with GPT-3.5 and GPT-4o mini as the models in the combination; we now investigate the performance of our approaches for alternative combinations of LLMs (in Table 3).</p>
<p>We use the following combinations for the 2-LLM framework: GPT-3.5 and GPT-4o mini, with GPT-3.5 as the evaluator and default, GPT-4o and GPT-3.5, again with GPT-3.5 as evaluator and default, and finally GPT-4o and GPT-4o mini, with GPT-4o mini as the evaluator and default.</p>
<p>These alternative combinations all outperform single-LLM baselines.We see a 54% improvement in the decentralized variant and a 59% for the centralized variant.Combinations with GPT-3.5 as a member and the evaluator/default choice offer larger improvements compared to those without GPT-3.5.Since we have used GPT-4o mini as the evaluator and tie-breaker where GPT-3.5 is absent, a possible reason the improvements for these pairings are less than those where GPT-3.5 is present is that GPT-3.5 is a larger model than GPT-4o mini.</p>
<p>C.3 Varying the Number of LLMs</p>
<p>In this experiment, we increase the number of LLMs in our multi-LLM system to ascertain the effects on summary quality, and present the results in Table 6.Here we use GPT-3.5, GPT-4o mini, and GPT-4o in the multi-LLM system.We see that while the 3-LLM system still outperform the single-LLM baseline, increasing the number of LLMs from 2 to 3 does not improve performance upon the 2-LLM system, contrary to the trend observed in the previous sections where 2-LLM system outperform single-LLM baselines.</p>
<p>We offer two possible explanations for this finding.First, adding an additional LLM increases the complexity of the pipeline, which may lead to propagation of noise or redundancy in intermediate summaries.This added complexity could dilute the strengths of individual LLMs and reduce overall coherence and relevance in the final output.Second, the integration of a third LLM introduces a greater risk of inconsistencies in summarization styles, which may negatively affect evaluation metrics like ROUGE that rely on lexical overlap.</p>
<p>C.4 Specialized Prompting</p>
<p>We now investigate using a single LLM to generate multiple different summaries of the text, and then using our framework to obtain the best summary.We explore the efficacy of varying prompt formulations and model parameters in regards to our framework.This experiment is grounded in the intuition that long documents contain very diverse sections within their content which may benefit from different summarization strategies.For example, differ-Generate a summary that enhances coherence of the text in around 160 words.Output the summary text only and nothing else.</p>
<p>[text] ent chunks of a long document may cover distinct topics, serve various purposes, have diverse writing styles, and/or contain differing density.Given this diversity, a simple uniform summarization prompt is less likely to actually capture the required essential information from each chunk.With this, we propose a form of specialized prompting as a way to leverage the distinctive capabilities and specializations of each model for specific chunks specifically in regards to our framework.We hypothesize that the use of specialized prompting can help further leverage LLM capabilities within our suggested multi-LLM framework to produce higher quality summaries which are more suitable for subsequent evaluation by multiple LLMs.</p>
<p>We begin by generating four initial summaries using two sets of specialized prompts designed for GPT-3.5 and GPT-4o mini, ensuring that each model receives two distinct prompts.One prompt focuses on enhancing the coherence of the resulting summary (see Figure 7), while the second prompt aims to maximize precision in conveying the key facts (see Figure 8).After producing these four baseline summaries, we feed them into our multi-LLM framework, which incorporates two agents -GPT-3.5 and GPT-4o mini-working collaboratively.GPT-3.5 and GPT-4o mini are used for the initial generation of summaries, and GPT-3.5 also serves as the evaluator.The framework and methodology following the generation of the four baseline summaries, as well as their inclusion as input, mirror the procedures used to obtain decentralized and centralized results on ArXiv and GovReport in Table 2, with GPT-3.5 functioning as the evaluator.Results for this experiment are provided in Table 7.This experiment demonstrates that employing specialized prompting strategies within both decentralized and centralized multi-LLM frameworks significantly enhances the quality of generated summaries.These results show the importance of prompt engineering and strategic framework design in multi-LLM summarization tasks and we leave this for future work.</p>
<p>C.5 Short-text vs. Long-text Summarization</p>
<p>In this section, we investigate the effectiveness of our approach for shorter text summarization.For this experiment, we leverage the ArXiv dataset and only use the introduction of the paper as input for summarization and evaluate against the same ground-truth.The introduction subsections of papers are typically rich in content yet contain enough brevity to serve as quality standardized reference bases for our goal of long and short text experimentation.With this experiment we present results that showcase the trade offs and performance differences of our methodologies on shorter text summarization compared to that of long document summarization.Generally, ArXiv papers contain detailed markers and section titles to distinguish introduction sections.However, using the Hugging Face dataset of ArXiv papers for our experimentation the format in which article is represented is a string containing the "body" of the paper which contains little to no explicit markers for section identification.Thus, we present a simple heuristic to distinguish the introduction text from the rest of the article text.We manually went through 5 randomized example articles, with an assumption that the beginning of the article text starts with the introduction section, and found at which inflection point the introduction section concludes.After averaging the word count of the introduction sections and including an extension buffer to capture certain articles which may have slightly longer introduction sections we establish a benchmark for the using the first 1,500 words in ArXiv articles as our reference introduction section.We algorithmically consider a word as a break between the article string wherever there is whitespace.Refer to Figure 9 for more detailed explanation.We ultimately curate 20% of the examples from the test set using this strategy for performance testing on our metrics.Full results are provided in Table 8.</p>
<p>We highlight several key aspects of our multi-LLM summarization methodology using both the centralized and decentralized approaches, showcas-ing distinct performance across both long and short text summarization tasks.As evident by our results, short articles consistently show better performance compared to long articles, showcasing the inherent complexities and nuances of longer texts that plague LLMs in terms of capturing and summarizing relevant content.The similar performance on metrics like ROUGE-1 and BLEU-4 in our centralized approach across different text lengths might indicate a consistency in how our methodology is able to capture the essential content and has the ability to reproduce the core narrative elements of the text regardless of length.Furthermore, we posit the difference in performance across long and short text for BLEU-1 is based primarily on the metrics itself as it measures the unigram overlap between the generated summary and the reference text.In the case of short texts, the decentralized approach and especially the 3 round performs best as each round and each model provides an opportunity to focus more accurately on and determine crucial unigrams that are significant within the context of a compact introduction section.This iterative refinement likely leads to a higher precision in capturing key terms and phrases, directly contributing to better BLEU-1 scores than in the case of the centralized approach which performs best as the context length is scaled up as shown in the results for long text.</p>
<p>D Human Evaluation</p>
<p>In the human evaluation, we select the first 10 pairs of summaries generated before the final evaluation step by the decentralized, one-round maximum framework (the best-performing setup for ArXiv; see Table 2), and prompt human raters to rate each summary according to Coherence, Conciseness, and Fluency metrics, each represented by a 5-point Likert scale.The goal is to compare human rater preferences with preferences of the LLM performing the final evaluation and therefore producing the final result of the multi-LLM pipeline.Each summary pair consists of texts generated by GPT-3.5 and GPT-4o mini randomized in order of presentation and anonymized such that the raters do not know which model produced which summaries.We do not prompt raters with the corresponding original text as in the multi-LLM method due to the original text's length and technicality, and therefore we remove the Relevance criterion used in Conroy and Dang (2008).For the remaining criteria we  8: Results on short summarization tasks using the ArXiv dataset for the decentralized and centralized Multi-LLM approaches.Note that these results use GPT-3.5 for the evaluator in the centralized approach, and for breaking ties in the decentralized multi-LLM approaches.</p>
<p>provide guidelines for each possible point value to improve reproducibility.Instructions provided to human raters and rating guidelines can be found in Figure 10.At the end of human evaluation, we collect 420 ratings from 7 raters (6 men, 1 woman; ages 23-31; 4 East Asian, 2 South Asian, 1 White). 2 To determine preferences for the human raters, we average the rating for each model and each summary in each evaluation criterion (Table 9).Thus, for each summary and for each criterion we obtain two averaged scores, one for GPT-3.5 and GPT-4o mini.We then determine the human preference by choosing the model with the higher score, and if the scores are the same, we fallback on the default choice GPT-3.5, consistent with the fallback default in the evaluation step in our multi-LLM framework (Table 10).We note that in all three criteria our framework show some agreement with the human raters, as measured by Cohen's kappa.For conciseness, we observe an agreement of κ = 0.6.</p>
<p>Figure 1 :
1
Figure 1: Centralized and Decentralized approaches using a 5-LLM example.Similar topologies can be applied to any ("k") number of LLMs.In centralized interactions, all models communicate with a central model; in decentralized interactions, each model communicate with every other model and also itself.</p>
<p>Figure 7 :
7
Figure 7: Prompt 1 for generating the initial summary in the first round.</p>
<p>Figure 8 :
8
Figure 8: Prompt 2 for generating the initial summary in the first round.</p>
<p>Table 1 :
1
Overview of Multi-LLM Summarization Framework (Sections 4-5).</p>
<p>Given the original text below, along with the summaries of that text by [k] agents, please evaluate the summaries and output the name of the agent that has the best summary.Output the exact name only and nothing else.
ORIGINAL:[chunk or concatenated chunk summaries S]Summary by agent_1:[LLM 1's summary]. . .Summary by agent_k:[LLM k's summary]Figure 4: Evaluation prompt for evaluating the sum-maries generated by different LLMs using our conversa-tional (decentralized) multi-LLM framework. "k" is aparameter reflecting the number of LLMs that generatesummaries.</p>
<p>Table 3 :
3
Max Rounds Multi-LLM Model Combination ROUGE-1 ↑ ROUGE-L ↑ BLEU-1 ↑ BLEU-4 ↑ Varying the combination of models in our Multi-LLM approaches.Note rounds is the max number of rounds allowed and all results are for ArXiv.Bolded numbers are best scores for each round-model combination.Underlined numbers are overall best scores for each metric in this table.Furthermore, the central LLM is highlighted in blue and for the decentralized multi-LLM approaches, we highlight the LLM used for tie-breaking in green.
GPT-3.5 &amp; GPT-4o mini0.3130.1630.2000.0293 RoundsGPT-4o &amp; GPT-3.50.3130.1590.1970.025DecentralizedGPT-4o &amp; GPT-4o mini0.3020.1520.1850.022GPT-3.5 &amp; GPT-4o mini0.3390.1800.2240.0431 RoundsGPT-4o &amp; GPT-3.50.3280.1700.2120.033GPT-4o &amp; GPT-4o mini0.3050.1530.1890.023GPT-3.5 &amp; GPT-4o mini0.3290.1680.2170.0313 RoundsGPT-4o &amp; GPT-3.50.3250.1660.2140.029CentralizedGPT-4o &amp; GPT-4o mini0.3040.1530.1880.022GPT-3.5 &amp; GPT-4o mini0.3330.1730.2190.0361 RoundsGPT-4o &amp; GPT-3.50.3390.1770.2280.039GPT-4o &amp; GPT-4o mini0.3060.1550.1900.022Input Tokens Output Tokens Average Tokens Total TokensDecentralizedMulti-LLM 3 round max Multi-LLM 1 round max383.73M 129.36M25.63M 11.89M14.62M 11.77M409.37M 141.25MCentralizedMulti-LLM 3 round max Multi-LLM 1 round max216.65M 77.69M19.55M 6.77M14.76M 10.56M236.2M 84.46M</p>
<p>Table 4 :
4
Cost Analysis of our Multi-LLM Decentralized and Centralized Summarization Methods.Note M = millions of tokens.</p>
<p>ArXivROUGE-1 ↑ ROUGE-L ↑ BLEU-1 ↑ BLEU-4 ↑
Long TextDecentralizedMulti-LLM 3 round max Multi-LLM 1 round max0.329 0.3330.168 0.1730.217 0.2190.031 0.036CentralizedMulti-LLM 3 round max Multi-LLM 1 round max0.313 0.3380.163 0.1800.200 0.2240.029 0.043Short TextDecentralizedMulti-LLM 3 round max Multi-LLM 1 round max0.360 0.3690.188 0.1980.328 0.3090.038 0.044CentralizedMulti-LLM 3 round max Multi-LLM 1 round max0.367 0.3790.194 0.2060.321 0.3050.041 0.049Table
Here our implementation requires votes exceeding absolute majority for a summary to be immediately selected. In the case
of 2 LLMs, this is equivalent to a unanimous decision because one vote does not satisfy absolute majority.
Raters are authors of this paper and close associates who have consented to submitting evaluations, and no rater has prior knowledge of authors of the set of summaries being evaluated.
Table5: Results for different evaluating and tie-breaking models for Multi-LLM approaches.The choice of the tie-breaker models is the same as the choice of evaluator model.We bold the best results for each combination of the experimental variables, and we underline the best results overall.For ease of comparison, we reproduce the best-performing 2-LLM results obtained in Table2ArXiv GovReport Table6: Multi-LLM framework with three models.We bold the best results for each combination of the experimental variables, and we underline the best results overall.For ease of comparison, we reproduce the best-performing 2-LLM results obtained in Table2ArXiv Table7: Results on the use of 2 specialized prompts on where the only change in the pipeline is that 4 total specialized baseline summaries are fed in initially instead of the 2 simple prompts fed in the methodology used to curate Table2.Note that these results use GPT-3.5 for the evaluator in the centralized approach, and for breaking ties in the decentralized multi-LLM approaches.This is for a 15 sample size for both datasets.Refer to Figure7and Figure8for the prompts used for initial generation.We bold the best results for each combination of the experimental variables, and we underline the best results overall.Generate a summary that maximizes precision related to the key facts of the text in around 160 words.Output the summary text only and nothing else.[text]Human raters Multi-LLM (Ours) SummaryCoherence Conciseness Fluency Averaged 1 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 2 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT-4o mini 3 GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini 4 GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini GPT-3.5 5 GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini 6 GPT-4o mini GPT-4o mini GPT-3.5 GPT-4o mini GPT-4o mini 7 GPT-4o mini GPT-3.5 GPT-4o mini GPT-4o mini GPT-3.5 8 GPT-4o mini GPT-3.5 GPT-3.5 GPT-4o mini GPT-3.5 9 GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini 10 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 Cohen's κ 0.2 0.6 0.1 0.2 -Table10: Human choices obtained from choosing the model with the higher average score, done separately for each evaluating criterion.At the right-most column we show the choices made in the final evaluation step by our multi-LLM framework, specifically the decentralized one-round-max setup with GPT-3.5 as the evaluator.At the bottom row we show the inter-rater agreement (as measured by Cohen's kappa) between the human choices and the machine choices for each criterion Figure9: Here we showcase an example of how we choose at which point an introduction ends.The total word count of this example article was 7,671 and the word count of the reference summary was 172.We highlight the inflection sentence which most serves as the transition from the actual background and theoretical setup of the paper to the actual methodologies which are then detailed in later text.From here we gather the word count of everything before the inflection sentence and classify it as our reference introduction text for experimentation, including the inflection sentence.In this case, the resulting introduction section had a total word count of 1203.Read the two summaries (column C and H) and grade them on the following aspects Give a grade from 1 -5 for each category, 1 being the lowest score and 5 being the highest score, as described below.For summary 1, fill in columns E-G; for summary 2, fill in columns J-L.CoherenceEvaluate the logical flow of sentences and organization of information within the summary.1: Ideas do not logically follow from one to another, even if they are relevant.2: There are some phrases to connect one idea to another.4: Most ideas are described in appropriate detail.There may be occasional verbosity.5: All ideas are described with appropriately complex or simple sentences.FluencyEvaluate the grammatical correctness of the generated summary.Check for any awkward phrasing or grammatical errors that might hinder comprehension.1: Text is grammatically incorrect or very difficult to understand.There are incomplete sentences or incorrectly used punctuations.2: The text is generally difficult to understand, but some sections convey meaningful ideas.3: The text is generally grammatically correct.Some sentences may have incorrect grammar.4: The text is grammatically correct and sentences have understandable structure.There are occasional incorrectly phrased sentences.5: Summary is easy to follow and understand.No grammatical errors.Figure10: Screenshot of the interface (with scores already filled in) and instructions given to raters for human evaluation.Instructions include the column indices (not shown in the screenshot) for easier reference.Each summary is rated according to the criteria listed in the instructions (i.e.Coherence, Conciseness, and Fluency).We provide guidelines for each criterion and each possible score for that criterion.
From sparse to dense: Gpt-4 summarization with chain of density prompting. Griffin Adams, Alexander Fabbri, Faisal Ladhak, Eric Lehman, Noémie Elhadad, 2023</p>
<p>Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang, Etc: Encoding long and structured inputs in transformers. 2020</p>
<p>Text summarization using large language models: A comparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models. Lochan Basyal, Mihir Sanghvi, 2023</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, 2020</p>
<p>Booookscore: A systematic exploration of book-length summarization in the era of llms. Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer, 2024</p>
<p>Reconcile: Round-table conference improves reasoning via consensus among diverse llms. Justin Chih-Yao Chen, Swarnadeep Saha, Mohit Bansal ; Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2024Jeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</p>
<p>Hierarchical summarization: Scaling up multi-document summarization. Janara Christensen, Stephen Soderland, Gagan Bansal, Mausam , Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 52nd Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20141</p>
<p>A discourse-aware attention model for abstractive summarization of long documents. Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang, Goharian, 2018</p>
<p>Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality. John M Conroy, Hoa Trang Dang, Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). the 22nd International Conference on Computational Linguistics (Coling 2008)Manchester2008. 2008Organizing Committee</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, Ruslan Salakhutdinov, 10.18653/v1/P19-1285Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, 2023</p>
<p>A divide-and-conquer approach to the summarization of long documents. Alexios Gidiotis, Grigorios Tsoumakas, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 282020</p>
<p>Generic text summarization using relevance measure and latent semantic analysis. Yihong Gong, Xin Liu, Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Tanya Goyal, Jessy Junyi, Li, the 24th Annual International ACM SIGIR Conference on Research and Development in Information RetrievalACM2001and Greg Durrett. 2023. News summarization and evaluation in the era of gpt-3</p>
<p>Longt5: Efficient text-to-text transformer for long sequences. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang, 2021</p>
<p>Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend. 2015</p>
<p>Efficient attentions for long document summarization. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Ji Heng, Lu Wang, 10.18653/v1/2021.naacl-main.112Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Long-input summarization using large language models. Emma Järvinen, 2024</p>
<p>Abstractive long text summarization using large language models. Gunjan Keswani, Wani Bisen, Hirkani Padwad, Yash Wankhedkar, Sudhanshu Pandey, Ayushi Soni, International Journal of Intelligent Systems and Applications in Engineering. 1212s2024</p>
<p>BillSum: A corpus for automatic summarization of US legislation. Anastassia Kornilova, Vladimir Eidelman, 10.18653/v1/D19-5406Proceedings of the 2nd Workshop on New Frontiers in Summarization. the 2nd Workshop on New Frontiers in SummarizationHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBartOnline. Association for Computational Linguistics2020</p>
<p>Hipool: Modeling long documents using graph neural networks. Irene Li, Aosong Feng, Dragomir Radev, Rex Ying, 10.18653/v1/2023.acl-short.16Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20232Short Papers). Association for Computational Linguistics</p>
<p>Improving multi-agent debate with sparse communication topology. Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, Eugene Ie, 2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, 2024</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 2023</p>
<p>Text summarization with pretrained encoders. Yang Liu, Mirella Lapata, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Brio: Bringing order to abstractive summarization. Yixin Liu, Pengfei Liu, Dragomir Radev, Graham Neubig, 2022</p>
<p>A survey on extractive text summarization. S Mallick, A Ghosh, Journal of Artificial Intelligence Research. 652019</p>
<p>Textrank: Bringing order into texts. Rada Mihalcea, Paul Tarau, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. the 2004 Conference on Empirical Methods in Natural Language Processing2004</p>
<p>Abstractive text summarization using sequence-tosequence rnns and beyond. Ramesh Nallapati, Bowen Zhou, 2016Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, 2018</p>
<p>Long document summarization with top-down and bottom-up inference. Bo Pang, Erik Nijkamp, Wojciech Kryściński, Silvio Savarese, Yingbo Zhou, Caiming Xiong, 2022</p>
<p>Incorporating distributions of discourse structure for long document abstractive summarization. Dongqi Pu, Yifan Wang, Vera Demberg, 2023a</p>
<p>Summarization is (almost) dead. Xiao Pu, Mingqi Gao, Xiaojun Wan, 2023b</p>
<p>A neural attention model for abstractive sentence summarization. Alexander M Rush, Sumit Chopra, Jason Weston, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas WolfRush. 2022. Multitask prompted training enables zero-shot task generalization</p>
<p>Get to the point: Summarization with pointergenerator networks. Abigail See, Peter J Liu, Christopher D Manning, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>Distilbart-cnn-12-6. Sam Shleifer, 2020</p>
<p>Adaptive attention span in transformers. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin, 10.18653/v1/P19-1032Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Extractive summarization of long documents by combining global and local context. Wen Xiao, Giuseppe Carenini, 10.18653/v1/D19-1298Proceedings of the Summary Coherence Conciseness Fluency Averaged Average scores by raters for: GPT-3.5 GPT-4o mini GPT-3.5 GPT-4o mini GPT-3.5 GPT-4o mini GPT-3.5 GPT-4o mini 1. the Summary Coherence Conciseness Fluency Averaged Average scores by raters for: GPT-3.5 GPT-4o mini GPT-3.5 GPT-4o mini GPT-3.5 GPT-4o mini GPT-3.5 GPT-4o mini 12019</p>
<p>Averaged scores (out of 5) given by human raters for each evaluation criterion for the first 10 summaries from the ArXiv dataset. The raters are asked to rate each summary on coherence, conciseness, and fluency on a 5-point Likert scale. We additionally show the score averaged from the scores for the three criteria. We bold the higher average score for each criterion, and underline the choice of the human raters between GPT-3.5 and GPT-4o mini summaries. When two summaries in a particular criterion have the same average score. 9we fallback on the default choice GPT-3.5, consistent with the evaluation step in our multi-LLM framework</p>            </div>
        </div>

    </div>
</body>
</html>
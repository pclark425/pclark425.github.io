<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1658 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1658</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1658</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-221040942</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2008.01594v1.pdf" target="_blank">An Imitation from Observation Approach to Sim-to-Real Transfer</a></p>
                <p><strong>Paper Abstract:</strong> The sim to real transfer problem deals with leveraging large amounts of inexpensive simulation experience to help artificial agents learn behaviors intended for the real world more efficiently. One approach to sim-to-real transfer is using interactions with the real world to make the simulator more realistic, called grounded sim to-real transfer. In this paper, we show that a particular grounded sim-to-real approach, grounded action transformation, is closely related to the problem of imitation from observation IfO, learning behaviors that mimic the observations of behavior demonstrations. After establishing this relationship, we hypothesize that recent state-of-the-art approaches from the IfO literature can be effectively repurposed for such grounded sim-to-real transfer. To validate our hypothesis we derive a new sim-to-real transfer algorithm - generative adversarial reinforced action transformation (GARAT) - based on adversarial imitation from observation techniques. We run experiments in several simulation domains with mismatched dynamics, and find that agents trained with GARAT achieve higher returns in the real world compared to existing black-box sim-to-real methods</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1658.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1658.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GARAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Reinforced Action Transformation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial imitation-from-observation algorithm that learns an action-transformation policy to ground a simulator by reducing the marginal transition-distribution mismatch between simulator and real-world (or target) dynamics, enabling improved sim-to-real transfer of control policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Multiple simulated robotic agents (MuJoCo and PyBullet task suite; Minitaur proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Pretrained control policies for standard locomotion and control tasks (InvertedPendulum, Hopper, HalfCheetah, Walker, Ant, Minitaur) representing robotic agents; policies are learned in a source simulator and intended to operate in a target (mismatched) environment or higher-fidelity simulator as proxy for the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics control / legged locomotion and control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo, PyBullet (via OpenAI Gym interface); OpenAI Gym benchmark environments</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics simulators modeling rigid-body dynamics, contacts, actuators, gravity and friction for standard reinforcement-learning control tasks; used both as the source simulator and (in experiments) as the target/'real' environment by modifying dynamics or using a higher-fidelity simulator (Minitaur).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Varied: from low/medium-fidelity mismatched dynamics (modified MuJoCo environments) to high-fidelity physics simulator used as 'real' proxy (Minitaur high-fidelity simulator).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, contact dynamics, gravity, actuator/torque dynamics, masses of bodies, joint dynamics, friction coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Real-world sensor noise and unmodeled environmental variability (wear, delays, exact contact micro-physics) were not explicitly modelled; many experiments used simplified or mismatched parameter settings (e.g. altered masses, friction, gravity) rather than full real-world noise models or hardware latency.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>In experiments the 'real' environment was either (a) a modified simulator with different dynamics parameters (heavier pendulum, increased mass, higher gravity or friction), or (b) a dedicated high-fidelity Minitaur simulator used as the real-world proxy; no physical robot experiments were reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Control and locomotion skills: balancing (InvertedPendulum) and locomotion for Hopper, HalfCheetah, Walker, Ant, and Minitaur (gaits, stable locomotion).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Agent policies pretrained in source simulator using reinforcement learning (TRPO); the action-transformation policy (π_g) trained using adversarial imitation-from-observation (GARAT) with a discriminator (GAN-style) and policy gradient (PPO); final agent policy re-trained in grounded simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Primary metrics: (1) environment return measured in the target/'real' environment (normalized such that π_real = 1 and π_sim = 0), averaged over episodes; (2) per-step transition L2 error between grounded simulator and 'real' environment transitions for simulator grounding evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Pretrained simulator policies (π_sim) performed poorly in mismatched target environments (baseline reference set to 0 in normalized plots); exact numeric returns per domain not tabulated in the paper text, but results show simulator-trained policies often collapsed in target environments (e.g. Ant with doubled gravity).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>GARAT-grounded re-trained policies achieved performance close to policies trained directly in the 'real' environment: in several domains GARAT reached on-par performance (normalized ≈1) with only a few thousand 'real' transitions; in Minitaur, GARAT achieved >80% of the optimal 'real' performance with 1,000 target-environment transitions whereas the next best baseline (GAT) reached ≲50% and required an order of magnitude more real transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Intentional dynamics mismatches: altered body masses, torso mass, pendulum mass, foot friction, doubled gravity, actuator vs torque modeling differences; general unmodelled differences in transition dynamics leading to marginal transition-distribution mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Learning an action-transformation policy that reduces the marginal transition-distribution mismatch (via adversarial IfO objective) using a small dataset of target trajectories; grounding the simulator via action transformation (black-box simulator approach) before re-training agent policy in grounded simulator; availability of a few hundred to a few thousand representative target-environment transitions to drive grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No strict quantitative fidelity threshold is given; the paper identifies that reducing per-step transition error and matching marginal transition distributions are critical for transfer success (e.g., GARAT lowered L2 transition error substantially and that correlated with improved transfer), and shows that improved grounding (even from limited data) enables effective transfer. No explicit numeric fidelity tolerances (e.g. 'within X%') are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Limited collection of target-environment ('real') trajectories by rolling out the pretrained agent policy to produce state-only trajectories; amounts reported in experiments include as few as one trajectory up to a few thousand transitions (notably Minitaur success with ~1,000 transitions); these target samples are used to train the discriminator and action-transformer, not for long agent re-training on real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparisons include ungrounded simulator vs GAT-grounded vs GARAT-grounded simulators and different amounts of target data; GARAT produced lower per-step transition L2 error and responses qualitatively more similar to the 'real' environment compared to GAT (e.g., with one target trajectory GARAT matched GAT trained with ~100 trajectories), and GARAT-enabled policies transferred better across a range of modified environments (MuJoCo and PyBullet) and particularly in Minitaur where a high-fidelity simulator served as the target.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grounded action transformation can be formulated as an imitation-from-observation problem; adversarial IfO methods (GARAT) effectively learn action-transformations that better ground black-box simulators than prior GAT, reducing marginal transition-distribution mismatch and per-step transition error; as a result, policies re-trained in GARAT-grounded simulators transfer significantly better to mismatched or higher-fidelity target environments using only a small number of target-environment trajectories (often a few hundred to a few thousand), outperforming robustness baselines (RARL, ANE) and GAT across most tested domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Imitation from Observation Approach to Sim-to-Real Transfer', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounded action transformation for robot learning in simulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Learning agile and dynamic motor skills for legged robots <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1658",
    "paper_id": "paper-221040942",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "GARAT",
            "name_full": "Generative Adversarial Reinforced Action Transformation",
            "brief_description": "An adversarial imitation-from-observation algorithm that learns an action-transformation policy to ground a simulator by reducing the marginal transition-distribution mismatch between simulator and real-world (or target) dynamics, enabling improved sim-to-real transfer of control policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Multiple simulated robotic agents (MuJoCo and PyBullet task suite; Minitaur proxy)",
            "agent_system_description": "Pretrained control policies for standard locomotion and control tasks (InvertedPendulum, Hopper, HalfCheetah, Walker, Ant, Minitaur) representing robotic agents; policies are learned in a source simulator and intended to operate in a target (mismatched) environment or higher-fidelity simulator as proxy for the real robot.",
            "domain": "general robotics control / legged locomotion and control tasks",
            "virtual_environment_name": "MuJoCo, PyBullet (via OpenAI Gym interface); OpenAI Gym benchmark environments",
            "virtual_environment_description": "Physics simulators modeling rigid-body dynamics, contacts, actuators, gravity and friction for standard reinforcement-learning control tasks; used both as the source simulator and (in experiments) as the target/'real' environment by modifying dynamics or using a higher-fidelity simulator (Minitaur).",
            "simulation_fidelity_level": "Varied: from low/medium-fidelity mismatched dynamics (modified MuJoCo environments) to high-fidelity physics simulator used as 'real' proxy (Minitaur high-fidelity simulator).",
            "fidelity_aspects_modeled": "Rigid-body dynamics, contact dynamics, gravity, actuator/torque dynamics, masses of bodies, joint dynamics, friction coefficients.",
            "fidelity_aspects_simplified": "Real-world sensor noise and unmodeled environmental variability (wear, delays, exact contact micro-physics) were not explicitly modelled; many experiments used simplified or mismatched parameter settings (e.g. altered masses, friction, gravity) rather than full real-world noise models or hardware latency.",
            "real_environment_description": "In experiments the 'real' environment was either (a) a modified simulator with different dynamics parameters (heavier pendulum, increased mass, higher gravity or friction), or (b) a dedicated high-fidelity Minitaur simulator used as the real-world proxy; no physical robot experiments were reported in this paper.",
            "task_or_skill_transferred": "Control and locomotion skills: balancing (InvertedPendulum) and locomotion for Hopper, HalfCheetah, Walker, Ant, and Minitaur (gaits, stable locomotion).",
            "training_method": "Agent policies pretrained in source simulator using reinforcement learning (TRPO); the action-transformation policy (π_g) trained using adversarial imitation-from-observation (GARAT) with a discriminator (GAN-style) and policy gradient (PPO); final agent policy re-trained in grounded simulator.",
            "transfer_success_metric": "Primary metrics: (1) environment return measured in the target/'real' environment (normalized such that π_real = 1 and π_sim = 0), averaged over episodes; (2) per-step transition L2 error between grounded simulator and 'real' environment transitions for simulator grounding evaluations.",
            "transfer_performance_sim": "Pretrained simulator policies (π_sim) performed poorly in mismatched target environments (baseline reference set to 0 in normalized plots); exact numeric returns per domain not tabulated in the paper text, but results show simulator-trained policies often collapsed in target environments (e.g. Ant with doubled gravity).",
            "transfer_performance_real": "GARAT-grounded re-trained policies achieved performance close to policies trained directly in the 'real' environment: in several domains GARAT reached on-par performance (normalized ≈1) with only a few thousand 'real' transitions; in Minitaur, GARAT achieved &gt;80% of the optimal 'real' performance with 1,000 target-environment transitions whereas the next best baseline (GAT) reached ≲50% and required an order of magnitude more real transitions.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Intentional dynamics mismatches: altered body masses, torso mass, pendulum mass, foot friction, doubled gravity, actuator vs torque modeling differences; general unmodelled differences in transition dynamics leading to marginal transition-distribution mismatch.",
            "transfer_enabling_conditions": "Learning an action-transformation policy that reduces the marginal transition-distribution mismatch (via adversarial IfO objective) using a small dataset of target trajectories; grounding the simulator via action transformation (black-box simulator approach) before re-training agent policy in grounded simulator; availability of a few hundred to a few thousand representative target-environment transitions to drive grounding.",
            "fidelity_requirements_identified": "No strict quantitative fidelity threshold is given; the paper identifies that reducing per-step transition error and matching marginal transition distributions are critical for transfer success (e.g., GARAT lowered L2 transition error substantially and that correlated with improved transfer), and shows that improved grounding (even from limited data) enables effective transfer. No explicit numeric fidelity tolerances (e.g. 'within X%') are provided.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Limited collection of target-environment ('real') trajectories by rolling out the pretrained agent policy to produce state-only trajectories; amounts reported in experiments include as few as one trajectory up to a few thousand transitions (notably Minitaur success with ~1,000 transitions); these target samples are used to train the discriminator and action-transformer, not for long agent re-training on real hardware.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparisons include ungrounded simulator vs GAT-grounded vs GARAT-grounded simulators and different amounts of target data; GARAT produced lower per-step transition L2 error and responses qualitatively more similar to the 'real' environment compared to GAT (e.g., with one target trajectory GARAT matched GAT trained with ~100 trajectories), and GARAT-enabled policies transferred better across a range of modified environments (MuJoCo and PyBullet) and particularly in Minitaur where a high-fidelity simulator served as the target.",
            "key_findings": "Grounded action transformation can be formulated as an imitation-from-observation problem; adversarial IfO methods (GARAT) effectively learn action-transformations that better ground black-box simulators than prior GAT, reducing marginal transition-distribution mismatch and per-step transition error; as a result, policies re-trained in GARAT-grounded simulators transfer significantly better to mismatched or higher-fidelity target environments using only a small number of target-environment trajectories (often a few hundred to a few thousand), outperforming robustness baselines (RARL, ANE) and GAT across most tested domains.",
            "uuid": "e1658.0",
            "source_info": {
                "paper_title": "An Imitation from Observation Approach to Sim-to-Real Transfer",
                "publication_date_yy_mm": "2020-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounded action transformation for robot learning in simulation",
            "rating": 2,
            "sanitized_title": "grounded_action_transformation_for_robot_learning_in_simulation"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Learning agile and dynamic motor skills for legged robots",
            "rating": 1,
            "sanitized_title": "learning_agile_and_dynamic_motor_skills_for_legged_robots"
        }
    ],
    "cost": 0.01131275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Imitation from Observation Approach to Sim-to-Real Transfer</p>
<p>Siddarth Desai sidrdesai@utexas.edu 
Department of Computer Science
Army Research Lab
The University of Texas at Austin
University of Edinburgh
The University of Texas at Austin and Sony AI</p>
<p>Ishan Durugkar ishand@cs.utexas.edu 
Department of Computer Science
Army Research Lab
The University of Texas at Austin
University of Edinburgh
The University of Texas at Austin and Sony AI</p>
<p>Haresh Karnan 
Department of Computer Science
Army Research Lab
The University of Texas at Austin
University of Edinburgh
The University of Texas at Austin and Sony AI</p>
<p>Garrett Warnell garrett.a.warnell.civ@mail.mil 
Department of Computer Science
Army Research Lab
The University of Texas at Austin
University of Edinburgh
The University of Texas at Austin and Sony AI</p>
<p>Josiah Hanna josiah.hanna@ed.ac.uk 
Department of Computer Science
Army Research Lab
The University of Texas at Austin
University of Edinburgh
The University of Texas at Austin and Sony AI</p>
<p>Peter Stone pstone@cs.utexas.edu 
Department of Computer Science
Army Research Lab
The University of Texas at Austin
University of Edinburgh
The University of Texas at Austin and Sony AI</p>
<p>An Imitation from Observation Approach to Sim-to-Real Transfer</p>
<p>The sim-to-real transfer problem deals with leveraging large amounts of inexpensive simulation experience to help artificial agents learn behaviors intended for the real world more efficiently. One approach to sim-to-real transfer is using interactions with the real world to make the simulator more realistic, called grounded simto-real transfer. In this paper, we show that a particular grounded sim-to-real approach, grounded action transformation, is closely related to the problem of imitation from observation (IfO): learning behaviors that mimic the observations of behavior demonstrations. After establishing this relationship, we hypothesize that recent state-of-the-art approaches from the IfO literature can be effectively repurposed for such grounded sim-to-real transfer. To validate our hypothesis we derive a new sim-to-real transfer algorithm -generative adversarial reinforced action transformation (GARAT) -based on adversarial imitation from observation techniques. We run experiments in several simulation domains with mismatched dynamics, and find that agents trained with GARAT achieve higher returns in the real world compared to existing black-box sim-to-real methods. § Equal contribution Preprint. Under review.</p>
<p>Introduction</p>
<p>In the robot learning community, sim-to-real approaches seek to leverage inexpensive simulation experience to more efficiently learn control policies that perform well in the real world. This paradigm allows us to utilize powerful machine learning techniques without extensive real-world testing, which can be expensive, time-consuming, and potentially dangerous. Sim-to-real transfer has been used effectively to learn a fast humanoid walk [14], dexterous manipulation [24], and agile locomotion skills [27]. In this work, we focus on the paradigm of simulator grounding [9,14,7], which modifies a simulator's dynamics to more closely match the real world dynamics using some real world data. Policies then learned in such a grounded simulator transfer better to the real world.</p>
<p>Separately, the machine learning community has also devoted attention to imitation learning [5], i.e. the problem of learning a policy to mimic demonstrations provided by another agent. In particular, recent work has considered the specific problem of imitation from observation (IfO) [21], in which an imitator mimics the expert's behavior without knowing which actions the expert took, only the outcomes of those actions (i.e. state-only demonstrations). While the lack of action information presents an additional challenge, recently-proposed approaches have suggested that this challenge may be addressable [42,44].</p>
<p>In this paper, we show that a particular grounded sim-to-real technique, called grounded action transformation (GAT) [14], can be seen as a form of IfO. We therefore hypothesize that recent, state-of-the-art approaches for addressing the IfO problem might also be effective for grounding the simulator leading to improved sim-to-real transfer. Specifically, we derive a distribution-matching objective similar to ones used in adversarial approaches for generative modeling [13], imitation learning [17], and IfO [43] with considerable empirical success. Based on this objective, we propose a novel algorithm, generative adversarial reinforced action transformation (GARAT), to ground the simulator by reducing the distribution mismatch between the simulator and the real world.</p>
<p>Our experiments confirm our hypothesis by showing that GARAT reduces the difference in the dynamics between two environments more effectively than GAT. Moreover, our experiments show that, in several domains, this improved grounding translates to better transfer of policies from one environment to the other.</p>
<p>In summary, our contributions are as follows: (1) we show that grounded action transformation can be seen as an IfO problem, (2) we derive a novel adversarial imitation learning algorithm, GARAT, to learn an action transformation policy for sim-to-real transfer, and (3) we experimentally evaluate the efficacy of GARAT for sim-to-real transfer.</p>
<p>Background</p>
<p>We begin by introducing notation, reviewing the sim-to-real-problem formulation, and describing the action transformation approach for sim-to-real transfer. We also provide a brief overview of imitation learning and imitation from observation.</p>
<p>Notation</p>
<p>We consider here sequential decision processes formulated as Markov decision processes (MDPs) [36]. An MDP M is a tuple S, A, R, P, γ, ρ 0 consisting of a set of states, S; a set of actions, A; a reward function, R : S × A × S −→ ∆([r min , r max ]) (where ∆([r min , r max ]) denotes a distribution over the interval [r min , r max ] ⊂ R); a discount factor, γ ∈ [0, 1); a transition function, P : S×A −→ ∆(S); and an initial state distribution, ρ 0 : ∆(S). An RL agent uses a policy π : S −→ ∆(A) to select actions in the environment. In an environment with transition function P ∈ T, the agent aims to learn a policy π ∈ Π to maximize its expected discounted return E π,
P [G 0 ] = E π,P [ ∞ t=0 γ t R t ], where R t ∼ R(s t , a t , s t+1 ), s t+1 ∼ P (s t , a t ), a t ∼ π(s t ), and s 0 ∼ ρ 0 .
Given a fixed π and a specific transition function P q , the marginal transition distribution is ρ q (s, a, s ) : =(1 − γ)π(a|s)P q (s |s, a) ∞ t=0 γ t p(s t = s|π, P q ) where p(s t = s|π, P q ) is the probability of being in state s at time t. The marginal transition distribution is the probability of being in state s marginalized over time t, taking action a under policy π, and ending up in state s under transition function P q (laid out more explicitly in Appendix A). We can denote the expected return under a policy π and a transition function P q in terms of this marginal distribution as:
E π,q [G 0 ] = 1 (1 − γ) s,a,s ρ q (s, a, s )R(s |s, a)(1)</p>
<p>Sim-to-real Transfer and Grounded Action Transformation</p>
<p>Let P sim , P real ∈ T be the transition functions for two otherwise identical MDPs, M sim and M real , representing the simulator and real world respectively. Sim-to-real transfer aims to train an agent policy to maximize return in M real with limited trajectories from M real , and as many as needed in M sim .</p>
<p>The work presented here is specifically concerned with a particular class of sim-to-real approaches known as simulator grounding approaches [1,7,9]. These approaches modify the simulator dynamics by using real-world interactions to ground them to be closer to the dynamics of the real world.</p>
<p>Because it may sometimes be difficult or impossible to modify the simulator itself, the recentlyproposed grounded action transformation (GAT) approach [14] seeks to instead induce grounding by modifying the agent's actions before using them in the simulator. This modification is accomplished via an action transformation function π g : S × A −→ ∆(A) that takes as input the state and action of the agent, and produces an action to be presented to the simulator. From the agent's perspective, composing the action transformation with the simulator changes the simulator's transition function. We call this modified simulator the grounded simulator, and its transition function is given by
P g (s |s, a) = ã∈A P sim (s |s,ã)π g (ã|s, a)(2)
The action transformation approach aims to learn function π g ∈ Π g such that the resulting transition function P g is as close as possible to P real . We denote the marginal transition distributions in sim and real by ρ sim and ρ real respectively, and ρ g ∈ P g for the grounded simulator.</p>
<p>GAT learns a model of the real world dynamicsP real (s |s, a), an inverse model of the simulator dynamicsP −1 sim (a|s, s ), and uses the composition of the two as the action transformation function, i.e. π g (ã|s, a) =P −1 sim (ã|s,P real (s |s, a)).</p>
<p>Imitation Learning</p>
<p>In parallel to advances in sim-to-real transfer, the machine learning community has also made considerable progress on the problem of imitation learning. Imitation learning [5,31,33] is the problem setting where an agent tries to mimic trajectories {ξ 0 , ξ 1 , . . .} where each ξ is a demonstrated trajectory {(s 0 , a 0 ), (s 1 , a 1 ), . . .} induced by an expert policy π exp .</p>
<p>Various methods have been proposed to address the imitation learning problem. Behavioral cloning [4] uses the expert's trajectories as labeled data and uses supervised learning to recover the maximum likelihood policy. Another approach instead relies on reinforcement learning to learn the policy, where the required reward function is recovered using inverse reinforcement learning (IRL) [23]. IRL aims to recover a reward function under which the demonstrated trajectories would be optimal.</p>
<p>A related setting to learning from state-action demonstrations is the imitation from observation (IfO) [21,25,42,43] problem. Here, an agent observes an expert's state-only trajectories {ζ 0 , ζ 1 , . . .} where each ζ is a sequence of states {s 0 , s 1 , . . .}. The agent must then learn a policy π(a|s) to imitate the expert's behavior, without being given labels of which actions to take.</p>
<p>GAT as Imitation from Observation</p>
<p>We now show that the underlying problem of GAT-i.e., learning an action transformation for sim-toreal transfer-can also been seen as an IfO problem. Adapting the definition by Liu et al. [21], an IfO problem is a sequential decision-making problem where the policy imitates state-only trajectories {ζ 0 , ζ 1 , . . .} produced by a Markov process, with no information about what actions generated those trajectories. To show that the action transformation learning problem fits this definition, we must show that it (1) is a sequential decision-making problem and (2) aims to imitate state-only trajectories produced by a Markov process, with no information about what actions generated those trajectories.</p>
<p>Starting with (1), it is sufficient to show that the action transformation function is a policy in an MDP [29]. This action transformation MDP can be seen clearly if we combine the target task MDP and the fixed agent policy π. Let the joint state and action space X := S × A with x : =(s, a) ∈ X be the state space of this new MDP. The combined transition function is P x sim (x |x,ã) = P sim (s |s,ã)π(a |s ), where x = (s , a ), and initial state distribution is ρ x 0 (x) = ρ 0 (s)π(a|s). For completeness, we consider a reward function R x : X×A×X −→ ∆([r min , r max ]) and discount factor γ x ∈ [0, 1), which are not essential for an IfO problem. With these components, the action transformation environment is an MDP X, A, R x , P x sim , γ x , ρ x 0 . The action transformation function π g (ã|s, a), now π x g (ã|x), is then clearly a mapping from states to a distribution over actions, i.e. it is a policy in an MDP. Thus, the action transformation learning problem is a sequential decision-making problem.</p>
<p>We now consider the action transformation objective to show (2). When learning the action transformation policy, we have trajectories {τ 0 , τ 1 , . . .}, where each trajectory τ = {(s 0 , a 0 ∼ π(s 0 )), (s 1 , a 1 ∼ π(s 1 )), . . .} is obtained by sampling actions from agent policy π in the real world. Re-writing τ in the above MDP, τ = {x 0 , x 1 , . . .}. If an expert action transformation policy π * g ∈ Π g is capable of mimicking the dynamics of the real world, P x real (x |x) = ã∈A P x sim (x |x,ã)π * g (ã|x), then we can consider the above trajectories to be produced by a Markov process with dynamics P x sim (x |x,ã) and policy π * g (ã|x). The action transformation aims to imitate the state-only trajectories {τ 0 , τ 1 , . . .} produced by a Markov process, with no information about what actions generated those trajectories.</p>
<p>The problem of learning the action transformation thus satisfies the conditions we identified above, and so it is an IfO problem.</p>
<p>Generative Adversarial Reinforced Action Transformation</p>
<p>The insight above naturally leads to the following question: if learning an action transformation for sim-to-real transfer is equivalent to IfO, might recently-proposed IfO approaches lead to better sim-to-real approaches? To investigate the answer, we derive a novel generative adversarial approach inspired by GAIfO[43] that can be used to train the action transformation policy using IfO. A simulator grounded with this action transformation policy can then be used to train an agent policy which can be expected to transfer effectively to the real world. We call our approach generative adversarial reinforced action transformation (GARAT), and Algorithm 1 lays out its details.</p>
<p>The rest of this section details our derivation of the objective used in GARAT. First, in Section 4.1, we formulate a procedure for action transformation using a computationally expensive IRL step to extract a reward function and then learning an action transformation policy based on that reward. Then, in Section 4.2, we show that this entire procedure is equivalent to directly reducing the marginal transition distribution discrepancy between the real world and the grounded simulator. This is important, as recent work [13,17,43] has shown that adversarial approaches are a promising algorithmic paradigm to reduce such discrepancies. Thus, in Section 4.3, we explicitly formulate a generative adversarial objective upon which we build the proposed approach.</p>
<p>Action Transformation Inverse Reinforcement Learning</p>
<p>We first lay out a procedure to learn the action transformation policy by extracting the appropriate cost function, which we term action transformation IRL (ATIRL). We use the cost function formulation in our derivation, similar to previous work [17,43]. ATIRL aims to identify a cost function such that the Algorithm 1 GARAT Input: Real world with P real , simulator with P sim , number of update steps N Agent policy π with parameters η , pretrained in simulator;</p>
<p>Initialize action transformation policy π g with parameters θ Initialize discriminator D φ with parameters φ while performance of policy π in real world not satisfactory do Rollout policy π in real world to obtain trajectories {τ real,1 , τ real,2 , . . .} for i = 0, 1, 2, . . . N do Rollout Policy π in grounded simulator and obtain trajectories {τ gsim,1 , τ gsim,2 , . . .} Update parameters φ of D φ using gradient descent to minimize
− E τgsim [log(D φ (s, a, s ))] + E τ real [log(1 − D φ (s, a, s ))
Update parameters θ of π g using policy gradient with reward −[log D φ (s, a, s )] end Optimize parameters η of π in simulator grounded with action transformer π g end observed real world transitions yield higher return than any other possible transitions. We consider the set of cost functions C as all functions
R S×A×S = {c : S × A × S −→ R}. ATIRL ψ (P real ) : = argmax c∈C −ψ(c) + min πg∈Πg E ρg [c(s, a, s )] − E ρ real <a href="3">c(s, a, s )</a>
where ψ : R S×A×S −→ R is a (closed, proper) convex reward function regularizer, and R denotes the extended real numbers R {∞}. This regularizer is used to avoid overfitting the expressive set C.</p>
<p>Note that π g influences ρ g (Equation 10 in Appendix A) and P real influences ρ real . Similar to GAIfO, we do not use causal entropy in our ATIRL objective due to the surjective mapping from Π g to P g .</p>
<p>The action transformation then uses this per-step cost function as a reward function in an RL procedure: RL(c) : = argmin πg∈Πg E ρg [c(s, a, s )]. We assume here for simplicity that there is an action transformation policy that can mimic the real world dynamics perfectly. That is, there exists a policy π g ∈ Π g , such that P g (s |s, a) = P real (s |s, a)∀s ∈ S, a ∈ A. We denote the RL procedure applied to the cost function recovered by ATIRL as RL • ATIRL ψ (P real ).</p>
<p>Characterizing the Policy Induced by ATIRL</p>
<p>This section shows that it is possible to bypass the ATIRL step and learn the action transformation policy directly from data. We show that ψ-regularized RL • ATIRL ψ (P real ) implicitly searches for policies that have a marginal transition distribution close to the real world's, as measured by the convex conjugate of ψ, which we denote as ψ * . As a practical consequence, we will then be able to devise a method for minimizing this divergence through the use of generative adversarial techniques in Section 4.3. But first, we state our main theoretical claim: Theorem 1. RL • ATIRL ψ (P real ) and argmin πg ψ * (ρ g − ρ real ) induce policies that have the same marginal transition distribution, ρ g .</p>
<p>To reiterate, the agent policy π is fixed. So the only decisions affecting the marginal transition distributions are of the action transformation policy π g . We can now state the following proposition: Proposition 4.1. For a given ρ g generated by a fixed policy π, P g is the only transition function whose marginal transition distribution is ρ g .</p>
<p>Proof in Appendix B.1. We can also show that if two transition functions are equal, then the optimal policy in one will be optimal in the other.
Proposition 4.2. If P real = P g , then argmax π∈Π E π,Pg [G 0 ] = argmax π∈Π E π,P real [G 0 ].
Proof in Appendix B.2. We now prove Theorem 1, which characterizes the policy learned by RL(c) on the cost functionc recovered by ATIRL ψ (P real ).</p>
<p>Proof of Theorem 1. To prove Theorem 1, we prove that RL • ATIRL ψ (P real ) and argmin πg ψ * (ρ g − ρ real ) result in the same marginal transition distribution. This proof has three parts, two of which are proving that both objectives above can be formulated as optimizing over marginal transition distributions. The third is to show that these equivalent objectives result in the same distribution.</p>
<p>The output of both RL • ATIRL ψ (P real ) and argmin πg ψ * (ρ g − ρ real ) are policies. To compare the marginal distributions, we first establish a different RL • ATIRL ψ (P real ) objective that we argue has the same marginal transition distribution as RL • ATIRL ψ (P real ). We define
ATIRL ψ (P real ) : = argmax c∈C −ψ(c) + min ρg∈Pg E ρg [c(s, a, s )] − E ρ real <a href="4">c(s, a, s )</a>
with the same ψ and C as Equation 3, and similar except the internal optimization for Equation 3 is over π g ∈ Π g , while it is over ρ g ∈ P g for Equation 4. We define an RL procedure RL(c) : = argmin ρg∈Pg E ρg c(s, a, s ) that returns a marginal transition distribution ρ g ∈ P g which minimizes the given cost function c. RL(c) will output the marginal transition distribution ρ g .</p>
<p>Lemma 4.1. RL • ATIRL ψ (P real ) outputs a marginal transition distribution ρ g which is equal toρ g induced by RL • ATIRL ψ (P real ).</p>
<p>Proof in Appendix B.3. The mapping from Π g to P g is not injective, and there could be multiple policies π g that lead to the same marginal transition distribution. The above lemma is sufficient for proof of Theorem 1, however, since we focus on the effect of the policy on the transitions.
Lemma 4.2. RL • ATIRL ψ (P real ) = argmin ρg∈Pg ψ * (ρ g − ρ real ).
The proof in Appendix B.4 relies on the optimal cost function and the optimal policy forming a saddle point, ψ * leading to a minimax objective, and these objectives being the same.
Lemma 4.3. The marginal transition distribution of argmin πg ψ * (ρ g − ρ real ) is equal to argmin ρg∈Pg ψ * (ρ g − ρ real ).
Proof in appendix B.5. With these three lemmas, we have proved that RL • ATIRL ψ (P real ) and argmin πg ψ * (ρ g − ρ real ) induce policies that have the same marginal transition distribution.</p>
<p>Theorem 1 thus tells us that the objective argmin πg ψ * (ρ g − ρ real ) is equivalent to the procedure from Section 4.1. In the next section, we choose a function ψ which leads to our adversarial objective.</p>
<p>Forming the Adversarial Objective</p>
<p>Section 4.2 laid out the objective we want to minimize. To solve argmin πg ψ * (ρ g − ρ real ) we require an appropriate regularizer ψ. GAIL [17] and GAIfO [43] optimize similar objectives and have shown a regularizer similar to the following to work well:
ψ(c) = E real [g(c(s, a, s ))] if c &lt; 0 +∞ otherwise where g(x) = −x − log(1 − e x ) if x &lt; 0 +∞ otherwise(5)
It is closed, proper, convex and has a convex conjugate leading to the following minimax objective:
min πg∈Πg ψ * (ρ g − ρ real ) = min πg∈Πg max D E Pg [log(D(s, a, s ))] + E P real [log(1 − D(s, a, s ))] (6)
where the reward for the action transformer policy π g is −[log(D(s, a, s ))], and D : S × A × S −→ (0, 1) is a discriminative classifier. These properties have been shown in previous works [17,43]. Algorithm 1 lays out the steps for learning the action transformer using the above procedure, which we call generative adversarial reinforced action transformation (GARAT).</p>
<p>Related Work</p>
<p>In this section, we discuss the variety of sim-to-real methods, work more closely related to GARAT, and some related methods in the IfO literature. Sim-to-real transfer can be improved by making the agent's policy more robust to variations in the environment or by making the simulator more accurate w.r.t. the real world. The first approach, which we call policy robustness methods, encompasses algorithms that train a robust policy that performs well on a range of environments [19,26,27,28,30,32,39,40]. Robust adversarial reinforcement learning (RARL) [28] is such an algorithm that learns a policy robust to adversarial perturbations [37]. While primarily focused on training with a modifiable simulator, a version of RARL treats the simulator as a black-box by adding the adversarial perturbation directly to the protagonist's action. Additive noise envelope (ANE) [20] is another black-box robustness method which adds an envelope of Gaussian noise to the agent's action during training.</p>
<p>The second approach, known as domain adaption or system identification, grounds the simulator using real world data to make its transitions more realistic. Since hand engineering accurate simulators [38,46] can be expensive and time consuming, real world data can be used to adapt low-fidelity simulators to the task at hand. Most simulator adaptation methods [1,7,9,18] rely on access to a parameterized simulator.</p>
<p>GARAT, on the other hand, does not require a modifiable simulator and relies on an action transformation policy applied in the simulator to bring its transitions closer to the real world. GAT [14] learns an action transformation function similar to GARAT. It was shown to have successfully learned and transferred one of the fastest known walk policies on the humanoid robot, Nao.</p>
<p>(a) L2 norm of per step transition errors (lower is better) between different simulator environments and the target environment, shown over number of action transformation policy updates for GARAT.</p>
<p>(b) Example trajectories of the same agent policy deployed in different environments, plotted using the pendulum angle across time. Response of GARAT grounded simulator is the most like "real" environment. Figure 1: Evaluation of simulator grounding with GARAT in InvertedPendulum domain GARAT draws from recent generative adversarial approaches to imitation learning (GAIL [17]) and IfO (GAIfO [43]). AIRL [10], FAIRL [12], and WAIL [45] are related approaches which use different divergence metrics to reduce the marginal distribution mismatch. GARAT can be adapted to use any of these metrics, as we show in the appendix.</p>
<p>One of the insights of this paper is that grounding the simulator using action transformation can be seen as a form of IfO. BCO [42] is an IfO technique that utilizes behavioral cloning. I2L [11] is an IfO algorithm that aims to learn in the presence of transition dynamics mismatch in the expert and agent's domains, but requires millions of real world interactions to be competent.</p>
<p>Experiments</p>
<p>In this section, we conduct experiments to verify our hypothesis that GARAT leads to improved sim-to-real transfer compared to previous methods. We also show that it leads to better simulator grounding compared to the previous action transformation approach, GAT.</p>
<p>We validate GARAT for sim-to-real transfer by transferring the agent policy between Open AI Gym [6] simulated environments with different transition dynamics. We highlight the Minitaur domain ( Figure 2) as a particularly useful test since there exist two simulators, one of which has been carefully engineered for high fidelity to the real robot [38]. For other environments, the "real" environment is the simulator modified in different ways such that a policy trained in the simulator does not transfer well to the "real" environment. Details of these modifications are provided in Appendix C.1. Apart from a thorough evaluation across multiple different domains, this sim-to-"real" setup also allows us to compare GARAT and other algorithms against a policy trained directly in the target domain with millions of interactions, which is otherwise prohibitively expensive on a real robot. This setup also allows us to perform a thorough evaluation of sim-to-real algorithms across multiple different domains. Throughout this section, we refer to the target environment as the "real" environment and the source environment as the simulator. We focus here on answering the following questions :</p>
<ol>
<li>How well does GARAT ground the simulator to the "real" environment? 2. Does GARAT lead to improved sim-to-"real" transfer, compared to other related methods?</li>
</ol>
<p>Simulator Grounding</p>
<p>In Figure 1, we evaluate how well GARAT grounds the simulator to the "real" environment both quantitatively and qualitatively. This evaluation is in the InvertedPendulum domain, where the "real" environment has a heavier pendulum than the simulator; implementation details are in Appendix C.1.</p>
<p>In Figure 1a, we plot the average error in transitions in simulators grounded with GARAT and GAT with different amounts of "real" data, collected by deploying π in the "real" environment. In Figure  1b we deploy the same policy π from the same start state in the different environments (simulator, "real" environment, and grounded simulators). From both these figures it is evident that GARAT leads to a grounded simulator with lower error on average, and responses qualitatively closer to the "real" environment compared to GAT. Details of how we obtained these plots are in Appendix C.2. Figure 3: Performance of different techniques evaluated in "real" environment. Environment return on the y-axis is scaled such that π real achieves 1 and π sim achieves 0. We now validate the effectiveness of GARAT at transferring a policy from sim to "real". For various MuJoCo [41] environments, we pretrain the agent policy π in the ungrounded simulator, collect real world data with π, use GARAT to ground the simulator, re-train the agent policy until convergence in these grounded simulators, and then evaluate mean return across 50 episodes for the updated agent policy in the "real" environment.</p>
<p>Sim-to-"Real" Transfer</p>
<p>The agent policy π and action transformation policy π g are trained with TRPO [34] and PPO [35] respectively. The specific hyperparameters used are provided in Appendix C. We use the implementations of TRPO and PPO provided in the stable-baselines library [16]. For every π g update, we update the GARAT discriminator D φ once as well. Results here use the losses detailed in Algorithm 1. However, we find that GARAT is just as effective with other divergence measures [10,12,45] (Appendix C).</p>
<p>GARAT is compared to GAT [14], RARL [28] adapted for a black-box simulator, and action-noiseenvelope (ANE) [20]. π real and π sim denote policies trained in the "real" environment and simulator respectively until convergence. We use the best performing hyperparameters for these methods, specified in Appendix C. Figure 3 shows that, in most of the domains, GARAT with just a few thousand transitions from the "real" environment facilitates transfer of policies that perform on par with policies trained directly in the "real" environment using 1 million transitions. GARAT also consistently performs better than previous methods on all domains, except HopperHighFriction, where most of the methods perform well. The shaded envelope denotes the standard error across 5 experiments with different random seeds for all the methods. Apart from the MuJoCo simulator, we also show successful transfer in the PyBullet simulator [8] using the Ant domain. Here the "real" environment has gravity twice that of the simulator, resulting in purely simulator-trained policies collapsing ineffectually in the "real" environment. In this relatively high dimensional domain, as well as in Walker, we see GARAT still transfers a competent policy while the related methods fail.</p>
<p>In the Minitaur domain [38] we use the high fidelity simulator as our "real" environment. Here as well, a policy trained in simulation does not directly transfer well to the "real" environment [47]. We see in this realistic setting that GARAT learns a policy that obtains more than 80% of the optimal "real" environment performance with just 1000 "real" environment transitions while the next best baseline (GAT) obtains at most 50%, requiring ten times more "real" environment data.</p>
<p>Conclusion</p>
<p>In this paper, we have shown that grounded action transformation, a particular kind of grounded sim-to-real transfer technique, can be seen as a form of imitation from observation. We use this insight to develop GARAT, an adversarial imitation from observation algorithm for grounded sim-to-real transfer. We hypothesized that such an algorithm would lead to improved grounding of the simulator as well as better sim-to-real transfer compared to related techniques. This hypothesis is validated in Section 6 where we show that GARAT leads to better grounding of the simulator as compared to GAT, and improved transfer to the "real" environment on various mismatched environment transfers, including the realistic Minitaur domain.</p>
<p>Broader Impact</p>
<p>Reinforcement learning [36] is being considered as an effective tool to train autonomous agents in various important domains like robotics, medicine, etc. A major hurdle to deploying learning agents in these environments is the massive exploration and data requirements [15] to ensure that these agents learn effective policies. Real world interactions and exploration in these situations could be extremely expensive (wear and tear on expensive robots), or dangerous (treating a patient in the medical domain).</p>
<p>Sim-to-real transfer aims to address this hurdle and enables agents to be trained mostly in simulation and then transferred to the real world based on very few interactions. Reducing the requirement for real world data for autonomous agents might open up the viability for autonomous agents in other fields as well.</p>
<p>Improved sim-to-real transfer will also reduce the pressure for high fidelity simulators, which require significant engineering effort [7,38]. Simulators are also developed with a task in mind, and are generally not reliable outside their specifications. Sim-to-real transfer might enable simulators that learn to adapt to the task that needs to be performed, a potential direction for future research.</p>
<p>Sim-to-real research needs to be handled carefully, however. Grounded simulators might lead to a false sense of confidence in a policy trained in such a simulator. However, a simulator grounded with real world data will still perform poorly in situations outside the data distribution. As has been noted in the broader field of machine learning [3], out of training distribution situations might lead to unexpected consequences. Simulator grounding must be done carefully in order to guarantee that the grounding is applied over all relevant parts of the environment.</p>
<p>Improved sim-to-real transfer could increase reliance on compute and reduce incentives for sample efficient methods. The field should be careful in not abandoning this thread of research as the increasing cost and impact of computation used by machine learning becomes more apparent [2].</p>
<p>A Marginal Distributions and Returns</p>
<p>We expand the marginal transition distribution (ρ sim ) definition to be more explicit below.</p>
<p>ρ sim,t (s, a, s ) : = ρ sim,t (s)π(a|s)P sim (s |s, a)</p>
<p>ρ sim,t (s ) : = s∈S a∈A ρ sim,t−1 (s, a, s )
ρ sim (s, a, s ) : =(1 − γ) ∞ t=0 γ t ρ sim,t (s, a, s )(8)
where ρ sim,0 (s) = ρ 0 (s) is the starting state distribution. Written in a single equation:
ρ sim (s, a, s ) = (1 − γ) s0∈S ρ 0 (s 0 ) ∞ t=0 γ t at∈A st+1∈S
π(a t |s t )P (s t+1 |s t , a t )</p>
<p>The expected return can be written more explicitly to show the dependence on the transition function. It then makes the connection to 1 more explicit.
E π,P [G 0 ] = E π,P ∞ t=0 γ t R(s t , a t , s t+1 ) = s0∈S ρ 0 (s 0 ) ∞ t=0 γ t at∈A st+1∈S
π(a t |s t )P (s t+1 |s t , a t )R(s t , a t , s t+1 )</p>
<p>In the grounded simulator, the action transformer policy π g transforms the transition function as specified in Section 2.2. Ideally, such a π g ∈ Π g exists. We denote the marginal transition distributions in sim and real by ρ sim and ρ real respectively, and ρ g ∈ P g for the grounded simulator.</p>
<p>The distribution ρ g relies on π g ∈ Π g as follows:</p>
<p>ρ g (s, a, s ) = (1 − γ)π(a|s) ã∈A P sim (s |s,ã)π g (ã|s, a) ∞ t=0 γ t p(s t = s|π, P g )</p>
<p>The marginal transition distribution of the simulator after action transformation, ρ g (s, a, s ), differs in Equation 7 as follows:</p>
<p>ρ g,t (s, a, s ) : = ρ g,t (s)π(a|s) ã∈A π g (ã|s, a)P g (s |s,ã)</p>
<p>B Proofs B.1 Proof of Proposition 4.1 Proposition 4.1. For a given ρ g generated by a fixed policy π, P g is the only transition function whose marginal transition distribution is ρ g .</p>
<p>Proof. We prove the above statement by contradiction. Consider two transition functions P 1 and P 2 that have the same marginal distribution ρ π under the same policy π, but differ in their likelihood for at least one transition (s, a, s ). P 1 (s |s, a) = P 2 (s |s, a)</p>
<p>Let us denote the marginal distributions for P 1 and P 2 under policy π as ρ π 1 and ρ π 2 . Thus, ρ π 1 (s) = ρ π 2 (s) ∀s ∈ S and ρ π 1 (s, a, s ) = ρ π 2 (s, a, s )∀s, s ∈ S, a ∈ A.</p>
<p>The marginal likelihood of the above transition for both P 1 and P 2 is:
ρ π 1 (s, a, s ) = T −1 t=0
ρ π 1 (s)π(a|s)P 1 (s |s, a) ρ π 2 (s, a, s ) = T −1 t=0 ρ π 2 (s)π(a|s)P 2 (s |s, a)</p>
<p>Since the marginal distributions match, and the policy is the same, this leads to the equality: 
P 1 (s |s, a) = P 2 (s |s, a)∀s, s ∈ S, a ∈ A(13)E π,Pg [G 0 ] = argmax π∈Π E π,P real [G 0 ].
Proof. We overload the notation slightly and refer to ρ π real as the marginal transition distribution in the real world while following agent policy π. Proposition 4.1 still holds under this expanded notation.</p>
<p>From Proposition 4.1, if P real = P g , we can say that ρ π real = ρ π g ∀π ∈ Π. From Equation 1, Proof. For every ρ g ∈ P g , there exists at least one action transformer policy π g ∈ Π g , from our definition of P g . Let RL • ATIRL ψ (P real ) lead to a policyπ g , with a marginal transition distributioñ ρ g . The marginal transition distribution induced by RL • ATIRL ψ (P real ) is ρ g .
E π,g [G 0 ] = E π,real [G 0 ]∀π ∈ Π, and argmax π∈Π E π,g [G 0 ] = argmax π∈Π E π,real [G 0 ].
We need to prove thatρ g = ρ g , and we do so by contradiction. We assume thatρ g = ρ g . For this inequality to be true, the marginal transition distribution of the result of RL(c) must be different than the result of RL(c), or the cost functionsc and c must be different. which leads to a contradiction. Now let's consider the cost functions presented by ATIRL ψ (P real ) and ATIRL ψ (P real ). Since RL(c) and RL(c) lead to the same marginal transition distributions, for the inequality we assumed at the beginning of this proof to be true, ATIRL ψ (P real ) and ATIRL ψ (P real ) must return different cost functions. which leads to another contradiction. Therefore, we can say that ρ g = ρg.</p>
<p>B.4 Proof of Lemma 4.2</p>
<p>We prove convexity under a particular agent policy π but across AT policies π g ∈ Π g Lemma B.1. P g is compact and convex.</p>
<p>Proof. We first prove convexity of ρ Πg,t for π g ∈ Π g and 0 ≤ t &lt; ∞, by means of induction.</p>
<p>Base case: λρ at1,0 + (1 − λ)ρ at2,0 ∈ ρ Πg,0 , for 0 ≤ λ ≤ 1.</p>
<p>λρ at1,0 (s, a, s ) + (1 − λ)ρ at2,0 (s, a, s ) = λρ 0 (s)π(a|s) ã∈A π at1 (ã|s, a)P sim (s |s,ã) + (1 − λ)ρ 0 (s)π(a|s) ã∈A π at2 (ã|s, a)P sim (s |s,ã)</p>
<p>= ρ 0 (s)π(a|s) ã∈A (λπ at1 (ã|s, a) + (1 − λπ at2 (ã|s, a))) P sim (s |s,ã) Π g is convex and hence ρ 0 (s)π(a|s) ã∈A (λπ at1 (ã|s, a) + (1 − λπ at2 (ã|s, a))) P sim (s |s,ã) is a valid distribution, meaning ρ Πg,0 is convex.</p>
<p>Induction</p>
<p>Step: If ρ Πg,t−1 is convex, ρ Πg,t is convex.</p>
<p>If ρ Πg,t−1 is convex, λρ at1,t (s) + (1 − λ)ρ at2,t (s) is a valid distribution. This is true simply by summing the distribution at time t − 1 over states and actions.</p>
<p>λρ at1,t (s, a, s ) + (1 − λ)ρ at2,t (s, a, s ) = λρ at1,t (s)π(a|s) ã∈A π at1 (ã|s, a)P sim (s |s,ã)
+ (1 − λ)ρ at2,t (s)π(a|s)
ã∈A π at2 (ã|s, a)P sim (s |s,ã) = (λρ at1,t (s) + (1 − λ)ρ at2,t (s)) π(a|s) ã∈A (λπ at1 (ã|s, a) + (1 − λπ at2 (ã|s, a))) P sim (s |s,ã) λρ π at1,t (s) + (1 − λ)ρ π at1,t (s) is a valid distribution, and Π g is convex. This proves that the transition distribution at each time step is convex. The normalized discounted sum of convex sets (Equation 9) is also convex. Since the exponential discounting factor γ ∈ [0, 1), the sum is bounded as well.</p>
<p>We now prove Lemma 4.2. Lemma 4.2. RL • ATIRL ψ (P real ) = argmin ρg∈Pg ψ * (ρ g − ρ real ).</p>
<p>Proof of Lemma 4.2. Let c = ATIRL(P real ), ρ g = RL(c) = RL • ATIRL(P real ) and
ρ g = argmin ρg ψ * (ρ g − ρ real ) = argmin ρg max c −ψ(c) + s,a,s (ρ g (s, a, s ) − ρ real (s, a, s ))c(s, a, s )(14)
where ψ * : C * −→R is the convex conjugate of ψ, defined as ψ * (c * ) : = sup c∈C c * , c − ψ(c). Applying the above definition to the rightmost term in the above equation gives us the middle term.</p>
<p>We now argue that ρ g =ρ g which are the two sides of the equation we want to prove. Let us consider loss function L : P g × R S×A×S −→ R to be
L(ρ g , c) = −ψ(c) + s,a,s (ρ g (s, a, s ) − ρ real (s, a, s ))c(s, a, s )(15)
We can then pose the above formulations as:
ρ g ∈ argmin ρg∈Pg max c L(ρ g , c) (16) c ∈ argmax c min ρg∈Pg L(ρ g , c) (17) ρ g ∈ argmin ρg∈Pg L(ρ g , c)(18)
P g is compact and convex (by Lemma B.1) and R S×A×S is convex. L(·, c) is convex over all c and L(ρ g , ·) is concave over all ρ g . Therefore, based on minimax duality: </p>
<p>From Equations 16 and 17, (ρ g , c) is a saddle point of L, implyingρ g = argmin ρg∈Pg L(ρ g , c) and so ρ g =ρ g .</p>
<p>B.5 Proof of Lemma 4.3 Lemma 4.3. The marginal transition distribution of argmin πg ψ * (ρ g − ρ real ) is equal to argmin ρg∈Pg ψ * (ρ g − ρ real ).</p>
<p>Proof. The proof of equivalence here is simply to prove that optimizing over π g is the same as optimizing over ρ g . From Equation 10 and from the fact that agent policy π and simulator transition function P sim are fixed, we can say that the only way to optimize ρ g is to optimize π g , which leads to the above equivalence.</p>
<p>C Experimental Details</p>
<p>To collect expert trajectories from the real world, we rollout the stochastic initial policy trained in sim for 1 million timesteps, on the real world. This dataset serves as the expert dataset during the imitation learning step of GARAT. At each GAN iteration, we sample a batch of data from the grounded simulator and expert dataset and update the discriminator. Similarly, we rollout the action transformer policy in its environment and update π g . We perform 50 such GAN updates to ground   Table 2. The hyperparameters used for the TRPO algorithm to update the agent policy can be found in Table 1.</p>
<p>We implemented different IfO algorithms and noticed that there was no significant difference between these backend algorithms in sim-to-real performance. During the discriminator update step in GAIfO-reverseKL (AIRL), GAIfO and GAIfO-W (WAIL), we use two regularizers in its loss function -L2 regularization of the discriminator's weights and a gradient penalty (GP) term, with a coefficient of 10. Adding the GP term has been shown to be helpful in stabilizing GAN training [22].</p>
<p>In our implementation of the AIRL [10] algorithm, we do not use the special form of the discriminator, described in the paper, because our goal is to simply imitate the expert and does not require recovering the reward function as was the objective of that work. We instead use the approach Ghasemipour et al. [12] use with state-only version of AIRL.</p>
<p>GAT uses a smoothing parameter α, which we set to 0.95 as suggested by Hanna and Stone [14]. RARL has a hyperparameter on the maximum action ratio allowed to the adversary, which measures how much the adversary can disrupt the agent's actions. This hyperparameter is chosen by a coarse grid-search. For each domain, we choose the best result and report the average return over five policies trained with those hyperparameters. We used the official implementation of RARL provided by the authors for the MuJoCo environments. However, since their official code does not readily support PyBullet environments, for the Ant and Minitaur domain, we use our own implementation of RARL, which we reimplemented to the best of our ability. When training a robust policy using Action space Noise Envelope (ANE), we do not know the right amount of noise to inject into the agent's actions. Hence, in our analysis, we perform a sweep across zero mean gaussian noise with multiple standard deviation values and report the highest return achieved in the target domain with the best hyperparameter, averaged across 5 different random seeds.  [38] Torque vs. Current linear non-linear Table 3: Details of the Modified Sim-to-"Real" environments for benchmarking GARAT against other black-box Sim-to-Real algorithms.</p>
<p>C.1 Modified environments</p>
<p>We evaluate GARAT against several algorithms in the domains shown in Figure 3. Table 3 shows the source domain along with the specific properties of the environment/agent modified. We modified the values such that a policy trained in the sim environment is unable to achieve similar returns in the modified environment. By modifying an environment, we incur the risk that the environment may become too hard for the agent to solve. We ensure this is not the case by training a policy π real directly in the "real" environment and verifying that it solves the task.</p>
<p>C.2 Simulator Grounding Experimental Details</p>
<p>In Section 6.1, we show results which validate our hypothesis that GARAT learns an action transformation policy which grounds the simulator better than GAT. Here we detail our experiments for Figure 1.</p>
<p>In Figure 1a, we plot the average error in transitions in simulators grounded with GARAT and GAT with different amounts of "real" data, collected by deploying π in the "real" environment. The per step transition error is calculated by resetting the simulator state to states seen in the "real" environment, taking the same action, and then measuring the error in the L2-norm with respect to "real" environment transitions. Figure 1a shows that with a single trajectory from the "real" environment, GARAT learns an action transformation that has similar average error in transitions compared to GAT with 100 trajectories of "real" environment data to learn from.</p>
<p>In Figure 1b, we compare GARAT and GAT more qualitatively. We deploy the agent policy π from the same start state in the "real" environment, the simulator, GAT-grounded simulator, and GARATgrounded simulator. Their resultant trajectories in one of the domain features (angular position of the pendulum) is plotted in Figure 1b. The trajectories in GARAT-grounded simulator keeps close to the Figure 4: Policies trained in "real" environment, GAT-grounded simulator, and GARAT-grounded simulator deployed in the "real" environment from the same starting state</p>
<p>Figure 2 :
2The Minitaur Domain</p>
<p>.
RL • ATIRL ψ (P real ) outputs a marginal transition distribution ρ g which is equal toρ g induced by RL • ATIRL ψ (P real ).</p>
<p>E
Let us compare the RL procedures first. Assume thatc = c. ρg [c(s, a, s )] ...(surjective mapping) = RL(c)(c = c)</p>
<p>(s, a, s )c(s, a, s ) = ATIRL ψ (P real )</p>
<p>Table 1: Hyperparameters for the TRPO algorithm used to update the Agent PolicyName </p>
<p>Value </p>
<p>Hidden Layers 
2 
Hidden layer size 
64 
timesteps per batch 
5000 
max KL constraint 
0.01 
λ 
0.97 
γ 
0.995 
learning rate 
0.0004 
cg damping 
0.1 
cg iters 
20 
value function step size 0.001 
value function iters 
5 </p>
<p>Name 
Value </p>
<p>Hidden Layers 
2 
Hidden layer size 
64 
nminibatches 
2 
Num epochs 
1 
λ 
0.95 
γ 
0.99 
clipping ratio 
0.1 
time steps 
5000 
learning rate 
0.0003 </p>
<p>Table 2 :
2Hyperparameters for the PPO algorithm used to update the Action Transformer Policy the simulator using GARAT. The hyperparameters for the PPO algorithm used to update the action transformer policy is provided in</p>
<p>Environment Name
NameProperty Modified Default Value Modified ValueInvertedPendulumHeavy 
Pendulum mass 
4.89 
100.0 
HopperHeavy 
Torso Mass 
3.53 
6.0 
HopperHighFriction 
Foot Friction 
2.0 
2.2 
HalfCheetahHeavy 
Total Mass 
14 
20 
WalkerHeavy 
Torso Mass 
3.534 
10.0 
Ant 
Gravity 
-4.91 
-9.81 
Minitaur </p>
<p>Tunenet: One-shot residual tuning for system identification and sim-to-real robot task transfer. Adam Allevato, Elaine Schaertl Short, Mitch Pryor, Andrea L Thomaz, Conference on Robot Learning (CoRL). Adam Allevato, Elaine Schaertl Short, Mitch Pryor, and Andrea L Thomaz. Tunenet: One-shot residual tuning for system identification and sim-to-real robot task transfer. In Conference on Robot Learning (CoRL), 2019.</p>
<p>Concrete problems in AI safety. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, arXiv:1606.06565arXiv preprintDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.</p>
<p>A framework for behavioural cloning. Michael Bain, Claude Sammut, Machine Intelligence 15. Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelli- gence 15, pages 103-129, 1995.</p>
<p>Robot see, robot do: An overview of robot imitation. Paul Bakker, Yasuo Kuniyoshi, AISB96 Workshop on Learning in Robots and Animals. Paul Bakker and Yasuo Kuniyoshi. Robot see, robot do: An overview of robot imitation. In AISB96 Workshop on Learning in Robots and Animals, pages 3-11, 1996.</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.01540Openai gym. arXiv preprintGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEEYevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA), pages 8973-8979. IEEE, 2019.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. Erwin Coumans, Yunfei Bai, GitHub repository. Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. GitHub repository, 2016.</p>
<p>Humanoid robots learning to walk faster: From the real world to simulation and back. Alon Farchy, Samuel Barrett, Patrick Macalpine, Peter Stone, Proc. of 12th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS). of 12th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS)Alon Farchy, Samuel Barrett, Patrick MacAlpine, and Peter Stone. Humanoid robots learning to walk faster: From the real world to simulation and back. In Proc. of 12th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS), May 2013.</p>
<p>Learning robust rewards with adverserial inverse reinforcement learning. Justin Fu, Katie Luo, Sergey Levine, International Conference on Learning Representations. Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforcement learning. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rkHywl-A-.</p>
<p>State-only imitation with transition dynamics mismatch. Tanmay Gangwani, Jian Peng, International Conference on Learning Representations. Tanmay Gangwani and Jian Peng. State-only imitation with transition dynamics mismatch. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=HJgLLyrYwB.</p>
<p>A divergence minimization perspective on imitation learning methods. Seyed Seyed Kamyar, Richard Ghasemipour, Shixiang Zemel, Gu, Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimiza- tion perspective on imitation learning methods, 2019.</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.</p>
<p>Grounded action transformation for robot learning in simulation. P Josiah, Peter Hanna, Stone, Thirty-First AAAI Conference on Artificial Intelligence. Josiah P Hanna and Peter Stone. Grounded action transformation for robot learning in simulation. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.</p>
<p>Data efficient reinforcement learning with off-policy and simulated data. Josiah Paul Hanna, University of Texas at AustinPhD thesisJosiah Paul Hanna. Data efficient reinforcement learning with off-policy and simulated data. PhD thesis, University of Texas at Austin, 2019.</p>
<p>. Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselinesAshley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https: //github.com/hill-a/stable-baselines, 2018.</p>
<p>Generative adversarial imitation learning. Jonathan Ho, ; D D Stefano Ermon, M Lee, U V Sugiyama, I Luxburg, R Guyon, Garnett, Advances in Neural Information Processing Systems. Curran Associates, Inc29Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4565-4573. Curran Associates, Inc., 2016. URL http://papers. nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf.</p>
<p>Learning agile and dynamic motor skills for legged robots. Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, Marco Hutter, Science Robotics. 4265872Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 4(26):eaau5872, 2019.</p>
<p>Evolutionary robotics and the radical envelope-of-noise hypothesis. Nick Jakobi, Adaptive behavior. 62Nick Jakobi. Evolutionary robotics and the radical envelope-of-noise hypothesis. Adaptive behavior, 6(2):325-368, 1997.</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. Nick Jakobi, Phil Husbands, Inman Harvey, 978-3-540-49286-3Advances in Artificial Life. Federico Morán, Alvaro Moreno, Juan Julián Merelo, and Pablo ChacónBerlin, Heidelberg; Berlin HeidelbergSpringerNick Jakobi, Phil Husbands, and Inman Harvey. Noise and the reality gap: The use of simulation in evolutionary robotics. In Federico Morán, Alvaro Moreno, Juan Julián Merelo, and Pablo Chacón, editors, Advances in Artificial Life, pages 704-720, Berlin, Heidelberg, 1995. Springer Berlin Heidelberg. ISBN 978-3-540-49286-3.</p>
<p>Imitation from observation: Learning to imitate behaviors from raw video via context translation. Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEYuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1118-1125. IEEE, 2018.</p>
<p>Which training methods for GANs do actually converge?. Lars Mescheder, Andreas Geiger, Sebastian Nowozin, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholmsmässan, Stockholm Sweden80Jennifer Dy and Andreas KrauseLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3481-3490, Stockholmsmässan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/mescheder18a.html.</p>
<p>Algorithms for inverse reinforcement learning. Y Andrew, Stuart J Ng, Russell, Icml. 1Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, page 663-670, 2000.</p>
<p>Solving rubik's cube with a robot hand. Ilge Openai, Marcin Akkaya, Maciek Andrychowicz, Mateusz Chociej, Bob Litwin, Arthur Mcgrew, Alex Petron, Matthias Paino, Glenn Plappert, Raphael Powell, Jonas Ribas, Schneider, Qiming Yuan, Wojciech Zaremba, and Lei ZhangNikolas Tezak, Jerry Tworek, Peter Welinder, Lilian WengOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik's cube with a robot hand, 2019.</p>
<p>Ridm: Reinforced inverse dynamics modeling for learning from a single observed demonstration. Faraz Brahma S Pavse, Torabi, P Josiah, Garrett Hanna, Peter Warnell, Stone, arXiv:1906.07372arXiv preprintBrahma S Pavse, Faraz Torabi, Josiah P Hanna, Garrett Warnell, and Peter Stone. Ridm: Reinforced inverse dynamics modeling for learning from a single observed demonstration. arXiv preprint arXiv:1906.07372, 2019.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. Marcin Xue Bin Peng, Wojciech Andrychowicz, Pieter Zaremba, Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEEXue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1-8. IEEE, 2018.</p>
<p>Xue Bin, Erwin Peng, Tingnan Coumans, Tsang-Wei Zhang, Jie Lee, Sergey Tan, Levine, arXiv:2004.00784Learning agile robotic locomotion skills by imitating animals. arXiv preprintXue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan, and Sergey Levine. Learning agile robotic locomotion skills by imitating animals. arXiv preprint arXiv:2004.00784, 2020.</p>
<p>Robust adversarial reinforcement learning. Lerrel Pinto, James Davidson, Rahul Sukthankar, Abhinav Gupta, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2817-2826. JMLR. org, 2017.</p>
<p>Markov decision processes. Handbooks in operations research and management science. Martin L Puterman, 2Martin L Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331-434, 1990.</p>
<p>Epopt: Learning robust neural network policies using model ensembles. Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, Balaraman Ravindran, abs/1610.01283CoRRAravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, and Balaraman Ravindran. Epopt: Learning robust neural network policies using model ensembles. CoRR, abs/1610.01283, 2016. URL http://arxiv.org/abs/1610.01283.</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsStéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627-635, 2011.</p>
<p>Cad2rl: Real single-image flight without a single real image. Fereshteh Sadeghi, Sergey Levine, arXiv:1611.04201arXiv preprintFereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a single real image. arXiv preprint arXiv:1611.04201, 2016.</p>
<p>Learning from demonstration. Stefan Schaal, Advances in neural information processing systems. Stefan Schaal. Learning from demonstration. In Advances in neural information processing systems, pages 1040-1046, 1997.</p>
<p>Trust region policy optimization. CoRR, abs/1502.05477. John Schulman, Sergey Levine, Philipp Moritz, Michael I Jordan, Pieter Abbeel, John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/ 1502.05477.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, arXiv:1312.6199Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel- low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, Vincent Vanhoucke, abs/1804.10332CoRRJie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. CoRR, abs/1804.10332, 2018. URL http://arxiv.org/abs/1804.10332.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23-30. IEEE, 2017.</p>
<p>Domain randomization and generative models for robotic grasping. Josh Tobin, Lukas Biewald, Rocky Duan, Marcin Andrychowicz, Ankur Handa, Vikash Kumar, Bob Mcgrew, Alex Ray, Jonas Schneider, Peter Welinder, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEJosh Tobin, Lukas Biewald, Rocky Duan, Marcin Andrychowicz, Ankur Handa, Vikash Kumar, Bob McGrew, Alex Ray, Jonas Schneider, Peter Welinder, et al. Domain randomization and generative models for robotic grasping. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3482-3489. IEEE, 2018.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.</p>
<p>Behavioral cloning from observation. Faraz Torabi, Garrett Warnell, Peter Stone, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceFaraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4950- 4957, 2018.</p>
<p>Generative adversarial imitation from observation. Faraz Torabi, Garrett Warnell, Peter Stone, arXiv:1807.06158arXiv preprintFaraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observa- tion. arXiv preprint arXiv:1807.06158, 2018.</p>
<p>Recent advances in imitation learning from observation. Faraz Torabi, Garrett Warnell, Peter Stone, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceFaraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from observation. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, Aug 2019.</p>
<p>Huang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, Thai Hong Linh, arXiv:1906.08113Wasserstein adversarial imitation learning. arXiv preprintHuang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, and Thai Hong Linh. Wasserstein adversarial imitation learning. arXiv preprint arXiv:1906.08113, 2019.</p>
<p>Learning locomotion skills for cassie: Iterative design and sim-to-real. Zhaoming Xie, Patrick Clary, Jeremy Dao, Pedro Morais, Jonathan Hurst, Michiel Van De Panne, Proc. Conference on Robot Learning (CORL 2019. Conference on Robot Learning (CORL 20194Zhaoming Xie, Patrick Clary, Jeremy Dao, Pedro Morais, Jonathan Hurst, and Michiel van de Panne. Learning locomotion skills for cassie: Iterative design and sim-to-real. In Proc. Conference on Robot Learning (CORL 2019), volume 4, 2019.</p>
<p>real" environment, which neither the ungrounded simulator nor the GAT-grounded simulator manage. The trajectory in the GAT-grounded simulator can be seen close to the one in the "real" environment initially. Wenhao Yu, C Karen Liu, Greg Turk, abs/1810.05751Policy transfer with strategy optimization. but since it disregards the sequential nature of the problem, the compounding errors cause the episode to terminate prematurelyWenhao Yu, C. Karen Liu, and Greg Turk. Policy transfer with strategy optimization. CoRR, abs/1810.05751, 2018. URL http://arxiv.org/abs/1810.05751. "real" environment, which neither the ungrounded simulator nor the GAT-grounded simulator manage. The trajectory in the GAT-grounded simulator can be seen close to the one in the "real" environment initially, but since it disregards the sequential nature of the problem, the compounding errors cause the episode to terminate prematurely.</p>
<p>This comparison is done by deploying them in the "real" environment from the same initial state. As we can see in Figure 4, the policies trained in the "real" environment and the GARAT-grounded simulator behave similarly, while the one trained in the GAT-grounded simulator acts differently. This comparison is another qualitative one. How well these policies perform. real" environment, GAT-grounded simulator and GARAT-grounded simulator. in w.r.t. the task at hand is explored in detail in Section 6.2An additional experiment we conducted was to compare the policies trained in the "real" environment, GAT-grounded simulator and GARAT-grounded simulator. This comparison is done by deploying them in the "real" environment from the same initial state. As we can see in Figure 4, the policies trained in the "real" environment and the GARAT-grounded simulator behave similarly, while the one trained in the GAT-grounded simulator acts differently. This comparison is another qualitative one. How well these policies perform in w.r.t. the task at hand is explored in detail in Section 6.2.</p>            </div>
        </div>

    </div>
</body>
</html>
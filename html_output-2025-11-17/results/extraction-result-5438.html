<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5438 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5438</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5438</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-34e1a8a75bf6f35084ac6d714a136f39d02c649e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/34e1a8a75bf6f35084ac6d714a136f39d02c649e" target="_blank">Self-Verification Improves Few-Shot Clinical Information Extraction</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work explores a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs and consistently improves accuracy for various LLMs in standard clinical information extraction tasks.</p>
                <p><strong>Paper Abstract:</strong> Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5438.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5438.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-verify pipeline that uses multiple calls to the same LLM to extract information, find omissions, ground each extracted element in an evidence span, and prune incorrect elements to improve accuracy and provide interpretable grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (GPT-4, ChatGPT, GPT-3.5 / text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper evaluates SV using OpenAI models accessed via the Azure OpenAI API: GPT-4 (gpt-4-0314 in chat mode), ChatGPT (gpt-3.5-turbo), and GPT-3.5 / text-davinci-003; sampling temperature set to 0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Four-step iterative pipeline where (1) Original extraction produces a bulleted list of elements, (2) Omission finds missing elements (repeated up to 5 times for long inputs), (3) Evidence grounds each element to a short text span in the input, and (4) Prune removes inaccurate elements using the supplied evidence. Each step is implemented as an independent prompt/call to the same LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical information extraction (clinical trial arm extraction; medication status extraction; ICD-9/ICD-10 code extraction on MIMIC-III and MIMIC-IV)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Variable-length list extraction tasks from clinical text: identify clinical trial arms, extract medications with status (active/discontinued/neither), and extract/translate diagnoses to ICD codes from MIMIC clinical notes/discharge summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>See per-model/per-task values in model entries; across models/tasks Full SV yields consistent F1 improvements (average F1 improvement reported: +0.056). Evidence spans also show high overlap with human annotations (GPT-4: 0.93 ± 0.02 span overlap).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline is single-step in-context extraction using the same LLMs; see per-model/per-task values in model entries.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: Table 2 reports consistent F1 improvements across all evaluated models and tasks (average F1 improvement +0.056). Ablations (Table 3) show Omission increases recall, Prune increases precision, and Full SV increases F1 (examples: GPT-4 clinical trial arm F1 0.419 → 0.530; GPT-4 MIMIC-IV ICD-10 F1 0.487 → 0.533). Evidence spans overlap with human annotations at high rates (GPT-4 span overlap 0.93 ±0.02, GPT-3.5 0.84 ±0.03), supporting interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SV increases computational cost due to multiple LLM calls. Performance is sensitive to prompt design. Omission can increase false positives (higher recall but lower precision) while Prune increases precision; balancing steps is necessary. A single large prompt concatenating all steps performed slightly worse than chaining steps. SV does not fully overcome model weaknesses (e.g., GPT-3.5 struggled on translating diagnoses to ICD codes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5438.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5438.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0314)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 evaluated in chat mode; used as one of the underlying LLMs for the SV pipeline and baseline extraction evaluations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Verification Improves Few-Shot Clinical Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0314)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 variant evaluated in chat mode via the Azure OpenAI API; sampling temperature set to 0.1. (The paper does not report model size or architecture details.)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same LLM (GPT-4) is called for each SV step: Original extraction → Omission (repeated for long inputs) → Evidence (produce supporting text spans) → Prune (remove items unsupported by evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical trial arm extraction; Medication status extraction; MIMIC-III ICD-9; MIMIC-IV ICD-9; MIMIC-IV ICD-10</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>List-extraction tasks from clinical texts: extract trial arms, medications+status, and map diagnoses to ICD codes on MIMIC datasets (250 examples subset for ICD tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Clinical trial arm F1: 0.530 ± 0.010; Medication name F1: 0.910 ± 0.001; MIMIC-III ICD-9 F1: 0.678 ± 0.007; MIMIC-IV ICD-9 F1: 0.755 ± 0.004; MIMIC-IV ICD-10 F1: 0.533 ± 0.002.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Clinical trial arm F1: 0.419 ± 0.008; Medication name F1: 0.884 ± 0.003; MIMIC-III ICD-9 F1: 0.652 ± 0.020; MIMIC-IV ICD-9 F1: 0.718 ± 0.030; MIMIC-IV ICD-10 F1: 0.487 ± 0.020.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Direct comparisons in Table 2 show consistent F1 gains when using SV (examples: trial arm +0.111, medication +0.026, MIMIC-IV ICD-10 +0.046). Ablations (Table 3) show Omission primarily increases recall while Prune increases precision; Full SV yields best combined F1. Span overlap with human-annotated evidence is high (Table 4: span overlap accuracy 0.93 ± 0.02).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Increased runtime/cost due to multiple calls. SV is prompt-sensitive. Some SV steps (e.g., Omission) can add false positives; Prune mitigates but requires correct evidence extraction. The paper notes diminishing returns in some tasks and that concatenating steps into one prompt underperforms chained calls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5438.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5438.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo evaluated as ChatGPT, used as an underlying LLM for baseline extraction and inside the SV pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Verification Improves Few-Shot Clinical Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo (referred to as ChatGPT in the paper), accessed via Azure OpenAI API; sampling temperature 0.1. The paper does not specify internal architecture/size.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Chained prompts calling the same model: produce original extraction, identify omissions (possibly repeated), extract evidence spans, and prune inaccurate items based on evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical trial arm extraction; Medication status extraction; MIMIC-III ICD-9; MIMIC-IV ICD-9; MIMIC-IV ICD-10</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: variable-length clinical IE tasks (trial arms, medication status, ICD code extraction/translation) across specified datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Clinical trial arm F1: 0.456 ± 0.007; Medication name F1: 0.898 ± 0.002; MIMIC-III ICD-9 F1: 0.619 ± 0.005; MIMIC-IV ICD-9 F1: 0.713 ± 0.005; MIMIC-IV ICD-10 F1: 0.464 ± 0.003.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Clinical trial arm F1: 0.342 ± 0.010; Medication name F1: 0.892 ± 0.004; MIMIC-III ICD-9 F1: 0.593 ± 0.003; MIMIC-IV ICD-9 F1: 0.693 ± 0.040; MIMIC-IV ICD-10 F1: 0.448 ± 0.040.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Table 2 shows consistent F1 increases with SV for ChatGPT across tasks (examples: trial arm +0.114, medication +0.006, MIMIC-III +0.026). Ablations show Omission raises recall while Prune raises precision; Full SV yields balanced F1 gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements are sometimes small (e.g., medication extraction +0.006 F1). For long-input ICD tasks, ChatGPT's improvements are present but less pronounced than GPT-4 on some metrics. Same SV caveats: cost and prompt sensitivity apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5438.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5438.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5/text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Older-generation OpenAI models (GPT-3.5 family and text-davinci-003) evaluated in the paper as baselines and used inside the SV pipeline; performance varies by task, sometimes outperforming GPT-4 on short-input tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Verification Improves Few-Shot Clinical Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (incl. text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models from the GPT-3.5 family and text-davinci-003 accessed via Azure OpenAI API; sampling temperature 0.1. The paper treats these as baseline LLMs but does not list parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same SV pipeline: Original extraction, Omission (repeat for long inputs), Evidence, Prune; applied using GPT-3.5/text-davinci-003 as the LLM backend.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical trial arm extraction; Medication status extraction; MIMIC-III ICD-9; MIMIC-IV ICD-9; MIMIC-IV ICD-10</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Clinical IE tasks described above; note the paper reports GPT-3.5 often does best on short-input tasks but struggles on ICD-code extraction that requires mapping diagnoses to codes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Clinical trial arm F1: 0.575 ± 0.003; Medication name F1: 0.935 ± 0.001; MIMIC-III ICD-9 F1: 0.435 ± 0.010; MIMIC-IV ICD-9 F1: 0.702 ± 0.020; MIMIC-IV ICD-10 F1: 0.442 ± 0.010.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Clinical trial arm F1: 0.512 ± 0.009; Medication name F1: 0.929 ± 0.002; MIMIC-III ICD-9 F1: 0.431 ± 0.030; MIMIC-IV ICD-9 F1: 0.691 ± 0.020; MIMIC-IV ICD-10 F1: 0.434 ± 0.030.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>SV yields consistent but sometimes small F1 gains for GPT-3.5 (examples: trial arm +0.063, medication +0.006). The paper reports GPT-3.5 outperforming GPT-4 on short-input clinical trial arm/medication tasks in some cases, but performing poorly on ICD-code translation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-3.5 shows limitations on ICD-code extraction (diagnosis→ICD mapping). SV helps but does not fully overcome this; also same SV drawbacks: cost, prompt sensitivity. Ablations indicate Omission can reduce precision if not balanced by Prune.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5438.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5438.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative refinement with self-feedback method cited in related work that chains LLM calls to refine outputs using the model's own feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (method mentioned; original paper applies to LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned in related work as an existing iterative refinement/self-feedback approach (Madaan et al., 2023); not evaluated in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative generate-and-refine approach where the model produces output and then generates self-feedback to revise it in subsequent iterations (described in the cited work; only mentioned in this paper's related work).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>N/A (referenced method; specific tasks are in the cited paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not evaluated in this paper; cited as related prior work on chaining LLM calls to improve outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned as prior work motivating SV; this paper does not provide experimental results for Self-refine but positions SV as related to iterative/refinement chains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper beyond being cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Check your facts and try again: Improving large language models with external knowledge and automated feedback <em>(Rating: 2)</em></li>
                <li>Rarr: Researching and revising what language models say, using language models <em>(Rating: 2)</em></li>
                <li>Iteratively prompt pre-trained language models for chain of thought <em>(Rating: 2)</em></li>
                <li>Program-aided language models (PAL) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5438",
    "paper_id": "paper-34e1a8a75bf6f35084ac6d714a136f39d02c649e",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "SV",
            "name_full": "Self-Verification",
            "brief_description": "A generate-then-verify pipeline that uses multiple calls to the same LLM to extract information, find omissions, ground each extracted element in an evidence span, and prune incorrect elements to improve accuracy and provide interpretable grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (GPT-4, ChatGPT, GPT-3.5 / text-davinci-003)",
            "model_description": "The paper evaluates SV using OpenAI models accessed via the Azure OpenAI API: GPT-4 (gpt-4-0314 in chat mode), ChatGPT (gpt-3.5-turbo), and GPT-3.5 / text-davinci-003; sampling temperature set to 0.1.",
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Four-step iterative pipeline where (1) Original extraction produces a bulleted list of elements, (2) Omission finds missing elements (repeated up to 5 times for long inputs), (3) Evidence grounds each element to a short text span in the input, and (4) Prune removes inaccurate elements using the supplied evidence. Each step is implemented as an independent prompt/call to the same LLM.",
            "num_iterations": 4,
            "task_name": "Clinical information extraction (clinical trial arm extraction; medication status extraction; ICD-9/ICD-10 code extraction on MIMIC-III and MIMIC-IV)",
            "task_description": "Variable-length list extraction tasks from clinical text: identify clinical trial arms, extract medications with status (active/discontinued/neither), and extract/translate diagnoses to ICD codes from MIMIC clinical notes/discharge summaries.",
            "performance_with_reflection": "See per-model/per-task values in model entries; across models/tasks Full SV yields consistent F1 improvements (average F1 improvement reported: +0.056). Evidence spans also show high overlap with human annotations (GPT-4: 0.93 ± 0.02 span overlap).",
            "performance_without_reflection": "Baseline is single-step in-context extraction using the same LLMs; see per-model/per-task values in model entries.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: Table 2 reports consistent F1 improvements across all evaluated models and tasks (average F1 improvement +0.056). Ablations (Table 3) show Omission increases recall, Prune increases precision, and Full SV increases F1 (examples: GPT-4 clinical trial arm F1 0.419 → 0.530; GPT-4 MIMIC-IV ICD-10 F1 0.487 → 0.533). Evidence spans overlap with human annotations at high rates (GPT-4 span overlap 0.93 ±0.02, GPT-3.5 0.84 ±0.03), supporting interpretability.",
            "limitations_or_failure_cases": "SV increases computational cost due to multiple LLM calls. Performance is sensitive to prompt design. Omission can increase false positives (higher recall but lower precision) while Prune increases precision; balancing steps is necessary. A single large prompt concatenating all steps performed slightly worse than chaining steps. SV does not fully overcome model weaknesses (e.g., GPT-3.5 struggled on translating diagnoses to ICD codes).",
            "uuid": "e5438.0",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (gpt-4-0314)",
            "brief_description": "OpenAI's GPT-4 evaluated in chat mode; used as one of the underlying LLMs for the SV pipeline and baseline extraction evaluations in this paper.",
            "citation_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0314)",
            "model_description": "OpenAI GPT-4 variant evaluated in chat mode via the Azure OpenAI API; sampling temperature set to 0.1. (The paper does not report model size or architecture details.)",
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Same LLM (GPT-4) is called for each SV step: Original extraction → Omission (repeated for long inputs) → Evidence (produce supporting text spans) → Prune (remove items unsupported by evidence).",
            "num_iterations": 4,
            "task_name": "Clinical trial arm extraction; Medication status extraction; MIMIC-III ICD-9; MIMIC-IV ICD-9; MIMIC-IV ICD-10",
            "task_description": "List-extraction tasks from clinical texts: extract trial arms, medications+status, and map diagnoses to ICD codes on MIMIC datasets (250 examples subset for ICD tasks).",
            "performance_with_reflection": "Clinical trial arm F1: 0.530 ± 0.010; Medication name F1: 0.910 ± 0.001; MIMIC-III ICD-9 F1: 0.678 ± 0.007; MIMIC-IV ICD-9 F1: 0.755 ± 0.004; MIMIC-IV ICD-10 F1: 0.533 ± 0.002.",
            "performance_without_reflection": "Clinical trial arm F1: 0.419 ± 0.008; Medication name F1: 0.884 ± 0.003; MIMIC-III ICD-9 F1: 0.652 ± 0.020; MIMIC-IV ICD-9 F1: 0.718 ± 0.030; MIMIC-IV ICD-10 F1: 0.487 ± 0.020.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Direct comparisons in Table 2 show consistent F1 gains when using SV (examples: trial arm +0.111, medication +0.026, MIMIC-IV ICD-10 +0.046). Ablations (Table 3) show Omission primarily increases recall while Prune increases precision; Full SV yields best combined F1. Span overlap with human-annotated evidence is high (Table 4: span overlap accuracy 0.93 ± 0.02).",
            "limitations_or_failure_cases": "Increased runtime/cost due to multiple calls. SV is prompt-sensitive. Some SV steps (e.g., Omission) can add false positives; Prune mitigates but requires correct evidence extraction. The paper notes diminishing returns in some tasks and that concatenating steps into one prompt underperforms chained calls.",
            "uuid": "e5438.1",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (gpt-3.5-turbo)",
            "brief_description": "OpenAI gpt-3.5-turbo evaluated as ChatGPT, used as an underlying LLM for baseline extraction and inside the SV pipeline.",
            "citation_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "OpenAI gpt-3.5-turbo (referred to as ChatGPT in the paper), accessed via Azure OpenAI API; sampling temperature 0.1. The paper does not specify internal architecture/size.",
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Chained prompts calling the same model: produce original extraction, identify omissions (possibly repeated), extract evidence spans, and prune inaccurate items based on evidence.",
            "num_iterations": 4,
            "task_name": "Clinical trial arm extraction; Medication status extraction; MIMIC-III ICD-9; MIMIC-IV ICD-9; MIMIC-IV ICD-10",
            "task_description": "As above: variable-length clinical IE tasks (trial arms, medication status, ICD code extraction/translation) across specified datasets.",
            "performance_with_reflection": "Clinical trial arm F1: 0.456 ± 0.007; Medication name F1: 0.898 ± 0.002; MIMIC-III ICD-9 F1: 0.619 ± 0.005; MIMIC-IV ICD-9 F1: 0.713 ± 0.005; MIMIC-IV ICD-10 F1: 0.464 ± 0.003.",
            "performance_without_reflection": "Clinical trial arm F1: 0.342 ± 0.010; Medication name F1: 0.892 ± 0.004; MIMIC-III ICD-9 F1: 0.593 ± 0.003; MIMIC-IV ICD-9 F1: 0.693 ± 0.040; MIMIC-IV ICD-10 F1: 0.448 ± 0.040.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Table 2 shows consistent F1 increases with SV for ChatGPT across tasks (examples: trial arm +0.114, medication +0.006, MIMIC-III +0.026). Ablations show Omission raises recall while Prune raises precision; Full SV yields balanced F1 gains.",
            "limitations_or_failure_cases": "Improvements are sometimes small (e.g., medication extraction +0.006 F1). For long-input ICD tasks, ChatGPT's improvements are present but less pronounced than GPT-4 on some metrics. Same SV caveats: cost and prompt sensitivity apply.",
            "uuid": "e5438.2",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5/text-davinci-003",
            "name_full": "GPT-3.5 / text-davinci-003",
            "brief_description": "Older-generation OpenAI models (GPT-3.5 family and text-davinci-003) evaluated in the paper as baselines and used inside the SV pipeline; performance varies by task, sometimes outperforming GPT-4 on short-input tasks.",
            "citation_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (incl. text-davinci-003)",
            "model_description": "Models from the GPT-3.5 family and text-davinci-003 accessed via Azure OpenAI API; sampling temperature 0.1. The paper treats these as baseline LLMs but does not list parameter counts.",
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Same SV pipeline: Original extraction, Omission (repeat for long inputs), Evidence, Prune; applied using GPT-3.5/text-davinci-003 as the LLM backend.",
            "num_iterations": 4,
            "task_name": "Clinical trial arm extraction; Medication status extraction; MIMIC-III ICD-9; MIMIC-IV ICD-9; MIMIC-IV ICD-10",
            "task_description": "Clinical IE tasks described above; note the paper reports GPT-3.5 often does best on short-input tasks but struggles on ICD-code extraction that requires mapping diagnoses to codes.",
            "performance_with_reflection": "Clinical trial arm F1: 0.575 ± 0.003; Medication name F1: 0.935 ± 0.001; MIMIC-III ICD-9 F1: 0.435 ± 0.010; MIMIC-IV ICD-9 F1: 0.702 ± 0.020; MIMIC-IV ICD-10 F1: 0.442 ± 0.010.",
            "performance_without_reflection": "Clinical trial arm F1: 0.512 ± 0.009; Medication name F1: 0.929 ± 0.002; MIMIC-III ICD-9 F1: 0.431 ± 0.030; MIMIC-IV ICD-9 F1: 0.691 ± 0.020; MIMIC-IV ICD-10 F1: 0.434 ± 0.030.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "SV yields consistent but sometimes small F1 gains for GPT-3.5 (examples: trial arm +0.063, medication +0.006). The paper reports GPT-3.5 outperforming GPT-4 on short-input clinical trial arm/medication tasks in some cases, but performing poorly on ICD-code translation tasks.",
            "limitations_or_failure_cases": "GPT-3.5 shows limitations on ICD-code extraction (diagnosis→ICD mapping). SV helps but does not fully overcome this; also same SV drawbacks: cost, prompt sensitivity. Ablations indicate Omission can reduce precision if not balanced by Prune.",
            "uuid": "e5438.3",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-refine (mentioned)",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "An iterative refinement with self-feedback method cited in related work that chains LLM calls to refine outputs using the model's own feedback.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "N/A (method mentioned; original paper applies to LLMs)",
            "model_description": "Mentioned in related work as an existing iterative refinement/self-feedback approach (Madaan et al., 2023); not evaluated in this paper's experiments.",
            "reflection_method_name": "Self-refine (iterative self-feedback)",
            "reflection_method_description": "Iterative generate-and-refine approach where the model produces output and then generates self-feedback to revise it in subsequent iterations (described in the cited work; only mentioned in this paper's related work).",
            "num_iterations": null,
            "task_name": "N/A (referenced method; specific tasks are in the cited paper)",
            "task_description": "Not evaluated in this paper; cited as related prior work on chaining LLM calls to improve outputs.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned as prior work motivating SV; this paper does not provide experimental results for Self-refine but positions SV as related to iterative/refinement chains.",
            "limitations_or_failure_cases": "Not discussed in this paper beyond being cited as related work.",
            "uuid": "e5438.4",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "rating": 2
        },
        {
            "paper_title": "Rarr: Researching and revising what language models say, using language models",
            "rating": 2
        },
        {
            "paper_title": "Iteratively prompt pre-trained language models for chain of thought",
            "rating": 2
        },
        {
            "paper_title": "Program-aided language models (PAL)",
            "rating": 1
        }
    ],
    "cost": 0.015796499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Verification Improves Few-Shot Clinical Information Extraction</h1>
<p>Zelalem Gero<em> ${ }^{1}$ Chandan Singh ${ }^{</em> 1}$ Hao Cheng ${ }^{1}$ Tristan Naumann ${ }^{1}$<br>Michel Galley ${ }^{1}$ Jianfeng Gao ${ }^{1}$ Hoifung Poon ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning, which requires costly human annotations. However, despite drastic advances, modern LLMs such as GPT-4 still struggle with issues regarding accuracy and interpretability, especially in safety-critical domains such as health. We explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This framework is made possible by the asymmetry between verification and generation, where the former is often much easier than the latter. Experimental results show that our method consistently improves accuracy for various LLMs across standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts. ${ }^{1}$</p>
<h2>1. Introduction and related work</h2>
<p>Clinical information extraction plays a pivotal role in the analysis of medical records and enables healthcare practitioners to efficiently access and utilize patient data (Zweigenbaum et al., 2007; Wang et al., 2018). Few-shot learning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>approaches have emerged as a promising solution to tackle the scarcity of labeled training data in clinical information extraction tasks (Agrawal et al., 2022; Laursen et al., 2023). However, these methods continue to struggle with accuracy and interpretability, both critical concerns in the medical domain (Gutiérrez et al., 2022).</p>
<p>Here, we address these issues by using self-verification (SV) to improve few-shot clinical information extraction. SV builds on recent works that chain together large language model (LLM) calls to improve an LLM's performance (Wu et al., 2022; Wang et al., 2022; Chase, 2023). Intuitively, these chains succeed because an LLM may be able to perform individual steps in a task, e.g. evidence verification, more accurately than the LLM can perform an entire task, e.g. information extraction (Ma et al., 2023; Madaan et al., 2023; Zhang et al., 2023). Such chains have been successful in settings such as multi-hop question answering (Press et al., 2022), retrieval-augmented/tool-augmented question answering (Peng et al., 2023; Paranjape et al., 2023; Schick et al., 2023; Gao et al., 2023), and code execution (Jojic et al., 2023). Here, we analyze whether building such a chain can improve clinical information extraction.</p>
<p>Fig. 1 shows the SV pipeline we build here. We broadly define self-verification as using multiple calls to the same LLM to verify its output, and also to ground each element of its output in evidence. Our SV pipeline consists of four steps, each of which calls the same LLM with different prompts. First, the Original extraction step queries the LLM directly for the desired information. Next, the Omission step finds missing elements in the output, the Evidence step grounds each element in the output to a text span in the input, and the Prune step removes inaccurate elements in the output. Taken together, we demonstrate that these steps improve the reliability of extracted information.</p>
<p>Additionally, SV provides interpretable grounding for each output, in the form of a short text span in the input. Interpretability has taken many forms in NLP, including posthoc feature importance (Lundberg \&amp; Lee, 2017; Ribeiro et al., 2016), intrinsically interpretable models (Rudin, 2019; Singh et al., 2022a), and visualizing model intermediates, e.g. attention (Wiegreffe \&amp; Pinter, 2019). The interpretable grounding we generate comes directly from an LLM, similar</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of self-verification pipeline for clinical information extraction. Each step calls the same LLM with different prompts to refine the information from the previous steps. Below each step we show abbreviated outputs for extracting a list of assigned diagnoses from a sample clinical note.
to recent works that use LLMs to generate explanations (Rajani et al., 2019; MacNeil et al., 2022; Singh et al., 2023) and ground those explanations in evidence (Rashkin et al., 2021; Gao et al., 2022).</p>
<p>Experiments on various clinical information extraction tasks and various LLMs, including GPT-4 (OpenAI, 2023) and ChatGPT (Ouyang et al., 2022), show the efficacy of SV. In addition to improving accuracy, we find that the extracted interpretations match human judgements of relevant information, enabling auditing by a human and helping to build a path towards trustworthy extraction of clinical information in resource-constrained scenarios.</p>
<h2>2. Methods and experimental setup</h2>
<h3>2.1. Methods: Self-verification</h3>
<p>Fig. 1 shows the four different steps of the introduced SV pipeline. The pipeline takes in a raw text input, e.g. a clinical note, and outputs information in a pre-specified format, e.g. a bulleted list. It consists of four steps, each of which calls the same LLM with different prompts in order to refine and ground the original output.</p>
<p>The original extraction step uses a task-specific prompt which instructs the model to output a variable-length bulleted list. In the toy example in Fig. 1, the goal is to identify the two diagnoses Hypertension and Right adrenal mass, but the original extraction step finds only Hypertension.</p>
<p>After the original LLM extraction, the Omission step finds missing elements in the output; in the Fig. 1 example it finds Right adrenal mass and Liver fibrosis. For tasks with long inputs (mean input length greater than 2,000 characters), we repeat the omission step to find more potential missed elements (we repeat five times, and continue repeating until the omission step stops finding new omissions).</p>
<p>Next, the Evidence step grounds each element in the output to a text span in the input. The grounding in this step provides interpretations that can be inspected by a human. In the Fig. 1 example, we find quotes supporting the first two diagnoses, but the quote for liver fibrosis shows that it was in fact ruled out, and is therefore an incorrect diagnosis. Finally, the Prune step uses the supplied evidence to remove inaccurate elements from the output. In Fig. 1 this results in removing liver fibrosis to return the correct final list. Taken together, these steps help to extract accurate and interpretable information.</p>
<p>We provide the exact prompts used in all steps in the Github repo. For the tasks with short inputs, we include 5 random data demonstrations in the original extraction prompt; otherwise all prompts are fixed across examples.</p>
<h3>2.2. Experimental setup</h3>
<p>Datasets Table 1 gives the details of each task we study here. Each task requires extracting a variable-length list of elements. In clinical trial arm extraction, these are names of different clinical trial arms, manually annotated from the EBM-NLP dataset (Nye et al., 2018). In the medication status extraction task, in addition to medication names the medication status must additionally be classified as active, discontinued, or neither. The text inputs for arm extraction / medication status extraction are relatively small (average length is 1,620 characters and 382 characters, respectively).</p>
<p>In the case of MIMIC-III and MIMIC-IV (Johnson et al., 2016; 2021), we predict ICD-9 or ICD-10 codes (corresponding to diagnoses and procedures). We predict ICD codes using relevant sections from all types of clinical notes for MIMIC-III (average length: 5,200 words) but only discharge summaries for MIMIC-IV (average length: 1,400 words). The ICD codes are not directly present in the text in-</p>
<p>put, and therefore the task requires translating the diagnoses to their relevant code. MIMIC data is preprocessed using a standard pipeline (see Appendix A.1) and we evaluate on a random subset of 250 inputs for each task.</p>
<p>Models We evaluate three different models: GPT3.5 (Brown et al., 2020), text-davinci-003, ChatGPT (Ouyang et al., 2022) gpt-3.5-turbo, and GPT4 (OpenAI, 2023) gpt 4-0314 (in chat mode), all accessed securely through the Azure OpenAI API. We set the sampling temperature for LLM decoding to 0.1 .</p>
<p>Evaluation Extraction is evaluated via case-insensitive exact string matching, and we report the resulting macro F1 scores, recall, and precision. In some cases, this evaluation may underestimate actual performance as a result of the presence of acronyms or different names within the output; nevertheless, the relative performance of different models/methods should still be preserved. Following common practice, we restrict ICD code evaluation to the top 50 codes appearing in the dataset.</p>
<h2>3. Results</h2>
<h3>3.1. Self-verification improves prediction performance</h3>
<p>Table 2 shows the results for clinical extraction performance with and without self-verification. Across different models and tasks, SV consistently provides a performance improvement. The performance improvement is occasionally quite large (e.g. GPT-4 shows more than a 0.1 improvement in F1 for clinical trial arm extraction and more than a 0.3 improvement for medication status extraction), and the average F1 improvement across models and tasks is 0.056 . We also compare to a baseline where we concatenate the prompts across different steps into a single large prompt which is then used to make a single LLM call for information extraction. We find that this large-prompt baseline performs slightly worse than the baseline reported in Table 2, which uses a straightforward prompt for extraction (see comparison details in Table A5).</p>
<p>For tasks with short inputs, we find that GPT-3.5 performs best, even outperforming GPT-4, as has been seen in some recent works (e.g. Patil et al. 2023). For the MIMIC tasks with larger inputs, GPT-4 performs best. In fact, GPT-3.5 performs very poorly on ICD-code extraction, perhaps because the task requires not only extracting diagnoses from the input text but also knowing the mapping between diagnoses and ICD codes.</p>
<p>Table 3 contains ablations showing how the different selfverification modules affect the results. The Omission step finds missing elements, which increases recall but at the cost of decreased precision. In contrast, the Prune step</p>
<h2>Medication status output</h2>
<ul>
<li>aspirin: discontinued</li>
<li>ibuprofen: neither</li>
<li>Naprosyn: neither</li>
<li>Tylenol: active</li>
<li>Plavix: active</li>
</ul>
<h2>Evidence highlighting</h2>
<p>Her aspirin ( 81 mg q.d.) is discontinued, and the patient is advised that she needs to avoid ibuprofen, Naprosyn, alcohol, caffeine, and chocolate. She is advised that Tylenol 325 mg or Tylenol ES (500 mg) is safe to take at 1 or $2 \mathrm{q} .4-6 \mathrm{~h}$. p.r.n. for pain or fever. Discharge activity is without restriction. DISCHARGE MEDICATIONS: 1. Plavix 75 mg p.o. q.d.</p>
<p>Figure 2. Example output and interpretation for medication status. For each element of the output list, our pipeline outputs the text span which contains evidence for that generated output (shown with highlighting).
(that incorporates the span from the Evidence step) removes extraneous elements, thereby increasing precision. Together (Full SV), the steps achieve a balance which improves F1. For tasks with longer inputs (e.g. MIMIC-IV ICD-10), the Omission step seems to provide more of the improvement in F1, likely because it is able to find evidence that was missed by a single extraction step.</p>
<h3>3.2. Self-verification yields interpretations</h3>
<p>Fig. 2 shows an example output from the self-verification pipeline for medication status (the underlying model is GPT4). In the example, the pipeline correctly identifies each medication and its corresponding status. In addition, the pipeline supplies the span of text which serves as evidence for each returned medication (shown with highlighting). This highlighting enables efficient auditing by a human for each element. In a human-in-the-loop setting, a human could also see results/highlights for elements which were pruned, to quickly check for any mistakes.</p>
<p>Table 4 evaluates the evidence spans provided by SV against human judgements collected in a prior work (Nye et al., 2018). Human reviewers annotated spans in the original text which correspond to interventions, which include clinical trial arms as a subset. Table 4 gives the fraction of generated evidence spans that overlap with a span provided by the human annotators. The fraction is quite large, e.g. $93 \%$ for GPT-4. At baseline, human annotators identify less than $3.7 \%$ of tokens as interventions, so these span overlap accuracies are much higher than expected by random chance.</p>
<p>Table 1. Tasks and associated datasets studied here.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Data</th>
<th style="text-align: left;">Example output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ICD code extraction <br> (ICD-9 and ICD-10)</td>
<td style="text-align: left;">250 MIMIC III reports (Johnson et al., 2016), <br> 250 MIMIC IV discharge summaries (Johnson et al., 2021)</td>
<td style="text-align: left;">$[205.0,724.1,96.04]$</td>
</tr>
<tr>
<td style="text-align: left;">Clinical trial arm extraction</td>
<td style="text-align: left;">100 annotations to EBM-NLP abstracts (Nye et al., 2018)</td>
<td style="text-align: left;">{propofol, droperidol, placebo}</td>
</tr>
<tr>
<td style="text-align: left;">Medication status extraction</td>
<td style="text-align: left;">105 Annotations (Agrawal et al., 2022) to snippets from <br> CASI (Moon et al., 2012)</td>
<td style="text-align: left;">{aspirin: discontinued, plavix: active}</td>
</tr>
</tbody>
</table>
<p>Table 2. F1 scores for extraction with and without self-verification (SV). Across different models and tasks, SV consistently provides a performance improvement, although it is sometimes small. Bolding shows SV compared to original, underline shows best model for each task. Averaged over 3 random seeds; error bars show the standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">ChatGPT</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clinical trial arm (Original / SV)</td>
<td style="text-align: left;">$0.342 \pm 0.010 / \mathbf{0 . 4 5 6} \pm \mathbf{0 . 0 0 7}$</td>
<td style="text-align: left;">$0.419 \pm 0.008 / \mathbf{0 . 5 3 0} \pm \mathbf{0 . 0 1 0}$</td>
<td style="text-align: left;">$0.512 \pm 0.009 / \mathbf{0 . 5 7 5} \pm \mathbf{0 . 0 0 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Medication name (Original / SV)</td>
<td style="text-align: left;">$0.892 \pm 0.004 / \mathbf{0 . 8 9 8} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.884 \pm 0.003 / \mathbf{0 . 9 1 0} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: left;">$0.929 \pm 0.002 / \mathbf{0 . 9 3 5} \pm \mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">MIMIC-III ICD-9 (Original / SV)</td>
<td style="text-align: left;">$0.593 \pm 0.003 / \mathbf{0 . 6 1 9} \pm \mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">$0.652 \pm 0.02 / \mathbf{0 . 6 7 8} \pm \mathbf{0 . 0 0 7}$</td>
<td style="text-align: left;">$0.431 \pm 0.03 / \mathbf{0 . 4 3 5} \pm \mathbf{0 . 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">MIMIC-IV ICD-9 (Original / SV)</td>
<td style="text-align: left;">$0.693 \pm 0.04 / \mathbf{0 . 7 1 3} \pm \mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">$0.718 \pm 0.03 / \mathbf{0 . 7 5 5} \pm \mathbf{0 . 0 0 4}$</td>
<td style="text-align: left;">$0.691 \pm 0.02 / \mathbf{0 . 7 0 2} \pm \mathbf{0 . 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;">MIMIC-IV ICD-10 (Original / SV)</td>
<td style="text-align: left;">$0.448 \pm 0.04 / \mathbf{0 . 4 6 4} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: left;">$0.487 \pm 0.02 / \mathbf{0 . 5 3 3} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.434 \pm 0.03 / \mathbf{0 . 4 4 2} \pm \mathbf{0 . 0 1}$</td>
</tr>
</tbody>
</table>
<p>Table 3. Ablation results when using different combinations of self-verification steps for two tasks. Omission increases Recall and Prune increases Precision. Together they increase both, improving F1. Evidence improves F1 for Medication Status. Underlying model is the best model for each task (GPT-3.5 for Medication name and GPT-4 for MIMIC-IV ICD-10). Averaged over 3 random seeds; error bars are standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Medication name</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: left;">Precision</td>
<td style="text-align: left;">Recall</td>
</tr>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: center;">$0.929 \pm 0.002$</td>
<td style="text-align: left;">$0.929 \pm 0.003$</td>
<td style="text-align: left;">$0.928 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">+ Omission</td>
<td style="text-align: center;">$0.913 \pm 0.001$</td>
<td style="text-align: left;">$0.881 \pm 0.003$</td>
<td style="text-align: left;">$\mathbf{0 . 9 4 6} \pm \mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">+ Prune</td>
<td style="text-align: center;">$0.932 \pm 0.002$</td>
<td style="text-align: left;">$\mathbf{0 . 9 4 9} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.916 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">+ Full SV</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 5} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: left;">$0.942 \pm 0.002$</td>
<td style="text-align: left;">$0.928 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">MIMIC-IV ICD-10</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: left;">Precision</td>
<td style="text-align: left;">Recall</td>
</tr>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: center;">$0.487 \pm 0.002$</td>
<td style="text-align: left;">$0.544 \pm 0.003$</td>
<td style="text-align: left;">$0.448 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">+ Omission</td>
<td style="text-align: center;">$0.517 \pm 0.003$</td>
<td style="text-align: left;">$0.553 \pm 0.003$</td>
<td style="text-align: left;">$\mathbf{0 . 5 0 1} \pm \mathbf{0 . 0 0 4}$</td>
</tr>
<tr>
<td style="text-align: left;">+ Prune</td>
<td style="text-align: center;">$0.504 \pm 0.004$</td>
<td style="text-align: left;">$\mathbf{0 . 5 5 7} \pm \mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">$0.451 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">+ Full SV</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 3} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 5 5 8} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.498 \pm 0.002$</td>
</tr>
</tbody>
</table>
<h2>4. Discussion</h2>
<p>Self-verification constitutes an important step towards unlocking the potential of LLMs in healthcare settings. As LLMs continue to generally improve in performance, clinical extraction with LLMs +SV seems likely to improve as well.</p>
<p>One limitation of SV is that it incurs a high computational cost as multiple LLM calls are chained together; however, these costs may continue to decrease as models become</p>
<p>Table 4. Evaluating evidence spans provided by the selfverification pipeline with human-annotated spans. Averaged over 3 random seeds; error bars show standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Span overlap accuracy</th>
<th style="text-align: left;">Span length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$0.93 \pm 0.02$</td>
<td style="text-align: left;">$8.20 \pm 0.48$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$0.84 \pm 0.03$</td>
<td style="text-align: left;">$7.33 \pm 0.47$</td>
</tr>
</tbody>
</table>
<p>more efficient (Dao et al., 2022). Another limitation is that LLMs and SV continue to be sensitive to prompts, increasing the need for methods to make LLMs more amenable to prompting (Ouyang et al., 2022; Scheurer et al., 2023) and to make finding strong prompts easier (Shin et al., 2020; Xu et al., 2023; Singh et al., 2022b).</p>
<p>Finally, SV can be harnessed in a variety of ways to improve clinical NLP beyond what is studied here, e.g. for studying clinical decision rules (Kornblith et al., 2022), clinical decision support systems (Liu et al., 2023), or improving model distillation (Wu et al., 2023; Toma et al., 2023).</p>
<h2>References</h2>
<p>Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., and Sontag, D. Large language models are few-shot clinical information extractors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1998-2022, 2022.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Chase, H. Langchain: Building applications with llms through composability. https://github.com/hwchase17/ langchain, 12023.</p>
<p>Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022.</p>
<p>Edin, J., Junge, A., Havtorn, J. D., Borgholt, L., Maistro, M., Ruotsalo, T., and Maaløe, L. Automated medical coding on mimic-iii and mimic-iv: A critical review and replicability study. arXiv preprint arXiv:2304.10909, 2023.</p>
<p>Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan, Y., Zhao, V. Y., Lao, N., Lee, H., Juan, D.-C., et al. Rarr: Researching and revising what language models say, using language models. arXiv preprint arXiv:2210.08726, 2022.</p>
<p>Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models, 2023.</p>
<p>Gutiérrez, B. J., McNeal, N., Washington, C., Chen, Y., Li, L., Sun, H., and Su, Y. Thinking about gpt-3 in-context learning for biomedical ie? think again. arXiv preprint arXiv:2203.08410, 2022.</p>
<p>Johnson, A., Bulgarelli, L., Pollard, T., Celi, L. A., Mark, R., and Horng IV, S. Mimic-iv-ed. PhysioNet, 2021.</p>
<p>Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L., and Mark, R. G. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1-9, 2016.</p>
<p>Jojic, A., Wang, Z., and Jojic, N. Gpt is becoming a turing machine: Here are some ways to program it, 2023.</p>
<p>Kornblith, A. E., Singh, C., Devlin, G., Addo, N., Streck, C. J., Holmes, J. F., Kuppermann, N., Grupp-Phelan, J., Fineman, J., Butte, A. J., et al. Predictability and stability testing to assess clinical decision instrument performance for children after blunt torso trauma. PLOS Digital Health, 1(8):e0000076, 2022.</p>
<p>Laursen, M., Pedersen, J., Hansen, R., Savarimuthu, T. R., and Vinholt, P. Danish clinical named entity recognition and relation extraction. In Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pp. 655-666, 2023.</p>
<p>Liu, S., Wright, A. P., Patterson, B. L., Wanderer, J. P., Turer, R. W., Nelson, S. D., McCoy, A. B., Sittig, D. F., and Wright, A. Assessing the value of chatgpt for clinical decision support optimization. MedRxiv, pp. 2023-02, 2023.</p>
<p>Lundberg, S. M. and Lee, S.-I. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.</p>
<p>Ma, Y., Cao, Y., Hong, Y., and Sun, A. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559, 2023.</p>
<p>MacNeil, S., Tran, A., Mogil, D., Bernstein, S., Ross, E., and Huang, Z. Generating diverse code explanations using the gpt-3 large language model. In Proceedings of the 2022 ACM Conference on International Computing Education ResearchVolume 2, pp. 37-39, 2022.</p>
<p>Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>Moon, S., Pakhomov, S., and Melton, G. Clinical abbreviation sense inventory. 2012.</p>
<p>Nye, B., Li, J. J., Patel, R., Yang, Y., Marshall, I. J., Nenkova, A., and Wallace, B. C. A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2018, pp. 197. NIH Public Access, 2018.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M. T. Art: Automatic multi-step reasoning and tool-use for large language models, 2023.</p>
<p>Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.</p>
<p>Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.</p>
<p>Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models, 2022.</p>
<p>Rajani, N. F., McCann, B., Xiong, C., and Socher, R. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019.</p>
<p>Rashkin, H., Nikolaev, V., Lamm, M., Aroyo, L., Collins, M., Das, D., Petrov, S., Tomar, G. S., Turc, I., and Reitter, D. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870, 2021.</p>
<p>Ribeiro, M. T., Singh, S., and Guestrin, C. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144, 2016.</p>
<p>Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence, 1(5):206-215, 2019.</p>
<p>Scheurer, J., Campos, J. A., Korbak, T., Chan, J. S., Chen, A., Cho, K., and Perez, E. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools, 2023.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.</p>
<p>Singh, C., Askari, A., Caruana, R., and Gao, J. Augmenting interpretable models with llms during training. arXiv preprint arXiv:2209.11799, 2022a.</p>
<p>Singh, C., Morris, J. X., Aneja, J., Rush, A. M., and Gao, J. Explaining patterns in data with language models via interpretable autoprompting. arXiv preprint arXiv:2210.01848, 2022b.</p>
<p>Singh, C., Hsu, A. R., Antonello, R., Jain, S., Huth, A. G., Yu, B., and Gao, J. Explaining black box text modules in natural language with language models. arXiv preprint arXiv:2305.09863, 2023.</p>
<p>Toma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B., and Wang, B. Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding. arXiv preprint arXiv:2305.12031, 2023.</p>
<p>Wang, B., Deng, X., and Sun, H. Iteratively prompt pre-trained language models for chain of thought. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2714-2730, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. emnlp-main. 174 .</p>
<p>Wang, Y., Wang, L., Rastegar-Mojarad, M., Moon, S., Shen, F., Afzal, N., Liu, S., Zeng, Y., Mehrabi, S., Sohn, S., et al. Clinical information extraction applications: a literature review. Journal of biomedical informatics, 77:34-49, 2018.</p>
<p>Wiegreffe, S. and Pinter, Y. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019.</p>
<p>Wu, C., Zhang, X., Zhang, Y., Wang, Y., and Xie, W. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023.</p>
<p>Wu, T., Terry, M., and Cai, C. J. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems, pp. 1-22, 2022.</p>
<p>Xu, B., Wang, Q., Mao, Z., Lyu, Y., She, Q., and Zhang, Y. $k$ nn prompting: Beyond-context learning with calibration-free nearest neighbor inference. arXiv preprint arXiv:2303.13824, 2023.</p>
<p>Zhang, H., Liu, X., and Zhang, J. Summit: Iterative text summarization via chatgpt. arXiv preprint arXiv:2305.14835, 2023.</p>
<p>Zweigenbaum, P., Demner-Fushman, D., Yu, H., and Cohen, K. B. Frontiers of biomedical text mining: current progress. Briefings in bioinformatics, 8(5):358-375, 2007.</p>
<h1>A. Appendix</h1>
<h2>A.1. Dataset details</h2>
<p>MIMIC To preprocess MIMIC data, we follow the steps used by (Edin et al., 2023). For MIMIC IV, we use the available discharge summaries for each patient while we retrieve more relevant sections from other types of clinical notes for MIMIC III. See the code on Github for complete details.</p>
<p>During LLM extraction, we find that directly extracting ICD codes with an LLM is difficult. Instead, we use the LLM to extract diagnoses, and then postprocess them at the end by asking the LLM to convert each diagnosis to its corresponding ICD code.</p>
<p>Clinical trial arm dataset We manually annotate the clinical trial arms from the first 100 abstract in EBM-NLP (Nye et al., 2018) without the use of any LLMs. All annotations are made available on Github. The mean number of extracted clinical trial arms is 2.14 , the maximum is 5 and the minimum is 1 .</p>
<h2>A.2. Extended extraction results</h2>
<p>Table A5. F1 scores for two tasks extracted using a single prompt which concatenates all steps in the SV pipeline. Results are slightly worse than the original extraction presented in Table 2. The prompt contains a paragraph similar to the following: Before you provide your final response: $\backslash n(1)$ Find any medications in the patient note that were missed. $\backslash n(2)$ Find evidence for each medication as a text span in the input. $\backslash n n(3)$ Verify whether each extracted medication is actually a medication and that its status is correct. Averaged over 3 random seeds; error bars are standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">ChatGPT</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clinical trial arm, original</td>
<td style="text-align: left;">$0.316 \pm 0.006$</td>
<td style="text-align: left;">$0.420 \pm 0.009$</td>
<td style="text-align: left;">$0.436 \pm 0.008$</td>
</tr>
<tr>
<td style="text-align: left;">Medication name, original</td>
<td style="text-align: left;">$0.758 \pm 0.003$</td>
<td style="text-align: left;">$0.850 \pm 0.016$</td>
<td style="text-align: left;">$0.913 \pm 0.002$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ All code is made available at $\bigcirc$ github.com/microsoft/clinical-self-verification.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
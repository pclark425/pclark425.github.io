<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-438 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-438</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-438</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-270095076</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.19221v1.pdf" target="_blank">Domain adaptation in small-scale and heterogeneous biological datasets</a></p>
                <p><strong>Paper Abstract:</strong> Machine-learning models are key to modern biology, yet models trained on one dataset are often not generalizable to other datasets from different cohorts or laboratories due to both technical and biological differences. Domain adaptation, a type of transfer learning, alleviates this problem by aligning different datasets so that models can be applied across them. However, most state-of-the-art domain adaptation methods were designed for large-scale data such as images, whereas biological datasets are smaller and have more features, and these are also complex and heterogeneous. This Review discusses domain adaptation methods in the context of such biological data to inform biologists and guide future domain adaptation research. We describe the benefits and challenges of domain adaptation in biological research and critically explore some of its objectives, strengths, and weaknesses. We argue for the incorporation of domain adaptation techniques to the computational biologist’s toolkit, with further development of customized approaches.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e438.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e438.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRECISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRECISE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-adaptation pipeline that transfers predictors of drug response trained on pre-clinical models (cell lines, PDXs) to human tumors by extracting shared factors and aligning subspaces prior to regression modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PRECISE: a domain adaptation approach to transfer predictors of drug response from pre-clinical models to tumors.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>PRECISE domain-adaptation pipeline for pharmacogenomics</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Extracts low-dimensional factors from multiple data sources (cell lines, PDXs, human tumors) using PCA, aligns subspaces across domains using geometric transformations to identify consensus (domain-invariant) gene features, and trains regression models on the aligned, consensus features to predict patient drug response; validated by concordance with known biomarker–drug associations.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / analytical pipeline (domain adaptation for prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>pre-clinical pharmacogenomics (cell lines, PDXs)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>clinical tumor gene-expression profiling (patient tumors)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (domain adaptation across biological model systems)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Used PCA factor extraction to reduce dimensionality for each source, applied geometric (subspace) alignment specifically between pre-clinical and tumor subspaces to identify consensus genes prior to regression; modifications aimed to compensate for systematic differences between in vitro/in vivo preclinical models and clinical tumor measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — reported ability to predict patient drug response and validation with known biomarker–drug associations (qualitative validation reported in cited study); no universal numeric accuracy reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Heterogeneity between pre-clinical and clinical expression distributions, differences in measurement contexts and scales, potential negative transfer if pre-clinical signals do not adapt to human tumors.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared biological signal across systems (genes/pathways), dimensionality reduction (PCA) to focus on dominant factors, geometric subspace alignment to reconcile systematic differences.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of matched or comparable gene-expression features across sources, sufficient sample sizes to extract reliable PCA factors, domain expertise to interpret consensus genes and validate biomarkers.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately generalizable within pharmacogenomics contexts where gene features overlap between model systems and tumors; method depends on biological overlap and may not generalize when joint distributions diverge strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and analytical principles (computational transfer methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e438.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AITL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Inductive Transfer Learning (AITL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial transfer-learning framework that adapts both input and output spaces to transfer predictive models from abundant preclinical datasets (e.g., cell lines) to scarce, heterogeneously labeled clinical patient data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AITL: Adversarial Inductive Transfer Learning with input and output space adaptation for pharmacogenomics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>AITL adversarial transfer for pharmacogenomics</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Uses a neural-network-based feature extractor to map source and target gene-expression data into a common feature space, trains a domain discriminator adversarially to enforce domain invariance, and employs task-specific predictors (regression for source IC50, classification for clinical outcomes) to adapt both input representation and output mapping between preclinical and clinical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / deep learning domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>pre-clinical pharmacogenomics (cell lines, clinical trials aggregated as source)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>patient gene-expression clinical datasets (drug-response classification)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (adversarial domain adaptation across biology subcontexts)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Combination of a global discriminator for input-space adaptation and separate predictive heads for disparate output spaces (regression in source vs classification in target); explicit handling of mismatched label spaces between preclinical IC50 and clinical response categories.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported as useful in cited studies (qualitative improvement over baselines described), but limited by distributional differences and heterogeneity; no aggregate numeric performance reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Heterogeneous labels and tasks between source and target (IC50 vs clinical response), large source heterogeneity across cancer types, and limited labeled patient samples.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Large preclinical datasets providing diverse training signal, architecture separating shared representation from task-specific predictors, adversarial loss encouraging domain invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of extensive preclinical data, some labeled target samples (in supervised or semi-supervised flavors), and careful architecture design to bridge different output spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Designed for pharmacogenomics transfer problems; conceptual approach generalizable to other settings with mismatched input/output spaces but requires domain-specific adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural architecture (adversarial training, multi-head predictors)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e438.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WENDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted Elastic Net Domain Adaptation (WENDA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A weighted elastic-net-based unsupervised domain-adaptation approach that prioritizes features behaving similarly across tissues to transfer DNA methylation–based age prediction models between tissues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Weighted elastic net for unsupervised domain adaptation with application to age prediction from DNA methylation data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>WENDA for DNA methylation age prediction across tissues</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Constructs a weighted elastic-net regression where feature weights reflect stability/robustness of methylation probes across source and target tissues (learned from multi-tissue source data), thereby biasing the model toward features with consistent cross-domain behavior to predict chronological age in tissue types not present in source training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / statistical domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>DNA methylation datasets across multiple tissues (epigenetics)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>DNA methylation from different tissue types (age prediction in novel tissues)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (statistical weighting to prioritize cross-domain-stable features)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td> learns feature-specific weights from many-source-tissue methylation data to guide elastic-net regularization for target tissues; treats each tissue as a separate target in applications described.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>presented as effective for cross-tissue age prediction in cited work (qualitative success reported); review does not list numeric accuracy here.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Heterogeneity of methylation patterns across tissues, potential lack of probe overlap or differential probe behavior across platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Large multi-tissue source compendium enabling estimation of feature robustness, and the elastic-net's ability to regularize in high-dimensional (many-feature) settings.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to methylation data from multiple tissues to estimate cross-tissue stability, and consistent preprocessing to ensure comparable features.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Method generalizable to other molecular assays with shared but heterogeneously behaving features (e.g., cross-tissue or cross-platform omics) where feature robustness can be estimated.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural/statistical knowledge (feature weighting and regularized regression)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e438.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DEBIAS-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DEBIAS-M</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A microbiome-specific domain-adaptation method that models and corrects sample-processing biases across studies to improve cross-study generalization of microbiome-based prediction models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Processing-bias correction with DEBIAS-M improves cross-study generalization of microbiome-based prediction models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>DEBIAS-M processing-bias correction</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Infers and models multiplicative processing biases (e.g., arising from extraction, amplification, or sequencing pipelines) that distort measured taxonomic abundances across studies, and applies an inferred correction to realign source study distributions to a target reference before downstream predictive modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data normalization and domain adaptation (field-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>microbiome sequencing studies with varying laboratory processing (metagenomics / 16S amplicon data)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>other microbiome studies / prediction tasks across cohorts (cross-study prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (field-specific bias modeling and correction)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Represents experimental variability as multiplicative bias on taxonomic abundances (informed by microbiome data-generating assumptions) and estimates per-study bias factors to re-scale abundances for alignment; uses domain knowledge of microbiome measurement processes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported to improve cross-study generalization in cited preprint (qualitative improvement described); review notes it as an example of field-specific DA addressing microbiome idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Complex, study-specific experimental pipelines, ambiguous taxonomic mappings across datasets, zero-inflation and sparsity in counts, and possible confounding between biological signal and processing biases.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Explicit modeling of known experimental bias forms (multiplicative), assumption that biases can be estimated from data distributions, and growing availability of cross-study microbiome compendia.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to sufficient cross-study data to estimate study-specific bias parameters, comparable taxonomic references or mapping strategies, and metadata about processing when available.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly tailored to microbiome data because it leverages specific measurement models; conceptual approach (modeling measurement bias) could be adapted to other assay types with well-characterized bias forms.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical knowledge combined with explicit procedural modeling (field-specific bias modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e438.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fader network (fMRI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fader networks for domain adaptation on fMRI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional autoencoder architecture that disentangles domain-specific factors to extract domain-invariant representations for multi-site fMRI aggregation and decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fader networks for domain adaptation on fMRI: ABIDE-II study.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Fader network autoencoder adaptation for multi-site fMRI</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Uses a convolutional autoencoder with a latent bottleneck in which adversarial or conditioning mechanisms force representations to be invariant to site-specific factors, enabling pooling of subjects from many sites and improving cross-subject decoding performance.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / deep learning (autoencoder-based domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>multi-site task-based or resting-state fMRI (neuroimaging)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>aggregated multi-site fMRI decoding tasks (cross-subject cognitive-state decoding, clinical classification)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (autoencoder architecture tuned to fMRI 3D/temporal data and multi-site heterogeneity)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Architecture adapted to volumetric fMRI (3D convolutions), uses domain-conditioning/adversarial components to remove site-specific signal from latent code; applied pooling across many small-site cohorts to increase effective sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported success in cited ABIDE-II application (improved decoding and aggregation across 19 sites, average per-site sample ~52); review cites these outcomes as promising for deep learning on limited per-site data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Per-site small sample sizes, varying preprocessing pipelines across sites, and complex spatiotemporal structure of fMRI requiring model architectural adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Pooling many sites to reach adequate overall sample size, convolutional architectures preserving spatial locality, and adversarial/conditioning mechanisms to suppress site-specific variance.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of many small cohorts with comparable tasks or anchors (e.g., same cognitive paradigm), consistent minimal preprocessing, and computational resources for 3D CNN training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Approach generalizable to other multi-site imaging modalities (MRI, PET) where spatial structure is relevant and domain-specific nuisances can be modeled/removed.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical knowledge (deep architecture design) and explicit procedural steps (training regimen with adversarial/conditioning losses)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e438.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperalignment / SRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperalignment and Shared Response Model (SRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Geometric and subspace-alignment methods developed in neuroimaging to map individual subjects' voxel/feature spaces into a common representational space, enabling cross-subject comparisons and pooling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hyperalignment: Modeling shared information encoded in idiosyncratic cortical topographies.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Hyperalignment / SRM for fMRI cross-subject alignment</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Hyperalignment applies orthogonal Procrustean transformations (rotations) to each subject's high-dimensional response patterns — often anchored by shared temporal stimuli — to map them into a shared high-dimensional space; SRM projects subject data into a lower-dimensional shared subspace using subject-specific basis functions and a common latent representation.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / geometric/algebraic alignment (neuroimaging)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>single-subject fMRI voxel response spaces (neuroimaging)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>group-level shared representational spaces for cross-subject analyses and decoding</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct adaptation of geometric alignment concepts within neuroimaging subcontexts (applied from single-subject modeling to cross-subject domain alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Hyperalignment assumes time-anchored shared stimuli and uses orthogonal rotations; SRM relaxes rigid-body assumptions by learning subject-specific basis projections into a lower-dimensional shared space — both tailored to fMRI's temporal and spatial structure.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful for many fMRI tasks where temporal anchors exist (e.g., movie-watching), enabling improved cross-subject decoding; SRM shown to perform well when a lower-dimensional shared representation suffices.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires common stimulation/temporal anchoring across subjects (for hyperalignment), and assumptions of linear transformability which may not hold for all brain regions or tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared experimental paradigms across subjects, sufficiently large total data to estimate transformations, and the spatial/temporal structure of fMRI that supports linear mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Temporally aligned stimuli or tasks across subjects for hyperalignment; appropriate preprocessing and region selection; sufficient data to estimate shared subspaces or transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Methods are broadly applicable within neuroimaging and conceptually to other domains where high-dimensional subject-specific measurements can be aligned via linear or low-rank transforms, but require domain-specific assumptions (e.g., temporal anchors).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles (geometric/algebraic alignment) and explicit procedural algorithms (Procrustes rotation, SRM projection)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e438.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correlation Alignment (CORAL / Deep CORAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fast alignment method that minimizes domain shift by aligning second-order statistics (covariances) between source and target feature distributions, implemented both in shallow form (CORAL) and within deep networks (Deep CORAL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep CORAL: Correlation Alignment for Deep Domain Adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>CORAL covariance-alignment</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Whitens source features and recolors them using the target covariance matrix (or minimizes covariance discrepancy in latent layers) to align second-order statistics across domains, thereby reducing domain shift prior to or during classifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / statistical alignment (domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general machine-learning feature spaces (images/text benchmarks) where covariance structure is informative</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>homogeneous biological datasets where feature identities match (examples discussed: some fMRI applications, other tabular biological features)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification for homogeneous feature spaces; adapted into deep networks (Deep CORAL) for representation layers</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Typically used as-is for homogeneous domains; in deep settings, a CORAL loss term is added to network training to align covariances in learned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>effective and extremely simple for homogeneous domain alignments; limited applicability to heterogeneous biological datasets where feature correspondences differ.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires homogeneous, comparable feature sets between domains; cannot handle non-overlapping or differently ordered features common in many biological datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simplicity, computational efficiency, and efficacy when feature identities align across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Shared feature identities and comparable preprocessing between source and target; sufficient samples to estimate covariances reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to any domain with aligned features; not generalizable to heterogeneous feature spaces without extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural/statistical knowledge (covariance alignment and loss function design)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e438.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DANN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-Adversarial Neural Network (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-reversal based adversarial neural architecture that trains feature extractors to be predictive for the source task while being domain-invariant as judged by a domain discriminator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain-adversarial training of neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>DANN adversarial domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Combines a task classifier and a domain discriminator with a gradient-reversal layer so that the feature extractor learns representations that minimize task loss while maximizing the domain discriminator's loss (i.e., making domains indistinguishable), producing domain-invariant features for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / adversarial deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning benchmarks and medical imaging (images) where DANN has been applied</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>biological datasets discussed in review (e.g., multi-site fMRI, potentially other omics) where unsupervised domain adaptation is desired</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied to biological data with architecture and loss adjustments)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Architectural adaptations for biological modalities (e.g., 3D convolutions for volumetric fMRI) and training strategies to accommodate small per-domain sample sizes; potential use in federated multi-site frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Used in cited neuroimaging and medical-imaging literature with reported improvements; review cautions DANN often requires many samples and careful tuning for biological-scale small datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Data-hungry nature of adversarial training, risk of overfitting on small biological cohorts, instability of adversarial training, and interpretability challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Powerful representation learning for nonlinear domain shifts, architecture flexibility, and demonstrated efficacy in imaging domains with sufficient data.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Sufficient aggregate sample size (often many subjects or many pooled sites), appropriate model backbone for the modality, and careful regularization to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Conceptually generalizable across modalities but practically limited in small-sample biological settings unless modified or combined with other approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles (adversarial training) and explicit procedural knowledge (network architecture and training scheme)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e438.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Cross-Subject Adaptation Decoding (DCAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, single-source heterogeneous DA approach using 3D convolutional feature extraction on volumetric fMRI to learn spatiotemporal patterns and minimize source–target discrepancy for cross-subject cognitive-state decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decoding Brain States From fMRI Signals by Using Unsupervised Domain Adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>DCAD 3D convolutional unsupervised domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Applies 3D convolution and pooling to volumetric fMRI to learn spatiotemporal features within a source domain; then employs an unsupervised domain-adaptation loss to minimize distributional discrepancy between source and target feature distributions, enabling label transfer without target annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / deep learning (unsupervised domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>task-fMRI datasets (e.g., HCP working-memory tfMRI)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>other subjects' fMRI data for cross-subject cognitive-state decoding</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (deep architecture tailored for volumetric fMRI and cross-subject heterogeneity)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Feature extractor uses 3D convolutions to capture volumetric spatiotemporal patterns; unsupervised DA module tailored to minimize distributional discrepancy specific to fMRI data characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — authors reported state-of-the-art decoding accuracies (81.9% and 84.9% for 4- and 9-state working memory decoding conditions respectively) on HCP tfMRI tasks, demonstrating effective unsupervised cross-subject adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Subject-specific variability in voxel layout and functional representation, preprocessing differences, and limited labeled target data for supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Rich spatiotemporal information in volumetric tfMRI, appropriate 3D CNN architectures, and effective unsupervised adaptation losses to reduce distributional gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to high-quality task-fMRI datasets (e.g., HCP), consistent preprocessing to reduce extraneous variability, and computational resources for 3D CNN training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly applicable within fMRI cross-subject decoding tasks and potentially adaptable to other volumetric imaging modalities with analogous spatiotemporal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical knowledge (deep-network architecture and unsupervised adaptation losses) and explicit procedural steps</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e438.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e438.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FEDA (and heterogeneous extension)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frustratingly Easy Domain Adaptation (FEDA) and heterogeneous extension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple feature-augmentation DA strategy that duplicates features into domain-specific and shared components; extensions add projection matrices to handle heterogeneous feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Frustratingly Easy Domain Adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>FEDA feature augmentation and heterogeneous-projection extension</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>FEDA augments each sample's feature vector into concatenated blocks representing (shared, source-only, target-only) components (e.g., [x, x, 0] for source), enabling a classifier to learn shared vs domain-specific weights; heterogeneous extensions introduce projection matrices P and Q to map differing feature spaces into compatible augmented representations.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / feature engineering for domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>text/image ML benchmarks (original), applied/considered for biological datasets in review</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>homogeneous biological datasets (if standard FEDA) or heterogeneous biological datasets using projection-extension (e.g., multi-cohort biological omics/fMRI with differing features)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (heterogeneous extension with projection matrices for biological data)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>For biological application, dimensionality reduction (e.g., PCA) is often applied prior to feature augmentation to reduce feature explosion; heterogeneous extension uses learned projection matrices P and Q to map between non-identical feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>mixed — naive application to biological high-dimensional data can exacerbate dimensionality problems; Schneider et al. found PCA + FEDA produced similar performance to simple concatenation in a cited evaluation, suggesting limited benefit without careful modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Biological datasets' high dimensionality makes feature duplication problematic (curse of dimensionality), and original FEDA assumes homogeneous feature spaces which many biological datasets violate.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simplicity of implementation and potential effectiveness when used with dimensionality reduction or appropriate projection mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Either homogeneous feature spaces or credible projection mappings between heterogeneous feature sets; careful feature-reduction to control dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable conceptually but requires significant modification (projections, reduction) to be practical on high-dimensional heterogeneous biological data.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (feature augmentation schema) and instrumental know-how (when and how to reduce features or learn projections)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Domain adaptation in small-scale and heterogeneous biological datasets', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PRECISE: a domain adaptation approach to transfer predictors of drug response from pre-clinical models to tumors. <em>(Rating: 2)</em></li>
                <li>AITL: Adversarial Inductive Transfer Learning with input and output space adaptation for pharmacogenomics. <em>(Rating: 2)</em></li>
                <li>Weighted elastic net for unsupervised domain adaptation with application to age prediction from DNA methylation data. <em>(Rating: 2)</em></li>
                <li>Processing-bias correction with DEBIAS-M improves cross-study generalization of microbiome-based prediction models. <em>(Rating: 2)</em></li>
                <li>Fader networks for domain adaptation on fMRI: ABIDE-II study. <em>(Rating: 2)</em></li>
                <li>Hyperalignment: Modeling shared information encoded in idiosyncratic cortical topographies. <em>(Rating: 2)</em></li>
                <li>Decoding Brain States From fMRI Signals by Using Unsupervised Domain Adaptation. <em>(Rating: 2)</em></li>
                <li>Domain-adversarial training of neural networks <em>(Rating: 2)</em></li>
                <li>Deep CORAL: Correlation Alignment for Deep Domain Adaptation. <em>(Rating: 1)</em></li>
                <li>Frustratingly Easy Domain Adaptation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-438",
    "paper_id": "paper-270095076",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "PRECISE",
            "name_full": "PRECISE",
            "brief_description": "A domain-adaptation pipeline that transfers predictors of drug response trained on pre-clinical models (cell lines, PDXs) to human tumors by extracting shared factors and aligning subspaces prior to regression modeling.",
            "citation_title": "PRECISE: a domain adaptation approach to transfer predictors of drug response from pre-clinical models to tumors.",
            "mention_or_use": "mention",
            "procedure_name": "PRECISE domain-adaptation pipeline for pharmacogenomics",
            "procedure_description": "Extracts low-dimensional factors from multiple data sources (cell lines, PDXs, human tumors) using PCA, aligns subspaces across domains using geometric transformations to identify consensus (domain-invariant) gene features, and trains regression models on the aligned, consensus features to predict patient drug response; validated by concordance with known biomarker–drug associations.",
            "procedure_type": "computational method / analytical pipeline (domain adaptation for prediction)",
            "source_domain": "pre-clinical pharmacogenomics (cell lines, PDXs)",
            "target_domain": "clinical tumor gene-expression profiling (patient tumors)",
            "transfer_type": "adapted/modified for new context (domain adaptation across biological model systems)",
            "modifications_made": "Used PCA factor extraction to reduce dimensionality for each source, applied geometric (subspace) alignment specifically between pre-clinical and tumor subspaces to identify consensus genes prior to regression; modifications aimed to compensate for systematic differences between in vitro/in vivo preclinical models and clinical tumor measurements.",
            "transfer_success": "partially successful — reported ability to predict patient drug response and validation with known biomarker–drug associations (qualitative validation reported in cited study); no universal numeric accuracy reported in this review.",
            "barriers_encountered": "Heterogeneity between pre-clinical and clinical expression distributions, differences in measurement contexts and scales, potential negative transfer if pre-clinical signals do not adapt to human tumors.",
            "facilitating_factors": "Shared biological signal across systems (genes/pathways), dimensionality reduction (PCA) to focus on dominant factors, geometric subspace alignment to reconcile systematic differences.",
            "contextual_requirements": "Availability of matched or comparable gene-expression features across sources, sufficient sample sizes to extract reliable PCA factors, domain expertise to interpret consensus genes and validate biomarkers.",
            "generalizability": "Moderately generalizable within pharmacogenomics contexts where gene features overlap between model systems and tumors; method depends on biological overlap and may not generalize when joint distributions diverge strongly.",
            "knowledge_type": "explicit procedural steps and analytical principles (computational transfer methodology)",
            "uuid": "e438.0",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AITL",
            "name_full": "Adversarial Inductive Transfer Learning (AITL)",
            "brief_description": "An adversarial transfer-learning framework that adapts both input and output spaces to transfer predictive models from abundant preclinical datasets (e.g., cell lines) to scarce, heterogeneously labeled clinical patient data.",
            "citation_title": "AITL: Adversarial Inductive Transfer Learning with input and output space adaptation for pharmacogenomics.",
            "mention_or_use": "mention",
            "procedure_name": "AITL adversarial transfer for pharmacogenomics",
            "procedure_description": "Uses a neural-network-based feature extractor to map source and target gene-expression data into a common feature space, trains a domain discriminator adversarially to enforce domain invariance, and employs task-specific predictors (regression for source IC50, classification for clinical outcomes) to adapt both input representation and output mapping between preclinical and clinical tasks.",
            "procedure_type": "computational method / deep learning domain adaptation",
            "source_domain": "pre-clinical pharmacogenomics (cell lines, clinical trials aggregated as source)",
            "target_domain": "patient gene-expression clinical datasets (drug-response classification)",
            "transfer_type": "adapted/modified for new context (adversarial domain adaptation across biology subcontexts)",
            "modifications_made": "Combination of a global discriminator for input-space adaptation and separate predictive heads for disparate output spaces (regression in source vs classification in target); explicit handling of mismatched label spaces between preclinical IC50 and clinical response categories.",
            "transfer_success": "reported as useful in cited studies (qualitative improvement over baselines described), but limited by distributional differences and heterogeneity; no aggregate numeric performance reported in this review.",
            "barriers_encountered": "Heterogeneous labels and tasks between source and target (IC50 vs clinical response), large source heterogeneity across cancer types, and limited labeled patient samples.",
            "facilitating_factors": "Large preclinical datasets providing diverse training signal, architecture separating shared representation from task-specific predictors, adversarial loss encouraging domain invariance.",
            "contextual_requirements": "Availability of extensive preclinical data, some labeled target samples (in supervised or semi-supervised flavors), and careful architecture design to bridge different output spaces.",
            "generalizability": "Designed for pharmacogenomics transfer problems; conceptual approach generalizable to other settings with mismatched input/output spaces but requires domain-specific adaptation.",
            "knowledge_type": "theoretical principles and explicit procedural architecture (adversarial training, multi-head predictors)",
            "uuid": "e438.1",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "WENDA",
            "name_full": "Weighted Elastic Net Domain Adaptation (WENDA)",
            "brief_description": "A weighted elastic-net-based unsupervised domain-adaptation approach that prioritizes features behaving similarly across tissues to transfer DNA methylation–based age prediction models between tissues.",
            "citation_title": "Weighted elastic net for unsupervised domain adaptation with application to age prediction from DNA methylation data.",
            "mention_or_use": "mention",
            "procedure_name": "WENDA for DNA methylation age prediction across tissues",
            "procedure_description": "Constructs a weighted elastic-net regression where feature weights reflect stability/robustness of methylation probes across source and target tissues (learned from multi-tissue source data), thereby biasing the model toward features with consistent cross-domain behavior to predict chronological age in tissue types not present in source training.",
            "procedure_type": "computational method / statistical domain adaptation",
            "source_domain": "DNA methylation datasets across multiple tissues (epigenetics)",
            "target_domain": "DNA methylation from different tissue types (age prediction in novel tissues)",
            "transfer_type": "adapted/modified for new context (statistical weighting to prioritize cross-domain-stable features)",
            "modifications_made": " learns feature-specific weights from many-source-tissue methylation data to guide elastic-net regularization for target tissues; treats each tissue as a separate target in applications described.",
            "transfer_success": "presented as effective for cross-tissue age prediction in cited work (qualitative success reported); review does not list numeric accuracy here.",
            "barriers_encountered": "Heterogeneity of methylation patterns across tissues, potential lack of probe overlap or differential probe behavior across platforms.",
            "facilitating_factors": "Large multi-tissue source compendium enabling estimation of feature robustness, and the elastic-net's ability to regularize in high-dimensional (many-feature) settings.",
            "contextual_requirements": "Access to methylation data from multiple tissues to estimate cross-tissue stability, and consistent preprocessing to ensure comparable features.",
            "generalizability": "Method generalizable to other molecular assays with shared but heterogeneously behaving features (e.g., cross-tissue or cross-platform omics) where feature robustness can be estimated.",
            "knowledge_type": "explicit procedural/statistical knowledge (feature weighting and regularized regression)",
            "uuid": "e438.2",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DEBIAS-M",
            "name_full": "DEBIAS-M",
            "brief_description": "A microbiome-specific domain-adaptation method that models and corrects sample-processing biases across studies to improve cross-study generalization of microbiome-based prediction models.",
            "citation_title": "Processing-bias correction with DEBIAS-M improves cross-study generalization of microbiome-based prediction models.",
            "mention_or_use": "mention",
            "procedure_name": "DEBIAS-M processing-bias correction",
            "procedure_description": "Infers and models multiplicative processing biases (e.g., arising from extraction, amplification, or sequencing pipelines) that distort measured taxonomic abundances across studies, and applies an inferred correction to realign source study distributions to a target reference before downstream predictive modeling.",
            "procedure_type": "computational method / data normalization and domain adaptation (field-specific)",
            "source_domain": "microbiome sequencing studies with varying laboratory processing (metagenomics / 16S amplicon data)",
            "target_domain": "other microbiome studies / prediction tasks across cohorts (cross-study prediction)",
            "transfer_type": "adapted/modified for new context (field-specific bias modeling and correction)",
            "modifications_made": "Represents experimental variability as multiplicative bias on taxonomic abundances (informed by microbiome data-generating assumptions) and estimates per-study bias factors to re-scale abundances for alignment; uses domain knowledge of microbiome measurement processes.",
            "transfer_success": "reported to improve cross-study generalization in cited preprint (qualitative improvement described); review notes it as an example of field-specific DA addressing microbiome idiosyncrasies.",
            "barriers_encountered": "Complex, study-specific experimental pipelines, ambiguous taxonomic mappings across datasets, zero-inflation and sparsity in counts, and possible confounding between biological signal and processing biases.",
            "facilitating_factors": "Explicit modeling of known experimental bias forms (multiplicative), assumption that biases can be estimated from data distributions, and growing availability of cross-study microbiome compendia.",
            "contextual_requirements": "Access to sufficient cross-study data to estimate study-specific bias parameters, comparable taxonomic references or mapping strategies, and metadata about processing when available.",
            "generalizability": "Highly tailored to microbiome data because it leverages specific measurement models; conceptual approach (modeling measurement bias) could be adapted to other assay types with well-characterized bias forms.",
            "knowledge_type": "instrumental/technical knowledge combined with explicit procedural modeling (field-specific bias modeling)",
            "uuid": "e438.3",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Fader network (fMRI)",
            "name_full": "Fader networks for domain adaptation on fMRI",
            "brief_description": "A convolutional autoencoder architecture that disentangles domain-specific factors to extract domain-invariant representations for multi-site fMRI aggregation and decoding.",
            "citation_title": "Fader networks for domain adaptation on fMRI: ABIDE-II study.",
            "mention_or_use": "mention",
            "procedure_name": "Fader network autoencoder adaptation for multi-site fMRI",
            "procedure_description": "Uses a convolutional autoencoder with a latent bottleneck in which adversarial or conditioning mechanisms force representations to be invariant to site-specific factors, enabling pooling of subjects from many sites and improving cross-subject decoding performance.",
            "procedure_type": "computational method / deep learning (autoencoder-based domain adaptation)",
            "source_domain": "multi-site task-based or resting-state fMRI (neuroimaging)",
            "target_domain": "aggregated multi-site fMRI decoding tasks (cross-subject cognitive-state decoding, clinical classification)",
            "transfer_type": "adapted/modified for new context (autoencoder architecture tuned to fMRI 3D/temporal data and multi-site heterogeneity)",
            "modifications_made": "Architecture adapted to volumetric fMRI (3D convolutions), uses domain-conditioning/adversarial components to remove site-specific signal from latent code; applied pooling across many small-site cohorts to increase effective sample size.",
            "transfer_success": "reported success in cited ABIDE-II application (improved decoding and aggregation across 19 sites, average per-site sample ~52); review cites these outcomes as promising for deep learning on limited per-site data.",
            "barriers_encountered": "Per-site small sample sizes, varying preprocessing pipelines across sites, and complex spatiotemporal structure of fMRI requiring model architectural adaptations.",
            "facilitating_factors": "Pooling many sites to reach adequate overall sample size, convolutional architectures preserving spatial locality, and adversarial/conditioning mechanisms to suppress site-specific variance.",
            "contextual_requirements": "Availability of many small cohorts with comparable tasks or anchors (e.g., same cognitive paradigm), consistent minimal preprocessing, and computational resources for 3D CNN training.",
            "generalizability": "Approach generalizable to other multi-site imaging modalities (MRI, PET) where spatial structure is relevant and domain-specific nuisances can be modeled/removed.",
            "knowledge_type": "instrumental/technical knowledge (deep architecture design) and explicit procedural steps (training regimen with adversarial/conditioning losses)",
            "uuid": "e438.4",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Hyperalignment / SRM",
            "name_full": "Hyperalignment and Shared Response Model (SRM)",
            "brief_description": "Geometric and subspace-alignment methods developed in neuroimaging to map individual subjects' voxel/feature spaces into a common representational space, enabling cross-subject comparisons and pooling.",
            "citation_title": "Hyperalignment: Modeling shared information encoded in idiosyncratic cortical topographies.",
            "mention_or_use": "mention",
            "procedure_name": "Hyperalignment / SRM for fMRI cross-subject alignment",
            "procedure_description": "Hyperalignment applies orthogonal Procrustean transformations (rotations) to each subject's high-dimensional response patterns — often anchored by shared temporal stimuli — to map them into a shared high-dimensional space; SRM projects subject data into a lower-dimensional shared subspace using subject-specific basis functions and a common latent representation.",
            "procedure_type": "computational method / geometric/algebraic alignment (neuroimaging)",
            "source_domain": "single-subject fMRI voxel response spaces (neuroimaging)",
            "target_domain": "group-level shared representational spaces for cross-subject analyses and decoding",
            "transfer_type": "direct adaptation of geometric alignment concepts within neuroimaging subcontexts (applied from single-subject modeling to cross-subject domain alignment)",
            "modifications_made": "Hyperalignment assumes time-anchored shared stimuli and uses orthogonal rotations; SRM relaxes rigid-body assumptions by learning subject-specific basis projections into a lower-dimensional shared space — both tailored to fMRI's temporal and spatial structure.",
            "transfer_success": "successful for many fMRI tasks where temporal anchors exist (e.g., movie-watching), enabling improved cross-subject decoding; SRM shown to perform well when a lower-dimensional shared representation suffices.",
            "barriers_encountered": "Requires common stimulation/temporal anchoring across subjects (for hyperalignment), and assumptions of linear transformability which may not hold for all brain regions or tasks.",
            "facilitating_factors": "Shared experimental paradigms across subjects, sufficiently large total data to estimate transformations, and the spatial/temporal structure of fMRI that supports linear mappings.",
            "contextual_requirements": "Temporally aligned stimuli or tasks across subjects for hyperalignment; appropriate preprocessing and region selection; sufficient data to estimate shared subspaces or transforms.",
            "generalizability": "Methods are broadly applicable within neuroimaging and conceptually to other domains where high-dimensional subject-specific measurements can be aligned via linear or low-rank transforms, but require domain-specific assumptions (e.g., temporal anchors).",
            "knowledge_type": "theoretical principles (geometric/algebraic alignment) and explicit procedural algorithms (Procrustes rotation, SRM projection)",
            "uuid": "e438.5",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CORAL",
            "name_full": "Correlation Alignment (CORAL / Deep CORAL)",
            "brief_description": "A fast alignment method that minimizes domain shift by aligning second-order statistics (covariances) between source and target feature distributions, implemented both in shallow form (CORAL) and within deep networks (Deep CORAL).",
            "citation_title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation.",
            "mention_or_use": "mention",
            "procedure_name": "CORAL covariance-alignment",
            "procedure_description": "Whitens source features and recolors them using the target covariance matrix (or minimizes covariance discrepancy in latent layers) to align second-order statistics across domains, thereby reducing domain shift prior to or during classifier training.",
            "procedure_type": "computational method / statistical alignment (domain adaptation)",
            "source_domain": "general machine-learning feature spaces (images/text benchmarks) where covariance structure is informative",
            "target_domain": "homogeneous biological datasets where feature identities match (examples discussed: some fMRI applications, other tabular biological features)",
            "transfer_type": "direct application without modification for homogeneous feature spaces; adapted into deep networks (Deep CORAL) for representation layers",
            "modifications_made": "Typically used as-is for homogeneous domains; in deep settings, a CORAL loss term is added to network training to align covariances in learned representations.",
            "transfer_success": "effective and extremely simple for homogeneous domain alignments; limited applicability to heterogeneous biological datasets where feature correspondences differ.",
            "barriers_encountered": "Requires homogeneous, comparable feature sets between domains; cannot handle non-overlapping or differently ordered features common in many biological datasets.",
            "facilitating_factors": "Simplicity, computational efficiency, and efficacy when feature identities align across domains.",
            "contextual_requirements": "Shared feature identities and comparable preprocessing between source and target; sufficient samples to estimate covariances reliably.",
            "generalizability": "Generalizable to any domain with aligned features; not generalizable to heterogeneous feature spaces without extensions.",
            "knowledge_type": "explicit procedural/statistical knowledge (covariance alignment and loss function design)",
            "uuid": "e438.6",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DANN",
            "name_full": "Domain-Adversarial Neural Network (DANN)",
            "brief_description": "A gradient-reversal based adversarial neural architecture that trains feature extractors to be predictive for the source task while being domain-invariant as judged by a domain discriminator.",
            "citation_title": "Domain-adversarial training of neural networks",
            "mention_or_use": "mention",
            "procedure_name": "DANN adversarial domain adaptation",
            "procedure_description": "Combines a task classifier and a domain discriminator with a gradient-reversal layer so that the feature extractor learns representations that minimize task loss while maximizing the domain discriminator's loss (i.e., making domains indistinguishable), producing domain-invariant features for transfer.",
            "procedure_type": "computational method / adversarial deep learning",
            "source_domain": "machine learning benchmarks and medical imaging (images) where DANN has been applied",
            "target_domain": "biological datasets discussed in review (e.g., multi-site fMRI, potentially other omics) where unsupervised domain adaptation is desired",
            "transfer_type": "adapted/modified for new context (applied to biological data with architecture and loss adjustments)",
            "modifications_made": "Architectural adaptations for biological modalities (e.g., 3D convolutions for volumetric fMRI) and training strategies to accommodate small per-domain sample sizes; potential use in federated multi-site frameworks.",
            "transfer_success": "Used in cited neuroimaging and medical-imaging literature with reported improvements; review cautions DANN often requires many samples and careful tuning for biological-scale small datasets.",
            "barriers_encountered": "Data-hungry nature of adversarial training, risk of overfitting on small biological cohorts, instability of adversarial training, and interpretability challenges.",
            "facilitating_factors": "Powerful representation learning for nonlinear domain shifts, architecture flexibility, and demonstrated efficacy in imaging domains with sufficient data.",
            "contextual_requirements": "Sufficient aggregate sample size (often many subjects or many pooled sites), appropriate model backbone for the modality, and careful regularization to avoid overfitting.",
            "generalizability": "Conceptually generalizable across modalities but practically limited in small-sample biological settings unless modified or combined with other approaches.",
            "knowledge_type": "theoretical principles (adversarial training) and explicit procedural knowledge (network architecture and training scheme)",
            "uuid": "e438.7",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DCAD",
            "name_full": "Deep Cross-Subject Adaptation Decoding (DCAD)",
            "brief_description": "An unsupervised, single-source heterogeneous DA approach using 3D convolutional feature extraction on volumetric fMRI to learn spatiotemporal patterns and minimize source–target discrepancy for cross-subject cognitive-state decoding.",
            "citation_title": "Decoding Brain States From fMRI Signals by Using Unsupervised Domain Adaptation.",
            "mention_or_use": "mention",
            "procedure_name": "DCAD 3D convolutional unsupervised domain adaptation",
            "procedure_description": "Applies 3D convolution and pooling to volumetric fMRI to learn spatiotemporal features within a source domain; then employs an unsupervised domain-adaptation loss to minimize distributional discrepancy between source and target feature distributions, enabling label transfer without target annotations.",
            "procedure_type": "computational method / deep learning (unsupervised domain adaptation)",
            "source_domain": "task-fMRI datasets (e.g., HCP working-memory tfMRI)",
            "target_domain": "other subjects' fMRI data for cross-subject cognitive-state decoding",
            "transfer_type": "adapted/modified for new context (deep architecture tailored for volumetric fMRI and cross-subject heterogeneity)",
            "modifications_made": "Feature extractor uses 3D convolutions to capture volumetric spatiotemporal patterns; unsupervised DA module tailored to minimize distributional discrepancy specific to fMRI data characteristics.",
            "transfer_success": "successful — authors reported state-of-the-art decoding accuracies (81.9% and 84.9% for 4- and 9-state working memory decoding conditions respectively) on HCP tfMRI tasks, demonstrating effective unsupervised cross-subject adaptation.",
            "barriers_encountered": "Subject-specific variability in voxel layout and functional representation, preprocessing differences, and limited labeled target data for supervised fine-tuning.",
            "facilitating_factors": "Rich spatiotemporal information in volumetric tfMRI, appropriate 3D CNN architectures, and effective unsupervised adaptation losses to reduce distributional gaps.",
            "contextual_requirements": "Access to high-quality task-fMRI datasets (e.g., HCP), consistent preprocessing to reduce extraneous variability, and computational resources for 3D CNN training.",
            "generalizability": "Highly applicable within fMRI cross-subject decoding tasks and potentially adaptable to other volumetric imaging modalities with analogous spatiotemporal structure.",
            "knowledge_type": "instrumental/technical knowledge (deep-network architecture and unsupervised adaptation losses) and explicit procedural steps",
            "uuid": "e438.8",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "FEDA (and heterogeneous extension)",
            "name_full": "Frustratingly Easy Domain Adaptation (FEDA) and heterogeneous extension",
            "brief_description": "A simple feature-augmentation DA strategy that duplicates features into domain-specific and shared components; extensions add projection matrices to handle heterogeneous feature spaces.",
            "citation_title": "Frustratingly Easy Domain Adaptation.",
            "mention_or_use": "mention",
            "procedure_name": "FEDA feature augmentation and heterogeneous-projection extension",
            "procedure_description": "FEDA augments each sample's feature vector into concatenated blocks representing (shared, source-only, target-only) components (e.g., [x, x, 0] for source), enabling a classifier to learn shared vs domain-specific weights; heterogeneous extensions introduce projection matrices P and Q to map differing feature spaces into compatible augmented representations.",
            "procedure_type": "computational method / feature engineering for domain adaptation",
            "source_domain": "text/image ML benchmarks (original), applied/considered for biological datasets in review",
            "target_domain": "homogeneous biological datasets (if standard FEDA) or heterogeneous biological datasets using projection-extension (e.g., multi-cohort biological omics/fMRI with differing features)",
            "transfer_type": "adapted/modified for new context (heterogeneous extension with projection matrices for biological data)",
            "modifications_made": "For biological application, dimensionality reduction (e.g., PCA) is often applied prior to feature augmentation to reduce feature explosion; heterogeneous extension uses learned projection matrices P and Q to map between non-identical feature spaces.",
            "transfer_success": "mixed — naive application to biological high-dimensional data can exacerbate dimensionality problems; Schneider et al. found PCA + FEDA produced similar performance to simple concatenation in a cited evaluation, suggesting limited benefit without careful modeling.",
            "barriers_encountered": "Biological datasets' high dimensionality makes feature duplication problematic (curse of dimensionality), and original FEDA assumes homogeneous feature spaces which many biological datasets violate.",
            "facilitating_factors": "Simplicity of implementation and potential effectiveness when used with dimensionality reduction or appropriate projection mappings.",
            "contextual_requirements": "Either homogeneous feature spaces or credible projection mappings between heterogeneous feature sets; careful feature-reduction to control dimensionality.",
            "generalizability": "Generalizable conceptually but requires significant modification (projections, reduction) to be practical on high-dimensional heterogeneous biological data.",
            "knowledge_type": "explicit procedural steps (feature augmentation schema) and instrumental know-how (when and how to reduce features or learn projections)",
            "uuid": "e438.9",
            "source_info": {
                "paper_title": "Domain adaptation in small-scale and heterogeneous biological datasets",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PRECISE: a domain adaptation approach to transfer predictors of drug response from pre-clinical models to tumors.",
            "rating": 2,
            "sanitized_title": "precise_a_domain_adaptation_approach_to_transfer_predictors_of_drug_response_from_preclinical_models_to_tumors"
        },
        {
            "paper_title": "AITL: Adversarial Inductive Transfer Learning with input and output space adaptation for pharmacogenomics.",
            "rating": 2,
            "sanitized_title": "aitl_adversarial_inductive_transfer_learning_with_input_and_output_space_adaptation_for_pharmacogenomics"
        },
        {
            "paper_title": "Weighted elastic net for unsupervised domain adaptation with application to age prediction from DNA methylation data.",
            "rating": 2,
            "sanitized_title": "weighted_elastic_net_for_unsupervised_domain_adaptation_with_application_to_age_prediction_from_dna_methylation_data"
        },
        {
            "paper_title": "Processing-bias correction with DEBIAS-M improves cross-study generalization of microbiome-based prediction models.",
            "rating": 2,
            "sanitized_title": "processingbias_correction_with_debiasm_improves_crossstudy_generalization_of_microbiomebased_prediction_models"
        },
        {
            "paper_title": "Fader networks for domain adaptation on fMRI: ABIDE-II study.",
            "rating": 2,
            "sanitized_title": "fader_networks_for_domain_adaptation_on_fmri_abideii_study"
        },
        {
            "paper_title": "Hyperalignment: Modeling shared information encoded in idiosyncratic cortical topographies.",
            "rating": 2,
            "sanitized_title": "hyperalignment_modeling_shared_information_encoded_in_idiosyncratic_cortical_topographies"
        },
        {
            "paper_title": "Decoding Brain States From fMRI Signals by Using Unsupervised Domain Adaptation.",
            "rating": 2,
            "sanitized_title": "decoding_brain_states_from_fmri_signals_by_using_unsupervised_domain_adaptation"
        },
        {
            "paper_title": "Domain-adversarial training of neural networks",
            "rating": 2,
            "sanitized_title": "domainadversarial_training_of_neural_networks"
        },
        {
            "paper_title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation.",
            "rating": 1,
            "sanitized_title": "deep_coral_correlation_alignment_for_deep_domain_adaptation"
        },
        {
            "paper_title": "Frustratingly Easy Domain Adaptation.",
            "rating": 1,
            "sanitized_title": "frustratingly_easy_domain_adaptation"
        }
    ],
    "cost": 0.02346125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Domain adaptation in small-scale and heterogeneous biological datasets</p>
<p>Seyedmehdi Orouji 
Department of Cognitive Sciences
University of California Irvine
IrvineCA</p>
<p>Martin C Liu 
Department of Biomedical Informatics
Columbia University Irving Medical Center
New YorkNY</p>
<p>Program for Mathematical Genomics
Department of Systems Biology
Columbia University Irving Medical Center
New YorkNY</p>
<p>Tal Korem tal.korem@columbia.edu 
Program for Mathematical Genomics
Department of Systems Biology
Columbia University Irving Medical Center
New YorkNY</p>
<p>Department of Obstetrics and Gynecology
Columbia University Irving Medical Center
New YorkNY</p>
<p>CIFAR Azrieli Global Scholars Program
CIFAR
TorontoCanada</p>
<p>Megan A K Peters megan.peters@uci.edu 
Department of Cognitive Sciences
University of California Irvine
IrvineCA</p>
<p>CIFAR Azrieli Global Scholars Program
CIFAR
TorontoCanada</p>
<p>CIFAR Fellow, Program in Brain, Mind, &amp; Consciousness
CIFAR
TorontoCanada</p>
<p>Domain adaptation in small-scale and heterogeneous biological datasets
DD524A8EE05147C3BFD517086FB57CF9Machine learningbiological-scale datasetssmall datasetsneuroimagingmicrobiomedomain adaptationtransfer learning
Machine learning techniques are steadily becoming more important in modern biology, and are used to build predictive models, discover patterns, and investigate biological problems.However, models trained on one dataset are often not generalizable to other datasets from different cohorts or laboratories, due to differences in the statistical properties of these datasets.These could stem from technical differences, such as the measurement technique used, or from relevant biological differences between the populations studied.Domain adaptation, a type of transfer learning, can alleviate this problem by aligning the statistical distributions of features and samples among different datasets so that similar models can be applied across them.However, a majority of state-of-the-art domain adaptation methods are designed to work with large-scale data, mostly text and images, while biological datasets often suffer from small sample sizes, and possess complexities such as heterogeneity of the feature space.This Review aims to synthetically discuss domain adaptation methods in the context of small-scale and highly heterogeneous biological data.We describe the benefits and challenges of domain adaptation in biological research and critically discuss some of its objectives, strengths, and weaknesses through key representative methodologies.We argue for the incorporation of domain adaptation techniques to the computational biologist's toolkit, with further development of customized approaches.</p>
<p>Introduction</p>
<p>In the computational biological sciences, we are interested in learning informative "truths" about biological systems through machine learning or similar quantitative modeling techniques ( 1 ) .Contrary to "purely statistical" correlations, we expect such "truths" to generalize beyond a specific dataset or population, indicating that they offer a grounded biological meaning.However, collecting (and sometimes labeling) biological datasets is difficult, expensive, and time-consuming, leading to many small but related datasets which are collected from different sources and under different environmental and experimental conditions (e.g.different labs, equipment, settings, humidity, etc).For example, in the widely used Autism Brain Imaging Dataset (ABIDE), functional magnetic resonance imaging (fMRI) data was collected at multiple sites, which hindered the ability to directly aggregate data ( 2 ) .Beyond creating challenges in data curation and metadata standards ( 3 , 4 ) , this variability in the sources of small biological datasets creates different domains of data that have different statistical distributions.</p>
<p>While this variety is a strength that can facilitate discovery of generalizable truths, it also presents a significant challenge to computational biology: Applying knowledge gained from one dataset (a source ) to another (a target ) will fail if the two datasets possess highly divergent distributions -a phenomenon known as domain shift or data bias (5,6) .In short, we cannot blindly apply a model (of any kind) trained on a source dataset collected under one set of conditions to new target data and expect it to perform effectively.In an age of open datasets and keen interest in adhering to FAIR principles (Findability, Accessibility, Interoperability, and Reuse of digital assets) to accelerate scientific discovery, it is increasingly urgent that we acknowledge the strengths and challenges of combining datasets.</p>
<p>To best extract generalizable insights while making use of all collected data from varying sources -especially in biological disciplines where data are expensive -and to apply these insights to newly collected data, we must discover how to best leverage the use of all existing and continuously growing small biological datasets ( 7 ) .In the field of machine learning, transfer learning aims to use knowledge gained from learning a task on one dataset to performing a similar task on a different but related dataset, with the purpose of transferring knowledge across datasets ( 8 -12 ) .Domain adaptation (DA), a subfield of transfer learning, has been developed to address this issue of different statistical distributions by aligning the distributions of the source and target domains.Of note, while there are some similarities to "batch correction" often applied in high-throughput molecular measurements ( 13 , 14 ) , the objective is different: domain adaptation aims to learn generalizable models across domains, while batch correction is primarily aimed at removing technical variation.Importantly, DA is more than just "lining up the features" and training a model on both datasets; not only is this often impossible to do (especially if features are unlabeled), but statistical differences between the domains can often guarantee that such a brute force aggregation is doomed to failure.Instead, through DA, a model is forced to learn domain invariant features, i.e. features that are common across all domains, such that the learned model can be generalized and perform relatively well on a separate target domain.Another benefit of DA is that the integration of multiple datasets effectively increases the sample size, allowing for improved inference of statistical signals.This allows better use of available data and resources, reducing the need to collect and annotate expensive data ( 15 -17 ) .</p>
<p>However, using DA methods to extract informative and generalizable insights from different datasets is difficult in general, and is particularly difficult in computational biology.Compared to datasets typically used to train machine learning models ( 18 -21 ) , many "biological-scale" datasets are smaller in sample size, have many more features than samples, and have a complicated feature space (e.g.different numbers of features in each dataset, missing values, etc.).Therefore, while developing effective DA techniques that can work well with these small "biological scale" datasets to find general truths about biological systems is highly desirable, it presents a specific set of challenges to machine learning research.</p>
<p>In this Review, we aim to critically discuss the benefits and challenges of applying current DA methodologies and frameworks to such biological datasets.To this end, we use the token examples of functional magnetic resonance imaging (fMRI) and microbiome datasets, two seemingly different disciplines in biology, to show the common considerations critical to developing effective DA techniques in such data.Our goal is to lay out the key components that require consideration in selecting an effective DA technique, and highlight important areas of future methodological research in DA methods that can be maximally effective in biological datasets -especially as data sharing and metadata curation continues to mature.</p>
<p>Domain adaptation: a powerful tool for biological data</p>
<p>In the biological sciences -and especially as re-analysis and meta-analysis is facilitated through open data sharing -researchers often work with multiple distinct datasets collected through various procedures and techniques.These datasets may contain unique idiosyncrasies that are specific to a dataset, and which may or may not necessarily offer any biological insights (for example, different MRI participants or scanners ( 2 , 22 -24 ) , or different patient populations for microbiome profiling ( 25 , 26 ) ).Additionally, each dataset alone may have high feature dimensionality despite small sample size, and thus may be overfit by modern, state of the art models ( 27 -30 ) -making it challenging to learn robust models that will generalize.These factors make it particularly attractive to apply DA to aggregate such biological datasets, reducing overfitting and facilitating the discovery of "generalizable truths".Before assessing the challenges of doing so, we would like to briefly examine three specific benefits of DA for biological research.</p>
<p>Mitigating small sample size and large feature space</p>
<p>Ideally, a successful approach in computational biology is to fit a model with few free parameters across many samples.However, complex biological systems often need to be modeled with many free parameters, while training samples remain quite few.This degree of model complexity in the face of insufficient training exemplars can reduce generalizability and increase the risk of overfitting, where a machine learning model fits the training data all too well but fails to generalize to new, unseen data (e.g., cross-validation fails).To address this issue, domain adaptation (DA) can be used to integrate multiple individual datasets to increase the number of training samples available.This approach helps to achieve two essential goals: it provides access to a larger and more diverse set of training data, thereby reducing the potential negative impact of having a large number of parameters, and it also encourages models to be more properly regulated so they can better extract true signals rather than being overly sensitive to noise.Increasing generalizability in this way can also support other benefits, discussed next.</p>
<p>Transferring knowledge</p>
<p>Beyond simply increasing the number of training samples available, DA can also be used to transfer knowledge across different biological contexts (different cells, tissues, organisms, individuals, ecosystems, in-vitro, and in-vivo) -assuming that domains share some commonalities in between features and task or goals.This could help scientists and physicians to transfer knowledge from some existing rich datasets to a different (but related) dataset that is smaller in size.For instance, in many situations there exists a large amount of labeled data from adults' MRI scans but much less data for infants; therefore, DA might be especially helpful to transfer insights gained from adults to newborns ( 31 ) .DA could also help transfer drug response insights gained from richly annotated pre-clinical cell lines to more poorly annotated human settings ( 32 ) , or to use DNA methylation data from multiple distinct tissues to predict donors' age ( 33 ) .In general, it is highly desirable to transfer knowledge gained from existing labeled datasets to other different, but related, datasets that are sparse in terms of sample size or annotation.It is easy to envision the benefit of applying models trained on publicly-available data to a locally-collected, small dataset -a process made potentially much more powerful through DA.</p>
<p>Discovering generalizable patterns</p>
<p>As introduced above, DA can also help drive at our primary scientific goal: to reveal true, meaningful, and generalizable biological insights rather than associations that are merely due to artifacts, confounds, idiosyncrasies unique to one dataset, or meaningful biological differences between domains which are separable from a particular question at hand.This is crucial since biological datasets are often composed of many different small cohorts collected from different laboratories and under different environmental and experimental conditions ( 20 , 29 ) .For example, many fMRI ( 34 -36 ) datasets are small, consisting of 30 human subjects or fewer per scanning site, but different hardware components or settings across MRI machines may result in data with different statistical distributions -e.g., different noise characteristics, signal magnitudes, correlations between features, or stationarity of these components across time for each scan site.In the microbiome field, the vaginal microbiome has been studied in over a dozen cohorts in the context of preterm birth ( 37 , 38 ) , and the gut microbiome has been similarly studied in the context of colorectal cancer ( 39 , 40 ) , yet variability in microbiome profiling across laboratories has been repeatedly noted ( 25 ) .</p>
<p>As these smaller, individual datasets are increasingly shared and curated into large databases, challenges of discovering domain-invariant patterns while using as much data as possible become immediately apparent.Because of the idiosyncratic nature of each individual dataset, machine learning models can learn non-relevant information in one single 'training' dataset that can lead to incorrect general conclusions about biological processes.For example, even sophisticated computer vision models can discover 'shortcuts' when detecting COVID-19 from chest radiographs: Instead of detecting clinically relevant factors, they rely on confounding factors such as laterality markers or patient positioning.This not only hinders the ability of the models to generalize to new data (i.e., when tested on a new patient from a new hospital) ( 41 ) but also might lead to misinterpretation of results within a single dataset.This issue is related also to batch effects (42) -essentially, the effect of non-biological artifacts that changes the distribution of the data for an experimental subset of a particular experiment.When experimental batches (e.g., plates for DNA extraction, or days for MRI appointments) are also associated with the outcome of interest, it may even lead to incorrect conclusions (batch confounding ).DA can be used to correct for these domain-specific idiosyncrasies when combining batches or cohorts, facilitating discovery of domain-invariant signals which may be more meaningful biologically.</p>
<p>Challenges of domain adaptation in bio-scale data and a path forward</p>
<p>Despite the clear utility of DA in biological data, its successful application to small datasets with complex features comes with significant challenges -many of which stem from the very reasons we would want to use it in the first place.In service of laying out a path forward to effective deployment of DA methodologies in biological scale datasets across multiple fields, we next explore in more detail why existing DA techniques may not be able to perform effectively on biological datasets.The purpose of this discussion is to help researchers learn to evaluate DA approaches for appropriateness in their own research, as well as to highlight deficiencies in current DA applications to biological questions which may be alleviated through improved collaboration between DA researchers and computational biologists.</p>
<p>Number of samples and features</p>
<p>Most DA methods have been designed in the fields of computer vision, text mining, or language processing ( 43 -46 ) with reference to -and evaluation on -large-scale text and image data, where there can be tens of thousands (or even millions) of samples available for training (e.g.MNIST, CIFAR10; refs.( 47 -49 ) ).In contrast, the number of samples in biological datasets is often small, but they simultaneously have many features, a problem known as curse of dimensionality ( 50 ) .For instance, in a typical fMRI or microbiome dataset we might only have a few dozens to hundreds of samples while the number of features could exceed thousands ( 26 , 51 , 52 ) .As introduced above, this imbalance between the number of samples and features can potentially lead to overfitting problems ( 27 , 28 ) , which in turn hinders the effectiveness of DA techniques on biological datasets ( 50 ) .There do exist several datasets typically used to benchmark DA approaches that may be somewhat closer in size to biological-scale data, including Office31 (ref.( 53 ) ), which contains image data of objects collected from 3 source domains with different resolutions, for a total of 4,110 images from 31 object categories (132 images per category).However, while one might hope that DA methods that have shown success on Office31 ( 54 -56 ) could be useful for biological data with similar sample size per category, it must be acknowledged that many biological datasets have significantly different properties than imaging data ( 57 -60 ) , and are even smaller, with only several hundred training samples in total.There is a need for DA algorithm development to specifically target success in the face of fewer training samples.</p>
<p>Differences in feature complexity</p>
<p>However, simply checking that DA approaches can perform adequately on small datasets is unfortunately unlikely to be enough.Another barrier to applying DA approaches to biological data is that features in biological domains are inherently much more complex than those in image data.For example, in many machine learning datasets such as MNIST or Office-31, image data are essentially pixel luminance values in the RGB and alpha channels that can be relatively simple to aggregate with other source data, for example by resizing the image ( 5 , 61 -64 ) .In the case of biological datasets however, the inherent complexity of features can significantly hinder our ability to aggregate different sources of data.For example, biological datasets often contain missing values ( 65 -68 ) , or have different numbers of features with unknown mapping orders between domains ( 24 ) (i.e., which features in a source are "the same" as which features in a target domain).They can also exhibit non-linear relationships or interactions between features ( 29 , 68 -70 ) , and unique data preprocessing requirements for each source can substantially increase the complexity of developing DA techniques for biological datasets.In other words, in addition to feature-to-sample ratio and number of categories, we need to take into account the complexity and heterogeneity of biological domains before using DA techniques on biological datasets.This increased complexity stems from several sources which we next discuss in more detail.</p>
<p>Missing values</p>
<p>Biological samples often contain many missing feature values.For example, microbiome data typically only consists of a few taxa that are shared by the majority of samples, and even less so across cohorts.Many taxa are rare and are only detected in very low abundances, a phenomenon known as zero inflation in statistics ( 71 ) .In human neuroimaging, PET or MRI scans combined with patients' genetic information can help with early diagnosis of Alzheimer's disease.However, the very common problem of missing values (i.e.not every subject has completed multi-modality data) can impede the ability of these multimodal models to make reliable predictions ( 72 -74 ) .Missing data is less problematic in many traditional datasets used to train DA approaches, meaning that these approaches may not deal with missing data well; to be successful with biological data, DA algorithms need to adequately handle small data and missing values.</p>
<p>Heterogeneity of features</p>
<p>Biological domains also often possess different numbers of features, and the features also often do not lie in the same rank order across domains.For example, fMRI data from a given brain region will have different numbers of voxels from one human subject to the next, and the information represented, for example, in voxel 1 in person A is unlikely to functionally align with the information encoded by voxel 1 in person B. While functional alignment approaches have been developed ( 24 , 75 ) , they do not explicitly perform DA operations.In microbiome research, it can be unclear whether a particular taxa is the same across datasets, especially because sometimes the measurement techniques differ (e.g., taxa are characterized using different regions of a marker gene, such that the same taxa might be represented by different features in different datasets).These examples are in stark contrast to most image-based DA approaches, which can exploit physical proximity of features (pixels) through spatial convolution or learn feature importance maps based on spatial features alone (e.g., the center of an image may often be more informative than the edges).</p>
<p>Additionally, domains may have some overlapping features but also some non-shared (distinct) features -i.e., those that are specific to one domain but not the other ( 76 ) .Current DA techniques may not be very effective on such datasets since domains may lack supplementary information such as labels ( 77 ) or information about matching features or samples between datasets ( 11 ) .This limitation could force researchers to remove domain-specific features and hence lose the capacity of DA models to benefit from these unique features in the learning process.Ideally, DA for biology could benefit from a specific focus on both feature alignment (ideally unlabeled) and principled ways to deal with shared versus non-shared features.</p>
<p>Distribution of feature importance</p>
<p>In biological datasets, feature importance distributions can be more highly skewed than in many standard benchmarks used to test DA approaches.That is, in biology, a few features can be very important for the ultimate performance of a model; in contrast, in typical benchmark datasets, many features can have similar importances ( 57 -60 ) .This difference in skewness of feature importance distributions can lead to extreme challenges for many DA approaches, such that DA models which succeed even on small 'typical' benchmark datasets may fail in biological applications.</p>
<p>Contributions of data collection and preprocessing procedures</p>
<p>Biological datasets often require extensive preprocessing after the data collection stage which can be inconsistent across datasets or laboratories (DADA2 or deblur for 16S rRNA amplicon data ( 78 , 79 ) , fMRIPrep ( 80 ) versus AFNI ( 81 , 82 ) , or FSL ( 83 -85 ) for fMRI images ( 86) ).As a result, machine learning methods used in biology typically are limited to being highly contextand preprocessing-specific, requiring careful design and tailoring to test the desired hypothesis appropriately ( 87) .This often occurs despite targeted efforts in bridging this gap by the means of setting up standards in generating and preprocessing the data ( 88 ) , since some lab-and individual-specific idiosyncrasies are wholly unavoidable.For example, in fMRI data correction for subject's head movement, using different scanning sequences or scanners can introduce data shifts that makes applying DA techniques even more difficult ( 2 , 89 -93 ) .Such preprocessing idiosyncracies can exacerbate or interact with other batch effects, including introducing or altering interdependencies among features ( 29 ) .</p>
<p>Interpretability of features and feature spaces</p>
<p>Interpretability is an important aspect of biological research, in contrast to at least some other ML applications.However, alignment steps in DA -which often require finding a latent representation of data by projecting the domains into a shared feature space ( 94 -96 ) -are frequently carried out by machine learning and deep learning methods.This means that DA in biological data inherits the same problem that plagues machine learning more broadly: failures in interpretability due to the black-box nature of these machine learning and deep learning methods.In fact, the shared feature space is particularly challenging to interpret ( 97 ) because the shared feature space is defined as a latent space that bridges two or more domains, rather than the latent space defined by one domain alone.Therefore, DA research can and should aim particularly at understanding how input features are related to the common feature space when utilizing these methods ( 98 , 99 ) .</p>
<p>Theoretical limitations of domain adaptation</p>
<p>It is also important that we discuss a critical theoretical limitation of DA, especially as it might impact biological data.The primary driver of DA's potential success is the adaptability between the source and target domains ( 100 , 101 ) -essentially, the theoretically maximal ability of an ideal model to jointly model them ( 102 , 103 ) .Failure of adaptability is thus a potentially fatal concern.While considering additional source domains provides the benefits of a larger and more diverse sample set (or additional labels), these domains might have inherently different distributions of features or different joint distribution with the labels, which would make a model considering them less accurate.Thus, applying DA might ultimately bring more cost than benefit ( 101 ) : in the worst-case scenario, negative transfer (i.e., applying knowledge from a source domain negatively affects the performance of the model in a target domain) can happen ( 100 , 104 ) .Crucially, the potential for negative transfer can be further amplified when working with biological data, due to its already-heterogeneous nature and the smaller sample size of each dataset.Therefore, it is imperative that the adaptability of the particular biological datasets in question be explicitly quantified or estimated before applying DA methods.Unfortunately, while there exist a few methods to quantify adaptability between domains ( 100 , 102 ) , analysis in the context of different biological sub-fields is exceedingly rare.The development of adaptability analysis methods thus may be a fruitful and critical area of future research into DA application to biological datasets.</p>
<p>Considerations for domain adaptation</p>
<p>Despite the challenges noted above, even in their current state, DA approaches can still provide benefit in biological data at this critical expansion of data sharing and open science practices in biology.But there are a great many methods to choose from.How should a scientist select the best DA approaches for their own datasets or scientific questions?In this section, we outline specific considerations for biologists in selecting and applying DA approaches in their own research.</p>
<p>We begin this section by presenting a formal definition of domain and domain adaptation .We then present a taxonomy which can be useful in gaining a better understanding of what to search for in the literature.In this Review we focus on the primary subcategory of DA which addresses data bias or covariate shift ; this DA subcategory tries to align shifts in the feature spaces between domains (or the change in the marginal distribution of data samples across domains).Other specialized subcategories of domain shift include label shift (105) , which indicates that different domains contain different number of labels for each class, and concept shift (106) , in which the data distribution remains the same but the conditional distribution changes (i.e.</p>
<p>). Interested readers should refer to these surveys ( 107 , 108 ) for
𝑃 𝑠 ( 𝑦 | 𝑋 ) ≠ 𝑃 𝑡 ( 𝑦 | 𝑋 )
a comprehensive overview of the different types of shifts in the DA field.</p>
<p>What is a domain?</p>
<p>A domain can be defined as , where is a feature space, is an
𝐷 = { χ , 𝑃 ( 𝑋 )} χ 𝑋 = { 𝑥</p>
<p>The terminology of domain adaptation</p>
<p>For a specific domain, we define the task (e.g., predicting what image a subject is looking at from neuroimaging data, or predicting a disease state from microbiome composition) as</p>
<p>, where denotes the labels to be predicted and denotes a decision function  = {  ,  (•)}   (•) (i.e., the posterior probability distribution of of the joint distribution ) that needs to  (  |  )  (  ,  ) be learned in order to map input features to the corresponding labels.</p>
<p>Given these definitions, domain adaptation is faced with the following problem, in which distributions or relative alignment of features across domains are different but the task remains approximately the same.Thus, a DA problem with covariate shift can be formally defined as follows:  and samples belonging to different domains so that the models emphasize learning domain invariant features that are not dependent on a specific dataset ( Figure 1 ). Figure 1.A cartoon representation of source and target domains before and after alignment.In this cartoon, features vary in their values along two dimensions, and each domain's features take on a different mean and covariance.Unless the domains are aligned, these differences could both obscure other meaningful variation in the data that is shared across domains, and prevent models trained on one domain from generalizing to another.</p>
<p>𝑃 ( 𝑋</p>
<p>A taxonomy of domain adaptation</p>
<p>Generally, when undertaking a DA analysis, we should consider three main factors:</p>
<p>1.The data used to train a model may be collected from multiple sources or just from a single source .2. Depending on the availability of labels in the target domain, we might choose supervised , semi-supervised , or unsupervised models.3. The feature spaces in the source(s) and target domains can be homogenous , meaning that they have the same dimensionality and "meaning", e.g., feat4-0w5ure A in source 1 represents the same "type" of information as feature A in source 2; or heterogenous , meaning that the feature spaces may differ in terms of dimensionality and/or meaning.</p>
<p>In the following, we discuss these three factors in more detail.Table 1 also shows a summary of these categories accompanied by mathematical annotations.</p>
<p>Single-vs. Multi-source</p>
<p>In selecting a DA method, one question you will want to ask is how many domains are present.As mentioned above, DA techniques can be divided into two categories of "single-source" and "multi-source" ( 111 ) .In single-source DA, the source domain is usually labeled, while the target domain belongs to another domain that possesses a different distribution ( 12 , 96 ) .Single-source DA is simpler than multi-source DA since there are only two distributions of data -source and target.Therefore, single-source DA is a good technique when there is enough data available in both the source and target domains to effectively train a model that can perform well on the target domain ( 112 , 113 ) ( 114 , 115 ) .</p>
<p>However, in modern real-world data sharing initiatives, most biological data come from many sources ( 116 , 117 ) , and using this data to its full extent can facilitate novel insights.Therefore it is advantageous to find models that leverage all available sources .This problem can be addressed through multi-source DA, which aims to combine multiple sources of labeled data to make predictions about a similar task on a target dataset ( 111 , 116 , 118 , 119 ) .A naive way to solve this problem is to combine multiple sources into one big source domain and then approach the problem as a single-source DA ( 111 , 120 ) .However, these methods can show very limited improvement in performance -and sometimes even worse performance -in comparison to using only one source ( 121) , specifically stemming from challenges of aligning the sources to begin with.Another way to tackle this problem could be to train a model on each source independently, apply each trained model to the target domain, and then vote for the 'correct' label in the target domain based on the prediction across sources ( 122 ) .One could also attempt to first discover domain-invariant features among all source and target domains ( 123) , or use a two-stage alignment technique that first tries to find domain-invariant feature spaces for each source-target pairing and then align model outputs across these spaces ( 121 ) .In all cases, though, Multi-source DA is significantly more challenging than single-source DA -a problem made worse by the particular characteristics of biological data, as discussed above.</p>
<p>Supervised vs. semi-supervised vs. unsupervised</p>
<p>It is also important to assess what kinds of labels are available for your data, across all the domains you need to align; this will dictate whether you should select a supervised , semi-supervised , or unsupervised DA method.These labels have been applied in varying ways ( 12 , 111 , 124 -126 ) .Here we have chosen a categorization based strictly on the usage of target labels: in unsupervised DA, no label is available in the target domain ( 55 , 96 , 127 , 128 ) ; in semi-supervised DA ( 129 -131 ) , some labels are available to use; and in supervised DA, labels in the target domain are available for most samples ( 107 ) .Although the majority of DA techniques in existing literature focus on unsupervised DA (since it is often utilized for the purpose of annotating unlabeled data in the target domain), in the case of biological data, any of the supervised, semi-supervised, or unsupervised scenarios is possible.This is because the primary goal of domain adaptation in biological settings is to uncover insights about biological systems that generalize across domains.Thus, even when labeled data are available in the target domain, one can still benefit from utilizing DA techniques on different datasets to find generalizable patterns across domains.</p>
<p>Homogeneous vs. heterogeneous</p>
<p>Finally, it is important to understand how the features are related across your different domains.DA can be divided into two categories based on the relationships between these features: homogeneous or heterogeneous ( 107 , 109 , 111 ) .In homogeneous DA, the source and target domains have the same feature space, = , but the data distributions of instances of these dimensionalities, and sometimes these features even have different labels or come from different modalities of data collection (e.g., fMRI versus another neuroimaging modality like electroencephalography).For instance, the fMRI data from the brains of two individuals have different numbers of voxels (features) which also are not meaningfully aligned across individuals regarding their functional properties (e.g., voxel 1 in person A is unlikely to encode the same information as voxel 1 in person B) -even when the scanner, protocol and performed task are exactly the same.</p>
<p>Categories | Definitions</p>
<p>Supervised</p>
<p>, with all target labels  (   ) ≠  (   )</p>
<p>When source and target domains are both labeled.</p>
<p>Semisupervised</p>
<p>, with some target  (   ) ≠  (   ) labels When source is labeled but target is partially labeled.</p>
<p>Unsupervised</p>
<p>, with no target labels  (   ) ≠  (   )</p>
<p>When source is labeled but target is not labeled.</p>
<p>Homogeneous</p>
<p>Case studies and practical examples</p>
<p>Given the nature of most biological datasets, which often contain limited samples and originate from many different sources, the most common DA setting in this field is multi-source heterogenous DA settings.For instance, aggregating fMRI data from multiple subjects or even multiple sites ( 34 -36 ) can be considered a multi-source heterogeneous domain adaptation.It is multi-source because the data is coming from multiple subjects or multiple sites with different MRI scanners, and it is heterogeneous because the number of voxels (i.e.features) from each subject and the information they represent is different.(Note: number of voxels can be equated through spatial normalization to a standardized template, but this does not address that each voxel will still represent different information across individuals.)In the microbiome field, integration of data from multiple microbiome datasets in order to predict a phenotype on a held-out study ( 37 , 38 , 141 ) is once again multi-source and heterogeneous, as data are often amplicons of different regions of the 16S rRNA gene.To illustrate the utility of existing DA approaches and explore their categorization with the taxonomy discussed above, here we select several methods to discuss in slightly more detail.</p>
<p>One DA method, the PRECISE method ( 32) , has been used to predict patients' drug response based on available pre-clinical datasets such as cell lines, and patient driven xenografts (PDXs).</p>
<p>To achieve this, the authors first extracted factors from cell lines, PDXs and human tumors using principal component analysis (PCA).Then they aligned these subspaces from human tumor data with pre-clinical data using geometric transformations, and extracted common features associated with biological processes followed by training a regression model using consensus genes and validated with known biomarker-drug associations to accurately predict drug response in patients.In this study, DA was homogenous, as the features (genes) in the source and target domains were the same; multi-source, as various source domains were used (i.e.cell lines); and supervised, as the labels of all samples were used.</p>
<p>Another method, Adversarial Inductive Transfer Learning (AITL) ( 142 ) , similarly aims to utilize largely available source domains such as cell lines and clinical trials to predict drug responses on small and hard-to-obtain gene expression data from patients.To this end, researchers first used a feature extractor network to map the source and target into a common feature space.This mapping aimed to alleviate the domain shift by using a global discriminator to learn domain-invariant features.Then, these domain-invariant features were used to build a regression model for the source task (i.e.predicting IC50) and a classification network to make predictions on the target task (i.e.predicting whether there is reduction in the size of the tumor).This study aimed to address both prior and covariate shifts in the source and target domains.</p>
<p>The data used in this study came from multiple heterogeneous sources including thousands of cell lines from different cancer types.Finally, the target samples were labeled.This study can thus be characterized as a multi-source and supervised heterogeneous (i.e.drug response is categorized differently between preclinical and clinical settings) DA scenario.</p>
<p>Other methods such as WENDA ( 33 ) (Weighted Elastic Net for unsupervised Domain Adaptation) aim to predict a human's age using DNA methylation data, which are known to be different across different tissues.WENDA aims to use the available DNA methylation data from some tissues (source domains) to predict the age of the human subject using DNA methylation from a different tissue (target domain) by giving more importance to features that are more robust and behave in a similar fashion across source and target domains.In this study, data from 19 different tissues with chronological age ranging from 0 to 103 years old were used as the source domain.The target domain came from 13 different tissues, with chronological age ranging from 0 to 70 years old.In the application of WENDA, the source domain remained unchanged, while each tissue type was viewed as a distinct target domain.This thus represents a multi-source, unsupervised, homogenous DA scenario.</p>
<p>In another study, Li and colleagues ( 2) propose a multi-source domain adaptation approach by using resting-state fMRI "Autism Brain Imaging Data Exchange" (ABIDE) datasets ( 143) from multiple academic sites (UMI, NYU, USM, UCLA).Their goal was to improve the classification accuracy of autism diagnosis by detecting biomarkers.In this study, the feature space, denoted as , was extracted features from fMRI sites such that , with and representing different χ χ  = χ    institutions (the data can be spatially normalized across participants by warping to MNI space).From this perspective, this problem is a homogeneous domain adaptation scenario.Subsequently, the authors utilized a Mixture of Experts (MoE) ( 144 , 145 ) , combining multiple neural networks -each of which is specialized in solving a specific task -in order to improve the overall performance of the model, and adversarial domain alignment methods to minimize the discrepancies between the domains, and successfully demonstrated the advantage of using federated domain adaptation techniques in using multi-site fMRI dataset to classify autism.Additionally, they were able to reveal possible biomarkers in the brain for autism classification.Therefore, in this framing this can be considered as a multi-source and supervised homogeneous DA problem.</p>
<p>Finally, Gao and colleagues proposed the deep cross-subject adaptation decoding (DCAD) ( 146) method: a single source, unsupervised, heterogeneous domain adaptation technique.DCAD uses a 3D feature extraction framework using 3D convolution and pooling operations based on volume fMRI data to learn common spatiotemporal patterns within a source domain to generate labels ( 146 ) .Subsequently, an unsupervised domain adaptation method minimizes the discrepancy between source and target distributions.This process considers different subjects as different sources and aids in the precise decoding of cognitive states (in working memory tasks) across subjects.To validate the approach, they applied task-fMRI (tfMRI) data from the HCP ( 147) dataset.The experimental outcomes revealed exceptional decoding performance, achieving state-of-the-art accuracy rates of 81.9% and 84.9% under two conditions (4 brain states and 9 brain states, respectively) during working memory tasks.Additionally, this study demonstrated that unsupervised domain adaptation effectively mitigates data distribution shifts, offering an excellent solution to enhance cross-subject decoding performance without relying on annotations.</p>
<p>Future directions</p>
<p>What is missing from DA approaches in biological applications?</p>
<p>Despite these exciting successes, continued development of DA approaches tailored to the challenges of biological data is critically needed.This is especially important in light of the increasing availability of curated open datasets, complemented by increasing standardization of metadata standards ( 3 , 4 ) .We thus hope the machine learning community will continue to develop techniques that can address relevant limitations of biological datasets, including:</p>
<ol>
<li>Models must be able to capture the non-linear and complex patterns in biological systems, ideally with minimal or no assumptions.Therefore, many linear-based domain adaptation techniques (usually focused on some sort of transformation from source to target domain) might not be adequate.2. Ideally we want to utilize domain adaptation to discover the underlying mechanisms of biological phenomena, rather than simply aggregating data for automatic annotation.Unfortunately, many existing techniques are primarily developed for addressing automatic annotation of unlabeled data.Therefore, to fully unleash the power of DA in biological systems, we must focus on methods that seek to discover domain-invariant features that are common across datasets.This usually happens by mapping all domains into a common feature space.3.This domain-invariant mapping should be done using methods that work with limited data in individual cohorts.Although deep learning models are great tools to uncover highly nonlinear and complex relations in data with no specific assumptions, they often require many samples.Recently, simpler neural network architectures such as TRACE ( 148) and Fader networks ( 90) have shown promise with small fMRI datasets.However, many of the powerful neural network architectures such as GANs might not be suitable for biological datasets as they usually require vast amounts of data ( 149 , 150 ) . 4. Methods should be developed to address domains' adaptability with specific focus on biological datasets.As mentioned earlier, methods do exist to quantify adaptability between domains ( 100 , 102 ) , but limited attention has been paid to how such methods may fare in biological contexts.</li>
</ol>
<p>In sum, it is incumbent upon us in the biological disciplines to challenge machine learning research to design more flexible and broadly applicable DA methods that can perform under the constraints of real-world biological datasets.An important step towards this goal will be to test and evaluate existing approaches on our own data, and on data available through increasingly comprehensive and consistently annotated shared data repositories, to comprehensively explore and categorize their current shortcomings.Thus, we hope that, with the help of the topics discussed in this Review, researchers in biological disciplines will feel empowered to try out existing DA approaches and to help catalog their successes and shortcomings.</p>
<p>If you would like to use DA techniques to augment your own data processing pipeline, we urge you to begin by gaining a comprehensive perspective on your data using the definitions and taxonomy described above.For example, How many sources do you have available?What is the sample size in each source?Do these sources contain equal amounts of features?If not, what are the nature of features in each source?Are these features in each source known and have a label?What task are you trying to achieve?Depending on the answers, you can choose the appropriate DA approaches, and set about examining their successes or failures.We hope that the tools and information provided in this Review will encourage you to do so, and to report your findings so that iterative improvements in DA approaches can be made to best serve our fields.</p>
<p>Promises for the future</p>
<p>In this piece we have focused on human neuroimaging (specifically fMRI) and microbiome sciences as token examples to speculate the potential promises of DA in computational biology as a whole.We hope that these selected case studies have helped to show off the potential of DA in numerous and varied biological disciplines, from electrophysiology, multi-omics, DNA sequencing, and scRNA sequencing to and protein localization -all of which face similar challenges in data collection and labeling to the case study fields discussed here.Differences in equipment, experimental setup, or even individuals can lead to a shift in the distribution of data, even when the task is identical.In all cases, however, our goal as researchers and clinicians is to go beyond domain-specific or dataset-specific models in order to discover domain-general and informative "truths" about biological systems.</p>
<p>Thus, DA could be extremely useful to aggregate diverse biological datasets available across the Open Science Framework, OpenNeuro, Neurosynth, Dryad, CEDAR, and more in search of meaningful and even clinically relevant outcomes ( 151 -154 ) .But much work is needed to address the existing challenges.It is the intention of this paper to help and facilitate these processes by bringing more awareness of DA, and the need to develop new techniques that are compatible with the limitations of biological datasets in order to make it accessible to biologists.If we are successful in identifying the challenges of performing DA on biological data, we are optimistic that DA and transfer learning methodologies can greatly benefit biologists.</p>
<p>Domain adaptation in small-scale and heterogeneous biological datasets -Supplementary Information</p>
<p>Seyedmehdi Orouji 1 , Martin C. Liu 2,3 , Tal Korem 3,4,5,*, † , Megan A. K. Peters 1,5,6, * , †</p>
<p>Here we provide more detail on specific domain adaptation methods than offered in the main text.The purpose of this section is to provide a greater degree of detail for the interested reader, as well as to suggest further reading through the works cited.</p>
<p>S1 Domain adaptation through adding or selecting features</p>
<p>Some DA methods discover the best way to transfer between domains by adding, deleting, or otherwise weighting features from each domain differently.In the following subsections, we describe some representative methods in detail.</p>
<p>S1.1 Adding features: feature augmentation / feature replication</p>
<p>The feature augmentation or feature replication strategy aggregates and transforms the source and target domain features together into an augmented feature space for use during model training.For example, Frustratingly Easy Adaptation (FEDA) ( 125) maps the augmented source and target feature space by duplicating the features into three vectors, and χ  = (   ,   , 0 ) , to regulate the trade-off between source/target and general weights for the χ  = (   , 0 ,   ) classifier to learn.Daumé and colleagues ( 155 ) provide a semi-supervised extension of the FEDA method by forcing the source and target domains to agree on the unlabelled data to leverage unlabeled data in the target domain for model training.</p>
<p>Although relatively easy to implement, one should be cautious when applying FEDA methods to small-scale biological data.First, biological data have high dimensionality to begin with, so duplicating features will make the dimensionality of the data even more troublesome -especially when multiple domains are intended to be leveraged simultaneously.Domain-independent feature reduction methods such as PCA can be applied, as was done by Schneider and colleagues ( 156 ) , to dramatically reduce the number of features.However, this approach's performance did not substantially differ from the results of simply concatenating the source and target domains.Furthermore, FEDA methods only work with homogenous datasets, while biological data is often heterogeneous across domains.Therefore, a proposed extension of FEDA that deals with data heterogeneity by using two projection matrices and ,   and , has been found to be generally more useful when
χ 𝑠 = ( 𝑃𝑋 𝑠 , 𝑋 𝑠 , 0 𝑑𝑡 ) χ 𝑡 = ( 𝑄 𝑋 𝑡 , 0 𝑑𝑠 , 𝑋 𝑡 )
dealing with biological data ( 8 ) .</p>
<p>S1.2 Selecting or weighting features</p>
<p>Data selection domain adaptation strategies include methods in which the respective local geometrical structures of data in both source and target domains remain unchanged.In other words, these methods select only the informative and relevant features or samples to use in training and testing, without implementing any transformational changes to the data itself.Data selection often occurs in conjunction with and before other transformation and alignment techniques (see next section), though using it on its own can offer a beneficial and simpler approach to bridging source and target domains in some situations.</p>
<p>Structural correspondence learning (SCL) ( 157) is an example of this data selection strategy.SCL first defines a set of frequently occurring and diverse pivot features (i.e.features that behave the same way for discriminative learning in all domains) on the unlabeled data from both domains, and then estimates the pivot features' covariances with non-pivot features to learn a mapping function between source and target.Another version, a manifold-based technique ( 158) , employs a method termed Statistically Invariant Sample Selection (SISS) to select landmark samples from both domains based on the pairwise Hellinger distances between the samples' distributions.In some cases, SISS has been shown to be more effective than assigning non-binary weighting to samples ( 159 ) .Of course, the drawbacks of discarding samples that do not meet these criteria when working with small-scale biological data --when samples are already too few --are obvious given the discussion in the main text.However, the tradeoffs between (a) using only a few informative samples, thus risking not having enough samples, and (b) selecting all samples thus risking negative transfer (i.e.applying knowledge from a source domain will negatively affect the performance of the model in a target domain), should be judged on an empirical and case by case basis.</p>
<p>S2 Domain adaptation through parametric transformations</p>
<p>Another important domain adaptation strategy concentrates on alignment .There are different ways to align domains, including label information or dependency structure and correspondence of features.Overall, the merit of alignment techniques is that most of them do not require that label information is available for the target domain, and some also reduce the dimensionality in a way that takes into account both the source and target.Many of these methods thus also discover a (often lower dimensional) shared (sub)space between the source and target domain, rather than a transformation that maps one domain directly onto the other.A major critique of these methods is thus that they often result in less interpretable features --and feature interpretability is a critical objective in many biological studies.Here, we briefly discuss several parametric alignment-based approaches to DA.</p>
<p>(Note that domain alignment can be done in a parametric or non-parametric way, depending on how the loss function is minimized; in this section we focus specifically on parametric alignment, with nonparametric approaches to both feature selection and alignment discussed in Section S2.2 .)</p>
<p>S2.1.1 Correlation based alignment</p>
<p>Canonical correlation analysis (CCA) ( 160) is a classical technique used to maximize the correlation between two sets of vectors --or in the case of DA, the correlation between the feature distributions across two domains.CCA is particularly useful in heterogeneous DA problems where the goal is to find a feature transformation to bridge the heterogeneous feature spaces via finding a subspace that is shared between two domains ( 161 ) .However, CCA by definition finds linear combinations of two domains, and so cannot work where a nonlinear feature subspace is desirable ( 162 -164 ) .Fortunately, advanced implementations of CCA, such as Kernel CCA (KCCA) ( 162 -164 ) , can be employed to find non-linear combinations of shared feature representations across domains.</p>
<p>Another correlation-based approach, Correlation Alignment (CORAL), is a simple and fast, yet powerful, technique to minimize domain shift by aligning the second-order statistics (covariance) of two distributions ( 137 ) .CORAL tries to minimize distances between domains using the original feature spaces rather than lower-dimensional subspaces.The first step is to remove feature correlations in the source domain (i.e.'whitening'), and then 'recolor' the source's features by the target domain's feature covariances.The advantage of the CORAL method is that it is incredibly simple and fast --for example, it can be implemented with four lines of code in MATLAB --and yet is still very effective in aligning the domains.However, we note that this method is limited to aligning homogeneous domains.</p>
<p>S2.1.2 Geometric transformation based alignment</p>
<p>Several methods assume that transformations must take on a specific functional form, potentially based on field-specific knowledge.For example, suppose one wishes to functionally align fMRI data across multiple human subjects to study shared cognitive characteristics and improve the samples-to-features ratio.Some methods to accomplish this goal use rigid-body geometric transformations because they assume that specific regions of the brain (e.g.ventral temporal cortex) encode similar features across domains, but that these features are not labeled (i.e., the coordinate system of voxels that represent specific features are not aligned).These unlabeled features across subjects must be aligned into a common feature space across multiple subjects without warping the feature space -i.e., using only translation, scaling, and rotation.Here, subjects are considered as different source domains, where the dimensionality and order of voxels (features) are different across subjects.These characteristics thus suggest a heterogeneous multi-source DA problem.</p>
<p>One such method, hyperalignment ( 24 , 165 ) , assumes that the shared feature space is high-dimensional.This method also assumes that data from all source domains share the same feature space that is also anchored by the same temporal variance (e.g.all domains/participants have watched the same sequence of a movie).Therefore, under the assumption that the features in each domain follow a shared trajectory in some shared feature space while anchored in time, it is possible to rotate the individual feature spaces into a common feature space.(Other variants of hyperalignment technique have been proposed ( 75 , 166 ) .)This rotation is done through Procrustean transformation -an orthogonal transformation that minimizes Euclidean distance between features across domains.This model finds hyperalignment parameters that will map each source into a common feature space of voxel responses.This method implements two main assumptions that must be considered before applying it to other datasets.First, there exists a feature space that is common between all domains, and second, there exists a linear transformation that can map the voxel pattern of multiple domains into a common feature space such that it can minimize the distance between two features in two domains.Therefore, hyperalignment might also be useful for aligning heterogeneous multi-source DA problems where these two assumptions are satisfied, especially when there is also the opportunity to exploit temporal anchoring.</p>
<p>S2.1.3 Other parametric transformations</p>
<p>Probability distributions lie on a Riemannian manifold, and there are some alignment-based methods that exploit this fact.One method ( 167 ) 77) expands manifold alignment to work on multiple domains with heterogeneous datasets and leverages the labels to align the domains rather than the often-inaccessible correspondence information (i.e.instances in one dataset that correspond to, or are in some way equivalent to, instances in another dataset).</p>
<p>Other methods can be seen as less rigid versions of the methods discussed above.For example, like hyperalignment, the Shared Response Model (SRM) was originally developed to aggregate fMRI data across many subjects to evaluate cognitive states across groups rather than within individuals ( 168 ) .However, unlike hyperalignment, SRM projects subjects' data into a shared lower-dimensional feature space, and thus relaxes the assumption of rigid-body transformation and precisely shared (but unlabeled) features within a given brain region that are shared across participants.The shared space, , can be calculated as follows Likewise, in the microbiome field, specific frameworks have been developed to model the processes that generate variability between different studies or batches -for example by modeling experimental variability as multiplicative bias that affects the measured taxonomic abundances ( 169 ) .These functional forms can then form the basis for field-specific domain adaptation techniques.One such method is DEBIAS-M ( 141) , a microbiome-specific domain adaptation method that realigns sources by inferring their underlying processing biases.</p>
<p>Likewise, in the microbiome field, specific frameworks have been developed to model the processes that generate variability between different studies or batches -for example by modeling experimental variability as multiplicative bias that affects the measured taxonomic abundances (McLaren et al. 2019).These functional forms can then form the basis for field-specific domain adaptation techniques.One such method is DEBIAS-M(Austin et al. 2024), a microbiome-specific domain adaptation method that realigns sources by inferring their underlying processing biases.</p>
<p>S2.2 Domain adaptation through nonparametric feature selection and transformation approaches (neural networks)</p>
<p>Sometimes, it is not possible to define a priori the type of transformation that might be appropriate to align domains or select features.In this case, it is advantageous to turn to neural networks, which can discover and approximate any parametric function.Neural network based DA techniques thus typically have a feature extractor section in their architectures, as well as relying on standard objective functions to align domains.This section will focus on introducing various neural network architectures and the type of biological problems that they may be well-suited for.</p>
<p>S2.2.1 Adversarial based</p>
<p>Adversarial based neural network methods usually incorporate a discriminator component in their architectures.Unlike discrepancy based methods that try to align features by minimizing a specific statistical distance measure between domains (e.g. the CORAL, hyperalignment, or SRM methods above), adversarial methods try to "fool" a discriminator until it cannot distinguish which data is coming from which distribution ( 123 , 140 , 170 , 171 ) , and as a consequence, the network will learn domain-invariant features ( 130 , 172 ) .</p>
<p>Adversarial-based methods can be separated into two categories: adversarial generative and adversarial discriminative.Adversarial generative methods, usually based on Generative Adversarial Networks (GANs), were originally designed to solve problems where there is interest in generative models ( 173 ) .For instance, Xu and colleagues ( 122) used GAN loss in the objective function in order to minimize the discrepancy between each source domain and target by using multi-adversarial learning.Although GANs can create fascinating visualizations, they are not optimized for discriminatory tasks and are limited to domains where the shift between distributions is small ( 171 ) .Moreover, these networks usually need many training samples, which of course is an issue in biological datasets, and hence might not be a good choice for biological data.On the other hand, adversarial discriminative methods aim to mitigate the negative effects of domain shift by learning a discriminative representation of the source and target domains ( 123 , 171 ) without the need for a generative component.These methods, however, have also typically been used on image data where there are tens of thousands of examples available, and so require further examination to explore whether they can be suitable for small biological datasets.</p>
<p>To utilize these methods, however, one has to use a feature extractor section in the architecture of the model in order to map the original input spaces into a (usually) lower-dimensional space.</p>
<p>A simple yet effective architecture of adversarial-based DA is the domain-adversarial neural network (DANN) ( 114 ) , developed by Ganin and colleagues, which uses a gradient reversal in order to maximize the loss of a categorical classifier and hence to ensure that features in the two domains are similar.</p>
<p>S2.2.2 Autoencoder based</p>
<p>Autoencoder (AE) networks are unsupervised learning algorithms that consist of an encoding and a decoding section, which enable them to learn hidden representations of an input ( 174 ) .</p>
<p>The encoding section of an autoencoder usually uses a nonlinear function in order to discover features in a bottleneck (the dimensionality of the bottleneck is usually lower than the initial input) that are informative enough to be used by the decoding section of the network to reconstruct the original input.In DA, it is possible to learn domain invariant features by sharing the same encoding section across multiple domains.The unsupervised nature of these AE networks makes them a good candidate to discover domain-invariant features even in the case of unlabeled or sparsely labeled source domains.</p>
<p>Previously, AEs have been successfully used to extract domain-invariant features.For instance, stacked denoising autoencoders (SDAEs) have been used before to extract high-level features that are common across source and target domains ( 175 ) .Therefore, a classifier trained on these high-level common features using the labels available in the source domain can also perform well on the target domain.Similarly, Bousmalis and colleagues ( 176 ) developed the Domain Separation Network (DSN) using two encoding sections.First, a shared encoding network between source and target domains learns the shared representation across domains, and second a private encoder learns domain-specific features.Then the decoding section will use both domain-invariant and domain-specific representations to reconstruct the input samples.Finally, the authors trained a classifier only on the shared representation so the classifier can also perform well on the target domain.As another example, Ponimova and colleagues ( 90 ) successfully developed the Fader network, which used a convolutional autoencoder to address the heterogeneous nature of multi-site fMRI data to increase the sample size and extract domain-invariant features across multiple subjects.Although the total number of samples were 1000, they were collected from 19 sites which means the average available data from each site was 52 samples.This success offers hope for implementing deep learning techniques with other sorts of biological datasets where the sample size from each source is limited but combining many sources can increase the overall sample size and thus prevent overfitting while discovering domain-invariant features.</p>
<p>S2.2.3 Convolutional neural network based</p>
<p>Convolutional neural networks (CNNs) ( 47 ) are great candidates for DA on data that contain spatial information such as image data.In CNNs, once features are learned using convolutional and pooling operations, the exact position of these features becomes less relevant.To address the variability of feature positions between individuals, CNNs often utilize pooling operations where the receptive field (i.e.regions in the image that the CNN's feature detector can see) is subsampled or downsampled.</p>
<p>Previously, many have used CCN based methods such as AlexNet ( 44 ) and ResNet ( 177 ) as the backbone in the architecture of feature extractor section in DA techniques in the field of medical imaging and computer vision ( 138 , 178 ) .For example, Chen and colleagues ( 179 ) developed a novel technique using a multi-view convolutional autoencoder, which combined latent variables and searchlight-based analysis, to align fMRI data from multiple human subjects and map those subjects' data into a common space.The technique was able to preserve the spatial locality of the voxels, showing comparable or superior decoding accuracy compared to other standard techniques such as SRM or standard searchlight analysis ( 179 ) .</p>
<p>S2.2.4 Recurrent neural network based</p>
<p>Recurrent neural networks (RNNs) contain one or more loops in their directed connection graph through which their internal state will evolve over time in discrete steps.RNNs are often used to process time-series data such as video or text, in which a new frame or character is fed into the network at each time step.Essentially, RNNs resemble Artificial Neural Networks (ANNs) with hidden layers that feed back onto themselves.This allows the layers to not only pass on the information to the next layer but also update their own weights and all the weights in previous layers.In other words, RNNs are many copies of the same ANNs that pass outputs to a successor.This makes RNNs great networks that can model phenomena which happen in a sequence of actions making them good candidates to extract features from times series data such as decoding emotions from EEG ( 180 ) , movement control of prosthetics using electromyography (EMG) ( 181 ) , or protein-protein interactions ( 182 ) ( 183 ) .RNNs can thus be used in DA approaches: For instance, Sonsil ( 181 ) and colleagues developed a method where the EMG data from one individual (source domain) was used to train a neural network to predict the gestures of another individual (target domain).They used a combination of RNN and adversarial domain adaptation (ADA) ( 184 ) by summing up the loss functions of the RNN predictor and the discriminator.</p>
<p>S3 Discovering domain-invariant spaces: a sampling of current algorithms</p>
<p>Having introduced classifications and useful vocabulary both above and in the main text, we also want to direct the reader to a usefully curated corner of the DA literature.In this section, we focus on DA methods that aim to discover domain-invariant (often lower dimensional) spaces of features, as these kinds of spaces may be argued to best serve the goal of discovering generalizable truths in biology.</p>
<p>To discover domain-invariant spaces, we aim to find a projection matrix that minimizes statistical distance between two domains (e.g. the Maximum Mean Discrepancy (MMD)).For instance, the Distribution Matching Embedding (DME) ( 185 ) aims to find a projection matrix, , that minimizes  MMD such that remains orthogonal (i.e.</p>
<p>).An alternative approach is to learn the     =  ) common feature space using deep learning models (i.e.deep domain adaptation).However, these methods usually require a large number of samples for training.There have been some deep domain adaptation methods that have been successful on relatively smaller datasets, such as the Office-31 ( 53) dataset (with ~ 132 samples per category form 3 domains), using with AlexNet ( 44 ) or ResNet-50 ( 177) backbones ( 121 , 139 , 186 , 187 ) .However, it is crucial to note that: (1) these are image datasets, which (2) contain substantially more samples compared to many biological datasets despite their relative sparseness compared to standard benchmark datasets.Therefore, careful consideration is necessary when applying deep DA on biological datasets.</p>
<p>Table S1 describes a summary of the subcategories of domain-invariant-feature based methods along with their limitations and strengths.These approaches and their kin can ideally be used, and further developed, to help discover generalizable truths in biological data which are not limited by the idiosyncrasies of a few small datasets.</p>
<p>Alignment type
Limitations</p>
<p>source domain, denotes the target domain, is the number of source    domains, is the marginal distribution of a specific instance set in a given domain, and is  (  )  the task performed in each domain.Here, the goal of DA is to improve the performance of target decision function in target domain by leveraging the information from source domain  (•)      and decision function (which is learned on the source domain after the source and target  (•)  domains are aligned).In other words, DA intends to adapt the model(s) trained from a source (or sources) to a different, but related, target dataset.It does this by aligning the distributions of</p>
<p>features</p>
<p>shared feature space that is chosen by the experimenter.</p>
<p>(  ) marginal probability distribution of all samples in that dataset.This formal definition is typically used in discussions of DA across a wide variety of disciplines( 109 , 110 ).
1, 𝑥 2, ..., 𝑥𝑛 }instance set with denoting a given feature, denotes the number of features or dimensions in 𝑥
the data (e.g., in fMRI data voxel activities or taxa in microbiome data), and denotes the</p>
<p>"meaning" as feature 1 in domain 2 -for example, they both represent a specific voxel at a specific coordinate in the brain, or represent the same microbe (Note: = that there is no difference between the source and target datasets at all).In heterogeneous DA, conversely, the feature space is related but different between the domains.Many DA techniques that have been developed so far tend to focus on homogeneous DA( 95 , 132 -140 ).For instance, the source data could be the fMRI data obtained from a subject with one scanner and the target domain is the fMRI data obtained from the same subject with the same protocol but a different scanner.Alternatively, different domains could contain gut metagenomic sequencing data from different studies aligned against the same reference database.Addressing the domain shift in a homogeneous DA problem is relatively simpler since it is possible to simply perform the feature alignment directly on the original instances of the domains without the need to project them into a common feature space.Unfortunately, however, most biological datasets are heterogeneous in nature( 29 , 68 )since these data are collected in different laboratories, under different environmental and experimental conditions, and sometimes even for answering different but related questions.In other words, neither the feature spaces nor the marginal distributions are the same (i.e.
χ𝑠 χ𝑡means that thefeature space in both domains is homogenous, but if = 𝑋 𝑠 𝑋 𝑡then it means that𝑋 𝑠and𝑋 𝑡areidentical datasets such , χ 𝑠 ≠ χχ 𝑠 χ
 feature spaces are different, .That is, feature 1 in domain 1 represents the same  (   ) ≠  (   )  ).As a result, biological datasets very often have different feature  (   ) ≠  (   )</p>
<p>Table 1 . Difference among traditional machine learning, transfer learning, and various kinds of domain adaptation
1
. ML, machine learning; DA, domain adaptation.represents χ feature space, and is the marginal distribution of instance set , denotes the performed  (  )   task, and is the decision function to map each sample to the corresponding label.denotes  (•)  the source domain, denotes the target domain, and is the number of source domains. 
When the feature spacesDA𝑃 ( 𝑋 𝑠 ) ≠ 𝑃 ( 𝑋 𝑡&amp; ) χ𝑠 = χ 𝑡&amp;𝑇 𝑠 ≈ 𝑇 𝑡have dimensionality and same the samemeaning.When the feature spacesHeterogeneous DA𝑃 ( 𝑋 𝑠 ) ≠ 𝑃 ( 𝑋 𝑡&amp; ) χ𝑠 ≠ χ 𝑡&amp;𝑇 𝑠 ≈ 𝑇 𝑡have dimensionality or different differentmeanings.</p>
<p>utilizes manifold alignment without the need for any pairwise correspondence information between the source and target.
More concretely, for agiven source𝑋 𝑠 = { 𝑥𝑖 , • • • , 𝑥 𝑚}of feature dimension and target 𝑝𝑋𝑡 = { 𝑥𝑗 , • • • , 𝑥𝑛 }of featuredimension , the method computes functions and to map source and target domains to a 𝑞 α βnew lower dimensional space so thatα𝑇 𝑥𝑖andβ 𝑇 𝑥𝑗can be directly compared in order tominimize the loss function. Another similar method (</p>
<p>Table S1 . Sample algorithms used in DA which seek domain-invariant spaces, shared across two or more domains, by finding a projection that minimizes the discrepancy between domains.
S1BenefitsSample methods/ citationsShallow DA:Is not assumption-free,Requires relativelyM3SDA ( 46 )discrepancy basedlimiting its learningless amount of data.Guo et al. ( 138 )ability in findingZhu et al. ( 121 )(e.g. MMD ( 188 ) ,domain-invariantHoffman etcorrelation alignmentfeatures.al. ( 178 )(CORAL) ( 137 ) ,Guo et al. ( 190 )Contrastive domaindiscrepancy(CCD) ( 189 )Deep DA: discrepancyRequires relativelyCan find highlyLong et al. ( 54 )basedmore samples; somenon-linearDeepmetrics such as MMDrelationships betweenCORAL ( 115 )and CORAL mightdomains and theContastiverequire assumptions of kernel function for calculating the distance between distributions.common feature space-the alignment part do not introduce new parameters.Domain Adaptation (CDA) ( 189 ) Deep-JDOT ( 191 )Reconstruction based Requires relativelyAssumption free; canDRCN ( 192 )more samples.find non-linearDSN ( 176 )relationships betweenMTAE ( 193 )domains and thecommon featurespace; does not needa distance metric.Adversarial basedRequires manyEffective for medicalADDA ( 171 )(e.g. GAN loss,samples, sinceimaging data such asTsai et al. ( 194 )H-divergence,discriminator is aMRI, CT.DANN ( 96 )Wassersteinnetwork that introducesCycleGAN ( 195 )distance ( 191 ) )more parameters to beCyckeEmotionGAlearned.N ( 196 )
AcknowledgementsWe thank Harry H. Lee for initial discussions on this topic.This work was supported by two Canadian Institute for Advanced Research Azrieli Global Scholars Fellowships (in Brain, Mind, &amp; Consciousness [to MAKP] and in Humans and the Microbiome [to TK]), and a Canadian Institute for Advanced Research Catalyst Grant (to MAKP and TK).The funders had no involvement in the design or content of this work.Author contributionsCompeting interests statementThe authors declare no competing interests.
Causation in neuroscience: keeping mechanism meaningful. L N Ross, D S Bassett, Nat. Rev. Neurosci. 252024</p>
<p>Multi-site fMRI analysis using privacy-preserving federated learning and domain adaptation: ABIDE results. X Li, Y Gu, N Dvornek, L H Staib, P Ventola, J S Duncan, Med. Image Anal. 651017652020</p>
<p>M Zizienová, New OSF Metadata to Support Data Sharing Policy Compliance. 2023</p>
<p>CEDAR team, The center for expanded data annotation and retrieval. M A Musen, C A Bean, K.-H Cheung, M Dumontier, K A Durante, O Gevaert, A Gonzalez-Beltran, P Khatri, S H Kleinstein, M J O'connor, Y Pouliot, P Rocca-Serra, S.-A Sansone, J A Wiser, J. Am. Med. Inform. Assoc. 222015</p>
<p>EmotionGAN: Unsupervised Domain Adaptation for Learning Discrete Probability Distributions of Image Emotions. S Zhao, X Zhao, G Ding, K Keutzer, Proceedings of the 26th ACM International Conference on Multimedia. the 26th ACM International Conference on MultimediaNew York, NY, USAMM201818</p>
<p>Unbiased look at dataset bias. A Torralba, A A Efros, CVPR 2011. 2011</p>
<p>H Kashyap, H A Ahmed, N Hoque, S Roy, D K Bhattacharyya, arXiv [cs.CE]Big Data Analytics in Bioinformatics: A Machine Learning Perspective. 2015</p>
<p>Learning with Augmented Features for Heterogeneous Domain Adaptation. L Duan, D Xu, I Tsang, arXiv [cs.LG]2012</p>
<p>M Harel, S Mannor, arXiv [cs.LG]Learning from Multiple Outlooks. 2010</p>
<p>Cross-Language Text Classification Using Structural Correspondence Learning. P Prettenhofer, B Stein, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. the 48th Annual Meeting of the Association for Computational LinguisticsUppsala, SwedenAssociation for Computational Linguistics2010</p>
<p>Hybrid Heterogeneous Transfer Learning through Deep Learning. J Zhou, S Pan, I Tsang, Y Yan, AAAI. 282014</p>
<p>A Survey on Transfer Learning. S J Pan, Q Yang, IEEE Trans. Knowl. Data Eng. 222010</p>
<p>Batch effects removal for microbiome data via conditional quantile regression. W Ling, J Lu, N Zhao, A Lulla, A M Plantinga, W Fu, A Zhang, H Liu, H Song, Z Li, J Chen, T W Randolph, W L A Koay, J R White, L J Launer, A A Fodor, K A Meyer, M C Wu, Nat. Commun. 1354182022</p>
<p>voom: Precision weights unlock linear model analysis tools for RNA-seq read counts. C W Law, Y Chen, W Shi, G K Smyth, Genome Biol. 15R292014</p>
<p>A survey on deep learning in medical image analysis. G Litjens, T Kooi, B E Bejnordi, A A A Setio, F Ciompi, M Ghafoorian, J A W M Van Der Laak, B Van Ginneken, C I Sánchez, Med. Image Anal. 422017</p>
<p>Collaborative Unsupervised Domain Adaptation for Medical Image Diagnosis. Y Zhang, Y Wei, Q Wu, P Zhao, S Niu, J Huang, M Tan, IEEE Trans. Image Process. 292020</p>
<p>Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation. C Chen, Q Dou, H Chen, J Qin, P.-A Heng, AAAI. 332019</p>
<p>A Survey of Data Mining and Deep Learning in Bioinformatics. K Lan, D.-T Wang, S Fong, L.-S Liu, K K L Wong, N Dey, J. Med. Syst. 421392018</p>
<p>K A Shastry, H A Sanjay, Machine Learning for Bioinformatics" in Statistical Modelling and Machine Learning Principles for Bioinformatics. K G Applications, G M Srinivasa, S R Siddesh, Manisekhar, Singapore; SingaporeSpringer2020</p>
<p>Computational Biology in the 21st Century: Scaling with Compressive Algorithms. B Berger, N M Daniels, Y W Yu, Commun. ACM. 592016</p>
<p>Machine learning applications in genetics and genomics. M W Libbrecht, W S Noble, Nat. Rev. Genet. 162015</p>
<p>Functional magnetic resonance imaging (fMRI) "brain reading": detecting and classifying distributed patterns of fMRI activity in human visual cortex. D D Cox, R L Savoy, 10.1016/s1053-8119(03)00049-1s1053-8119(03)00049-12003Preprint</p>
<p>A Model of Representational Spaces in Human Cortex. J S Guntupalli, M Hanke, Y O Halchenko, A C Connolly, P J Ramadge, J V Haxby, Cereb. Cortex. 262016</p>
<p>A common, high-dimensional model of the representational space in human ventral temporal cortex. J V Haxby, J S Guntupalli, A C Connolly, Y O Halchenko, B R Conroy, M I Gobbini, M Hanke, P J Ramadge, Neuron. 722011</p>
<p>. P I Costea, G Zeller, S Sunagawa, E Pelletier, A Alberti, F Levenez, M Tramontano, M Driessen, R Hercog, F.-E Jung, J R Kultima, M R Hayward, L P Coelho, E Allen-Vercoe, L Bertrand, M Blaut, J R M Brown, T Carton, S Cools-Portier, M Daigneault, M Derrien, A Druesne, W M De Vos, B B Finlay, H J Flint, F Guarner, M Hattori, H Heilig, R A Luna, J Van Hylckama, J Vlieg, I Junick, P Klymiuk, E Langella, V Le Chatelier, C Mai, J C Manichanh, C Martin, H Mery, P W Morita, C O'toole, K R Orvain, J Patil, S Penders, N Persson, M Pons, A Popova, D Salonen, K P Saulnier, B Scott, K Singh, P Slezak, J Veiga, L Versalovic, E G Zhao, S D Zoetendal, J Ehrlich, P Dore, Bork, Nat. Biotechnol. 352017Towards standards for human fecal sample processing in metagenomic studies</p>
<p>Identifying and Overcoming Threats to Reproducibility, Replicability, Robustness, and Generalizability in Microbiome Research. P D Schloss, MBio. 92018</p>
<p>Combating the Small Sample Class Imbalance Problem Using Feature Selection. M Wasikowski, X.-W Chen, IEEE Trans. Knowl. Data Eng. 222010</p>
<p>Learning from Imbalanced Data. H He, E A Garcia, IEEE Trans. Knowl. Data Eng. 212009</p>
<p>Machine learning and complex biological data. C Xu, S A Jackson, Genome Biol. 20762019</p>
<p>Statistics versus machine learning. D Bzdok, N Altman, M Krzywinski, Nat. Methods. 152018</p>
<p>Naıve bayes domain adaptation for biological sequences. N Herndon, D Caragea, Proceedings of the 4th International Conference on Bioinformatics Models. the 4th International Conference on Bioinformatics Models2013BIOINFORMATICS</p>
<p>PRECISE: a domain adaptation approach to transfer predictors of drug response from pre-clinical models to tumors. S Mourragui, M Loog, M A Van De Wiel, M J T Reinders, L F A Wessels, Bioinformatics. 352019</p>
<p>Weighted elastic net for unsupervised domain adaptation with application to age prediction from DNA methylation data. L Handl, A Jalali, M Scherer, R Eggeling, N Pfeifer, Bioinformatics. 352019</p>
<p>Identifying Autism Spectrum Disorder With Multi-Site fMRI via Low-Rank Domain Adaptation. M Wang, D Zhang, J Huang, P.-T Yap, D Shen, M Liu, 10.1109/tmi.2019.29331602020Preprint</p>
<p>Learning Neural Representations of Human Cognition across Many fMRI Studies. A Mensch, J Mairal, D Bzdok, B Thirion, G Varoquaux, arXiv [stat.ML]2017</p>
<p>Transfer Learning on fMRI Datasets. H Zhang, P.-H Chen, P Ramadge, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics. A Storkey, F Perez-Cruz, Eds Pmlr, the Twenty-First International Conference on Artificial Intelligence and Statistics201884of Proceedings of Machine Learning Research</p>
<p>Meta-analysis reveals the vaginal microbiome is a better predictor of earlier than later preterm birth. C Huang, C Gin, J Fettweis, B Foxman, B Gelaye, D A Macintyre, A Subramaniam, W Fraser, N Tabatabaei, B Callahan, BMC Biol. 211992023</p>
<p>. J L Golob, T T Oskotsky, A S Tang, A Roldan, V Chung, C W Y Ha, R J Wong, K J Flynn, A Parraga-Leo, C Wibrand, S S Minot, B Oskotsky, G Andreoletti, I Kosti, J Bletz, A Nelson, J Gao, Z Wei, G Chen, Z.-Z Tang, P Novielli, D Romano, E Pantaleo, N Amoroso, A Monaco, M Vacca, M De Angelis, R Bellotti, S Tangaro, A Kuntzleman, I Bigcraft, S Techtmann, D Bae, E Kim, J Jeon, S Joe, Preterm Birth, K R Community, S Theis, Y S Ng, P Lee, P R Diaz-Gimeno, D A Bennett, G Macintyre, S V Stolovitzky, J Lynch, N Albrecht, R Gomez-Lopez, D K Romero, N Stevenson, A L Aghaeepour, J C Tarca, M Costello, Sirota, Cell Rep Med. 51013502024Microbiome preterm birth DREAM challenge: Crowdsourcing machine learning approaches to advance preterm birth research</p>
<p>Meta-analysis of fecal metagenomes reveals global microbial signatures that are specific for colorectal cancer. J Wirbel, P T Pyl, E Kartal, K Zych, A Kashani, A Milanese, J S Fleck, A Y Voigt, A Palleja, R Ponnudurai, S Sunagawa, L P Coelho, P Schrotz-King, E Vogtmann, N Habermann, E Niméus, A M Thomas, P Manghi, S Gandini, D Serrano, S Mizutani, H Shiroma, S Shiba, T Shibata, S Yachida, T Yamada, L Waldron, A Naccarati, N Segata, R Sinha, C M Ulrich, H Brenner, M Arumugam, P Bork, G Zeller, Nat. Med. 252019</p>
<p>Metagenomic analysis of colorectal cancer datasets identifies cross-cohort microbial diagnostic signatures and a link with choline degradation. A M Thomas, P Manghi, F Asnicar, E Pasolli, F Armanini, M Zolfo, F Beghini, S Manara, N Karcher, C Pozzi, S Gandini, D Serrano, S Tarallo, A Francavilla, G Gallo, M Trompetto, G Ferrero, S Mizutani, H Shiroma, S Shiba, T Shibata, S Yachida, T Yamada, J Wirbel, P Schrotz-King, C M Ulrich, H Brenner, M Arumugam, P Bork, G Zeller, F Cordero, E Dias-Neto, Nat. Med. J. C. Setubal, A. Tett, B. Pardini, M. Rescigno, L. Waldron, A. Naccarati, N. Segata252019</p>
<p>AI for radiographic COVID-19 detection selects shortcuts over signal. A J Degrave, J Janizek, S.-I Lee, Nature Machine Intelligence. 32021</p>
<p>Tackling the widespread and critical impact of batch effects in high-throughput data. J T Leek, R B Scharpf, H C Bravo, D Simcha, B Langmead, W E Johnson, D Geman, K Baggerly, R A Irizarry, Nat. Rev. Genet. 112010</p>
<p>P Liu, X Qiu, X Huang, arXiv [cs.CLAdversarial Multi-task Learning for Text Classification. 2017</p>
<p>ImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Commun. ACM. 602017</p>
<p>Deep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Moment matching for multi-source domain adaptation. X Peng, Q Bai, X Xia, Z Huang, K Saenko, B Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proc. IEEE. IEEE199886</p>
<p>80 million tiny images: a large data set for nonparametric object and scene recognition. A Torralba, R Fergus, W T Freeman, IEEE Trans. Pattern Anal. Mach. Intell. 302008</p>
<p>Learning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Others, 2009</p>
<p>The curse(s) of dimensionality. N Altman, M Krzywinski, Nat. Methods. 152018</p>
<p>Small sample sizes reduce the replicability of task-based fMRI studies. B O Turner, E J Paul, M B Miller, A K Barbey, Commun Biol. 1622018</p>
<p>Improving Whole-Brain Neural Decoding of fMRI with Domain Adaptation. S Zhou, C R Cox, H Lu, Machine Learning in Medical Imaging. Springer International Publishing2019</p>
<p>Adapting Visual Category Models to New Domains. K Saenko, B Kulis, M Fritz, T Darrell, Computer Vision -ECCV 2010. Berlin; HeidelbergSpringer2010</p>
<p>M Long, H Zhu, J Wang, M I Jordan, arXiv [cs.LG]Deep Transfer Learning with Joint Adaptation Networks. 2016</p>
<p>Learning Transferable Features with Deep Adaptation Networks. M Long, Y Cao, J Wang, M Jordan, Proceedings of the 32nd International Conference on Machine Learning. D Bach, Eds Blei, Pmlr, the 32nd International Conference on Machine LearningLille, France201537of Proceedings of Machine Learning Research</p>
<p>E Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell, arXiv [cs.CV]Deep Domain Confusion: Maximizing for Domain Invariance. 2014</p>
<p>I Shavitt, E Segal, arXiv [stat.ML]Regularization Learning Networks: Deep Learning for Tabular Datasets. 2018</p>
<p>. S Ö Arik, T Pfister, TabNet: Attentive Interpretable Tabular Learning. AAAI. 352021</p>
<p>When Do Neural Nets Outperform Boosted Trees on Tabular Data?. D Mcelfresh, S Khandagale, J Valverde, V Prasad, C , G Ramakrishnan, M Goldblum, C White, Adv. Neural Inf. Process. Syst. 362023</p>
<p>Why do tree-based models still outperform deep learning on typical tabular data?. L Grinsztajn, E Oyallon, G Varoquaux, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>FDA: Fourier domain adaptation for semantic segmentation. Y Yang, S Soatto, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE2020</p>
<p>Conditional generative adversarial network for structured domain adaptation. W Hong, Z Wang, M Yang, J Yuan, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE2018</p>
<p>Few-shot adversarial domain adaptation. S Motiian, Q Jones, S Iranmanesh, G Doretto, Adv. Neural Inf. Process. Syst. 302017</p>
<p>K Sohn, S Liu, G Zhong, X Yu, M.-H Yang, M Chandraker, arXiv [cs.CVUnsupervised Domain Adaptation for Face Recognition in Unlabeled Videos. 2017</p>
<p>Multiple Edit/Multiple Imputation for Multivariate Continuous Data. B Ghosh-Dastidar, J L Schafer, J. Am. Stat. Assoc. 982003</p>
<p>Imputation of missing values of tumour stage in population-based cancer registration. N Eisemann, A Waldmann, A Katalinic, BMC Med. Res. Methodol. 111292011</p>
<p>MAGIC: A diffusion-based imputation method reveals gene-gene interactions in single-cell RNA-sequencing data. D Van Dijk, J Nainys, R Sharma, P Kaithail, A J Carr, K R Moon, L Mazutis, G Wolf, S Krishnaswamy, D Pe'er, bioRxiv. 1115912017</p>
<p>M Zitnik, F Nguyen, B Wang, J Leskovec, A Goldenberg, M M Hoffman, Machine Learning for Integrating Data in Biology and Medicine: Principles, Practice, and Opportunities. 201950</p>
<p>Identification of nonlinear biological systems using Laguerre expansions of kernels. V Z Marmarelis, Ann. Biomed. Eng. 211993</p>
<p>FsNet: Feature Selection Network on High-dimensional Biological Data. D Singh, H Climente-Gonzalez, M Petrovich, E Kawakami, M Yamada, 2023 International Joint Conference on Neural Networks (IJCNN). IEEE2023</p>
<p>Statistical analysis of microbiome data: The challenge of sparsity. A Y Pan, Current Opinion in Endocrine and Metabolic Research. 192021</p>
<p>Latent Representation Learning for Alzheimer's Disease Diagnosis With Incomplete Multi-Modality Neuroimaging and Genetic Data. T Zhou, M Liu, K.-H Thung, D Shen, IEEE Trans. Med. Imaging. 382019</p>
<p>Estimating the prevalence of missing experiments in a neuroimaging meta-analysis. P Samartsidis, S Montagna, A R Laird, P T Fox, T D Johnson, T E Nichols, Res. Synth. Methods. 112020</p>
<p>Multi-modal latent space inducing ensemble SVM classifier for early dementia diagnosis with neuroimaging data. T Zhou, K.-H Thung, M Liu, F Shi, C Zhang, D Shen, Med. Image Anal. 601016302020</p>
<p>Hybrid Hyperalignment: A single high-dimensional model of shared information embedded in cortical patterns of response and functional connectivity. E L Busch, L Slipski, M Feilong, J Swaroop, M V Guntupalli, J F Di Oleggio Castello, S A Huckins, M Nastase, Ida, T D Gobbini, J V Wager, Haxby, 2020.11.25.3988832020Cold Spring Harbor Laboratory</p>
<p>A General Domain Specific Feature Transfer Framework for Hybrid Domain Adaptation. P Wei, Y Ke, C K Goh, IEEE Trans. Knowl. Data Eng. 312019</p>
<p>Heterogeneous Domain Adaptation using Manifold Alignment. C Wang, S Mahadevan, Twenty-Second International Joint Conference on Artificial Intelligence. 2011</p>
<p>High-resolution sample inference from Illumina amplicon data. B J Callahan, P J Mcmurdie, M J Rosen, A W Han, A J A Johnson, S P Holmes, Nat. Methods. 22016</p>
<p>A Amir, D Mcdonald, J A Navas-Molina, E Kopylova, J T Morton, Z Xu, E P Kightley, L R Thompson, E R Hyde, A Gonzalez, R Knight, Deblur Rapidly Resolves Single-Nucleotide Community Sequence Patterns. mSystems. 20172</p>
<p>fMRIPrep: a robust preprocessing pipeline for functional MRI. O Esteban, C J Markiewicz, R W Blair, C A Moodie, A I Isik, A Erramuzpe, J D Kent, M Goncalves, E Dupre, M Snyder, H Oya, S S Ghosh, J Wright, J Durnez, R A Poldrack, K J Gorgolewski, Nat. Methods. 162019</p>
<p>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages. R W Cox, Comput. Biomed. Res. 291996</p>
<p>Software tools for analysis and visualization of fMRI data. R W Cox, J S Hyde, NMR Biomed. 101997</p>
<p>Bayesian analysis of neuroimaging data in FSL. M W Woolrich, S Jbabdi, B Patenaude, M Chappell, S Makni, T Behrens, C Beckmann, M Jenkinson, S M Smith, Neuroimage. 452009</p>
<p>S M Smith, M Jenkinson, M W Woolrich, C F Beckmann, T E J Behrens, H Johansen-Berg, P R Bannister, M De Luca, I Drobnjak, D E Flitney, R K Niazy, J Saunders, J Vickers, Y Zhang, N De Stefano, J M Brady, P M Matthews, Advances in functional and structural MR image analysis and implementation as FSL. 200423Suppl</p>
<p>. M Jenkinson, C F Beckmann, T E J Behrens, M W Woolrich, S M Smith, Fsl , Neuroimage. 622012</p>
<p>Coma Research Centre (CRC) -Besta Institute, Impact of functional MRI data preprocessing pipeline on default-mode network detectability in patients with disorders of consciousness. A Andronache, C Rosazza, D Sattin, M Leonardi, L D'incerti, L Minati, Front. Neuroinform. 7162013</p>
<p>Gut microbiome, big data and machine learning to promote precision medicine for cancer. G Cammarota, G Ianiro, A Ahern, C Carbone, A Temko, M J Claesson, A Gasbarrini, G Tortora, Nat. Rev. Gastroenterol. Hepatol. 172020</p>
<p>Experimenting with reproducibility: a case study of robustness in bioinformatics. Y.-M Kim, J.-B Poline, G Dumas, Gigascience. 72018</p>
<p>Domain Adaptation via Low Rank and Class Discriminative Representation for Autism Spectrum Disorder identification: A Multi-site fMRI Study. X Liu, J Wu, W Li, Q Liu, L Tian, H Huang, IEEE Trans. Neural Syst. Rehabil. Eng. PP. 2023</p>
<p>Fader networks for domain adaptation on fMRI: ABIDE-II study. M Pominova, E Kondrateva, M Sharaev, A Bernstein, E Burnaev, Thirteenth International Conference on Machine Vision (SPIE, 2021). 11605</p>
<p>Alterations of functional connectivities associated with autism spectrum disorder symptom severity: a multi-site study using multivariate pattern analysis. X Liu, H Huang, Sci. Rep. 1043302020</p>
<p>Transfer learning improves supervised image segmentation across imaging protocols. A Van Opbroek, M A Ikram, M W Vernooij, M De Bruijne, IEEE Trans. Med. Imaging. 342015</p>
<p>W M Kouw, M Loog, L W Bartels, A M Mendrik, arXiv [cs.CV]MR Acquisition-Invariant Representation Learning. 2017</p>
<p>Heterogeneous Graph Attention Network for Unsupervised Multiple-Target Domain Adaptation. X Yang, C Deng, T Liu, D Tao, IEEE Trans. Pattern Anal. Mach. Intell. 441992-2003 (2022</p>
<p>Heterogeneous Domain Adaptation: An Unsupervised Approach. F Liu, G Zhang, J Lu, IEEE Trans Neural Netw Learn Syst. 312020</p>
<p>Advances in computer vision and pattern recognition. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, 2017Springer International PublishingChamDomain-adversarial training of neural networks</p>
<p>Interpretable deep-learning models to help achieve the Sustainable Development Goals. R Vinuesa, B Sirmacek, Nature Machine Intelligence. 32021</p>
<p>A Closer Look at Memorization in Deep Networks. D Arpit, S Jastrzębski, N Ballas, D Krueger, E Bengio, M S Kanwal, T Maharaj, A Fischer, A Courville, Y Bengio, S Lacoste-Julien, Proceedings of the 34th International Conference on Machine Learning. D Precup, Y W Teh, Eds Pmlr, the 34th International Conference on Machine LearningAug 201770of Proceedings of Machine Learning Research</p>
<p>Understanding Black-box Predictions via Influence Functions. P W Koh, P Liang, Proceedings of the 34th International Conference on Machine Learning. D Precup, Y W Teh, Eds Pmlr, the 34th International Conference on Machine LearningAug 201770of Proceedings of Machine Learning Research</p>
<p>Understanding the limits of unsupervised domain adaptation via data poisoning. A Mehra, B Kailkhura, P.-Y Chen, J Hamm, Adv. Neural Inf. Process. Syst. 2021</p>
<p>Impossibility theorems for domain adaptation. S Ben-David, T Lu, T Luu, D Pál, AISTATS. 2010</p>
<p>On the analysis of adaptability in multi-source domain adaptation. I Redko, A Habrard, M Sebban, Mach. Learn. 1082019</p>
<p>Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers. H Liu, M Long, J Wang, M Jordan, Proceedings of the 36th International Conference on Machine Learning. K Chaudhuri, R Salakhutdinov, Eds Pmlr, the 36th International Conference on Machine Learning15 Jun 201997of Proceedings of Machine Learning Research</p>
<p>Characterizing and Avoiding Negative Transfer. Z Wang, Z Dai, B Póczos, J Carbonell, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR. IEEE2019</p>
<p>Word sense disambiguation with distribution estimation. Y S Chan, H T Ng, </p>
<p>An introduction to domain adaptation and transfer learning. W M Kouw, M Loog, arXiv [cs.LG]2018</p>
<p>Deep visual domain adaptation: A survey. M Wang, W Deng, Neurocomputing. 2018</p>
<p>A Survey of Unsupervised Deep Domain Adaptation. G Wilson, D J Cook, ACM Trans Intell Syst Technol. 112020</p>
<p>G Csurka, arXiv [cs.CVDomain Adaptation for Visual Applications: A Comprehensive Survey. 2017</p>
<p>Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives. X Liu, C Yoo, F Xing, H Oh, G El Fakhri, J.-W Kang, J Woo, APSIPA Transactions on Signal and Information Processing. 112022</p>
<p>S Zhao, B Li, C Reed, P Xu, K Keutzer, arXiv [cs.LG]Multi-source Domain Adaptation in the Deep Learning Era: A Systematic Survey. 2020</p>
<p>Unsupervised visual domain adaptation using subspace alignment. B Fernando, A Habrard, M Sebban, T Tuytelaars, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2013</p>
<p>Geodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, 2012 IEEE Conference on Computer Vision and Pattern Recognition. 2012ieeexplore.ieee.org</p>
<p>Learning Transferable Features with Deep Adaptation Networks. M Long, Y Cao, J Wang, M Jordan, Proceedings of the 32nd International Conference on Machine Learning. D Bach, Eds Blei, Pmlr, the 32nd International Conference on Machine LearningLille, France201537of Proceedings of Machine Learning Research</p>
<p>Deep CORAL: Correlation Alignment for Deep Domain Adaptation. B Sun, K Saenko, Computer Vision -ECCV 2016 Workshops. Springer International Publishing2016</p>
<p>A survey of multi-source domain adaptation. S Sun, H Shi, Y Wu, Inf. Fusion. 2015</p>
<p>H S Bhatt, A Rajkumar, S Roy, Multi-Source Iterative Adaptation for Cross-Domain Classification. </p>
<p>Domain Generalization Using a Mixture of Multiple Latent Domains. T Matsuura, T Harada, AAAI. 342020</p>
<p>Wasserstein Barycenter for Multi-Source Domain Adaptation. E F Montesuma, F M N Mboula, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR. IEEE2021</p>
<p>Domain Adaptation with Multiple Sources. Y Mansour, M Mohri, A Rostamizadeh, Adv. Neural Inf. Process. Syst. 212008</p>
<p>Aligning Domain-Specific Distribution and Classifier for Cross-Domain Classification from Multiple Sources. Y Zhu, F Zhuang, D Wang, AAAI. 332019</p>
<p>Deep cocktail network: Multi-source unsupervised domain adaptation with category shift. R Xu, Z Chen, W Zuo, J Yan, L Lin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Adversarial multiple source domain adaptation. H Zhao, S Zhang, G Wu, J M F Moura, J P Costeira, G J Gordon, Adv. Neural Inf. Process. Syst. 312018</p>
<p>Domain Adaptation for Medical Image Analysis: A Survey. H Guan, M Liu, IEEE Trans. Biomed. Eng. 692022</p>
<p>H Daumé, arXiv [cs.LG]Frustratingly Easy Domain Adaptation. 2009</p>
<p>Asymmetric Tri-training for Unsupervised Domain Adaptation. K Saito, Y Ushiku, T Harada, Proceedings of the 34th International Conference on Machine Learning. D Precup, Y W Teh, Eds Pmlr, the 34th International Conference on Machine LearningAug 201770of Proceedings of Machine Learning Research</p>
<p>A Shrivastava, T Pfister, O Tuzel, J Susskind, W Wang, R Webb, arXiv [cs.CVLearning from Simulated and Unsupervised images through adversarial training. 2016</p>
<p>Deep Unsupervised Convolutional Domain Adaptation. J Zhuo, S Wang, W Zhang, Q Huang, Proceedings of the 25th ACM International Conference on Multimedia. the 25th ACM International Conference on MultimediaNew York, NY, USAAssociation for Computing Machinery201717</p>
<p>Learning With Augmented Features for Supervised and Semi-Supervised Heterogeneous Domain Adaptation. Wen Li, Lixin Duan, Dong Xu, I W Tsang, IEEE Trans. Pattern Anal. Mach. Intell. 362014</p>
<p>Simultaneous Deep Transfer Across Domains and Tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, 10.1109/iccv.2015.4632015Preprint</p>
<p>Semi-supervised domain adaptation via minimax entropy. K Saito, D Kim, S Sclaroff, T Darrell, K Saenko, 2019 IEEE/CVF International Conference on Computer Vision (ICCV. IEEE2019</p>
<p>Improving predictive inference under covariate shift by weighting the log-likelihood function. H Shimodaira, 10.1016/s0378-3758(00)00115-42000Preprint</p>
<p>Cross-domain video concept detection using adaptive svms. J Yang, R Yan, A G Hauptmann, MM '07Proceedings of the 15th ACM International Conference on Multimedia. the 15th ACM International Conference on MultimediaNew York, NY, USAAssociation for Computing Machinery2007</p>
<p>Domain adaptation from multiple sources via auxiliary classifiers. L Duan, I W Tsang, D Xu, T.-S Chua, ICML '09Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningNew York, NY, USAAssociation for Computing Machinery2009</p>
<p>Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach. L Duan, D Xu, S.-F Chang, 2012 IEEE Conference on Computer Vision and Pattern Recognition. 2012</p>
<p>Heterogeneous Domain Adaptation for Multiple Classes. J T Zhou, I W Tsang, S J Pan, M Tan, Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics. S Kaski, J Corander, Eds Pmlr, the Seventeenth International Conference on Artificial Intelligence and StatisticsReykjavik, Iceland201433of Proceedings of Machine Learning Research</p>
<p>Return of Frustratingly Easy Domain Adaptation. B Sun, J Feng, K Saenko, AAAI. 302016</p>
<p>Multi-Source Domain Adaptation with Mixture of Experts. J Guo, D Shah, R Barzilay, 10.18653/v1/d18-14982018Preprint</p>
<p>Boosting domain adaptation by discovering latent domains. M Mancini, L Porzi, S R Bulo, B Caputo, E Ricci, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE2018</p>
<p>Multi-Source Distilling Domain Adaptation. S Zhao, G Wang, S Zhang, Y Gu, Y Li, Z Song, P Xu, R Hu, H Chai, K Keutzer, AAAI. 342020</p>
<p>Processing-bias correction with DEBIAS-M improves cross-study generalization of microbiome-based prediction models. G I Austin, A B Kav, H Park, J Biermann, A.-C Uhlemann, T Korem, 10.1101/2024.02.09.579716bioRxiv. 2024</p>
<p>AITL: Adversarial Inductive Transfer Learning with input and output space adaptation for pharmacogenomics. H Sharifi-Noghabi, S Peng, O Zolotareva, C C Collins, M Ester, Bioinformatics. 362020</p>
<p>. A Di Martino, C.-G Yan, Q Li, E Denio, F X Castellanos, K Alaerts, J S Anderson, M Assaf, S Y Bookheimer, M Dapretto, B Deen, S Delmonte, I Dinstein, B Ertl-Wagner, D A Fair, L Gallagher, D P Kennedy, C L Keown, C Keysers, J E Lainhart, C Lord, B Luna, V Menon, N J Minshew, C S Monk, S Mueller, R.-A Müller, M B Nebel, J T Nigg, K O'hearn, K A Pelphrey, S J Peltier, Mol. Psychiatry. J. D. Rudie, S. Sunaert, M. Thioux, J. M. Tyszka, L. Q. Uddin, J. S. Verhoeven, N. Wenderoth, J. L. Wiggins, S. H. Mostofsky, M. P. Milham192014The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</p>
<p>Mixture of experts: a literature survey. S Masoudnia, R Ebrahimpour, Artificial Intelligence Review. 422014</p>
<p>N Shazeer, A Mirhoseini, K Maziarz, A Davis, Q Le, G Hinton, J Dean, arXiv [cs.LGOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. 2017</p>
<p>Decoding Brain States From fMRI Signals by Using Unsupervised Domain Adaptation. Y Gao, Y Zhang, Z Cao, X Guo, J Zhang, IEEE J Biomed Health Inform. 242020</p>
<p>The Human Connectome Project: a data acquisition perspective. D C Van Essen, K Ugurbil, E Auerbach, D Barch, T E J Behrens, R Bucholz, A Chang, L Chen, M Corbetta, S W Curtiss, S Della Penna, D Feinberg, M F Glasser, N Harel, A C Heath, L Larson-Prior, D Marcus, G Michalareas, S Moeller, R Oostenveld, S E Petersen, F Prior, B L Schlaggar, S M Smith, A Z Snyder, J Xu, E Yacoub, Hcp Wu-Minn, Consortium, Neuroimage. 622012</p>
<p>Task-relevant autoencoding" enhances machine learning for human neuroscience. S Orouji, V Taschereau-Dumouchel, A Cortese, arXiv preprint arXiv. 2022</p>
<p>Regularizing label-augmented generative adversarial networks under limited data. L Hou, IEEE Access. 112023</p>
<p>R Webster, J Rabin, L Simon, F Jurie, arXiv [cs.LGDetecting overfitting of deep generative networks via latent recovery. 2019</p>
<p>A Gonzalez, J A Navas-Molina, T Kosciolek, D Mcdonald, Y Vázquez-Baeza, G Ackermann, J Dereus, S Janssen, A D Swafford, S B Orchanian, J G Sanders, J Shorenstein, H Holste, S Petrus, A Robbins-Pianka, C J Brislawn, M Wang, J R Rideout, E Bolyen, M Dillon, J G Caporaso, P C Dorrestein, R Knight, Qiita: rapid, web-enabled microbiome meta-analysis. 201815</p>
<p>Accessible, curated metagenomic data through ExperimentHub. E Pasolli, L Schiffer, P Manghi, A Renson, V Obenchain, D T Truong, F Beghini, F Malik, M Ramos, J B Dowd, C Huttenhower, M Morgan, N Segata, L Waldron, Nat. Methods. 142017</p>
<p>Achieving pan-microbiome biological insights via the dbBact knowledge base. A Amir, E Ozel, Y Haberman, N Shental, Nucleic Acids Res. 512023</p>
<p>Integration of 168,000 samples reveals global patterns of the human gut microbiome. R J Abdill, S P Graham, V Rubinetti, F W Albert, C S Greene, S Davis, R Blekhman, 10.1101/2023.10.11.560955bioRxiv. 2023</p>
<p>Frustratingly easy semi-supervised domain adaptation. H Daumé, Iii , A Kumar, A Saha, Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing. the 2010 Workshop on Domain Adaptation for Natural Language Processing2010</p>
<p>Evaluation of Domain Adaptation Approaches for Robust Classification of Heterogeneous Biological Data Sets. M Schneider, L Wang, C Marr, Artificial Neural Networks and Machine Learning -ICANN 2019: Deep Learning. Springer International Publishing2019</p>
<p>Domain adaptation with structural correspondence learning. J Blitzer, R Mcdonald, F Pereira, Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. the 2006 Conference on Empirical Methods in Natural Language Processing2006</p>
<p>Domain adaptation on the statistical manifold. M Baktashmotlagh, M T Harandi, B C Lovell, M Salzmann, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2014</p>
<p>Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation. B Gong, K Grauman, F Sha, Proceedings of the 30th International Conference on Machine Learning. D Dasgupta, Eds Mcallester, Pmlr, the 30th International Conference on Machine LearningAtlanta, Georgia, USA201328Proceedings of Machine Learning Research</p>
<p>Relations Between Two Sets of Variates. H Hotelling, 10.2307/23339551936Preprint</p>
<p>Canonical correlation analysis: an overview with application to learning methods. D R Hardoon, S Szedmak, J Shawe-Taylor, Neural Comput. 162004</p>
<p>Heterogeneous domain adaptation and classification by exploiting the correlation subspace. Y.-R Yeh, C.-H Huang, Y.-C F Wang, IEEE Trans. Image Process. 232014</p>
<p>. P L Lai, C Fyfe, Kernel And Nonlinear Canonical Correlation Analysis, 10.1142/s012906570000034x2000Preprint</p>
<p>F R Bach, Kernel Independent Component Analysis. 2002</p>
<p>J V Haxby, J S Guntupalli, S A Nastase, M Feilong, Hyperalignment: Modeling shared information encoded in idiosyncratic cortical topographies. 20209</p>
<p>A computational model of shared fine-scale structure in the human connectome. J S Guntupalli, M Feilong, J V Haxby, PLoS Comput. Biol. 14e10061202018</p>
<p>Manifold alignment without correspondence. C Wang, S Mahadevan, Twenty-First International Joint Conference on Artificial Intelligence. 2009</p>
<p>A Reduced-Dimension fMRI Shared Response Model. P.-H Chen, J Chen, Y Yeshurun, U Hasson, J Haxby, P J Ramadge, Advances in Neural Information Processing Systems. C Cortes, N Lawrence, D Lee, M Sugiyama, R Garnett, Curran Associates, Inc201528</p>
<p>Consistent and correctable bias in metagenomic sequencing experiments. M R Mclaren, A D Willis, B J Callahan, Elife. 82019</p>
<p>TMDA: Task-Specific Multi-source Domain Adaptation via Clustering Embedded Adversarial Training. H Wang, W Yang, Z Lin, Y Yu, 2019 IEEE International Conference on Data Mining (ICDM). 2019</p>
<p>Adversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Coupled Generative Adversarial Networks. M.-Y Liu, O Tuzel, Advances in Neural Information Processing Systems. D Lee, M Sugiyama, U Luxburg, I Guyon, R Garnett, Curran Associates, Inc201629</p>
<p>Generative Adversarial Nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in Neural Information Processing Systems. Z Ghahramani, M Welling, C Cortes, N Lawrence, K Q Weinberger, Curran Associates, Inc201427</p>
<p>Learning Deep Architectures for AI. Y Bengio, 2009Now Publishers Inc</p>
<p>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. P Vincent, 2010. ------------------------</p>
<p>Domain Separation Networks. K Bousmalis, G Trigeorgis, N Silberman, D Krishnan, D Erhan, Advances in Neural Information Processing Systems. D Lee, M Sugiyama, U Luxburg, I Guyon, R Garnett, Curran Associates, Inc201629</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>J Hoffman, M Mohri, N Zhang, arXiv [cs.LG]Algorithms and Theory for Multiple-Source Adaptation. 2018</p>
<p>P.-H Chen, X Zhu, H Zhang, J S Turek, J Chen, T L Willke, U Hasson, P J Ramadge, arXiv [stat.ML]A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation. 2016</p>
<p>An efficient LSTM network for emotion recognition from multichannel EEG signals. X Du, C Ma, G Zhang, J Li, Y.-K Lai, G Zhao, X Deng, Y.-J Liu, H Wang, IEEE Trans. Affect. Comput. 2020</p>
<p>Continuous Gesture Recognition from sEMG Sensor Data with Recurrent Neural Networks and Adversarial Domain Adaptation. I Sosin, D Kudenko, A Shpilman, 2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV. IEEE2018</p>
<p>Attention mechanism enhanced LSTM with residual architecture and its application for protein-protein interaction residue pairs prediction. J Liu, X Gong, BMC Bioinformatics. 206092019</p>
<p>Identifying Protein-Protein Interaction Using Tree LSTM and Structured Attention. M Ahmed, J Islam, M R Samee, R E Mercer, 2019 IEEE 13th International Conference on Semantic Computing (ICSC). 2019</p>
<p>Unsupervised Domain Adaptation by Backpropagation. Y Ganin, V Lempitsky, Proceedings of the 32nd International Conference on Machine Learning. D Bach, Eds Blei, Pmlr, the 32nd International Conference on Machine LearningLille, FranceJul 201537of Proceedings of Machine Learning Research</p>
<p>Distribution-matching embedding for visual domain adaptation. M Baktashmotlagh, M Salzmann, U Dogan, M Kloft, F Orabona, T Tommasi, </p>
<p>CyCADA: Cycle-Consistent Adversarial Domain Adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A Efros, T Darrell, Proceedings of the 35th International Conference on Machine Learning. J Dy, A Krause, Eds Pmlr, the 35th International Conference on Machine LearningJul 201880of Proceedings of Machine Learning Research</p>
<p>Unsupervised Multi-source Domain Adaptation Driven by Deep Adversarial Ensemble Learning. S Rakshit, B Banerjee, G Roig, S Chaudhuri, Pattern Recognition. Springer International Publishing2019</p>
<p>A kernel method for the two-sample-problem. A Gretton, K Borgwardt, M Rasch, B Schölkopf, A Smola, Adv. Neural Inf. Process. Syst. 192006</p>
<p>Contrastive adaptation network for unsupervised domain adaptation. G Kang, L Jiang, Y Yang, A G Hauptmann, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR. IEEE2019</p>
<p>Multi-Source Domain Adaptation for Text Classification via DistanceNet-Bandits. H Guo, R Pasunuru, M Bansal, AAAI. 342020</p>
<p>DeepJDOT: Deep joint distribution optimal transport for unsupervised domain adaptation. B B Damodaran, B Kellenberger, R Flamary, D Tuia, N Courty, arXiv [cs.CV]2018</p>
<p>Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation. M Ghifary, W B Kleijn, M Zhang, D Balduzzi, W Li, Computer Vision -ECCV 2016. Springer International Publishing2016</p>
<p>Domain generalization for object recognition with Multi-task autoencoders. M Ghifary, W B Kleijn, M Zhang, D Balduzzi, arXiv [cs.CV2015</p>
<p>Learning to adapt structured output space for semantic segmentation. Y.-H Tsai, W.-C Hung, S Schulter, K Sohn, M.-H Yang, M Chandraker, arXiv [cs.CV]2018</p>
<p>Unpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, 2017 IEEE International Conference on Computer Vision (ICCV. </p>
<p>CycleEmotionGAN: Emotional Semantic Consistency Preserved CycleGAN for Adapting Image Emotions. S Zhao, C Lin, P Xu, S Zhao, Y Guo, R Krishna, G Ding, K Keutzer, AAAI. 332019</p>
<p>DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images. M Goetz, C Weber, F Binczyk, J Polanska, R Tarnawski, B Bobek-Billewicz, U Koethe, J Kleesiek, B Stieltjes, K H Maier-Hein, IEEE Trans. Med. Imaging. 352016</p>
<p>SODA: Detecting COVID-19 in Chest X-rays with Semi-supervised Open Set Domain Adaptation. J Zhou, B Jing, Z Wang, H Xin, H Tong, IEEE/ACM Trans. Comput. Biol. Bioinform. PP. 2021</p>
<p>Adapting SVM Classifiers to Data with Shifted Distributions. J Yang, R Yan, A G Hauptmann, Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007. 2007</p>
<p>A SVM-based model-transferring method for heterogeneous domain adaptation. A S Mozafari, M Jamzad, Pattern Recognit. 562016</p>
<p>SVM-Based Boosting of Active Learning Strategies for Efficient Domain Adaptation. G Matasci, D Tuia, M Kanevski, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 52012</p>
<p>Hierarchical Adaptive Structural SVM for Domain Adaptation. J Xu, S Ramos, D Vázquez, A M López, Int. J. Comput. Vis. 1192016</p>
<p>Domain Generalization and Adaptation Using Low Rank Exemplar SVMs. W Li, Z Xu, D Xu, D Dai, L Van Gool, IEEE Trans. Pattern Anal. Mach. Intell. 402018</p>
<p>. F Iorio, T A Knijnenburg, D J Vis, G R Bignell, M P Menden, M Schubert, N Aben, E Gonçalves, S Barthorpe, H Lightfoot, T Cokelaer, P Greninger, E Van Dyk, H Chang, H Silva, H Heyn, X Deng, R K Egan, Q Liu, T Mironenko, X Mitropoulos, L Richardson, J Wang, T Zhang, S Moran, S Sayols, M Soleimani, D Tamborero, N Lopez-Bigas, P Ross-Macdonald, M Esteller, N S Gray, D A Haber, M R Stratton, C H Benes, L F A Wessels, J Saez-Rodriguez, U Mcdermott, M J Garnett, A Landscape of Pharmacogenomic Interactions in Cancer. Cell. 1662016</p>
<p>. H Gao, J M Korn, S Ferretti, J E Monahan, Y Wang, M Singh, C Zhang, C Schnell, G Yang, Y Zhang, O A Balbin, S Barbe, H Cai, F Casey, S Chatterjee, D Y Chiang, S Chuai, S M Cogan, S D Collins, E Dammassa, N Ebel, M Embry, J Green, A Kauffmann, C Kowal, R J Leary, J Lehar, Y Liang, A Loo, E Lorenzana, E Robert Mcdonald 3rd, M E Mclaughlin, J Merkin, R Meyer, T L Naylor, M Patawaran, A Reddy, C Röelli, D A Ruddy, F Salangsang, F Santacroce, A P Singh, Y Tang, W Tinetto, S Tobler, R Velazquez, K Venkatesan, F Von Arx, H Q Wang, Z Wang, M Wiesmann, D Wyss, F Xu, H Bitter, P Atadja, E Lees, F Hofmann, E Li, N Keen, R Cozens, M R Jensen, N K Pryer, J A Williams, W R Sellers, Nat. Med. 212015High-throughput screening using patient-derived tumor xenografts to predict clinical trial drug response</p>
<p>Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. A Subramanian, P Tamayo, V K Mootha, S Mukherjee, B L Ebert, M A Gillette, A Paulovich, S L Pomeroy, T R Golub, E S Lander, J P Mesirov, Proc. Natl. Acad. Sci. U. S. A. 1022005</p>
<p>Dynamic DNA methylation across diverse human cell lines and tissues. K E Varley, J Gertz, K M Bowling, S L Parker, T E Reddy, F Pauli-Behn, M K Cross, B A Williams, J A Stamatoyannopoulos, G E Crawford, D M Absher, B J Wold, R M Myers, Genome Res. 232013</p>
<p>Interpretable per case weighted ensemble method for cancer associations. A Jalali, N Pfeifer, BMC Genomics. 175012016</p>
<p>LAmbDA: label ambiguous domain adaptation dataset integration reduces batch effects and improves subtype detection. T S Johnson, T Wang, Z Huang, C Y Yu, Y Wu, Y Han, Y Zhang, K Huang, J Zhang, Bioinformatics. 352019</p>
<p>scmap: projection of single-cell RNA-seq data across data sets. V Y Kiselev, A Yiu, M Hemberg, Nat. Methods. 2018</p>
<p>CaSTLe -Classification of single cells by transfer learning: Harnessing the power of publicly available single cell RNA sequencing experiments to annotate new experiments. Y Lieberman, L Rokach, T Shay, PLoS One. 13e02054992018</p>
<p>D P Kingma, J Ba, arXiv [cs.LG]Adam: A Method for Stochastic Optimization. 2014</p>
<p>Heterogeneous Domain Adaptation Based on Class Decomposition Schemes. F Ismailoglu, E Smirnov, R Peeters, S Zhou, P Collins, Advances in Knowledge Discovery and Data Mining. Springer International Publishing2018</p>
<p>Heterogeneous Domain Adaptation for IHC Classification of Breast Cancer Subtypes. F Ismailoglu, R Cavill, E Smirnov, S Zhou, P Collins, R Peeters, IEEE/ACM Trans. Comput. Biol. Bioinform. 172020</p>
<p>MSVM-RFE: extensions of SVM-RFE for multiclass gene selection on DNA microarray data. X Zhou, D P Tuck, Bioinformatics. 232007</p>
<p>Few shot domain adaptation for in situ macromolecule structural classification in cryoelectron tomograms. L Yu, R Li, X Zeng, H Wang, J Jin, Y Ge, R Jiang, M Xu, Bioinformatics. 372021</p>
<p>Domain Adaptation Using a Three-Way Decision Improves the Identification of Autism Patients from Multisite fMRI Data. C Shi, X Xin, J Zhang, Brain Sci. 112021</p>
<p>Integrating structured biological data by Kernel Maximum Mean Discrepancy. K M Borgwardt, A Gretton, M J Rasch, H.-P Kriegel, B Schölkopf, A J Smola, Bioinformatics. 222006</p>
<p>A Farshchian, J A Gallego, J P Cohen, Y Bengio, L E Miller, S A Solla, arXiv [cs.LG]Adversarial Domain Adaptation for Stable Brain-Machine Interfaces. 2018</p>
<p>Correcting sample selection bias by unlabeled data. J Huang, A Gretton, K Borgwardt, B Schölkopf, A Smola, Adv. Neural Inf. Process. Syst. 192006</p>
<p>Wasserstein Distance Guided Representation Learning for Domain Adaptation. J Shen, Y Qu, W Zhang, Y Yu, Thirty-Second AAAI Conference on Artificial Intelligence. 2018</p>
<p>Extracting relationships by multi-domain matching. Y Li, D E Carlson, Others, Adv. Neural Inf. Process. Syst. 312018</p>
<p>A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Y Freund, R E Schapire, J. Comput. System Sci. 551997</p>
<p>Bias, variance, and arcing classifiers. L Breiman, 1996Statistics Department, University of California, Berkeley …Tech. Rep. 460</p>
<p>Boosting for transfer learning. W Dai, Q Yang, G -R. Xue, Y Yu, 10.1145/1273496.12735212007Preprint</p>
<p>D Pardoe, P Stone, Boosting for Regression Transfer. 2010</p>
<p>Boosting for transfer learning with multiple sources. Y Yao, G Doretto, 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2010</p>
<p>F Altaf, S M S Islam, N K Janjua, N Akhtar, arXiv [cs.CV]Boosting Deep Transfer Learning for COVID-19 Classification. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2026 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2026</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2026</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-276724951</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.20834v1.pdf" target="_blank">Learning to Substitute Components for Compositional Generalization</a></p>
                <p><strong>Paper Abstract:</strong> Despite the rising prevalence of neural language models, recent empirical evidence suggests their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, which aims to introduce additional compositional inductive bias. However, existing handcrafted augmentation strategies offer limited improvement when systematic generalization of neural language models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases alone) or when training sentences have an imbalanced difficulty distribution. To address these challenges, we first propose a novel compositional augmentation strategy called Component Substitution (CompSub), which enables multi-grained composition of substantial substructures across the entire training set. Furthermore, we introduce the Learning Component Substitution (LCS) framework. This framework empowers the learning of component substitution probabilities in CompSub in an end-to-end manner by maximizing the loss of neural language models, thereby prioritizing challenging compositions with elusive concepts and novel contexts. We extend the key ideas of CompSub and LCS to the recently emerging in-context learning scenarios of pre-trained large language models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot compositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we provide insights into why applying our algorithms to language models can improve compositional generalization performance. Empirically, our results on four standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and COGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with improvements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2026.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2026.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCAN-Seq2Seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCAN compositional generalization experiments with LSTM sequence-to-sequence (attention+copy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of span-based augmentation (CompSub) and learned selective augmentation (LCS) on SCAN splits using an LSTM encoder-decoder with attention and copy; reports exact-match accuracy on standard SCAN splits (jump, around-right, length, MCD1-3).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM seq-to-seq (attention + copy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder LSTM sequence-to-sequence model with attention and copy mechanism (configurations: 1- or 2-layer variants used; embeddings and hidden sizes per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard LSTM encoder-decoder; attention mechanism; copy mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>procedural / sequential (navigation commands → action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SCAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic mapping from natural-language navigation commands to action-sequences; tests recombination of known primitives into novel action sequences and structural rules (lexical and structural composition).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>varies by split; includes longer-sequence generalization (length split) and maximal compound divergence (MCD splits); depth not precisely quantified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel word/phrase combinations and novel action-sequence compositions (including novel compositional rules and longer compositions)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>multiple: jump split (novel combination of seen primitive), around-right split (novel compositional rule), length split (longer sequences than training), MCD1/MCD2/MCD3 (Maximum Compound Divergence splits with strong train/test distribution shift).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised training augmented with CompSub (span substitution augmentation) and LCS (learned difficulty-aware augmenter) in alternating optimization</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>CompSub: large synthesized augmented set generated via span-substitution across training set; LCS: up-stream augmenter trained to prioritize generated examples that maximize downstream negative log-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Reported exact-match accuracies (mean ± std over 5 seeds) for LSTM Seq-to-Seq baseline and augmentation: Baseline Seq-to-Seq: jump 1.3% ± 0.4, around-right 10.2% ± 4.6, length 12.5% ± 2.5, MCD1 8.9% ± 1.6, MCD2 11.9% ± 9.4, MCD3 6.0% ± 0.9. +CompSub: jump 100.0% ± 0.0, around-right 99.9% ± 0.1, length 85.6% ± 8.8, MCD1 63.4% ± 13.1, MCD2 72.9% ± 10.1, MCD3 74.0% ± 10.2. +CompSub + LCS: jump 100.0% ± 0.0, around-right 100.0% ± 0.0, length 97.1% ± 3.2, MCD1 67.4% ± 12.1, MCD2 73.0% ± 10.1, MCD3 80.2% ± 1.8.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Detailed per-split results above; improvements are largest on hard structural splits (length, MCD) where LCS further improves over CompSub.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared with prior augmentation and architecture baselines (GECA, LexSym, Prim2PrimX+MET, Comp-IBT, etc.). Example: Seq-to-Seq +GECA and +LexSym give much smaller gains on hard splits (e.g., length and MCD), whereas +CompSub and +CompSub+LCS substantially outperform them on those splits (see numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Also evaluated with a Transformer base (see separate entry); relative pattern: CompSub and LCS improve both LSTM and Transformer, but absolute numbers differ by base architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Span-based augmentation (CompSub) dramatically increases SCAN compositional accuracy (e.g., jump/around-right to ~100% and large gains on length/MCD splits); LCS (learned selection) further improves hard structural cases (e.g., length accuracy from 85.6% to 97.1%, MCD3 from 74.0% to 80.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Even with CompSub, some hard splits (MCD family) remain challenging; random (non-learned) augmentation yields weaker gains than LCS on hardest categories.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Best results when multi-grained span substitutions are used (CompSub) plus difficulty-aware selection (LCS); benefits consistent across seeds and both when used with LSTM seq-to-seq base models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2026.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2026.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCAN-Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCAN compositional experiments with Transformer encoder-decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same SCAN experimental protocol but using a vanilla Transformer (3-layer in setup); evaluated with CompSub and LCS to probe architecture-agnostic benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard Transformer encoder-decoder architecture (three-layer variant used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>self-attention layers, positional encoding, standard Transformer blocks</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>procedural / sequential (navigation commands → action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SCAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See SCAN description in prior entry.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>varies; includes MCD splits and length split as above</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel action sequences and compositional rules</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>jump, around-right, length, MCD1-3</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised training with CompSub augmentation and LCS learned augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>CompSub-generated augmented set; LCS trained to select high-loss augmentations</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Transformer baseline: jump 3.5% ± 1.7, around-right 19.9% ± 10.4, length 12.2% ± 5.6, MCD1 1.7% ± 0.7, MCD2 4.3% ± 1.3, MCD3 4.4% ± 1.2. +CompSub: jump 92.4% ± 1.1, around-right 92.1% ± 1.5, length 93.5% ± 0.9, MCD1 24.8% ± 1.7, MCD2 79.4% ± 1.5, MCD3 61.3% ± 0.9. +CompSub + LCS: jump 92.2% ± 1.6, around-right 93.9% ± 1.5, length 95.4% ± 1.0, MCD1 27.0% ± 4.4, MCD2 80.2% ± 1.9, MCD3 63.3% ± 2.3.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Transformers benefit from CompSub on lexical and some structural splits; MCD1 remains difficult compared to LSTM case, but LCS improves several splits.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against GECA, LexSym, Prim2PrimX+MET and other augmentation baselines; CompSub (with LCS) outperforms these on most SCAN splits for Transformer base as well.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Direct comparison to LSTM seq-to-seq shows both architectures benefit from CompSub and LCS, though absolute performance and which splits are improved most vary by architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CompSub yields substantial improvements for Transformer models on SCAN; LCS further helps on several hard splits (notably length), demonstrating method is model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Some MCD splits remain more challenging for Transformers than for LSTM-based training in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Multi-grained span substitution and learned selection improve Transformer compositional generalization, especially for length-type splits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2026.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2026.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COGS-Seq2Seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COGS compositional generalization experiments with LSTM sequence-to-sequence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluations on the COGS semantic-interpretation benchmark (English sentences → logical forms) showing large gains from CompSub and modest additional gains from LCS; detailed per-method comparisons to prior SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM seq-to-seq (attention + copy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-layer encoder-decoder LSTM with attention and copy, trained on COGS with augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>LSTM encoder-decoder, attention, copy mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (semantic parsing / logical form generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>COGS</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic dataset mapping English sentences to logical forms (Lambda calculus / structured output). Designed to probe a wide array of compositional phenomena (active→passive, argument alternations, recursive structures) with 21 test categories.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>multiple phenomena including recursion and nested structures; finer-grained categories (21 test types) but not summarized as single numeric depth</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>lexical substitutions, subtree-level and general span-level recombinations; novel syntactic/semantic constructions</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>out-of-distribution compositional generalization split (COGS test typologies including novel constructions not seen in training)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised training augmented with CompSub (span-level substitution) and optional LCS learned augmentor</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>CompSub generates many augmented examples (paper: N up to 4e5 before deduplication for COGS); LCS prioritizes hard generated examples according to downstream loss.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Reported exact-match accuracies: Seq-to-Seq baseline 55.4% ± 4.2. Prior strong baselines: LexLearn 82.0%, LexSym 81.4%, Prim2PrimX+MET 81.1%, IR-Transformer 78.4%, T5-Base 85.9%, RoBERTa+Dangle 87.6%. +CompSub (this work): 91.8% ± 0.1. +CompSub + LCS: 92.3% ± 0.2.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Paper provides breakdown across four categories (lexical and structural types) showing CompSub improves both lexical-level and structural-level generalization; LCS gives additional gains specifically on harder structural types like 'cp recursion'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>COGS categories: lexical generalization (improved), structural generalization including obj-pp→sub-pp, pp recursion, cp recursion — CompSub improves across categories and LCS further helps recursion-type cases.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compares to many prior methods: MAML (64.1%), IR-Transformer (78.4%), Roberta+Dangle (87.6%), T5-Base (85.9%), LexSym/LexLearn/Prim2PrimX+MET (≈81%). CompSub improved over best prior methods by ≈ 4-10 percentage points (paper reports up to 10.3% improvement of LCS over prior SOTA on COGS).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>CompSub used with LSTM base outperforms methods that rely on specialized architectures or pretraining (in reported comparisons); experiments suggest augmentation is model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Span-based augmentation (CompSub) yields a large jump on COGS (from 55.4% baseline to 91.8%); learned selection (LCS) provides small additional improvement (to 92.3%). CompSub enables multi-grained recombinations that better cover the compositional test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Certain recursion and deeply structural phenomena remain the most challenging; LCS particularly helps examples requiring precise recombination of special structures (cp recursion).</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>High benefit when augmentation explores span-level (multi-grained) substitutions and when selection prioritizes rare/difficult recombinations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2026.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2026.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COGS-QL-LCS-ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning compositional experiments (COGS-QL) with LCS-ICL on LLaMA-2/3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adapting CompSub and LCS to few-shot in-context learning: recombining demonstrations and selecting the most difficult augmented demonstrations (by model perplexity) to improve LLM few-shot compositional generalization on COGS-QL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B and LLaMA3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained instruction-tuned LLMs used in few-shot (k-shot) in-context learning; evaluated in 6/12/18/24-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (LLaMA2-13B), 8B (LLaMA3-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>decoder-only transformer LLMs, pretrained then used via in-context learning (no parameter updates)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (semantic parsing / compositional generalization in few-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>COGS-QL (COGS variant with FunQL-style outputs suitable for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>COGS-QL reformulates COGS outputs to FunQL form to make few-shot evaluation feasible; test queries are recombinations of components seen in demonstration pool but not in training set (out-of-distribution compositional queries).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>varied; prompts contain recombined components producing OOD compositions; not summarized numerically</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel recombination of linguistic components into new queries requiring compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>few-shot in-context evaluation where demonstrations are drawn from a pool and recombined; test instances not present in pool but derivable from components</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>No fine-tuning; LCS-ICL augments and selects few-shot demonstrations by (1) coarse-screen to cover primitive concepts, (2) CompSub augmentation of selected demonstrations, (3) fine-screen selecting demonstrations with highest model perplexity (LCS-Score).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>ICL prompt composition is altered via CompSub-generated demonstrations (recombined spans); selection prioritized by model perplexity to expose hard compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Exact-match accuracies reported per shot: (selected entries) For LLaMA2-13B, best baseline PrimCoverage: 34.5% (6-shot), 39.5% (12), 43.0% (18), 44.5% (24). LCS-ICL (learned) for LLaMA2-13B: 37.0% (6), 42.0% (12), 44.5% (18), 47.5% (24). For LLaMA3-8B, best baseline PrimCoverage: (examples) up to 48.8% at 18-shot; LCS-ICL (learned) achieves 45.5% (6), 52.2% (12), 57.6% (18), 58.7% (24). The paper highlights a maximum observed gain of up to 8.8% absolute improvement over best baseline (LLaMA3-8B, 18-shot: PrimCoverage 48.8% vs LCS 57.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baselines include standard few-shot ICL, chain-of-thought prompting, PrimCoverage, BM25, CSR, BSR, GSR, and augmentation-based ICL (GECA, LexSym). LCS-ICL (learned) consistently outperforms most baselines, with stronger gains at larger shot counts (e.g., LLaMA3-8B: +8.8% vs best baseline at 18-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Experiments conducted on two LLM sizes (13B and 8B); LCS-ICL gains are more pronounced for the LLaMA3-8B setting in some shot configurations, indicating model and shot-size interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Performance generally increases with number of demonstrations (shots) when using LCS-ICL; LCS-ICL shows monotonic improvement with shots for tested LLMs while several baselines do not always show monotone gains.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recombining and selecting hard demonstrations (LCS-ICL) improves few-shot compositional generalization of LLMs; learned-selection version (perplexity-driven) outperforms random recombination and many retrieval baselines, with biggest gains at higher shot counts for LLaMA3-8B (e.g., 18-shot improvement of 8.8% over best baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>LCS-ICL underperforms some baselines at very small shot counts in some settings (e.g., LLaMA2-13B at 6/12 shots compared to GSR), likely due to limited initial search space for recombinations; success depends on adequate demonstration budget.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Works best when sufficient demonstration budget (≥18 shots in reported cases) allows LCS to find informative recombined examples; selection by perplexity yields benefits over random recombination.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2026.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2026.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LCS-framework-ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Component Substitution (LCS) framework and ablation comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable augmenter trained to maximize downstream model loss to prioritize hard augmentations; ablations compare Learned Augmentation vs Random Augmentation and show learned selection improves hardest splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LCS augmenter + downstream seq2seq models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Upstream neural augmenter parameterized to select spans to swap out and swap in using gumbel-softmax (differentiable); downstream seq-to-seq trained on generated examples. Training alternates: update downstream θ to minimize loss, update upstream ϕ to maximize downstream loss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>learnable span embeddings, sequence encoder (bi-LSTM used in augmentor), similarity scoring, gumbel-softmax sampling for differentiable discrete selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>applied across linguistic compositional benchmarks (SCAN, COGS, GeoQuery) as an augmentation policy</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CompSub + LCS experiments (various tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LCS learns probabilities for span-substitutions (CompSub actions) and prioritizes augmentations judged as difficult by downstream model (via loss feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>multi-grained span recombinations (words, subtrees, general substructures)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>applied to multiple splits (SCAN jump/around/length/MCD; COGS categories; GeoQuery query vs question splits)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>alternating optimization: downstream model updated to minimize NLL on generated examples; augmenter updated to maximize downstream loss (weighted expectation over sampled actions), with warm-up steps and multiple sampled actions per example.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Augmented datasets generated via CompSub; LCS further prioritizes rare and hard recombinations during training, increasing prevalence of minority but challenging compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Ablation table (Table IX) examples: Seq-to-Seq on MCD splits — +Random Augmentation (no CompSub) yields [46.6%, 52.3%, 58.8%] (MCD1,MCD2,MCD3), +Learned Augmentation yields [55.1%, 54.3%, 70.8%]. When combined with CompSub: +CompSub [63.4%, 72.9%, 74.0%]; +CompSub + Random Aug. [63.3%, 66.2%, 71.2%]; +CompSub + Learned Aug. [67.4%, 73.0%, 80.2%]. Similar consistent advantages of Learned Augmentation are shown for Transformer base.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>LCS particularly helps on hard examples containing elusive concepts and novel surroundings ('walk around right' style cases and recursion-type COGS examples).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Direct comparison between learned-selection and random-selection augmentation shows learned-selection consistently outperforms random selection, both with and without CompSub pre-generation.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning to prioritize hard augmentations (LCS) reduces downstream errors on difficult compositional cases and tightens generalization bounds theoretically (Rademacher complexity argument); empirically yields consistent gains over random augmentation (often sizable on hardest splits).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>LCS effectiveness depends on sufficient warm-up and downstream model capacity; small numbers of sampled actions or too-early optimization can limit benefits (noted for some SCAN-MCD2 initial experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Works when augmenter is given adequate capacity and training alternation schedule (warm-up epochs, T sampled actions) and when downstream model feedback (loss/perplexity) accurately highlights genuinely hard compositions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2026.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2026.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoQuery-Seq2Seq-BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoQuery compositional experiments with Seq-to-Seq and BART</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation on GeoQuery (natural-language questions → formal queries) in both IID question split and compositional query/template split, comparing CompSub to subtree and token-level augmentation baselines and to pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM seq-to-seq and BART-base (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>For LSTM: OpenNMT implementation with attention + copy; for pre-trained: BART-base fine-tuned on augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>mixed (BART is pretrained; LSTM is trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>LSTM seq2seq with attention/copy; BART is standard pretrained encoder-decoder transformer</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (semantic parsing to FunQL / SQL-like queries)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GeoQuery</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural question to database-query mapping; includes compositional template split where output templates in train and test are disjoint (tests compositional generalization), and an IID question split.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>template-level recombinations and lexical substitutions affecting query structure</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>question split (IID) and query/template split (compositional)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised training with CompSub augmentation (span substitution); comparison to subtree-substitution (SUBS) and lexical-symmetry (LexSym) methods.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>For GeoQuery training set is small (519 examples); CompSub searches potential augmentations exhaustively to expand training data.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Seq-to-Seq baseline (question/IID): 75.2% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Seq-to-Seq baseline (query/template compositional split): 58.6% (reported). Paper states CompSub leads to substantial and consistent improvement over baselines; prior strong augmentation/subtree methods (SUBS, LexSym) and pretrained methods reported higher numbers in some conditions (e.g., SUBS and LexSym reported strong numbers on some splits). Exact per-method numbers for all rows are presented in Table III of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Baseline Seq-to-Seq falls from 75.2% (IID) to 58.6% (compositional), indicating a ~16.6 percentage-point drop pre-augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against SpanParse, code-davinci-002 with coverage LS, GECA, LexSym, SUBS; CompSub reported competitive or superior results depending on split (paper claims 'substantial and consistent improvement' over other augmentation baselines and good compatibility with BART fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Both LSTM and BART bases evaluated; CompSub improves both though absolute numbers differ; BART-based experiments follow prior SUBS setup for comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CompSub can effectively expand small GeoQuery training sets via span-level substitutions and improves compositional-template generalization; exhaustive search of potential augmentations is feasible due to small dataset size. The paper also shows CompSub helps in low-data regimes (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>GeoQuery benefits from exhaustive CompSub due to small training set; pairing with pretrained BART yields robust performance on both IID and compositional splits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. <em>(Rating: 2)</em></li>
                <li>COGS: A compositional generalization challenge based on semantic interpretation. <em>(Rating: 2)</em></li>
                <li>LexSym: Compositionality as lexical symmetry. <em>(Rating: 2)</em></li>
                <li>Good-enough compositional data augmentation (GECA). <em>(Rating: 2)</em></li>
                <li>SUBS: Subtree substitution for compositional semantic parsing. <em>(Rating: 2)</em></li>
                <li>Prim2PrimX (Mutual exclusivity training and primitive augmentation to induce compositionality). <em>(Rating: 1)</em></li>
                <li>Comp-IBT (Revisiting iterative back-translation from the perspective of compositional generalization). <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2026",
    "paper_id": "paper-276724951",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "SCAN-Seq2Seq",
            "name_full": "SCAN compositional generalization experiments with LSTM sequence-to-sequence (attention+copy)",
            "brief_description": "Evaluation of span-based augmentation (CompSub) and learned selective augmentation (LCS) on SCAN splits using an LSTM encoder-decoder with attention and copy; reports exact-match accuracy on standard SCAN splits (jump, around-right, length, MCD1-3).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM seq-to-seq (attention + copy)",
            "model_description": "Encoder-decoder LSTM sequence-to-sequence model with attention and copy mechanism (configurations: 1- or 2-layer variants used; embeddings and hidden sizes per paper).",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard LSTM encoder-decoder; attention mechanism; copy mechanism",
            "task_domain": "procedural / sequential (navigation commands → action sequences)",
            "task_name": "SCAN",
            "task_description": "Synthetic mapping from natural-language navigation commands to action-sequences; tests recombination of known primitives into novel action sequences and structural rules (lexical and structural composition).",
            "compositional_depth": "varies by split; includes longer-sequence generalization (length split) and maximal compound divergence (MCD splits); depth not precisely quantified in paper",
            "composition_type": "novel word/phrase combinations and novel action-sequence compositions (including novel compositional rules and longer compositions)",
            "split_type": "multiple: jump split (novel combination of seen primitive), around-right split (novel compositional rule), length split (longer sequences than training), MCD1/MCD2/MCD3 (Maximum Compound Divergence splits with strong train/test distribution shift).",
            "training_strategy": "standard supervised training augmented with CompSub (span substitution augmentation) and LCS (learned difficulty-aware augmenter) in alternating optimization",
            "curriculum_details": null,
            "inoculation_details": "CompSub: large synthesized augmented set generated via span-substitution across training set; LCS: up-stream augmenter trained to prioritize generated examples that maximize downstream negative log-likelihood.",
            "iid_performance": null,
            "compositional_performance": "Reported exact-match accuracies (mean ± std over 5 seeds) for LSTM Seq-to-Seq baseline and augmentation: Baseline Seq-to-Seq: jump 1.3% ± 0.4, around-right 10.2% ± 4.6, length 12.5% ± 2.5, MCD1 8.9% ± 1.6, MCD2 11.9% ± 9.4, MCD3 6.0% ± 0.9. +CompSub: jump 100.0% ± 0.0, around-right 99.9% ± 0.1, length 85.6% ± 8.8, MCD1 63.4% ± 13.1, MCD2 72.9% ± 10.1, MCD3 74.0% ± 10.2. +CompSub + LCS: jump 100.0% ± 0.0, around-right 100.0% ± 0.0, length 97.1% ± 3.2, MCD1 67.4% ± 12.1, MCD2 73.0% ± 10.1, MCD3 80.2% ± 1.8.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": "Detailed per-split results above; improvements are largest on hard structural splits (length, MCD) where LCS further improves over CompSub.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared with prior augmentation and architecture baselines (GECA, LexSym, Prim2PrimX+MET, Comp-IBT, etc.). Example: Seq-to-Seq +GECA and +LexSym give much smaller gains on hard splits (e.g., length and MCD), whereas +CompSub and +CompSub+LCS substantially outperform them on those splits (see numbers above).",
            "architectural_comparison": "Also evaluated with a Transformer base (see separate entry); relative pattern: CompSub and LCS improve both LSTM and Transformer, but absolute numbers differ by base architecture.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Span-based augmentation (CompSub) dramatically increases SCAN compositional accuracy (e.g., jump/around-right to ~100% and large gains on length/MCD splits); LCS (learned selection) further improves hard structural cases (e.g., length accuracy from 85.6% to 97.1%, MCD3 from 74.0% to 80.2%).",
            "failure_analysis": "Even with CompSub, some hard splits (MCD family) remain challenging; random (non-learned) augmentation yields weaker gains than LCS on hardest categories.",
            "success_conditions": "Best results when multi-grained span substitutions are used (CompSub) plus difficulty-aware selection (LCS); benefits consistent across seeds and both when used with LSTM seq-to-seq base models.",
            "uuid": "e2026.0"
        },
        {
            "name_short": "SCAN-Transformer",
            "name_full": "SCAN compositional experiments with Transformer encoder-decoder",
            "brief_description": "Same SCAN experimental protocol but using a vanilla Transformer (3-layer in setup); evaluated with CompSub and LCS to probe architecture-agnostic benefits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (vanilla)",
            "model_description": "Standard Transformer encoder-decoder architecture (three-layer variant used in experiments).",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "self-attention layers, positional encoding, standard Transformer blocks",
            "task_domain": "procedural / sequential (navigation commands → action sequences)",
            "task_name": "SCAN",
            "task_description": "See SCAN description in prior entry.",
            "compositional_depth": "varies; includes MCD splits and length split as above",
            "composition_type": "novel action sequences and compositional rules",
            "split_type": "jump, around-right, length, MCD1-3",
            "training_strategy": "supervised training with CompSub augmentation and LCS learned augmentation",
            "curriculum_details": null,
            "inoculation_details": "CompSub-generated augmented set; LCS trained to select high-loss augmentations",
            "iid_performance": null,
            "compositional_performance": "Transformer baseline: jump 3.5% ± 1.7, around-right 19.9% ± 10.4, length 12.2% ± 5.6, MCD1 1.7% ± 0.7, MCD2 4.3% ± 1.3, MCD3 4.4% ± 1.2. +CompSub: jump 92.4% ± 1.1, around-right 92.1% ± 1.5, length 93.5% ± 0.9, MCD1 24.8% ± 1.7, MCD2 79.4% ± 1.5, MCD3 61.3% ± 0.9. +CompSub + LCS: jump 92.2% ± 1.6, around-right 93.9% ± 1.5, length 95.4% ± 1.0, MCD1 27.0% ± 4.4, MCD2 80.2% ± 1.9, MCD3 63.3% ± 2.3.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": "Transformers benefit from CompSub on lexical and some structural splits; MCD1 remains difficult compared to LSTM case, but LCS improves several splits.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against GECA, LexSym, Prim2PrimX+MET and other augmentation baselines; CompSub (with LCS) outperforms these on most SCAN splits for Transformer base as well.",
            "architectural_comparison": "Direct comparison to LSTM seq-to-seq shows both architectures benefit from CompSub and LCS, though absolute performance and which splits are improved most vary by architecture.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "CompSub yields substantial improvements for Transformer models on SCAN; LCS further helps on several hard splits (notably length), demonstrating method is model-agnostic.",
            "failure_analysis": "Some MCD splits remain more challenging for Transformers than for LSTM-based training in this setup.",
            "success_conditions": "Multi-grained span substitution and learned selection improve Transformer compositional generalization, especially for length-type splits.",
            "uuid": "e2026.1"
        },
        {
            "name_short": "COGS-Seq2Seq",
            "name_full": "COGS compositional generalization experiments with LSTM sequence-to-sequence",
            "brief_description": "Evaluations on the COGS semantic-interpretation benchmark (English sentences → logical forms) showing large gains from CompSub and modest additional gains from LCS; detailed per-method comparisons to prior SOTA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM seq-to-seq (attention + copy)",
            "model_description": "Two-layer encoder-decoder LSTM with attention and copy, trained on COGS with augmentation.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "LSTM encoder-decoder, attention, copy mechanism",
            "task_domain": "linguistic/semantic (semantic parsing / logical form generation)",
            "task_name": "COGS",
            "task_description": "Synthetic dataset mapping English sentences to logical forms (Lambda calculus / structured output). Designed to probe a wide array of compositional phenomena (active→passive, argument alternations, recursive structures) with 21 test categories.",
            "compositional_depth": "multiple phenomena including recursion and nested structures; finer-grained categories (21 test types) but not summarized as single numeric depth",
            "composition_type": "lexical substitutions, subtree-level and general span-level recombinations; novel syntactic/semantic constructions",
            "split_type": "out-of-distribution compositional generalization split (COGS test typologies including novel constructions not seen in training)",
            "training_strategy": "supervised training augmented with CompSub (span-level substitution) and optional LCS learned augmentor",
            "curriculum_details": null,
            "inoculation_details": "CompSub generates many augmented examples (paper: N up to 4e5 before deduplication for COGS); LCS prioritizes hard generated examples according to downstream loss.",
            "iid_performance": null,
            "compositional_performance": "Reported exact-match accuracies: Seq-to-Seq baseline 55.4% ± 4.2. Prior strong baselines: LexLearn 82.0%, LexSym 81.4%, Prim2PrimX+MET 81.1%, IR-Transformer 78.4%, T5-Base 85.9%, RoBERTa+Dangle 87.6%. +CompSub (this work): 91.8% ± 0.1. +CompSub + LCS: 92.3% ± 0.2.",
            "generalization_gap": null,
            "performance_by_depth": "Paper provides breakdown across four categories (lexical and structural types) showing CompSub improves both lexical-level and structural-level generalization; LCS gives additional gains specifically on harder structural types like 'cp recursion'.",
            "performance_by_composition_type": "COGS categories: lexical generalization (improved), structural generalization including obj-pp→sub-pp, pp recursion, cp recursion — CompSub improves across categories and LCS further helps recursion-type cases.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compares to many prior methods: MAML (64.1%), IR-Transformer (78.4%), Roberta+Dangle (87.6%), T5-Base (85.9%), LexSym/LexLearn/Prim2PrimX+MET (≈81%). CompSub improved over best prior methods by ≈ 4-10 percentage points (paper reports up to 10.3% improvement of LCS over prior SOTA on COGS).",
            "architectural_comparison": "CompSub used with LSTM base outperforms methods that rely on specialized architectures or pretraining (in reported comparisons); experiments suggest augmentation is model-agnostic.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Span-based augmentation (CompSub) yields a large jump on COGS (from 55.4% baseline to 91.8%); learned selection (LCS) provides small additional improvement (to 92.3%). CompSub enables multi-grained recombinations that better cover the compositional test distribution.",
            "failure_analysis": "Certain recursion and deeply structural phenomena remain the most challenging; LCS particularly helps examples requiring precise recombination of special structures (cp recursion).",
            "success_conditions": "High benefit when augmentation explores span-level (multi-grained) substitutions and when selection prioritizes rare/difficult recombinations.",
            "uuid": "e2026.2"
        },
        {
            "name_short": "COGS-QL-LCS-ICL",
            "name_full": "In-Context Learning compositional experiments (COGS-QL) with LCS-ICL on LLaMA-2/3",
            "brief_description": "Adapting CompSub and LCS to few-shot in-context learning: recombining demonstrations and selecting the most difficult augmented demonstrations (by model perplexity) to improve LLM few-shot compositional generalization on COGS-QL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B and LLaMA3-8B",
            "model_description": "Large pre-trained instruction-tuned LLMs used in few-shot (k-shot) in-context learning; evaluated in 6/12/18/24-shot settings.",
            "model_size": "13B (LLaMA2-13B), 8B (LLaMA3-8B)",
            "is_pretrained": true,
            "architectural_features": "decoder-only transformer LLMs, pretrained then used via in-context learning (no parameter updates)",
            "task_domain": "linguistic/semantic (semantic parsing / compositional generalization in few-shot prompts)",
            "task_name": "COGS-QL (COGS variant with FunQL-style outputs suitable for LLMs)",
            "task_description": "COGS-QL reformulates COGS outputs to FunQL form to make few-shot evaluation feasible; test queries are recombinations of components seen in demonstration pool but not in training set (out-of-distribution compositional queries).",
            "compositional_depth": "varied; prompts contain recombined components producing OOD compositions; not summarized numerically",
            "composition_type": "novel recombination of linguistic components into new queries requiring compositional generalization",
            "split_type": "few-shot in-context evaluation where demonstrations are drawn from a pool and recombined; test instances not present in pool but derivable from components",
            "training_strategy": "No fine-tuning; LCS-ICL augments and selects few-shot demonstrations by (1) coarse-screen to cover primitive concepts, (2) CompSub augmentation of selected demonstrations, (3) fine-screen selecting demonstrations with highest model perplexity (LCS-Score).",
            "curriculum_details": null,
            "inoculation_details": "ICL prompt composition is altered via CompSub-generated demonstrations (recombined spans); selection prioritized by model perplexity to expose hard compositions.",
            "iid_performance": null,
            "compositional_performance": "Exact-match accuracies reported per shot: (selected entries) For LLaMA2-13B, best baseline PrimCoverage: 34.5% (6-shot), 39.5% (12), 43.0% (18), 44.5% (24). LCS-ICL (learned) for LLaMA2-13B: 37.0% (6), 42.0% (12), 44.5% (18), 47.5% (24). For LLaMA3-8B, best baseline PrimCoverage: (examples) up to 48.8% at 18-shot; LCS-ICL (learned) achieves 45.5% (6), 52.2% (12), 57.6% (18), 58.7% (24). The paper highlights a maximum observed gain of up to 8.8% absolute improvement over best baseline (LLaMA3-8B, 18-shot: PrimCoverage 48.8% vs LCS 57.6%).",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baselines include standard few-shot ICL, chain-of-thought prompting, PrimCoverage, BM25, CSR, BSR, GSR, and augmentation-based ICL (GECA, LexSym). LCS-ICL (learned) consistently outperforms most baselines, with stronger gains at larger shot counts (e.g., LLaMA3-8B: +8.8% vs best baseline at 18-shot).",
            "architectural_comparison": "Experiments conducted on two LLM sizes (13B and 8B); LCS-ICL gains are more pronounced for the LLaMA3-8B setting in some shot configurations, indicating model and shot-size interactions.",
            "scale_effects": "Performance generally increases with number of demonstrations (shots) when using LCS-ICL; LCS-ICL shows monotonic improvement with shots for tested LLMs while several baselines do not always show monotone gains.",
            "transfer_results": null,
            "key_findings": "Recombining and selecting hard demonstrations (LCS-ICL) improves few-shot compositional generalization of LLMs; learned-selection version (perplexity-driven) outperforms random recombination and many retrieval baselines, with biggest gains at higher shot counts for LLaMA3-8B (e.g., 18-shot improvement of 8.8% over best baseline).",
            "failure_analysis": "LCS-ICL underperforms some baselines at very small shot counts in some settings (e.g., LLaMA2-13B at 6/12 shots compared to GSR), likely due to limited initial search space for recombinations; success depends on adequate demonstration budget.",
            "success_conditions": "Works best when sufficient demonstration budget (≥18 shots in reported cases) allows LCS to find informative recombined examples; selection by perplexity yields benefits over random recombination.",
            "uuid": "e2026.3"
        },
        {
            "name_short": "LCS-framework-ablation",
            "name_full": "Learning Component Substitution (LCS) framework and ablation comparisons",
            "brief_description": "A differentiable augmenter trained to maximize downstream model loss to prioritize hard augmentations; ablations compare Learned Augmentation vs Random Augmentation and show learned selection improves hardest splits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LCS augmenter + downstream seq2seq models",
            "model_description": "Upstream neural augmenter parameterized to select spans to swap out and swap in using gumbel-softmax (differentiable); downstream seq-to-seq trained on generated examples. Training alternates: update downstream θ to minimize loss, update upstream ϕ to maximize downstream loss.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "learnable span embeddings, sequence encoder (bi-LSTM used in augmentor), similarity scoring, gumbel-softmax sampling for differentiable discrete selection",
            "task_domain": "applied across linguistic compositional benchmarks (SCAN, COGS, GeoQuery) as an augmentation policy",
            "task_name": "CompSub + LCS experiments (various tasks)",
            "task_description": "LCS learns probabilities for span-substitutions (CompSub actions) and prioritizes augmentations judged as difficult by downstream model (via loss feedback).",
            "compositional_depth": null,
            "composition_type": "multi-grained span recombinations (words, subtrees, general substructures)",
            "split_type": "applied to multiple splits (SCAN jump/around/length/MCD; COGS categories; GeoQuery query vs question splits)",
            "training_strategy": "alternating optimization: downstream model updated to minimize NLL on generated examples; augmenter updated to maximize downstream loss (weighted expectation over sampled actions), with warm-up steps and multiple sampled actions per example.",
            "curriculum_details": null,
            "inoculation_details": "Augmented datasets generated via CompSub; LCS further prioritizes rare and hard recombinations during training, increasing prevalence of minority but challenging compositions.",
            "iid_performance": null,
            "compositional_performance": "Ablation table (Table IX) examples: Seq-to-Seq on MCD splits — +Random Augmentation (no CompSub) yields [46.6%, 52.3%, 58.8%] (MCD1,MCD2,MCD3), +Learned Augmentation yields [55.1%, 54.3%, 70.8%]. When combined with CompSub: +CompSub [63.4%, 72.9%, 74.0%]; +CompSub + Random Aug. [63.3%, 66.2%, 71.2%]; +CompSub + Learned Aug. [67.4%, 73.0%, 80.2%]. Similar consistent advantages of Learned Augmentation are shown for Transformer base.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": "LCS particularly helps on hard examples containing elusive concepts and novel surroundings ('walk around right' style cases and recursion-type COGS examples).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Direct comparison between learned-selection and random-selection augmentation shows learned-selection consistently outperforms random selection, both with and without CompSub pre-generation.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Learning to prioritize hard augmentations (LCS) reduces downstream errors on difficult compositional cases and tightens generalization bounds theoretically (Rademacher complexity argument); empirically yields consistent gains over random augmentation (often sizable on hardest splits).",
            "failure_analysis": "LCS effectiveness depends on sufficient warm-up and downstream model capacity; small numbers of sampled actions or too-early optimization can limit benefits (noted for some SCAN-MCD2 initial experiments).",
            "success_conditions": "Works when augmenter is given adequate capacity and training alternation schedule (warm-up epochs, T sampled actions) and when downstream model feedback (loss/perplexity) accurately highlights genuinely hard compositions.",
            "uuid": "e2026.4"
        },
        {
            "name_short": "GeoQuery-Seq2Seq-BART",
            "name_full": "GeoQuery compositional experiments with Seq-to-Seq and BART",
            "brief_description": "Evaluation on GeoQuery (natural-language questions → formal queries) in both IID question split and compositional query/template split, comparing CompSub to subtree and token-level augmentation baselines and to pretrained LMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM seq-to-seq and BART-base (fine-tuned)",
            "model_description": "For LSTM: OpenNMT implementation with attention + copy; for pre-trained: BART-base fine-tuned on augmented data.",
            "model_size": null,
            "is_pretrained": "mixed (BART is pretrained; LSTM is trained from scratch)",
            "architectural_features": "LSTM seq2seq with attention/copy; BART is standard pretrained encoder-decoder transformer",
            "task_domain": "linguistic/semantic (semantic parsing to FunQL / SQL-like queries)",
            "task_name": "GeoQuery",
            "task_description": "Natural question to database-query mapping; includes compositional template split where output templates in train and test are disjoint (tests compositional generalization), and an IID question split.",
            "compositional_depth": null,
            "composition_type": "template-level recombinations and lexical substitutions affecting query structure",
            "split_type": "question split (IID) and query/template split (compositional)",
            "training_strategy": "supervised training with CompSub augmentation (span substitution); comparison to subtree-substitution (SUBS) and lexical-symmetry (LexSym) methods.",
            "curriculum_details": null,
            "inoculation_details": "For GeoQuery training set is small (519 examples); CompSub searches potential augmentations exhaustively to expand training data.",
            "iid_performance": "Seq-to-Seq baseline (question/IID): 75.2% (reported).",
            "compositional_performance": "Seq-to-Seq baseline (query/template compositional split): 58.6% (reported). Paper states CompSub leads to substantial and consistent improvement over baselines; prior strong augmentation/subtree methods (SUBS, LexSym) and pretrained methods reported higher numbers in some conditions (e.g., SUBS and LexSym reported strong numbers on some splits). Exact per-method numbers for all rows are presented in Table III of the paper.",
            "generalization_gap": "Baseline Seq-to-Seq falls from 75.2% (IID) to 58.6% (compositional), indicating a ~16.6 percentage-point drop pre-augmentation.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against SpanParse, code-davinci-002 with coverage LS, GECA, LexSym, SUBS; CompSub reported competitive or superior results depending on split (paper claims 'substantial and consistent improvement' over other augmentation baselines and good compatibility with BART fine-tuning).",
            "architectural_comparison": "Both LSTM and BART bases evaluated; CompSub improves both though absolute numbers differ; BART-based experiments follow prior SUBS setup for comparability.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "CompSub can effectively expand small GeoQuery training sets via span-level substitutions and improves compositional-template generalization; exhaustive search of potential augmentations is feasible due to small dataset size. The paper also shows CompSub helps in low-data regimes (Figure 6).",
            "failure_analysis": null,
            "success_conditions": "GeoQuery benefits from exhaustive CompSub due to small training set; pairing with pretrained BART yields robust performance on both IID and compositional splits.",
            "uuid": "e2026.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks.",
            "rating": 2
        },
        {
            "paper_title": "COGS: A compositional generalization challenge based on semantic interpretation.",
            "rating": 2
        },
        {
            "paper_title": "LexSym: Compositionality as lexical symmetry.",
            "rating": 2
        },
        {
            "paper_title": "Good-enough compositional data augmentation (GECA).",
            "rating": 2
        },
        {
            "paper_title": "SUBS: Subtree substitution for compositional semantic parsing.",
            "rating": 2
        },
        {
            "paper_title": "Prim2PrimX (Mutual exclusivity training and primitive augmentation to induce compositionality).",
            "rating": 1
        },
        {
            "paper_title": "Comp-IBT (Revisiting iterative back-translation from the perspective of compositional generalization).",
            "rating": 1
        }
    ],
    "cost": 0.0261285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Substitute Components for Compositional Generalization</p>
<p>Zhaoyi Li 
Gangwei Jiang 
Chenwang Wu 
Ying Wei 
Defu Lian 
Fellow, IEEEEnhong Chen 
Learning to Substitute Components for Compositional Generalization
833F5C223EF061D77A534532E503B8F0Compositional GeneralizationNatural Language ProcessingLanguage ModelsData Augmentation
Despite the rising prevalence of neural language models, recent empirical evidence suggests their deficiency in compositional generalization.One of the current de-facto solutions to this problem is compositional data augmentation, which aims to introduce additional compositional inductive bias.However, existing handcrafted augmentation strategies offer limited improvement when systematic generalization of neural language models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases alone) or when training sentences have an imbalanced difficulty distribution.To address these challenges, we first propose a novel compositional augmentation strategy called Component Substitution (Comp-Sub), which enables multi-grained composition of substantial substructures across the entire training set.Furthermore, we introduce the Learning Component Substitution (LCS) framework.This framework empowers the learning of component substitution probabilities in CompSub in an end-to-end manner by maximizing the loss of neural language models, thereby prioritizing challenging compositions with elusive concepts and novel contexts.We extend the key ideas of CompSub and LCS to the recently emerging in-context learning scenarios of pretrained large language models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot compositional generalization of state-of-the-art (SOTA) LLMs.Theoretically, we provide insights into why applying our algorithms to language models can improve compositional generalization performance.Empirically, our results on four standard compositional generalization benchmarks-SCAN, COGS, GeoQuery, and COGS-QL-demonstrate the superiority of CompSub, LCS, and LCS-ICL, with improvements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively.</p>
<p>I. INTRODUCTION</p>
<p>The secret for human beings to learning so quickly with little supervision has been demonstrated to be associated with the powerful ability of compositional generalization [3]- [6], being capable of producing an infinite number of novel combinations on the basis of known components [4], [7].</p>
<p>In stark contrast, a large body of recent evidence suggests that current state-of-the-art neural language models (in-Corresponding authors: Ying Wei and Defu Lian.Zhaoyi Li, Gangwei Jiang, Defu Lian and Enhong Chen are affiliated with the School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230026, P.R.China (e-mail: lizhaoyi777@mail.ustc.edu.cn,gwjiang@mail.ustc.edu.cn,liandefu@ustc.edu.cn,cheneh@ustc.edu.cn).</p>
<p>Chenwang Wu is affiliated with the Department of Computer Science, Hong Kong Baptist University, Kowloon, Kowloon Tong 999077, Hong Kong (email:wcw1996@mail.ustc.edu.cn).</p>
<p>Ying Wei is affiliated with the College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang 310058, P.R.China (email:ying.wei@zju.edu.cn).[1], which require word-level, subtree-level and general substructure-level recombinations of training data, respectively.Besides, (d) shows concepts in distinct difficulty in the SCAN [2] dataset, where the interpretation of walk around right is much more complex than that of the other two concepts.</p>
<p>cluding LSTM-based Sequence-to-Sequence models (Seq-to-Seq) [8]- [10], Transformers [11], pre-trained language models (PLMs) [12], [13] and large language models (LLMs) [14]- [16]) lack of adequate power for compositional generalization (a.k.a., systematic generalization) [1], [2], [17]- [19].For instance, a model which has observed the two training sentences of "look opposite right twice and jump right thrice" and "walk around right and run twice" likely fails to understand the testing sentence of "walk around right twice and jump right thrice".Sharpening the compositional generalization ability of neural language models is beyond important to fill the gap with human-like natural language understanding, catalyzing not only better performances but also fewer expensive annotations.</p>
<p>Inspired by the tight relationship between compositionality and group-equivariance of neural models [20]- [22], a series of compositional data augmentation solutions have made great arXiv:2502.20834v1[cs.CL] 28 Feb 2025 strides via injecting compositional inductive bias into neural language models [21], [23]- [26].The key idea behind compositional data augmentation is to substitute a part in one original training example with a part from another training example, thus composing a novel example that complements the training data with compositional bias.</p>
<p>Introducing comprehensive enough compositional bias to embrace a diversity of testing tasks, however, is not trivial.First, the "part"1 to be substituted out and in is expected to be in multiple levels, ranging from words [21] in Figure 1(a), to complete substrees [25] in Figure 1(b), to more general substructures in Figure 1(c).How to develop an augmentation method that flexibly accommodates multiple levels of parts remains an open question.Second, the "parts" are uneven in their difficulty levels.As shown in Figure 1(d), though the numbers of both training and testing sentences containing the three concepts in the SCAN MCD split [17] are comparable and we have applied compositional data augmentation via the proposed CompSub (which will be detailed later in Section III-A), the predicted error rates of testing sentences grouped by the three concepts still differ quite significantly, which is in alignment with the observations in [27].There is an urgent need to augment with difficulty awareness and allow more compositions on the challenging concepts (e.g., concept 3 in Figure 1(d), "walk around right", is supposed to be mapped to a more complex structure in comparison with the other two concepts "walk right" and "walk opposite right").</p>
<p>In the previous conference version of our paper [28], we propose two methods to address the two aforementioned challenges.We first introduce a novel compositional data augmentation scheme, dubbed Component Substitution (CompSub).This method substitutes a component in a training sentence with one in another sentence.Here a component refers to a consecutive fragment of tokens that encompasses all multigrained possibilities of a word, a subtree, as well as a more general substructure (e.g., "a cat on the mat" in Figure 1(c)).The core of CompSub lies in the extraction of such components and the identification of exchangeable components.We define the exchangeability of components based on the syntactic equivalence of their first and last tokens.However, purely random component substitution in CompSub overlooks the imbalanced difficulty distribution of compositions and hence may lead to suboptimal results.To address this challenge, we propose the Learning Component Substitution (LCS) framework.This framework includes a LCS augmenter, which is a differentiable version of CompSub with all substitution actions equipped with probabilities.By training downstream neural language models to evaluate the difficulty of various components (i.e., the concepts in Figure 1(d)) and maximizing their losses, the LCS framework seeks to train the LCS augmenter to prioritize substitution actions that introduce challenging compositions involving elusive components and novel contexts.</p>
<p>In this paper, we additionally provide significant extensions based on the conference paper [28].Specifically: (1) We offer theoretical insights and analysis for our proposed CompSub and LCS algorithms (Section IV).We prove that using our compositional data augmentation method, CompSub, is equivalent to imposing an additional regularization term on the original optimization objective.This highly encourages language models to achieve invariant learning [29], [30] of component understanding in novel compositional context surroundings and mitigates the learning of spurious correlations in the training data [31].In addition, we show that using LCS to select challenging compositions of elusive components and novel contexts when augmenting data can decrease the Rademacher complexity [32] term in the generalization bound, which provides an explanation for why LCS achieves better compositional generalization performance compared to purely random augmentation to some extent.(2) We extend the key ideas of our algorithms to the recently emerging and powerful LLMs' In-Context Learning (ICL) scenarios [14], [33] (Section III-C).Unlike traditional learning paradigms, ICL only prepends a few exemplars (i.e., input-output pairs from an off-the-shelf demonstration pool, so-called "demonstrations") to the beginning of the input query and leverages these demonstrations to activate LLMs' ability to solve the query without updating LLMs' parameters.However, recent works [19], [34]- [36] have shown the deficiency of the ICL in compositional generalization.To address this, we propose the LCS-ICL algorithm, which introduces additional compositional inductive bias by recombining existing demonstrations.This method enhances the exposure of the composition of elusive concepts and novel context surroundings by selecting the most informative recombined demonstrations according to the feedback from the LLMs.(3) We enrich the experimental section with additional analysis experiments (Section V-D).For example, we present the experiment results on another challenging length split of SCAN dataset and additional experiments using the Transformer [11] as the base model across all splits of the SCAN dataset in Table I.We also include a new Figure (Figure 6) to demonstrate the superiority of our CompSub data augmentation method in the low-data regimes of the GeoQuery dataset compared to other state-ofthe-art data augmentation methods.Furthermore, we provide a comprehensive evaluation of our LCS-ICL algorithm using LLaMA2-13B [15] and LLaMA3-8B [16] in Table IV, highlighting the effectiveness of our approach.</p>
<p>In summary, the main contributions of this paper are threefold:</p>
<p>• We introduce a new compositional data augmentation method, CompSub, which is the first to explore spanbased compositional data augmentation, thus flexibly supporting to inject multi-grained compositional bias in to the training set.We theoretically analyze its effectiveness by proving that using CompSub is equivalent to imposing an implicit regularization term of learning the semantic invariance of individual components in different contexts on the optimization objective.Empirically we demonstrate its superiority on three benchmarks (SCAN, COGS and GeoQuery), with the improvement of at most 60.3%, 9.8% and 1.2% over previous state-of-the-art (SOTA) methods, respectively.• Based on CompSub, we introduce LCS as a differentiable data augmentation framework that first empowers difficulty-aware composition, being compatible with various down-stream language models.We theorectically analyze its effectiveness by proving that using LCS can decrease the Rademacher complexity term in the compositional generalization bound.We empirically demonstrate its superiority on three benchmarks (SCAN, COGS and GeoQuery), with the improvement of at most 66.5%, 10.3% and 1.4% over previous SOTA methods, respectively.</p>
<p>II. RELATED WORK A. Compositional Generalization in Neural Language Modeling</p>
<p>A large body of literature pursues various ways of introducing compositional inductive bias into neural language models, in a bid to improve compositional generalization.The first category of studies, e.g., CGPS [37], SyntAtt [38], GroupEqu [20], MultiGroupEqu [39] customizes neural architectures that promote lexical generalization via explicit disentanglement of the meaning of tokens.The second strand aims to align words or substructures in the input sequences with their counterparts in the output sequences by auxiliary tasks (e.g., IR-Transformer [40]), additional architectural modules (e.g., LexLearn [41], Composed Layer Module [42] and LRF [43]), as well as extra objectives imposed on attention layers (e.g., SpanAtt [44]).Third, the works of Meta-seq2seq [45], Comp-MAML [46], and MET [26] resorts to the meta-learning paradigm to directly encourage compositional generalization of neural models.Last but not least, compositional data augmentation that composes in-distribution data to accommodate out-of-distribution compositional sequences has been empirically demonstrated to enjoy not only the performance but also the model-agnostic benefits.The explored principles for augmentation include exchangeability of tokens in the same context (e.g., GECA [23]), tokenlevel mixup [47] (e.g., SeqMix [24]), group-equivariance of language models [22], [39] by substituting training tokens (e.g., LexSym [21], Prim2PrimX [26], ARCHER [48]) or subtrees (e.g., SUBS [25]) with virtual or off-the-shelf tokens or subtrees.Note that the aforementioned approaches guarantee the validity of composed sequences by following the widely accepted alignment practices in NLP, e.g., SpanTree [49] and FastAlign [50].Our work further pushes ahead with compositional data augmentation by (1) substituting spans, which offers more diverse and flexible generalization than substituting monotonous tokens or subtrees, and (2) endowing the augmentation strategy to be differentiable and learnable in an end-to-end manner, which dynamically adapts to the difficulty of down-stream neural sequence tasks.</p>
<p>B. Compositional Generalization in In-Context Learning of LLMs</p>
<p>Recent works [19], [34]- [36], [51]- [53] also study the compositional generalization performance of LLMs in ICL scenarios.Early works by Qiu et al. [51] and Hosseini et al. [52] comprehensively investigate the impact of the scale of LLMs and the number of demonstrations (i.e., shot number) in ICL prompts on their compositional generalization performance, revealing a significant compositional generalization gap [52] in the state-of-the-art LLMs.To address this gap, the first category of works [34], [53] design "Chain-of-Thought"like ICL prompts to decompose complex queries into simpler sub-problems and then solve them step-by-step, thereby enhancing compositional generalization.However, these methods require experts to carefully design the prompting templates and may suffer from poor generalization from the in-context demonstrations to unseen queries [54].The second category of works [19], [35], [36] focus on retrieving appropriate exemplars from a pool of candidates as the ICL demonstrations.PrimCoverage [19] and BM25 [55] aim to retrieve demonstrations that are similar to the query (covering parts of the query in the retrieved exemplars).In contrast, CSR and BSR [35] emphasize the importance of selecting exemplars that, while less similar, contain more missing information in the query (i.e., more "informative") as ICL demonstrations.GSR [36] trains an encoder for exemplar retrieval, which induces an attention masking bottleneck between exemplar inputs and outputs [56].By training with this bottleneck comprising a few gist tokens, the model is forced to store taskspecific salient input information in the activations of those tokens, which GSR uses to retrieve the most informative ICL demonstrations.Our work also falls into the second category of works.Different from existing works, we leverage our compositional data augmentation technique to inject additional compositional inductive bias into the ICL demonstrations and select demonstrations that are most helpful for compositional generalization based on the perplexity of LLMs themselves.</p>
<p>III. METHODOLOGY</p>
<p>In this section, we present the details of our proposed algorithms.We first propose a novel compositional augmentation strategy, CompSub, which enables multi-grained composition of substantial substructures in the whole training set (Section III-A).Over and above that, we introduce the LCS framework which empowers the learning of span substitution probabilities in CompSub in an end-to-end manner by maximizing the loss of neural language models, so as to outweigh those challenging compositions with elusive concepts and novel context surroundings (Section III-B).Furthermore, we extend the key ideas of CompSub and LCS to the recently emerging in-context learning scenarios of pre-trained LLMs, proposing the LCS-ICL algorithm to enhance the few-shot compositional generalization of SOTA LLMs.(Section III-C).</p>
<p>A. Component Substitution</p>
<p>We propose CompSub to generate novel examples through exchanging multi-grained spans, which refer to consecutive Preprocessing The techniques of extracting span alignment from paired linguistic data and identifying syntactically equivalent words (e.g., Part-of-Speech tagging) have been well studied in the NLP community.Following the practice in a wealth of literature on compositional augmentation [21], [25], [26], we also directly adapt the off-the-shelf techniques, which we introduce as below for self-contained purpose, to preprocess rather than delving into them.More details about the CompSub Algorithm (including the results of preprocessing for all the datasets) are available in the Supplementary Materials (Section I).Extraction of span alignment Span alignment refers to establish the correspondence between spans in the input sequence (e.g., "largest city in the smallest") and their counterparts (e.g., "largest(city(loc 2(smallest())))") in the output sequence of a training example.For the SCAN dataset, we extract span alignment by extending SimpleAlign [41] that targets single words (e.g., jump → JUMP right → TURN RIGHT) to support alignment of consecutive fragments (e.g., jump right → TURN RIGHT JUMP).As there always exists a deterministic function program [25], [40] that transforms the output sequence y to a tree for COGS and GeoQuery, we resort to the intermediate representation [57] of COGS from [40] and the span tree of GeoQuery from [49] to map the input sequence x to the tree form T , respectively.The tree T , in such a way, serves as a bridge to align the input and output.Inference of the equivalence class of a word The aim is to infer the equivalence class of a word w, i.e., π(w), according to the cluster it belongs to.Exemplar clusters include verbs and nouns.Fortunately, the COGS dataset has intrinsic clusters of words by their tree structure representations.As for SCAN and GeoQuery, we follow [21], [26] to assign those words sharing the context into a single cluster.For example, the words of "largest" and "smallest" fall into the same cluster in Fig. 2. Substitution Strategy The equivalence or exchangeability of spans, which a substitution strategy aims to establish, boils down to answering the following two questions: (1) what is an eligible span?(2) how to define the equivalence?First, given a consecutive span s = [w p , w p+1 , ..., w p+k ] where w p+i (0 ≤ i ≤ k) represents a semantic unit (i.e., a word with semantic meaning), we define the span to be eligible if and only if it is semantically self-contained and unitary.Fig. 3 shows a non-eligible span example "the yard ate the cake" which corresponds to an union set of two disconnected fragments of the tree and has an ambiguity (the subject of "ate" should be "the bird" rather than "the yard".).Such constraints imposed on eligible spans prevent substitutions with duplicate or missing parts.Due to page limit, we leave the formal mathematical definition of an eligible span into the Supplementary Materials (Section I.C, Definition 1).Second, we formalize a heuristic rule to define the equivalence class of an eligible span s as the combined equivalence classes of its first and last token, i.e., Π(s) = Π([wp, wp+1, ..., w p+k ]) = (π(wp), π(w p+k )),</p>
<p>where π indicates the equivalence class of a single word as defined in Section III-A.By defining as above, it is legal to substitute a span s 1 with another span s 2 if and only if (1) both s 1 and s 2 are eligible and (2) Π(s 1 ) = Π(s 2 ).Detailed pseudo codes of CompSub is also available (i.e., Algorithm.1).When dealing with tree structured tasks like GeoQuery and COGS, there are two special cases that need to be considered: • s = [w p ] (e.g., "largest" in Fig. 2) degenerates to a single word: we specify that s can only be substituted with another span s ′ (either degenerated or undegenerated) with Π(s ′ ) = [π(w p ), π(w p )]. • s is a subtree with its root token w r : we specify that s can exchange with either another subtree s ′ with Π(s ′ ) = [π(w r ), π(w r )] or another span s ′ with Π(s ′ ) = [π(w p ), π(w p+k )]).</p>
<p>B. Learning-based Component Substitution</p>
<p>Beyond the benefit of multi-grained compositional bias introduced by CompSub, the following three observations lead us to take a step further towards augmentation with 15 return D aug attention on challenging spans.(1) The distinct combinations for a linear number of distinct spans could be as many as the super-linear number [58].(2) The spans constitute both easy-to-comprehend and elusive ones, while oftentimes elusive ones are so rare that those combinations by them account for a very small portion.(3) It is imperative to increase the percentage of these minority combinations to improve the compositional generalization in a broad range of down-stream tasks.Concretely, we introduce an online and optimizable LCS framework consisting of a LCS augmenter that inherits the idea of span substitution with CompSub.More importantly, through maximizing the loss of down-stream neural sequence models, we learn span substitution probabilities in the upstreaming LCS augmenter to put high values on those chanllenging compositions of elusive spans and novel surroundings.The overview of the LCS framework is shown in Fig. 4. a) Parameterizing the LCS Augmenter.:Given a training example d = (x, y), the objective of the LCS augmenter is to synthesize a new example d gen = (x gen , y gen ) via a sequence of two actions a = (a out , a in ): (1) a out which selects the span s out to be swaped out from the span set S 1 = {s i 1 } u i=1 extracted from x 2 , and (2) a in which selects the span s in to be swapped in from the span set S 2 = {s i 2 } v i=1 extracted from the whole training dataset, following a out .Note that the preprocessing and span set extraction procedures are similar with Section III-A, and S 1 ⊂ S 2 .Once s out and s in are selected, we have d gen via recombination, i.e.,</p>
<p>• x gen = x.replace(sout ,s in ),</p>
<p>• y gen = y.replace(align(sout ),align(s in )), where replace(p, q) denotes p is replaced with q.</p>
<p>2 We can also identify spans in the y.This depends on the task type.</p>
<dl>
<dt>LCS Augmenter (𝝓)</dt>
<dd>
<p>TL W TL W TL W TL W … TR J TR J TR J TR J …        (x, y), the upstream LCS augmentor (parameter:ϕ) predicts the probability distribution of the spans in (x, y) to be substituted out and the probability distribution of the spans in the training set to be substituted in.Sampling the spans to be substituted out and substituted in from the above distributions, we augment the original training example to generate (xgen, ygen) and send it into the down stream neural seq-to-seq model (parameter:θ).In the parameter-update phase, we iteratively update ϕ by maximizing the loss of the downstream model and update θ by minimizing the loss of the downstream model.</p>
</dd>
</dl>
<p>The probability of generating an ideal d gen based on d is intuitively factorized as follows:
p(d gen |d; ϕ) = p(a|d; ϕ) = p((a out , a in )|d; ϕ) = p(a out |d; ϕ) • p(a in |a out , d; ϕ)(2)
where ϕ denotes the parameters of the LCS augmenter.In the following, we will detail how to model the two probabilities, during which we will introduce the the three parts that constitute ϕ.Parameterizing p(a out |d; ϕ) for selection of spans to be substituted out Whether a span should be swapped out conditions on the equivalence class and the surroundings of the span, which are dictated by the representation of the span and that of the original training sequence x, respectively.To this end, we formulate the probability distribution p(a out |d; ϕ) over all u candidate spans in S 1 as follows,
p(a out |d; ϕ) = τ (M(ϕ e (x), ϕ o (S 1 ))),(3)
where ϕ e as the first part of ϕ represents the parameters of a sequence encoder R(•), and ϕ o (the second part of ϕ) denotes the embedding module for each candidate span in the span set S 1 .M(•, •) is a similarity function that measures the distance between two vectors.τ refers to the gumbelsoftmax function [59], which guarantees sampling of the span with the largest probability, i.e., a * out ∼ p(a out |d; ϕ), to be differentiable.Implementation of the sampled action a * out results in the selected span s * out to be substituted out.Parameterizing p(a in |a out ; d; ϕ) for selection of spans to be substituted in The factors that govern the selection of a span to be swapped in from the whole span set S 2 include the representations of (1) the span itself, (2) the input sentence x for augmentation, and (3) the previously selected swapout span s * out , so that those elusive spans that share the equivalence class with s * out but contribute novel compositions via recombination with surroundings in x are prioritized.Consequently, the probability distribution p(a in |a out , d; ϕ) over all v candidate spans in S 2 follows,
c = [ϕ e (x); ϕ o (s * out )]), p(a in |a out , d; ϕ) = τ (M(ϕ f (c), ϕ i (S 2 ))),(4)
where ϕ f and ϕ i altogether act as the third part of ϕ.Specifically, ϕ i is the embedding module for all spans in the span set S 2 and ϕ f aligns the concatenated representation of the sentence and the swap-out span, i.e., c, with ϕ i (S 2 ) into the commensurable space.Being consistent with the previous paragraph, we leverage the similarity function M(•, •) and the gumbel-softmax trick τ to sample a * in ∼ p(a in |a * out , d; ϕ).It is noteworthy that we manually set the probability Training objective for the seq-to-seq model The objective of training the seq-to-seq model is to minimize the expected negative log-likelihood of producing the output sequence y gen from the input one x gen conditioned on the its parameters θ, i.e.,
a in → 0 if Π(s in ) ̸ = Π(s * out )min θ L s (θ) = min θ E dgen∼Dgen [− log p(ygen|xgen; θ)] ≈ min θ − 1 N T N n=1 T t=1 log p(y n,t gen |x n,t gen ; θ).(5)
We would highlight that the empirical estimation samples over not only N examples but also T sequences of actions for each example, thus avoiding the randomness and high variance induced by the gumbel softmax trick.Thus, (x n,t gen , y n,t gen ) denotes a generated example from the n-th original training example by following the t-th sampled action sequence (a n,t out , a n,t in ).D gen represents the distribution of all generated samples by the augmenter.Training objective for the LCS augmenter Our main purpose is to encourage the upstream LCS augmenter to outweigh those challenging compositions by the elusive spans and novel surroundings.To achieve this goal, we evaluate the difficulty of a newly composed example d gen by the feedback from the down-stream seq-to-seq model, i.e., the negative log-likelihood of predicting it; the larger the negative log-likelihood is, the more challenging the generated example is.Intuitively, we solve the following optimization problem to train the LCS augmenter to maximize the difficulty of synthesized examples.Optimize θ on B gen through Objective 5
13 return ϕ, θ max ϕ L a (ϕ) = max ϕ E dgen∼Dgen [− log p(ygen|xgen; θ)] ≈ max ϕ − 1 N T N n=1 T t=1 p(d n,t gen |d n,t ; ϕ) log p(y n,t gen |x n,t gen ; θ),(6)
where p(d n,t gen |d n,t ; ϕ) refers to the gumbel softmax probability distribution of the t-th sampled action sequence (a n,t out , a n,t in ) that translates d n,t into d n,t gen .To keep the LCS augmenter timely posted of the training state of the neural seq-to-seq model, we alternatingly optimize these two parts.We present the pseudo codes for training LCS in Algorithm.2.</p>
<p>C. Generalize the methods to In-Context Learning Scenarios</p>
<p>Large language models (LLMs) have garnered significant attention primarily due to their effectiveness as "few-shot learners" [14] and their ability to be adapted to various downstream tasks through the In-Context Learning (ICL) paradigm [33].ICL typically involves selecting few-shot (e.g., k-shot) exemplars, comprising both inputs {x i } k i=1 and outputs {y i } k i=1 (referred to as "few-shot demonstrations"), from the available demonstration pool D (i.e., the training data in the classical learning paradigm) and attaching these demonstrations {(x i , y i )} k i=1 immediately before the real query input x q to form the prompt for the LLM.In realistic applications of ICL, although x q may not directly appear in D, it is often a recombination of components observed in D [34], [35].Hence, it is important to investigate and enhance the capacity of LLMs to grasp the meaning of individual components from the very limited demonstrations and generalize the acquired knowledge to the recombined queries (i.e., the compositional generalization under ICL).We formally define the compositional generalization under the k-shot ICL of LLMs as follows: First we have a demonstration pool D and a testing set D test in which each instance (x q , y q ) satisfies that (x q , y q ) / ∈ D and (x q , y q ) can be derived through recombining the components that appear in the instances in D. For each testing case (x q , y q ) ∈ D test , we prepend k demonstrations (x i , y i ) k i=1 ∈ D before x q to form a k-shot ICL prompt Prompt(x q ) and then use it to query the LLM f θ (•) (i.e., the predicted output ŷq = f θ (Prompt(x q ))).The compositional generalization performance under the k-shot ICL of LLMs is defined as the average prediction accuracy:
1 |Dtest| (xq,yq)∈Dtest I(f θ (Prompt(x q )), y q ).
The strategy of selecting the demonstrations for the given query is the key to the compositional generalization performance of the in-context learning.Recent studies show that randomly sampling demonstrations from the demonstration pool results in poor compositional generalization [19], [35].Inspired by previous theoretical insights on ICL [60], [61], LLMs implicitly perform parameter update via gradient descent on the few-shot in-context demonstrations.Given that the number of demonstrations in the ICL prompt is typically much smaller than the entire demonstration set [62], [63], adaptively determining appropriate few-shot demonstrations for a specific query significantly impacts the compositional generalization performance of the ICL in LLMs [19], [35].</p>
<p>To this end, we extend the main idea of the LCS method to the ICL scenarios, resulting in the LCS-ICL algorithm (the workflow of LCS-ICL is depicted in Figure 5).Our LCS-ICL algorithm incorporates two key ideas: one is recombining existing demonstrations to introduce additional compositional inductive bias in the in-context learning stage; another is selecting the recombined demonstrations according to the feedback from the LLM (the most difficult demonstrations) to enhance the exposure of the composition of elusive concepts and novel context surroundings.</p>
<p>Specifically, supposing that we are going to select k demonstrations {(x i , y i )} k i=1 from the demonstration pool D for the current query x q , we present the workflow of the LCS-ICL algorithm as the following three stages: (1) Coarse Screening Stage:
Select m = ⌈k/2⌉ examples from D: {(x i , y i )} m i=1
to guarantee that all primitive concepts (e.g., the query "The child on a table burned the pizza beside a stage" contains the concepts of {"child", "on", "table", "burned", ..., "stage"}) in the query x q are covered in {x i } m i=1 .{(x i , y i )} m i=1 are selected as the first m demonstrations for the ICL prompt.Note that if ⌈k/2⌉ demonstrations are insufficient to fully cover all primitive concepts in x q , we select m &gt; ⌈k/2⌉ demonstrations until full coverage is achieved (m ≤ n).We use the greedy algorithm for the Set-Cover problem to implement the first stage.( 2 We input the Prompt into the large language model and calculate the Perplexity score 3 (PPL) of the y * cand part (We define this value as the LCS-Score of this candidate demonstration).We traverse all possible candidates and select the one with the highest LCS-Score as the selected demonstration {x * i , y * i }.We then proceed to retrieve the next demonstration {x * i+1 , y * i+1 } from the remaining pool D * /{x * j , y * j } i j=1 .In real applications, we typically select the demonstration from a random sampled subset S ⊆ D * (e.g., |S| = 100) to speed up the running time.The pseudo codes for the LCS-ICL algorithm is provided in the Supplementary Materials (Section IV).</p>
<p>IV. THEORETICAL INSIGHTS</p>
<p>In this section, we aim to provide theoretical insights into the algorithms we introduced in Section III.We demonstrate that: from the perspective of optimization, our proposed compositional data augmentation strategy, CompSub (Algorithm 1), serves as an implicit regularization term.It characterizes the semantic invariance of specific components that, when combined with different context surroundings, make up the input text data.Learning this semantic invariance helps the language model disentangle the meaning of different individual components, thereby mitigating the spurious correlation [29], [64], [65] between the target component and its context surroundings that co-occur frequently in the training set in the end-to-end training manner; from the perspective of generalization risk, our learning-based compositional data augmentation framework (Algorithm 2, after fully leveraging CompSub to extend the training data distribution) can be interpreted as automatically generating the most challenging compositional examples.This process reduces the Rademacher complexity [32] and consequently, tightens the upper bound of the generalization risk.Due to the page limitations, we present most of the proof details in the Supplementary Materials (Section II).</p>
<p>Following the analysis framework of Chen et al. [66], we formulate all possible component exchange operations (in the following discussion, we call each component exchange operation a 'transform') as a permutation group [67] G.Each permutation element g ∈ G naturally represents a set of component substituting operations (e.g., "jump left"→"run around right"; "look right"→"walk opposite left"; ...), where we apply these operations to compositionally augment the 3 https://en.wikipedia.org/wiki/Perplexityoriginal training data.For a given permutation element g and an original data point x ∈ X , we denote that g • x refers to apply the corresponding component substitution operations to x.In alignment with the previous theoretical analysis [68], [69] on data augmentation, we make the following assumption.</p>
<p>Assumption 1 (Unbiased on average) Let x represent a sample from the training set D, f θ represent the neural network mapping function, and g represent a transform operation sampled from the group G.We assume that:
E g∈G [f θ (g •x)] = f θ (x).
Considering the example (x, y) (x refers to the input and y refers to the output) in the training set, the input sequence x could be divided into two parts x * (the key component we want to disentangle from the whole sequence) and x s (the surrounding context of x * in x): x = x * ⊕x s .Correspondingly, the output sequence y could be divided into y * and y s (the counterparts of x * and x s ): y = y * ⊕ y s .The operator ⊕ denotes the 'composition' of a component and its surrounding context.By way of example, in the SCAN dataset [2]: x ="jump around right twice and walk left", y ="TR J TR J TR J TR J TR J TR J TR J TR J TL W".Supposing we focus on the component "jump around right twice", x * ="jump around right twice", x s ="and walk left", correspondingly y * ="TR J TR J TR J TR J TR J TR J TR J TR J", y s ="TL W".</p>
<p>The objective of modeling the sequence-to-sequence probability of a language model θ is p θ (y|x) = p θ (y s ⊕y * |x s ⊕x * ).Note that y s can be splitted into two parts y 1 s and y 2 s .y 1 s refers to the surrounding context that is ahead of y * and y 2 s refers to the surrounding context that is behind of y * .We have the following probability decomposition:
p θ (y|x) = p θ (y s ⊕ y * |x s ⊕ x * ) = p θ (y 1 s |x s ⊕ x * )p θ (y * |y 1 s , x s ⊕ x * )p θ (y 2 s |y * , y 1 s , x s ⊕ x * ).
In this section, we mainly consider the errors that frequently occur when predicting the key component y * : p θ (y * |y 1 s , x s ⊕ x * ).In the following paragraphs, for the simplicity, we directly use the notation y s to represent y 1 s and we focus on the probability p θ (y * |y s , x s ⊕ x * ).</p>
<p>Theorem 1 Let h denote the negative likelihood loss function: h(p) = −log(p).We have the following inequality: When we optimize θ with gradient descent based methods, we force the compositional augmentation loss E g,xs,ys [h(p θ (y * |g • y s , (g • x s ) ⊕ x * ))] to approach 0, which meanwhile turns out to force right-hand side (RHS) to approach 0. Theorem 3 shows that, based on the original loss term, our compositional data augmentation strategy implements an implicit regularization term g∈G ∥p θ (y * |g • y s , (g
E xs,ys [h(p θ (y * |y s , x s ⊕x * ))+2 g∈G ∥p θ (y * |g •y s , (g •x s )⊕ x * )−p θ (y * |y s , x s ⊕x * )∥ 2 2 ] ≤ E xs,ys E g [p θ (y * |g •y s , (g •x s )⊕ x * )],• x s )⊕x * )−p θ (y * |y s , x s ⊕x * )∥ 2
2 .The meaning of this term is to encourage the model to predict the same y * given a specific</p>
<p>x * and diverse surrounding contexts, i.e., to disentangle the meaning of the individual semantic component x * from the entire input x.</p>
<p>Corollary 1 In the ideal conditions, where the training loss converges to zero and our compositional data augmentation fully substitutes the surrounding context (x s , y s ) for every (x * , y * ), we can disentangle the language modeling probability:
p θ (y s ⊕ y * |x s ⊕ x * ) = p θ (y s |x s ) • p θ (y * |x * ).
Such disentanglement, which is widely pursued in the compositional generalization literature [37], [70], helps to learn the invariance [29], [30] and eliminate spurious correlation [31], [64] in end-to-end language modeling and compositional generalization.</p>
<p>Denote the augmented training data distribution as X aug and the unseen compositional testing data distribution as X comp .Let S = {(x i , y i )} n i=1 be a sample set drawn independently identically distributed from X aug , H be a model class and l : → [0, 1] be a loss function.For a h ∈ H, we define the empirical risk as R S (h) = 1 n n i=1 l(h(x i ), y i ), the expected risk R aug (h) = E (x,y)∼Xaug [l(h(x), y)] and the expected risk for the compositional generalization [71].L is the lipschitz constant of h and W 1 denotes the 1-Wasserstein distance [72].This value of the expression LW 1 (X aug , X comp ) depends on the inherent ability of the data augmentation scheme: in comparison with previous compositional data augmentation scheme (e.g., SUBS [25] and LexSym [21]), CompSub can achieve a relatively low 1-Wasserstein distance between the augmented training data distribution and the true compositional testing data distribution (we present empirical results in Supplementary Materials Section IV).Then we focus on analyzing the first term R aug (h).(Rademacher Complexity) The empirical rademacher complexity of M (M is interpreted as the family of loss function l associated with the model class H mapping from the data distribution X to R) is defined as: R S (M) = σ[sup m∈M
R comp (h) = E (x,y)∼Xcomp [l(h(x), y)]. We have R comp (h) = R aug (h) + (R comp (h) − R aug (h)), where the second term (R comp (h) − R aug (h)) is upper bounded by LW 1 (X aug , X comp )1 n n i=1 σ i m(x i , y i )].
Theorem 2 Given a substitution operation group G, define l as l(x i , y i ) = max g∈G (l(h(g • x), g • y)).We have that the expected risk for the compositional generalization R comp (h)
is upper bounded by R S ( l • h) + 2R S ( l • h) + 3 log 2 δ 2n + LW 1 (X aug , X comp ).
As in the previous discussion, the distribution distance is determined by the inherent ability of the compositional augmentation scheme.Hence we focus on the rademacher complexity term R S ( l • h).As shown in Zhu et al. [73], R S ( l•h) is upper bounded by the original complexity R S (h).In the LCS training framework (Algorithm 2), given an original case (x, y) ∈ D aug we train a neural network to select components to substitute out and substitute in, generating the augment example (x ′ , y ′ ) that has the largest loss value.Ideally, the generated example (x ′ , y ′ ) = (g • x, g • y), where g = argmax g l(h(g • x), g • y).Hence we demonstrate that with the LCS algorithm, the Rademacher complexity will be decreased.Meanwhile, we use the generated examples (x ′ , y ′ ) to train the sequence model by minimizing the empirical risk R S ( l • h) (to near zero).These factors together guarantee that the generalization bound R S ( l
• h) + 2R S ( l • h) + 3 log 2 δ 2n + LW 1 (X aug , X comp
) is tighter compared with the naive training algorithm (training with the l objective).</p>
<p>V. EXPERIMENT</p>
<p>In this section, we present our empirical results for the methods proposed in Section III.Specifically, in Section V-A, we introduce the datasets and the corresponding evaluation metrics used to assess our methods.In Section V-B, we briefly review the representative baseline methods to demonstrate the superiority of our methods.In Section V-C, we present and describe the main experiments results.Finally, in Section V-D, we provide a detailed analysis of the empirical results, including a breakdown of the performance, ablation studies, and additional insights.</p>
<p>A. Datasets and Evaluation Metric</p>
<p>We evaluate our proposed methods on the following four popular and representative datasets (including ten sub-tasks in total) which are specifically designed for assessing the compositional generalization capacity of neural language models.The datasets adopted in the paper are diverse for (1) all kinds of lexical-level generalization testing cases [1], [2], [19], [37] and structural-level generalization testing cases [1], [17], [19], [74] are covered in these datasets, making the evaluation holistic and (2) there are not only synthetic evaluations deliberately designed for diverse categories of compositional generalization but also non-synthetic ones additionally requiring capabilities of neural models in handling natural language variations [75].In the following paragraphs, we introduce these datasets and tasks one-by-one.SCAN Initially introduced by Lake and Baroni [2], SCAN contains a large set of synthetic paired sequences whose input is a sequence of navigation commands in natural language and output is the corresponding action sequence.Following previous works [21], [23], [26], [41], [76], we evaluate our methods on the two splits of SCAN: jump split (designed to evaluate a novel combination of a seen primitive, i.e., jump, and other seen surroundings) and around right split (designed to evaluate a novel compositional rule).In particular, we also consider two more complex and challenging scenarios: length split (designed to evaluate longer sequences when trained on shorter ones) and Maximum Compound Divergence (MCD) splits of SCAN established in Keysers et al. [17], which distinguish the compound distributions of the training and the testing set as sharply as possible.COGS Another synthetic COGS dataset [1] contains 24,155 pairs of English sentences and their corresponding logical forms (with the Lambda calculus).COGS contains a variety of systematic linguistic abstractions (e.g., active → passive, nominative → accusative and transtive verbs → intranstive verbs), reflecting compositionality of natural utterance.It is noteworthy that COGS with its testing data categorized into 21 classes by the compositional generalization type supports fine-grained evaluations.GeoQuery The non-synthetic dataset of GeoQeury [74] collects 880 anthropogenic questions regarding the US geography (e.g., "what states does the mississippi run through?")paired with their corresponding database query statements (e.g., "answer ( state ( traverse 1 ( riverid ( mississippi ) ) ) )").Following [25], [57], we also adopt the FunQl formalism of GeoQuery introduced by [77] and evaluate our methods on the compositional template split (query split) from Finegan-Dollak et al. [78] where the output query statement templates of the training and testing set are disjoint and the i.i.d.split (question split) where training set and testing set are randomly separated from the whole dataset.COGS-QL COGS-QL [19], [79] is a variant of COGS [1] which can be used for accessing large language models' compositional generalization capacity in the In-Context Learning (ICL) scenarios.The traditional COGS task might be hard to be fully resolved by current open-sourced LLMs [15], [16] due to its complex output form.Different with COGS, COGS-QL reconstruct the output sequences from the Lambda calculus form to the FunQL [77] form (which is much simpler), making the generalization task more focus on the compositional skills themselves.For experiments with LLaMA2-13B, we randomly sample 200 examples from the COGS-QL dataset to test the performance.For experiments with LLaMA3-8B, we randomly sample 1,000 examples from the COGS-QL dataset to test the performance.Evaluation Metric Following the convention of previous works [1], [2], [78], [79], we adopt the evaluation metric of exact-match accuracy in all of our experiments.</p>
<p>B. Baselines and Base Models</p>
<p>We compare our proposed approaches, including Comp-Sub and LCS (for both training-based paradigm and ICL paradigm), with following prior state-of-the-art baseline approaches for compositional generalization.Compositional Data Augmentation Approaches: Note that our method also falls into this category.We compare our proposed CompSub and LCS with GECA [23] and LexSym [21] on SCAN, COGS and GeoQuery tasks, Prim2PrimX [26] (in combination with a mutual exclusive training technique) on SCAN and COGS tasks, Comp-IBT [76] on the SCAN tasks, and SUBS [25] on the GeoQuery tasks.Methods with Specifically Designed Architectures: We include multiple baseline methods that design specific architectures for compositional generalization tasks, such as CGPS [37], LexLearn [41], IR-Transformer [40], Dangle [80] and SpanParse [49].</p>
<p>Methods That Leverage Special Training Algorithms:</p>
<p>We also consider using meta-learning training method MAML [46] and mutual exclusive training technique [26] to improve compositional generalization as two baselines.Pre-trained Language Models: Apart from the aforementioned approaches which are specialized for compositional generalization, we also include several widely-used pre-trained language models including T5 [13], BART [12] and GPT-3.5 [81] (code-davinci-002) in the experiment results for readers' reference.Prompting Techniques: For ICL experiments, we adopt different types of prompting techniques as our baselines.For basic prompting techniques, we select standard few-shot ICL [14] and few-shot Chain-of-Thought (CoT) [53] prompt as our baselines.We also select two approaches, Primitive Coverage [19] and BM25 [55] that are selecting few-shot demonstrations that are most relevant to the testing case.On their basis, we adopt three approaches, Cosine-Score Retrieval (CSR) [35], Bert-Score Retrieval (BSR) [35] and Gist-Score Retrieval (GSR) [36], that are selecting not only relevant but also most informative demonstrations as the ICL demonstrations.Last but not least, we also extend other state-of-the-art data augmentation methods, GECA [23] and LexSym [21], to the ICL scenarios as our baselines for a comprehensive comparison.For each approach, we test its performance with four different shot numbers: 6, 12, 18 and 24.</p>
<p>Besides, since our proposed methods is model-agnostic, in our experiments we adopt diverse base model architectures (note that the comparison between our methods and other baseline methods is with the same base models to guarantee fairness): (1) Long Short Term Memory (LSTM) -based [8] + Attention-based Seq-to-Seq [9] Models, which is a traditional and widely-used architecture to model sequential data (e.g., text data).We conduct experiments with both one-layer LSTM models aligned with [23] and two-layer LSTM models aligned with [41].(2) Vanilla Transformers [11], which is another powerful architecture to model language.We conduct experiments with three-layer Transformers following the setting in [26].(3) Pre-trained LMs, which are typically pre-trained on large-scale unlabeled text data in a self-supervised manner.We aim to investigate whether our methods consistently boost pre-trained language models' performance or not.We adopt BART-base [12] in experiments on the GeoQuery dataset.(4) LLMs, which are used in our experiments regarding to ICL scenarios.We choose two LLMs that are widely-used in academic works and open-source community: LLaMA-2-13B [15] and LLaMA-3-8B [16] as our base models.</p>
<p>C. Main Results</p>
<p>The main results of our experiments on SCAN, COGS, GeoQuery and COGS-QL tasks are shown in Table I  experiment with five different seeds and report both of the mean and the standard deviation.On GeoQuery tasks, we run each experiment with three different seeds and report the mean of testing performances.Detailed experiment results on each dataset are listed as follows.</p>
<p>SCAN Results</p>
<p>In Table I, we observe that our proposed method CompSub leads to significant improvement (in comparison with existing baselines) for both of Seq-to-Seq and Transformer architectures on almost all of six subtasks that we study (jump, around right, length, MCD1, MCD2 and MCD3).We find that on the easier testing tasks jump and around right CompSub can reach near perfect prediction accuracy (near 100.0%), while on the more difficult tasks [17] length, MCD1, MCD2 and MCD3 though achieving tremendous improvement over the base model (by at most 73.1%) there still exist large room for further improvement.We observe that exactly on these harder tasks, with the LCS training framework we can consistently further achieve generalization performance gain on the basis of solely adopting CompSub, which also implies that LCS can help model focus more on these difficult compositional patterns.Specifically, on the length and MCD3 sub-tasks, adopting LCS can additionally bring improvement of 11.5% and 6.2% on average.Comparing our method and exisiting methods, we observe that the combination of CompSub and LCS exceeds most baseline approaches by a large margin.The only exception is the performance on the MCD2 tasks with Comp-IBT is a little bit higher than the performance with Seq-to-Seq architecture and CompSub.Note that Comp-IBT requires to access 30% compositional generalization testing data in advance (in the training phase) and hence it is not directly comparable with ours.</p>
<p>Method</p>
<p>COGS [1] MAML [46] 64.1%±3.2%IR-Transformer [40] 78.4% Roberta+Dangle [80] 87.6% T5-Base [13] 85.9%</p>
<p>Seq-to-Seq [9] 55.4%±4.2%</p>
<p>+GECA [23] 48.0%±5.0%+LexLearn [41] 82.0% ±0.0% +LexSym [21] 81.4%±0.5% +Prim2PrimX+MET [26] 81.1%±1.0%+CompSub (Ours) 91.8%±0.1% +CompSub+LCS (Ours) 92.3%±0.2%</p>
<p>COGS Results</p>
<p>In Table II, on the COGS task the performance of our base model (Seq-to-Seq) increases from 55.4% to 91.8% when we leverage CompSub to generate additional training data.CompSub has approximately 10% lead compared with our baseline methods (LexLearn, LexSym, Prim2PrimX+MET) implemented on the exact same base model.Even compared with methods that leverage powerful pre-trained language models (e.g., Roberta+Dangle and T5-Base), LSTM+CompSub still has some advantages.Furthermore, we can improve the performance of our base model from 91.8% to 92.3% when adopting the LCS training framework based on CompSub.</p>
<p>GeoQuery Results</p>
<p>In Table III, we show that on the compositional template query split CompSub leads to substantial and consistent improvement over other baseline</p>
<p>Method</p>
<p>GeoQuery [74], [78] Question Query SpanParse [49] 78.9% 76.3% code-davinci-002 [81]+Cover-LS [82] 88.7% 85.3%</p>
<p>Seq-to-Seq [9] 75.2% 58.6% +GECA [23] 76.8% 60.6% +LexSym [21] 81.6% 80.2% +SUBS [25] 80.5% 90.2% 71.9% +GECA [23] 87.9% 83.0% +LexSym [21] 90.2% 87.7% +SUBS [25] 91.8%</p>
<p>COGS-QL Results</p>
<p>In Table IV, we show the empirical results of utilizing our LCS algorithm to boost LLMs' in-context learning performance on the COGS-QL task.Firstly, we find that directly using standard few-shot ICL prompting or chainof-thought ICL prompting can hardly solve the COGS-QL task.We clearly observe that for both of two LLMs (LLaMA2-13B and LLaMA3-8B) using more in-context demonstrations in our LCS approach leads to better performance.However, many existing baselines (e.g., CSR, GSR, LexSym and stuff) do not show such an monotone increasing trend.When comparing our approach (LCS) with existing baselines, we find that in most cases LCS outperform best baseline methods by at most 8.8% using LLaMA3-8B and 18-shot ICL prompt: best baseline PrimCoverage (48.8%) versus our LCS (57.6%).Meanwhile we also note that with LLaMA2-13B and small shot numbers (6 and 12) the performance of LCS is a little bit worse than best baselines (GSR) while when shot number increases (18 and 24) LCS surpasses GSR (e.g., shot number = 24: LCS (47.5%)) versus GSR (42.5%).This is mainly because the small number of ICL demonstrations limits the initial search space of LCS, hence weakening its performance.</p>
<p>To draw a short conclusion, we conduct extensive experiments and empirically demonstrate the superiority of Comp-Sub and LCS from the perspective of overall compositional generalization performance.In this process, we consider different tasks, different base model architectures and different shot numbers of in the ICL prompts to ensure the rigor and the comprehensiveness of our study.</p>
<p>D. Result Interpretation, Analysis and Ablation Study</p>
<p>In this section, we mainly present results for some analysis experiments.Specifically, we aim to further answer the following Research Questions (RQs).RQ1: (Analyzing the performance of CompSub) Grounded on the same original training set, does the CompSub help with fully exploring the potential augmentation space as supposed in Section I, especially when compared with existing augmentation approaches [21], [23], [25]?RQ2: (Analyzing the performance of LCS) Is the LCS truly capable of automatically mining hard and rare data re-combination patterns as supposed in Section I? What kind of re-combination patterns are recognized as difficult for language models?RQ3: (Ablation Studies) One of the key modules in the LCS framework (both for the Algorithm 2 and the LCS-ICL Algorithm in our design is the selective augmentation module.Does this up-stream selective module really play a necessary role in compositional generalization?RQ4: (Applicability to different base models) Are our proposed methods applicable to different architectures (e.g., for sequence modeling architecture: LSTM [8] and Transformer [11]; for pre-trained language model: BART [12], LLaMA2 [15] and LLaMA3 [16])?RQ1: Analyzing the performance of CompSub To help deepen the understanding of the performance improvement brought by CompSub, we first break down the overall performance on COGS task into four different parts (for specific examples please refer to Table V), including lexical generalization performance and three different types of structural generalization performance.</p>
<p>The results are shown in Table VII.Compared with LexSym, which only enable single-grained substitutions (i.e., substituting for single words), we find that CompSub can not only improve generalization on testing cases of different structural types, but also further boost the lexical level generalization.We have similar observation in Table I.In SCAN results, we find that for lexical generalization (i.e., jump split), CompSub achieves similar performance with the best augmentation baseline LexSym.When it comes to diverse and more difficult structural generalization cases (e.g., MCD1, MCD2 and MCD3), CompSub exceeds existing augmentation baselines (GECA, LexSym and Prim2PrimX) by a very large margin.Besides, in Section I, we hypothesize that CompSub enables multi-grained compositions of substantial substructures in the whole training set and thus lead to improvement for various kinds of compositional generalization.We provide a statistic on the maximum number of augmented examples (after deduplication) on the query split of GeoQuery dataset (      CompSub and testing the trained models' performance.We plot the performance (in comparison with existing augmentation baselines) in Figure 6, demonstrating that our method consistently outperform the baselines in low-data regime.RQ2: Analyzing the performance of LCS Our main question is Is the LCS truly capable of automatically mining hard and rare data re-combination patterns?.</p>
<p>For results on SCAN-MCD tasks, we present the results of leveraging CompSub in Figure 1 (d Fig. 7.A composition that helps to improve "cp recursion" generalization.The composition of "Liam was told that Peter hoped that" and "John saw that the cake was ate ."results in deeper recursion of that-structure: the recursion depth increases from 2 to 3.</p>
<p>present the results of using the LCS augmentation and a random augmentation baseline (which is realized by substituting the learned augmentation module in the LCS framework with a random augmentation module.) in Table VIII.We can conclude that LCS is especially helpful for improving the performance of down-stream neural seq-to-seq models on the prediction of harder examples (here harder examples refer to those contain the compositions of elusive concepts like "walk around right" and novel surroundings).</p>
<p>For results on the COGS task: as shown in Table VII, we find that the utilization of LCS framework training can help models better generalize on testing cases of "cp recursion" type, which requires precisely combining two special structures (we depict an example in Figure 7: "cp recursion" requires to precisely combine "John knew that the cake was ate ." in (a) and "Liam was told that Peter hoped that" in (b) to generate (c) which contains a deeper recursion of thatstructure.RQ3: The importance of adopting the selective augmentation module in the LCS framework We aim to leverage control experiments to verify the effectiveness of the selective augmentation module in the LCS framework: as for the experimental group, we select the augmented examples according to the probability output by the learned neural network in Algorithm 2 or directly the feedback output by the  down-stream LLMs in LCS-ICL Algorithm; as for the control group, we select the augmented examples in a completely random manner.For Algorithm 2, we show the comparison between the experimental group (denoted by "+Learned Augmentation") and the control group (denoted by "+Random Augmentation") on three SCAN-MCD splits in Table IX.Through observing the results, we conclude that (for both Seq-to-Seq [9] and Transformer [11] architectures; directly use or use in combination with CompSub) "+Learned Augmentation" groups consistently exhibit better performance than the corresponding "+Random Augmentation" groups, highlighting the importance of adopting the selective augmentation module in Algorithm 2.</p>
<p>For the LCS-ICL Algorithm, we show the comparison between the experimental group (denoted by "LCS(selective version)") and the control group (denoted by "LCS(random version)") on the COGS-QL dataset in Table IV.We find that in most cases "LCS(selective version)" perform better than "LCS(random version)", verifying the effectiveness of the selective augmentation module in the in-context learning scenarios.We additionally study the effect of the order of the selected demonstrations and the effect of successively selecting demonstrations in the fine screening stage of the LCS-ICL algorithm.We conduct experiments with LLaMA3-8B and show the results in Table X."shuffling" refers to that we shuffle the order of the demonstrations that are selected with the LCS-ICL algorithm (with three different random seeds)."w.o.succ.select."refers to that we select all of the rest demonstrations in the fine screening stage of LCS-ICL algorithm at once (instead of selecting demonstrations successively).We observe that "w.o.succ.select."achieves much lower performance than the original LCS-ICL (Table IV), demonstrating that the successively selecting demonstrations is necessary in our design.We also find that perturbing the order of selected demonstrations has little effect on the ICL performance.RQ4: Applicability to different base models We demonstrate that our methods can be applicable to different base models:</p>
<p>(1) we show the CompSub (Algorithm 1) and the LCS (Algorithm 2) results on SCAN tasks with LSTM-based seq-to-seq models [9] and Transformer models [11] in Table I. (2) we show the CompSub (Algorithm 1) and the LCS (Algorithm 2) results on GeoQuery tasks with LSTM-based seq-to-seq models [9] and pre-trained BART models [12] in Table III.(3) we show the LCS (the LCS-ICL Algorithm) results on COGS-QL tasks with LLaMA2-13B [15] and LLaMA3-8B [16] in Table IV.</p>
<p>VI. CONCLUSION</p>
<p>In this paper, we focus on improving the compositional generalization performance of neural language models from the perspective of data augmentation.Specifically, we propose a new compositional data augmentation method, Comp-Sub, which is the first to explore span-based compositional data augmentation, thus flexibly supporting to inject multigrained compositional bias into the training data.Building on CompSub, we introduce LCS as a differentiable augmentation framework that first enables difficulty-aware composition and is compatible with various downstream language models.We further extend the key ideas of CompSub and LCS to incontext learning (ICL) scenarios of large language models (LLMs), proposing LCS-ICL to select the most appropriate demonstrations to enhance the few-shot compositional generalization capacity of state-of-the-art LLMs.Moreover, we provide theoretical insights into the CompSub and LCS algorithms, showing that leveraging CompSub is equivalent to introducing an additional regularization term in the optimization objective.This term encourages the model to learn the invariance of concepts.Additionally, using the LCS framework to train neural language models can reduce the Rademacher complexity, thereby deriving a tighter upper bound on the compositional generalization error.Finally, our comprehensive experimental results on four datasets (across more than ten tasks) and the corresponding analysis strongly demonstrate the empirical effectiveness of our algorithms.</p>
<p>APPENDIX</p>
<p>A. Extraction of span alignments</p>
<p>For SCAN dataset, since there is no off-the-shelf technique to map sequential data in SCAN dataset to tree-form, we slightly the modify algorithm SimpleAlign from [41] to extract consecutive span alignments for our experiments on SCAN.We denote the input sequence as x, the output sequence as y, the span, which is going to be extracted from the input sequence, as v and its counterpart in the output sequence as w.Basically, we extract a pair of span alignment (v, w) following the maximally restrictive criterion:
nec.(v, w) = ∀xy.(w ∈ y) → (v ∈ x) suf f.(v, w) = ∀xy.(v ∈ x) → (w ∈ y) C 1 (v, w) = nec.(v, w) ∧ suf f.(v, w)(7)
Both v and w are supposed to be consecutive fragments in the input sequence and output sequence respectively.We additionally apply appropriate relaxations on the top of criterion (7) to enable the extraction of more spans: we tolerate many-to-one mapping and one-to-many mapping to some extent to avoid discarding of "[verb] around [direction]" and "[verb] [direction]"(e.g., both of interpretations of "walk around right" and "walk right" cover "TR W").Besides, we manually set the maximum number of words in v to 3 and the maximum number of words in w to 8.</p>
<p>For COGS, we directly use the intermediate representation from [40].An   For GeoQuery, following [25], we directly adopt the span trees (gold trees) extracted and aligned by [49].And we refer the readers to get more detailed information about how to construct such span trees from the original paper [49].Note that we slightly correct several denotations in the original gold trees from [49], for they are slightly differing from the ground-truth.To clarify it, we put an example of modification here (given that the others are similar, we do not present the others here):
geoquery["input"] =
"what is the population of washington dc ?" geoquery["program"] = "answer ( population_1 ( cityid ( 'washington', 'dc' ) ) )" // the original gold_spans geoquery["gold_spans"] = {"span": [5,5], "type": "cityid#'washington'"} // after correction geoquery["gold_spans"] = {"span": [5,6], "type": "cityid#'washington'"} // this is just one of the spans // washington dc is the capital city of USA; // washington is a state of USA;</p>
<p>To ensure a fair comparison with previous substitution-based data augmentation methods [21], [25], we rerun their methods on the modified gold trees.</p>
<p>B. Inferring the equivalence class of words</p>
<p>For COGS, we directly leverage the information in the intermediate representations to infer the equivalence class of the words (e.g., NOUN, VERB or PREP).For SCAN and GeoQuery, we use the technique of inferring the types of words form [21], which cluster the words according to their shared contexts in the training set.For GeoQuery, we additionally adopt context2vec methods [83] (where we train a simple one-layer LSTMbased mask-reconstruction model) to boost the exploration of potentially syntactically-equivalent words (i.e., candidates to fill in the masked blank).We put the final result of word-clustering on GeoQuery here as follows:(We cluster the words in the target side)
] cluster3 = ['loc_2','traverse_2'] cluster4 = ['countryid','cityid','stateid', 'placeid'] cluster5 = ['traverse_1','loc_1','capital_2'] cluster6 = ['largest_one','smallest_one'] cluster7 = ['area_1','density_1','population_1'] cluster8 = ['size','high_point_1'] cluster9 = ['most','fewest']</p>
<p>C. Substitution Strategy</p>
<p>In Definition 1, We present the definition of "eligible span" mentioned in Section III.A.</p>
<p>Definition 1 (Eligible Span) Given a sentence or a program sequence S = [e 0 , e 1 , ..., e n ], there exists one and only one multi-way tree T corresponding to S, the inorder traversal sequence4 Λ of which is v 0 → v 1 → ... → v n (node v i corresponds to token e i , 0 ≤ i ≤ n).Any span S ′ = [e p , e p+1 , ..., e p+k ] ⊆ S, where 0 ≤ p ≤ p + k ≤ n, corresponds to a sub-sequence Λ ′ of Λ (i.e., v p → v p+1 → ... → v p+k ).Moreover, an eligible span S ′ also corresponds to a connected substructure T ′ of T , which meet the following 2 requirements:</p>
<p>• there is at most one node v i ∈ Λ ′ which is the child node of node v ∈ Λ\Λ ′5 ; • there is at most one node v o ∈ Λ ′ which is the parent node of node v ∈ Λ\Λ ′ ;</p>
<p>Note that each node in the tree T has one parent node and at least one child node.Specially, the parent node of the root node and the child node(s) of the leaf node(s) are special imaginary nodes.</p>
<p>In this Section, we present the proofs for our theoretical insights (Section IV).Proof For any p 1 , p 2 ∈ (0, 1), we have the following derivations:
(∇h(p1) − ∇h(p2))(p1 − p2) ∥p1 − p2∥ 2 = 1 p1 • p2 ≥ 4 (p1 + p2) 2 &gt; 4. That is ∇h(p 1 ) − ∇h(p 2 ))(p 1 − p 2 ) ≥ λ(= 4) • ∥p 1 − p 2 ∥ 2 , which finishes the proof.
Assumption 2 (Unbiased on average) Let x represent a sample from the training set D, f θ represent the neural network mapping function and g represent a transform operation sampled from the group G.We assume that:
E g∈G [f θ (g •x)] = f θ (x).
Theorem 3 Let h denotes the negative likelihood loss function: h(p) = −log(p).We have the following inequality: Through computing the expectation over the substitution operation g and the surrounding text (x s , y s ) on both two sides, we induce that: Theorem 4 Given a substitution operation group G, define l as l(x i , y i ) = max g∈G (l(h(g • x), g • y)).We have that the expected risk for the compositional generalization R comp (h) is upper bounded by R S ( l • h) + 2R S ( l • h) + 3
E xs,ys [h(p θ (y * |y s , x s ⊕x * ))+2 g∈G ∥p θ (y * |g •y s , (g •x s )⊕ x * )−p θ (y * |y s , x s ⊕x * )∥ 2 2 ] ≤ E xs,ys E g [p θ (y * |g •y s , (g •x s )⊕ x * )],log 2 δ 2n + LW 1 (X aug , X comp ).
Proof We have the following derivations: .</p>
<p>The last step is derived from Mohri et al. [84].This finishes the proof.</p>
<p>In this section, we detailedly describe the training details of our models in our framework (up-stream LCS Augmentor and down-stream neural seq-to-seq model), the selection of hyper-parameters in our Algorithms(CompSub and LCS), the generation configurations for the LCS-ICL inference of LLMs (LLaMA2-13B [15] and LLaMA3-8B [16]) and other details.</p>
<p>D. LCS Augmentor</p>
<p>For both of SCAN and COGS experiments, we use an two layer bidirectional LSTM (with 128 hidden units and an embedding size of 128, a dropout rate of 0.5) as our sequence encoder.We separately use an embedding layer with an embedding size of 512 for the embedding module for spans to be substituted out and another embedding layer with an embedding size of 512 for the embedding module for spans to be substituted in.We use (cosine-similarity•2) ∈ [−2, 2] as all of our similarity functions in LCS augmentor.We set all of the temperatures for gumbel-softmax sampling in LCS augmentor to 1. Besides, we use a Adam optimizer [85] to optimize our LCS augmentor with an learning rate of 1e-3.The above hyper-parameters are commonly used for LSTMbased models in NLP community and hence we did not spend extra efforts to tune them in our experiments.</p>
<p>E. Neural Language Models</p>
<p>We keep this part of hyper-parameters aligned with previous baselines.For jump and around right splits of SCAN and COGS experiments, we keep the hyperparameters of our LSTM in align with [21], [26], [41].We use a 2-layer encoderdecoder LSTM (with attention [10] and copy [86] mechanisms) with 512 hidden units and an embedding size of 512, a droupout rate of 0.4.For MCD1, MCD2 and MCD3 splits of SCAN experiments, the hyperparameters of our LSTM are adopted form [23].We use a 1-layer bidirectional encoderdecoder LSTM (with attention and copy mechanisms) with 512 hidden units and an embedding size of 64, a droupout rate of 0.5.For all of these above experiments, we train our model with an Adam optimizer with an initial learning rate of 1e-3.We use an ReduceLROnPlateau scheduler (implemented in PyTorch) with a scale factor of 0.5 to automatically reduce our learning rate.We set all of the batch size to 128.</p>
<p>For GeoQuery tasks, in align with SUBS [25], we also directly use OpenNMT [87] to implement our LSTM-based model with attention and copy mechanisms and we utilize fairseq [88] to implement our BART-based model.For LSTMbased experiments, we use one-layer bidirectional LSTM in the encoder side and one-layer unidirectional LSTM in the decoder side.We use dropout with a rate of 0.5 and Adam optimizer with a learning rate of 1e-3.We use MLP attention and directly use the attention scores as copying scores and we set the batch size for experiments based on LSTM to 64.For BART-based experiments, we use BART-base models updated by Adam optimizer with a learning rate of 1e-5.We set the rate for both dropout and attention dropout to 0.1 and we use label smoothing with a rate of 0.1.We set the batch size for all of the experiments based on BART to 1024 tokens.Besides, we set the rate of the weight-decay to 0.01.</p>
<p>F. Hyper-parameters in CompSub (Algorithm 1)</p>
<p>For jump and around right splits of SCAN and GeoQuery experiments, we set the iterative depth K in CompSub augmentation scheme to 1.For MCD splits of SCAN experiments, we set the iterative depth K in CompSub augmentation scheme to 2. For COGS experiments, we set the iterative depth K in CompSub augmentation scheme to 4. For SCAN experiments, we set the number of generated examples N (without de-duplicating) to 1e5.For COGS experiments, we set the number of generated examples N (without de-duplicating) to 4e5.For GeoQuery experiments, we simply searching for every potential augmentations in the training set (because the training set for GeoQuery contains merely 519 examples, we try to make the best use of each example.), and the size of augmented set is shown in Table 6.Following [89], [90], we also ensure approximately equal number of the original examples and the augmented examples being used for training in CompSub experiments, giving consideration to both of i.i.d.generalization and compositional generalization.</p>
<p>We decide the iterative depth K through observing that from which iteration there are nearly no more novel data generated.For N , we simply set a number which is large enough compared with the size of the original dataset, and then we de-duplicate the augmented dataset.</p>
<p>G. Hyper-parameters in LCS (Algorithm 2)</p>
<p>One crucial hyper-parameter in Training LCS framework is the warm-up epochs / update steps.In most cases, we need to set an appropriate value to warm-up update steps to guarantee the down-stream sequence model to be fully aware of the distribution (hardness) of the original training examples while not over-fit to them.For most of our experiments(jump, around right, MCD1 and MCD2 splits of SCAN experiments, COGS experiments), we set the warm-up epoch to 5, and then we alternatively train the up-stream module and down-stream module in the LCS framework to 150 epochs in total.For MCD2 split of SCAN experiments, we first train our neural seq-to-seq model for 80 epochs, and then we alternatively train the up-stream LCS augmentor and the down-stream neural seq-to-seq model for 70 epochs 6 .For experiments with LCS framework, we set the number of sampled actions T for each example to 4. All of this part of hyper-parameters are decided by cross-validation.</p>
<p>H. Generation Configurations of LLMs</p>
<p>We conduct all of the LCS-ICL experiments with both LLaMA2-13B [15] and LLaMA3-8B [16].For LLaMA2-13B, we use the version of meta-llama/Llama-2-13b-hf 7 ; For LLaMA3-8B, we use the version of meta-llama/Meta-Llama-3-8B-Instruct8 .We use the default generation configurations for both two LLMs: (1) for LLaMA2-13B, the generation configuration is:</p>
<p>Fig. 1 .
1
Fig. 1. (a), (b) and (c) illustrate three distinct compositional generalization types in COGS[1], which require word-level, subtree-level and general substructure-level recombinations of training data, respectively.Besides, (d) shows concepts in distinct difficulty in the SCAN[2]  dataset, where the interpretation of walk around right is much more complex than that of the other two concepts.</p>
<p>Fig. 2 .
2
Fig. 2.An augmentation example by CompSub.CompSub substitutes a span "largest" with another span "largest city in the smallest", and augments a new question "What is the population of the largest city in the smallest state?".</p>
<p>Fig. 3 .
3
Fig. 3. Examples of non-eligible and eligible spans in COGS.(a) shows a non-eligible span which corresponds to an union set of disconnected fragments of the tree.</p>
<p>opposite right twice after walk around left twice. : TL W TL W TL W TL W … TR TR L TR TR L. to-Sequence Model ()   : jump around right thrice after walk around left twice.</p>
<p>Fig. 4 .
4
Fig.4.Illustration of the LCS training framework.LCS training framework contains an upstream LCS augmentor and a downstream neural seq-toseq model.Given an original training example (x, y), the upstream LCS augmentor (parameter:ϕ) predicts the probability distribution of the spans in (x, y) to be substituted out and the probability distribution of the spans in the training set to be substituted in.Sampling the spans to be substituted out and substituted in from the above distributions, we augment the original training example to generate (xgen, ygen) and send it into the down stream neural seq-to-seq model (parameter:θ).In the parameter-update phase, we iteratively update ϕ by maximizing the loss of the downstream model and update θ by minimizing the loss of the downstream model.</p>
<p>to excluse those potentially illegal synthesized examples.The action a * in finalizes the span s * in to be substituted in.b) Training Procedures for LCS.: Training LCS boils down to two alternating procedures: first, the generated examples by the LCS augmenter pass forward to train the downstream neural sequence-to-sequence model parameterized by θ; second, the performance of the neural sequence model serves as feedback to update the upstream augmenter parameterized by ϕ = {ϕ e , ϕ o , ϕ i , ϕ f }.</p>
<p>Algorithm 2 : 3 Sample B ∼ D; 4 Optimize θ on B through Objective 5 5 while not converged do 6 Sample B ∼ D; 7 for t ← 1 to T do 8 Sample 10 Sample B ∼ D; 11 Sample
23456781011
Training LCS framework Input: Original dataset D, LCS generator initialized parameters ϕ 0 , Seq-to-Seq Model initialized parameters θ 0 , Warm-up update number m, Sampled action number for each given example T .Output: LCS generator parameters ϕ f , Seq-to-Seq Model parameters θ f . 1 θ ← θ 0 ; ϕ ← ϕ 0 2 for step ← 1 to m do B gen,t ∼ p(B gen |B, ϕ); 9 Optimize ϕ on {B gen,t } T t=1 through Objective 6 B gen ∼ p(B gen |B, ϕ); 12</p>
<p>1 LargeFig. 5 .
15
Fig. 5.The figure illustrates the workflow of the LCS-ICL algorithm when constructing a k-shot ICL-style prompt.The whole workflow mainly contains three stage.(1) Coarse Screening Stage: Select m ≈ ⌈k/2⌉ examples from D: {(x i , y i )} m i=1 to guarantee that as many primitive concepts in the query xq are covered in {x i } m i=1 as possible.(2) Demonstration Augmentation Stage: Introduce additional compositional inductive bias by running CompSub on {(x i , y i )} m i=1 to get an augmented demonstration pool D * = CompSub({(x i , y i )} m i=1 ).(3) Fine Screening Stage: Successively retrieve the rest n = k − m demonstrations from D * with the policy of choosing the candidate demonstration that the model are difficult to handle (with the highest perplexity score) by in-context learning from currently selected demonstrations.</p>
<p>where the expression on the left hand side represents the original seq-to-seq loss function E xs,ys [h(p θ (y * |y s , x s ⊕ x * )) with an additional regularization term g∈G ∥p θ (y * |g•y s , (g• x s ) ⊕ x * ) − p θ (y * |y s , x s ⊕ x * )∥ 2 2 ] and the expression on the right hand side represents the seq-to-seq loss function expected on the augmented dataset E xs,ys E g [p θ (y * |g•y s , (g•x s )⊕x * )].</p>
<p>Fig. 6 .
6
Fig. 6.Performance of CompSub in the low-data regime, in comparison with existing augmentation baselines: GECA, LexSym and SUBS.</p>
<p>instance of intermediate representation is shown in Fig 8.We search for every consecutive fragments in the intermediate presentations of COGS to extract eligible spans according to Definition 1.The naive implementation of the</p>
<p>Fig. 8 .
8
Fig. 8.An instance for an intermediate representation, its corresponding treeform and a potential extracted span for COGS.</p>
<p>Fig. 9 .
9
Fig. 9.An instance for a constructed span tree and extracting a consecutive span from the span tree.</p>
<p>Lemma 1 (
1
Strong convexity of h) Given a vocabulary V and a specific target index y ∈ [0, V ), let p ∈ (0, 1) represent prediction probability in the y-th dimension output by the sequence model and the negative likelihood h(p) = −log(p) ∈ R denote the optimization objective function of p. h(p) is λ(= 4)-strongly convex for on the feasible region of p.</p>
<p>where the expression on the left hand side represents the original seq-to-seq loss function E xs,ys [h(p θ (y * |y s , x s ⊕ x * )) with an additional regularization term g∈G ∥p θ (y* |g•y s , (g• x s ) ⊕ x * ) − p θ (y * |y s , x s ⊕ x * )∥ 2 2] and the expression on the right hand side represents the seq-to-seq loss function expected on the augmented datasetE xs,ys E g [p θ (y * |g•y s , (g•x s )⊕x * )].Proof Let x s (y s ) represents the original surrounding text in the input (output) and g ∈ G represents a substitution operation.Hence (x s ⊕ x * ), (y s ⊕ y * ) represents an original example and (g • x s ⊕ x * ), (g • y s ⊕ y * ) represents an augmented example.Considering the loss objective function on (g• x s ⊕ x * ), (g • y s ⊕ y * ), h(p θ (y * |g • y s , (g • x s ) ⊕ x * )),we have that (according to Lemma 1): h(p θ (y * |g • ys, (g • xs) ⊕ x * )) ≥ h(p θ (y * |ys, xs ⊕ x * )) + ∇h(p θ (y * |ys, xs ⊕ x * )(p θ (g • ys, (g • xs) ⊕ x * ) − p θ (y * |ys, xs ⊕ x * )) + λ 2 ∥p θ (g • ys, (g • xs) ⊕ x * ) − p θ (y * |ys, xs ⊕ x * )∥ 2 2 .</p>
<p>Rcomp(h) = Raug(h) + (Rcomp(h) − Raug(h)) ≤ Raug(h) + LW1(Xaug, Xcomp) = E (x,y)∼Xaug [l(h(x), y)] + LW1(Xaug, Xcomp) ≤ E (x,y)∼Xaug [max g∈G l(h(g • x), g • y)] + LW1(Xaug, Xcomp) = Raug( l • h) + LW1(Xaug, Xcomp) ≤ RS ( l • h)</p>
<p>•</p>
<p>We extend the key ideas of CompSub and LCS to ICL scenarios of LLMs, proposing LCS-ICL to enhance the few-shot compositional generalization capacity of stateof-the-art LLMs.We conduct comprehensive experiments on the COGS-QL dataset and show that LCS-ICL can exceed previous SOTA methods by at most 8.8%.</p>
<p>, TableII, TableIIIand Table IV respectively, where with SCAN, COGS and GeoQuery tasks we mainly test the performance of training (or fine-tuning) version of our proposed approaches and with COGS-QL task we mainly test the performance of ICL version of our proposed approaches.It is noteworthy that in Table I, II and III, "+CompSub" refers to directly leveraging Algorithm 1 to generate additional training data and train (or fine-tune in the case of pre-trained LMs) the model on the original training data and the generated additional data as well; "+CompSub+LCS" refers to leveraging Algorithm 2 to train models with the LCS framework on the original training data and the additional data generated by CompSub.In Table IV, "+LCS" refers to utilize the LCS-ICL Algorithm in the LLM and ICL scenarios.On SCAN and COGS tasks, we run each</p>
<p>TABLE I TEST
I
ACCURACY ON SCAN JUMP, AROUND RIGHT, LENGTH AND MCD SPLITS.
MethodJumpSCAN-Original [2] Around RightLengthMCD1SCAN-MCD [17] MCD2MCD3CGPS [37]98.8%± 1.4%83.2%± 13.2%20.3%± 1.1%1.2%± 1.0%1.7%± 2.0%0.6%± 0.3%GECA+MAML [46]---58.9%± 6.4%34.5%± 2.5%12.3%± 4.9%Comp-IBT [76]99.6%37.8%77.7%64.3%80.8%52.2%T5-11B [13]98.3%49.2%2.0%7.9%2.4%16.2%Seq-to-Seq [9]1.3%± 0.4%10.2%± 4.6%12.5%± 2.5%8.9%± 1.6%11.9%± 9.4%6.0%± 0.9%+GECA [23]95.2%± 8.0%84.3%± 6.3%12.8%±2.1%23.4%± 9.1%25.5%± 8.8%10.9%± 4.6%+LexLearn [41]91.2%± 11.9%95.3%±1.6%13.8%±3.0%12.5%± 2.0%19.3%± 1.9%11.6%± 0.9%+LexSym [21]100.0%± 0.0%84.0%±7.1%14.3%±2.7%47.4%± 7.1%30.8%± 8.4%13.7%± 3.6%+Prim2PrimX+MET [26]7.3%± 5.6%97.6%± 1.0%15.2%±4.5%31.5%± 4.1%33.5%± 2.7%11.6%± 1.0%+GECA+MAML [46]95.8%± 6.9%86.2%± 5.6%13.5%±2.2%28.2%± 9.6%31.8%± 8.5%11.2%± 4.2%+CompSub (Ours)100.0%± 0.0%99.9%±0.1%85.6%± 8.8%63.4%± 13.1%72.9%± 10.1%74.0%± 10.2%+CompSub+LCS (Ours)100.0%± 0.0%100.0%± 0.0%97.1%± 3.2%67.4%± 12.1%73.0%± 10.1%80.2%± 1.8%Transformer [11]3.5%± 1.7%19.9%± 10.4%12.2%± 5.6%1.7%± 0.7%4.3%± 1.3%4.4%± 1.2%+GECA [23]90.8%± 2.6%88.8%± 2.0%13.5%± 6.3%5.2%± 1.4%8.4%± 1.7%7.8%± 2.0%+LexSym [21]91.7%± 0.7%90.5%± 3.1%16.7%± 3.4%19.0%± 2.8%78.9%± 6.4%61.0%± 5.5%+Prim2PrimX+MET [26]62.5%± 15.5%62.4%± 24.4%26.2%± 8.3%20.5%± 3.2%77.4%± 10.7%58.5%± 2.7%+CompSub(Ours)92.4%± 1.1%92.1%± 1.5%93.5%± 0.9%24.8%± 1.7%79.4%± 1.5%61.3%± 0.9%+CompSub+LCS (Ours)92.2%± 1.6%93.9%± 1.5%95.4%± 1.0%27.0%± 4.4%80.2%± 1.9%63.3%± 2.3%</p>
<p>TABLE II OVERALL
II
TEST ACCURACY ON COGS DATASET.</p>
<p>TABLE III TEST
III
ACCURACY ON GEOQUERY QUESTION (I.I.D.) AND QUERY (COMPOSITIONAL) SPLITS.</p>
<p>the original training set only contains 519 examples.)with different augmentation methods, including GECA, LexSym, SUBS and CompSub in Table VI.It is noteworthy that Comp-Sub (augmenting to near 100, 000 examples) overwhelmingly outweighs other augmentation methods (less than 30, 000 examples), which reflects its superiority of exploring potential compositions of substantial substructures in the entire training</p>
<p>TABLE IV TEST
IV
ACCURACY ON COGS-QL WITH IN-CONTEXT LEARNING (OUT-OF-DISTRIBUTION PERFORMANCE).
ModelLLaMA2-13BLLaMA3-8BMethod and Shot61218246121824Basic Prompting Techniques+Standard ICL [14]15.0%17.5%24.5%30.5%18.0%23.6%28.6%31.3%+CoT Prompt [53]19.0%22.0%18.0%21.5%21.3%25.8%28.9%29.6%Selecting Relevant Demonstrations+PrimCoverage [19]34.5%39.5%43.0%44.5%43.5%48.3%48.8%50.0%+BM25 [55]12.5%18.0%21.0%17.0%19.7%24.9%33.3%28.6%Selecting Informative Demonstrations+CSR [35]39.5%41.5%44.0%44.0%27.8%32.4%31.7%32.8%+BSR [35]42.0%43.0%44.0%44.5%33.0%35.1%37.0%37.8%+GSR [36]42.5%44.0%44.0%42.5%29.8%33.9%35.1%36.0%Augmenting Demonstrations+GECA [23]30.5%35.0%37.0%38.5%40.6%44.8%45.6%46.3%+LexSym [21]29.0%36.0%39.5%40.5%41.7%46.1%47.5%47.7%+LCS-ICL(random version)35.5%39.5%42.0%46.0%45.3%50.5%54.4%55.9%+LCS-ICL(learned version)37.0%42.0%44.5%47.5%45.5%52.2%57.6%58.7%</p>
<p>TABLE V SPECIFIC
V
EXAMPLES OF DIFFERENT GENERALIZATION TYPES IN COGS.WE INCLUDE A LEXICAL GENERALIZATION CASE AND THREE STRUCTURAL GENERALIZATION CASES (CORRESPONDING TO OBJ PP TO SUB PP, PP RECURSION AND CP RECURSION, RESPECTIVELY).
generalization typetraininggeneralizationlexicalLina gave the cake to Olivia.The cat liked Lina.
obj pp to sub pp Noah ate the cake on the plate.The cake on the table burned.pp recursion Ava saw the ball in the bottle on the table.Ava saw the ball in the bottle on the table on the floor.cp recursion Emma said that Noah knew that the cat danced.Emma said that Noah knew that Lucas saw that the cat danced.</p>
<p>TABLE IX THE
IX
IMPORTANCE OF ADOPTING THE SELECTIVE AUGMENTATION MODULE IN THE LCS FRAMEWORK: COMPARISON BETWEEN THE EXPERIMENTAL GROUP (DENOTED BY "+LEARNED AUGMENTATION") AND THE CONTROL GROUP (DENOTED BY "+RANDOM AUGMENTATION")ON THREE SCAN-MCD SPLITS.
MethodSCAN-MCD [17] MCD1 MCD2 MCD3Seq-to-Seq [9]8.9%11.9%6.0%+Random Augmentation46.6%52.3%58.8%+Learned Augmentation55.1%54.3%70.8%+CompSub63.4%72.9%74.0%+CompSub+Random Aug.63.3%66.2%71.2%+CompSub+Learned Aug.67.4%73.0%80.2%Transformer [11]1.7%4.3%4.4%+Random Augmentation11.2%37.0%48.1%+Learned Augmentation19.3%68.1%57.8%+CompSub24.8%79.4%61.3%+CompSub+Random Aug.21.0%80.2%60.3%+CompSub+Learned Aug.27.0%80.2%63.3%</p>
<p>TABLE X COMPARING
X
THE PERFORMANCE OF TWO OTHER VERSIONS OF LCS-ICL ALGORITHM ON LLAMA3-8B.THE FIRST ROW REPRESENTS THE RESULTS FOR SHUFFLING THE ORDER OF THE DEMONSTRATIONS THAT ARE SELECTED BY THE ORIGINAL LCS-ICL ALGORITHM.THE SECOND ROW REPRESENTS THE RESULTS FOR THE LCS-ICL ALGORITHM THAT SELECT ALL OF THE DEMONSTRATIONS IN THE FINE SCREENING STAGE AT ONCE INSTEAD OF SELECTING IN A SUCCESSIVE MANNER.
Shot6121824shuffling46.3%±0.4%52.6%±0.7%57.2%±0.7%58.6%±0.4%w.o. succ. select.43.8%52.2%54.8%56.9%</p>
<p>Eg,x s,ys [h(p θ (y * |g • ys, (g • xs) ⊕ x * ))] ≥ Ex s,ys [h(p θ (y * |ys, xs ⊕ x * ))] + Eg,x s ,ys [∇h(p θ (y * |ys, xs ⊕ x * ))(p θ (y * |g • ys, (g • xs) ⊕ x * ) − p θ (y * |ys, xs ⊕ x * ))]We denote that ∇h(p θ (y<em> |y s , x s ⊕ x * ))(p θ (y * |g • y s , (g • x s ) ⊕ x * ) − p θ (y * |y s , x s ⊕ x * )) ≜ ∆ 1 and p θ (y * |g • y s , (g • x s ) ⊕ x * )] − p θ (y * |y s , x s ⊕ x * ) ≜ ∆ 2 .With Assumption 2, we have the following derivations:Eg,x s ,ys [∆1] = Ex s,ys [∇h(xs, ys, x * )(Eg[∆2])] = 0. Eg,x s,ys [h(p θ (y * |g • ys, (g • xs) ⊕ x * ))] ≥ Ex s,ys [h(p .)and the formula of the conditional probability: we have p θ (y s ⊕ y * |x s ⊕ x When the training loss converge to zero, we have that the regularization term introduced in the Theorem 3 takes the value of zero:∥p θ (y * |g • y 1 s , (g • x 1 s ) ⊕ x * ) − p θ (y * |y 1 s , x 1 s ⊕ x * )∥ 2 2 = 0.In other words, p θ (y * |g • y 1 s , (g • x 1 s ) ⊕ x * ) = p θ (y * |y 1 s , x 1 s ⊕ x </em>) holds for any substitution operation g.If our compositional data augmentation can fully substitute the surrounding context (x s , y s ) of the component (x * , y * ), we can derive that the condition probability p θ (y * |y 1 s , x s ⊕ x * ) is independent to the selection (x s , y s ):p θ (y * |y 1 s , x s ⊕ x * ) = p θ (y * |x * ).Similarly, we have p θ (y 1 s |x * ⊕ x s ) = p θ (y 1 s |x s ), p θ (y 2 s |y * , y 1 s , x s ⊕ x * ) = p θ (y 1 s , x s ) (Considering (x * , y * )as the surrounding context of (x s , y s )).Hence we derive that:p(y s ⊕ y * |x s ⊕ x * ) = p θ (y 1 s |x s ⊕ x * )p θ (y * |y 1 s , x s ⊕ x * )p θ (y 2 s |y * , y 1 s , x s ⊕ x * ) = p θ (y1 s |x s ) • p θ (y * |x * ) • p θ (y 2 s |y 1 s , x s ) = p θ (y 1 s , y 2 s |x s ) • p θ (y * |x * ) = p θ (y s |x s ) • p θ (y * |x * ), which ends the proof.
+ Eg,x s ,ys [λ 2∥p θ (y * |g • ys, (g • xs) ⊕ x * ) − p θ (y * |ys, xs ⊕ x * )∥ 2 2 ].Then we derive that:
θ (y * |ys, xs ⊕ x * ))] + Eg,x s ,ys [ λ 2 ∥p θ (y * |g • ys, (g • xs) ⊕ x * ) − p θ (y * |ys, xs ⊕ x * )∥ 2 2 ].which finishes the proof.Corollary 2 In the ideal conditions, where the training loss converges to zero and our compositional data augmentation fully substitutes the surrounding context (x s , y s ) for every (x * , y * ), we can disentangle the language modeling probability:p θ (y s ⊕ y * |x s ⊕ x * ) = p θ (y s |x s ) • p θ (y * |x * ).Proof Following previous notations (y s = (y 1 s , y 2 s ), where y 1 s is ahead of y * and y 2 s is behind of y * * ) = p θ (y 1 s |x s ⊕ x * )p θ (y * |y 1 s , x s ⊕ x * )p θ (y 2 s |y * , y 1 s , x s ⊕ x * ).</p>
<p>TABLE XI 1
XI
-WASSERSTEIN DISTANCE BETWEEN THE AUGMENTED TRAINING DATA DISTRIBUTION AND THE REAL COMPOSITIONAL TESTING DATA DISTRIBUTION.
DatasetSUBS [25]LexSym [21]CompSub (Ours)SCAN [2]-2.372.10COGS [1]6.716.676.49
In this paper, We use the words of "part", "concept", "span" and "component" later interchangeably.
In our case in-order traversal of a multi-way tree is to traverse the most left child, traverse the root node and then traverse left childs from right to left in order.
If there is no such node, we specifiy that the first node in the in-order traversal sequence is v i .
In our initial experiments, we found that LCS method only slightly works on the MCD2 split of SCAN dataset when using 1 layer LSTM-based model as the down-stream sequence model. However, in the following experiments in TableIX, we found that it works well on other 2 down-stream sequence models (we set warm-up epoch number to 5 for other down-stream seq-to-seq models).
https://huggingface.co/meta-llama/Llama-2-13b-hf
https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
The official github repo is https://github.com/google-research/ google-research/tree/master/cfq#scan-mcd-splits, and one can download the dataset from https://storage.cloud.google.com/cfq dataset/scan-splits.tar.gz
ACKNOWLEDGEMENT REFERENCESJ. Pseudo codes for the LCS-ICL algorithmWe present the pseudo codes for the LCS-ICL algorithm in Algorithm 3: we conduct the coarse screening in line 6 ∼ 22, the demonstration augmentation in line 24 and the fine screening in line 28 ∼ 37.K. Calculating the Wasserstein distance between the augmented data distribution and the compositional testing data distributionIn our paper, we argue that the value of the expression LW 1 (X aug , X comp ) depends on the inherent ability of the data augmentation scheme: in comparison with previous compositional data augmentation scheme (e.g., SUBS[25]and LexSym[21]), CompSub can achieve a relatively low 1-Wasserstein distance between the augmented training data distribution with the true compositional testing data distribution.We calculate the 1-Wasserstein distances between the augmented training data (generated by our algorithm CompSub, LexSym[21]and SUBS[25]) and the true compositional testing data.We conduct experiments on the SCAN dataset (MCD1 split) and the COGS dataset.The results are shown in TableXI.We observe that CompSub indeed achieves lower 1-Wasserstein distance (to the real compositional testing data) compared with previous baseline augmentation methods (LexSym and SUBS).Note that for the experiments on the SCAN dataset, since the vocabulary set is small, we use the 3-gram language modeling method to encode each sentence; for the experiments on the COGS dataset, since the vocabulary set is large, we use a pre-trained sentence embedding model (paraphrase-MiniLM-L6-v2 10 ) to encode each sentence.To calculate the wasserstein distance, we use the off-the-shelf POT (Python Optimal Transport) package [?]. 10 https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2 Algorithm 3: LCS-ICL Input: Original demonstration pool D, Query input x q , The number of the in-context demonstrations k, Large language model f θ (•).Output: k demonstrations C = {(inp i , out i )} k i=1 . 1 target = Tokenize(x q ); ▷ Tokenize(x) will return a set of tokens that are covered in x.
COGS: A compositional generalization challenge based on semantic interpretation. N Kim, T Linzen, EMNLP. Nov. 2020</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. B M Lake, M Baroni, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. J G Dy, A Krause, the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLRJuly 10-15, 2018. 201880ser. Proceedings of Machine Learning Research</p>
<p>Curriculum learning for human compositional generalization. R B Dekker, F Otto, C Summerfield, 10.1073/pnas.2205582119Proceedings of the National Academy of Sciences. 11941e22055821192022</p>
<p>Human-like systematic generalization through a meta-learning neural network. B M Lake, M Baroni, 10.1038/s41586-023-06668-3Nature. 6237985Nov 2023</p>
<p>Natural language instructions induce compositional generalization in networks of neurons. R Riveland, A Pouget, 10.1038/s41593-024-01607-5Nature Neuroscience. 275May 2024</p>
<p>Sequential compositional generalization in multimodal models. S Yagcioglu, O B İnce, A Erdem, E Erdem, D Elliott, D Yuret, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJun. 20241</p>
<p>Syntactic Structures. The Hague: Mouton and Co. N Chomsky, 1957</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 981997</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, NIPS. 2014</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, 2014</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin ; I. Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S , Advances in Neural Information Processing Systems. R Vishwanathan, Garnett, Curran Associates, Inc201730</p>
<p>BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H T , 2023</p>
<p>The llama 3 herd of models. A D , 2024</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. D Keysers, N Schärli, N Scales, H Buisman, D Furrer, S Kashubin, N Momchev, D Sinopalnikov, L Stafiniak, T Tihon, D Tsarkov, X Wang, M Van Zee, O Bousquet, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. OpenReview.net, 20202020</p>
<p>Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. D Furrer, M Van Zee, N Scales, N Schärli, CoRR. 2007.08970. 2020</p>
<p>How do in-context examples affect compositional generalization?. S An, Z Lin, Q Fu, B Chen, N Zheng, J.-G Lou, D Zhang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 2023152</p>
<p>Permutation equivariant models for compositional generalization in language. J Gordon, D Lopez-Paz, M Baroni, D Bouchacourt, International Conference on Learning Representations. 2020</p>
<p>LexSym: Compositionality as lexical symmetry. E Akyurek, J Andreas, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>Equi-tuning: Group equivariant fine-tuning of pretrained models. S Basu, P Sattigeri, K N Ramamurthy, V Chenthamarakshan, K R Varshney, L R Varshney, P Das, abs/2210.06475ArXiv. 2022</p>
<p>Good-enough compositional data augmentation. J Andreas, ACL. Jul. 2020</p>
<p>Sequence-level mixed sample data augmentation. D Guo, Y Kim, A Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNov. 2020Online</p>
<p>Subs: Subtree substitution for compositional semantic parsing. J Yang, L Zhang, D Yang, 2022the Association for Computational Linguisticsin North American Chapter</p>
<p>Mutual exclusivity training and primitive augmentation to induce compositionality. Y Jiang, X Zhou, M Bansal, ArXiv. 2211.15578, 2022</p>
<p>Unobserved local structures make compositional generalization hard. B Bogin, S Gupta, J Berant, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Learning to substitute spans towards improving compositional generalization. Z Li, Y Wei, D Lian, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>Invariant risk minimization. M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, 2020</p>
<p>On the benefits of invariance in neural networks. C Lyle, M Van Der Wilk, M Kwiatkowska, Y Gal, B Bloem-Reddy, 2020</p>
<p>Spurious correlations in machine learning: A survey. W Ye, G Zheng, X Cao, Y Ma, A Zhang, 2024</p>
<p>Rademacher and gaussian complexities: risk bounds and structural results. P L Bartlett, S Mendelson, J. Mach. Learn. Res. 3mar 2003</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, C Zheng, J Ma, R Li, H Xia, J Xu, Z Wu, T Liu, B Chang, X Sun, L Li, Z Sui, 2024</p>
<p>Compositional semantic parsing with large language models. A Drozdov, N Schärli, E Akyürek, N Scales, X Song, X Chen, O Bousquet, D Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Coverage-based example selection for in-context learning. S Gupta, M Gardner, S Singh, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational LinguisticsDec. 202313950</p>
<p>Gistscore: Learning better representations for in-context example selection with gist bottlenecks. S Gupta, C Rosenbaum, E R Elenberg, Forty-first International Conference on Machine Learning. 2024</p>
<p>Compositional generalization for primitive substitutions. Y Li, L Zhao, J Wang, J Hestness, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsNov. 2019</p>
<p>Compositional generalization by factorizing alignment and translation. J Russin, J Jo, R O'reilly, Y Bengio, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. the 58th Annual Meeting of the Association for Computational Linguistics: Student Research WorkshopAssociation for Computational LinguisticsJul. 2020</p>
<p>Efficient model-agnostic multi-group equivariant networks. R Baltaji, S Basu, L R Varshney, Transactions on Machine Learning Research. 2024</p>
<p>Making transformers solve compositional tasks. S Ontanon, J Ainslie, Z Fisher, V Cvicek, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Lexicon learning for few shot sequence modeling. E Akyurek, J Andreas, ACL. Aug. 2021</p>
<p>Learning to compose representations of different encoder layers towards improving compositional generalization. L Lin, S Li, Y Zheng, B Fu, S Liu, Y Chen, X Shi, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Layer-wise representation fusion for compositional generalization. Y Zheng, L Lin, S Li, Y Yuan, Z Lai, S Liu, B Fu, Y Chen, X Shi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438714</p>
<p>Compositional generalization for neural semantic parsing via span-level supervised attention. P Yin, H Fang, G Neubig, A Pauls, E A Platanios, Y Su, S Thomson, J Andreas, North American Chapter. the Association for Computational Linguistics2021</p>
<p>Compositional Generalization through Meta Sequence-to-Sequence Learning. B M Lake, 2019Curran Associates IncRed Hook, NY, USA</p>
<p>Meta-learning to compositionally generalize. H Conklin, B Wang, K Smith, I Titov, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAug. 20211</p>
<p>mixup: Beyond empirical risk minimization. H Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, International Conference on Learning Representations. 2018</p>
<p>Align and augment: Generative data augmentation for compositional generalization. F Cazzaro, D Locatelli, A Quattoni, Proceedings of the 18th Conference of the European Chapter. Long Papers. St, Julian, the 18th Conference of the European Chapter's, MaltaAssociation for Computational LinguisticsMar. 20241</p>
<p>Span-based semantic parsing for compositional generalization. J Herzig, J Berant, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAug. 20211</p>
<p>A simple, fast, and effective reparameterization of ibm model 2. C Dyer, V Chahuneau, N A Smith, 2013the Association for Computational Linguisticsin North American Chapter</p>
<p>Evaluating the impact of model scale for compositional generalization in semantic parsing. L Qiu, P Shaw, P Pasupat, T Shi, J Herzig, E Pitler, F Sha, K Toutanova, 2022</p>
<p>On the compositional generalization gap of in-context learning. A Hosseini, A Vani, D Bahdanau, A Sordoni, A Courville, ; , J Bastings, Y Belinkov, Y Elazar, D Hupkes, N Saphra, S Wiegreffe, Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. Abu Dhabi, the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPUnited Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Ichter, E Xia, Q V Chi, D Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, A Mohamed, D Agarwal, K Belgrave, A Cho, Oh, Curran Associates, Inc202235</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q V Le, E H Chi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>The probabilistic relevance framework: Bm25 and beyond. S Robertson, H Zaragoza, 10.1561/1500000019Found. Trends Inf. Retr. 34apr 2009</p>
<p>Learning to compress prompts with gist tokens. J Mu, X L Li, N Goodman, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Unlocking compositional generalization in pre-trained models using intermediate representations. J Herzig, P Shaw, M.-W Chang, K Guu, P Pasupat, Y Zhang, abs/2104.07478ArXiv. 2021</p>
<p>Finding needles in a haystack: Sampling structurally-diverse training sets from synthetic data for compositional generalization. I Oren, J Herzig, J Berant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNov. 202110Online and Punta Cana</p>
<p>Categorical reparameterization with gumbel-softmax. E Jang, S Gu, B Poole, International Conference on Learning Representations. 2017</p>
<p>What learning algorithm is in-context learning? investigations with linear models. E Akyürek, D Schuurmans, J Andreas, T Ma, D Zhou, 2023</p>
<p>Transformers learn in-context by gradient descent. J Oswald, E Niklasson, E Randazzo, J Sacramento, A Mordvintsev, A Zhmoginov, M Vladymyrov, 2023</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, 2023</p>
<p>Long-context llms struggle with long in-context learning. T Li, G Zhang, Q D Do, X Yue, W Chen, 2024</p>
<p>Learning generalizable models via disentangling spurious and enhancing potential correlations. N Wang, L Qi, J Guo, Y Shi, Y Gao, IEEE Transactions on Image Processing. 332024</p>
<p>Invariant representation via decoupling style and spurious features from images. R Li, Y Pu, Z Li, H Xie, D Lian, 2024</p>
<p>A group-theoretic framework for data augmentation. S Chen, E Dobriban, J H Lee, Journal of Machine Learning Research. 212452020</p>
<p>Permutation Groups, ser. Dover books on mathematics. D Passman, 2012Dover PublicationsIncorporated</p>
<p>The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective. C.-H Lin, C Kaushik, E L Dyer, V Muthukumar, Journal of Machine Learning Research. 25912024</p>
<p>Provably strict generalisation benefit for equivariant models. B Elesedy, S Zaidi, Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning. M Research, T Meila, Zhang, the 38th International Conference on Machine Learning, ser. Machine LearningPMLR18-24 Jul 2021139</p>
<p>Understanding compositional data augmentation in typologically diverse morphological inflection. F Samir, M Silfverberg, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. J Bouamor, K Pino, Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>A general theory for compositional generalization. J Fu, Z Zhang, Y Lu, N Zheng, 2024</p>
<p>Tighter expected generalization error bounds via wasserstein distance. B Rodríguez-Gálvez, G Bassi, R Thobaben, M Skoglund, Proceedings of the 35th International Conference on Neural Information Processing Systems, ser. NIPS '21. the 35th International Conference on Neural Information Processing Systems, ser. NIPS '21Red Hook, NY, USACurran Associates Inc2024</p>
<p>Understanding the generalization benefit of model invariance from a data perspective. S Zhu, B An, F Huang, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P Liang, J W Vaughan, Curran Associates, Inc202134</p>
<p>Learning to parse database queries using inductive logic programming. J M Zelle, R J Mooney, AAAI/IAAI. 19962</p>
<p>Compositional generalization and natural language variation: Can a semantic parsing approach handle both?. P Shaw, M.-W Chang, P Pasupat, K Toutanova, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAug. 20211</p>
<p>Revisiting iterative back-translation from the perspective of compositional generalization. Y Guo, H Zhu, Z Lin, B Chen, J.-G Lou, D Zhang, AAAI Conference on Artificial Intelligence. 2020</p>
<p>Learning to transform natural to formal languages. R J Kate, Y W Wong, R J Mooney, AAAI Conference on Artificial Intelligence. 2005</p>
<p>Improving text-to-SQL evaluation methodology. C Finegan-Dollak, J K Kummerfeld, L Zhang, K Ramanathan, S Sadasivam, R Zhang, D Radev, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJul. 20181</p>
<p>Does deep learning learn to abstract? a systematic probing framework. S An, Z Lin, B Chen, Q Fu, N Zheng, J.-G Lou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Disentangled sequence to sequence learning for compositional generalization. H Zheng, M Lapata, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2022</p>
<p>Diverse demonstrations improve in-context compositional generalization. I Levy, B Bogin, J Berant, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>context2vec: Learning generic context embedding with bidirectional LSTM. O Melamud, J Goldberger, I Dagan, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language LearningBerlin, GermanyAssociation for Computational LinguisticsAug. 2016</p>
<p>M Mohri, A Rostamizadeh, A Talwalkar, Foundations of Machine Learning. The MIT Press2012</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, CoRR. 69802014</p>
<p>Get to the point: Summarization with pointer-generator networks. A See, P J Liu, C D Manning, 2017</p>
<p>OpenNMT: Open-source toolkit for neural machine translation. G Klein, Y Kim, Y Deng, J Senellart, A Rush, Proceedings of ACL 2017, System Demonstrations. ACL 2017, System DemonstrationsVancouver, CanadaAssociation for Computational LinguisticsJul. 2017</p>
<p>fairseq: A fast, extensible toolkit for sequence modeling. M Ott, S Edunov, A Baevski, A Fan, S Gross, N Ng, D Grangier, M Auli, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational LinguisticsJun. 2019</p>
<p>Data recombination for neural semantic parsing. R Jia, P Liang, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsAug. 20161</p>
<p>Improving compositional generalization with latent structure and data augmentation. L Qiu, P Shaw, P Pasupat, P Nowak, T Linzen, F Sha, K Toutanova, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsJul. 2022llama2-13b"]={ "bos_token_id": 1, "do_sample": True, "eos_token_id": 2, "pad_token_id": 0, "temperature": 0.6, "max_length": 100, "top_p": 0.9, "transformers_version": "4.31.0.dev0" }</p>
<p>} I. Other Training and Inference Details We conduct all of our training experiments on eight NVIDIA GeForce RTX2080Ti GPUs. For jump and around right splits of SCAN, COGS and GeoQuery, we select our model for testing with the best development accuracy. We conduct all of our LLM In-Context Learning inference experiments on two NVIDIA A100 GPUs. Note that we use 16 bit quantization for both LLaMA2-13B and LLaMA3-8B. For all MCD splits of SCAN. for LLaMA3-8B, the generation configuration is: GEN_CONFIGS["llama3-8b"]={ "bos_token_id": 128000, "do_sample": True, "eos_token_id": [128000,128009], "temperature": 0.6, "max_length": 300, "top_p": 0.9, "transformers_version": "4.31.0.dev0". we use the train/dev/test splits from the original paper [17] 9 , we also select our model for testing with the best accuracy on dev set</p>            </div>
        </div>

    </div>
</body>
</html>